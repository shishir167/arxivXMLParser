<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:05:29Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|37001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3371</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3371</id><created>2012-10-11</created><updated>2013-10-03</updated><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Holzer</keyname><forenames>Stephan</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>The Power of Non-Uniform Wireless Power</title><categories>cs.DS</categories><comments>Appeared in SODA 2013, 16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a fundamental measure for wireless interference in the SINR model
known as (weighted) inductive independence. This measure characterizes the
effectiveness of using oblivious power --- when the power used by a transmitter
only depends on the distance to the receiver --- as a mechanism for improving
wireless capacity.
  We prove optimal bounds for inductive independence, implying a number of
algorithmic applications. An algorithm is provided that achieves --- due to
existing lower bounds --- capacity that is asymptotically best possible using
oblivious power assignments. Improved approximation algorithms are provided for
a number of problems for oblivious power and for power control, including
distributed scheduling, connectivity, secondary spectrum auctions, and dynamic
packet scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3375</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3375</id><created>2012-10-11</created><authors><author><keyname>Ezzeddine</keyname><forenames>Benaissa</forenames></author><author><keyname>Abdellatif</keyname><forenames>Benabdelhafid</forenames></author><author><keyname>Mounir</keyname><forenames>Benaissa</forenames></author></authors><title>An Agent-based framework for cooperation in Supply Chain</title><categories>cs.AI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  5, No 3, September 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supply Chain coordination has become a critical success factor for Supply
Chain management (SCM) and effectively improving the performance of
organizations in various industries. Companies are increasingly located at the
intersection of one or more corporate networks which are designated by &quot;Supply
Chain&quot;. Managing this chain is mainly based on an 'information sharing' and
redeployment activities between the various links that comprise it. Several
attempts have been made by industrialists and researchers to educate
policymakers about the gains to be made by the implementation of cooperative
relationships. The approach presented in this paper here is among the works
that aim to propose solutions related to information systems distributed Supply
Chains to enable the different actors of the chain to improve their
performance. We propose in particular solutions that focus on cooperation
between actors in the Supply Chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3384</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3384</id><created>2012-10-11</created><updated>2013-11-02</updated><authors><author><keyname>Jiao</keyname><forenames>Wei</forenames></author><author><keyname>Vembu</keyname><forenames>Shankar</forenames></author><author><keyname>Deshwar</keyname><forenames>Amit G.</forenames></author><author><keyname>Stein</keyname><forenames>Lincoln</forenames></author><author><keyname>Morris</keyname><forenames>Quaid</forenames></author></authors><title>Inferring clonal evolution of tumors from single nucleotide somatic
  mutations</title><categories>cs.LG q-bio.PE q-bio.QM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-throughput sequencing allows the detection and quantification of
frequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor
cell populations. In some cases, the evolutionary history and population
frequency of the subclonal lineages of tumor cells present in the sample can be
reconstructed from these SNV frequency measurements. However, automated methods
to do this reconstruction are not available and the conditions under which
reconstruction is possible have not been described.
  We describe the conditions under which the evolutionary history can be
uniquely reconstructed from SNV frequencies from single or multiple samples
from the tumor population and we introduce a new statistical model, PhyloSub,
that infers the phylogeny and genotype of the major subclonal lineages
represented in the population of cancer cells. It uses a Bayesian nonparametric
prior over trees that groups SNVs into major subclonal lineages and
automatically estimates the number of lineages and their ancestry. We sample
from the joint posterior distribution over trees to identify evolutionary
histories and cell population frequencies that have the highest probability of
generating the observed SNV frequency data. When multiple phylogenies are
consistent with a given set of SNV frequencies, PhyloSub represents the
uncertainty in the tumor phylogeny using a partial order plot. Experiments on a
simulated dataset and two real datasets comprising tumor samples from acute
myeloid leukemia and chronic lymphocytic leukemia patients demonstrate that
PhyloSub can infer both linear (or chain) and branching lineages and its
inferences are in good agreement with ground truth, where it is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3395</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3395</id><created>2012-10-11</created><updated>2014-02-13</updated><authors><author><keyname>Eftekhari</keyname><forenames>Armin</forenames></author><author><keyname>Yap</keyname><forenames>Han Lun</forenames></author><author><keyname>Rozell</keyname><forenames>Christopher J.</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>The Restricted Isometry Property for Random Block Diagonal Matrices</title><categories>cs.IT math.IT math.PR</categories><msc-class>94A20, 60B20, 46B09,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Compressive Sensing, the Restricted Isometry Property (RIP) ensures that
robust recovery of sparse vectors is possible from noisy, undersampled
measurements via computationally tractable algorithms. It is by now well-known
that Gaussian (or, more generally, sub-Gaussian) random matrices satisfy the
RIP under certain conditions on the number of measurements. Their use can be
limited in practice, however, due to storage limitations, computational
considerations, or the mismatch of such matrices with certain measurement
architectures. These issues have recently motivated considerable effort towards
studying the RIP for structured random matrices. In this paper, we study the
RIP for block diagonal measurement matrices where each block on the main
diagonal is itself a sub-Gaussian random matrix. Our main result states that
such matrices can indeed satisfy the RIP but that the requisite number of
measurements depends on certain properties of the basis in which the signals
are sparse. In the best case, these matrices perform nearly as well as dense
Gaussian random matrices, despite having many fewer nonzero entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3404</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3404</id><created>2012-10-11</created><updated>2012-10-15</updated><authors><author><keyname>van der Walt</keyname><forenames>St&#xe9;fan J.</forenames></author><author><keyname>Herbst</keyname><forenames>B. M.</forenames></author></authors><title>A polygon-based interpolation operator for super-resolution imaging</title><categories>cs.CV</categories><comments>10 pages; update typo in abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline the super-resolution reconstruction problem posed as a
maximization of probability. We then introduce an interpolation method based on
polygonal pixel overlap, express it as a linear operator, and use it to improve
reconstruction. Polygon interpolation outperforms the simpler bilinear
interpolation operator and, unlike Gaussian modeling of pixels, requires no
parameter estimation. A free software implementation that reproduces the
results shown is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3412</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3412</id><created>2012-10-11</created><authors><author><keyname>Wee</keyname><forenames>Loo Kang</forenames></author><author><keyname>Lye</keyname><forenames>Sze Yee</forenames></author></authors><title>Designing Open Source Computer Models for Physics by Inquiry using Easy
  Java Simulation</title><categories>physics.ed-ph cs.CY physics.comp-ph</categories><comments>3 pages, 1 Figure, 20th International Conference on Computers in
  Education (ICCE 2012) Singapore Interactive Event Ministry of Education,
  Education Technology Division, Singapore</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Open Source Physics community has created hundreds of physics computer
models (Wolfgang Christian, Esquembre, &amp; Barbato, 2011; F. K. Hwang &amp;
Esquembre, 2003) which are mathematical computation representations of
real-life Physics phenomenon. Since the source codes are available and can be
modified for redistribution licensed Creative Commons Attribution or other
compatible copyrights like GNU General Public License (GPL), educators can
customize (Wee &amp; Mak, 2009) these models for more targeted productive (Wee,
2012) activities for their classroom teaching and redistribute them to benefit
all humankind. In this interactive event, we will share the basics of using the
free authoring toolkit called Easy Java Simulation (W. Christian, Esquembre, &amp;
Mason, 2010; Esquembre, 2010) so that participants can modify the open source
computer models for their own learning and teaching needs. These computer
models has the potential to provide the experience and context, essential for
deepening students conceptual understanding of Physics through student centred
guided inquiry approach (Eick, Meadows, &amp; Balkcom, 2005; Jackson, Dukerich, &amp;
Hestenes, 2008; McDermott, Shaffer, &amp; Rosenquist, 1995; Wee, Lee, &amp; Goh, 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3420</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3420</id><created>2012-10-11</created><authors><author><keyname>Zhang</keyname><forenames>Bin</forenames></author><author><keyname>Thomas</keyname><forenames>A. C.</forenames></author><author><keyname>Doreian</keyname><forenames>Patrick</forenames></author><author><keyname>Krackhardt</keyname><forenames>David</forenames></author><author><keyname>Krishnan</keyname><forenames>Ramayya</forenames></author></authors><title>Contrasting Multiple Social Network Autocorrelations for Binary
  Outcomes, With Applications To Technology Adoption</title><categories>cs.SI physics.soc-ph stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of socially targeted marketing suggests that decisions made by
consumers can be predicted not only from their personal tastes and
characteristics, but also from the decisions of people who are close to them in
their networks. One obstacle to consider is that there may be several different
measures for &quot;closeness&quot; that are appropriate, either through different types
of friendships, or different functions of distance on one kind of friendship,
where only a subset of these networks may actually be relevant. Another is that
these decisions are often binary and more difficult to model with conventional
approaches, both conceptually and computationally. To address these issues, we
present a hierarchical model for individual binary outcomes that uses and
extends the machinery of the auto-probit method for binary data. We demonstrate
the behavior of the parameters estimated by the multiple network-regime
auto-probit model (m-NAP) under various sensitivity conditions, such as the
impact of the prior distribution and the nature of the structure of the
network, and demonstrate on several examples of correlated binary data in
networks of interest to Information Systems, including the adoption of Caller
Ring-Back Tones, whose use is governed by direct connection but explained by
additional network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3427</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3427</id><created>2012-10-12</created><authors><author><keyname>LI</keyname><forenames>Cheuk Ting</forenames></author></authors><title>On Multi-rate Sequential Data Transmission</title><categories>cs.IT math.IT</categories><comments>Final year project report for the degree of bachelor of Information
  Engineering, The Chinese University of Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we investigate the data transmission model in which a
sequence of data is broadcasted to a number of receivers. The receivers, which
have different channel capacities, wish to decode the data sequentially at
different rates. Our results are applicable to a wide range of scenarios. For
instance, it can be employed in the broadcast streaming of a video clip through
the internet, so that receivers with different bandwidths can play the video at
different speed. Receivers with greater bandwidths can provide a smooth
playback, while receivers with smaller bandwidths can play the video at a
slower speed, or with short pauses or rebuffering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3435</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3435</id><created>2012-10-12</created><authors><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>C.</forenames></author></authors><title>Multiple Service providers sharing Spectrum using Cognitive Radio in
  Wireless Communication Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>7 pages; International Journal of Scientific &amp; Engineering Research,
  Volume 3, Issue 3, March-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current utilization of the spectrum is quite inefficient; consequently,
if properly used, there is no shortage of the spectrum that is at present
available. Therefore, it is anticipated that more flexible use of spectrum and
spectrum sharing between radio systems will be key enablers to facilitate the
successful implementation of future systems. Cognitive radio, however, is known
as the most intelligent and promising technique in solving the problem of
spectrum sharing. In this paper, we consider the technique of spectrum sharing
among users of service providers to share the licensed spectrum of licensed
service providers. It is shown that the proposed technique reduces the call
blocking rate and improves the spectrum utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3437</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3437</id><created>2012-10-12</created><authors><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>C.</forenames></author></authors><title>Comparing Spectrum Utilization using Fuzzy Logic System for
  Heterogeneous Wireless Networks via Cognitive Radio</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages; International Journal of Scientific &amp; Engineering Research,
  Volume 3, Issue 7, July-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present, lots of works focus on spectrum allocation of wireless networks.
In this paper, we proposed a Cognitive based spectrum access by
opportunistically approach of Heterogeneous Wireless networks based on Fuzzy
Logic system. The Cognitive Radio is a technology where a network or a wireless
system changes its environment parameters to communicate efficiently by
avoiding the interference with the users. By applying FLS (Fuzzy Logic System),
the available spectrum utilization is effectively utilized with the help of the
three antecedents namely Spectrum utilization efficiency, Degree of mobility,
Distance from primary user to the secondary users. The proposed work is
compared with normal Spectrum Utilization method. Finally, Simulation results
of the proposed work Fuzzy Logic System shows more efficient than the normal
Spectrum utilization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3438</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3438</id><created>2012-10-12</created><authors><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Stochastic Surveillance Strategies for Spatial Quickest Detection</title><categories>cs.RO cs.MA</categories><comments>Conditionally accepted, International Journal of Robotic Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design persistent surveillance strategies for the quickest detection of
anomalies taking place in an environment of interest. From a set of predefined
regions in the environment, a team of autonomous vehicles collects noisy
observations, which a control center processes. The overall objective is to
minimize detection delay while maintaining the false alarm rate below a desired
threshold. We present joint (i) anomaly detection algorithms for the control
center and (ii) vehicle routing policies. For the control center, we propose
parallel cumulative sum (CUSUM) algorithms (one for each region) to detect
anomalies from noisy observations. For the vehicles, we propose a stochastic
routing policy, in which the regions to be visited are chosen according to a
probability vector. We study stationary routing policy (the probability vector
is constant) as well as adaptive routing policies (the probability vector
varies in time as a function of the likelihood of regional anomalies). In the
context of stationary policies, we design a performance metric and minimize it
to design an efficient stationary routing policy. Our adaptive policy improves
upon the stationary counterpart by adaptively increasing the selection
probability of regions with high likelihood of anomaly. Finally, we show the
effectiveness of the proposed algorithms through numerical simulations and a
persistent surveillance experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3448</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3448</id><created>2012-10-12</created><authors><author><keyname>Barriuso</keyname><forenames>Adela</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Notes on image annotation</title><categories>cs.CV cs.HC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are under the illusion that seeing is effortless, but frequently the
visual system is lazy and makes us believe that we understand something when in
fact we don't. Labeling a picture forces us to become aware of the difficulties
underlying scene understanding. Suddenly, the act of seeing is not effortless
anymore. We have to make an effort in order to understand parts of the picture
that we neglected at first glance.
  In this report, an expert image annotator relates her experience on
segmenting and labeling tens of thousands of images. During this process, the
notes she took try to highlight the difficulties encountered, the solutions
adopted, and the decisions made in order to get a consistent set of
annotations. Those annotations constitute the SUN database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3449</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3449</id><created>2012-10-12</created><updated>2013-01-23</updated><authors><author><keyname>Jithamithra</keyname><forenames>G. R.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Construction of Block Orthogonal STBCs and Reducing Their Sphere
  Decoding Complexity</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures; Minor changes in lemmas and constructions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Construction of high rate Space Time Block Codes (STBCs) with low decoding
complexity has been studied widely using techniques such as sphere decoding and
non Maximum-Likelihood (ML) decoders such as the QR decomposition decoder with
M paths (QRDM decoder). Recently Ren et al., presented a new class of STBCs
known as the block orthogonal STBCs (BOSTBCs), which could be exploited by the
QRDM decoders to achieve significant decoding complexity reduction without
performance loss. The block orthogonal property of the codes constructed was
however only shown via simulations. In this paper, we give analytical proofs
for the block orthogonal structure of various existing codes in literature
including the codes constructed in the paper by Ren et al. We show that codes
formed as the sum of Clifford Unitary Weight Designs (CUWDs) or Coordinate
Interleaved Orthogonal Designs (CIODs) exhibit block orthogonal structure. We
also provide new construction of block orthogonal codes from Cyclic Division
Algebras (CDAs) and Crossed-Product Algebras (CPAs). In addition, we show how
the block orthogonal property of the STBCs can be exploited to reduce the
decoding complexity of a sphere decoder using a depth first search approach.
Simulation results of the decoding complexity show a 30% reduction in the
number of floating point operations (FLOPS) of BOSTBCs as compared to STBCs
without the block orthogonal structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3456</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3456</id><created>2012-10-12</created><updated>2014-06-30</updated><authors><author><keyname>Zhong</keyname><forenames>Mingjun</forenames></author><author><keyname>Liu</keyname><forenames>Rong</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author></authors><title>Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data</title><categories>stat.AP cs.LG q-bio.GN q-bio.MN stat.ML</categories><comments>21 pages, 11 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play
important regulatory roles in post-transcriptional gene regulation by
inhibiting the translation of the mRNA into proteins or otherwise cleaving the
target mRNA. Inferring miRNA targets provides useful information for
understanding the roles of miRNA in biological processes that are potentially
involved in complex diseases. Statistical methodologies for point estimation,
such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,
have been proposed to identify the interactions of miRNA and mRNA based on
sequence and expression data. In this paper, we propose using the Bayesian
LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the
interactions between miRNA and mRNA using expression data. The proposed
Bayesian methods explore the posterior distributions for those parameters
required to model the miRNA-mRNA interactions. These approaches can be used to
observe the inferred effects of the miRNAs on the targets by plotting the
posterior distributions of those parameters. For comparison purposes, the Least
Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO
(nLASSO), and the proposed Bayesian approaches were applied to four public
datasets. We concluded that nLASSO and nBLASSO perform best in terms of
sensitivity and specificity. Compared to the point estimate algorithms, which
only provide single estimates for those parameters, the Bayesian methods are
more meaningful and provide credible intervals, which take into account the
uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,
Bayesian methods naturally provide statistical significance to select
convincing inferred interactions, while point estimate algorithms require a
manually chosen threshold, which is less meaningful, to choose the possible
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3491</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3491</id><created>2012-10-12</created><authors><author><keyname>Basu</keyname><forenames>Joydeep</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Tarun K.</forenames></author></authors><title>Microelectromechanical system cantilever-based frequency doublers</title><categories>cs.OH</categories><comments>The final, definitive version of this paper has been published in
  Journal of Intelligent Material Systems and Structures, 2012 by SAGE
  Publications Ltd. (http://online.sagepub.com), All rights reserved. Journal
  of Intelligent Material Systems and Structures, 2012</comments><doi>10.1177/1045389X12461695</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microelectromechanical system (MEMS) based on-chip resonators offer great
potential for high frequency signal processing circuits like reference
oscillators and filters. This is due to their exceptional features like small
size, large frequency-quality factor product, integrability with CMOS ICs, low
power consumption, low cost batch fabrication etc. A capacitively transduced
cantilever beam resonator is one such popular MEMS resonator topology. In this
letter, the inherent square-law nonlinearity of the voltage-to-force transfer
function of a cantilever resonator's capacitive transducer has been employed
for the realization of frequency doubling effect. Using this concept, frequency
doubling of input signals of 500 kHz to 1 MHz, and 227.5 kHz to 455 kHz has
been experimentally demonstrated for two cantilever beams of length 51.75 and
76.75 micrometer respectively. The MEMS cantilevers have been fabricated with
polysilicon using the PolyMUMPs surface micromachining process, and their
testing has been performed using Laser Doppler Vibrometry. The test results
obtained are in reasonable compliance with the analytical and CoventorWare
finite-element simulation results. The high efficiency demonstrated by the
cantilever frequency doubler makes it a promising choice for signal generation
at high frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3494</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3494</id><created>2012-10-12</created><updated>2012-10-24</updated><authors><author><keyname>Tehrani</keyname><forenames>Ali Soltani</forenames></author><author><keyname>Nemati</keyname><forenames>Hossein Mashad</forenames></author><author><keyname>Cao</keyname><forenames>Haiying</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Fager</keyname><forenames>Christian</forenames></author></authors><title>Varactor-Based Dynamic Load Modulation of High Power Amplifiers</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, dynamic load modulation of high power amplifiers using a
varactor-based tunable matching network is presented. The feasibility of
dynamic tuning and efficiency enhancement of this technique is demonstrated
using a modular design approach for two existing high efficiency power
amplifiers (PA), a 7-W class-E, and a 10-W class-J power amplifier PA at 1 GHz.
For this purpose and for each of the PAs, a simple quasi-static inverse model
is developed allowing an efficiency-optimized control of the PA and the
varactor-based tunable matching network. Modulated measurements using a single
carrier WCDMA signal with 11.3 dB peak-to-average ratio (PAR) indicate about 10
to 14 percentage units improvements in the average power-added efficiency (PAE)
for the complete architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3512</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3512</id><created>2012-10-12</created><updated>2012-10-25</updated><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>Digital Network Coding Aided Two-way Relaying: Energy Minimization and
  Queue Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a three node, two-way relay system with digital
network coding over static channels where all link gains are assumed to be
constant during transmission. The aim is to minimize total energy consumption
while ensuring queue stability at all nodes, for a given pair of random packet
arrival rates. Specifically, we allow for a set of transmission modes and solve
for the optimal fraction of resources allocated to each mode, including
multiaccess uplink transmission mode and network coding broadcasting mode. In
addition, for the downlink, we find the condition to determine whether
superposition coding with excess data over the better link and network coded
data for both users is energy efficient and the corresponding optimization is
formulated and solved. To tackle the queue evolution in this network, we
present a detailed analysis of the queues at each node using a random
scheduling method that closely approximates the theoretical design, through a
two-dimensional Markov chain model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3539</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3539</id><created>2012-10-11</created><updated>2013-01-09</updated><authors><author><keyname>Bohy</keyname><forenames>Aaron</forenames></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Synthesis from LTL Specifications with Mean-Payoff Objectives</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical LTL synthesis problem is purely qualitative: the given LTL
specification is realized or not by a reactive system. LTL is not expressive
enough to formalize the correctness of reactive systems with respect to some
quantitative aspects. This paper extends the qualitative LTL synthesis setting
to a quantitative setting. The alphabet of actions is extended with a weight
function ranging over the rational numbers. The value of an infinite word is
the mean-payoff of the weights of its letters. The synthesis problem then
amounts to automatically construct (if possible) a reactive system whose
executions all satisfy a given LTL formula and have mean-payoff values greater
than or equal to some given threshold. The latter problem is called LTLMP
synthesis and the LTLMP realizability problem asks to check whether such a
system exists. We first show that LTLMP realizability is not more difficult
than LTL realizability: it is 2ExpTime-Complete. This is done by reduction to
two-player mean-payoff parity games. While infinite memory strategies are
required to realize LTLMP specifications in general, we show that
epsilon-optimality can be obtained with finite memory strategies, for any
epsilon &gt; 0. To obtain an efficient algorithm in practice, we define a
Safraless procedure to decide whether there exists a finite-memory strategy
that realizes a given specification for some given threshold. This procedure is
based on a reduction to two-player energy safety games which are in turn
reduced to safety games. Finally, we show that those safety games can be solved
efficiently by exploiting the structure of their state spaces and by using
antichains as a symbolic data-structure. All our results extend to
multi-dimensional weights. We have implemented an antichain-based procedure and
we report on some promising experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3548</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3548</id><created>2012-10-12</created><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>De Pril</keyname><forenames>Julie</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author></authors><title>Multiplayer Cost Games with Simple Nash Equilibria</title><categories>cs.GT cs.LO</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplayer games with selfish agents naturally occur in the design of
distributed and embedded systems. As the goals of selfish agents are usually
neither equivalent nor antagonistic to each other, such games are non zero-sum
games. We study such games and show that a large class of these games,
including games where the individual objectives are mean- or discounted-payoff,
or quantitative reachability, and show that they do not only have a solution,
but a simple solution. We establish the existence of Nash equilibria that are
composed of k memoryless strategies for each agent in a setting with k agents,
one main and k-1 minor strategies. The main strategy describes what happens
when all agents comply, whereas the minor strategies ensure that all other
agents immediately start to co-operate against the agent who first deviates
from the plan. This simplicity is important, as rational agents are an
idealisation. Realistically, agents have to decide on their moves with very
limited resources, and complicated strategies that require exponential--or even
non-elementary--implementations cannot realistically be implemented. The
existence of simple strategies that we prove in this paper therefore holds a
promise of implementability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3552</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3552</id><created>2012-10-12</created><updated>2014-03-18</updated><authors><author><keyname>Skjegstad</keyname><forenames>Magnus</forenames></author><author><keyname>Ellings&#xe6;ter</keyname><forenames>Brage</forenames></author><author><keyname>Maseng</keyname><forenames>Torleiv</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author><author><keyname>Kure</keyname><forenames>&#xd8;ivind</forenames></author></authors><title>Large-Scale Distributed Internet-based Discovery Mechanism for Dynamic
  Spectrum Allocation</title><categories>cs.NI</categories><comments>Accepted for publication at IEEE DySPAN 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scarcity of frequencies and the demand for more bandwidth is likely to
increase the need for devices that utilize the available frequencies more
efficiently. Radios must be able to dynamically find other users of the
frequency bands and adapt so that they are not interfered, even if they use
different radio protocols. As transmitters far away may cause as much
interference as a transmitter located nearby, this mechanism can not be based
on location alone. Central databases can be used for this purpose, but require
expensive infrastructure and planning to scale. In this paper, we propose a
decentralized protocol and architecture for discovering radio devices over the
Internet. The protocol has low resource requirements, making it suitable for
implementation on limited platforms. We evaluate the protocol through
simulation in network topologies with up to 2.3 million nodes, including
topologies generated from population patterns in Norway. The protocol has also
been implemented as proof-of-concept in real Wi-Fi routers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3560</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3560</id><created>2012-10-12</created><updated>2012-11-03</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author></authors><title>Simple and Nearly Optimal Multi-Item Auctions</title><categories>cs.GT</categories><comments>SODA 2013, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a Polynomial Time Approximation Scheme (PTAS) for the Bayesian
optimal multi-item multi-bidder auction problem under two conditions. First,
bidders are independent, have additive valuations and are from the same
population. Second, every bidder's value distributions of items are independent
but not necessarily identical monotone hazard rate (MHR) distributions. For
non-i.i.d. bidders, we also provide a PTAS when the number of bidders is small.
Prior to our work, even for a single bidder, only constant factor
approximations are known.
  Another appealing feature of our mechanism is the simple allocation rule.
Indeed, the mechanism we use is either the second-price auction with reserve
price on every item individually, or VCG allocation with a few outlying items
that requires additional treatments. It is surprising that such simple
allocation rules suffice to obtain nearly optimal revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3563</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3563</id><created>2012-10-12</created><authors><author><keyname>Wang</keyname><forenames>Zhao</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Degrees of Freedom of Multi-hop MIMO Broadcast Networks with Delayed
  CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sum degrees of freedom (DoF) of a class of multi-layer
relay-aided MIMO broadcast networks with delayed channel state information at
transmitters (CSIT). In the assumed network a K-antenna source intends to
communicate to K single-antenna destinations, with the help of N-2 layers of K
full-duplex single-antenna relays. We consider two practical delayed CSIT
feedback scenarios. If the source can obtain the CSI feedback signals from all
layers, we prove the optimal sum DoF of the network to be K/(1+1/2+...+1/K). If
the CSI feedback is only within each hop, we show that when K=2 the optimal sum
DoF is 4/3, and when K &gt;= 3 the sum DoF 3/2 is achievable. Our results reveal
that the sum DoF performance in the considered class of N-layer MIMO broadcast
networks with delayed CSIT may depend not on N, the number of layers in the
network, but only on K, the number of antennas/terminals in each layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3569</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3569</id><created>2012-10-12</created><updated>2013-05-14</updated><authors><author><keyname>Kazerounian</keyname><forenames>Sohrob</forenames></author><author><keyname>Luciw</keyname><forenames>Matthew</forenames></author><author><keyname>Richter</keyname><forenames>Mathis</forenames></author><author><keyname>Sandamirskaya</keyname><forenames>Yulia</forenames></author></authors><title>Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics</title><categories>cs.NE</categories><comments>Sohrob Kazerounian, Matthew Luciw are Joint first authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a dynamic neural algorithm called Dynamic Neural (DN)
SARSA(\lambda) for learning a behavioral sequence from delayed reward.
DN-SARSA(\lambda) combines Dynamic Field Theory models of behavioral sequence
representation, classical reinforcement learning, and a computational
neuroscience model of working memory, called Item and Order working memory,
which serves as an eligibility trace. DN-SARSA(\lambda) is implemented on both
a simulated and real robot that must learn a specific rewarding sequence of
elementary behaviors from exploration. Results show DN-SARSA(\lambda) performs
on the level of the discrete SARSA(\lambda), validating the feasibility of
general reinforcement learning without compromising neural dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3583</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3583</id><created>2012-10-12</created><authors><author><keyname>Farias</keyname><forenames>Rodrigo Cabral</forenames></author><author><keyname>Brossier</keyname><forenames>Jean-Marc</forenames></author></authors><title>Adaptive Quantizers for Estimation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, adaptive estimation based on noisy quantized observations is
studied. A low complexity adaptive algorithm using a quantizer with adjustable
input gain and offset is presented. Three possible scalar models for the
parameter to be estimated are considered: constant, Wiener process and Wiener
process with deterministic drift. After showing that the algorithm is
asymptotically unbiased for estimating a constant, it is shown, in the three
cases, that the asymptotic mean squared error depends on the Fisher information
for the quantized measurements. It is also shown that the loss of performance
due to quantization depends approximately on the ratio of the Fisher
information for quantized and continuous measurements. At the end of the paper
the theoretical results are validated through simulation under two different
classes of noise, generalized Gaussian noise and Student's-t noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3587</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3587</id><created>2012-10-12</created><authors><author><keyname>Zong</keyname><forenames>Bo</forenames></author><author><keyname>Wu</keyname><forenames>Yinghui</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author><author><keyname>Yan</keyname><forenames>Xifeng</forenames></author></authors><title>Inferring the Underlying Structure of Information Cascades</title><categories>cs.SI cs.AI physics.soc-ph</categories><comments>The extended version of the paper &quot;Inferring the Underlying Structure
  of Information Cascades&quot;, to appear in ICDM'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks, information and influence diffuse among users as
cascades. While the importance of studying cascades has been recognized in
various applications, it is difficult to observe the complete structure of
cascades in practice. Moreover, much less is known on how to infer cascades
based on partial observations. In this paper we study the cascade inference
problem following the independent cascade model, and provide a full treatment
from complexity to algorithms: (a) We propose the idea of consistent trees as
the inferred structures for cascades; these trees connect source nodes and
observed nodes with paths satisfying the constraints from the observed temporal
information. (b) We introduce metrics to measure the likelihood of consistent
trees as inferred cascades, as well as several optimization problems for
finding them. (c) We show that the decision problems for consistent trees are
in general NP-complete, and that the optimization problems are hard to
approximate. (d) We provide approximation algorithms with performance
guarantees on the quality of the inferred cascades, as well as heuristics. We
experimentally verify the efficiency and effectiveness of our inference
algorithms, using real and synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3593</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3593</id><created>2012-10-12</created><authors><author><keyname>B&#xed;lka</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Pattern matching in compilers</title><categories>cs.PL cs.FL</categories><comments>master thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis we develop tools for effective and flexible pattern matching.
We introduce a new pattern matching system called amethyst. Amethyst is not
only a generator of parsers of programming languages, but can also serve as an
alternative to tools for matching regular expressions.
  Our framework also produces dynamic parsers. Its intended use is in the
context of IDE (accurate syntax highlighting and error detection on the fly).
Amethyst offers pattern matching of general data structures. This makes it a
useful tool for implementing compiler optimizations such as constant folding,
instruction scheduling, and dataflow analysis in general.
  The parsers produced are essentially top-down parsers. Linear time complexity
is obtained by introducing the novel notion of structured grammars and
regularized regular expressions. Amethyst uses techniques known from compiler
optimizations to produce effective parsers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3595</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3595</id><created>2012-10-12</created><authors><author><keyname>Trencs&#xe9;ni</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>Plane-Sweep Incremental Algorithm: Computing Delaunay Tessellations of
  Large Datasets</title><categories>cs.CG</categories><comments>Technical Report from 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the plane-sweep incremental algorithm, a hybrid approach for
computing Delaunay tessellations of large point sets whose size exceeds the
computer's main memory. This approach unites the simplicity of the incremental
algorithms with the comparatively low memory requirements of plane-sweep
approaches. The procedure is to first sort the point set along the first
principal component and then to sequentially insert the points into the
tessellation, essentially simulating a sweeping plane. The part of the
tessellation that has been passed by the sweeping plane can be evicted from
memory and written to disk, limiting the memory requirement of the program to
the &quot;thickness&quot; of the data set along its first principal component. We
implemented the algorithm and used it to compute the Delaunay tessellation and
Voronoi partition of the Sloan Digital Sky Survey magnitude space consisting of
287 million points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3597</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3597</id><created>2012-10-12</created><authors><author><keyname>Pellegrini</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Canevet</keyname><forenames>S&#xe9;bastien</forenames><affiliation>CERSA</affiliation></author></authors><title>Le droit du num\'erique : une histoire \`a pr\'eserver</title><categories>cs.GL</categories><comments>No. RR-8100 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the history of informatics is recent, this field poses unusual
problems with respect to its preservation. These problems are amplified by
legal issues, digital law being in itself a subject matter whose history is
also worth presenting in a computer science museum. The purpose of this paper
is to present a quick overview of the evolution of law regarding digital
matters, from an historical perspective as well as with respect to the
preservation and presentation of the works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3598</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3598</id><created>2012-10-12</created><authors><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Garcia</keyname><forenames>Nuria</forenames></author><author><keyname>Faridi</keyname><forenames>Azadeh</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Modelling a Decentralized Constraint Satisfaction Solver for
  Collision-Free Channel Access</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of assigning channel slots to a number of
contending stations is modeled as a Constraint Satisfaction Problem (CSP). A
learning MAC protocol that uses deterministic backoffs after successful
transmissions is used as a decentralized solver for the CSP. The convergence
process of the solver is modeled by an absorbing Markov chain (MC), and
analytical, closed-form expressions for its transition probabilities are
derived. Using these, the expected number of steps required to reach a solution
is found. The analysis is validated by means of simulations and the model is
extended to account for the presence of channel errors. The results are
applicable in various resource allocation scenarios in wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3599</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3599</id><created>2012-10-12</created><authors><author><keyname>Padovani</keyname><forenames>Vincent</forenames><affiliation>PPS</affiliation></author></authors><title>Decidability of All Minimal Models (Revised Version - 2012)</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This unpublished note is an alternate, shorter (and hopefully more readable)
proof of the decidability of all minimal models. The decidability follows from
a proof of the existence of a cellular term in each observational equivalence
class of a minimal model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3604</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3604</id><created>2012-10-12</created><authors><author><keyname>Bakhshi</keyname><forenames>Mahdi</forenames></author><author><keyname>Hashemi</keyname><forenames>Mohsen</forenames></author></authors><title>User-Centric Optimization for Constraint Web Service Composition using a
  Fuzzy-guided Genetic Algorithm System</title><categories>cs.SE</categories><comments>15 pages, 7 figures</comments><journal-ref>International Journal on Web Service Computing Vol.3, No.3, (2012)
  1-15</journal-ref><doi>10.5121/ijwsc.2012.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-Oriented Applications (SOA) are being regarded as the main pragmatic
solution for distributed environments. In such systems, however each service
responds the user request independently, it is essential to compose them for
delivering a compound value-added service. Since, there may be a number of
compositions to create the requested service, it is important to find one which
its properties are close to user's desires and meet some non-functional
constraints and optimize criteria such as overall cost or response time. In
this paper, a user-centric approach is presented for evaluating the service
compositions which attempts to obtain the user desires. This approach uses
fuzzy logic in order to inference based on quality criteria ranked by user and
Genetic Algorithms to optimize the QoS-aware composition problem. Results show
that the Fuzzy-based Genetic algorithm system enables user to participate in
the process of web service composition easier and more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3609</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3609</id><created>2012-10-12</created><authors><author><keyname>Jiang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Junhua</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Optimal Power Allocation Policy over Two Identical Gilbert-Elliott
  Channels</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.6630</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of optimal power allocation over two
identical Gilbert-Elliott (Binary Markov) communication channels. Our goal is
to maximize the expected discounted number of bits transmitted over an infinite
time span by judiciously choosing one of the four actions for each time slot:
1) allocating power equally to both channels, 2) allocating all the power to
channel 1, 3) allocating all the power to channel 2, and 4) allocating no power
to any of the channels. As the channel state is unknown when power allocation
decision is made, we model this problem as a partially observable Markov
decision process(POMDP), and derive the optimal policy which gives the optimal
action to take under different possible channel states. Two different
structures of the optimal policy are derived analytically and verified by
linear programming simulation. We also illustrate how to construct the optimal
policy by the combination of threshold calculation and linear programming
simulation once system parameters are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3634</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3634</id><created>2012-10-12</created><authors><author><keyname>Wahlstedt</keyname><forenames>Robert</forenames></author></authors><title>Quick Summary</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Quick Summary is an innovate implementation of an automatic document
summarizer that inputs a document in the English language and evaluates each
sentence. The scanner or evaluator determines criteria based on its grammatical
structure and place in the paragraph. The program then asks the user to specify
the number of sentences the person wishes to highlight. For example should the
user ask to have three of the most important sentences, it would highlight the
first and most important sentence in green. Commonly this is the sentence
containing the conclusion. Then Quick Summary finds the second most important
sentence usually called a satellite and highlights it in yellow. This is
usually the topic sentence. Then the program finds the third most important
sentence and highlights it in red. The implementations of this technology are
useful in a society of information overload when a person typically receives 42
emails a day (Microsoft). The paper also is a candid look at difficulty that
machine learning has in textural translating. However, it speaks on how to
overcome the obstacles that historically prevented progress. This paper
proposes mathematical meta-data criteria that justify the place of importance
of a sentence. Just as tools for the study of relational symmetry in
bio-informatics, this tool seeks to classify words with greater clarity.
&quot;Survey Finds Workers Average Only Three Productive Days per Week.&quot; Microsoft
News Center. Microsoft. Web. 31 Mar. 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3637</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3637</id><created>2012-10-12</created><updated>2013-10-22</updated><authors><author><keyname>Bermejo-Vega</keyname><forenames>Juan</forenames></author><author><keyname>Nest</keyname><forenames>Maarten Van den</forenames></author></authors><title>Classical simulations of Abelian-group normalizer circuits with
  intermediate measurements</title><categories>quant-ph cs.CC</categories><comments>26 pages+appendices. Title has changed in this second version. To
  appear in Quantum Information and Computation, Vol.14 No.3&amp;4, 2014</comments><journal-ref>Quantum Information &amp; Computation, Volume 14 Issue 3-4, March
  2014, Pages 181-216, Rinton Press Incorporated</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum normalizer circuits were recently introduced as generalizations of
Clifford circuits [arXiv:1201.4867]: a normalizer circuit over a finite Abelian
group $G$ is composed of the quantum Fourier transform (QFT) over G, together
with gates which compute quadratic functions and automorphisms. In
[arXiv:1201.4867] it was shown that every normalizer circuit can be simulated
efficiently classically. This result provides a nontrivial example of a family
of quantum circuits that cannot yield exponential speed-ups in spite of usage
of the QFT, the latter being a central quantum algorithmic primitive. Here we
extend the aforementioned result in several ways. Most importantly, we show
that normalizer circuits supplemented with intermediate measurements can also
be simulated efficiently classically, even when the computation proceeds
adaptively. This yields a generalization of the Gottesman-Knill theorem (valid
for n-qubit Clifford operations [quant-ph/9705052, quant-ph/9807006] to quantum
circuits described by arbitrary finite Abelian groups. Moreover, our
simulations are twofold: we present efficient classical algorithms to sample
the measurement probability distribution of any adaptive-normalizer
computation, as well as to compute the amplitudes of the state vector in every
step of it. Finally we develop a generalization of the stabilizer formalism
[quant-ph/9705052, quant-ph/9807006] relative to arbitrary finite Abelian
groups: for example we characterize how to update stabilizers under generalized
Pauli measurements and provide a normal form of the amplitudes of generalized
stabilizer states using quadratic functions and subgroup cosets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3640</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3640</id><created>2012-10-12</created><authors><author><keyname>Al-Neaimi</keyname><forenames>A.</forenames></author><author><keyname>Qatawneh</keyname><forenames>S.</forenames></author><author><keyname>Saiyd</keyname><forenames>Nedhal Al</forenames></author></authors><title>Conducting Verification And Validation Of Multi- Agent Systems</title><categories>cs.SE</categories><comments>10 pages,3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Verification and Validation (V&amp;V) is a series of activities, technical and
managerial, which performed by system tester not the system developer in order
to improve the system quality, system reliability and assure that product
satisfies the users operational needs. Verification is the assurance that the
products of a particular development phase are consistent with the requirements
of that phase and preceding phase(s), while validation is the assurance that
the final product meets system requirements. an outside agency can be used to
performed V&amp;V, which is indicate by Independent V&amp;V, or IV&amp;V, or by a group
within the organization but not the developer, referred to as Internal V&amp;V. Use
of V&amp;V often accompanies testing, can improve quality assurance, and can reduce
risk. This paper putting guidelines for performing V&amp;V of Multi-Agent Systems
(MAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3652</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3652</id><created>2012-10-12</created><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author><author><keyname>Cheong</keyname><forenames>Michelle</forenames></author></authors><title>A Flexible Mixed Integer Programming framework for Nurse Scheduling</title><categories>cs.DS cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a nurse-scheduling model is developed using mixed integer
programming model. It is deployed to a general care ward to replace and
automate the current manual approach for scheduling. The developed model
differs from other similar studies in that it optimizes both hospitals
requirement as well as nurse preferences by allowing flexibility in the
transfer of nurses from different duties. The model also incorporated
additional policies which are part of the hospitals requirement but not part of
the legislations. Hospitals key primary mission is to ensure continuous ward
care service with appropriate number of nursing staffs and the right mix of
nursing skills. The planning and scheduling is done to avoid additional non
essential cost for hospital. Nurses preferences are taken into considerations
such as the number of night shift and consecutive rest days. We will also
reformulate problems from another paper which considers the penalty objective
using the model but without the flexible components. The models are built using
AIMMS which solves the problem in very short amount of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3664</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3664</id><created>2012-10-12</created><updated>2014-07-08</updated><authors><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Secure Cooperative Regenerating Codes for Distributed Storage Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes enable trading off repair bandwidth for storage in
distributed storage systems (DSS). Due to their distributed nature, these
systems are intrinsically susceptible to attacks, and they may also be subject
to multiple simultaneous node failures. Cooperative regenerating codes allow
bandwidth efficient repair of multiple simultaneous node failures. This paper
analyzes storage systems that employ cooperative regenerating codes that are
robust to (passive) eavesdroppers. The analysis is divided into two parts,
studying both minimum bandwidth and minimum storage cooperative regenerating
scenarios. First, the secrecy capacity for minimum bandwidth cooperative
regenerating codes is characterized. Second, for minimum storage cooperative
regenerating codes, a secure file size upper bound and achievability results
are provided. These results establish the secrecy capacity for the minimum
storage scenario for certain special cases. In all scenarios, the achievability
results correspond to exact repair, and secure file size upper bounds are
obtained using min-cut analyses over a suitable secrecy graph representation of
DSS. The main achievability argument is based on an appropriate pre-coding of
the data to eliminate the information leakage to the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3667</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3667</id><created>2012-10-12</created><authors><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author></authors><title>A New Analysis of the DS-CDMA Cellular Downlink Under Spatial
  Constraints</title><categories>cs.IT math.IT</categories><comments>to appear at ICNC-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The direct-sequence code-division multiple access (DS-CDMA) cellular downlink
is modeled by a constrained random spatial model involving a fixed number of
base stations placed over a finite area with a minimum separation. The analysis
is driven by a new closed-form expression for the conditional outage
probability at each mobile, where the conditioning is with respect to the
network realization. The analysis features a flexible channel model, accounting
for path loss, Nakagami fading, and shadowing. By generating many random
networks and applying a given resource allocation policy, the distribution of
the rates provided to each user is obtained. In addition to determining the
average rate, the analysis can determine the transmission capacity of the
network and can characterize fairness in terms of the fraction of users that
achieve a specified rate. The analysis is used to compare a rate-control policy
against a power-control policy and investigate the influence of the minimum
base-station separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3684</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3684</id><created>2012-10-13</created><updated>2013-07-22</updated><authors><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author></authors><title>Heuristic algorithms for the bipartite unconstrained 0-1 quadratic
  programming problem</title><categories>cs.DM math.CO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Bipartite Unconstrained 0-1 Quadratic Programming Problem (BQP)
which is a relaxation of the Unconstrained 0-1 Quadratic Programming Problem
(QP). Applications of the BQP include mining discrete patterns from binary
data, approximating matrices by rank-one binary matrices, computing cut-norm of
a matrix, and solving optimization problems such as maximum weight biclique,
bipartite maximum weight cut, maximum weight induced subgraph of a bipartite
graph, etc. We propose several classes of heuristic approaches to solve the BQP
and discuss a number of construction algorithms, local search algorithms and
their combinations. Results of extensive computational experiments are reported
to establish the practical performance of our algorithms. For this purpose, we
propose several sets of test instances based on various applications of the
BQP. Our algorithms are compared with state-of-the-art heuristics for QP which
can also be used to solve BQP with reformulation. We also study theoretical
properties of the neighborhoods and algorithms. In particular, we establish
complexity of all neighborhood search algorithms and establish tight worst-case
performance ratio for the greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3702</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3702</id><created>2012-10-13</created><authors><author><keyname>Juluru</keyname><forenames>Tarun Kumar</forenames></author><author><keyname>Kankacharla</keyname><forenames>Anitha sheela</forenames></author></authors><title>Estimation and compensation of inter carrier interference in wimax
  physical layer under various channel models</title><categories>cs.NI</categories><comments>15 pages</comments><journal-ref>International Journal of Next-Generation Networks,Vol.4,
  No.3,September 2012</journal-ref><doi>10.5121/ijngn.2012.4306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiMAX is Wireless Interoperability for Microwave Access has emerged as a
promising solution for transmission of higher data rates for fixed and mobile
applications. IEEE 802.16d and e are the standards proposed. To attain higher
data rates the Multi Carrier System with Multiple Input and Multiple Output
MIMO is incorporated in the WiMAX. And all these sub carriers are considered to
be orthogonal to each other. As the number of sub carriers is increased there
is no guarantee of sustained orthogonality, i.e. at some point the carriers are
not independent to each other, and hence where the orthogonality can be loosed
which leads to interference and also owing to the synchronization between
transmitter and receiver local oscillator, it causes interference known as
Inter Carrier Interference (ICI).In this scheme at the transmitter side the
modulated data and a few predefined pilot symbols are mapped onto the non
neighboring sub carriers with weighting coefficients of +1 and -1. With the aid
of pilot symbols the frequency offset is exactly estimated by using Maximum
Likelihood Estimation MLE and hence can be minimized. At demodulation stage the
received signals are linearly combined along with their weighted oefficients
and pilot symbols, called as Pilot Aided Self Cancellation Method PASCS. The
simulations are carried out on Stanford University Interim (SUI)channels. The
simulation results shows that by incorporating this method into WiMAX systems
it performs better when the Line Of Sight (LOS) component is present in the
transmission and also it improves the Bit Error Rate (BER) and Carrier to
Interference Ratio (CIR). The CIR can be improved 20 dB. In this paper the
effectiveness of PASCS scheme is compared with the Self Cancellation Method
(SCM). It provides accurate estimation of frequency offset and when residual
CFO is less significant the ICI can be diminished successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3709</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3709</id><created>2012-10-13</created><updated>2015-06-22</updated><authors><author><keyname>Miao</keyname><forenames>Weimin</forenames></author><author><keyname>Pan</keyname><forenames>Shaohua</forenames></author><author><keyname>Sun</keyname><forenames>Defeng</forenames></author></authors><title>A Rank-Corrected Procedure for Matrix Completion with Fixed Basis
  Coefficients</title><categories>math.OC cs.IT cs.NA math.IT stat.ML</categories><comments>51 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the problems of low-rank matrix completion, the efficiency of the
widely-used nuclear norm technique may be challenged under many circumstances,
especially when certain basis coefficients are fixed, for example, the low-rank
correlation matrix completion in various fields such as the financial market
and the low-rank density matrix completion from the quantum state tomography.
To seek a solution of high recovery quality beyond the reach of the nuclear
norm, in this paper, we propose a rank-corrected procedure using a nuclear
semi-norm to generate a new estimator. For this new estimator, we establish a
non-asymptotic recovery error bound. More importantly, we quantify the
reduction of the recovery error bound for this rank-corrected procedure.
Compared with the one obtained for the nuclear norm penalized least squares
estimator, this reduction can be substantial (around 50%). We also provide
necessary and sufficient conditions for rank consistency in the sense of Bach
(2008). Very interestingly, these conditions are highly related to the concept
of constraint nondegeneracy in matrix optimization. As a byproduct, our results
provide a theoretical foundation for the majorized penalty method of Gao and
Sun (2010) and Gao (2010) for structured low-rank matrix optimization problems.
Extensive numerical experiments demonstrate that our proposed rank-corrected
procedure can simultaneously achieve a high recovery accuracy and capture the
low-rank structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3718</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3718</id><created>2012-10-13</created><authors><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Mus&#xe9;</keyname><forenames>Pablo</forenames></author><author><keyname>Almansa</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>On the Role of Contrast and Regularity in Perceptual Boundary Saliency</title><categories>cs.CV stat.AP</categories><doi>10.1007/s10851-012-0411-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical Morphology proposes to extract shapes from images as connected
components of level sets. These methods prove very suitable for shape
recognition and analysis. We present a method to select the perceptually
significant (i.e., contrasted) level lines (boundaries of level sets), using
the Helmholtz principle as first proposed by Desolneux et al. Contrarily to the
classical formulation by Desolneux et al. where level lines must be entirely
salient, the proposed method allows to detect partially salient level lines,
thus resulting in more robust and more stable detections. We then tackle the
problem of combining two gestalts as a measure of saliency and propose a method
that reinforces detections. Results in natural images show the good performance
of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3719</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3719</id><created>2012-10-13</created><authors><author><keyname>Gupta</keyname><forenames>Divya</forenames></author><author><keyname>Sahai</keyname><forenames>Amit</forenames></author></authors><title>On Constant-Round Concurrent Zero-Knowledge from a Knowledge Assumption</title><categories>cs.CR</categories><comments>30 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the long-standing open question of constructing
constant-round concurrent zero-knowledge protocols in the plain model.
Resolving this question is known to require non-black-box techniques.
  We consider non-black-box techniques for zero-knowledge based on knowledge
assumptions, a line of thinking initiated by the work of Hada and Tanaka
(CRYPTO 1998). Prior to our work, it was not known whether knowledge
assumptions could be used for achieving security in the concurrent setting, due
to a number of significant limitations that we discuss here. Nevertheless, we
obtain the following results:
  1. We obtain the first constant round concurrent zero-knowledge argument for
\textbf{NP} in the plain model based on a new variant of knowledge of exponent
assumption. Furthermore, our construction avoids the inefficiency inherent in
previous non-black-box techniques such that those of Barak (FOCS 2001); we
obtain our result through an efficient protocol compiler.
  2. Unlike Hada and Tanaka, we do not require a knowledge assumption to argue
the soundness of our protocol. Instead, we use a discrete log like assumption,
which we call Diffie-Hellman Logarithm Assumption, to prove the soundness of
our protocol.
  3. We give evidence that our new variant of knowledge of exponent assumption
is in fact plausible. In particular, we show that our assumption holds in the
generic group model.
  4. Knowledge assumptions are especially delicate assumptions whose
plausibility may be hard to gauge. We give a novel framework to express
knowledge assumptions in a more flexible way, which may allow for formulation
of plausible assumptions and exploration of their impact and application in
cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3727</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3727</id><created>2012-10-13</created><updated>2012-11-21</updated><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>How are academic age, productivity and collaboration related to citing
  behavior of researchers?</title><categories>physics.soc-ph cs.DL</categories><comments>Accepted for publication in PLoS ONE</comments><doi>10.1371/journal.pone.0049176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  References are an essential component of research articles and therefore of
scientific communication. In this study we investigate referencing (citing)
behavior in five diverse fields (astronomy, mathematics, robotics, ecology and
economics) based on 213,756 core journal articles. At the macro level we find:
(a) a steady increase in the number of references per article over the period
studied (50 years), which in some fields is due to a higher rate of usage,
while in others reflects longer articles and (b) an increase in all fields in
the fraction of older, foundational references since the 1980s, with no obvious
change in citing patterns associated with the introduction of the Internet. At
the meso level we explore current (2006-2010) referencing behavior of different
categories of authors (21,562 total) within each field, based on their academic
age, productivity and collaborative practices. Contrary to some previous
findings and expectations we find that senior researchers use references at the
same rate as their junior colleagues, with similar rates of re-citation (use of
same references in multiple papers). High Modified Price Index (MPI, which
measures the speed of the research front more accurately than the traditional
Price Index) of senior authors indicates that their research has the similar
cutting-edge aspect as that of their younger colleagues. In all fields both the
productive researchers and especially those who collaborate more use a
significantly lower fraction of foundational references and have much higher
MPI and lower re-citation rates, i.e., they are the ones pushing the research
front regardless of researcher age. This paper introduces improved bibliometric
methods to measure the speed of the research front, disambiguate lead authors
in co-authored papers and decouple measures of productivity and collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3729</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3729</id><created>2012-10-13</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Sivaji</forenames></author></authors><title>Inference of Fine-grained Attributes of Bengali Corpus for Stylometry
  Detection</title><categories>cs.CL cs.CV</categories><comments>5 pages, 2 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:1208.6268</comments><journal-ref>Polibits (44) 2011, pp. 79-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stylometry, the science of inferring characteristics of the author from the
characteristics of documents written by that author, is a problem with a long
history and belongs to the core task of Text categorization that involves
authorship identification, plagiarism detection, forensic investigation,
computer security, copyright and estate disputes etc. In this work, we present
a strategy for stylometry detection of documents written in Bengali. We adopt a
set of fine-grained attribute features with a set of lexical markers for the
analysis of the text and use three semi-supervised measures for making
decisions. Finally, a majority voting approach has been taken for final
classification. The system is fully automatic and language-independent.
Evaluation results of our attempt for Bengali author's stylometry detection
show reasonably promising accuracy in comparison to the baseline model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3735</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3735</id><created>2012-10-13</created><authors><author><keyname>Kothapalli</keyname><forenames>Kishore</forenames></author><author><keyname>Pemmaraju</keyname><forenames>Sriram V.</forenames></author><author><keyname>Sardeshmukh</keyname><forenames>Vivek</forenames></author></authors><title>On the Analysis of a Label Propagation Algorithm for Community Detection</title><categories>cs.DC cs.SI physics.soc-ph</categories><comments>17 pages. Submitted to ICDCN 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates formal analysis of a simple, distributed algorithm for
community detection on networks. We analyze an algorithm that we call
\textsc{Max-LPA}, both in terms of its convergence time and in terms of the
&quot;quality&quot; of the communities detected. \textsc{Max-LPA} is an instance of a
class of community detection algorithms called \textit{label propagation}
algorithms. As far as we know, most analysis of label propagation algorithms
thus far has been empirical in nature and in this paper we seek a theoretical
understanding of label propagation algorithms. In our main result, we define a
clustered version of \er random graphs with clusters $V_1, V_2,..., V_k$ where
the probability $p$, of an edge connecting nodes within a cluster $V_i$ is
higher than $p'$, the probability of an edge connecting nodes in distinct
clusters. We show that even with fairly general restrictions on $p$ and $p'$
($p = \Omega(\frac{1}{n^{1/4-\epsilon}})$ for any $\epsilon &gt; 0$, $p' =
O(p^2)$, where $n$ is the number of nodes), \textsc{Max-LPA} detects the
clusters $V_1, V_2,..., V_n$ in just two rounds. Based on this and on empirical
results, we conjecture that \textsc{Max-LPA} can correctly and quickly identify
communities on clustered \er graphs even when the clusters are much sparser,
i.e., with $p = \frac{c\log n}{n}$ for some $c &gt; 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3741</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3741</id><created>2012-10-13</created><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>Online computation of sparse representations of time varying stimuli
  using a biologically motivated neural network</title><categories>q-bio.NC cs.NE</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural stimuli are highly redundant, possessing significant spatial and
temporal correlations. While sparse coding has been proposed as an efficient
strategy employed by neural systems to encode sensory stimuli, the underlying
mechanisms are still not well understood. Most previous approaches model the
neural dynamics by the sparse representation dictionary itself and compute the
representation coefficients offline. In reality, faced with the challenge of
constantly changing stimuli, neurons must compute the sparse representations
dynamically in an online fashion. Here, we describe a leaky linearized Bregman
iteration (LLBI) algorithm which computes the time varying sparse
representations using a biologically motivated network of leaky rectifying
neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits
the temporal correlation of stimuli and demonstrate better performance both in
representation error and the smoothness of temporal evolution of sparse
coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3756</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3756</id><created>2012-10-14</created><authors><author><keyname>Alkazemi</keyname><forenames>Basem Y.</forenames></author></authors><title>A Conceptual Framework to Analyze Enterprise Business Solutions from a
  Software Architecture Perspective</title><categories>cs.SE</categories><comments>8 pages, 2 figures</comments><msc-class>68N99</msc-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 3, No 1, May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The architectural aspects of software systems are not always explicitly
exposed to customers when a product is presented to them by software vendors.
Therefore, customers might be put at a major risk if new emerging business
needs come to light that require modification of some of the core business
processes within their organizations. So they might need to replace their
existing systems or re-architect old ones to comply with new architectural
standards. This paper describes a proposed framework that helps organizations
to build a comprehensive view of their system architecture prior to dealing
with vendors. Consequently, every organization can have a reference model that
facilitates negotiation and communication with software vendors. The paper
applies the proposed framework to an organization in the region of Saudi Arabia
to validate its applicability and generates an architectural design for their
software systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3758</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3758</id><created>2012-10-14</created><authors><author><keyname>Alkazemi</keyname><forenames>Basem Y.</forenames></author></authors><title>On verification of software components</title><categories>cs.SE</categories><comments>13 pages, 3 figures</comments><msc-class>68N99</msc-class><acm-class>D.2.7</acm-class><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.3, No.5, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing third party software components in the development of new systems
became somewhat unfavourable approach among many organizations nowadays. This
reluctance is primarily built due to the lack of support to verify the quality
attributes of software components in order to avoid potential mismatches with
systems requirements. This paper presents an approach to overcome this problem
by providing a tool support to check component compatibility to a specification
provided by developers. So, components compatibility can be checked and
developers can verify components that match their quality attributes prior of
integrating them into their system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3761</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3761</id><created>2012-10-14</created><updated>2013-04-08</updated><authors><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author><author><keyname>Papavasileiou</keyname><forenames>Vasilis</forenames></author></authors><title>ILP Modulo Theories</title><categories>cs.LO</categories><comments>CAV 2013 version plus proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Integer Linear Programming (ILP) Modulo Theories (IMT). An IMT
instance is an Integer Linear Programming instance, where some symbols have
interpretations in background theories. In previous work, the IMT approach has
been applied to industrial synthesis and design problems with real-time
constraints arising in the development of the Boeing 787. Many other problems
ranging from operations research to software verification routinely involve
linear constraints and optimization. Thus, a general ILP Modulo Theories
framework has the potential to be widely applicable. The logical next step in
the development of IMT and the main goal of this paper is to provide
theoretical underpinnings. This is accomplished by means of BC(T), the Branch
and Cut Modulo T abstract transition system. We show that BC(T) provides a
sound and complete optimization procedure for the ILP Modulo T problem, as long
as T is a decidable, stably-infinite theory. We compare a prototype of BC(T)
against leading SMT solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3762</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3762</id><created>2012-10-14</created><authors><author><keyname>Vakati</keyname><forenames>Sudheer</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>On Two Graph-Theoretic Characterizations of Tree Compatibility</title><categories>cs.DM</categories><msc-class>68R10, 92B10</msc-class><acm-class>F.2.2; G.2.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding whether a collection of unrooted trees is compatible is a
fundamental problem in phylogenetics. Two different graph-theoretic
characterizations of tree compatibility have recently been proposed. In one of
these, tree compatibility is characterized in terms of the existence of a
specific kind of triangulation in a structure known as the display graph. An
alternative characterization expresses the tree compatibility problem as a
chordal graph sandwich problem in a structure known as the edge label
intersection graph. In this paper we show that the characterization using edge
label intersection graphs transforms to a characterization in terms of minimal
cuts of the display graph. We show how these two characterizations are related
to compatibility of splits. We also show how the characterization in terms of
minimal cuts of display graph is related to the characterization in terms of
triangulation of the display graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3768</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3768</id><created>2012-10-14</created><authors><author><keyname>Wu</keyname><forenames>Shih-Jung</forenames></author><author><keyname>Huang</keyname><forenames>Shih-Yi</forenames></author><author><keyname>Huang</keyname><forenames>Kuo-Feng</forenames></author></authors><title>Adaptive Priority-Based Downlink Scheduling for WiMAX Networks</title><categories>cs.NI</categories><comments>12 pages, 16 figures, 6 algorithms, 27 equations, 40 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supporting quality of service (QoS) guarantees for diverse multimedia
services are the primary concerns for WiMAX (IEEE 802.16) networks. A
scheduling scheme that satisfies QoS requirements has become more important for
wireless communications. We propose a downlink scheduling scheme called
adaptive priority-based downlink scheduling (APDS) for providing QoS guarantees
in IEEE 802.16 networks. APDS comprises two major components: priority
assignment and resource allocation. Different service-type connections
primarily depend on their QoS requirements to adjust priority assignments and
dispatch bandwidth resources dynamically. We consider both starvation avoidance
and resource management. Simulation results show that our APDS methodology
outperforms the representative scheduling approaches in QoS satisfaction and
maintains fairness in starvation prevention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3769</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3769</id><created>2012-10-14</created><updated>2014-04-29</updated><authors><author><keyname>Mehta</keyname><forenames>Mahima</forenames></author><author><keyname>Jain</keyname><forenames>Ranjan Bala</forenames></author><author><keyname>Karandikar</keyname><forenames>Abhay</forenames></author></authors><title>Analysis of Blocking Probability in a Relay-based Cellular OFDMA Network</title><categories>cs.IT cs.NI math.IT</categories><comments>33 pages, 10 figures</comments><journal-ref>Pre-print version (May 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relay deployment in Orthogonal Frequency Division Multiple Access (OFDMA)
based cellular networks helps in coverage extension and or capacity
improvement. In OFDMA system, each user requires different number of
subcarriers to meet its rate requirement. This resource requirement depends on
the Signal to Interference Ratio (SIR) experienced by a user. Traditional
methods to compute blocking probability cannot be used in relay based cellular
OFDMA networks. In this paper, we present an approach to compute the blocking
probability of such networks. We determine an expression of the probability
distribution of the users resource requirement based on its experienced SIR and
then classify the users into various classes depending upon their subcarrier
requirement. We consider the system to be a multidimensional system with
different classes and evaluate the blocking probability of system using the
multi-dimensional Erlang loss formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3778</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3778</id><created>2012-10-14</created><authors><author><keyname>Missaoui</keyname><forenames>Ibrahim</forenames></author><author><keyname>Lachiri</keyname><forenames>Zied</forenames></author></authors><title>Blind speech separation based on undecimated wavelet packet-perceptual
  filterbanks and independent component analysis</title><categories>cs.SD</categories><comments>Perceptual Filter-Bank, Undecimated Wavelet Packet Decomposition,
  Independent Component Analysis, Blind speech separation. Downloaded from
  http://www.ijcsi.org/papers/IJCSI-8-3-1-265-272.pdf</comments><msc-class>92C55</msc-class><acm-class>I.5.4; H.5.5</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, vol. 8,
  Issue 3, no. 1, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of blind separation of speech mixtures.
We propose a new blind speech separation system, which integrates a perceptual
filterbank and independent component analysis (ICA) and using kurtosis
criterion. The perceptual filterbank was designed by adjusting undecimated
wavelet packet decomposition (UWPD) tree in order to accord to critical band
characteristics of psycho-acoustic model. Our proposed technique consists on
transforming the observations signals into an adequate representation using
UWPD and Kurtosis maximization criterion in a new preprocessing step in order
to increase the non-Gaussianity which is a pre-requirement for ICA. Experiments
were carried out with the instantaneous mixture of two speech sources using two
sensors. The obtained results show that the proposed method gives a
considerable improvement when compared with FastICA and other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3812</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3812</id><created>2012-10-14</created><authors><author><keyname>Ntogramatzidis</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zanasi</keyname><forenames>Roberto</forenames></author><author><keyname>Cuoghi</keyname><forenames>Stefania</forenames></author></authors><title>A Unified Analytical Design Method of Standard Controllers using
  Inversion Formulae</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a comprehensive range of design
techniques for the synthesis of the standard compensators (Lead and Lag
networks as well as PID controllers) that in the last twenty years have proved
to be of great educational value in a vast number of undergraduate and
postgraduate courses in Control throughout Italy, but that to-date remain
mostly confined within this country. These techniques hinge upon a set of
simple closed-form formulae for the computation of the parameters of the
controller as functions of the typical specifications introduced in Control
courses, i.e., the steady-state performance, the stability margins and the
crossover frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3819</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3819</id><created>2012-10-14</created><updated>2013-07-21</updated><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On Precoding for Constant K-User MIMO Gaussian Interference Channel with
  Finite Constellation Inputs</title><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers linear precoding for constant channel-coefficient
$K$-User MIMO Gaussian Interference Channel (MIMO GIC) where each
transmitter-$i$ (Tx-$i$), requires to send $d_i$ independent complex symbols
per channel use that take values from fixed finite constellations with uniform
distribution, to receiver-$i$ (Rx-$i$) for $i=1,2,\cdots,K$. We define the
maximum rate achieved by Tx-$i$ using any linear precoder, when the
interference channel-coefficients are zero, as the signal to noise ratio (SNR)
tends to infinity to be the Constellation Constrained Saturation Capacity
(CCSC) for Tx-$i$. We derive a high SNR approximation for the rate achieved by
Tx-$i$ when interference is treated as noise and this rate is given by the
mutual information between Tx-$i$ and Rx-$i$, denoted as $I[X_i;Y_i]$. A set of
necessary and sufficient conditions on the precoders under which $I[X_i;Y_i]$
tends to CCSC for Tx-$i$ is derived. Interestingly, the precoders designed for
interference alignment (IA) satisfy these necessary and sufficient conditions.
Further, we propose gradient-ascent based algorithms to optimize the sum-rate
achieved by precoding with finite constellation inputs and treating
interference as noise. Simulation study using the proposed algorithms for a
3-user MIMO GIC with two antennas at each node with $d_i=1$ for all $i$, and
with BPSK and QPSK inputs, show more than 0.1 bits/sec/Hz gain in the ergodic
sum-rate over that yielded by precoders obtained from some known IA algorithms,
at moderate SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3832</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3832</id><created>2012-10-14</created><authors><author><keyname>Ram</keyname><forenames>Idan</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Cohen</keyname><forenames>Israel</forenames></author></authors><title>Image Processing using Smooth Ordering of its Patches</title><categories>cs.CV</categories><comments>8 pages, 7 figures, 4 tables, submitted to IEEE Transactions on Image
  Processing</comments><doi>10.1109/TIP.2013.2257813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an image processing scheme based on reordering of its patches. For
a given corrupted image, we extract all patches with overlaps, refer to these
as coordinates in high-dimensional space, and order them such that they are
chained in the &quot;shortest possible path&quot;, essentially solving the traveling
salesman problem. The obtained ordering applied to the corrupted image, implies
a permutation of the image pixels to what should be a regular signal. This
enables us to obtain good recovery of the clean image by applying relatively
simple 1D smoothing operations (such as filtering or interpolation) to the
reordered set of pixels. We explore the use of the proposed approach to image
denoising and inpainting, and show promising results in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3833</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3833</id><created>2012-10-14</created><authors><author><keyname>Alam</keyname><forenames>Md. Shafiul</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Asish</forenames></author></authors><title>Improved upper and lower bounds for the point placement problem</title><categories>cs.DS</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The point placement problem is to determine the positions of a set of $n$
distinct points, P = {p1, p2, p3, ..., pn}, on a line uniquely, up to
translation and reflection, from the fewest possible distance queries between
pairs of points. Each distance query corresponds to an edge in a graph, called
point placement graph ppg, whose vertex set is P. The uniqueness requirement of
the placement translates to line rigidity of the ppg. In this paper we show how
to construct in 2 rounds a line rigid point placement graph of size 9n/7 +
O(1). This improves the existing best result of 4n/3 + O(1). We also improve
the lower bound on 2-round algorithms from 17n/16 to 9n/8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3835</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3835</id><created>2012-10-14</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Exploiting Network Cooperation in Green Wireless Communication</title><categories>cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing interest in energy efficient or so-called &quot;green&quot; wireless
communication to reduce the energy consumption in cellular networks. Since
today's wireless terminals are typically equipped with multiple network access
interfaces such as Bluetooth, Wi-Fi, and cellular networks, this paper
investigates user terminals cooperating with each other in transmitting their
data packets to the base station (BS), by exploiting the multiple network
access interfaces, called inter-network cooperation. We also examine the
conventional schemes without user cooperation and with intra-network
cooperation for comparison. Given target outage probability and data rate
requirements, we analyze the energy consumption of conventional schemes as
compared to the proposed inter-network cooperation by taking into account both
physical-layer channel impairments (including path loss, fading, and thermal
noise) and upper-layer protocol overheads. It is shown that distances between
different network entities (i.e., user terminals and BS) have a significant
influence on the energy efficiency of proposed inter-network cooperation
scheme. Specifically, when the cooperating users are close to BS or the users
are far away from each other, the inter-network cooperation may consume more
energy than conventional schemes without user cooperation or with intra-network
cooperation. However, as the cooperating users move away from BS and the
inter-user distance is not too large, the inter-network cooperation
significantly reduces the energy consumption over conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3839</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3839</id><created>2012-10-14</created><authors><author><keyname>John</keyname><forenames>Annu</forenames></author><author><keyname>Konnov</keyname><forenames>Igor</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author><author><keyname>Widder</keyname><forenames>Josef</forenames></author></authors><title>Starting a Dialog between Model Checking and Fault-tolerant Distributed
  Algorithms</title><categories>cs.FL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault-tolerant distributed algorithms are central for building reliable
spatially distributed systems. Unfortunately, the lack of a canonical precise
framework for fault-tolerant algorithms is an obstacle for both verification
and deployment. In this paper, we introduce a new domain-specific framework to
capture the behavior of fault-tolerant distributed algorithms in an adequate
and precise way. At the center of our framework is a parameterized system model
where control flow automata are used for process specification. To account for
the specific features and properties of fault-tolerant distributed algorithms
for message-passing systems, our control flow automata are extended to model
threshold guards as well as the inherent non-determinism stemming from
asynchronous communication, interleavings of steps, and faulty processes.
  We demonstrate the adequacy of our framework in a representative case study
where we formalize a family of well-known fault-tolerant broadcasting
algorithms under a variety of failure assumptions. Our case study is supported
by model checking experiments with safety and liveness specifications for a
fixed number of processes. In the experiments, we systematically varied the
assumptions on both the resilience condition and the failure model. In all
cases, our experiments coincided with the theoretical results predicted in the
distributed algorithms literature. This is giving clear evidence for the
adequacy of our model.
  In a companion paper, we are addressing the new model checking techniques
necessary for parametric verification of the distributed algorithms captured in
our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3846</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3846</id><created>2012-10-14</created><updated>2013-02-03</updated><authors><author><keyname>John</keyname><forenames>Annu</forenames></author><author><keyname>Konnov</keyname><forenames>Igor</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author><author><keyname>Widder</keyname><forenames>Josef</forenames></author></authors><title>Counter Attack on Byzantine Generals: Parameterized Model Checking of
  Fault-tolerant Distributed Algorithms</title><categories>cs.LO cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an automated parameterized verification method for
fault-tolerant distributed algorithms (FTDA). FTDAs are parameterized by both
the number of processes and the assumed maximum number of Byzantine faulty
processes. At the center of our technique is a parametric interval abstraction
(PIA) where the interval boundaries are arithmetic expressions over parameters.
Using PIA for both data abstraction and a new form of counter abstraction, we
reduce the parameterized problem to finite-state model checking. We demonstrate
the practical feasibility of our method by verifying several variants of the
well-known distributed algorithm by Srikanth and Toueg. Our semi-decision
procedures are complemented and motivated by an undecidability proof for FTDA
verification which holds even in the absence of interprocess communication. To
the best of our knowledge, this is the first paper to achieve parameterized
automated verification of Byzantine FTDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3853</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3853</id><created>2012-10-14</created><authors><author><keyname>Wu</keyname><forenames>Peiran</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay</forenames></author></authors><title>Transceiver Design For SC-FDE Based MIMO Relay Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures, Submitted to Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a joint transceiver design for single-carrier
frequency-domain equalization (SC-FDE) based multiple-input multiple-output
(MIMO) relay systems. To this end, we first derive the optimal minimum
mean-squared error linear and decision-feedback frequency-domain equalization
filters at the destination along with the corresponding error covariance
matrices at the output of the equalizer. Subsequently, we formulate the source
and relay precoding matrix design problem as the minimization of a family of
Schur-convex and Schur-concave functions of the mean-squared errors at the
output of the equalizer under separate power constraints for the source and the
relay. By exploiting properties of the error covariance matrix and results from
majorization theory, we derive the optimal structures of the source and relay
precoding matrices, which allows us to transform the matrix optimization
problem into a scalar power optimization problem. Adopting a high
signal-to-noise ratio approximation for the objective function, we obtain the
global optimal solution for the power allocation variables. Simulation results
illustrate the excellent performance of the proposed system and its superiority
compared to conventional orthogonal frequency-division multiplexing based MIMO
relay systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3858</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3858</id><created>2012-10-14</created><authors><author><keyname>Fkih</keyname><forenames>Fethi</forenames></author><author><keyname>Haddar</keyname><forenames>Kais</forenames></author></authors><title>Application of classical compilation techniques for syntactic and
  semantic analysis of specification written in Object Z</title><categories>cs.SE</categories><comments>JS-EABA 2007-5\`emes Journ\'ees Scientifiques de Borj el Amri</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Building a parser for a formal specification language such as Object Z is not
an easy task. Indeed, it requires a double competence both in the compilation
field than in the field of formal specification. In this paper, we first
present some tools for analyzing specifications written in Z and Object Z by
showing the characteristics of each. Then, we identify some common semantic
constraints in Object Z. Finally, we propose an approach for building a parser
for Object Z based on the conventional techniques of compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3865</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3865</id><created>2012-10-14</created><authors><author><keyname>Chen</keyname><forenames>Chien-Liang</forenames></author><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Chang</keyname><forenames>Yuan-Chen</forenames></author><author><keyname>Tsai</keyname><forenames>Hsiang-Ping</forenames></author></authors><title>Opinion Mining for Relating Subjective Expressions and Annual Earnings
  in US Financial Statements</title><categories>cs.CL cs.AI cs.IR q-fin.GN</categories><comments>24 pages, 3 figures, 13 tables, partially appeared in two conference
  proceedings: (1) Proceedings of the IEEE International Conference on
  e-Business Engineering 2011 and (2) Proceedings of the 2011 Conference on
  Technologies and Applications of Artificial Intelligence; Journal of
  Information Science and Engineering, 29(3), May 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial statements contain quantitative information and manager's
subjective evaluation of firm's financial status. Using information released in
U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for
quality financial decisions. To extract such opinioned statements from the
reports, we built tagging models based on the conditional random field (CRF)
techniques, considering a variety of combinations of linguistic factors
including morphology, orthography, predicate-argument structure, syntax, and
simple semantics. Our results show that the CRF models are reasonably effective
to find opinion holders in experiments when we adopted the popular MPQA corpus
for training and testing. The contribution of our paper is to identify opinion
patterns in multiword expressions (MWEs) forms rather than in single word
forms.
  We find that the managers of corporations attempt to use more optimistic
words to obfuscate negative financial performance and to accentuate the
positive financial performance. Our results also show that decreasing earnings
were often accompanied by ambiguous and mild statements in the reporting year
and that increasing earnings were stated in assertive and positive way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3872</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3872</id><created>2012-10-14</created><updated>2013-05-02</updated><authors><author><keyname>Deng</keyname><forenames>Na</forenames></author><author><keyname>Zhao</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Sihai</forenames></author><author><keyname>Zhou</keyname><forenames>Wuyang</forenames></author></authors><title>Traffic-Aware Relay Sleep Control to Improve Energy Efficiency in Joint
  Macro-Relay Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author because this work needs
  to be further improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider a joint macro-relay network with densely deployed
relay stations (RSs) and dynamically varied traffic load measured by the number
of users. An energy-efficient strategy is proposed by intelligently adjusting
the RS working modes (active or sleeping) according to the traffic variation.
Explicit expressions related to the network energy efficiency are derived based
on stochastic geometry theory. Simulation results demonstrate that the derived
analytic results are reasonable and the proposed strategy can significantly
improve the network energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3876</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3876</id><created>2012-10-14</created><authors><author><keyname>Xu</keyname><forenames>Xi</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Khokhar</keyname><forenames>Ashfaq</forenames></author></authors><title>Power-efficient Hierarchical Data Aggregation using Compressive Sensing
  in WSN</title><categories>cs.DC</categories><comments>6 pages,10 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing (CS) method is a burgeoning technique being applied to
diverse areas including wireless sensor networks (WSNs). In WSNs, it has been
studied in the context of data gathering and aggregation, particularly aimed at
reducing data transmission cost and improving power efficiency. Existing CS
based data gathering work in WSNs assume fixed and uniform compression
threshold across the network, regard- less of the data field characteristics.
In this paper, we present a novel data aggregation architecture model that
combines a multi- resolution structure with compressed sensing. The compression
thresholds vary over the aggregation hierarchy, reflecting the underlying data
field. Compared with previous relevant work, the proposed model shows its
significant energy saving from theoretical analysis. We have also implemented
the proposed CS- based data aggregation framework on a SIDnet SWANS platform,
discrete event simulator commonly used for WSN simulations. Our experiments
show substantial energy savings, ranging from 37% to 77% for different nodes in
the networking depending on the position of hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3877</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3877</id><created>2012-10-14</created><authors><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Inapproximability of the Smallest Superpolyomino Problem</title><categories>cs.CG</categories><comments>An abstract version has been submitted to Fall Workshop on
  Computational Geometry 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the \emph{smallest superpolyomino problem}: given a set of
colored polyominoes, find the smallest polyomino containing each input
polyomino as a subshape. This problem is shown to be NP-hard, even when
restricted to a set of polyominoes using a single common color. Moreover, for
sets of polyominoes using two or more colors, the problem is shown to be
NP-hard to approximate within a $O(n^{1/3-\varepsilon})$-factor for any
$\varepsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3906</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3906</id><created>2012-10-15</created><authors><author><keyname>Park</keyname><forenames>Hosung</forenames></author><author><keyname>Hong</keyname><forenames>Seokbeom</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Design of Multiple-Edge Protographs for QC LDPC Codes Avoiding Short
  Inevitable Cycles</title><categories>cs.IT math.IT</categories><comments>42 pages, submitted to IEEE Transactions on Information Theory on
  June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been lots of efforts on the construction of quasi-cyclic (QC)
low-density parity-check (LDPC) codes with large girth. However, most of them
are focused on protographs with single edges and little research has been done
for the construction of QC LDPC codes lifted from protographs with multiple
edges. Compared to single-edge protographs, multiple-edge protographs have
benefits such that QC LDPC codes lifted from them can potentially have larger
minimum Hamming distance. In this paper, all subgraph patterns of multiple-edge
protographs, which prevent QC LDPC codes from having large girth by inducing
inevitable cycles, are fully investigated based on graph-theoretic approach. By
using combinatorial designs, a systematic construction method of multiple-edge
protographs is proposed for regular QC LDPC codes with girth at least 12 and
also other method is proposed for regular QC LDPC codes with girth at least 14.
A construction algorithm of QC LDPC codes by lifting multiple-edge protographs
is proposed and it is shown that the resulting QC LDPC codes have larger upper
bounds on the minimum Hamming distance than those lifted from single-edge
protographs. Simulation results are provided to compare the performance of the
proposed QC LDPC codes, the progressive edge-growth (PEG) LDPC codes, and the
PEG QC LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3921</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3921</id><created>2012-10-15</created><updated>2013-04-04</updated><authors><author><keyname>Ley</keyname><forenames>Christophe</forenames></author><author><keyname>Swan</keyname><forenames>Yvik</forenames></author></authors><title>Stein's density approach and information inequalities</title><categories>math.PR cs.IT math.IT</categories><comments>This is a revised version of our paper &quot;On a connection between Stein
  characterizations and Fisher information&quot; (arXiv reference :
  arXiv:1111.2368). Essential changes have been made. Certain elements of the
  previous version remain relevant to the literature and have not been included
  in the present version, therefore we upload this as a new arXiv submission</comments><journal-ref>Electronic Communications in Probability 18 1-14 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new perspective on Stein's so-called density approach by
introducing a new operator and characterizing class which are valid for a much
wider family of probability distributions on the real line. We prove an
elementary factorization property of this operator and propose a new Stein
identity which we use to derive information inequalities in terms of what we
call the \emph{generalized Fisher information distance}. We provide explicit
bounds on the constants appearing in these inequalities for several important
cases. We conclude with a comparison between our results and known results in
the Gaussian case, hereby improving on several known inequalities from the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3926</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3926</id><created>2012-10-15</created><updated>2012-10-31</updated><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Learning Attitudes and Attributes from Multi-Aspect Reviews</title><categories>cs.CL cs.IR cs.LG</categories><comments>11 pages, 6 figures, extended version of our ICDM 2012 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of online reviews consist of plain-text feedback together with a
single numeric score. However, there are multiple dimensions to products and
opinions, and understanding the `aspects' that contribute to users' ratings may
help us to better understand their individual preferences. For example, a
user's impression of an audiobook presumably depends on aspects such as the
story and the narrator, and knowing their opinions on these aspects may help us
to recommend better products. In this paper, we build models for rating systems
in which such dimensions are explicit, in the sense that users leave separate
ratings for each aspect of a product. By introducing new corpora consisting of
five million reviews, rated with between three and six aspects, we evaluate our
models on three prediction tasks: First, we use our model to uncover which
parts of a review discuss which of the rated aspects. Second, we use our model
to summarize reviews, which for us means finding the sentences that best
explain a user's rating. Finally, since aspect ratings are optional in many of
the datasets we consider, we use our model to recover those ratings that are
missing from a user's evaluation. Our model matches state-of-the-art approaches
on existing small-scale datasets, while scaling to the real-world datasets we
introduce. Moreover, our model is able to `disentangle' content and sentiment
words: we automatically learn content words that are indicative of a particular
aspect as well as the aspect-specific sentiment words that are indicative of a
particular rating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3937</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3937</id><created>2012-10-15</created><authors><author><keyname>Dovier</keyname><forenames>Agostino</forenames></author><author><keyname>Costa</keyname><forenames>V&#xed;tor Santos</forenames></author></authors><title>Introduction to the 28th International Conference on Logic Programming
  Special Issue</title><categories>cs.PL cs.AI</categories><acm-class>D.1.6</acm-class><journal-ref>TPLP 12 (4-5): 421-426, 2012</journal-ref><doi>10.1017/S1471068412000300</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are proud to introduce this special issue of the Journal of Theory and
Practice of Logic Programming (TPLP), dedicated to the full papers accepted for
the 28th International Conference on Logic Programming (ICLP). The ICLP
meetings started in Marseille in 1982 and since then constitute the main venue
for presenting and discussing work in the area of logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3943</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3943</id><created>2012-10-15</created><authors><author><keyname>Baggio</keyname><forenames>Rodolfo</forenames></author><author><keyname>Del Chiappa</keyname><forenames>Giacomo</forenames></author></authors><title>Tourism destinations as digital business ecosystems</title><categories>cs.CY physics.soc-ph</categories><comments>9 pages, 3 figures, 2 tables; accepted for publication in the
  proceedings of: ENTER2013: 20th International Conference on Information
  Technology and Travel and Tourism, January 23-25, 2013, Innsbruck (Austria)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tourism has been experiencing very relevant changes since when Information
and Communication Technologies (ICTs), in all their forms, have started to
pervade the industry and the market. In the last decade, a new concept gained
the attention of both researchers and practitioners, that of Digital Business
Ecosystem (DBE). It can be considered as a technological infrastructure aimed
at creating a digital environment to support and enhance networking between
enterprises and stakeholders operating within a sector. Aim of this paper is to
assess the extent to which the technological connection has affected the
structural configuration of the tourism system and, specifically, of tourism
destinations. The present study argues that two components can be considered
when assessing the relationships among stakeholders within a tourism
destination: a real and a virtual one. Further it shows how these two
components are structurally strongly coupled and co-evolve forming a single
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3946</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3946</id><created>2012-10-15</created><authors><author><keyname>Daolio</keyname><forenames>Fabio</forenames><affiliation>ISI</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ochoa</keyname><forenames>Gabriela</forenames><affiliation>ISI</affiliation></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames><affiliation>ISI</affiliation></author></authors><title>Local optima networks and the performance of iterated local search</title><categories>cs.AI</categories><comments>Proceedings of the fourteenth international conference on Genetic and
  evolutionary computation conference, Philadelphia : United States (2012)</comments><proxy>ccsd</proxy><doi>10.1145/2330163.2330217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local Optima Networks (LONs) have been recently proposed as an alternative
model of combinatorial fitness landscapes. The model compresses the information
given by the whole search space into a smaller mathematical object that is the
graph having as vertices the local optima and as edges the possible weighted
transitions between them. A new set of metrics can be derived from this model
that capture the distribution and connectivity of the local optima in the
underlying configuration space. This paper departs from the descriptive
analysis of local optima networks, and actively studies the correlation between
network features and the performance of a local search heuristic. The NK family
of landscapes and the Iterated Local Search metaheuristic are considered. With
a statistically-sound approach based on multiple linear regression, it is shown
that some LONs' features strongly influence and can even partly predict the
performance of a heuristic search algorithm. This study validates the
expressive power of LONs as a model of combinatorial fitness landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3953</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3953</id><created>2012-10-15</created><authors><author><keyname>Shukla</keyname><forenames>Srishti</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Wireless Network-Coded Four-Way Relaying Using Latin Hyper-Cubes</title><categories>cs.IT math.IT</categories><comments>14 pages, 6 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1112.1584</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with physical layer network-coding for the four-way wireless
relaying scenario where four nodes A, B, C and D wish to communicate their
messages to all the other nodes with the help of the relay node R. The scheme
given in the paper is based on the denoise-and-forward scheme proposed first by
Popovski et al. Intending to minimize the number of channel uses, the protocol
employs two phases: Multiple Access (MA) phase and Broadcast (BC) phase with
each phase utilizing one channel use. This paper does the equivalent for the
four-way relaying scenario as was done for the two-way relaying scenario by
Koike-Akino et al., and for three-way relaying scenario in [3]. It is observed
that adaptively changing the network coding map used at the relay according to
the channel conditions greatly reduces the impact of multiple access
interference which occurs at the relay during the MA phase. These network
coding maps are so chosen so that they satisfy a requirement called exclusive
law. We show that when the four users transmit points from the same M-PSK
constellation, every such network coding map that satisfies the exclusive law
can be represented by a 4-fold Latin Hyper-Cube of side M. The network code map
used by the relay for the BC phase is explicitly obtained and is aimed at
reducing the effect of interference at the MA stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3962</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3962</id><created>2012-10-15</created><authors><author><keyname>Zhou</keyname><forenames>Xiaojun</forenames></author></authors><title>Improved Canonical Dual Algorithms for the Maxcut Problem</title><categories>math.OC cs.DS math.CO math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By introducing a quadratic perturbation to the canonical dual of the maxcut
problem, we transform the integer programming problem into a concave
maximization problem over a convex positive domain under some circumstances,
which can be solved easily by the well-developed optimization methods.
Considering that there may exist no critical points in the dual feasible
domain, a reduction technique is used gradually to guarantee the feasibility of
the reduced solution, and a compensation technique is utilized to strengthen
the robustness of the solution. The similar strategy is also applied to the
maxcut problem with linear perturbation and its hybrid with quadratic
perturbation. Experimental results demonstrate the effectiveness of the
proposed algorithms when compared with other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3978</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3978</id><created>2012-10-15</created><authors><author><keyname>Crampton</keyname><forenames>J.</forenames></author><author><keyname>Crowston</keyname><forenames>R.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author></authors><title>Fixed-Parameter Tractability of Workflow Satisfiability in the Presence
  of Seniority Constraints</title><categories>cs.CR cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workflow satisfiability problem is concerned with determining whether it
is possible to find an allocation of authorized users to the steps in a
workflow in such a way that all constraints are satisfied. The problem is
NP-hard in general, but is known to be fixed-parameter tractable for certain
classes of constraints. The known results on fixed-parameter tractability rely
on the symmetry (in some sense) of the constraints. In this paper, we provide
the first results that establish fixed-parameter tractability of the
satisfiability problem when the constraints are asymmetric. In particular, we
introduce the notion of seniority constraints, in which the execution of steps
is determined, in part, by the relative seniority of the users that perform
them. Our results require new techniques, which make use of tree decompositions
of the graph of the binary relation defining the constraint. Finally, we
establish a lower bound for the hardness of the workflow satisfiability
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4006</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4006</id><created>2012-10-15</created><authors><author><keyname>Harel</keyname><forenames>Maayan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>The Perturbed Variation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new discrepancy score between two distributions that gives an
indication on their similarity. While much research has been done to determine
if two samples come from exactly the same distribution, much less research
considered the problem of determining if two finite samples come from similar
distributions. The new score gives an intuitive interpretation of similarity;
it optimally perturbs the distributions so that they best fit each other. The
score is defined between distributions, and can be efficiently estimated from
samples. We provide convergence bounds of the estimated score, and develop
hypothesis testing procedures that test if two data sets come from similar
distributions. The statistical power of this procedures is presented in
simulations. We also compare the score's capacity to detect similarity with
that of other known measures on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4007</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4007</id><created>2012-10-15</created><updated>2013-02-12</updated><authors><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Murata</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Wakita</keyname><forenames>Ken</forenames></author></authors><title>Extending modularity by capturing the similarity attraction feature in
  the null model</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is a widely used measure for evaluating community structure in
networks. The definition of modularity involves a comparison of
within-community edges in the observed network and that number in an equivalent
randomized network. This equivalent randomized network is called the null
model, which serves as a reference. To make the comparison significant, the
null model should characterize some features of the observed network. However,
the null model in the original definition of modularity is unrealistically
mixed, in the sense that any node can be linked to any other node without
preference and only connectivity matters. Thus, it fails to be a good
representation of real-world networks. A common feature of many real-world
networks is &quot;similarity attraction&quot;, i.e., edges tend to link to nodes that are
similar to each other. We propose a null model that captures the similarity
attraction feature. This null model enables us to create a framework for
defining a family of Dist-Modularity adapted to various networks, including
networks with additional information on nodes. We demonstrate that
Dist-Modularity is useful in identifying communities at different scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4008</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4008</id><created>2012-10-15</created><authors><author><keyname>Santos</keyname><forenames>Augusto Dias Pereira dos</forenames></author><author><keyname>Wives</keyname><forenames>Leandro Krug</forenames></author><author><keyname>Alvares</keyname><forenames>Luis Otavio</forenames></author></authors><title>Location-Based Events Detection on Micro-Blogs</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>10 pages, 5 figures, submitted and rejected for SBBD 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of social networks generates enormous amounts of data that
can be used for many types of analysis. Some of these data have temporal and
geographical information, which can be used for comprehensive examination. In
this paper, we propose a new method to analyze the massive volume of messages
available in Twitter to identify places in the world where topics such as TV
shows, climate change, disasters, and sports are emerging. The proposed method
is based on a neural network that is used to detect outliers from a time
series, which is built upon statistical data from tweets located on different
political divisions (i.e., countries, cities). The outliers are used to
identify topics within an abnormal behavior in Twitter. The effectiveness of
our method is evaluated in an online environment indicating new findings on
modeling local people's behavior from different places.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4014</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4014</id><created>2012-10-15</created><updated>2012-11-26</updated><authors><author><keyname>Fournier</keyname><forenames>Laurent</forenames></author></authors><title>\'Economie des biens immat\'eriels - Economics of Intangible Goods</title><categories>cs.OH</categories><comments>First draft in French...expected in English in December</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new economic system suited for Intangible Goods ({\sc ig}). We
argue that such system can now be implemented in the real world using advance
technics in distributed network computing and cryptography. The specification
of the so called \net{} is presented. To Limit the number of financial
transactions, the system is forced to define its own currency, with many
benefits. The new &quot;cup&quot; currency, extended worldwide, is dedicated to {\sc ig},
available only for person-to-person trading, protected from speculation and
adapted for tax recovery with no additional computation. Those nices features
makes the \net{} a new democratic tool, fixing specific issues in {\sc ig}
trading and reviving a whole domain activity. We emphasis on the fact that all
proposed documentation, algorithm, program in any language related to this
proposal shall be open-source without any possibility to post any patent of any
sort on the system or subsystem. This new trading model should be considered as
a pure intellectual construction, like parts of Mathematics and then belongs to
nobody or everybody, like $1+1=2$. Next step will be to test, validate the
security of various implementations details, and to ask for legal rules
adaptations. The first draft paper is written in French language and posted to
arXiv.org and hal.archive-ouverte.fr . We expect to provide an English
translation before Christmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4021</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4021</id><created>2012-10-15</created><authors><author><keyname>Chicano</keyname><forenames>Francisco</forenames><affiliation>ISI</affiliation></author><author><keyname>Daolio</keyname><forenames>Fabio</forenames><affiliation>ISI</affiliation></author><author><keyname>Ochoa</keyname><forenames>Gabriela</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames><affiliation>ISI</affiliation></author><author><keyname>Alba</keyname><forenames>Enrique</forenames></author></authors><title>Local Optima Networks, Landscape Autocorrelation and Heuristic Search
  Performance</title><categories>cs.AI cs.NE</categories><comments>Parallel Problem Solving from Nature - PPSN XII, Taormina : Italy
  (2012)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-32964-7_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in fitness landscape analysis include the study of Local
Optima Networks (LON) and applications of the Elementary Landscapes theory.
This paper represents a first step at combining these two tools to explore
their ability to forecast the performance of search algorithms. We base our
analysis on the Quadratic Assignment Problem (QAP) and conduct a large
statistical study over 600 generated instances of different types. Our results
reveal interesting links between the network measures, the autocorrelation
measures and the performance of heuristic search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4053</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4053</id><created>2012-10-15</created><updated>2012-11-22</updated><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Tuval</keyname><forenames>Omry</forenames></author></authors><title>Joint Cache Partition and Job Assignment on Multi-Core Processors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicore shared cache processors pose a challenge for designers of embedded
systems who try to achieve minimal and predictable execution time of workloads
consisting of several jobs. To address this challenge the cache is statically
partitioned among the cores and the jobs are assigned to the cores so as to
minimize the makespan. Several heuristic algorithms have been proposed that
jointly decide how to partition the cache among the cores and assign the jobs.
We initiate a theoretical study of this problem which we call the joint cache
partition and job assignment problem.
  By a careful analysis of the possible cache partitions we obtain a constant
approximation algorithm for this problem. For some practical special cases we
obtain a 2-approximation algorithm, and show how to improve the approximation
factor even further by allowing the algorithm to use additional cache. We also
study possible improvements that can be obtained by allowing dynamic cache
partitions and dynamic job assignments.
  We define a natural special case of the well known scheduling problem on
unrelated machines in which machines are ordered by &quot;strength&quot;. Our joint cache
partition and job assignment problem generalizes this scheduling problem which
we think is of independent interest. We give a polynomial time algorithm for
this scheduling problem for instances obtained by fixing the cache partition in
a practical case of the joint cache partition and job assignment problem where
job loads are step functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4081</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4081</id><created>2012-10-15</created><authors><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author><author><keyname>Schmidt</keyname><forenames>Stefan</forenames></author></authors><title>Getting Feasible Variable Estimates From Infeasible Ones: MRF Local
  Polytope Study</title><categories>cs.NA cs.CV cs.DS cs.LG math.OC</categories><comments>20 page, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method for construction of approximate feasible primal
solutions from dual ones for large-scale optimization problems possessing
certain separability properties. Whereas infeasible primal estimates can
typically be produced from (sub-)gradients of the dual function, it is often
not easy to project them to the primal feasible set, since the projection
itself has a complexity comparable to the complexity of the initial problem. We
propose an alternative efficient method to obtain feasibility and show that its
properties influencing the convergence to the optimum are similar to the
properties of the Euclidean projection. We apply our method to the local
polytope relaxation of inference problems for Markov Random Fields and
demonstrate its superiority over existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4120</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4120</id><created>2012-10-12</created><updated>2012-10-20</updated><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>NP-Completeness of deciding the feasibility of Linear Equations over
  binary-variables with coefficients and constants that are 0, 1, or -1</title><categories>cs.CC cs.DM</categories><comments>Proving strong NP-Completeness of our problem. Included a
  polynomial-time reduction from the 3-SAT problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We convert, within polynomial-time and sequential processing, NP-Complete
Problems into a problem of deciding feasibility of a given system S of linear
equations with constants and coefficients of binary-variables that are 0, 1, or
-1. S is feasible, if and only if, the NP-Complete problem has a feasible
solution. We show separate polynomial-time conversions to S, from the
SUBSET-SUM and 3-SAT problems, both of which are NP-Complete. The number of
equations and variables in S is bounded by a polynomial function of the size of
the NP-Complete problem, showing that deciding the feasibility of S is
strongly-NP-Complete. We also show how to apply the approach used for the
SUBSET-SUM problem to decide the feasibility of Integer Linear Programs, as it
involves reducing the coefficient-magnitudes of variables to the logarithm of
their initial values, though the number of variables and equations are
increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4130</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4130</id><created>2012-10-15</created><authors><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author><author><keyname>Pichotta</keyname><forenames>Karl</forenames></author><author><keyname>Yang</keyname><forenames>Fangkai</forenames></author></authors><title>Relational Theories with Null Values and Non-Herbrand Stable Models</title><categories>cs.LO cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized relational theories with null values in the sense of Reiter are
first-order theories that provide a semantics for relational databases with
incomplete information. In this paper we show that any such theory can be
turned into an equivalent logic program, so that models of the theory can be
generated using computational methods of answer set programming. As a step
towards this goal, we develop a general method for calculating stable models
under the domain closure assumption but without the unique name assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4145</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4145</id><created>2012-10-15</created><authors><author><keyname>Sokoloski</keyname><forenames>Sacha</forenames></author></authors><title>A Biologically Realistic Model of Saccadic Eye Control with
  Probabilistic Population Codes</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The posterior parietal cortex is believed to direct eye movements, especially
in regards to target tracking tasks, and a number of debates exist over the
precise nature of the computations performed by the parietal cortex, with each
side supported by different sets of biological evidence. In this paper I will
present my model which navigates a course between some of these debates,
towards the end of presenting a model which can explain some of the competing
interpretations among the data sets. In particular, rather than assuming that
proprioception or efference copies form the key source of information for
computing eye position information, I use a biological plausible implementation
of a Kalman filter to optimally combine the two signals, and a simple gain
control mechanism in order to accommodate the latency of the proprioceptive
signal. Fitting within the Bayesian brain hypothesis, the result is a Bayes
optimal solution to the eye control problem, with a range of data supporting
claims of biological plausibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4184</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4184</id><created>2012-10-15</created><authors><author><keyname>Chatzis</keyname><forenames>Sotirios P.</forenames></author><author><keyname>Korkinof</keyname><forenames>Dimitrios</forenames></author><author><keyname>Demiris</keyname><forenames>Yiannis</forenames></author></authors><title>The Kernel Pitman-Yor Process</title><categories>cs.LG cs.AI stat.ML</categories><comments>This is a Technical Report summarizing our ongoing work on the Kernel
  Pitman-Yor Process. Experiments will be added by D. Korkinof prior to journal
  or conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose the kernel Pitman-Yor process (KPYP) for
nonparametric clustering of data with general spatial or temporal
interdependencies. The KPYP is constructed by first introducing an infinite
sequence of random locations. Then, based on the stick-breaking construction of
the Pitman-Yor process, we define a predictor-dependent random probability
measure by considering that the discount hyperparameters of the
Beta-distributed random weights (stick variables) of the process are not
uniform among the weights, but controlled by a kernel function expressing the
proximity between the location assigned to each weight and the given
predictors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4211</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4211</id><created>2012-10-15</created><updated>2013-06-05</updated><authors><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>Profit Maximization over Social Networks</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>19 pages, 8 figures. An abbreviated version appears in 2012 IEEE
  International Conference on Data Mining (ICDM'12). The second version
  includes some minor fixes</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is the problem of finding a set of influential users
in a social network such that the expected spread of influence under a certain
propagation model is maximized. Much of the previous work has neglected the
important distinction between social influence and actual product adoption.
However, as recognized in the management science literature, an individual who
gets influenced by social acquaintances may not necessarily adopt a product (or
technology), due, e.g., to monetary concerns. In this work, we distinguish
between influence and adoption by explicitly modeling the states of being
influenced and of adopting a product. We extend the classical Linear Threshold
(LT) model to incorporate prices and valuations, and factor them into users'
decision-making process of adopting a product. We show that the expected profit
function under our proposed model maintains submodularity under certain
conditions, but no longer exhibits monotonicity, unlike the expected influence
spread function. To maximize the expected profit under our extended LT model,
we employ an unbudgeted greedy framework to propose three profit maximization
algorithms. The results of our detailed experimental study on three real-world
datasets demonstrate that of the three algorithms, \textsf{PAGE}, which assigns
prices dynamically based on the profit potential of each candidate seed, has
the best performance both in the expected profit achieved and in running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4231</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4231</id><created>2012-10-15</created><authors><author><keyname>Grastien</keyname><forenames>Alban</forenames></author></authors><title>An example illustrating the imprecision of the efficient approach for
  diagnosis of Petri nets via integer linear programming</title><categories>cs.SY cs.AI</categories><comments>3 pages</comments><msc-class>68T37, 93C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document demonstrates that the efficient approach for diagnosis of Petri
nets via integer linear programming may be unable to detect a fault even if the
system is diagnosable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4235</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4235</id><created>2012-10-15</created><authors><author><keyname>Poulakakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Scardovi</keyname><forenames>Luca</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Node Classification in Networks of Stochastic Evidence Accumulators</title><categories>cs.SY math.OC</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a network of stochastic evidence accumulators, each
represented by a drift-diffusion model accruing evidence towards a decision in
continuous time by observing a noisy signal and by exchanging information with
other units according to a fixed communication graph. We bring into focus the
relationship between the location of each unit in the communication graph and
its certainty as measured by the inverse of the variance of its state. We show
that node classification according to degree distributions or geodesic
distances cannot faithfully capture node ranking in terms of certainty.
Instead, all possible paths connecting each unit with the rest in the network
must be incorporated. We make this precise by proving that node classification
according to information centrality provides a rank ordering with respect to
node certainty, thereby affording a direct interpretation of the certainty
level of each unit in terms of the structural properties of the underlying
communication graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4243</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4243</id><created>2012-10-16</created><updated>2012-10-18</updated><authors><author><keyname>Barua</keyname><forenames>Bappi</forenames></author><author><keyname>Abolhasan</keyname><forenames>Mehran</forenames></author><author><keyname>Franklin</keyname><forenames>Daniel</forenames></author><author><keyname>Safaei</keyname><forenames>Farzad</forenames></author></authors><title>Outage Probability Analysis of Dual Hop Relay Networks in Presence of
  Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative relaying improves the performance of wireless networks by forming
a network of multiple independent virtual sources transmitting the same
information as the source node. However, interference induced in the network
reduces the performance of cooperative communications. In this work the
statistical properties, the cumulative distribution function (CDF) and the
probability density function (PDF) for a basic dual hop cooperative relay
network with an arbitrary number of interferers over Rayleigh fading channels
are derived. Two system models are considered: in the first system model, the
interferers are only at the relay node; and in the second system model,
interferers are both at the relay and the destination. This work is further
extended to Nakagami-m faded interfering channels. Simulation results are
presented on outage probability performance to verify the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4246</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4246</id><created>2012-10-16</created><authors><author><keyname>Larusso</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Ruttenberg</keyname><forenames>Brian E.</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj</forenames></author></authors><title>A Latent Parameter Node-Centric Model for Spatial Networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0071293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial networks, in which nodes and edges are embedded in space, play a
vital role in the study of complex systems. For example, many social networks
attach geo-location information to each user, allowing the study of not only
topological interactions between users, but spatial interactions as well. The
defining property of spatial networks is that edge distances are associated
with a cost, which may subtly influence the topology of the network. However,
the cost function over distance is rarely known, thus developing a model of
connections in spatial networks is a difficult task.
  In this paper, we introduce a novel model for capturing the interaction
between spatial effects and network structure. Our approach represents a unique
combination of ideas from latent variable statistical models and spatial
network modeling. In contrast to previous work, we view the ability to form
long/short-distance connections to be dependent on the individual nodes
involved. For example, a node's specific surroundings (e.g. network structure
and node density) may make it more likely to form a long distance link than
other nodes with the same degree. To capture this information, we attach a
latent variable to each node which represents a node's spatial reach. These
variables are inferred from the network structure using a Markov Chain Monte
Carlo algorithm.
  We experimentally evaluate our proposed model on 4 different types of
real-world spatial networks (e.g. transportation, biological, infrastructure,
and social). We apply our model to the task of link prediction and achieve up
to a 35% improvement over previous approaches in terms of the area under the
ROC curve. Additionally, we show that our model is particularly helpful for
predicting links between nodes with low degrees. In these cases, we see much
larger improvements over previous models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4247</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4247</id><created>2012-10-16</created><authors><author><keyname>Woo</keyname><forenames>Jun-Young</forenames></author><author><keyname>Joo</keyname><forenames>Hyun-Seung</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Deterministic Selection of Phase Sequences in Low Complexity SLM Scheme</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selected mapping (SLM) is a suitable scheme, which can solve the
peak-to-average power ratio (PAPR) problem. Recently, many researchers have
concentrated on reducing the computational complexity of the SLM schemes. One
of the low complexity SLM schemes is the Class III SLM scheme which uses only
one inverse fast fourier transform (IFFT) operation for generating one
orthogonal frequency division multiplexing (OFDM) signal sequence. By selecting
rotations and cyclic shifts randomly, it can generate $N^3$ alternative OFDM
signal sequences, where $N$ is the FFT size. But this selection can not
guarantee the optimal PAPR reduction performances. Therefore, in this paper, we
propose a simple deterministic cyclic shifts selection method which is optimal
in case of having low variance of correlation coefficient between two
alternative OFDM signal sequences. And we show that cyclic shifts are highly
dependent on the PAPR reduction performance than rotations. For small FFT size
and the number of alternative signal sequences is close to $N/8$, simulation
results show that the proposed scheme can achieve better PAPR reduction
performance than the Class III SLM scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4251</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4251</id><created>2012-10-16</created><authors><author><keyname>Suhartanto</keyname><forenames>Heru</forenames></author><author><keyname>Yanuar</keyname><forenames>Arry</forenames></author><author><keyname>Wibisono</keyname><forenames>Ari</forenames></author></authors><title>Performance Analysis Cluster and GPU Computing Environment on Molecular
  Dynamic Simulation of BRV-1 and REM2 with GROMACS</title><categories>cs.DC cs.CE q-bio.BM</categories><comments>5 pages, 1 figure, 5 tables</comments><journal-ref>Int. J. Comp. Sci. Issue (2011), Vol. 8, Issue 4, No 2, p131-135</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of application that needs high performance computing resources is
molecular d ynamic. There is some software available that perform molecular
dynamic, one of these is a well known GROMACS. Our previous experiment
simulating molecular dynamics of Indonesian grown herbal compounds show
sufficient speed up on 32 n odes Cluster computing environment. In order to
obtain a reliable simulation, one usually needs to run the experiment on the
scale of hundred nodes. But this is expensive to develop and maintain. Since
the invention of Graphical Processing Units that is also useful for general
programming, many applications have been developed to run on this. This paper
reports our experiments that evaluate the performance of GROMACS that runs on
two different environment, Cluster computing resources and GPU based PCs. We
run the experiment on BRV-1 and REM2 compounds. Four different GPUs are
installed on the same type of PCs of quad cores; they are Gefore GTS 250, GTX
465, GTX 470 and Quadro 4000. We build a cluster of 16 nodes based on these
four quad cores PCs. The preliminary experiment shows that those run on GTX 470
is the best among the other type of GPUs and as well as the cluster computing
resource. A speed up around 11 and 12 is gained, while the cost of computer
with GPU is only about 25 percent that of Cluster we built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4263</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4263</id><created>2012-10-16</created><updated>2012-10-17</updated><authors><author><keyname>Boutier</keyname><forenames>Matthieu</forenames><affiliation>PPS</affiliation></author><author><keyname>Kerneis</keyname><forenames>Gabriel</forenames><affiliation>PPS</affiliation></author></authors><title>Generating events with style</title><categories>cs.PL</categories><comments>Submitted for publication</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threads and events are two common abstractions for writing concurrent
programs. Because threads are often more convenient, but events more efficient,
it is natural to want to translate the former into the latter. However, whereas
there are many different event-driven styles, existing translators often apply
ad-hoc rules which do not reflect this diversity. We analyse various
control-flow and data-flow encodings in real-world event-driven code, and we
observe that it is possible to generate any of these styles automatically from
threaded code, by applying certain carefully chosen classical program
transformations. In particular, we implement two of these transformations,
lambda lifting and environments, in CPC, an extension of the C language for
writing concurrent systems. Finally, we find out that, although rarely used in
real-world programs because it is tedious to perform manually, lambda lifting
yields better performance than environments in most of our benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4276</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4276</id><created>2012-10-16</created><authors><author><keyname>Lebichot</keyname><forenames>Bertrand</forenames></author><author><keyname>Kivim&#xe4;ki</keyname><forenames>Ilkka</forenames></author><author><keyname>Fran&#xe7;oisse</keyname><forenames>Kevin</forenames></author><author><keyname>Saerens</keyname><forenames>Marco</forenames></author></authors><title>Semi-Supervised Classification Through the Bag-of-Paths Group
  Betweenness</title><categories>stat.ML cs.LG</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel, well-founded, betweenness measure, called the
Bag-of-Paths (BoP) betweenness, as well as its extension, the BoP group
betweenness, to tackle semisupervised classification problems on weighted
directed graphs. The objective of semi-supervised classification is to assign a
label to unlabeled nodes using the whole topology of the graph and the labeled
nodes at our disposal. The BoP betweenness relies on a bag-of-paths framework
assigning a Boltzmann distribution on the set of all possible paths through the
network such that long (high-cost) paths have a low probability of being picked
from the bag, while short (low-cost) paths have a high probability of being
picked. Within that context, the BoP betweenness of node j is defined as the
sum of the a posteriori probabilities that node j lies in-between two arbitrary
nodes i, k, when picking a path starting in i and ending in k. Intuitively, a
node typically receives a high betweenness if it has a large probability of
appearing on paths connecting two arbitrary nodes of the network. This quantity
can be computed in closed form by inverting a n x n matrix where n is the
number of nodes. For the group betweenness, the paths are constrained to start
and end in nodes within the same class, therefore defining a group betweenness
for each class. Unlabeled nodes are then classified according to the class
showing the highest group betweenness. Experiments on various real-world data
sets show that BoP group betweenness outperforms all the tested state
of-the-art methods. The benefit of the BoP betweenness is particularly
noticeable when only a few labeled nodes are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4277</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4277</id><created>2012-10-16</created><updated>2013-03-14</updated><authors><author><keyname>Oxvig</keyname><forenames>Christian Schou</forenames></author><author><keyname>Pedersen</keyname><forenames>Patrick Steffen</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author></authors><title>Improving Smoothed l0 Norm in Compressive Sensing Using Adaptive
  Parameter Selection</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal reconstruction in compressive sensing involves finding a sparse
solution that satisfies a set of linear constraints. Several approaches to this
problem have been considered in existing reconstruction algorithms. They each
provide a trade-off between reconstruction capabilities and required
computation time. In an attempt to push the limits for this trade-off, we
consider a smoothed l0 norm (SL0) algorithm in a noiseless setup. We argue that
using a set of carefully chosen parameters in our proposed adaptive SL0
algorithm may result in significantly better reconstruction capabilities in
terms of phase transition while retaining the same required computation time as
existing SL0 algorithms. A large set of simulations further support this claim.
Simulations even reveal that the theoretical l1 curve may be surpassed in major
parts of the phase space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4289</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4289</id><created>2012-10-16</created><updated>2014-05-13</updated><authors><author><keyname>Ganty</keyname><forenames>Pierre</forenames></author><author><keyname>Iosif</keyname><forenames>Radu</forenames></author><author><keyname>Konecny</keyname><forenames>Filip</forenames></author></authors><title>Underapproximation of Procedure Summaries for Integer Programs</title><categories>cs.PL cs.FL</categories><comments>16 pages, 1 figure</comments><acm-class>D.2.4</acm-class><doi>10.1007/978-3-642-36742-7_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to underapproximate the procedure summaries of recursive programs
over the integers using off-the-shelf analyzers for non-recursive programs. The
novelty of our approach is that the non-recursive program we compute may
capture unboundedly many behaviors of the original recursive program for which
stack usage cannot be bounded. Moreover, we identify a class of recursive
programs on which our method terminates and returns the precise summary
relations without underapproximation. Doing so, we generalize a similar result
for non-recursive programs to the recursive case. Finally, we present
experimental results of an implementation of our method applied on a number of
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4290</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4290</id><created>2012-10-16</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Shen</keyname><forenames>Yanning</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>A Fast Iterative Algorithm for Recovery of Sparse Signals from One-Bit
  Quantized Measurements</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of reconstructing sparse or compressible
signals from one-bit quantized measurements. We study a new method that uses a
log-sum penalty function, also referred to as the Gaussian entropy, for sparse
signal recovery. Also, in the proposed method, sigmoid functions are introduced
to quantify the consistency between the acquired one-bit quantized data and the
reconstructed measurements. A fast iterative algorithm is developed by
iteratively minimizing a convex surrogate function that bounds the original
objective function, which leads to an iterative reweighted process that
alternates between estimating the sparse signal and refining the weights of the
surrogate function. Connections between the proposed algorithm and other
existing methods are discussed. Numerical results are provided to illustrate
the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4293</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4293</id><created>2012-10-16</created><authors><author><keyname>Bravo-Santos</keyname><forenames>Angel</forenames></author><author><keyname>Djuric</keyname><forenames>Petar M.</forenames></author></authors><title>Communications with decode-and-forward relays in mesh networks</title><categories>cs.IT math.IT</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider mesh networks composed of groups of relaying nodes which operate
in decode-and-forward mode, where each node from a group relays information to
all the nodes in the next group. We study these networks in two setups, one
where the nodes have complete channel state information from the nodes that
transmit to them, and another when they only have the statistics of the
channel. We derive recursive expressions for the probabilities of errors of the
nodes and present several implementations of detectors used in these networks.
We compare the mesh networks with multihop networks, the latter being formed by
a set of parallel sections of multiple relaying nodes. We demonstrate with
numerous simulations that there are significant improvements in performance of
mesh over multihop networks in various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4301</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4301</id><created>2012-10-16</created><updated>2014-01-28</updated><authors><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Singh</keyname><forenames>Y. N.</forenames></author></authors><title>Reputation Aggregation in Peer-to-Peer Network Using Differential Gossip
  Algorithm</title><categories>cs.NI cs.SI</categories><doi>10.1109/TKDE.2015.2427793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation aggregation in peer to peer networks is generally a very time and
resource consuming process. Moreover, most of the methods consider that a node
will have same reputation with all the nodes in the network, which is not true.
This paper proposes a reputation aggregation algorithm that uses a variant of
gossip algorithm called differential gossip. In this paper, estimate of
reputation is considered to be having two parts, one common component which is
same with every node, and the other one is information received from immediate
neighbours based on the neighbours' direct interaction with the node. The
differential gossip is fast and requires less amount of resources. This
mechanism allows computation of independent reputation value by a node, of
every other node in the network, for each node. The differential gossip trust
has been investigated for a power law network formed using preferential
attachment \emph{(PA)} Model. The reputation computed using differential gossip
trust shows good amount of immunity to the collusion. We have verified the
performance of the algorithm on the power law networks of different sizes
ranging from 100 nodes to 50,000 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4321</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4321</id><created>2012-10-16</created><updated>2012-10-17</updated><authors><author><keyname>Chen</keyname><forenames>Lily</forenames></author></authors><title>A quantum algorithm for solving the 3-SAT problem</title><categories>cs.CC</categories><comments>This paper is wrong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method in this paper is wrong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4327</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4327</id><created>2012-10-16</created><updated>2013-05-13</updated><authors><author><keyname>Choure</keyname><forenames>Ayush</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Sundar</forenames></author></authors><title>Improved bounds on the sandpile diffusions on Grid graphs</title><categories>math-ph cs.DM math.CO math.MP</categories><comments>This paper has been withdrawn because of typographical error in a
  proof, which lead to wrong bounds. the corrected version has been
  incorporated into the journal version of our paper, &quot;random walks, electric
  networks, and transience class problem of sandpile&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Abelian Sandpile Model is a discrete di?usion process de?ned on graphs
(Dhar [10], Dhar et al. [11]) which serves as the standard model of
self-organized criticality. The transience class of a sandpile is de?ned as the
maximum number of particles that can be added without making the system
recurrent ([3]). Using elementary combinatorial arguments and symmetry
properties, Babai and Gorodezky (SODA 2007,[2]) demonstrated a bound of O(n^30)
on the transience class of an n?xn grid. This was later improved by Choure and
Vishwanathan (SODA 2012,[7]) to O(n^7) using techniques based on harmonic
functions on graphs. We improve this bound to O(n^7 log n). We also demonstrate
tight bounds on certain resistance ratios over grid networks. The tools used
for deriving these bounds may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4329</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4329</id><created>2012-10-16</created><authors><author><keyname>Boussemart</keyname><forenames>Vincent</forenames></author><author><keyname>Marini</keyname><forenames>Loris</forenames></author><author><keyname>Berioli</keyname><forenames>Matteo</forenames></author></authors><title>Impact of Scheduling in the Return-Link of Multi-Beam Satellite MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted and accepted to IEEE GLOBECOM 2012 Conference, 6 pages, 10
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The utilization of universal frequency reuse in multi-beam satellite systems
introduces a non-negligible level of co-channel interference (CCI), which in
turn penalizes the quality of service experienced by users. Taking this as
starting point, the paper focuses on resource management performed by the
gateway (hub) on the return-link, with particular emphasis on a scheduling
algorithm based on bipartite graph approach. The study gives important insights
into the achievable per-user rate and the role played by the number of users
and spot beams considered for scheduling. More interestingly, it is shown that
a free-slot assignment strategy helps to exploit the available satellite
resources, thus guaranteeing a max-min rate requirement to users. Remarks about
the trade-off between efficiency-loss and performance increase are finally
drawn at the end of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4347</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4347</id><created>2012-10-16</created><authors><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author></authors><title>Hilbert Space Embedding for Dirichlet Process Mixtures</title><categories>stat.ML cs.LG</categories><comments>NIPS 2012 Workshop in confluence between kernel methods and graphical
  models</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Hilbert space embedding for Dirichlet Process mixture
models via a stick-breaking construction of Sethuraman. Although Bayesian
nonparametrics offers a powerful approach to construct a prior that avoids the
need to specify the model size/complexity explicitly, an exact inference is
often intractable. On the other hand, frequentist approaches such as kernel
machines, which suffer from the model selection/comparison problems, often
benefit from efficient learning algorithms. This paper discusses the
possibility to combine the best of both worlds by using the Dirichlet Process
mixture model as a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4352</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4352</id><created>2012-10-16</created><updated>2014-01-28</updated><authors><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author></authors><title>The Recognition of Simple-Triangle Graphs and of Linear-Interval Orders
  is Polynomial</title><categories>cs.DS cs.DM math.CO</categories><comments>27 pages, 4 figures, 5 algorithms</comments><acm-class>G.2.2; F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intersection graphs of geometric objects have been extensively studied, both
due to their interesting structure and their numerous applications; prominent
examples include interval graphs and permutation graphs. In this paper we study
a natural graph class that generalizes both interval and permutation graphs,
namely \emph{simple-triangle} graphs. Simple-triangle graphs - also known as
\emph{PI} graphs (for Point-Interval) - are the intersection graphs of
triangles that are defined by a point on a line $L_{1}$ and an interval on a
parallel line $L_{2}$. They lie naturally between permutation and trapezoid
graphs, which are the intersection graphs of line segments between $L_{1}$ and
$L_{2}$ and of trapezoids between $L_{1}$ and $L_{2}$, respectively. Although
various efficient recognition algorithms for permutation and trapezoid graphs
are well known to exist, the recognition of simple-triangle graphs has remained
an open problem since their introduction by Corneil and Kamula three decades
ago. In this paper we resolve this problem by proving that simple-triangle
graphs can be recognized in polynomial time. As a consequence, our algorithm
also solves a longstanding open problem in the area of partial orders, namely
the recognition of \emph{linear-interval orders}, i.e. of partial orders
$P=P_{1}\cap P_{2}$, where $P_{1}$ is a linear order and $P_{2}$ is an interval
order. This is one of the first results on recognizing partial orders $P$ that
are the intersection of orders from two different classes $\mathcal{P}_{1}$ and
$\mathcal{P}_{2}$. In complete contrast to this, partial orders $P$ which are
the intersection of orders from the same class $\mathcal{P}$ have been
extensively investigated, and in most cases the complexity status of these
recognition problems has been already established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4353</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4353</id><created>2012-10-16</created><authors><author><keyname>Speelman</keyname><forenames>Florian</forenames></author></authors><title>Position-Based Quantum Cryptography and the Garden-Hose Game</title><categories>quant-ph cs.CR</categories><comments>MSc Thesis for University of Amsterdam, the Netherlands, 2011, 34
  pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study position-based cryptography in the quantum setting. We examine a
class of protocols that only require the communication of a single qubit and 2n
bits of classical information. To this end, we define a new model of
communication complexity, the garden-hose model, which enables us to prove
upper bounds on the number of EPR pairs needed to attack such schemes. This
model furthermore opens up a way to link the security of position-based quantum
cryptography to traditional complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4367</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4367</id><created>2012-10-16</created><updated>2013-04-04</updated><authors><author><keyname>Evain</keyname><forenames>Laurent</forenames></author><author><keyname>Lederer</keyname><forenames>Mathias</forenames></author><author><keyname>Roune</keyname><forenames>Bjarke Hammersholt</forenames></author></authors><title>Connect Four and Graph Decomposition</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the standard decomposition, a way of decomposing a labeled graph
into a sum of certain labeled subgraphs. We motivate this graph-theoretic
concept by relating it to Connect Four decompositions of standard sets. We
prove that all standard decompositions can be generated in polynomial time,
which implies that all Connect Four decompositions can be generated in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4377</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4377</id><created>2012-10-16</created><authors><author><keyname>Olhede</keyname><forenames>Sofia C.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Order statistics of observed network degrees</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the properties of extremes of degree sequences
calculated from network data. We introduce the notion of a normalized degree,
in order to permit a comparison of degree sequences between networks with
differing numbers of nodes. We model each normalized degree as a bounded
continuous random variable, and determine the properties of the ordered
k-maxima and minima of the normalized network degrees when they comprise a
random sample from a Beta distribution. In this setting, their means and
variances take a simplified form given by their ordering, and we discuss the
relation of these quantities to other prescribed decays such as power laws. We
verify the derived properties from simulated sets of normalized degrees, and
discuss possible extensions to more flexible classes of distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4383</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4383</id><created>2012-10-16</created><authors><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Hadjicostis</keyname><forenames>Christoforos N.</forenames></author></authors><title>Distributed Formation of Balanced and Bistochastic Weighted Diagraphs in
  Multi-Agent Systems</title><categories>cs.MA</categories><comments>18 pages, 10 figures, submitted to European Control Conference (ECC)
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consensus strategies find a variety of applications in distributed
coordination and decision making in multi-agent systems. In particular, average
consensus plays a key role in a number of applications and is closely
associated with two classes of digraphs, weight-balanced (for continuous-time
systems) and bistochastic (for discrete-time systems). A weighted digraph is
called balanced if, for each node, the sum of the weights of the edges outgoing
from that node is equal to the sum of the weights of the edges incoming to that
node. In addition, a weight-balanced digraph is bistochastic if all weights are
nonnegative and, for each node, the sum of weights of edges incoming to that
node and the sum of the weights of edges out-going from that node is unity;
this implies that the corresponding weight matrix is column and row stochastic
(i.e., doubly stochastic). We propose two distributed algorithms: one solves
the weight-balance problem and the other solves the bistochastic matrix
formation problem for a distributed system whose components (nodes) can
exchange information via interconnection links (edges) that form an arbitrary,
possibly directed, strongly connected communication topology (digraph). Both
distributed algorithms achieve their goals asymptotically and operate
iteratively by having each node adapt the (nonnegative) weights on its outgoing
edges based on the weights of its incoming links (i.e., based on purely local
information). We also provide examples to illustrate the operation,
performance, and potential advantages of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4396</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4396</id><created>2012-10-16</created><updated>2012-11-18</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Strand</keyname><forenames>&#xd8;ivind</forenames></author></authors><title>The Swedish System of Innovation: Regional Synergies in a
  Knowledge-Based Economy</title><categories>cs.CY</categories><comments>Journal of the American Society for Information Science and
  Technology (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the complete set of firm data for Sweden (N = 1,187,421; November
2011), we analyze the mutual information among the geographical, technological,
and organizational distributions in terms of synergies at regional and national
levels. Mutual information in three dimensions can become negative and thus
indicate a net export of uncertainty by a system or, in other words, synergy in
how knowledge functions are distributed over the carriers. Aggregation at the
regional level (NUTS3) of the data organized at the municipal level (NUTS5)
shows that 48.5% of the regional synergy is provided by the three metropolitan
regions of Stockholm, Gothenburg, and Malm\&quot;o/Lund. Sweden can be considered as
a centralized and hierarchically organized system. Our results accord with
other statistics, but this Triple Helix indicator measures synergy more
specifically and quantitatively. The analysis also provides us with validation
for using this measure in previous studies of more regionalized systems of
innovation (such as Hungary and Norway).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4400</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4400</id><created>2012-10-16</created><authors><author><keyname>Carver</keyname><forenames>Hywel B.</forenames></author><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Nash</keyname><forenames>Rupert W.</forenames></author><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Coalesced communication: a design pattern for complex parallel
  scientific software</title><categories>cs.DC cs.SE</categories><comments>Submitted to Parallel Computing, 7 pages, 2 figures</comments><msc-class>68N19, 97P50</msc-class><acm-class>D.1.3; D.1.5; D.2.11; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new design pattern for high-performance parallel scientific
software, named coalesced communication. This pattern allows for a structured
way to improve the communication performance through coalescence of multiple
communication needs using two communication management components. We apply the
design pattern to several simulations of a lattice-Boltzmann blood flow solver
with streaming visualisation which engenders a reduction in the communication
overhead of approximately 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4405</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4405</id><created>2012-10-05</created><updated>2012-10-24</updated><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Depraetere</keyname><forenames>Kristof</forenames></author><author><keyname>De Roo</keyname><forenames>Jos</forenames></author><author><keyname>De Vloed</keyname><forenames>Boris</forenames></author><author><keyname>Mels</keyname><forenames>Giovanni</forenames></author><author><keyname>Colaert</keyname><forenames>Dirk</forenames></author></authors><title>Semantic integration and analysis of clinical data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing need to semantically process and integrate clinical data
from different sources for Clinical Data Management and Clinical Decision
Support in the healthcare IT industry. In the clinical practice domain, the
semantic gap between clinical information systems and domain ontologies is
quite often difficult to bridge in one step. In this paper, we report our
experience in using a two-step formalization approach to formalize clinical
data, i.e. from database schemas to local formalisms and from local formalisms
to domain (unifying) formalisms. We use N3 rules to explicitly and formally
state the mapping from local ontologies to domain ontologies. The resulting
data expressed in domain formalisms can be integrated and analyzed, though
originating from very distinct sources. Practices of applying the two-step
approach in the infectious disorders and cancer domains are introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4408</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4408</id><created>2012-10-16</created><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>On efficient constructions of short lists containing mostly Ramsey
  graphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the earliest and best-known application of the probabilistic method is
the proof of existence of a 2 log n$-Ramsey graph, i.e., a graph with n nodes
that contains no clique or independent set of size 2 log n. The explicit
construction of such a graph is a major open problem. We show that a reasonable
hardness assumption implies that in polynomial time one can construct a list
containing polylog(n) graphs such that most of them are 2 log n-Ramsey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4416</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4416</id><created>2012-10-16</created><authors><author><keyname>Marro</keyname><forenames>Giovanni</forenames></author></authors><title>A Direct Proof of a Theorem Concerning Singular Hamiltonian Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report presents a direct proof of Theorem~1 in [1] and some
consequences that also account for (20) in [1]. This direct proof exploits a
state space change of basis which replaces the coupled difference equations
(10) in [1] with two equivalent difference equations which, instead, are
decoupled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4446</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4446</id><created>2012-10-16</created><authors><author><keyname>Asgeirsson</keyname><forenames>Eyjolfur I.</forenames></author><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author></authors><title>Wireless Network Stability in the SINR Model</title><categories>cs.NI cs.DC cs.DS</categories><comments>10 pages, appeared in SIROCCO'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stability of wireless networks under stochastic arrival
processes of packets, and design efficient, distributed algorithms that achieve
stability in the SINR (Signal to Interference and Noise Ratio) interference
model.
  Specifically, we make the following contributions. We give a distributed
algorithm that achieves $\Omega(\frac{1}{\log^2 n})$-efficiency on all networks
(where $n$ is the number of links in the network), for all length monotone,
sub-linear power assignments. For the power control version of the problem, we
give a distributed algorithm with $\Omega(\frac{1}{\log n(\log n + \log \log
\Delta)})$-efficiency (where $\Delta$ is the length diversity of the link set).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4451</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4451</id><created>2012-10-16</created><updated>2015-03-03</updated><authors><author><keyname>Chuangpishit</keyname><forenames>Huda</forenames></author><author><keyname>Ghandehari</keyname><forenames>Mahya</forenames></author><author><keyname>Hurshman</keyname><forenames>Matt</forenames></author><author><keyname>Janssen</keyname><forenames>Jeannette</forenames></author><author><keyname>Kalyaniwalla</keyname><forenames>Nauzer</forenames></author></authors><title>Linear embeddings of graphs and graph limits</title><categories>math.CO cs.DM</categories><comments>In press</comments><msc-class>Primary 46L07, 47B47</msc-class><journal-ref>J. Combin. Th. B 113, July 2015, pp.162-184</journal-ref><doi>10.1016/j.jctb.2015.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random graph process where vertices are chosen from the interval
$[0,1]$, and edges are chosen independently at random, but so that, for a given
vertex $x$, the probability that there is an edge to a vertex $y$ decreases as
the distance between $x$ and $y$ increases. We call this a random graph with a
linear embedding. We define a new graph parameter $\Gamma^*$, which aims to
measure the similarity of the graph to an instance of a random graph with a
linear embedding. For a graph $G$, $\Gamma^*(G)=0$ if and only if $G$ is a unit
interval graph, and thus a deterministic example of a graph with a linear
embedding. We show that the behaviour of $\Gamma^*$ is consistent with the
notion of convergence as defined in the theory of dense graph limits. In this
theory, graph sequences converge to a symmetric, measurable function on
$[0,1]^2$. We define an operator $\Gamma$ which applies to graph limits, and
which assumes the value zero precisely for graph limits that have a linear
embedding. We show that, if a graph sequence $\{ G_n\}$ converges to a function
$w$, then $\{ \Gamma^*(G_n)\}$ converges as well. Moreover, there exists a
function $w^*$ arbitrarily close to $w$ under the box distance, so that
$\lim_{n\rightarrow \infty}\Gamma^*(G_n)$ is arbitrarily close to $\Gamma
(w^*)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4459</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4459</id><created>2012-10-16</created><updated>2013-06-25</updated><authors><author><keyname>Lindblom</keyname><forenames>Johannes</forenames></author><author><keyname>Karipidis</keyname><forenames>Eleftherios</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Efficient Computation of Pareto Optimal Beamforming Vectors for the MISO
  Interference Channel with Successive Interference Cancellation</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2271748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the two-user multiple-input single-output (MISO) Gaussian
interference channel where the transmitters have perfect channel state
information and employ single-stream beamforming. The receivers are capable of
performing successive interference cancellation, so when the interfering signal
is strong enough, it can be decoded, treating the desired signal as noise, and
subtracted from the received signal, before the desired signal is decoded. We
propose efficient methods to compute the Pareto-optimal rate points and
corresponding beamforming vector pairs, by maximizing the rate of one link
given the rate of the other link. We do so by splitting the original problem
into four subproblems corresponding to the combinations of the receivers'
decoding strategies - either decode the interference or treat it as additive
noise. We utilize recently proposed parameterizations of the optimal
beamforming vectors to equivalently reformulate each subproblem as a
quasi-concave problem, which we solve very efficiently either analytically or
via scalar numerical optimization. The computational complexity of the proposed
methods is several orders-of-magnitude less than the complexity of the
state-of-the-art methods. We use the proposed methods to illustrate the effect
of the strength and spatial correlation of the channels on the shape of the
rate region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4460</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4460</id><created>2012-10-16</created><updated>2014-05-11</updated><authors><author><keyname>Aksu</keyname><forenames>Yaman</forenames></author></authors><title>Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin,
  Soft-Margin</title><categories>stat.ML cs.LG</categories><comments>Incomplete but good, again. To Apr 28 version, made few misc text and
  notation improvements including typo corrections, probably mostly in
  Appendix, but probably best to read in whole again. New results for one of
  the datasets (Leukemia gene dataset)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Margin maximization in the hard-margin sense, proposed as feature elimination
criterion by the MFE-LO method, is combined here with data radius utilization
to further aim to lower generalization error, as several published bounds and
bound-related formulations pertaining to lowering misclassification risk (or
error) pertain to radius e.g. product of squared radius and weight vector
squared norm. Additionally, we propose additional novel feature elimination
criteria that, while instead being in the soft-margin sense, too can utilize
data radius, utilizing previously published bound-related formulations for
approaching radius for the soft-margin sense, whereby e.g. a focus was on the
principle stated therein as &quot;finding a bound whose minima are in a region with
small leave-one-out values may be more important than its tightness&quot;. These
additional criteria we propose combine radius utilization with a novel and
computationally low-cost soft-margin light classifier retraining approach we
devise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. We
correct an error in the MFE-LO description, find MFE-LO achieves the highest
generalization accuracy among the previously published margin-based feature
elimination (MFE) methods, discuss some limitations of MFE-LO, and find our
novel methods herein outperform MFE-LO, attain lower test set classification
error rate. On several datasets that each both have a large number of features
and fall into the `large features few samples' dataset category, and on
datasets with lower (low-to-intermediate) number of features, our novel methods
give promising results. Especially, among our methods the tunable ones, that do
not employ (the non-tunable) LO approach, can be tuned more aggressively in the
future than herein, to aim to demonstrate for them even higher performance than
herein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4469</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4469</id><created>2012-10-16</created><authors><author><keyname>Nu&#xf1;ez</keyname><forenames>F.</forenames></author><author><keyname>Ravello</keyname><forenames>C.</forenames></author><author><keyname>Urbina</keyname><forenames>H.</forenames></author><author><keyname>Perez-Acle</keyname><forenames>T.</forenames></author></authors><title>A Rule-based Model of a Hypothetical Zombie Outbreak: Insights on the
  role of emotional factors during behavioral adaptation of an artificial
  population</title><categories>q-bio.PE cs.MA cs.SI physics.soc-ph</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models of infectious diseases have been developed since the first half of the
twentieth century. Most models haven't considered the role that emotional
factors of the individual may play on the population's behavioral adaptation
during the spread of a pandemic disease. Considering that local interactions
among individuals generate patterns that -at a large scale- govern the action
of masses, we have studied the behavioral adaptation of a population induced by
the spread of an infectious disease. Therefore, we have developed a rule-based
model of a hypothetical zombie outbreak, written in Kappa language, and
simulated using Guillespie's stochastic approach. Our study addresses the
specificity and heterogeneity of the system at the individual level, a highly
desirable characteristic, mostly overlooked in classic epidemic models.
Together with the basic elements of a typical epidemiological model, our model
includes an individual representation of the disease progression and the
traveling of agents among cities being affected. It also introduces an
approximation to measure the effect of panic in the population as a function of
the individual situational awareness. In addition, the effect of two possible
countermeasures to overcome the zombie threat is considered: the availability
of medical treatment and the deployment of special armed forces. However, due
to the special characteristics of this hypothetical infectious disease, even
using exaggerated numbers of countermeasures, only a small percentage of the
population can be saved at the end of the simulations. As expected from a
rule-based model approach, the global dynamics of our model resulted primarily
governed by the mechanistic description of local interactions occurring at the
individual level. As a whole, people's situational awareness resulted essential
to modulate the inner dynamics of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4481</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4481</id><created>2012-10-08</created><authors><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Chu</keyname><forenames>Xinqi</forenames></author><author><keyname>Ng</keyname><forenames>Tian-Tsong</forenames></author><author><keyname>Chia</keyname><forenames>Alex Yong-Sang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Epitome for Automatic Image Colorization</title><categories>cs.CV cs.LG cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image colorization adds color to grayscale images. It not only increases the
visual appeal of grayscale images, but also enriches the information contained
in scientific images that lack color information. Most existing methods of
colorization require laborious user interaction for scribbles or image
segmentation. To eliminate the need for human labor, we develop an automatic
image colorization method using epitome. Built upon a generative graphical
model, epitome is a condensed image appearance and shape model which also
proves to be an effective summary of color information for the colorization
task. We train the epitome from the reference images and perform inference in
the epitome to colorize grayscale images, rendering better colorization results
than previous method in our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4482</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4482</id><created>2012-10-16</created><updated>2014-05-16</updated><authors><author><keyname>Chou</keyname><forenames>Remi A.</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>Separation of Reliability and Secrecy in Rate-Limited Secret-Key
  Generation</title><categories>cs.IT math.IT</categories><comments>18 pages, two-column, 9 figures, accepted to IEEE Transactions on
  Information Theory; corrected typos; updated references; minor change in
  title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a discrete or a continuous source model, we study the problem of
secret-key generation with one round of rate-limited public communication
between two legitimate users. Although we do not provide new bounds on the
wiretap secret-key (WSK) capacity for the discrete source model, we use an
alternative achievability scheme that may be useful for practical applications.
As a side result, we conveniently extend known bounds to the case of a
continuous source model. Specifically, we consider a sequential key-generation
strategy, that implements a rate-limited reconciliation step to handle
reliability, followed by a privacy amplification step performed with extractors
to handle secrecy. We prove that such a sequential strategy achieves the best
known bounds for the rate-limited WSK capacity (under the assumption of
degraded sources in the case of two-way communication). However, we show that,
unlike the case of rate-unlimited public communication, achieving the
reconciliation capacity in a sequential strategy does not necessarily lead to
achieving the best known bounds for the WSK capacity. Consequently, reliability
and secrecy can be treated successively but not independently, thereby
exhibiting a limitation of sequential strategies for rate-limited public
communication. Nevertheless, we provide scenarios for which reliability and
secrecy can be treated successively and independently, such as the two-way
rate-limited SK capacity, the one-way rate-limited WSK capacity for degraded
binary symmetric sources, and the one-way rate-limited WSK capacity for
Gaussian degraded sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4502</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4502</id><created>2012-10-10</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Pascan</keyname><forenames>Cristian</forenames></author><author><keyname>Hajdu-Macelaru</keyname><forenames>Mara</forenames></author></authors><title>Comparing several heuristics for a packing problem</title><categories>cs.NE</categories><comments>5 figures, 2 tables; accepted: International Journal of Advanced
  Intelligence Paradigms</comments><journal-ref>Int J Advanced Intelligence Paradigms 4(3/4) (2012) 268-277</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packing problems are in general NP-hard, even for simple cases. Since now
there are no highly efficient algorithms available for solving packing
problems. The two-dimensional bin packing problem is about packing all given
rectangular items, into a minimum size rectangular bin, without overlapping.
The restriction is that the items cannot be rotated. The current paper is
comparing a greedy algorithm with a hybrid genetic algorithm in order to see
which technique is better for the given problem. The algorithms are tested on
different sizes data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4505</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4505</id><created>2012-10-16</created><authors><author><keyname>Ramos</keyname><forenames>Alberto Gil C. P.</forenames><affiliation>Cambridge Centre for Analysis, University of Cambridge</affiliation></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames><affiliation>Department of Electronic and Electrical Engineering, University College London</affiliation></author></authors><title>Coherent Fading Channels Driven by Arbitrary Inputs: Asymptotic
  Characterization of the Constrained Capacity and Related Information- and
  Estimation-Theoretic Quantities</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the characterization of the asymptotic behavior of the average
minimum mean-squared error (MMSE) and the average mutual information in scalar
and vector fading coherent channels, where the receiver knows the exact fading
channel state but the transmitter knows only the fading channel distribution,
driven by a range of inputs. We construct low-snr and -- at the heart of the
novelty of the contribution -- high-snr asymptotic expansions for the average
MMSE and the average mutual information for coherent channels subject to
Rayleigh fading, Ricean fading or Nakagami fading and driven by discrete inputs
(with finite support) or various continuous inputs. We reveal the role that the
so-called canonical MMSE in a standard additive white Gaussian noise (AWGN)
channel plays in the characterization of the asymptotic behavior of the average
MMSE and the average mutual information in a fading coherent channel. We also
reveal connections to and generalizations of the MMSE dimension. The most
relevant element that enables the construction of these non-trivial expansions
is the realization that the integral representation of the estimation- and
information- theoretic quantities can be seen as an h-transform of a kernel
with a monotonic argument: this enables the use of a novel asymptotic expansion
of integrals technique -- the Mellin transform method -- that leads immediately
to not only the high-snr but also the low-snr expansions of the average MMSE
and -- via the I-MMSE relationship -- to expansions of the average mutual
information. We conclude with applications of the results to the
characterization and optimization of the constrained capacity of a bank of
parallel independent coherent fading channels driven by arbitrary discrete
inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4507</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4507</id><created>2012-10-16</created><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Moran</keyname><forenames>William</forenames></author><author><keyname>Howard</keyname><forenames>Stephen D.</forenames></author></authors><title>Submodularity and Optimality of Fusion Rules in Balanced Binary Relay
  Trees</title><categories>cs.IT cs.MA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distributed detection problem in a balanced binary relay tree,
where the leaves of the tree are sensors generating binary messages. The root
of the tree is a fusion center that makes the overall decision. Every other
node in the tree is a fusion node that fuses two binary messages from its child
nodes into a new binary message and sends it to the parent node at the next
level. We assume that the fusion nodes at the same level use the same fusion
rule. We call a string of fusion rules used at different levels a fusion
strategy. We consider the problem of finding a fusion strategy that maximizes
the reduction in the total error probability between the sensors and the fusion
center. We formulate this problem as a deterministic dynamic program and
express the solution in terms of Bellman's equations. We introduce the notion
of stringsubmodularity and show that the reduction in the total error
probability is a stringsubmodular function. Consequentially, we show that the
greedy strategy, which only maximizes the level-wise reduction in the total
error probability, is within a factor of the optimal strategy in terms of
reduction in the total error probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4517</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4517</id><created>2012-10-16</created><authors><author><keyname>Pelechrinis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Prashant</forenames></author><author><keyname>Zhang</keyname><forenames>Ke</forenames></author></authors><title>Gaming the Game: Honeypot Venues Against Cheaters in Location-based
  Social Networks</title><categories>cs.SI cs.CR</categories><comments>Preprint - ACM HotMobile Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of location-based social networks (LBSNs) has provided the
community with an abundant source of information that can be exploited and used
in many different ways. LBSNs offer a number of conveniences to its
participants, such as - but not limited to - a list of places in the vicinity
of a user, recommendations for an area never explored before provided by other
peers, tracking of friends, monetary rewards in the form of special deals from
the venues visited as well as a cheap way of advertisement for the latter.
However, service convenience and security have followed disjoint paths in LBSNs
and users can misuse the offered features. The major threat for the service
providers is that of fake check-ins. Users can easily manipulate the
localization module of the underlying application and declare their presence in
a counterfeit location. The incentives for these behaviors can be both earning
monetary as well as virtual rewards. Therefore, while fake check-ins driven
from the former motive can cause monetary losses, those aiming in virtual
rewards are also harmful. In particular, they can significantly degrade the
services offered from the LBSN providers (such as recommendations) or third
parties using these data (e.g., urban planners). In this paper, we propose and
analyze a honeypot venue-based solution, enhanced with a challenge-response
scheme, that flags users who are generating fake spatial information. We
believe that our work will stimulate further research on this important topic
and will provide new directions with regards to possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4537</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4537</id><created>2012-10-16</created><updated>2013-06-05</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Winschel</keyname><forenames>Viktor</forenames></author></authors><title>Coalgebraic Analysis of Subgame-perfect Equilibria in Infinite Games
  without Discounting</title><categories>cs.GT</categories><doi>10.1017/S0960129515000365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel coalgebraic formulation of infinite extensive games. We
define both the game trees and the strategy profiles by possibly infinite
systems of corecursive equations. Certain strategy profiles are proved to be
subgame perfect equilibria using a novel proof principle of predicate
coinduction. We characterize all subgame perfect equilibria for the dollar
auction game. The economically interesting feature is that in order to prove
these results we do not need to rely on continuity assumptions on the payoffs
which amount to discounting the future. In particular, we prove a form of
one-deviation principle without any such assumptions. This suggests that
coalgebra supports a more adequate treatment of infinite-horizon models in game
theory and economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4539</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4539</id><created>2012-10-16</created><updated>2012-10-17</updated><authors><author><keyname>Baudin</keyname><forenames>Michael</forenames></author><author><keyname>Smith</keyname><forenames>Robert L.</forenames></author></authors><title>A Robust Complex Division in Scilab</title><categories>cs.MS cs.NA</categories><acm-class>G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most widely used algorithm for floating point complex division, known as
Smith's method, may fail more often than expected. This document presents two
improved complex division algorithms. We present a proof of the robustness of
the first improved algorithm. Numerical simulations show that this algorithm
performs well in practice and is significantly more robust than other known
implementations. By combining additionnal scaling methods with this first
algorithm, we were able to create a second algorithm, which rarely fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4567</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4567</id><created>2012-10-16</created><updated>2014-05-12</updated><authors><author><keyname>Bamman</keyname><forenames>David</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Tyler</forenames></author></authors><title>Gender identity and lexical variation in social media</title><categories>cs.CL</categories><comments>submission version</comments><journal-ref>Journal of Sociolinguistics 18 (2014) 135-160</journal-ref><doi>10.1111/josl.12080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a study of the relationship between gender, linguistic style, and
social networks, using a novel corpus of 14,000 Twitter users. Prior
quantitative work on gender often treats this social variable as a female/male
binary; we argue for a more nuanced approach. By clustering Twitter users, we
find a natural decomposition of the dataset into various styles and topical
interests. Many clusters have strong gender orientations, but their use of
linguistic resources sometimes directly conflicts with the population-level
language statistics. We view these clusters as a more accurate reflection of
the multifaceted nature of gendered language styles. Previous corpus-based work
has also had little to say about individuals whose linguistic styles defy
population-level gender patterns. To identify such individuals, we train a
statistical classifier, and measure the classifier confidence for each
individual in the dataset. Examining individuals whose language does not match
the classifier's model for their gender, we find that they have social networks
that include significantly fewer same-gender social connections and that, in
general, social network homophily is correlated with the use of same-gender
language markers. Pairing computational methods and social theory thus offers a
new perspective on how gender emerges as individuals position themselves
relative to audiences, topics, and mainstream gender norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4594</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4594</id><created>2012-10-16</created><updated>2013-08-23</updated><authors><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>A Simplification of the MV Matching Algorithm and its Proof</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For all practical purposes, the Micali-Vazirani general graph maximum
matching algorithm is still the most efficient known algorithm for the problem.
The purpose of this paper is to provide a complete proof of correctness of the
algorithm in the simplest possible terms; graph-theoretic machinery developed
for this purpose also helps simplify the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4595</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4595</id><created>2012-10-16</created><authors><author><keyname>Horn</keyname><forenames>Paul</forenames></author><author><keyname>Lippner</keyname><forenames>Gabor</forenames></author></authors><title>Two Layer 3D Floor Planning</title><categories>math.CO cs.DM</categories><comments>14 pages, 10 figures</comments><msc-class>05A16, 52C45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 3D floor plan is a non-overlapping arrangement of blocks within a large
box. Floor planning is a central notion in chip-design, and with recent
advances in 3D integrated circuits, understanding 3D floor plans has become
important. In this paper, we study so called mosaic 3D floor plans where the
interior blocks partition the host box under a topological equivalence. We give
representations which give an upper bound on the number of general 3D floor
plans, and further consider the number of two layer mosaic floorplans. We prove
that the number of two layer mosaic floor plans is $n^{(1+o(1))n/3}$. This
contrasts with previous work which has studied `corner free' mosaic floor
plans, where the number is just exponential. The upper bound is by giving a
representation, while the lower bound is a randomized construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4596</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4596</id><created>2012-10-16</created><updated>2012-11-27</updated><authors><author><keyname>Bandemer</keyname><forenames>Bernd</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Optimal Achievable Rates for Interference Networks with Random Codes</title><categories>cs.IT math.IT</categories><comments>28 pages, 10 figures; extended version with complete proofs,
  submitted for publication to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal rate region for interference networks is characterized when
encoding is restricted to random code ensembles with superposition coding and
time sharing. A simple simultaneous nonunique decoding rule, under which each
receiver decodes for the intended message as well as the interfering messages,
is shown to achieve this optimal rate region regardless of the relative
strengths of signal, interference, and noise. This result implies that the
Han-Kobayashi bound, the best known inner bound on the capacity region of the
two-user-pair interference channel, cannot be improved merely by using the
optimal maximum likelihood decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4601</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4601</id><created>2012-10-16</created><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>A Direct Approach to Multi-class Boosting and Extensions</title><categories>cs.LG</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting methods combine a set of moderately accurate weaklearners to form a
highly accurate predictor. Despite the practical importance of multi-class
boosting, it has received far less attention than its binary counterpart. In
this work, we propose a fully-corrective multi-class boosting formulation which
directly solves the multi-class problem without dividing it into multiple
binary classification problems. In contrast, most previous multi-class boosting
algorithms decompose a multi-boost problem into multiple binary boosting
problems. By explicitly deriving the Lagrange dual of the primal optimization
problem, we are able to construct a column generation-based fully-corrective
approach to boosting which directly optimizes multi-class classification
performance. The new approach not only updates all weak learners' coefficients
at every iteration, but does so in a manner flexible enough to accommodate
various loss functions and regularizations. For example, it enables us to
introduce structural sparsity through mixed-norm regularization to promote
group sparsity and feature sharing. Boosting with shared features is
particularly beneficial in complex prediction problems where features can be
expensive to compute. Our experiments on various data sets demonstrate that our
direct multi-class boosting generalizes as well as, or better than, a range of
competing multi-class boosting methods. The end result is a highly effective
and compact ensemble classifier which can be trained in a distributed fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4614</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4614</id><created>2012-10-10</created><updated>2012-11-21</updated><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Random Sequences Based on the Divisor Pairs Function</title><categories>cs.CR cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the randomness properties of a function of the
divisor pairs of a natural number. This function, the antecedents of which go
to very ancient times, has randomness properties that can find applications in
cryptography, key distribution, and other problems of computer science. It is
shown that the function is aperiodic and it has excellent autocorrelation
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4640</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4640</id><created>2012-10-17</created><authors><author><keyname>Maurer</keyname><forenames>Alexandre</forenames><affiliation>LIP6, LINCS</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>A Scalable Byzantine Grid</title><categories>cs.DC cs.CR cs.NI</categories><comments>17 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern networks assemble an ever growing number of nodes. However, it remains
difficult to increase the number of channels per node, thus the maximal degree
of the network may be bounded. This is typically the case in grid topology
networks, where each node has at most four neighbors. In this paper, we address
the following issue: if each node is likely to fail in an unpredictable manner,
how can we preserve some global reliability guarantees when the number of nodes
keeps increasing unboundedly ? To be more specific, we consider the problem or
reliably broadcasting information on an asynchronous grid in the presence of
Byzantine failures -- that is, some nodes may have an arbitrary and potentially
malicious behavior. Our requirement is that a constant fraction of correct
nodes remain able to achieve reliable communication. Existing solutions can
only tolerate a fixed number of Byzantine failures if they adopt a worst-case
placement scheme. Besides, if we assume a constant Byzantine ratio (each node
has the same probability to be Byzantine), the probability to have a fatal
placement approaches 1 when the number of nodes increases, and reliability
guarantees collapse. In this paper, we propose the first broadcast protocol
that overcomes these difficulties. First, the number of Byzantine failures that
can be tolerated (if they adopt the worst-case placement) now increases with
the number of nodes. Second, we are able to tolerate a constant Byzantine
ratio, however large the grid may be. In other words, the grid becomes
scalable. This result has important security applications in ultra-large
networks, where each node has a given probability to misbehave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4643</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4643</id><created>2012-10-17</created><updated>2013-04-19</updated><authors><author><keyname>Sato</keyname><forenames>Aki-Hiro</forenames></author></authors><title>Econoinformatics meets Data-Centric Social Sciences</title><categories>q-fin.GN cs.SI physics.soc-ph</categories><comments>22 pages, 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our society has been computerised and globalised due to emergence and spread
of information and communication technology (ICT). This enables us to
investigate our own socio-economic systems based on large amounts of data on
human activities. In this article, methods of treating complexity arising from
a vast amount of data, and linking data from different sources, are discussed.
Furthermore, several examples are given of studies into the applications of
econoinformatics for the Japanese stock exchange, foreign exchange markets,
domestic hotel booking data and international flight booking data are shown. It
is the main message that spatio-temporal information is a key element to
synthesise data from different data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4644</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4644</id><created>2012-10-17</created><authors><author><keyname>Singh</keyname><forenames>Yashpal</forenames></author><author><keyname>Gulati</keyname><forenames>Kapil</forenames></author><author><keyname>Niranjan</keyname><forenames>S.</forenames></author></authors><title>Dimensions and issues of mobile agent technology</title><categories>cs.OH</categories><comments>11 pages, 6 figure</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.5, 2012, 51-61</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Agent is a type of software system which acts &quot;intelligently&quot; on one's
behalf with the feature of autonomy, learning ability and most importantly
mobility. Now mobile agents are gaining interest in the research community. In
this article mobile agents will be addressed as tools for mobile computing.
Mobile agents have been used in applications ranging from network management to
information management. We present mobile agent concept, characteristics,
classification, need, applications and technical constraints in the mobile
technology. We also provide a brief case study about how mobile agent is used
for information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4657</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4657</id><created>2012-10-17</created><authors><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author><author><keyname>Vilanova</keyname><forenames>Pedro</forenames></author></authors><title>Mean-Field Learning: a Survey</title><categories>cs.LG cs.GT cs.MA math.DS stat.ML</categories><comments>36 pages. 5 figures. survey style</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study iterative procedures for stationary equilibria in
games with large number of players. Most of learning algorithms for games with
continuous action spaces are limited to strict contraction best reply maps in
which the Banach-Picard iteration converges with geometrical convergence rate.
When the best reply map is not a contraction, Ishikawa-based learning is
proposed. The algorithm is shown to behave well for Lipschitz continuous and
pseudo-contractive maps. However, the convergence rate is still unsatisfactory.
Several acceleration techniques are presented. We explain how cognitive users
can improve the convergence rate based only on few number of measurements. The
methodology provides nice properties in mean field games where the payoff
function depends only on own-action and the mean of the mean-field (first
moment mean-field games). A learning framework that exploits the structure of
such games, called, mean-field learning, is proposed. The proposed mean-field
learning framework is suitable not only for games but also for non-convex
global optimization problems. Then, we introduce mean-field learning without
feedback and examine the convergence to equilibria in beauty contest games,
which have interesting applications in financial markets. Finally, we provide a
fully distributed mean-field learning and its speedup versions for satisfactory
solution in wireless networks. We illustrate the convergence rate improvement
with numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4661</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4661</id><created>2012-10-17</created><authors><author><keyname>Oliveira</keyname><forenames>Jose N.</forenames></author></authors><title>Functions as types or the &quot;Hoare logic&quot; of functional dependencies</title><categories>cs.LO</categories><acm-class>D.2.4; H.1.1; H.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the trend on unifying theories of programming, this paper shows
how the algebraic treatment of standard data dependency theory equips
relational data with functional types and an associated type system which is
useful for type checking database operations and for query optimization.
  Such a typed approach to database programming is then shown to be of the same
family as other programming logics such as eg. Hoare logic or that of strongest
invariant functions which has been used in the analysis of while statements.
  The prospect of using automated deduction systems such as Prover9 for
type-checking and query optimization on top of such an algebraic approach is
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4662</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4662</id><created>2012-10-17</created><authors><author><keyname>Karawia</keyname><forenames>A. A.</forenames></author></authors><title>A New Recursive Algorithm For Inverting A General Comrade Matrix</title><categories>cs.SC cs.MS</categories><msc-class>15A15, 15A23, 68W30, 11Y05, 33F10, F.2.1, G.1.0</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the author present a reliable symbolic computational algorithm
for inverting a general comrade matrix by using parallel computing along with
recursion. The computational cost of our algorithm is O(n^2). The algorithm is
implementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and
MATHEMATICA. Three examples are presented for the sake of illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4663</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4663</id><created>2012-10-17</created><updated>2013-10-22</updated><authors><author><keyname>Wang</keyname><forenames>Zhi Jie</forenames></author><author><keyname>Wang</keyname><forenames>Dong-Hua</forenames></author><author><keyname>Yao</keyname><forenames>Bin</forenames></author></authors><title>Probabilistic Range Query over Uncertain Moving Objects in Constrained
  Two-dimensional Space</title><categories>cs.DB cs.CG cs.DS</categories><comments>14 pages</comments><acm-class>H.2.8; H.3.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic range query (PRQ) over uncertain moving objects has attracted
much attentions in recent years. Most of existing works focus on the PRQ for
objects moving freely in two-dimensional (2D) space. In contrast, this paper
studies the PRQ over objects moving {in a constrained 2D space} where objects
are forbidden to be located in some specific areas. We dub it the constrained
space probabilistic range query (CSPRQ). We analyse its unique properties and
show that to process the CSPRQ using a straightforward solution is infeasible.
The key idea of our solution is to use a strategy called
\textit{pre-approximation} that can reduce the initial problem to a highly
simplified version, implying that it makes the rest of steps easy to tackle. In
particular, this strategy itself is pretty simple and easy to implement.
Furthermore, motivated by the cost analysis, we further optimize our solution.
The optimizations are mainly based on two insights: (\romannumeral 1) the
number of \textit{effective subdivision}s is no more than 1; and (\romannumeral
2) an entity with the larger \textit{span} is more likely to subdivide a single
region. We demonstrate the effectiveness and efficiency of our proposed
approaches through extensive experiments under various experimental settings,
and highlight an extra finding, i.e., the precomputation based approach suffers
a non-trivial preprocessing time, which offers an important indication sign for
the future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4673</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4673</id><created>2012-10-17</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author></authors><title>Approximation algorithms for energy, reliability and makespan
  optimization problems</title><categories>cs.DS</categories><comments>Work supported by ANR RESCUE</comments><report-no>INRIA Research Report 8107</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of scheduling an application on a
parallel computational platform. The application is a particular task graph,
either a linear chain of tasks, or a set of independent tasks. The platform is
made of identical processors, whose speed can be dynamically modified. It is
also subject to failures: if a processor is slowed down to decrease the energy
consumption, it has a higher chance to fail. Therefore, the scheduling problem
requires to re-execute or replicate tasks (i.e., execute twice a same task,
either on the same processor, or on two distinct processors), in order to
increase the reliability. It is a tri-criteria problem: the goal is to minimize
the energy consumption, while enforcing a bound on the total execution time
(the makespan), and a constraint on the reliability of each task.
  Our main contribution is to propose approximation algorithms for these
particular classes of task graphs. For linear chains, we design a fully
polynomial time approximation scheme. However, we show that there exists no
constant factor approximation algorithm for independent tasks, unless P=NP, and
we are able in this case to propose an approximation algorithm with a
relaxation on the makespan constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4686</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4686</id><created>2012-10-17</created><authors><author><keyname>Arlt</keyname><forenames>Stephan</forenames></author><author><keyname>Ermis</keyname><forenames>Evren</forenames></author><author><keyname>Feo-Arenis</keyname><forenames>Sergio</forenames></author><author><keyname>Podelski</keyname><forenames>Andreas</forenames></author></authors><title>Black-Box Verification for GUI Applications</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In black-box testing of GUI applications (a form of system testing), a
dynamic analysis of the GUI application is used to infer a black-box model; the
black-box model is then used to derive test cases for the test of the GUI
application. In this paper, we propose to supplement the test with the
verification of the black-box model. We present a method that can give a
guarantee of the absence of faults, i.e., the correctness of all test cases of
the black-box model. The black-model allows us to formulate a parametrized
verification problem. As we will show, it also allows us to circumvent the
static analysis of the GUI tool kit. We have implemented our approach;
preliminary experiments indicate its practical potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4690</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4690</id><created>2012-10-17</created><updated>2012-11-15</updated><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>A Primarily Survey on Energy Efficiency in Cloud and Distributed
  Computing Systems</title><categories>cs.DC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A survey of available techniques in hardware to reduce energy consumption
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4694</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4694</id><created>2012-10-17</created><updated>2014-04-05</updated><authors><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author><author><keyname>von Stengel</keyname><forenames>Bernhard</forenames></author></authors><title>Oriented Euler Complexes and Signed Perfect Matchings</title><categories>cs.DM cs.GT</categories><comments>43 pages, journal version for Mathematical Programming Series B plus
  2 Appendices</comments><msc-class>90C33</msc-class><acm-class>G.2.2</acm-class><journal-ref>Mathematical Programming Series B 150:1, 153-178 (2014)</journal-ref><doi>10.1007/s10107-014-0770-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents &quot;oriented pivoting systems&quot; as an abstract framework for
complementary pivoting. It gives a unified simple proof that the endpoints of
complementary pivoting paths have opposite sign. A special case are the Nash
equilibria of a bimatrix game at the ends of Lemke-Howson paths, which have
opposite index. For Euler complexes or &quot;oiks&quot;, an orientation is defined which
extends the known concept of oriented abstract simplicial manifolds. Ordered
&quot;room partitions&quot; for a family of oriented oiks come in pairs of opposite sign.
For an oriented oik of even dimension, this sign property holds also for
unordered room partitions. In the case of a two-dimensional oik, these are
perfect matchings of an Euler graph, with the sign as defined for Pfaffian
orientations of graphs. A near-linear time algorithm is given for the following
problem: given a graph with an Eulerian orientation with a perfect matching,
find another perfect matching of opposite sign. In contrast, the complementary
pivoting algorithm for this problem may be exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4695</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4695</id><created>2012-10-17</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Regulating the information in spikes: a useful bias</title><categories>q-bio.NC cs.IT cs.LG math.IT</categories><comments>NIPS 2012 workshop on Information in Perception and Action</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bias/variance tradeoff is fundamental to learning: increasing a model's
complexity can improve its fit on training data, but potentially worsens
performance on future samples. Remarkably, however, the human brain
effortlessly handles a wide-range of complex pattern recognition tasks. On the
basis of these conflicting observations, it has been argued that useful biases
in the form of &quot;generic mechanisms for representation&quot; must be hardwired into
cortex (Geman et al).
  This note describes a useful bias that encourages cooperative learning which
is both biologically plausible and rigorously justified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4700</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4700</id><created>2012-10-17</created><updated>2012-10-17</updated><authors><author><keyname>Santhanam</keyname><forenames>Narayana</forenames></author><author><keyname>Modha</keyname><forenames>Dharmendra</forenames></author></authors><title>Optimal Lempel-Ziv based lossy compression for memoryless data: how to
  make the right mistakes</title><categories>cs.IT math.IT</categories><comments>This file is not the final version, and will be updated for the next
  few days. (Edited 10/17)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression refers to encoding data using bits, so that the representation
uses as few bits as possible. Compression could be lossless: i.e. encoded data
can be recovered exactly from its representation) or lossy where the data is
compressed more than the lossless case, but can still be recovered to within
prespecified distortion metric. In this paper, we prove the optimality of
Codelet Parsing, a quasi-linear time algorithm for lossy compression of
sequences of bits that are independently and identically distributed (\iid) and
Hamming distortion. Codelet Parsing extends the lossless Lempel Ziv algorithm
to the lossy case---a task that has been a focus of the source coding
literature for better part of two decades now. Given \iid sequences $\x$, the
expected length of the shortest lossy representation such that $\x$ can be
reconstructed to within distortion $\dist$ is given by the rate distortion
function, $\rd$. We prove the optimality of the Codelet Parsing algorithm for
lossy compression of memoryless bit sequences. It splits the input sequence
naturally into phrases, representing each phrase by a codelet, a potentially
distorted phrase of the same length. The codelets in the lossy representation
of a length-$n$ string ${\x}$ have length roughly $(\log n)/\rd$, and like the
lossless Lempel Ziv algorithm, Codelet Parsing constructs codebooks logarithmic
in the sequence length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4710</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4710</id><created>2012-10-17</created><updated>2012-10-18</updated><authors><author><keyname>Khare</keyname><forenames>Niraj</forenames></author></authors><title>Graphs whose edge set can be partitioned into maximum matchings</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides structural characterization of simple graphs whose
edge-set can be partitioned into maximum matchings. We use Vizing's
classification of simple graphs based on edge chromatic index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4728</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4728</id><created>2012-10-17</created><updated>2014-11-05</updated><authors><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author></authors><title>Approximating Source Location and Star Survivable Network Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Source Location (SL) problems the goal is to select a mini-mum cost source
set $S \subseteq V$ such that the connectivity (or flow) $\psi(S,v)$ from $S$
to any node $v$ is at least the demand $d_v$ of $v$. In many SL problems
$\psi(S,v)=d_v$ if $v \in S$, namely, the demand of nodes selected to $S$ is
completely satisfied. In a node-connectivity variant suggested recently by
Fukunaga, every node $v$ gets a &quot;bonus&quot; $p_v \leq d_v$ if it is selected to
$S$. Fukunaga showed that for undirected graphs one can achieve ratio $O(k \ln
k)$ for his variant, where $k=\max_{v \in V}d_v$ is the maximum demand. We
improve this by achieving ratio $\min\{p^*\lnk,k\}\cdot O(\ln (k/q^*))$ for a
more general version with node capacities, where $p^*=\max_{v \in V} p_v$ is
the maximum bonus and $q^*=\min_{v \in V} q_v$ is the minimum capacity. In
particular, for the most natural case $p^*=1$ considered by Fukunaga, we
improve the ratio from $O(k \ln k)$ to $O(\ln^2k)$. We also get ratio $O(k)$
for the edge-connectivity version, for which no ratio that depends on $k$ only
was known before. To derive these results, we consider a particular case of the
Survivable Network (SN) problem when all edges of positive cost form a star. We
give ratio $O(\min\{\ln n,\ln^2 k\})$ for this variant, improving over the best
ratio known for the general case $O(k^3 \ln n)$ of Chuzhoy and Khanna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4732</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4732</id><created>2012-10-17</created><authors><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author><author><keyname>Kholosha</keyname><forenames>Alexander</forenames></author><author><keyname>Mesnager</keyname><forenames>Sihem</forenames></author></authors><title>Niho Bent Functions and Subiaco/Adelaide Hyperovals</title><categories>math.CO cs.CR cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the relation between binomial Niho bent functions discovered
by Dobbertin et al. and o-polynomials that give rise to the Subiaco and
Adelaide classes of hyperovals is found. This allows to expand the class of
bent functions that corresponds to Subiaco hyperovals, in the case when
$m\equiv 2 (\bmod 4)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4749</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4749</id><created>2012-10-17</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>An Auction Approach to Distributed Power Allocation for Multiuser
  Cooperative Networks</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a wireless network where multiple users cooperate with
each other to improve the overall network performance. Our goal is to design an
optimal distributed power allocation algorithm that enables user cooperation,
in particular, to guide each user on the decision of transmission mode
selection and relay selection. Our algorithm has the nice interpretation of an
auction mechanism with multiple auctioneers and multiple bidders. Specifically,
in our proposed framework, each user acts as both an auctioneer (seller) and a
bidder (buyer). Each auctioneer determines its trading price and allocates
power to bidders, and each bidder chooses the demand from each auctioneer. By
following the proposed distributed algorithm, each user determines how much
power to reserve for its own transmission, how much power to purchase from
other users, and how much power to contribute for relaying the signals of
others. We derive the optimal bidding and pricing strategies that maximize the
weighted sum rates of the users. Extensive simulations are carried out to
verify our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4752</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4752</id><created>2012-10-17</created><updated>2012-12-27</updated><authors><author><keyname>Sandryhaila</keyname><forenames>Aliaksei</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author></authors><title>Discrete Signal Processing on Graphs</title><categories>cs.SI physics.soc-ph</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 7, pp.
  1644-1656, 2013</journal-ref><doi>10.1109/TSP.2013.2238935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social settings, individuals interact through webs of relationships. Each
individual is a node in a complex network (or graph) of interdependencies and
generates data, lots of data. We label the data by its source, or formally
stated, we index the data by the nodes of the graph. The resulting signals
(data indexed by the nodes) are far removed from time or image signals indexed
by well ordered time samples or pixels. DSP, discrete signal processing,
provides a comprehensive, elegant, and efficient methodology to describe,
represent, transform, analyze, process, or synthesize these well ordered time
or image signals. This paper extends to signals on graphs DSP and its basic
tenets, including filters, convolution, z-transform, impulse response, spectral
representation, Fourier transform, frequency response, and illustrates DSP on
graphs by classifying blogs, linear predicting and compressing data from
irregularly located weather stations, or predicting behavior of customers of a
mobile service provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4753</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4753</id><created>2012-10-17</created><authors><author><keyname>Kashiwabara</keyname><forenames>Kenji</forenames></author><author><keyname>Sakuma</keyname><forenames>Tadashi</forenames></author></authors><title>On ideal minimally non-packing clutters</title><categories>math.CO cs.DM</categories><comments>18 pages, 3 figures</comments><msc-class>05B40</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following conjecture proposed by Cornu\'ejols, Guenin and
Margot: every ideal minimally non-packing clutter has a transversal of size 2.
For a clutter C, the tilde clutter is the set of hyperedges of C which
intersect any minimum transversal in exactly one element. We divide the
(non-)existence problem of an ideal minimally non-packing clutter D into two
steps. In the first step, we give necessary conditions for C = the tilde
clutter of D when a clutter D is an ideal minimally non-packing clutter. In the
second step, for a clutter C satisfying the conditions in the first step, we
consider whether C has an ideal minimally non-packing clutter D with C= the
tilde clutter of D. We show that the clutter of a combinatorial affine plane
satisfies the conditions in the first step. Moreover, we show that the clutter
of a combinatorial affine plane does not have any ideal minimally non-packing
clutter of blocking number at least 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4759</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4759</id><created>2012-10-17</created><updated>2014-07-12</updated><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Grosse</keyname><forenames>Andree</forenames></author></authors><title>From Unbalanced Initial Occupant Distribution to Balanced Exit Usage in
  a Simulation Model of Pedestrian Dynamics</title><categories>physics.soc-ph cs.MA</categories><comments>Preprint of contribution to proceedings of &quot;Human Behaviour in Fire&quot;
  symposium 2012</comments><journal-ref>Human Behaviour in Fire Symposium, pp. 536-540 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is tested in this contribution if and to which extend a method of a
pedestrian simulation tool that attempts to make pedestrians walk into the
direction of estimated earliest arrival can help to automatically distribute
pedestrians - who are initially distributed arbitrarily in the scenario -
equally on the various exits of the scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4778</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4778</id><created>2012-10-17</created><updated>2013-02-22</updated><authors><author><keyname>Hadjicostis</keyname><forenames>Christoforos N.</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author></authors><title>Average Consensus in the Presence of Delays and Dynamically Changing
  Directed Graph Topologies</title><categories>cs.MA cs.DC</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical approaches for asymptotic convergence to the global average in a
distributed fashion typically assume timely and reliable exchange of
information between neighboring components of a given multi-component system.
These assumptions are not necessarily valid in practical settings due to
varying delays that might affect transmissions at different times, as well as
possible changes in the underlying interconnection topology (e.g., due to
component mobility). In this work, we propose protocols to overcome these
limitations. We first consider a fixed interconnection topology (captured by a
- possibly directed - graph) and propose a discrete-time protocol that can
reach asymptotic average consensus in a distributed fashion, despite the
presence of arbitrary (but bounded) delays in the communication links. The
protocol requires that each component has knowledge of the number of its
outgoing links (i.e., the number of components to which it sends information).
We subsequently extend the protocol to also handle changes in the underlying
interconnection topology and describe a variety of rather loose conditions
under which the modified protocol allows the components to reach asymptotic
average consensus. The proposed algorithms are illustrated via examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4787</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4787</id><created>2012-10-17</created><updated>2013-02-01</updated><authors><author><keyname>Fu</keyname><forenames>Hongfei</forenames></author></authors><title>Approximating Acceptance Probabilities of CTMC-Paths on Multi-Clock
  Deterministic Timed Automata</title><categories>cs.SY cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating the probability mass of the set of
timed paths under a continuous-time Markov chain (CTMC) that are accepted by a
deterministic timed automaton (DTA). As opposed to several existing works on
this topic, we consider DTA with multiple clocks. Our key contribution is an
algorithm to approximate these probabilities using finite difference methods.
An error bound is provided which indicates the approximation error. The
stepping stones towards this result include rigorous proofs for the
measurability of the set of accepted paths and the integral-equation system
characterizing the acceptance probability, and a differential characterization
for the acceptance probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4791</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4791</id><created>2012-10-17</created><authors><author><keyname>Sauer</keyname><forenames>Roger A.</forenames></author><author><keyname>Duong</keyname><forenames>Thang X.</forenames></author><author><keyname>Corbett</keyname><forenames>Callum J.</forenames></author></authors><title>A computational formulation for constrained solid and liquid membranes
  considering isogeometric finite elements</title><categories>cs.CE physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A geometrically exact membrane formulation is presented that is based on
curvilinear coordinates and isogeometric finite elements, and is suitable for
both solid and liquid membranes. The curvilinear coordinate system is used to
describe both the theory and the finite element equations of the membrane. In
the latter case this avoids the use of local cartesian coordinates at the
element level. Consequently, no transformation of derivatives is required. The
formulation considers a split of the in-plane and out-of-plane membrane
contributions, which allows the construction of a stable formulation for liquid
membranes with constant surface tension. The proposed membrane formulation is
general, and accounts for dead and live loading, as well as enclosed volume,
area, and contact constraints. The new formulation is illustrated by several
challenging examples, considering linear and quadratic Lagrange elements, as
well as isogeometric elements based on quadratic NURBS and cubic T-splines. It
is seen that the isogeometric elements are much more accurate than standard
Lagrange elements. The gain is especially large for the liquid membrane
formulation since it depends explicitly on the surface curvature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4792</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4792</id><created>2012-10-17</created><updated>2013-03-07</updated><authors><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Quang</keyname><forenames>Minh Ha</forenames></author><author><keyname>Lozano</keyname><forenames>Aurelie C.</forenames></author></authors><title>Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear
  Multivariate Regression and Granger Causality</title><categories>stat.ML cs.LG</categories><comments>22 pages. Presentation changes; Corrections made to Theorem 2
  (section 6.2) in this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general matrix-valued multiple kernel learning framework for
high-dimensional nonlinear multivariate regression problems. This framework
allows a broad class of mixed norm regularizers, including those that induce
sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel
Hilbert Spaces. We develop a highly scalable and eigendecomposition-free
algorithm that orchestrates two inexact solvers for simultaneously learning
both the input and output components of separable matrix-valued kernels. As a
key application enabled by our framework, we show how high-dimensional causal
inference tasks can be naturally cast as sparse function estimation problems,
leading to novel nonlinear extensions of a class of Graphical Granger Causality
techniques. Our algorithmic developments and extensive empirical studies are
complemented by theoretical analyses in terms of Rademacher generalization
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4795</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4795</id><created>2012-10-17</created><authors><author><keyname>Fakoorian</keyname><forenames>S. Ali A.</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Full Rank Solutions for the MIMO Gaussian Wiretap Channel with an
  Average Power Constraint</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2013.2253774</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multiple-input multiple-output (MIMO) Gaussian wiretap
channel model, where there exists a transmitter, a legitimate receiver and an
eavesdropper, each equipped with multiple antennas. In this paper, we first
revisit the rank property of the optimal input covariance matrix that achieves
the secrecy capacity of the multiple antenna MIMO Gaussian wiretap channel
under the average power constraint. Next, we obtain necessary and sufficient
conditions on the MIMO wiretap channel parameters such that the optimal input
covariance matrix is full-rank, and we fully characterize the resulting
covariance matrix as well. Numerical results are presented to illustrate the
proposed theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4808</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4808</id><created>2012-10-17</created><authors><author><keyname>Lee</keyname><forenames>Christopher J.</forenames></author><author><keyname>Harper</keyname><forenames>Marc</forenames></author></authors><title>Basic Experiment Planning via Information Metrics: the RoboMendel
  Problem</title><categories>cs.IT math.IT</categories><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we outline some mathematical questions that emerge from trying
to &quot;turn the scientific method into math&quot;. Specifically, we consider the
problem of experiment planning (choosing the best experiment to do next) in
explicit probabilistic and information theoretic terms. We formulate this as an
information measurement problem; that is, we seek a rigorous definition of an
information metric to measure the likely information yield of an experiment,
such that maximizing the information metric will indeed reliably choose the
best experiment to perform. We present the surprising result that defining the
metric purely in terms of prediction power on observable variables yields a
metric that can converge to the classical mutual information measuring how
informative the experimental observation is about an underlying hidden
variable. We show how the expectation potential information metric can compute
the &quot;information rate&quot; of an experiment as well its total possible yield, and
the information value of experimental controls. To illustrate the utility of
these concepts for guiding fundamental scientific inquiry, we present an
extensive case study (RoboMendel) applying these metrics to propose sequences
of experiments for discovering the basic principles of genetics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4811</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4811</id><created>2012-10-17</created><authors><author><keyname>&#x141;&#x105;cki</keyname><forenames>Jakub</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Single Source - All Sinks Max Flows in Planar Digraphs</title><categories>cs.DM cs.DS math.CO</categories><comments>25 pages, 4 figures; extended abstract appeared in FOCS 2012</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G = (V,E) be a planar n-vertex digraph. Consider the problem of computing
max st-flow values in G from a fixed source s to all sinks t in V\{s}. We show
how to solve this problem in near-linear O(n log^3 n) time. Previously, no
better solution was known than running a single-source single-sink max flow
algorithm n-1 times, giving a total time bound of O(n^2 log n) with the
algorithm of Borradaile and Klein.
  An important implication is that all-pairs max st-flow values in G can be
computed in near-quadratic time. This is close to optimal as the output size is
Theta(n^2). We give a quadratic lower bound on the number of distinct max flow
values and an Omega(n^3) lower bound for the total size of all min cut-sets.
This distinguishes the problem from the undirected case where the number of
distinct max flow values is O(n).
  Previous to our result, no algorithm which could solve the all-pairs max flow
values problem faster than the time of Theta(n^2) max-flow computations for
every planar digraph was known.
  This result is accompanied with a data structure that reports min cut-sets.
For fixed s and all t, after O(n^{3/2} log^{3/2} n) preprocessing time, it can
report the set of arcs C crossing a min st-cut in time roughly proportional to
the size of C.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4820</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4820</id><created>2012-10-17</created><updated>2015-02-16</updated><authors><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>Whole Genome Sequencing: Innovation Dream or Privacy Nightmare?</title><categories>cs.CR cs.ET q-bio.GN</categories><comments>This version is superseded by arXiv:1306.1264</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past several years, DNA sequencing has emerged as one of the driving
forces in life-sciences, paving the way for affordable and accurate whole
genome sequencing. As genomes represent the entirety of an organism's
hereditary information, the availability of complete human genomes prompts a
wide range of revolutionary applications. The hope for improving modern
healthcare and better understanding the human genome propels many interesting
and challenging research frontiers. Unfortunately, however, the proliferation
of human genomes amplifies worrisome privacy concerns, since a genome
represents a treasure trove of highly personal and sensitive information. In
this article, we provide an overview of positive results and biomedical
advances in the field, and discuss privacy issues associated with human genomic
information. Finally, we survey available privacy-enhancing technologies and
list a number of open research challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4822</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4822</id><created>2012-10-17</created><updated>2013-05-15</updated><authors><author><keyname>Kutten</keyname><forenames>Shay</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Trehan</keyname><forenames>Amitabh</forenames></author></authors><title>Sublinear Bounds for Randomized Leader Election</title><categories>cs.DS cs.DC</categories><comments>Best Paper Award winner at ICDCN 2013, CDCN 2013 14th International
  Conference on Distributed Computing and Networking. Tata Institute of
  Fundamental Research, Mumbai, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns {\em randomized} leader election in synchronous
distributed networks. A distributed leader election algorithm is presented for
complete $n$-node networks that runs in O(1) rounds and (with high probability)
uses only $O(\sqrt{n}\log^{3/2} n)$ messages to elect a unique leader (with
high probability). When considering the &quot;explicit&quot; variant of leader election
where eventually every node knows the identity of the leader, our algorithm
yields the asymptotically optimal bounds of O(1) rounds and O(n) messages. This
algorithm is then extended to one solving leader election on any connected
non-bipartite $n$-node graph $G$ in $O(\tau(G))$ time and
$O(\tau(G)\sqrt{n}\log^{3/2} n)$ messages, where $\tau(G)$ is the mixing time
of a random walk on $G$. The above result implies highly efficient (sublinear
running time and messages) leader election algorithms for networks with small
mixing times, such as expanders and hypercubes. In contrast, previous leader
election algorithms had at least linear message complexity even in complete
graphs. Moreover, super-linear message lower bounds are known for
time-efficient {\em deterministic} leader election algorithms.
  Finally, we present an almost matching lower bound for randomized leader
election, showing that $\Omega(\sqrt n)$ messages are needed for any leader
election algorithm that succeeds with probability at least $1/e + \eps$, for
any small constant $\eps &gt; 0$.
  We view our results as a step towards understanding the randomized complexity
ofleader election in distributed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4831</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4831</id><created>2012-10-17</created><authors><author><keyname>Xiang</keyname><forenames>Xingyu</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Closing the Gap to the Capacity of APSK: Constellation Shaping and
  Degree Distributions</title><categories>cs.IT math.IT</categories><comments>To appear at ICNC-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constellation shaping is an energy-efficient strategy involving the
transmission of lower-energy signals more frequently than higher-energy
signals. Previous work has shown that shaping is particularly effective when
used with coded amplitude phase-shift keying (APSK), a modulation that has been
popularized recently due to its inclusion in the DVB-S2 standard. While shaped
APSK can provide significant gains when used with standard off-the-shelf LDPC
codes, such as the codes in the DVB-S2 standard, additional non-negligible
gains can be achieved by optimizing the LDPC code with respect to the shaped
APSK modulation. In this paper, we optimize the degree distributions of the
LDPC code used in conjunction with shaped APSK. The optimization process is an
extension of the EXIT-chart technique of ten Brink, et al., which has been
adapted to account for the shaped APSK modulation. We begin by constraining the
code to have the same number of distinct variable-node degrees as the codes in
the DVB-S2 standard, and show that the optimization provides 32-APSK systems
with an additional coding gain of 0.34 dB at a system rate of R=3 bits per
symbol, compared to shaped systems that use the long LDPC code from the DVB-S2
standard. We then increase the number of allowed variable node degrees by one,
and find that an additional 0.1 dB gain is achievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4837</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4837</id><created>2012-10-16</created><authors><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Ruberry</keyname><forenames>Mike</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Designing Informative Securities</title><categories>cs.GT q-fin.TR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-185-195</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We create a formal framework for the design of informative securities in
prediction markets. These securities allow a market organizer to infer the
likelihood of events of interest as well as if he knew all of the traders'
private signals. We consider the design of markets that are always informative,
markets that are informative for a particular signal structure of the
participants, and informative markets constructed from a restricted selection
of securities. We find that to achieve informativeness, it can be necessary to
allow participants to express information that may not be directly of interest
to the market organizer, and that understanding the participants' signal
structure is important for designing informative prediction markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4838</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4838</id><created>2012-10-16</created><authors><author><keyname>Chan</keyname><forenames>Hau</forenames></author><author><keyname>Ceyko</keyname><forenames>Michael</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>Interdependent Defense Games: Modeling Interdependent Security under
  Deliberate Attacks</title><categories>cs.GT cs.CR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-152-162</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose interdependent defense (IDD) games, a computational game-theoretic
framework to study aspects of the interdependence of risk and security in
multi-agent systems under deliberate external attacks. Our model builds upon
interdependent security (IDS) games, a model due to Heal and Kunreuther that
considers the source of the risk to be the result of a fixed
randomizedstrategy. We adapt IDS games to model the attacker's deliberate
behavior. We define the attacker's pure-strategy space and utility function and
derive appropriate cost functions for the defenders. We provide a complete
characterization of mixed-strategy Nash equilibria (MSNE), and design a simple
polynomial-time algorithm for computing all of them, for an important subclass
of IDD games. In addition, we propose a randominstance generator of (general)
IDD games based on a version of the real-world Internet-derived Autonomous
Systems (AS) graph (with around 27K nodes and 100K edges), and present
promising empirical results using a simple learning heuristics to compute
(approximate) MSNE in such games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4839</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4839</id><created>2012-10-16</created><authors><author><keyname>Caron</keyname><forenames>Stephane</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author></authors><title>Leveraging Side Observations in Stochastic Bandits</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-142-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers stochastic bandits with side observations, a model that
accounts for both the exploration/exploitation dilemma and relationships
between arms. In this setting, after pulling an arm i, the decision maker also
observes the rewards for some other actions related to i. We will see that this
model is suited to content recommendation in social networks, where users'
reactions may be endorsed or not by their friends. We provide efficient
algorithms based on upper confidence bounds (UCBs) to leverage this additional
information and derive new bounds improving on standard regret guarantees. We
also evaluate these policies in the context of movie recommendation in social
networks: experiments on real datasets show substantial learning rate speedups
ranging from 2.2x to 14x on dense networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4840</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4840</id><created>2012-10-16</created><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Choi</keyname><forenames>Arthur</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Lifted Relax, Compensate and then Recover: From Approximate to Exact
  Lifted Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-131-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to lifted approximate inference for first-order
probabilistic models, such as Markov logic networks. It is based on performing
exact lifted inference in a simplified first-order model, which is found by
relaxing first-order constraints, and then compensating for the relaxation.
These simplified models can be incrementally improved by carefully recovering
constraints that have been relaxed, also at the first-order level. This leads
to a spectrum of approximations, with lifted belief propagation on one end, and
exact lifted inference on the other. We discuss how relaxation, compensation,
and recovery can be performed, all at the firstorder level, and show
empirically that our approach substantially improves on the approximations of
both propositional solvers and lifted belief propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4841</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4841</id><created>2012-10-16</created><authors><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>An Efficient Message-Passing Algorithm for the M-Best MAP Problem</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-121-130</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much effort has been directed at algorithms for obtaining the highest
probability configuration in a probabilistic random field model known as the
maximum a posteriori (MAP) inference problem. In many situations, one could
benefit from having not just a single solution, but the top M most probable
solutions known as the M-Best MAP problem. In this paper, we propose an
efficient message-passing based algorithm for solving the M-Best MAP problem.
Specifically, our algorithm solves the recently proposed Linear Programming
(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than
a generic LP-solver. Our approach relies on studying a particular partial
Lagrangian relaxation of the M-Best MAP LP which exposes a natural
combinatorial structure of the problem that we exploit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4842</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4842</id><created>2012-10-16</created><authors><author><keyname>Bareinboim</keyname><forenames>Elias</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causal Inference by Surrogate Experiments: z-Identifiability</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-113-120</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of estimating the effect of intervening on a set of
variables X from experiments on a different set, Z, that is more accessible to
manipulation. This problem, which we call z-identifiability, reduces to
ordinary identifiability when Z = empty and, like the latter, can be given
syntactic characterization using the do-calculus [Pearl, 1995; 2000]. We
provide a graphical necessary and sufficient condition for z-identifiability
for arbitrary sets X,Z, and Y (the outcomes). We further develop a complete
algorithm for computing the causal effect of X on Y using information provided
by experiments on Z. Finally, we use our results to prove completeness of
do-calculus relative to z-identifiability, a result that does not follow from
completeness relative to ordinary identifiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4843</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4843</id><created>2012-10-16</created><authors><author><keyname>Arora</keyname><forenames>Raman</forenames></author><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Deterministic MDPs with Adversarial Rewards and Bandit Feedback</title><categories>cs.GT cs.LG</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-93-101</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Markov decision process with deterministic state transition
dynamics, adversarially generated rewards that change arbitrarily from round to
round, and a bandit feedback model in which the decision maker only observes
the rewards it receives. In this setting, we present a novel and efficient
online decision making algorithm named MarcoPolo. Under mild assumptions on the
structure of the transition dynamics, we prove that MarcoPolo enjoys a regret
of O(T^(3/4)sqrt(log(T))) against the best deterministic policy in hindsight.
Specifically, our analysis does not rely on the stringent unichain assumption,
which dominates much of the previous work on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4845</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4845</id><created>2012-10-16</created><authors><author><keyname>Apsel</keyname><forenames>Udi</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author></authors><title>Exploiting Uniform Assignments in First-Order MPE</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-74-83</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MPE (Most Probable Explanation) query plays an important role in
probabilistic inference. MPE solution algorithms for probabilistic relational
models essentially adapt existing belief assessment method, replacing summation
with maximization. But the rich structure and symmetries captured by relational
models together with the properties of the maximization operator offer an
opportunity for additional simplification with potentially significant
computational ramifications. Specifically, these models often have groups of
variables that define symmetric distributions over some population of formulas.
The maximizing choice for different elements of this group is the same. If we
can realize this ahead of time, we can significantly reduce the size of the
model by eliminating a potentially significant portion of random variables.
This paper defines the notion of uniformly assigned and partially uniformly
assigned sets of variables, shows how one can recognize these sets efficiently,
and how the model can be greatly simplified once we recognize them, with little
computational effort. We demonstrate the effectiveness of these ideas
empirically on a number of models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4846</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4846</id><created>2012-10-16</created><authors><author><keyname>Amizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Thiesson</keyname><forenames>Bo</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>Variational Dual-Tree Framework for Large-Scale Transition Matrix
  Approximation</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-64-73</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, non-parametric methods utilizing random walks on graphs have
been used to solve a wide range of machine learning problems, but in their
simplest form they do not scale well due to the quadratic complexity. In this
paper, a new dual-tree based variational approach for approximating the
transition matrix and efficiently performing the random walk is proposed. The
approach exploits a connection between kernel density estimation, mixture
modeling, and random walk on graphs in an optimization of the transition matrix
for the data graph that ties together edge transitions probabilities that are
similar. Compared to the de facto standard approximation method based on
k-nearestneighbors, we demonstrate order of magnitudes speedup without
sacrificing accuracy for Label Propagation tasks on benchmark data sets in
semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4847</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4847</id><created>2012-10-16</created><authors><author><keyname>Amin</keyname><forenames>Kareem</forenames></author><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Key</keyname><forenames>Peter</forenames></author><author><keyname>Schwaighofer</keyname><forenames>Anton</forenames></author></authors><title>Budget Optimization for Sponsored Search: Censored Learning in MDPs</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-54-63</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the budget optimization problem faced by an advertiser
participating in repeated sponsored search auctions, seeking to maximize the
number of clicks attained under that budget. We cast the budget optimization
problem as a Markov Decision Process (MDP) with censored observations, and
propose a learning algorithm based on the wellknown Kaplan-Meier or
product-limit estimator. We validate the performance of this algorithm by
comparing it to several others on a large set of search auction data from
Microsoft adCenter, demonstrating fast convergence to optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4848</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4848</id><created>2012-10-16</created><authors><author><keyname>Ahmed</keyname><forenames>Asrar</forenames></author><author><keyname>Varakantham</keyname><forenames>Pradeep</forenames></author><author><keyname>Cheng</keyname><forenames>Shih-Fen</forenames></author></authors><title>Uncertain Congestion Games with Assorted Human Agent Populations</title><categories>cs.GT cs.MA</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-44-53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Congestion games model a wide variety of real-world resource congestion
problems, such as selfish network routing, traffic route guidance in congested
areas, taxi fleet optimization and crowd movement in busy areas. However,
existing research in congestion games assumes: (a) deterministic movement of
agents between resources; and (b) perfect rationality (i.e. maximizing their
own expected value) of all agents. Such assumptions are not reasonable in
dynamic domains where decision support has to be provided to humans. For
instance, in optimizing the performance of a taxi fleet serving a city,
movement of taxis can be involuntary or nondeterministic (decided by the
specific customer who hires the taxi) and more importantly, taxi drivers may
not follow advice provided by the decision support system (due to bounded
rationality of humans). To that end, we contribute: (a) a general framework for
representing congestion games under uncertainty for populations with assorted
notions of rationality. (b) a scalable approach for solving the decision
problem for perfectly rational agents which are in the mix with boundedly
rational agents; and (c) a detailed evaluation on a synthetic and realworld
data set to illustrate the usefulness of our new approach with respect to key
social welfare metrics in the context of an assorted human-agent population. An
interesting result from our experiments on a real-world taxi fleet optimization
problem is that it is better (in terms of revenue and operational efficiency)
for taxi drivers to follow perfectly rational strategies irrespective of the
percentage of drivers not following the advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4849</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4849</id><created>2012-10-16</created><authors><author><keyname>Agussurja</keyname><forenames>Lucas</forenames></author><author><keyname>Lau</keyname><forenames>Hoong Chuin</forenames></author></authors><title>Toward Large-Scale Agent Guidance in an Urban Taxi Service</title><categories>cs.MA cs.AI cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-36-43</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empty taxi cruising represents a wastage of resources in the context of urban
taxi services. In this work, we seek to minimize such wastage. An analysis of a
large trace of taxi operations reveals that the services' inefficiency is
caused by drivers' greedy cruising behavior. We model the existing system as a
continuous time Markov chain. To address the problem, we propose that each taxi
be equipped with an intelligent agent that will guide the driver when cruising
for passengers. Then, drawing from AI literature on multiagent planning, we
explore two possible ways to compute such guidance. The first formulation
assumes fully cooperative drivers. This allows us, in principle, to compute
systemwide optimal cruising policy. This is modeled as a Markov decision
process. The second formulation assumes rational drivers, seeking to maximize
their own profit. This is modeled as a stochastic congestion game, a
specialization of stochastic games. Nash equilibrium policy is proposed as the
solution to the game, where no driver has the incentive to singly deviate from
it. Empirical result shows that both formulations improve the efficiency of the
service significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4850</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4850</id><created>2012-10-16</created><authors><author><keyname>Affandi</keyname><forenames>Raja Hafiz</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author></authors><title>Markov Determinantal Point Processes</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-26-35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A determinantal point process (DPP) is a random process useful for modeling
the combinatorial problem of subset selection. In particular, DPPs encourage a
random subset Y to contain a diverse set of items selected from a base set Y.
For example, we might use a DPP to display a set of news headlines that are
relevant to a user's interests while covering a variety of topics. Suppose,
however, that we are asked to sequentially select multiple diverse sets of
items, for example, displaying new headlines day-by-day. We might want these
sets to be diverse not just individually but also through time, offering
headlines today that are unlike the ones shown yesterday. In this paper, we
construct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The
proposed M-DPP defines a stationary process that maintains DPP margins.
Crucially, the induced union process Zt = Yt u Yt-1 is also marginally
DPP-distributed. Jointly, these properties imply that the sequence of random
sets are encouraged to be diverse both at a given time step as well as across
time steps. We describe an exact, efficient sampling procedure, and a method
for incrementally learning a quality measure over items in the base set Y based
on external preferences. We apply the M-DPP to the task of sequentially
displaying diverse and relevant news articles to a user with topic preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4851</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4851</id><created>2012-10-16</created><authors><author><keyname>Acharyya</keyname><forenames>Sreangsu</forenames></author><author><keyname>Koyejo</keyname><forenames>Oluwasanmi</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>Learning to Rank With Bregman Divergences and Monotone Retargeting</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-15-25</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach for learning to rank (LETOR) based on
the notion of monotone retargeting. It involves minimizing a divergence between
all monotonic increasing transformations of the training scores and a
parameterized prediction function. The minimization is both over the
transformations as well as over the parameters. It is applied to Bregman
divergences, a large class of &quot;distance like&quot; functions that were recently
shown to be the unique class that is statistically consistent with the
normalized discounted gain (NDCG) criterion [19]. The algorithm uses
alternating projection style updates, in which one set of simultaneous
projections can be computed independent of the Bregman divergence and the other
reduces to parameter estimation of a generalized linear model. This results in
easily implemented, efficiently parallelizable algorithm for the LETOR task
that enjoys global optimum guarantees under mild conditions. We present
empirical results on benchmark datasets showing that this approach can
outperform the state of the art NDCG consistent techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4852</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4852</id><created>2012-10-16</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>The Do-Calculus Revisited</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-3-11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The do-calculus was developed in 1995 to facilitate the identification of
causal effects in non-parametric models. The completeness proofs of [Huang and
Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of
[Tian and Shpitser, 2010] have laid this identification problem to rest. Recent
explorations unveil the usefulness of the do-calculus in three additional
areas: mediation analysis [Pearl, 2012], transportability [Pearl and
Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the
task of fusing empirical results from several diverse studies, conducted on
heterogeneous populations and under different conditions, so as to synthesize
an estimate of a causal relation in some target environment, potentially
different from those under study. The talk surveys these results with emphasis
on the challenges posed by meta-synthesis. For background material, see
http://bayes.cs.ucla.edu/csl_papers.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4853</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4853</id><created>2012-10-16</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Leung</keyname><forenames>Samantha</forenames></author></authors><title>Weighted Sets of Probabilities and MinimaxWeighted Expected Regret: New
  Approaches for Representing Uncertainty and Making Decisions</title><categories>cs.GT cs.AI q-fin.TR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012). For full version of this article, see
  arXiv:1302.5681</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-336-345</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting where an agent's uncertainty is represented by a set of
probability measures, rather than a single measure. Measure-bymeasure updating
of such a set of measures upon acquiring new information is well-known to
suffer from problems; agents are not always able to learn appropriately. To
deal with these problems, we propose using weighted sets of probabilities: a
representation where each measure is associated with a weight, which denotes
its significance. We describe a natural approach to updating in such a
situation and a natural approach to determining the weights. We then show how
this representation can be used in decision-making, by modifying a standard
approach to decision making-minimizing expected regret-to obtain minimax
weighted expected regret (MWER).We provide an axiomatization that characterizes
preferences induced by MWER both in the static and dynamic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4854</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4854</id><created>2012-10-16</created><authors><author><keyname>Hajishirzi</keyname><forenames>Hannaneh</forenames></author><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author><author><keyname>Hodgins</keyname><forenames>Jessica K.</forenames></author></authors><title>Semantic Understanding of Professional Soccer Commentaries</title><categories>cs.CL cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-326-335</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to the problem of semantic parsing via
learning the correspondences between complex sentences and rich sets of events.
Our main intuition is that correct correspondences tend to occur more
frequently. Our model benefits from a discriminative notion of similarity to
learn the correspondence between sentence and an event and a ranking machinery
that scores the popularity of each correspondence. Our method can discover a
group of events (called macro-events) that best describes a sentence. We
evaluate our method on our novel dataset of professional soccer commentaries.
The empirical results show that our method significantly outperforms the
state-of-theart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4855</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4855</id><created>2012-10-16</created><authors><author><keyname>Gupta</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>Phung</keyname><forenames>Dinh Q.</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>A Slice Sampler for Restricted Hierarchical Beta Process with
  Applications to Shared Subspace Learning</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-316-325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical beta process has found interesting applications in recent years.
In this paper we present a modified hierarchical beta process prior with
applications to hierarchical modeling of multiple data sources. The novel use
of the prior over a hierarchical factor model allows factors to be shared
across different sources. We derive a slice sampler for this model, enabling
tractable inference even when the likelihood and the prior over parameters are
non-conjugate. This allows the application of the model in much wider contexts
without restrictions. We present two different data generative models a linear
GaussianGaussian model for real valued data and a linear Poisson-gamma model
for count data. Encouraging transfer learning results are shown for two real
world applications text modeling and content based image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4856</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4856</id><created>2012-10-16</created><authors><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan R</forenames></author><author><keyname>Freeman</keyname><forenames>William T.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Exploiting compositionality to explore a large space of model structures</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-306-315</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent proliferation of richly structured probabilistic models raises the
question of how to automatically determine an appropriate model for a dataset.
We investigate this question for a space of matrix decomposition models which
can express a variety of widely used models from unsupervised learning. To
enable model selection, we organize these models into a context-free grammar
which generates a wide variety of structures through the compositional
application of a few simple rules. We use our grammar to generically and
efficiently infer latent components and estimate predictive likelihood for
nearly 2500 structures using a small toolbox of reusable algorithms. Using a
greedy search over our grammar, we automatically choose the decomposition
structure from raw data by evaluating only a small fraction of all models. The
proposed method typically finds the correct structure for synthetic data and
backs off gracefully to simpler models under heavy noise. It learns sensible
structures for datasets as diverse as image patches, motion capture, 20
Questions, and U.S. Senate votes, all using exactly the same code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4857</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4857</id><created>2012-10-16</created><authors><author><keyname>Gelfand</keyname><forenames>Andrew E.</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Generalized Belief Propagation on Tree Robust Structured Region Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-296-305</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides some new guidance in the construction of region graphs
for Generalized Belief Propagation (GBP). We connect the problem of choosing
the outer regions of a LoopStructured Region Graph (SRG) to that of finding a
fundamental cycle basis of the corresponding Markov network. We also define a
new class of tree-robust Loop-SRG for which GBP on any induced (spanning) tree
of the Markov network, obtained by setting to zero the off-tree interactions,
is exact. This class of SRG is then mapped to an equivalent class of
tree-robust cycle bases on the Markov network. We show that a treerobust cycle
basis can be identified by proving that for every subset of cycles, the graph
obtained from the edges that participate in a single cycle only, is multiply
connected. Using this we identify two classes of tree-robust cycle bases:
planar cycle bases and &quot;star&quot; cycle bases. In experiments we show that
tree-robustness can be successfully exploited as a design principle to improve
the accuracy and convergence of GBP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4858</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4858</id><created>2012-10-16</created><authors><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Patrini</keyname><forenames>Giorgio</forenames></author><author><keyname>Rocco</keyname><forenames>Marco</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Combining local search techniques and path following for bimatrix games</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-286-295</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing a Nash equilibrium (NE) is a central task in computer science. An
NE is a particularly appropriate solution concept for two-agent settings
because coalitional deviations are not an issue. However, even in this case,
finding an NE is PPAD-complete. In this paper, we combine path following
algorithms with local search techniques to design new algorithms for finding
exact and approximate NEs. We show that our algorithms largely outperform the
state of the art and that almost all the known benchmark game classes are
easily solvable or approximable (except for the GAMUT CovariantGameRand class).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4859</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4859</id><created>2012-10-16</created><authors><author><keyname>Garg</keyname><forenames>Dinesh</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourangshu</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author></authors><title>Mechanism Design for Cost Optimal PAC Learning in the Presence of
  Strategic Noisy Annotators</title><categories>cs.LG cs.GT stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-275-285</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Probably Approximate Correct (PAC) learning of a
binary classifier from noisy labeled examples acquired from multiple annotators
(each characterized by a respective classification noise rate). First, we
consider the complete information scenario, where the learner knows the noise
rates of all the annotators. For this scenario, we derive sample complexity
bound for the Minimum Disagreement Algorithm (MDA) on the number of labeled
examples to be obtained from each annotator. Next, we consider the incomplete
information scenario, where each annotator is strategic and holds the
respective noise rate as a private information. For this scenario, we design a
cost optimal procurement auction mechanism along the lines of Myerson's optimal
auction design framework in a non-trivial manner. This mechanism satisfies
incentive compatibility property, thereby facilitating the learner to elicit
true noise rates of all the annotators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4860</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4860</id><created>2012-10-16</created><authors><author><keyname>Freno</keyname><forenames>Antonino</forenames></author><author><keyname>Keller</keyname><forenames>Mikaela</forenames></author><author><keyname>Garriga</keyname><forenames>Gemma C.</forenames></author><author><keyname>Tommasi</keyname><forenames>Marc</forenames></author></authors><title>Spectral Estimation of Conditional Random Graph Models for Large-Scale
  Network Data</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-265-274</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models for graphs have been typically committed to strong prior
assumptions concerning the form of the modeled distributions. Moreover, the
vast majority of currently available models are either only suitable for
characterizing some particular network properties (such as degree distribution
or clustering coefficient), or they are aimed at estimating joint probability
distributions, which is often intractable in large-scale networks. In this
paper, we first propose a novel network statistic, based on the Laplacian
spectrum of graphs, which allows to dispense with any parametric assumption
concerning the modeled network properties. Second, we use the defined statistic
to develop the Fiedler random graph model, switching the focus from the
estimation of joint probability distributions to a more tractable conditional
estimation setting. After analyzing the dependence structure characterizing
Fiedler random graphs, we evaluate them experimentally in edge prediction over
several real-world networks, showing that they allow to reach a much higher
prediction accuracy than various alternative statistical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4861</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4861</id><created>2012-10-16</created><authors><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Uniform Solution Sampling Using a Constraint Solver As an Oracle</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-255-264</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sampling from solutions defined by a set of hard
constraints on a combinatorial space. We propose a new sampling technique that,
while enforcing a uniform exploration of the search space, leverages the
reasoning power of a systematic constraint solver in a black-box scheme. We
present a series of challenging domains, such as energy barriers and highly
asymmetric spaces, that reveal the difficulties introduced by hard constraints.
We demonstrate that standard approaches such as Simulated Annealing and Gibbs
Sampling are greatly affected, while our new technique can overcome many of
these difficulties. Finally, we show that our sampling scheme naturally defines
a new approximate model counting technique, which we empirically show to be
very accurate on a range of benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4862</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4862</id><created>2012-10-16</created><authors><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-247-254</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and prove properties of a new offline policy evaluator for an
exploration learning setting which is superior to previous evaluators. In
particular, it simultaneously and correctly incorporates techniques from
importance weighting, doubly robust evaluation, and nonstationary policy
evaluation approaches. In addition, our approach allows generating longer
histories by careful control of a bias-variance tradeoff, and further decreases
variance by incorporating information about randomness of the target policy.
Empirical evidence from synthetic and realworld exploration learning problems
shows the new evaluator successfully unifies previous approaches and uses
information an order of magnitude more efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4863</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4863</id><created>2012-10-16</created><authors><author><keyname>Dubuisson</keyname><forenames>Severine</forenames></author><author><keyname>Gonzales</keyname><forenames>Christophe</forenames></author><author><keyname>NGuyen</keyname><forenames>Xuan Son</forenames></author></authors><title>DBN-Based Combinatorial Resampling for Articulated Object Tracking</title><categories>cs.CV</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-237-246</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle Filter is an effective solution to track objects in video sequences
in complex situations. Its key idea is to estimate the density over the
possible states of the object using a weighted sample whose elements are called
particles. One of its crucial step is a resampling step in which particles are
resampled to avoid some degeneracy problem. In this paper, we introduce a new
resampling method called Combinatorial Resampling that exploits some features
of articulated objects to resample over an implicitly created sample of an
exponential size better representing the density to estimate. We prove that it
is sound and, through experimentations both on challenging synthetic and real
video sequences, we show that it outperforms all classical resampling methods
both in terms of the quality of its results and in terms of response times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4864</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4864</id><created>2012-10-16</created><authors><author><keyname>Dong</keyname><forenames>Wen</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author><author><keyname>Heller</keyname><forenames>Katherine A.</forenames></author></authors><title>Graph-Coupled HMMs for Modeling the Spread of Infection</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-227-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop Graph-Coupled Hidden Markov Models (GCHMMs) for modeling the
spread of infectious disease locally within a social network. Unlike most
previous research in epidemiology, which typically models the spread of
infection at the level of entire populations, we successfully leverage mobile
phone data collected from 84 people over an extended period of time to model
the spread of infection on an individual level. Our model, the GCHMM, is an
extension of widely-used Coupled Hidden Markov Models (CHMMs), which allow
dependencies between state transitions across multiple Hidden Markov Models
(HMMs), to situations in which those dependencies are captured through the
structure of a graph, or to social networks that may change over time. The
benefit of making infection predictions on an individual level is enormous, as
it allows people to receive more personalized and relevant health advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4865</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4865</id><created>2012-10-16</created><authors><author><keyname>Dibangoye</keyname><forenames>Jilles S.</forenames></author><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Doniec</keyname><forenames>Arnoud</forenames></author></authors><title>Scaling Up Decentralized MDPs Through Heuristic Search</title><categories>cs.AI cs.MA</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-217-226</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized partially observable Markov decision processes (Dec-POMDPs) are
rich models for cooperative decision-making under uncertainty, but are often
intractable to solve optimally (NEXP-complete). The transition and observation
independent Dec-MDP is a general subclass that has been shown to have
complexity in NP, but optimal algorithms for this subclass are still
inefficient in practice. In this paper, we first provide an updated proof that
an optimal policy does not depend on the histories of the agents, but only the
local observations. We then present a new algorithm based on heuristic search
that is able to expand search nodes by using constraint optimization. We show
experimental results comparing our approach with the state-of-the-art DecMDP
and Dec-POMDP solvers. These results show a reduction in computation time and
an increase in scalability by multiple orders of magnitude in a number of
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4866</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4866</id><created>2012-10-16</created><authors><author><keyname>Claassen</keyname><forenames>Tom</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>A Bayesian Approach to Constraint Based Causal Inference</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-207-216</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We target the problem of accuracy and robustness in causal inference from
finite data sets. Some state-of-the-art algorithms produce clear output
complete with solid theoretical guarantees but are susceptible to propagating
erroneous decisions, while others are very adept at handling and representing
uncertainty, but need to rely on undesirable assumptions. Our aim is to combine
the inherent robustness of the Bayesian approach with the theoretical strength
and clarity of constraint-based methods. We use a Bayesian score to obtain
probability estimates on the input statements used in a constraint-based
procedure. These are subsequently processed in decreasing order of reliability,
letting more reliable decisions take precedence in case of con icts, until a
single output model is obtained. Tests show that a basic implementation of the
resulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already
outperforms established procedures such as FCI and Conservative PC. It can also
indicate which causal decisions in the output have high reliability and which
do not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4867</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4867</id><created>2012-10-16</created><authors><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author><author><keyname>Amir</keyname><forenames>Eyal</forenames></author></authors><title>Lifted Relational Variational Inference</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-196-206</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid continuous-discrete models naturally represent many real-world
applications in robotics, finance, and environmental engineering. Inference
with large-scale models is challenging because relational structures
deteriorate rapidly during inference with observations. The main contribution
of this paper is an efficient relational variational inference algorithm that
factors largescale probability models into simpler variational models, composed
of mixtures of iid (Bernoulli) random variables. The algorithm takes
probability relational models of largescale hybrid systems and converts them to
a close-to-optimal variational models. Then, it efficiently calculates marginal
probabilities on the variational models by using a latent (or lifted) variable
elimination or a lifted stochastic sampling. This inference is unique because
it maintains the relational structure upon individual observations and during
inference steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4868</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4868</id><created>2012-10-16</created><authors><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Chunming</forenames></author><author><keyname>McCarty</keyname><forenames>Catherine</forenames></author><author><keyname>Peissig</keyname><forenames>Peggy</forenames></author><author><keyname>Burnside</keyname><forenames>Elizabeth</forenames></author><author><keyname>Page</keyname><forenames>David</forenames></author></authors><title>Graphical-model Based Multiple Testing under Dependence, with
  Applications to Genome-wide Association Studies</title><categories>stat.ME cs.CE stat.AP</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-511-522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale multiple testing tasks often exhibit dependence, and leveraging
the dependence between individual tests is still one challenging and important
problem in statistics. With recent advances in graphical models, it is feasible
to use them to perform multiple testing under dependence. We propose a multiple
testing procedure which is based on a Markov-random-field-coupled mixture
model. The ground truth of hypotheses is represented by a latent binary Markov
random field, and the observed test statistics appear as the coupled mixture
variables. The parameters in our model can be automatically learned by a novel
EM algorithm. We use an MCMC algorithm to infer the posterior probability that
each hypothesis is null (termed local index of significance), and the false
discovery rate can be controlled accordingly. Simulations show that the
numerical performance of multiple testing can be improved substantially by
using our procedure. We apply the procedure to a real-world genome-wide
association study on breast cancer, and we identify several SNPs with strong
association evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4869</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4869</id><created>2012-10-16</created><authors><author><keyname>Ling</keyname><forenames>Guang</forenames></author><author><keyname>Yang</keyname><forenames>Haiqin</forenames></author><author><keyname>Lyu</keyname><forenames>Michael R.</forenames></author><author><keyname>King</keyname><forenames>Irwin</forenames></author></authors><title>Response Aware Model-Based Collaborative Filtering</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-501-510</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work on recommender systems mainly focus on fitting the ratings
provided by users. However, the response patterns, i.e., some items are rated
while others not, are generally ignored. We argue that failing to observe such
response patterns can lead to biased parameter estimation and sub-optimal model
performance. Although several pieces of work have tried to model users'
response patterns, they miss the effectiveness and interpretability of the
successful matrix factorization collaborative filtering approaches. To bridge
the gap, in this paper, we unify explicit response models and PMF to establish
the Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We
show that RAPMF subsumes PMF as a special case. Empirically we demonstrate the
merits of RAPMF from various aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4870</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4870</id><created>2012-10-16</created><authors><author><keyname>Lin</keyname><forenames>Christopher H.</forenames></author><author><keyname>Mausam</keyname></author><author><keyname>Weld</keyname><forenames>Daniel</forenames></author></authors><title>Crowdsourcing Control: Moving Beyond Multiple Choice</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-491-500</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To ensure quality results from crowdsourced tasks, requesters often aggregate
worker responses and use one of a plethora of strategies to infer the correct
answer from the set of noisy responses. However, all current models assume
prior knowledge of all possible outcomes of the task. While not an unreasonable
assumption for tasks that can be posited as multiple-choice questions (e.g.
n-ary classification), we observe that many tasks do not naturally fit this
paradigm, but instead demand a free-response formulation where the outcome
space is of infinite size (e.g. audio transcription). We model such tasks with
a novel probabilistic graphical model, and design and implement LazySusan, a
decision-theoretic controller that dynamically requests responses as necessary
in order to infer answers to these tasks. We also design an EM algorithm to
jointly learn the parameters of our model while inferring the correct answers
to multiple tasks at a time. Live experiments on Amazon Mechanical Turk
demonstrate the superiority of LazySusan at solving SAT Math questions,
eliminating 83.2% of the error and achieving greater net utility compared to
the state-ofthe-art strategy, majority-voting. We also show in live experiments
that our EM algorithm outperforms majority-voting on a visualization task that
we design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4871</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4871</id><created>2012-10-16</created><authors><author><keyname>Lin</keyname><forenames>Hui</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff A.</forenames></author></authors><title>Learning Mixtures of Submodular Shells with Application to Document
  Summarization</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-479-490</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method to learn a mixture of submodular &quot;shells&quot; in a
large-margin setting. A submodular shell is an abstract submodular function
that can be instantiated with a ground set and a set of parameters to produce a
submodular function. A mixture of such shells can then also be so instantiated
to produce a more complex submodular function. What our algorithm learns are
the mixture weights over such shells. We provide a risk bound guarantee when
learning in a large-margin structured-prediction setting using a projected
subgradient method when only approximate submodular optimization is possible
(such as with submodular function maximization). We apply this method to the
problem of multi-document summarization and produce the best results reported
so far on the widely used NIST DUC-05 through DUC-07 document summarization
corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4872</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4872</id><created>2012-10-16</created><authors><author><keyname>Li</keyname><forenames>Lingbo</forenames></author><author><keyname>Zhang</keyname><forenames>XianXing</forenames></author><author><keyname>Zhou</keyname><forenames>Mingyuan</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Nested Dictionary Learning for Hierarchical Organization of Imagery and
  Text</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-469-478</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tree-based dictionary learning model is developed for joint analysis of
imagery and associated text. The dictionary learning may be applied directly to
the imagery from patches, or to general feature vectors extracted from patches
or superpixels (using any existing method for image feature extraction). Each
image is associated with a path through the tree (from root to a leaf), and
each of the multiple patches in a given image is associated with one node in
that path. Nodes near the tree root are shared between multiple paths,
representing image characteristics that are common among different types of
images. Moving toward the leaves, nodes become specialized, representing
details in image classes. If available, words (text) are also jointly modeled,
with a path-dependent probability over words. The tree structure is inferred
via a nested Dirichlet process, and a retrospective stick-breaking sampler is
used to infer the tree depth and width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4873</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4873</id><created>2012-10-16</created><authors><author><keyname>Letchford</keyname><forenames>Joshua</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author></authors><title>Computing Optimal Security Strategies for Interdependent Assets</title><categories>cs.GT cs.CR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-459-468</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel framework for computing optimal randomized security
policies in networked domains which extends previous approaches in several
ways. First, we extend previous linear programming techniques for Stackelberg
security games to incorporate benefits and costs of arbitrary security
configurations on individual assets. Second, we offer a principled model of
failure cascades that allows us to capture both the direct and indirect value
of assets, and extend this model to capture uncertainty about the structure of
the interdependency network. Third, we extend the linear programming
formulation to account for exogenous (random) failures in addition to targeted
attacks. The goal of our work is two-fold. First, we aim to develop techniques
for computing optimal security strategies in realistic settings involving
interdependent security. To this end, we evaluate the value of our technical
contributions in comparison with previous approaches, and show that our
approach yields much better defense policies and scales to realistic graphs.
Second, our computational framework enables us to attain theoretical insights
about security on networks. As an example, we study how allowing security to be
endogenous impacts the relative resilience of different network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4874</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4874</id><created>2012-10-16</created><authors><author><keyname>Lau</keyname><forenames>Hoong Chuin</forenames></author><author><keyname>Yeoh</keyname><forenames>William</forenames></author><author><keyname>Varakantham</keyname><forenames>Pradeep</forenames></author><author><keyname>Nguyen</keyname><forenames>Duc Thien</forenames></author><author><keyname>Chen</keyname><forenames>Huaxing</forenames></author></authors><title>Dynamic Stochastic Orienteering Problems for Risk-Aware Applications</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-448-458</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orienteering problems (OPs) are a variant of the well-known prize-collecting
traveling salesman problem, where the salesman needs to choose a subset of
cities to visit within a given deadline. OPs and their extensions with
stochastic travel times (SOPs) have been used to model vehicle routing problems
and tourist trip design problems. However, they suffer from two limitations
travel times between cities are assumed to be time independent and the route
provided is independent of the risk preference (with respect to violating the
deadline) of the user. To address these issues, we make the following
contributions: We introduce (1) a dynamic SOP (DSOP) model, which is an
extension of SOPs with dynamic (time-dependent) travel times; (2) a
risk-sensitive criterion to allow for different risk preferences; and (3) a
local search algorithm to solve DSOPs with this risk-sensitive criterion. We
evaluated our algorithms on a real-world dataset for a theme park navigation
problem as well as synthetic datasets employed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4875</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4875</id><created>2012-10-16</created><authors><author><keyname>Kolobov</keyname><forenames>Andrey</forenames></author><author><keyname>Mausam</keyname></author><author><keyname>Weld</keyname><forenames>Daniel</forenames></author></authors><title>A Theory of Goal-Oriented MDPs with Dead Ends</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-438-447</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Shortest Path (SSP) MDPs is a problem class widely studied in AI,
especially in probabilistic planning. They describe a wide range of scenarios
but make the restrictive assumption that the goal is reachable from any state,
i.e., that dead-end states do not exist. Because of this, SSPs are unable to
model various scenarios that may have catastrophic events (e.g., an airplane
possibly crashing if it flies into a storm). Even though MDP algorithms have
been used for solving problems with dead ends, a principled theory of SSP
extensions that would allow dead ends, including theoretically sound algorithms
for solving such MDPs, has been lacking. In this paper, we propose three new
MDP classes that admit dead ends under increasingly weaker assumptions. We
present Value Iteration-based as well as the more efficient heuristic search
algorithms for optimally solving each class, and explore theoretical
relationships between these classes. We also conduct a preliminary empirical
study comparing the performance of our algorithms on different MDP classes,
especially on scenarios with unavoidable dead ends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4876</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4876</id><created>2012-10-16</created><authors><author><keyname>Judah</keyname><forenames>Kshitij</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas G.</forenames></author></authors><title>Active Imitation Learning via Reduction to I.I.D. Active Learning</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-428-437</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In standard passive imitation learning, the goal is to learn a target policy
by passively observing full execution trajectories of it. Unfortunately,
generating such trajectories can require substantial expert effort and be
impractical in some cases. In this paper, we consider active imitation learning
with the goal of reducing this effort by querying the expert about the desired
action at individual states, which are selected based on answers to past
queries and the learner's interactions with an environment simulator. We
introduce a new approach based on reducing active imitation learning to i.i.d.
active learning, which can leverage progress in the i.i.d. setting. Our first
contribution, is to analyze reductions for both non-stationary and stationary
policies, showing that the label complexity (number of queries) of active
imitation learning can be substantially less than passive learning. Our second
contribution, is to introduce a practical algorithm inspired by the reductions,
which is shown to be highly effective in four test domains compared to a number
of alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4877</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4877</id><created>2012-10-16</created><authors><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author></authors><title>Incentive Decision Processes</title><categories>cs.GT cs.MA</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-418-427</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Incentive Decision Processes, where a principal seeks to reduce
its costs due to another agent's behavior, by offering incentives to the agent
for alternate behavior. We focus on the case where a principal interacts with a
greedy agent whose preferences are hidden and static. Though IDPs can be
directly modeled as partially observable Markov decision processes (POMDP), we
show that it is possible to directly reduce or approximate the IDP as a
polynomially-sized MDP: when this representation is approximate, we prove the
resulting policy is boundedly-optimal for the original IDP. Our empirical
simulations demonstrate the performance benefit of our algorithms over simpler
approaches, and also demonstrate that our approximate representation results in
a significantly faster algorithm whose performance is extremely close to the
optimal policy for the original IDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4878</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4878</id><created>2012-10-16</created><authors><author><keyname>Ihler</keyname><forenames>Alexander T.</forenames></author><author><keyname>Flerova</keyname><forenames>Natalia</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Otten</keyname><forenames>Lars</forenames></author></authors><title>Join-graph based cost-shifting schemes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-397-406</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop several algorithms taking advantage of two common approaches for
bounding MPE queries in graphical models: minibucket elimination and
message-passing updates for linear programming relaxations. Both methods are
quite similar, and offer useful perspectives for the other; our hybrid
approaches attempt to balance the advantages of each. We demonstrate the power
of our hybrid algorithms through extensive empirical evaluation. Most notably,
a Branch and Bound search guided by the heuristic function calculated by one of
our new algorithms has recently won first place in the PASCAL2 inference
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4879</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4879</id><created>2012-10-16</created><authors><author><keyname>Hyttinen</keyname><forenames>Antti</forenames></author><author><keyname>Eberhardt</keyname><forenames>Frederick</forenames></author><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author></authors><title>Causal Discovery of Linear Cyclic Models from Multiple Experimental Data
  Sets with Overlapping Variables</title><categories>stat.ME cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-387-396</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of scientific data is collected as randomized experiments intervening on
some and observing other variables of interest. Quite often, a given phenomenon
is investigated in several studies, and different sets of variables are
involved in each study. In this article we consider the problem of integrating
such knowledge, inferring as much as possible concerning the underlying causal
structure with respect to the union of observed variables from such
experimental or passive observational overlapping data sets. We do not assume
acyclicity or joint causal sufficiency of the underlying data generating model,
but we do restrict the causal relationships to be linear and use only second
order statistics of the data. We derive conditions for full model
identifiability in the most generic case, and provide novel techniques for
incorporating an assumption of faithfulness to aid in inference. In each case
we seek to establish what is and what is not determined by the data at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4880</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4880</id><created>2012-10-16</created><authors><author><keyname>Hostetler</keyname><forenames>Jesse</forenames></author><author><keyname>Dereszynski</keyname><forenames>Ethan W.</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas G.</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author></authors><title>Inferring Strategies from Limited Reconnaissance in Real-time Strategy
  Games</title><categories>cs.AI cs.GT cs.LG</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-367-376</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In typical real-time strategy (RTS) games, enemy units are visible only when
they are within sight range of a friendly unit. Knowledge of an opponent's
disposition is limited to what can be observed through scouting. Information is
costly, since units dedicated to scouting are unavailable for other purposes,
and the enemy will resist scouting attempts. It is important to infer as much
as possible about the opponent's current and future strategy from the available
observations. We present a dynamic Bayes net model of strategies in the RTS
game Starcraft that combines a generative model of how strategies relate to
observable quantities with a principled framework for incorporating evidence
gained via scouting. We demonstrate the model's ability to infer unobserved
aspects of the game from realistic observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4881</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4881</id><created>2012-10-16</created><authors><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Peng</keyname><forenames>Jian</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>Tightening Fractional Covering Upper Bounds on the Partition Function
  for High-Order Region Graphs</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-356-366</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach for tightening upper bounds on the
partition function. Our upper bounds are based on fractional covering bounds on
the entropy function, and result in a concave program to compute these bounds
and a convex program to tighten them. To solve these programs effectively for
general region graphs we utilize the entropy barrier method, thus decomposing
the original programs by their dual programs and solve them with dual block
optimization scheme. The entropy barrier method provides an elegant framework
to generalize the message-passing scheme to high-order region graph, as well as
to solve the block dual steps in closed-form. This is a key for computational
relevancy for large problems with thousands of regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4882</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4882</id><created>2012-10-16</created><authors><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Shah</keyname><forenames>Nisarg</forenames></author></authors><title>A Maximum Likelihood Approach For Selecting Sets of Alternatives</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-695-704</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of selecting a subset of alternatives given noisy
evaluations of the relative strength of different alternatives. We wish to
select a k-subset (for a given k) that provides a maximum likelihood estimate
for one of several objectives, e.g., containing the strongest alternative.
Although this problem is NP-hard, we show that when the noise level is
sufficiently high, intuitive methods provide the optimal solution. We thus
generalize classical results about singling out one alternative and identifying
the hidden ranking of alternatives by strength. Extensive experiments show that
our methods perform well in practical settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4883</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4883</id><created>2012-10-16</created><authors><author><keyname>Poon</keyname><forenames>Leonard K. M.</forenames></author><author><keyname>Liu</keyname><forenames>April H.</forenames></author><author><keyname>Liu</keyname><forenames>Tengfei</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author></authors><title>A Model-Based Approach to Rounding in Spectral Clustering</title><categories>cs.LG cs.NA stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-685-694</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spectral clustering, one defines a similarity matrix for a collection of
data points, transforms the matrix to get the Laplacian matrix, finds the
eigenvectors of the Laplacian matrix, and obtains a partition of the data using
the leading eigenvectors. The last step is sometimes referred to as rounding,
where one needs to decide how many leading eigenvectors to use, to determine
the number of clusters, and to partition the data points. In this paper, we
propose a novel method for rounding. The method differs from previous methods
in three ways. First, we relax the assumption that the number of clusters
equals the number of eigenvectors used. Second, when deciding the number of
leading eigenvectors to use, we not only rely on information contained in the
leading eigenvectors themselves, but also use subsequent eigenvectors. Third,
our method is model-based and solves all the three subproblems of rounding
using a class of graphical models called latent tree models. We evaluate our
method on both synthetic and real-world data. The results show that our method
works correctly in the ideal case where between-clusters similarity is 0, and
degrades gracefully as one moves away from the ideal case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4884</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4884</id><created>2012-10-16</created><authors><author><keyname>Parikh</keyname><forenames>Ankur P.</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Ishteva</keyname><forenames>Mariya</forenames></author><author><keyname>Teodoru</keyname><forenames>Gabi</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>A Spectral Algorithm for Latent Junction Trees</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-675-684</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent variable models are an elegant framework for capturing rich
probabilistic dependencies in many applications. However, current approaches
typically parametrize these models using conditional probability tables, and
learning relies predominantly on local search heuristics such as Expectation
Maximization. Using tensor algebra, we propose an alternative parameterization
of latent variable models (where the model structures are junction trees) that
still allows for computation of marginals among observed variables. While this
novel representation leads to a moderate increase in the number of parameters
for junction trees of low treewidth, it lets us design a local-minimum-free
algorithm for learning this parameterization. The main computation of the
algorithm involves only tensor operations and SVDs which can be orders of
magnitude faster than EM algorithms for large datasets. To our knowledge, this
is the first provably consistent parameter learning technique for a large class
of low-treewidth latent graphical models beyond trees. We demonstrate the
advantages of our method on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4885</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4885</id><created>2012-10-16</created><authors><author><keyname>Otten</keyname><forenames>Lars</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>A Case Study in Complexity Estimation: Towards Parallel Branch-and-Bound
  over Graphical Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-665-674</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of complexity estimation in the context of parallelizing
an advanced Branch and Bound-type algorithm over graphical models. The
algorithm's pruning power makes load balancing, one crucial element of every
distributed system, very challenging. We propose using a statistical regression
model to identify and tackle disproportionally complex parallel subproblems,
the cause of load imbalance, ahead of time. The proposed model is evaluated and
analyzed on various levels and shown to yield robust predictions. We then
demonstrate its effectiveness for load balancing in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4886</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4886</id><created>2012-10-16</created><authors><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author><author><keyname>Spaan</keyname><forenames>Matthijs T. J.</forenames></author></authors><title>Exploiting Structure in Cooperative Bayesian Games</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-654-665</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative Bayesian games (BGs) can model decision-making problems for teams
of agents under imperfect information, but require space and computation time
that is exponential in the number of agents. While agent independence has been
used to mitigate these problems in perfect information settings, we propose a
novel approach for BGs based on the observation that BGs additionally possess a
different types of structure, which we call type independence. We propose a
factor graph representation that captures both forms of independence and
present a theoretical analysis showing that non-serial dynamic programming
cannot effectively exploit type independence, while Max-Sum can. Experimental
results demonstrate that our approach can tackle cooperative Bayesian games of
unprecedented size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4887</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4887</id><created>2012-10-16</created><authors><author><keyname>Nishiyama</keyname><forenames>Yu</forenames></author><author><keyname>Boularias</keyname><forenames>Abdeslam</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Hilbert Space Embeddings of POMDPs</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-644-653</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonparametric approach for policy learning for POMDPs is proposed. The
approach represents distributions over the states, observations, and actions as
embeddings in feature spaces, which are reproducing kernel Hilbert spaces.
Distributions over states given the observations are obtained by applying the
kernel Bayes' rule to these distribution embeddings. Policies and value
functions are defined on the feature space over states, which leads to a
feature space expression for the Bellman equation. Value iteration may then be
used to estimate the optimal value function and associated policy. Experimental
results confirm that the correct policy is learned using the feature space
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4888</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4888</id><created>2012-10-16</created><authors><author><keyname>Niinimaki</keyname><forenames>Teppo</forenames></author><author><keyname>Parviainen</keyname><forenames>Pekka</forenames></author></authors><title>Local Structure Discovery in Bayesian Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-634-643</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a Bayesian network structure from data is an NP-hard problem and
thus exact algorithms are feasible only for small data sets. Therefore, network
structures for larger networks are usually learned with various heuristics.
Another approach to scaling up the structure learning is local learning. In
local learning, the modeler has one or more target variables that are of
special interest; he wants to learn the structure near the target variables and
is not interested in the rest of the variables. In this paper, we present a
score-based local learning algorithm called SLL. We conjecture that our
algorithm is theoretically sound in the sense that it is optimal in the limit
of large sample size. Empirical results suggest that SLL is competitive when
compared to the constraint-based HITON algorithm. We also study the prospects
of constructing the network structure for the whole node set based on local
results by presenting two algorithms and comparing them to several heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4889</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4889</id><created>2012-10-16</created><authors><author><keyname>Mourao</keyname><forenames>Kira</forenames></author><author><keyname>Zettlemoyer</keyname><forenames>Luke S.</forenames></author><author><keyname>Petrick</keyname><forenames>Ronald P. A.</forenames></author><author><keyname>Steedman</keyname><forenames>Mark</forenames></author></authors><title>Learning STRIPS Operators from Noisy and Incomplete Observations</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-614-623</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agents learning to act autonomously in real-world domains must acquire a
model of the dynamics of the domain in which they operate. Learning domain
dynamics can be challenging, especially where an agent only has partial access
to the world state, and/or noisy external sensors. Even in standard STRIPS
domains, existing approaches cannot learn from noisy, incomplete observations
typical of real-world domains. We propose a method which learns STRIPS action
models in such domains, by decomposing the problem into first learning a
transition function between states in the form of a set of classifiers, and
then deriving explicit STRIPS rules from the classifiers' parameters. We
evaluate our approach on simulated standard planning domains from the
International Planning Competition, and show that it learns useful domain
descriptions from noisy, incomplete observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4890</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4890</id><created>2012-10-16</created><authors><author><keyname>Maua</keyname><forenames>Denis D.</forenames></author><author><keyname>de Campos</keyname><forenames>Cassio Polpo</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>The Complexity of Approximately Solving Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-604-613</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagrams allow for intuitive and yet precise description of complex
situations involving decision making under uncertainty. Unfortunately, most of
the problems described by influence diagrams are hard to solve. In this paper
we discuss the complexity of approximately solving influence diagrams. We do
not assume no-forgetting or regularity, which makes the class of problems we
address very broad. Remarkably, we show that when both the tree-width and the
cardinality of the variables are bounded the problem admits a fully
polynomial-time approximation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4891</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4891</id><created>2012-10-16</created><authors><author><keyname>Matusevych</keyname><forenames>Sergiy</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author></authors><title>Hokusai - Sketching Streams in Real Time</title><categories>cs.DB cs.DS</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-594-603</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Hokusai, a real time system which is able to capture frequency
information for streams of arbitrary sequences of symbols. The algorithm uses
the CountMin sketch as its basis and exploits the fact that sketching is
linear. It provides real time statistics of arbitrary events, e.g. streams of
queries as a function of time. We use a factorizing approximation to provide
point estimates at arbitrary (time, item) combinations. Queries can be answered
in constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4892</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4892</id><created>2012-10-16</created><authors><author><keyname>Mattar</keyname><forenames>Marwan A.</forenames></author><author><keyname>Hanson</keyname><forenames>Allen R.</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik G.</forenames></author></authors><title>Unsupervised Joint Alignment and Clustering using Bayesian
  Nonparametrics</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-584-593</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint alignment of a collection of functions is the process of independently
transforming the functions so that they appear more similar to each other.
Typically, such unsupervised alignment algorithms fail when presented with
complex data sets arising from multiple modalities or make restrictive
assumptions about the form of the functions or transformations, limiting their
generality. We present a transformed Bayesian infinite mixture model that can
simultaneously align and cluster a data set. Our model and associated learning
scheme offer two key advantages: the optimal number of clusters is determined
in a data-driven fashion through the use of a Dirichlet process prior, and it
can accommodate any transformation function parameterized by a continuous
parameter vector. As a result, it is applicable to a wide range of data types,
and transformation functions. We present positive results on synthetic
two-dimensional data, on a set of one-dimensional curves, and on various image
data sets, showing large improvements over previous work. We discuss several
variations of the model and conclude with directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4893</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4893</id><created>2012-10-16</created><authors><author><keyname>Mahadevan</keyname><forenames>Sridhar</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author></authors><title>Sparse Q-learning with Mirror Descent</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-564-573</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores a new framework for reinforcement learning based on
online convex optimization, in particular mirror descent and related
algorithms. Mirror descent can be viewed as an enhanced gradient method,
particularly suited to minimization of convex functions in highdimensional
spaces. Unlike traditional gradient methods, mirror descent undertakes gradient
updates of weights in both the dual space and primal space, which are linked
together using a Legendre transform. Mirror descent can be viewed as a proximal
algorithm where the distance generating function used is a Bregman divergence.
A new class of proximal-gradient based temporal-difference (TD) methods are
presented based on different Bregman divergences, which are more powerful than
regular TD learning. Examples of Bregman divergences that are studied include
p-norm functions, and Mahalanobis distance based on the covariance of sample
gradients. A new family of sparse mirror-descent reinforcement learning methods
are proposed, which are able to find sparse fixed points of an l1-regularized
Bellman equation at significantly less computational cost than previous methods
based on second-order matrix methods. An experimental study of mirror-descent
reinforcement learning is presented using discrete and continuous Markov
decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4894</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4894</id><created>2012-10-16</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author><author><keyname>Martinez</keyname><forenames>Maria Vanina</forenames></author><author><keyname>Orsi</keyname><forenames>Giorgio</forenames></author><author><keyname>Simari</keyname><forenames>Gerardo I.</forenames></author></authors><title>Heuristic Ranking in Tightly Coupled Probabilistic Description Logics</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-554-563</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Web effort has steadily been gaining traction in the recent
years. In particular,Web search companies are recently realizing that their
products need to evolve towards having richer semantic search capabilities.
Description logics (DLs) have been adopted as the formal underpinnings for
Semantic Web languages used in describing ontologies. Reasoning under
uncertainty has recently taken a leading role in this arena, given the nature
of data found on theWeb. In this paper, we present a probabilistic extension of
the DL EL++ (which underlies the OWL2 EL profile) using Markov logic networks
(MLNs) as probabilistic semantics. This extension is tightly coupled, meaning
that probabilistic annotations in formulas can refer to objects in the
ontology. We show that, even though the tightly coupled nature of our language
means that many basic operations are data-intractable, we can leverage a
sublanguage of MLNs that allows to rank the atomic consequences of an ontology
relative to their probability values (called ranking queries) even when these
values are not fully computed. We present an anytime algorithm to answer
ranking queries, and provide an upper bound on the error that it incurs, as
well as a criterion to decide when results are guaranteed to be correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4895</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4895</id><created>2012-10-16</created><authors><author><keyname>Lu</keyname><forenames>Tyler</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Bayesian Vote Manipulation: Optimal Strategies and Impact on Welfare</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-543-553</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most analyses of manipulation of voting schemes have adopted two assumptions
that greatly diminish their practical import. First, it is usually assumed that
the manipulators have full knowledge of the votes of the nonmanipulating
agents. Second, analysis tends to focus on the probability of manipulation
rather than its impact on the social choice objective (e.g., social welfare).
We relax both of these assumptions by analyzing optimal Bayesian manipulation
strategies when the manipulators have only partial probabilistic information
about nonmanipulator votes, and assessing the expected loss in social welfare
(in the broad sense of the term). We present a general optimization framework
for the derivation of optimal manipulation strategies given arbitrary voting
rules and distributions over preferences. We theoretically and empirically
analyze the optimal manipulability of some popular voting rules using
distributions and real data sets that go well beyond the common, but
unrealistic, impartial culture assumption. We also shed light on the stark
difference between the loss in social welfare and the probability of
manipulation by showing that even when manipulation is likely, impact to social
welfare is slight (and often negligible).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4896</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4896</id><created>2012-10-16</created><authors><author><keyname>Lowd</keyname><forenames>Daniel</forenames></author></authors><title>Closed-Form Learning of Markov Networks from Dependency Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-533-542</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov networks (MNs) are a powerful way to compactly represent a joint
probability distribution, but most MN structure learning methods are very slow,
due to the high cost of evaluating candidates structures. Dependency networks
(DNs) represent a probability distribution as a set of conditional probability
distributions. DNs are very fast to learn, but the conditional distributions
may be inconsistent with each other and few inference algorithms support DNs.
In this paper, we present a closed-form method for converting a DN into an MN,
allowing us to enjoy both the efficiency of DN learning and the convenience of
the MN representation. When the DN is consistent, this conversion is exact. For
inconsistent DNs, we present averaging methods that significantly improve the
approximation. In experiments on 12 standard datasets, our methods are orders
of magnitude faster than and often more accurate than combining conditional
distributions using weight learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4897</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4897</id><created>2012-10-16</created><authors><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander T.</forenames></author></authors><title>Belief Propagation for Structured Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-523-532</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational inference algorithms such as belief propagation have had
tremendous impact on our ability to learn and use graphical models, and give
many insights for developing or understanding exact and approximate inference.
However, variational approaches have not been widely adoped for decision making
in graphical models, often formulated through influence diagrams and including
both centralized and decentralized (or multi-agent) decisions. In this work, we
present a general variational framework for solving structured cooperative
decision-making problems, use it to propose several belief propagation-like
algorithms, and analyze them both theoretically and empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4898</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4898</id><created>2012-10-16</created><authors><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Parr</keyname><forenames>Ron</forenames></author></authors><title>Value Function Approximation in Noisy Environments Using Locally
  Smoothed Regularized Approximate Linear Programs</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-835-842</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear
Programming (RALP) could produce value functions and policies which compared
favorably to established linear value function approximation techniques like
LSPI. RALP's success primarily stems from the ability to solve the feature
selection and value function approximation steps simultaneously. RALP's
performance guarantees become looser if sampled next states are used. For very
noisy domains, RALP requires an accurate model rather than samples, which can
be unrealistic in some practical scenarios. In this paper, we demonstrate this
weakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear
Programming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies
stemming from noise even without an accurate model. We show that, given some
smoothness assumptions, as the number of samples increases, error from noise
approaches zero, and provide experimental examples of LS-RALP's success on
common reinforcement learning benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4899</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4899</id><created>2012-10-16</created><authors><author><keyname>Tarlow</keyname><forenames>Daniel</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author></authors><title>Fast Exact Inference for Recursive Cardinality Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-825-834</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardinality potentials are a generally useful class of high order potential
that affect probabilities based on how many of D binary variables are active.
Maximum a posteriori (MAP) inference for cardinality potential models is
well-understood, with efficient computations taking O(DlogD) time. Yet
efficient marginalization and sampling have not been addressed as thoroughly in
the machine learning community. We show that there exists a simple algorithm
for computing marginal probabilities and drawing exact joint samples that runs
in O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief
propagation in a low order tree-structured model that includes additional
auxiliary variables. We then develop a new, more general class of models,
termed Recursive Cardinality models, which take advantage of this efficiency.
Finally, we show how to do efficient exact inference in models composed of a
tree structure and a cardinality potential. We explore the expressive power of
Recursive Cardinality models and empirically demonstrate their utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4900</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4900</id><created>2012-10-16</created><authors><author><keyname>Sun</keyname><forenames>Wei</forenames></author><author><keyname>Hanson</keyname><forenames>Robin</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Twardy</keyname><forenames>Charles</forenames></author></authors><title>Probability and Asset Updating using Bayesian Networks for Combinatorial
  Prediction Markets</title><categories>cs.AI q-fin.TR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-815-824</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A market-maker-based prediction market lets forecasters aggregate information
by editing a consensus probability distribution either directly or by trading
securities that pay off contingent on an event of interest. Combinatorial
prediction markets allow trading on any event that can be specified as a
combination of a base set of events. However, explicitly representing the full
joint distribution is infeasible for markets with more than a few base events.
A factored representation such as a Bayesian network (BN) can achieve tractable
computation for problems with many related variables. Standard BN inference
algorithms, such as the junction tree algorithm, can be used to update a
representation of the entire joint distribution given a change to any local
conditional probability. However, in order to let traders reuse assets from
prior trades while never allowing assets to become negative, a BN based
prediction market also needs to update a representation of each user's assets
and find the conditional state in which a user has minimum assets. Users also
find it useful to see their expected assets given an edit outcome. We show how
to generalize the junction tree algorithm to perform all these computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4901</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4901</id><created>2012-10-16</created><authors><author><keyname>Petrik</keyname><forenames>Marek</forenames></author><author><keyname>Subramanian</keyname><forenames>Dharmashankar</forenames></author></authors><title>An Approximate Solution Method for Large Risk-Averse Markov Decision
  Processes</title><categories>q-fin.PM cs.AI cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-805-814</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic domains often involve risk-averse decision makers. While recent
work has focused on how to model risk in Markov decision processes using risk
measures, it has not addressed the problem of solving large risk-averse
formulations. In this paper, we propose and analyze a new method for solving
large risk-averse MDPs with hybrid continuous-discrete state spaces and
continuous action spaces. The proposed method iteratively improves a bound on
the value function using a linearity structure of the MDP. We demonstrate the
utility and properties of the method on a portfolio optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4902</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4902</id><created>2012-10-16</created><authors><author><keyname>Sontag</keyname><forenames>David</forenames></author><author><keyname>Choe</keyname><forenames>Do Kook</forenames></author><author><keyname>Li</keyname><forenames>Yitao</forenames></author></authors><title>Efficiently Searching for Frustrated Cycles in MAP Inference</title><categories>cs.DS cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-795-804</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual decomposition provides a tractable framework for designing algorithms
for finding the most probable (MAP) configuration in graphical models. However,
for many real-world inference problems, the typical decomposition has a large
integrality gap, due to frustrated cycles. One way to tighten the relaxation is
to introduce additional constraints that explicitly enforce cycle consistency.
Earlier work showed that cluster-pursuit algorithms, which iteratively
introduce cycle and other higherorder consistency constraints, allows one to
exactly solve many hard inference problems. However, these algorithms
explicitly enumerate a candidate set of clusters, limiting them to triplets or
other short cycles. We solve the search problem for cycle constraints, giving a
nearly linear time algorithm for finding the most frustrated cycle of arbitrary
length. We show how to use this search algorithm together with the dual
decomposition framework and clusterpursuit. The new algorithm exactly solves
MAP inference problems arising from relational classification and stereo
vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4903</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4903</id><created>2012-10-16</created><authors><author><keyname>Sinn</keyname><forenames>Mathieu</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Keller</keyname><forenames>Karsten</forenames></author></authors><title>Detecting Change-Points in Time Series by Maximum Mean Discrepancy of
  Ordinal Pattern Distributions</title><categories>stat.ME cs.CE</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-786-794</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a new method for detecting change-points in high-resolution time series,
we apply Maximum Mean Discrepancy to the distributions of ordinal patterns in
different parts of a time series. The main advantage of this approach is its
computational simplicity and robustness with respect to (non-linear) monotonic
transformations, which makes it particularly well-suited for the analysis of
long biophysical time series where the exact calibration of measurement devices
is unknown or varies with time. We establish consistency of the method and
evaluate its performance in simulation studies. Furthermore, we demonstrate the
application to the analysis of electroencephalography (EEG) and
electrocardiography (ECG) recordings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4904</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4904</id><created>2012-10-16</created><authors><author><keyname>Singh</keyname><forenames>Ajit P.</forenames></author><author><keyname>Halloran</keyname><forenames>John</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff A.</forenames></author><author><keyname>Kirchoff</keyname><forenames>Katrin</forenames></author><author><keyname>Noble</keyname><forenames>William S.</forenames></author></authors><title>Spectrum Identification using a Dynamic Bayesian Network Model of Tandem
  Mass Spectra</title><categories>cs.CE q-bio.QM</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-775-785</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shotgun proteomics is a high-throughput technology used to identify unknown
proteins in a complex mixture. At the heart of this process is a prediction
task, the spectrum identification problem, in which each fragmentation spectrum
produced by a shotgun proteomics experiment must be mapped to the peptide
(protein subsequence) which generated the spectrum. We propose a new algorithm
for spectrum identification, based on dynamic Bayesian networks, which
significantly outperforms the de-facto standard tools for this task: SEQUEST
and Mascot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4905</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4905</id><created>2012-10-16</created><authors><author><keyname>Silva</keyname><forenames>Ricardo</forenames></author></authors><title>Latent Composite Likelihood Learning for the Structured Canonical
  Correlation Model</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-765-774</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent variable models are used to estimate variables of interest quantities
which are observable only up to some measurement error. In many studies, such
variables are known but not precisely quantifiable (such as &quot;job satisfaction&quot;
in social sciences and marketing, &quot;analytical ability&quot; in educational testing,
or &quot;inflation&quot; in economics). This leads to the development of measurement
instruments to record noisy indirect evidence for such unobserved variables
such as surveys, tests and price indexes. In such problems, there are
postulated latent variables and a given measurement model. At the same time,
other unantecipated latent variables can add further unmeasured confounding to
the observed variables. The problem is how to deal with unantecipated latents
variables. In this paper, we provide a method loosely inspired by canonical
correlation that makes use of background information concerning the &quot;known&quot;
latent variables. Given a partially specified structure, it provides a
structure learning approach to detect &quot;unknown unknowns,&quot; the confounding
effect of potentially infinitely many other latent variables. This is done
without explicitly modeling such extra latent factors. Because of the special
structure of the problem, we are able to exploit a new variation of composite
likelihood fitting to efficiently learn this structure. Validation is provided
with experiments in synthetic data and the analysis of a large survey done with
a sample of over 100,000 staff members of the National Health Service of the
United Kingdom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4906</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4906</id><created>2012-10-16</created><authors><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author><author><keyname>Schmidt</keyname><forenames>Stefan</forenames></author><author><keyname>Kappes</keyname><forenames>Joerg</forenames></author><author><keyname>Schnoerr</keyname><forenames>Christoph</forenames></author></authors><title>Efficient MRF Energy Minimization via Adaptive Diminishing Smoothing</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-746-755</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the linear programming relaxation of an energy minimization
problem for Markov Random Fields. The dual objective of this problem can be
treated as a concave and unconstrained, but non-smooth function. The idea of
smoothing the objective prior to optimization was recently proposed in a series
of papers. Some of them suggested the idea to decrease the amount of smoothing
(so called temperature) while getting closer to the optimum. However, no
theoretical substantiation was provided. We propose an adaptive smoothing
diminishing algorithm based on the duality gap between relaxed primal and dual
objectives and demonstrate the efficiency of our approach with a smoothed
version of Sequential Tree-Reweighted Message Passing (TRW-S) algorithm. The
strategy is applicable to other algorithms as well, avoids adhoc tuning of the
smoothing during iterations, and provably guarantees convergence to the
optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4907</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4907</id><created>2012-10-16</created><authors><author><keyname>Sanfilippo</keyname><forenames>Giuseppe</forenames></author></authors><title>From imprecise probability assessments to conditional probabilities with
  quasi additive classes of conditioning events</title><categories>cs.AI math.PR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-736-745</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, starting from a generalized coherent (i.e. avoiding uniform
loss) intervalvalued probability assessment on a finite family of conditional
events, we construct conditional probabilities with quasi additive classes of
conditioning events which are consistent with the given initial assessment.
Quasi additivity assures coherence for the obtained conditional probabilities.
In order to reach our goal we define a finite sequence of conditional
probabilities by exploiting some theoretical results on g-coherence. In
particular, we use solutions of a finite sequence of linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4909</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4909</id><created>2012-10-16</created><authors><author><keyname>Roeder</keyname><forenames>Jens</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Kunzmann</keyname><forenames>Kevin</forenames></author><author><keyname>Hamprecht</keyname><forenames>Fred A.</forenames></author></authors><title>Active Learning with Distributional Estimates</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-715-725</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Learning (AL) is increasingly important in a broad range of
applications. Two main AL principles to obtain accurate classification with few
labeled data are refinement of the current decision boundary and exploration of
poorly sampled regions. In this paper we derive a novel AL scheme that balances
these two principles in a natural way. In contrast to many AL strategies, which
are based on an estimated class conditional probability ^p(y|x), a key
component of our approach is to view this quantity as a random variable, hence
explicitly considering the uncertainty in its estimated value. Our main
contribution is a novel mathematical framework for uncertainty-based AL, and a
corresponding AL scheme, where the uncertainty in ^p(y|x) is modeled by a
second-order distribution. On the practical side, we show how to approximate
such second-order distributions for kernel density classification. Finally, we
find that over a large number of UCI, USPS and Caltech4 datasets, our AL scheme
achieves significantly better learning curves than popular AL methods such as
uncertainty sampling and error reduction sampling, when all use the same kernel
density classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4910</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4910</id><created>2012-10-16</created><authors><author><keyname>Refaat</keyname><forenames>Khaled S.</forenames></author><author><keyname>Choi</keyname><forenames>Arthur</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>New Advances and Theoretical Insights into EDML</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-705-714</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EDML is a recently proposed algorithm for learning MAP parameters in Bayesian
networks. In this paper, we present a number of new advances and insights on
the EDML algorithm. First, we provide the multivalued extension of EDML,
originally proposed for Bayesian networks over binary variables. Next, we
identify a simplified characterization of EDML that further implies a simple
fixed-point algorithm for the convex optimization problem that underlies it.
This characterization further reveals a connection between EDML and EM: a fixed
point of EDML is a fixed point of EM, and vice versa. We thus identify also a
new characterization of EM fixed points, but in the semantics of EDML. Finally,
we propose a hybrid EDML/EM algorithm that takes advantage of the improved
empirical convergence behavior of EDML, while maintaining the monotonic
improvement property of EM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4911</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4911</id><created>2012-10-16</created><authors><author><keyname>Marinescu</keyname><forenames>Radu</forenames></author><author><keyname>Razak</keyname><forenames>Abdul</forenames></author><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>Multi-objective Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-574-583</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe multi-objective influence diagrams, based on a set of p
objectives, where utility values are vectors in Rp, and are typically only
partially ordered. These can still be solved by a variable elimination
algorithm, leading to a set of maximal values of expected utility. If the
Pareto ordering is used this set can often be prohibitively large. We consider
approximate representations of the Pareto set based on e-coverings, allowing
much larger problems to be solved. In addition, we define a method for
incorporating user tradeoffs, which also greatly improves the efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4912</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4912</id><created>2012-10-16</created><authors><author><keyname>Zhang</keyname><forenames>Zhongzhang</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoping</forenames></author></authors><title>FHHOP: A Factored Hybrid Heuristic Online Planning Algorithm for Large
  POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-934-943</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning in partially observable Markov decision processes (POMDPs) remains a
challenging topic in the artificial intelligence community, in spite of recent
impressive progress in approximation techniques. Previous research has
indicated that online planning approaches are promising in handling large-scale
POMDP domains efficiently as they make decisions &quot;on demand&quot; instead of
proactively for the entire state space. We present a Factored Hybrid Heuristic
Online Planning (FHHOP) algorithm for large POMDPs. FHHOP gets its power by
combining a novel hybrid heuristic search strategy with a recently developed
factored state representation. On several benchmark problems, FHHOP
substantially outperformed state-of-the-art online heuristic search approaches
in terms of both scalability and quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4913</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4913</id><created>2012-10-16</created><authors><author><keyname>Yuan</keyname><forenames>Changhe</forenames></author><author><keyname>Malone</keyname><forenames>Brandon</forenames></author></authors><title>An Improved Admissible Heuristic for Learning Optimal Bayesian Networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-924-933</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently two search algorithms, A* and breadth-first branch and bound
(BFBnB), were developed based on a simple admissible heuristic for learning
Bayesian network structures that optimize a scoring function. The heuristic
represents a relaxation of the learning problem such that each variable chooses
optimal parents independently. As a result, the heuristic may contain many
directed cycles and result in a loose bound. This paper introduces an improved
admissible heuristic that tries to avoid directed cycles within small groups of
variables. A sparse representation is also introduced to store only the unique
optimal parent choices. Empirical results show that the new techniques
significantly improved the efficiency and scalability of A* and BFBnB on most
of datasets tested in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4914</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4914</id><created>2012-10-16</created><authors><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Blitzer</keyname><forenames>John</forenames></author></authors><title>Latent Structured Ranking</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-903-913</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many latent (factorized) models have been proposed for recommendation tasks
like collaborative filtering and for ranking tasks like document or image
retrieval and annotation. Common to all those methods is that during inference
the items are scored independently by their similarity to the query in the
latent embedding space. The structure of the ranked list (i.e. considering the
set of items returned as a whole) is not taken into account. This can be a
problem because the set of top predictions can be either too diverse (contain
results that contradict each other) or are not diverse enough. In this paper we
introduce a method for learning latent structured rankings that improves over
existing methods by providing the right blend of predictions at the top of the
ranked list. Particular emphasis is put on making this method scalable.
Empirical results on large scale image annotation and music recommendation
tasks show improvements over existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4915</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4915</id><created>2012-10-16</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author><author><keyname>Sodomka</keyname><forenames>Eric</forenames></author><author><keyname>Greenwald</keyname><forenames>Amy</forenames></author></authors><title>Self-Confirming Price Prediction Strategies for Simultaneous One-Shot
  Auctions</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-893-902</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidding in simultaneous auctions is challenging because an agent's value for
a good in one auction may depend on the uncertain outcome of other auctions:
the so-called exposure problem. Given the gap in understanding of general
simultaneous auction games, previous works have tackled this problem with
heuristic strategies that employ probabilistic price predictions. We define a
concept of self-confirming prices, and show that within an independent private
value model, Bayes-Nash equilibrium can be fully characterized as a profile of
optimal price prediction strategies with self-confirming predictions. We
exhibit practical procedures to compute approximately optimal bids given a
probabilistic price prediction, and near self-confirming price predictions
given a price-prediction strategy. An extensive empirical game-theoretic
analysis demonstrates that self-confirming price prediction strategies are
effective in simultaneous auction games with both complementary and
substitutable preference structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4916</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4916</id><created>2012-10-16</created><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Gelfand</keyname><forenames>Andrew E.</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander T.</forenames></author></authors><title>A Cluster-Cumulant Expansion at the Fixed Points of Belief Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-883-892</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new cluster-cumulant expansion (CCE) based on the fixed points
of iterative belief propagation (IBP). This expansion is similar in spirit to
the loop-series (LS) recently introduced in [1]. However, in contrast to the
latter, the CCE enjoys the following important qualities: 1) it is defined for
arbitrary state spaces 2) it is easily extended to fixed points of generalized
belief propagation (GBP), 3) disconnected groups of variables will not
contribute to the CCE and 4) the accuracy of the expansion empirically improves
upon that of the LS. The CCE is based on the same M\&quot;obius transform as the
Kikuchi approximation, but unlike GBP does not require storing the beliefs of
the GBP-clusters nor does it suffer from convergence issues during belief
updating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4917</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4917</id><created>2012-10-16</created><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Xia</keyname><forenames>Yinglong</forenames></author></authors><title>Fast Graph Construction Using Auction Algorithm</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-873-882</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical machine learning systems, graph based data representation has
been widely used in various learning paradigms, ranging from unsupervised
clustering to supervised classification. Besides those applications with
natural graph or network structure data, such as social network analysis and
relational learning, many other applications often involve a critical step in
converting data vectors to an adjacency graph. In particular, a sparse subgraph
extracted from the original graph is often required due to both theoretic and
practical needs. Previous study clearly shows that the performance of different
learning algorithms, e.g., clustering and classification, benefits from such
sparse subgraphs with balanced node connectivity. However, the existing graph
construction methods are either computationally expensive or with
unsatisfactory performance. In this paper, we utilize a scalable method called
auction algorithm and its parallel extension to recover a sparse yet nearly
balanced subgraph with significantly reduced computational cost. Empirical
study and comparison with the state-ofart approaches clearly demonstrate the
superiority of the proposed method in both efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4918</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4918</id><created>2012-10-16</created><authors><author><keyname>Walsh</keyname><forenames>Thomas J.</forenames></author><author><keyname>Goschin</keyname><forenames>Sergiu</forenames></author></authors><title>Dynamic Teaching in Sequential Decision Making Environments</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-863-872</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe theoretical bounds and a practical algorithm for teaching a model
by demonstration in a sequential decision making environment. Unlike previous
efforts that have optimized learners that watch a teacher demonstrate a static
policy, we focus on the teacher as a decision maker who can dynamically choose
different policies to teach different parts of the environment. We develop
several teaching frameworks based on previously defined supervised protocols,
such as Teaching Dimension, extending them to handle noise and sequences of
inputs encountered in an MDP.We provide theoretical bounds on the learnability
of several important model classes in this setting and suggest a practical
algorithm for dynamic teaching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4919</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4919</id><created>2012-10-16</created><authors><author><keyname>Wahabzada</keyname><forenames>Mirwaes</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Roemer</keyname><forenames>Christoph</forenames></author><author><keyname>Ballvora</keyname><forenames>Agim</forenames></author><author><keyname>Pinto</keyname><forenames>Francisco</forenames></author><author><keyname>Rascher</keyname><forenames>Uwe</forenames></author><author><keyname>Leon</keyname><forenames>Jens</forenames></author><author><keyname>Ploemer</keyname><forenames>Lutz</forenames></author></authors><title>Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought
  Stressed Plants</title><categories>cs.LG cs.CE stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-852-862</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the adaptation process of plants to drought stress is essential
in improving management practices, breeding strategies as well as engineering
viable crops for a sustainable agriculture in the coming decades.
Hyper-spectral imaging provides a particularly promising approach to gain such
understanding since it allows to discover non-destructively spectral
characteristics of plants governed primarily by scattering and absorption
characteristics of the leaf internal structure and biochemical constituents.
Several drought stress indices have been derived using hyper-spectral imaging.
However, they are typically based on few hyper-spectral images only, rely on
interpretations of experts, and consider few wavelengths only. In this study,
we present the first data-driven approach to discovering spectral drought
stress indices, treating it as an unsupervised labeling problem at massive
scale. To make use of short range dependencies of spectral wavelengths, we
develop an online variational Bayes algorithm for latent Dirichlet allocation
with convolved Dirichlet regularizer. This approach scales to massive datasets
and, hence, provides a more objective complement to plant physiological
practices. The spectral topics found conform to plant physiological knowledge
and can be computed in a fraction of the time compared to existing LDA
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4920</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4920</id><created>2012-10-16</created><authors><author><keyname>Virtanen</keyname><forenames>Seppo</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Klami</keyname><forenames>Arto</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Factorized Multi-Modal Topic Model</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-843-851</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-modal data collections, such as corpora of paired images and text
snippets, require analysis methods beyond single-view component and topic
models. For continuous observations the current dominant approach is based on
extensions of canonical correlation analysis, factorizing the variation into
components shared by the different modalities and those private to each of
them. For count data, multiple variants of topic models attempting to tie the
modalities together have been presented. All of these, however, lack the
ability to learn components private to one modality, and consequently will try
to force dependencies even between minimally correlating modalities. In this
work we combine the two approaches by presenting a novel HDP-based topic model
that automatically learns both shared and private topics. The model is shown to
be especially useful for querying the contents of one domain given samples of
the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4959</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4959</id><created>2012-10-17</created><authors><author><keyname>Khovanova</keyname><forenames>Tanya</forenames></author><author><keyname>Yang</keyname><forenames>Dai</forenames></author></authors><title>Halving Lines and Their Underlying Graphs</title><categories>math.CO cs.DM</categories><comments>26 pages, 13 figures</comments><msc-class>05C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study underlying graphs corresponding to a set of halving
lines. We establish many properties of such graphs. In addition, we tighten the
upper bound for the number of halving lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4960</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4960</id><created>2012-10-17</created><updated>2013-01-29</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author></authors><title>A new Truncated Fourier Transform algorithm</title><categories>cs.SC</categories><comments>8 pages, submitted to the 38th International Symposium on Symbolic
  and Algebraic Computation (ISSAC 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Truncated Fourier Transforms (TFTs), first introduced by Van der Hoeven,
refer to a family of algorithms that attempt to smooth &quot;jumps&quot; in complexity
exhibited by FFT algorithms. We present an in-place TFT whose time complexity,
measured in terms of ring operations, is comparable to existing not-in-place
TFT methods. We also describe a transformation that maps between two families
of TFT algorithms that use different sets of evaluation points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4962</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4962</id><created>2012-10-17</created><authors><author><keyname>arrag</keyname><forenames>Sliman</forenames></author><author><keyname>Hamdoun</keyname><forenames>Abdellatif</forenames></author><author><keyname>Tragha</keyname><forenames>Abderrahim</forenames></author><author><keyname>Khamlich</keyname><forenames>Salah eddine</forenames></author></authors><title>Several AES Variants under VHDL language In FPGA</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides four different architectures for encrypting and
decrypting 128 bit information via the AES. The encryption algorithm includes
the Key Expansion module which generates Key for all iterations on the fly,
Double AEStwo-key triple AES, AESX and AES-EXE. These architectures are
implemented and studied in Altera Cyclone III and STRATIX Family devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4980</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4980</id><created>2012-10-17</created><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Minimization of semilinear automata</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate finite deterministic automata in sets with non-homogeneous
atoms: integers with successor. As there are uncount- ably many deterministic
finite automata in this setting, we restrict our attention to automata with
semilinear transition function. The main re- sults is a minimization procedure
for semilinear automata. The proof is subtle and refers to decidability of
existential Presburger arithmetic with divisibility predicates. Interestingly,
the minimization is not obtained by the standard partition refinement
procedure, and we demonstrate that this procedure does not necessarily
terminate for semilinear automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4981</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4981</id><created>2012-10-17</created><authors><author><keyname>Garlan</keyname><forenames>David</forenames></author><author><keyname>Dwivedi</keyname><forenames>Vishal</forenames></author><author><keyname>Ruchkin</keyname><forenames>Ivan</forenames></author><author><keyname>Schmerl</keyname><forenames>Bradley</forenames></author></authors><title>Foundations and Tools for End-User Architecting</title><categories>cs.SE cs.HC cs.SI</categories><acm-class>D.2.11; H.5.2</acm-class><journal-ref>Large-Scale Complex IT Systems. Development, Operation and
  Management. Lecture Notes in Computer Science, 2012, Volume 7539/2012,
  157-182</journal-ref><doi>10.1007/978-3-642-34059-8_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within an increasing number of domains an important emerging need is the
ability for technically naive users to compose computational elements into
novel configurations. Examples include astronomers who create new analysis
pipelines to process telescopic data, intelligence analysts who must process
diverse sources of unstructured text to discover socio-technical trends, and
medical researchers who have to process brain image data in new ways to
understand disease pathways. Creating such compositions today typically
requires low-level technical expertise, limiting the use of computational
methods and increasing the cost of using them. In this paper we describe an
approach - which we term end-user architecting - that exploits the similarity
between such compositional activities and those of software architects. Drawing
on the rich heritage of software architecture languages, methods, and tools, we
show how those techniques can be adapted to support end users in composing rich
computational systems through domain-specific compositional paradigms and
component repositories, without requiring that they have knowledge of the
low-level implementation details of the components or the compositional
infrastructure. Further, we outline a set of open research challenges that the
area of end-user architecting raises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.4992</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.4992</id><created>2012-10-17</created><authors><author><keyname>Medeiros</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Mascarenhas</keyname><forenames>Fabio</forenames></author><author><keyname>Ierusalimschy</keyname><forenames>Roberto</forenames></author></authors><title>From Regexes to Parsing Expression Grammars</title><categories>cs.FL cs.PL</categories><doi>10.1016/j.scico.2012.11.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most scripting languages nowadays use regex pattern-matching libraries. These
regex libraries borrow the syntax of regular expressions, but have an informal
semantics that is different from the semantics of regular expressions, removing
the commutativity of alternation and adding ad-hoc extensions that cannot be
expressed by formalisms for efficient recognition of regular languages, such as
deterministic finite automata.
  Parsing Expression Grammars are a formalism that can describe all
deterministic context-free languages and has a simple computational model. In
this paper, we present a formalization of regexes via transformation to Parsing
Expression Grammars. The proposed transformation easily accommodates several of
the common regex extensions, giving a formal meaning to them. It also provides
a clear computational model that helps to estimate the efficiency of
regex-based matchers, and a basis for specifying provably correct optimizations
for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5012</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5012</id><created>2012-10-17</created><updated>2013-01-16</updated><authors><author><keyname>Hua</keyname><forenames>Sha</forenames></author><author><keyname>Zhuo</keyname><forenames>Xuejun</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author></authors><title>A Truthful Auction based Incentive Framework for Femtocell Access</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As cellular operators are suffering from a data explosion problem, and users
are consequently experiencing poor data services, the introduction of
femtocells offers a cost-effective way to mitigate this problem. Femtocells
enable larger network capacity by increasing spatial reuse of the spectrum and
shortening the distance to the users. Existing work has shown that open access
femtocells, which allow unregistered macro users to connect, are efficient in
reducing inter-cell interference and offloading traffic. However, a major
obstacle constraining the potential capability of femtocells and open access is
the lack of incentives for privately-owned femtocells to serve unregistered
users. Hence in this paper, we propose a Vickrey-Clarke-Groves (VCG) auction
based incentive framework for accessing such selfish femtocells. We consider
two scenarios: One scenario involves a single macro user and another scenario
has multiple macro users. We design auction schemes for both scenarios and show
analytically that our schemes are truthful and have low computational
complexity. Extensive simulations validate these properties and show huge
performance improvement to the macro users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5031</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5031</id><created>2012-10-18</created><authors><author><keyname>Ekambaram</keyname><forenames>Venkatesan</forenames></author><author><keyname>Fanti</keyname><forenames>Giulia</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Semi-Definite Programming Relaxation for Non-Line-of-Sight Localization</title><categories>cs.IT cs.MA cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the locations of a set of points in a
k-dimensional euclidean space given a subset of the pairwise distance
measurements between the points. We focus on the case when some fraction of
these measurements can be arbitrarily corrupted by large additive noise. Given
that the problem is highly non-convex, we propose a simple semidefinite
programming relaxation that can be efficiently solved using standard
algorithms. We define a notion of non-contractibility and show that the
relaxation gives the exact point locations when the underlying graph is
non-contractible. The performance of the algorithm is evaluated on an
experimental data set obtained from a network of 44 nodes in an indoor
environment and is shown to be robust to non-line-of-sight errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5034</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5034</id><created>2012-10-18</created><updated>2012-10-21</updated><authors><author><keyname>Machart</keyname><forenames>Pierre</forenames><affiliation>LIF, LSIS</affiliation></author><author><keyname>Anthoine</keyname><forenames>Sandrine</forenames><affiliation>LATP</affiliation></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames><affiliation>EPFL</affiliation></author></authors><title>Optimal Computational Trade-Off of Inexact Proximal Methods</title><categories>cs.LG cs.CV cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the trade-off between convergence rate and
computational cost when minimizing a composite functional with
proximal-gradient methods, which are popular optimisation tools in machine
learning. We consider the case when the proximity operator is computed via an
iterative procedure, which provides an approximation of the exact proximity
operator. In that case, we obtain algorithms with two nested loops. We show
that the strategy that minimizes the computational cost to reach a solution
with a desired accuracy in finite time is to set the number of inner iterations
to a constant, which differs from the strategy indicated by a convergence rate
analysis. In the process, we also present a new procedure called SIP (that is
Speedy Inexact Proximal-gradient algorithm) that is both computationally
efficient and easy to implement. Our numerical experiments confirm the
theoretical findings and suggest that SIP can be a very competitive alternative
to the standard procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5035</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5035</id><created>2012-10-18</created><authors><author><keyname>Zhou</keyname><forenames>Xiaojun</forenames></author></authors><title>A Comparative Study of State Transition Algorithm with Harmony Search
  and Artificial Bee Colony</title><categories>math.OC cs.IT math.IT math.PR</categories><journal-ref>Advances in Intelligent Systems and Computing, Volume 212, 2013,
  pp 651-659</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on a comparative study of three recently developed nature-inspired
optimization algorithms, including state transition algorithm, harmony search
and artificial bee colony. Their core mechanisms are introduced and their
similarities and differences are described. Then, a suit of 27 well-known
benchmark problems are used to investigate the performance of these algorithms
and finally we discuss their general applicability with respect to the
structure of optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5041</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5041</id><created>2012-10-18</created><updated>2013-06-17</updated><authors><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Daribo</keyname><forenames>Ismael</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Navigation domain representation for interactive multiview imaging</title><categories>cs.MM cs.CV</categories><doi>10.1109/TIP.2013.2270183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling users to interactively navigate through different viewpoints of a
static scene is a new interesting functionality in 3D streaming systems. While
it opens exciting perspectives towards rich multimedia applications, it
requires the design of novel representations and coding techniques in order to
solve the new challenges imposed by interactive navigation. Interactivity
clearly brings new design constraints: the encoder is unaware of the exact
decoding process, while the decoder has to reconstruct information from
incomplete subsets of data since the server can generally not transmit images
for all possible viewpoints due to resource constrains. In this paper, we
propose a novel multiview data representation that permits to satisfy bandwidth
and storage constraints in an interactive multiview streaming system. In
particular, we partition the multiview navigation domain into segments, each of
which is described by a reference image and some auxiliary information. The
auxiliary information enables the client to recreate any viewpoint in the
navigation segment via view synthesis. The decoder is then able to navigate
freely in the segment without further data request to the server; it requests
additional data only when it moves to a different segment. We discuss the
benefits of this novel representation in interactive navigation systems and
further propose a method to optimize the partitioning of the navigation domain
into independent segments, under bandwidth and storage constraints.
Experimental results confirm the potential of the proposed representation;
namely, our system leads to similar compression performance as classical
inter-view coding, while it provides the high level of flexibility that is
required for interactive streaming. Hence, our new framework represents a
promising solution for 3D data representation in novel interactive multimedia
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5048</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5048</id><created>2012-10-18</created><updated>2013-06-22</updated><authors><author><keyname>Doherty</keyname><forenames>Andrew C.</forenames></author><author><keyname>Wehner</keyname><forenames>Stephanie</forenames></author></authors><title>Convergence of SDP hierarchies for polynomial optimization on the
  hypersphere</title><categories>math.OC cs.DS math-ph math.MP quant-ph</categories><comments>45 pages, amsmath, comments welcome, for readers in quantum
  information: contains de Finetti theorem, v2: improved explanations,
  additional bound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to bound the accuracy of a family of semi-definite programming
relaxations for the problem of polynomial optimization on the hypersphere. Our
method is inspired by a set of results from quantum information known as
quantum de Finetti theorems. In particular, we prove a de Finetti theorem for a
special class of real symmetric matrices to establish the existence of
approximate representing measures for moment matrix relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5058</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5058</id><created>2012-10-18</created><authors><author><keyname>Gmeiner</keyname><forenames>Peter</forenames></author></authors><title>Properties of Persistent Mutual Information and Emergence</title><categories>math-ph cs.IT math.IT math.MP</categories><comments>45 pages excerpt of Diploma-Thesis</comments><msc-class>94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The persistent mutual information (PMI) is a complexity measure for
stochastic processes. It is related to well-known complexity measures like
excess entropy or statistical complexity. Essentially it is a variation of the
excess entropy so that it can be interpreted as a specific measure of system
internal memory. The PMI was first introduced in 2010 by Ball, Diakonova and
MacKay as a measure for (strong) emergence. In this paper we define the PMI
mathematically and investigate the relation to excess entropy and statistical
complexity. In particular we prove that the excess entropy is an upper bound of
the PMI. Furthermore we show some properties of the PMI and calculate it
explicitly for some example processes. We also discuss to what extend it is a
measure for emergence and compare it with alternative approaches used to
formalize emergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5065</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5065</id><created>2012-10-18</created><updated>2013-10-02</updated><authors><author><keyname>Krivine</keyname><forenames>Jean-Louis</forenames></author></authors><title>Realizability algebras III: some examples</title><categories>cs.LO math.LO</categories><comments>30 pages</comments><msc-class>03E35</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the technique of &quot;classical realizability&quot; to build new models of ZF +
DC in which R is not well ordered. This gives new relative consistency results,
probably not obtainable by forcing. This gives also a new method to get
programs from proofs of arithmetical formulas with dependent choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5083</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5083</id><created>2012-10-18</created><authors><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author></authors><title>An Improved Lower Bound of The Spark With Application</title><categories>cs.CC</categories><comments>8 pages</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.5, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spark plays a great role in studying uniqueness of sparse solutions of the
underdetermined linear equations. In this article, we derive a new lower bound
of spark. As an application, we obtain a new criterion for the uniqueness of
sparse solutions of linear equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5091</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5091</id><created>2012-10-18</created><updated>2014-03-06</updated><authors><author><keyname>Banerjee</keyname><forenames>Anirban</forenames></author></authors><title>A few properties of the eigenvalues of normalized graph Laplacian</title><categories>math.CO cs.DM</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  equation 17</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we have investigated a few properties of the eigenvalues of normalized
(geometric) graph Laplacian in different graphs. Preservation of eigenvalue 1
from a particular subgraph to the entire graph, the spectrum of the graph
constructed with triangles share a common vertex have been addressed. Further
using the number and degrees of common neighbors between vertices some new
upper bounds for the largest eigenvalue have been introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5093</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5093</id><created>2012-10-18</created><updated>2013-07-22</updated><authors><author><keyname>Ko</keyname><forenames>Yousun</forenames></author><author><keyname>Jung</keyname><forenames>Minyoung</forenames></author><author><keyname>Han</keyname><forenames>Yo-Sub</forenames></author><author><keyname>Burgstaller</keyname><forenames>Bernd</forenames></author></authors><title>A Speculative Parallel DFA Membership Test for Multicore, SIMD and Cloud
  Computing Environments</title><categories>cs.DC cs.FL</categories><doi>10.1007/s10766-013-0258-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present techniques to parallelize membership tests for Deterministic
Finite Automata (DFAs). Our method searches arbitrary regular expressions by
matching multiple bytes in parallel using speculation. We partition the input
string into chunks, match chunks in parallel, and combine the matching results.
Our parallel matching algorithm exploits structural DFA properties to minimize
the speculative overhead. Unlike previous approaches, our speculation is
failure-free, i.e., (1) sequential semantics are maintained, and (2)
speed-downs are avoided altogether. On architectures with a SIMD
gather-operation for indexed memory loads, our matching operation is fully
vectorized. The proposed load-balancing scheme uses an off-line profiling step
to determine the matching capacity of each par- ticipating processor. Based on
matching capacities, DFA matches are load-balanced on inhomogeneous parallel
architectures such as cloud computing environments. We evaluated our
speculative DFA membership test for a representative set of benchmarks from the
Perl-compatible Regular Expression (PCRE) library and the PROSITE protein
database. Evaluation was conducted on a 4 CPU (40 cores) shared-memory node of
the Intel Manycore Testing Lab (Intel MTL), on the Intel AVX2 SDE simulator for
8-way fully vectorized SIMD execution, and on a 20-node (288 cores) cluster on
the Amazon EC2 computing cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5117</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5117</id><created>2012-10-18</created><updated>2012-11-26</updated><authors><author><keyname>Burchardt</keyname><forenames>Harald</forenames></author><author><keyname>Sinanovic</keyname><forenames>Sinan</forenames></author><author><keyname>Bharucha</keyname><forenames>Zubin</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>Distributed and Autonomous Resource and Power Allocation for Wireless
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2013.053013.120916</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a distributed and autonomous technique for resource and power
allocation in orthogonal frequency division multiple access (OFDMA)
femto-cellular networks is presented. Here, resource blocks (RBs) and their
corresponding transmit powers are assigned to the user(s) in each cell
individually without explicit coordination between femto base stations (FBSs).
The &quot;allocatability&quot; of each resource is determined utilising only locally
available information of the following quantities: - the required rate of the
user; - the quality (i.e., strength) of the desired signal; - the
frequency-selective fading on each RB; and - the level of interference incident
on each RB. Using a fuzzy logic system, the time-averaged values of each of
these inputs are combined to determine which RBs are most suitable to be
allocated in a particular cell, i.e., which resources can be allocated such
that the user requested rate(s) in that cell are satisfied. Furthermore, link
adaptation (LA) is included, enabling users to adjust to varying channel
conditions. A comprehensive study of this system in a femto-cell environment is
performed, yielding system performance improvements in terms of throughput,
energy efficiency and coverage over state-of-the-art ICIC techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5118</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5118</id><created>2012-10-18</created><authors><author><keyname>Butler</keyname><forenames>Matthew</forenames></author><author><keyname>Kazakov</keyname><forenames>Dimitar</forenames></author></authors><title>Creating a level playing field for all symbols in a discretization</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In time series analysis research there is a strong interest in discrete
representations of real valued data streams. One approach that emerged over a
decade ago and is still considered state-of-the-art is the Symbolic Aggregate
Approximation algorithm. This discretization algorithm was the first symbolic
approach that mapped a real-valued time series to a symbolic representation
that was guaranteed to lower-bound Euclidean distance. The interest of this
paper concerns the SAX assumption of data being highly Gaussian and the use of
the standard normal curve to choose partitions to discretize the data. Though
not necessarily, but generally, and certainly in its canonical form, the SAX
approach chooses partitions on the standard normal curve that would produce an
equal probability for each symbol in a finite alphabet to occur. This procedure
is generally valid as a time series is normalized before the rest of the SAX
algorithm is applied. However there exists a caveat to this assumption of
equi-probability due to the intermediate step of Piecewise Aggregate
Approximation (PAA). What we will show in this paper is that when PAA is
applied the distribution of the data is indeed altered, resulting in a
shrinking standard deviation that is proportional to the number of points used
to create a segment of the PAA representation and the degree of
auto-correlation within the series. Data that exhibits statistically
significant auto-correlation is less affected by this shrinking distribution.
As the standard deviation of the data contracts, the mean remains the same,
however the distribution is no longer standard normal and therefore the
partitions based on the standard normal curve are no longer valid for the
assumption of equal probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5125</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5125</id><created>2012-10-18</created><updated>2016-01-28</updated><authors><author><keyname>Mehatari</keyname><forenames>Ranjit</forenames></author><author><keyname>banerjee</keyname><forenames>Anirban</forenames></author></authors><title>Effect on normalized graph Laplacian spectrum by motif attachment and
  duplication</title><categories>math.CO cs.DM</categories><journal-ref>Appl. Math. Comput. 261 (2015) 382-387</journal-ref><doi>10.1016/j.amc.2015.03.118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To some extent, graph evolutionary mechanisms can be explained by its
spectra. Here, we are interested in two graph operations, namely, motif
(subgraph) doubling and attachment that are biologically relevant. We
investigate how these two processes affect the spectrum of the normalized graph
Laplacian. A high (algebraic) multiplicity of the eigenvalues $1, 1\pm 0.5,
1\pm \sqrt{0.5}$ and others has been observed in the spectrum of many real
networks. We attempt to explain the production of distinct eigenvalues by motif
doubling and attachment. Results on the eigenvalue $1$ are discussed
separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5128</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5128</id><created>2012-10-18</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Qian</keyname><forenames>Weikang</forenames></author><author><keyname>Zhang</keyname><forenames>Shuchang</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author></authors><title>A Novel Learning Algorithm for Bayesian Network and Its Efficient
  Implementation on GPU</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational inference of causal relationships underlying complex networks,
such as gene-regulatory pathways, is NP-complete due to its combinatorial
nature when permuting all possible interactions. Markov chain Monte Carlo
(MCMC) has been introduced to sample only part of the combinations while still
guaranteeing convergence and traversability, which therefore becomes widely
used. However, MCMC is not able to perform efficiently enough for networks that
have more than 15~20 nodes because of the computational complexity. In this
paper, we use general purpose processor (GPP) and general purpose graphics
processing unit (GPGPU) to implement and accelerate a novel Bayesian network
learning algorithm. With a hash-table-based memory-saving strategy and a novel
task assigning strategy, we achieve a 10-fold acceleration per iteration than
using a serial GPP. Specially, we use a greedy method to search for the best
graph from a given order. We incorporate a prior component in the current
scoring function, which further facilitates the searching. Overall, we are able
to apply this system to networks with more than 60 nodes, allowing inferences
and modeling of bigger and more complex networks than current methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5135</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5135</id><created>2012-10-18</created><authors><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Mengying</forenames></author><author><keyname>Li</keyname><forenames>Menglu</forenames></author><author><keyname>Zhu</keyname><forenames>Qili</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author></authors><title>LSBN: A Large-Scale Bayesian Structure Learning Framework for Model
  Averaging</title><categories>cs.LG stat.ML</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The motivation for this paper is to apply Bayesian structure learning using
Model Averaging in large-scale networks. Currently, Bayesian model averaging
algorithm is applicable to networks with only tens of variables, restrained by
its super-exponential complexity. We present a novel framework, called
LSBN(Large-Scale Bayesian Network), making it possible to handle networks with
infinite size by following the principle of divide-and-conquer. The method of
LSBN comprises three steps. In general, LSBN first performs the partition by
using a second-order partition strategy, which achieves more robust results.
LSBN conducts sampling and structure learning within each overlapping community
after the community is isolated from other variables by Markov Blanket. Finally
LSBN employs an efficient algorithm, to merge structures of overlapping
communities into a whole. In comparison with other four state-of-art
large-scale network structure learning algorithms such as ARACNE, PC, Greedy
Search and MMHC, LSBN shows comparable results in five common benchmark
datasets, evaluated by precision, recall and f-score. What's more, LSBN makes
it possible to learn large-scale Bayesian structure by Model Averaging which
used to be intractable. In summary, LSBN provides an scalable and parallel
framework for the reconstruction of network structures. Besides, the complete
information of overlapping communities serves as the byproduct, which could be
used to mine meaningful clusters in biological networks, such as
protein-protein-interaction network or gene regulatory network, as well as in
social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5155</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5155</id><created>2012-10-18</created><updated>2013-12-09</updated><authors><author><keyname>Szil&#xe1;gyi</keyname><forenames>Zsolt</forenames></author></authors><title>Computation of Jeffrey-Kirwan residue using Gr\&quot;obner basis</title><categories>math.AC cs.DS</categories><comments>12 pages. Section 6 rewritten</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Jeffrey-Kirwan residue is a powerful tool for computation of intersection
numbers or volume of symplectic quotients. In this article, we give an
algorithm to compute it using Gr\&quot;obner bases. Our result is parallel to that
of Cattani-Dickenstein for Grothendieck residues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5156</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5156</id><created>2012-10-18</created><authors><author><keyname>Zhou</keyname><forenames>Hui</forenames></author><author><keyname>Hu</keyname><forenames>Donglin</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author><author><keyname>Agrawal</keyname><forenames>Prathima</forenames></author><author><keyname>Reddy</keyname><forenames>Saketh Anuma</forenames></author></authors><title>Cell Association and Handover Management in Femtocell Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the technology of femtocells is highly promising, many challenging
problems should be addressed before fully harvesting its potential. In this
paper, we investigate the problem of cell association and handover management
in femtocell networks. Two extreme cases for cell association are first
discussed and analyzed. Then we propose our algorithm to maximize network
capacity while achieving fairness among users. Based on this algorithm, we
further develop a handover algorithm to reduce the number of unnecessary
handovers using Bayesian estimation. The proposed handover algorithm is
demonstrated to outperform a heuristic scheme with considerable gains in our
simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5161</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5161</id><created>2012-10-18</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Ko&#x142;oszczyk</keyname><forenames>Bartosz</forenames></author></authors><title>Predicting Group Evolution in the Social Network</title><categories>cs.SI physics.soc-ph</categories><comments>Br\'odka P., Kazienko P, Ko{\l}oszczyk B., Predicting Group Evolution
  in the Social Network K. Aberer et al. (Eds.): SocInfo 2012, LNCS 7710, pp.
  54-67, 2012</comments><journal-ref>K. Aberer et al. (Eds.): SocInfo 2012, LNCS 7710, pp. 54-67, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Groups - social communities are important components of entire societies,
analysed by means of the social network concept. Their immanent feature is
continuous evolution over time. If we know how groups in the social network has
evolved we can use this information and try to predict the next step in the
given group evolution. In the paper, a new aproach for group evolution
prediction is presented and examined. Experimental studies on four evolving
social networks revealed that (i) the prediction based on the simple input
features may be very accurate, (ii) some classifiers are more precise than the
others and (iii) parameters of the group evolution extracion method
significantly influence the prediction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5167</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5167</id><created>2012-10-18</created><authors><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Influence of the Dynamic Social Network Timeframe Type and Size on the
  Group Evolution Discovery</title><categories>cs.SI physics.soc-ph</categories><comments>The 2012 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining, IEEE Computer Society, 2012, pp. 678-682</comments><journal-ref>IEEE Computer Society, 2012</journal-ref><doi>10.1109/ASONAM.2012.113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New technologies allow to store vast amount of data about users interaction.
From those data the social network can be created. Additionally, because
usually also time and dates of this activities are stored, the dynamic of such
network can be analysed by splitting it into many timeframes representing the
state of the network during specific period of time. One of the most
interesting issue is group evolution over time. To track group evolution the
GED method can be used. However, choice of the timeframe type and length might
have great influence on the method results. Therefore, in this paper, the
influence of timeframe type as well as timeframe length on the GED method
results is extensively analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5171</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5171</id><created>2012-10-18</created><authors><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Identification of Group Changes in Blogosphere</title><categories>cs.SI physics.soc-ph</categories><comments>The 2012 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining, IEEE Computer Society, 2012, pp. 1233-1238</comments><journal-ref>IEEE Computer Society, 2012</journal-ref><doi>10.1109/ASONAM.2012.207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses a problem of change identification in social group
evolution. A new SGCI method for discovering of stable groups was proposed and
compared with existing GED method. The experimental studies on a Polish
blogosphere service revealed that both methods are able to identify similar
evolution events even though both use different concepts. Some differences were
demonstrated as well
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5176</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5176</id><created>2012-10-18</created><authors><author><keyname>Fiol</keyname><forenames>M. A.</forenames></author><author><keyname>Vilaltella</keyname><forenames>J.</forenames></author></authors><title>A simple and fast heuristic algorithm for edge-coloring of graphs</title><categories>math.CO cs.DM</categories><msc-class>05C15, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple but empirically efficient heuristic algorithm for the edge-coloring
of graphs is presented. Its basic idea is the displacement of &quot;conflicts&quot;
(repeated colors in the edges incident to a vertex) along paths of adjacent
vertices whose incident edges are recolored by swapping alternating colors
(that is, doing a Kempe interchange). The results of performance tests on
random cubic and $\Delta$-regular graphs are presented, and a full
implementation of the algorithm is given to facilitate its use and the
reproducibility of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5180</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5180</id><created>2012-10-18</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Stawiak</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Shortest Path Discovery in the Multi-layered Social Network</title><categories>cs.SI physics.soc-ph</categories><comments>This is an extended version of the paper ASONAM 2011, IEEE Computer
  Society, pp. 497-501 DOI 10.1109/ASONAM.2011.67</comments><journal-ref>IEEE Computer Society, 2011</journal-ref><doi>10.1109/ASONAM.2011.67</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-layered social networks consist of the fixed set of nodes linked by
multiple connections. These connections may be derived from different types of
user activities logged in the IT system. To calculate any structural measures
for multi-layered networks this multitude of relations should be coped with in
the parameterized way. Two separate algorithms for evaluation of shortest paths
in the multi-layered social network are proposed in the paper. The first one is
based on pre-processing - aggregation of multiple links into single
multi-layered edges, whereas in the second approach, many edges are processed
'on the fly' in the middle of path discovery. Experimental studies carried out
on the DBLP database converted into the multi-layered social network are
presented as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5183</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5183</id><created>2012-10-18</created><updated>2013-04-16</updated><authors><author><keyname>Rosati</keyname><forenames>Stefano</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Butussi</keyname><forenames>Matteo</forenames></author><author><keyname>Rimoldi</keyname><forenames>Bixio</forenames></author></authors><title>LLR Compression for BICM Systems Using Large Constellations</title><categories>cs.IT math.IT</categories><comments>12 pages, 11 figures, submitted to IEEE Trans. on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital video broadcasting (DVB-C2) and other modern communication standards
increase diversity by means of a symbol-level interleaver that spans over
several codewords. De-interleaving at the receiver requires a large memory,
which has a significant impact on the implementation cost. In this paper, we
propose a technique that reduces the de-interleaver memory size. By quantizing
log-likelihood ratios with bit-specific quantizers and compressing the
quantized output, we can significantly reduce the memory size with a negligible
increase in computational complexity. Both the quantizer and compressor are
designed via a GMI-based maximization procedure. For a typical DVB-C2 scenario,
numerical results show that the proposed solution enables a memory saving up to
30%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5184</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5184</id><created>2012-10-18</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Skibicki</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Musia&#x142;</keyname><forenames>Katarzyna</forenames></author></authors><title>A degree centrality in multi-layered social network</title><categories>cs.SI physics.soc-ph</categories><comments>Brodka, P.; Skibicki, K.; Kazienko, P.; Musial, K.; &quot;A degree
  centrality in multi-layered social network,&quot; Computational Aspects of Social
  Networks (CASoN), 2011 International Conference on, vol., no., pp.237-242,
  19-21 Oct. 2011 doi: 10.1109/CASON.2011.6085951;
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6085951&amp;isnumber=6085907</comments><doi>10.1109/CASON.2011.6085951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-layered social networks reflect complex relationships existing in
modern interconnected IT systems. In such a network each pair of nodes may be
linked by many edges that correspond to different communication or
collaboration user activities. Multi-layered degree centrality for
multi-layered social networks is presented in the paper. Experimental studies
were carried out on data collected from the real Web 2.0 site. The
multi-layered social network extracted from this data consists of ten distinct
layers and the network analysis was performed for different degree centralities
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5196</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5196</id><created>2012-10-18</created><authors><author><keyname>Foygel</keyname><forenames>Rina</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Matrix reconstruction with the local max norm</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new family of matrix norms, the &quot;local max&quot; norms,
generalizing existing methods such as the max norm, the trace norm (nuclear
norm), and the weighted or smoothed weighted trace norms, which have been
extensively used in the literature as regularizers for matrix reconstruction
problems. We show that this new family can be used to interpolate between the
(weighted or unweighted) trace norm and the more conservative max norm. We test
this interpolation on simulated data and on the large-scale Netflix and
MovieLens ratings data, and find improved accuracy relative to the existing
matrix norms. We also provide theoretical results showing learning guarantees
for some of the new norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5198</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5198</id><created>2012-10-18</created><authors><author><keyname>Shayovitz</keyname><forenames>Shachar</forenames></author><author><keyname>Raphaeli</keyname><forenames>Dan</forenames></author></authors><title>Multiple Hypotheses Iterative Decoding of LDPC in the Presence of Strong
  Phase Noise</title><categories>cs.IT math.IT</categories><comments>accepted to 2012 IEEE 27-th Convention of Electrical and Electronics
  Engineers in Israel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many satellite communication systems operating today employ low cost
upconverters or downconverters which create phase noise. This noise can
severely limit the information rate of the system and pose a serious challenge
for the detection systems. Moreover, simple solutions for phase noise tracking
such as PLL either require low phase noise or otherwise require many pilot
symbols which reduce the effective data rate. In the last decade we have
witnessed a significant amount of research done on joint estimation and
decoding of phase noise and coded information. These algorithms are based on
the factor graph representation of the joint posterior distribution. The
framework proposed in [5], allows the design of efficient message passing
algorithms which incorporate both the code graph and the channel graph. The use
of LDPC or Turbo decoders, as part of iterative message passing schemes, allows
the receiver to operate in low SNR regions while requiring less pilot symbols.
In this paper we propose a multiple hypotheses algorithm for joint detection
and estimation of coded information in a strong phase noise channel. We also
present a low complexity mixture reduction procedure which maintains very good
accuracy for the belief propagation messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5215</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5215</id><created>2012-10-18</created><updated>2014-07-08</updated><authors><author><keyname>Schl&#xe4;pfer</keyname><forenames>Markus</forenames></author><author><keyname>Bettencourt</keyname><forenames>Luis M. A.</forenames></author><author><keyname>Grauwin</keyname><forenames>Sebastian</forenames></author><author><keyname>Raschke</keyname><forenames>Mathias</forenames></author><author><keyname>Claxton</keyname><forenames>Rob</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>West</keyname><forenames>Geoffrey B.</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>The scaling of human interactions with city size</title><categories>physics.soc-ph cs.SI physics.data-an</categories><journal-ref>J. R. Soc. Interface 11 (98), 20130789 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The size of cities is known to play a fundamental role in social and economic
life. Yet, its relation to the structure of the underlying network of human
interactions has not been investigated empirically in detail. In this paper, we
map society-wide communication networks to the urban areas of two European
countries. We show that both the total number of contacts and the total
communication activity grow superlinearly with city population size, according
to well-defined scaling relations and resulting from a multiplicative increase
that affects most citizens. Perhaps surprisingly, however, the probability that
an individual's contacts are also connected with each other remains largely
unaffected. These empirical results predict a systematic and scale-invariant
acceleration of interaction-based spreading phenomena as cities get bigger,
which is numerically confirmed by applying epidemiological models to the
studied networks. Our findings should provide a microscopic basis towards
understanding the superlinear increase of different socioeconomic quantities
with city size, that applies to almost all urban systems and includes, for
instance, the creation of new inventions or the prevalence of certain
contagious diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5219</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5219</id><created>2012-10-18</created><authors><author><keyname>Ellings&#xe6;ter</keyname><forenames>Brage</forenames></author><author><keyname>Maseng</keyname><forenames>Torleiv</forenames></author></authors><title>The Domino Effect in Decentralized Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convergence of resource allocation algorithms is well covered in the
literature as convergence to a steady state is important due to stability and
performance. However, research is lacking when it comes to the propagation of
change that occur in a network due to new nodes arriving or old nodes leaving
or updating their allocation. As change can propagate through the network in a
manner similar to how domino pieces falls, we call this propagation of change
the domino effect. In this paper we investigate how change at one node can
affect other nodes for a simple power control algorithm. We provide analytical
results from a deterministic network as well as a Poisson distributed network
through percolation theory and provide simulation results that highlight some
aspects of the domino effect. The difficulty of mitigating this domino effect
lies in the fact that to avoid it, one needs to have a margin of tolerance for
changes in the network. However, a high margin leads to poor system performance
in a steady-state and therefore one has to consider a trade-off between
performance and propagation of change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5222</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5222</id><created>2012-10-18</created><authors><author><keyname>Babb</keyname><forenames>Joseph</forenames></author><author><keyname>Lee</keyname><forenames>Joohyung</forenames></author></authors><title>Module Theorem for The General Theory of Stable Models</title><categories>cs.AI cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S1471068412000269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The module theorem by Janhunen et al. demonstrates how to provide a modular
structure in answer set programming, where each module has a well-defined
input/output interface which can be used to establish the compositionality of
answer sets. The theorem is useful in the analysis of answer set programs, and
is a basis of incremental grounding and reactive answer set programming. We
extend the module theorem to the general theory of stable models by Ferraris et
al. The generalization applies to non-ground logic programs allowing useful
constructs in answer set programming, such as choice rules, the count
aggregate, and nested expressions. Our extension is based on relating the
module theorem to the symmetric splitting theorem by Ferraris et al. Based on
this result, we reformulate and extend the theory of incremental answer set
computation to a more general class of programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5224</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5224</id><created>2012-10-18</created><updated>2012-10-31</updated><authors><author><keyname>Texier</keyname><forenames>Jose</forenames></author><author><keyname>De Giusti</keyname><forenames>Marisa</forenames></author><author><keyname>Oviedo</keyname><forenames>Nestor</forenames></author><author><keyname>Villarreal</keyname><forenames>Gonzalo</forenames></author><author><keyname>Lira</keyname><forenames>Ariel</forenames></author></authors><title>Use of Repositories and its Significance for Engineering Education / El
  Uso de Repositorios y su Importancia para la Educaci\'on en Ingenier\'ia</title><categories>cs.DL</categories><comments>This paper is a preprint in WEEF (World Engineering Education Forum)
  2012. http://www.weef2012.edu.ar/home1.php. ISBN: 978-987-1896-03-5</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Institutional repositories are deposits of different types of digital files
for access, disseminate and preserve them. This paper aims to explain the
importance of repositories in the academic field of engineering as a way to
democratize knowledge by teachers, researchers and students to contribute to
social and human development. These repositories, usually framed in the Open
Access Initiative, allow to ensure access free and open (unrestricted legal and
economic) to different sectors of society and, thus, can make use of the
services they offer. Finally, that repositories are evolving in the academic
and scientific, and different disciplines of engineering should be prepared to
provide a range of services through these systems to society of today and
tomorrow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5227</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5227</id><created>2012-10-18</created><authors><author><keyname>Miller</keyname><forenames>Gary</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Approximate Maximum Flow on Separable Undirected Graphs</title><categories>cs.DS</categories><comments>to appear in SODA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present faster algorithms for approximate maximum flow in undirected
graphs with good separator structures, such as bounded genus, minor free, and
geometric graphs. Given such a graph with $n$ vertices, $m$ edges along with a
recursive $\sqrt{n}$-vertex separator structure, our algorithm finds an
$1-\epsilon$ approximate maximum flow in time $\tilde{O}(m^{6/5}
\poly{\epsilon^{-1}})$, ignoring poly-logarithmic terms. Similar speedups are
also achieved for separable graphs with larger size separators albeit with
larger run times. These bounds also apply to image problems in two and three
dimensions.
  Key to our algorithm is an intermediate problem that we term grouped $L_2$
flow, which exists between maximum flows and electrical flows. Our algorithm
also makes use of spectral vertex sparsifiers in order to remove vertices while
preserving the energy dissipation of electrical flows. We also give faster
spectral vertex sparsification algorithms on well separated graphs, which may
be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5232</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5232</id><created>2012-10-18</created><authors><author><keyname>Cai</keyname><forenames>Yiqing</forenames></author><author><keyname>Ghrist</keyname><forenames>Robert</forenames></author></authors><title>Cyclic Network Automata and Cohomological Waves</title><categories>math.AT cs.NI nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a dynamic coverage problem for sensor networks that are
sufficiently dense but not localized. Only a small fraction of sensors may be
in an awake state at any given time. The goal is to find a decentralized
protocol for establishing dynamic, sweeping barriers of awake-state sensors.
Following Baryshnikov-Coffman-Kwak, we use network cyclic cellular automata to
generate waves. This paper gives a rigorous analysis of network-based cyclic
cellular automata in the context of a system of narrow hallways and shows that
waves of awake-state nodes turn corners and automatically solve
pusuit/evasion-type problems without centralized coordination.
  As a corollary of this work, we unearth some interesting topological
interpretations of features previously observed in cyclic cellular automata
(CCA). By considering CCA over networks and completing to simplicial complexes,
we induce dynamics on the higher-dimensional complex. In this setting, waves
are seen to be generated by topological defects with a nontrivial degree (or
winding number). The simplicial complex has the topological type of the
underlying map of the workspace (a subset of the plane), and the resulting
waves can be classified cohomologically. This allows one to &quot;program&quot; pulses in
the sensor network according to cohomology class. We give a realization theorem
for such pulse waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5240</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5240</id><created>2012-10-18</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Tracking Group Evolution in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Br\'odka P., Saganowski P., Kazienko P.: Tracking Group Evolution in
  Social Networks. SocInfo'11, The Third International Conference on Social
  Informatics, Lecture Notes in Artificial Intelligence LNAI, Springer, 2011,
  pp. 316-319. To see extended version of this paper check
  http://arxiv.org/abs/1207.4297. arXiv admin note: substantial text overlap
  with arXiv:1210.5167</comments><journal-ref>Lecture Notes in Artificial Intelligence LNAI , Springer, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Easy access and vast amount of data, especially from long period of time,
allows to divide social network into timeframes and create temporal social
network. Such network enables to analyse its dynamics. One aspect of the
dynamics is analysis of social communities evolution, i.e., how particular
group changes over time. To do so, the complete group evolution history is
needed. That is why in this paper the new method for group evolution extraction
called GED is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5262</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5262</id><created>2012-10-18</created><authors><author><keyname>Kerr</keyname><forenames>Colin A.</forenames></author></authors><title>Hands-Off Spreadsheets</title><categories>cs.SE</categories><comments>10 pages, 5 colour figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2012</comments><proxy>Grenville Croll</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wealth of functionality in the Excel software package means it can go
beyond use as a static evaluator of predefined cell formulae, to be used
actively in manipulating and transforming data. Due to human error it is
impossible to ensure a process like this is always error free, and frequently
the sequence of actions is recorded only in the operator's head. If done
regularly by highly paid staff it will be expensive. This paper applies to
those spreadsheets which involve significant operator intervention, describes a
method that has been used to improve reliability and efficiency, and reports on
how it has worked in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5268</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5268</id><created>2012-10-18</created><updated>2014-11-23</updated><authors><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Diffusion of Lexical Change in Social Media</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>preprint of PLOS-ONE paper from November 2014; PLoS ONE 9(11) e113114</comments><doi>10.1371/journal.pone.0113114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-mediated communication is driving fundamental changes in the nature
of written language. We investigate these changes by statistical analysis of a
dataset comprising 107 million Twitter messages (authored by 2.7 million unique
user accounts). Using a latent vector autoregressive model to aggregate across
thousands of words, we identify high-level patterns in diffusion of linguistic
change over the United States. Our model is robust to unpredictable changes in
Twitter's sampling rate, and provides a probabilistic characterization of the
relationship of macro-scale linguistic influence to a set of demographic and
geographic predictors. The results of this analysis offer support for prior
arguments that focus on geographical proximity and population size. However,
demographic similarity -- especially with regard to race -- plays an even more
central role, as cities with similar racial demographics are far more likely to
share linguistic influence. Rather than moving towards a single unified
&quot;netspeak&quot; dialect, language evolution in computer-mediated communication
reproduces existing fault lines in spoken American English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5287</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5287</id><created>2012-10-18</created><authors><author><keyname>Sahai</keyname><forenames>Amit</forenames></author><author><keyname>Waters</keyname><forenames>Brent</forenames></author></authors><title>Attribute-Based Encryption for Circuits from Multilinear Maps</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we provide the first construction of Attribute-Based Encryption
(ABE) for general circuits. Our construction is based on the existence of
multilinear maps. We prove selective security of our scheme in the standard
model under the natural multilinear generalization of the BDDH assumption. Our
scheme achieves both Key-Policy and Ciphertext-Policy variants of ABE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5288</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5288</id><created>2012-10-18</created><updated>2013-04-25</updated><authors><author><keyname>Durak</keyname><forenames>Nurcan</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>A Scalable Null Model for Directed Graphs Matching All Degree
  Distributions: In, Out, and Reciprocal</title><categories>cs.SI physics.soc-ph</categories><comments>Camera ready version for IEEE Workshop on Network Science; fixed some
  typos in table</comments><journal-ref>Proceedings of IEEE 2013 2nd International Network Science
  Workshop (NSW 2013), pp. 22--30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Degree distributions are arguably the most important property of real world
networks. The classic edge configuration model or Chung-Lu model can generate
an undirected graph with any desired degree distribution. This serves as a good
null model to compare algorithms or perform experimental studies. Furthermore,
there are scalable algorithms that implement these models and they are
invaluable in the study of graphs. However, networks in the real-world are
often directed, and have a significant proportion of reciprocal edges. A
stronger relation exists between two nodes when they each point to one another
(reciprocal edge) as compared to when only one points to the other (one-way
edge). Despite their importance, reciprocal edges have been disregarded by most
directed graph models.
  We propose a null model for directed graphs inspired by the Chung-Lu model
that matches the in-, out-, and reciprocal-degree distributions of the real
graphs. Our algorithm is scalable and requires $O(m)$ random numbers to
generate a graph with $m$ edges. We perform a series of experiments on real
datasets and compare with existing graph models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5290</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5290</id><created>2012-10-18</created><updated>2013-07-27</updated><authors><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author><author><keyname>Mudunuru</keyname><forenames>M. K.</forenames></author><author><keyname>Valocchi</keyname><forenames>A. J.</forenames></author></authors><title>A numerical framework for diffusion-controlled bimolecular-reactive
  systems to enforce maximum principles and non-negative constraint</title><categories>cs.NA cs.CE</categories><doi>10.1016/j.jcp.2013.07.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel computational framework for diffusive-reactive systems
that satisfies the non-negative constraint and maximum principles on general
computational grids. The governing equations for the concentration of reactants
and product are written in terms of tensorial diffusion-reaction equations. %
We restrict our studies to fast irreversible bimolecular reactions. If one
assumes that the reaction is diffusion-limited and all chemical species have
the same diffusion coefficient, one can employ a linear transformation to
rewrite the governing equations in terms of invariants, which are unaffected by
the reaction. This results in two uncoupled tensorial diffusion equations in
terms of these invariants, which are solved using a novel non-negative solver
for tensorial diffusion-type equations. The concentrations of the reactants and
the product are then calculated from invariants using algebraic manipulations.
The novel aspect of the proposed computational framework is that it will always
produce physically meaningful non-negative values for the concentrations of all
chemical species. Several representative numerical examples are presented to
illustrate the robustness, convergence, and the numerical performance of the
proposed computational framework. We will also compare the proposed framework
with other popular formulations. In particular, we will show that the Galerkin
formulation (which is the standard single-field formulation) does not produce
reliable solutions, and the reason can be attributed to the fact that the
single-field formulation does not guarantee non-negative solutions. We will
also show that the clipping procedure (which produces non-negative solutions
but is considered as a variational crime) does not give accurate results when
compared with the proposed computational framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5292</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5292</id><created>2012-10-18</created><updated>2014-05-15</updated><authors><author><keyname>Joo</keyname><forenames>Hyun-Seung</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Low-Complexity Demodulation for Interleaved OFDMA Downlink System Using
  Circular Convolution</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new low-complexity demodulation scheme is proposed for
interleaved orthogonal frequency division multiple access (OFDMA) downlink
system with N subcarriers and M users using circular convolution. In the
proposed scheme, each user's signal is extracted from the received interleaved
OFDMA signal of M users by using circular convolution in the time domain and
then fast Fourier transformed in the reduced size N over M. It is shown that
the computational complexity of the proposed scheme for the interleaved OFDMA
downlink system is much less than that of the conventional one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5297</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5297</id><created>2012-10-18</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author></authors><title>Adaptive Differential Feedback in Time-Varying Multiuser MIMO Channels</title><categories>cs.IT math.IT</categories><comments>IEEE 22nd International Conference on Personal, Indoor and Mobile
  Radio Communications (2011)</comments><journal-ref>IEEE 22nd International Conference on Personal, Indoor and Mobile
  Radio Communications (2011), page 2055-2059</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of a time-varying multiuser multiple-input-multiple-output
(MIMO) system, we design recursive least squares based adaptive predictors and
differential quantizers to minimize the sum mean squared error of the overall
system. Using the fact that the scalar entries of the left singular matrix of a
Gaussian MIMO channel becomes almost Gaussian distributed even for a small
number of transmit antennas, we perform adaptive differential quantization of
the relevant singular matrix entries. Compared to the algorithms in the
existing differential feedback literature, our proposed quantizer provides
three advantages: first, the controller parameters are flexible enough to adapt
themselves to different vehicle speeds; second, the model is backward adaptive
i.e., the base station and receiver can agree upon the predictor and variance
estimator coefficients without explicit exchange of the parameters; third, it
can accurately model the system even when the correlation between two
successive channel samples becomes as low as 0.05. Our simulation results show
that our proposed method can reduce the required feedback by several kilobits
per second for vehicle speeds up to 20 km/h (channel tracker) and 10 km/h
(singular vector tracker). The proposed system also outperforms a fixed
quantizer, with same feedback overhead, in terms of bit error rate up to 30
km/h.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5307</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5307</id><created>2012-10-18</created><authors><author><keyname>Duck</keyname><forenames>Gregory J.</forenames></author></authors><title>SMCHR: Satisfiability Modulo Constraint Handling Rules</title><categories>cs.PL</categories><comments>International Conference on Logic Programming 2012</comments><journal-ref>Theory and Practice of Logic Programming, 12(4-5):601-618, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Handling Rules (CHRs) are a high-level rule-based programming
language for specification and implementation of constraint solvers. CHR
manipulates a global store representing a flat conjunction of constraints. By
default, CHR does not support goals with a more complex propositional structure
including disjunction, negation, etc., or CHR relies on the host system to
provide such features. In this paper we introduce Satisfiability Modulo
Constraint Handling Rules (SMCHR): a tight integration of CHR with a modern
Boolean Satisfiability (SAT) solver for quantifier-free formulae with an
arbitrary propositional structure. SMCHR is essentially a Satisfiability Modulo
Theories (SMT) solver where the theory T is implemented in CHR. The execution
algorithm of SMCHR is based on lazy clause generation, where a new clause for
the SAT solver is generated whenever a rule is applied. We shall also explore
the practical aspects of building an SMCHR system, including extending a
&quot;built-in&quot; constraint solver supporting equality with unification and
justifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5314</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5314</id><created>2012-10-19</created><updated>2012-10-27</updated><authors><author><keyname>Jose</keyname><forenames>Renu</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Maximum Likelihood Algorithms for Joint Estimation of Synchronization
  Impairments and Channel in MIMO-OFDM System</title><categories>cs.IT math.IT</categories><comments>18 pages, 5 figures, Submitted to IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum Likelihood (ML) algorithms, for the joint estimation of
synchronization impairments and channel in Multiple Input Multiple
Output-Orthogonal Frequency Division Multiplexing (MIMO-OFDM) system, are
investigated in this work. A system model that takes into account the effects
of carrier frequency offset, sampling frequency offset, symbol timing error,
and channel impulse response is formulated. Cram\'{e}r-Rao Lower Bounds for the
estimation of continuous parameters are derived, which show the coupling effect
among different impairments and the significance of the joint estimation. We
propose an ML algorithm for the estimation of synchronization impairments and
channel together, using grid search method. To reduce the complexity of the
joint grid search in ML algorithm, a Modified ML (MML) algorithm with multiple
one-dimensional searches is also proposed. Further, a Stage-wise ML (SML)
algorithm using existing algorithms, which estimate fewer number of parameters,
is also proposed. Performance of the estimation algorithms is studied through
numerical simulations and it is found that the proposed ML and MML algorithms
exhibit better performance than SML algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5321</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5321</id><created>2012-10-19</created><authors><author><keyname>Ohnishi</keyname><forenames>Koji</forenames></author></authors><title>The origin of Mayan languages from Formosan language group of
  Austronesian</title><categories>cs.CL q-bio.PE</categories><comments>6 pages, 1 Table. Proceedings of the 145th Annual Meeting of the
  Linguistic Society of Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basic body-part names (BBPNs) were defined as body-part names in Swadesh
basic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensively
searched for, by comparing with non-MY vocabulary, including ca.1300 basic
words of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs)
in non-MY are listed in Table 1, as classified by language groups to which most
similar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23
mutually unrelated CG-items, of which 17.5 CG-items have their MSCs in
Austronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5,
which consists of 10.33 MSCs in Formosan, 1.83 MSCs in Western
Malayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic
[i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs for
language (sub)groups are also listed in the underline portion of every section
of (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1)
using [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed in
Formosan in significantly higher frequency (P &lt; 0.001) than in other subgroups
of AN, as well as than in non-AN languages. MY is thus concluded to have been
derived from Formosan of AN. Eskimo shows some BBPN similarities to FORM and
MY.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5323</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5323</id><created>2012-10-19</created><updated>2013-07-17</updated><authors><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>The performance of orthogonal multi-matching pursuit under RIP</title><categories>cs.IT cs.LG math.IT math.NA</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The orthogonal multi-matching pursuit (OMMP) is a natural extension of
orthogonal matching pursuit (OMP). We denote the OMMP with the parameter $M$ as
OMMP(M) where $M\geq 1$ is an integer. The main difference between OMP and
OMMP(M) is that OMMP(M) selects $M$ atoms per iteration, while OMP only adds
one atom to the optimal atom set. In this paper, we study the performance of
orthogonal multi-matching pursuit (OMMP) under RIP. In particular, we show
that, when the measurement matrix A satisfies $(9s, 1/10)$-RIP, there exists an
absolutely constant $M_0\leq 8$ so that OMMP(M_0) can recover $s$-sparse signal
within $s$ iterations. We furthermore prove that, for slowly-decaying
$s$-sparse signal, OMMP(M) can recover s-sparse signal within $O(\frac{s}{M})$
iterations for a large class of $M$. In particular, for $M=s^a$ with $a\in
[0,1/2]$, OMMP(M) can recover slowly-decaying $s$-sparse signal within
$O(s^{1-a})$ iterations. The result implies that OMMP can reduce the
computational complexity heavily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5338</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5338</id><created>2012-10-19</created><updated>2013-02-01</updated><authors><author><keyname>Furtlehner</keyname><forenames>Cyril</forenames></author><author><keyname>Han</keyname><forenames>Yufei</forenames></author><author><keyname>Lasgouttes</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Martin</keyname><forenames>Victorin</forenames></author></authors><title>Pairwise MRF Calibration by Perturbation of the Bethe Reference Point</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML</categories><comments>54 pages, 8 figure. section 5 and refs added in V2</comments><report-no>Inria RR-8059</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate different ways of generating approximate solutions to the
pairwise Markov random field (MRF) selection problem. We focus mainly on the
inverse Ising problem, but discuss also the somewhat related inverse Gaussian
problem because both types of MRF are suitable for inference tasks with the
belief propagation algorithm (BP) under certain conditions. Our approach
consists in to take a Bethe mean-field solution obtained with a maximum
spanning tree (MST) of pairwise mutual information, referred to as the
\emph{Bethe reference point}, for further perturbation procedures. We consider
three different ways following this idea: in the first one, we select and
calibrate iteratively the optimal links to be added starting from the Bethe
reference point; the second one is based on the observation that the natural
gradient can be computed analytically at the Bethe point; in the third one,
assuming no local field and using low temperature expansion we develop a dual
loop joint model based on a well chosen fundamental cycle basis. We indeed
identify a subclass of planar models, which we refer to as \emph{Bethe-dual
graph models}, having possibly many loops, but characterized by a singly
connected dual factor graph, for which the partition function and the linear
response can be computed exactly in respectively O(N) and $O(N^2)$ operations,
thanks to a dual weight propagation (DWP) message passing procedure that we set
up. When restricted to this subclass of models, the inverse Ising problem being
convex, becomes tractable at any temperature. Experimental tests on various
datasets with refined $L_0$ or $L_1$ regularization procedures indicate that
these approaches may be competitive and useful alternatives to existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5342</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5342</id><created>2012-10-19</created><authors><author><keyname>Sinha</keyname><forenames>Rahul</forenames></author><author><keyname>Arora</keyname><forenames>Sonika</forenames></author></authors><title>Design &amp; Simulation of 128x Interpolator Filter</title><categories>cs.ET cs.AR</categories><comments>10 Pages, 11 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design consideration and simulation of interpolator
of OSR 128. The proposed structure uses the half band filers &amp; Comb/Sinc
filter. Experimental result shows that proposed interpolator achieves the
design specification, and also has good noise rejection capabilities. The
interpolator accepts the input at 44.1 kHz for applications like CD &amp; DVD
audio. The interpolation filter can be applied to the delta sigma DAC. The
related work is done with the MATLAB &amp; XILINX ISE simulators. The maximum
operating frequency is achieved as 34.584 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5363</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5363</id><created>2012-10-19</created><authors><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Computing cutwidth and pathwidth of semi-complete digraphs via degree
  orderings</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notions of cutwidth and pathwidth of digraphs play a central role in the
containment theory for tournaments, or more generally semi-complete digraphs,
developed in a recent series of papers by Chudnovsky, Fradkin, Kim, Scott, and
Seymour [2, 3, 4, 8, 9, 11]. In this work we introduce a new approach to
computing these width measures on semi-complete digraphs, via degree orderings.
Using the new technique we are able to reprove the main results of [2, 9] in a
unified and significantly simplified way, as well as obtain new results. First,
we present polynomial-time approximation algorithms for both cutwidth and
pathwidth, faster and simpler than the previously known ones; the most
significant improvement is in case of pathwidth, where instead of previously
known O(OPT)-approximation in fixed-parameter tractable time [6] we obtain a
constant-factor approximation in polynomial time. Secondly, by exploiting the
new set of obstacles for cutwidth and pathwidth, we show that topological
containment and immersion in semi-complete digraphs can be tested in
single-exponential fixed-parameter tractable time. Finally, we present how the
new approach can be used to obtain exact fixed-parameter tractable algorithms
for cutwidth and pathwidth, with single- exponential running time dependency on
the optimal width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5374</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5374</id><created>2012-10-19</created><authors><author><keyname>Mtibaa</keyname><forenames>Sabri</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>Timing Constraints Support on Petri-Net Model for Healthcare System
  Design</title><categories>cs.SE cs.SY</categories><comments>In Journal of Computing, September 2012 issue (Volume 4, Issue 9)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The worldwide healthcare organizations are facing a number of daunting
challenges forcing systems to benefit from modern technologies and telecom
capabilities. Hence, systems evolution through extension of the existing
information technology infrastructure becomes one of the most challenging
aspects of healthcare. In this paper, we present a newly architecture for
evolving healthcare systems towards a service-oriented architecture. Since
healthcare process exists in temporal context, timing constraints
satisfiability verification techniques are growing to enable designers to test
and repair design errors. Thanks to Hierarchical Timed Predicate Petri-Net
based conceptual framework, desirable properties such as deadlock free and safe
as well as timing constraints satisfiability can be easily checked by designer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5386</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5386</id><created>2012-10-19</created><authors><author><keyname>Guillaume</keyname><forenames>Romain</forenames></author><author><keyname>Kobylanski</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>A Robust Lot Sizing Problem with Ill-known Demands</title><categories>cs.OH</categories><journal-ref>Fuzzy Sets and Systems 206 (2012) 39-57</journal-ref><doi>10.1016/j.fss.2012.01.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with a lot sizing problem with ill-known demands modeled by
fuzzy intervals whose membership functions are possibility distributions for
the values of the uncertain demands. Optimization criteria, in the setting of
possibility theory, that lead to choose robust production plans under fuzzy
demands are given. Some algorithms for determining optimal robust production
plans with respect to the proposed criteria, and for evaluating production
plans are provided. Some computational experiments are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5393</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5393</id><created>2012-10-19</created><authors><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Becker</keyname><forenames>Monique</forenames></author></authors><title>Enhancing Information Dissemination in Dynamic Wireless Network using
  Stability and Beamforming</title><categories>cs.NI</categories><comments>Submitted to Ad hoc Network journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility causes network structures to change. In PSNs where underlying
network structure is changing rapidly, we are interested in studying how
information dissemination can be enhanced in a sparse disconnected network
where nodes lack the global knowledge about the network. We use beamforming to
study the enhancement in the information dissemination process. In order to
identify potential beamformers and nodes to which beams should be directed we
use the concept of stability. We first predict the stability of a node in the
dynamic network using truncated levy walk nature of jump lengths of human
mobility and then use this measure to identify beamforming nodes and the nodes
to which the beams are directed. We also develop our algorithm such that it
does not require any global knowledge about the network and works in a
distributed manner. We also show the effect of various parameters such as
number of sources, number of packets, mobility parameters, antenna parameters,
type of stability used and density of the network on information dissemination
in the network. We validate our findings with three validation model, no
beamforming, beamforming using different stability measure and when no
stability measure is associated but same number of node beamform and the
selection of the beamforming nodes is random. Our simulation results show that
information dissemination can be enhanced using our algorithm over other
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5394</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5394</id><created>2012-10-19</created><authors><author><keyname>Amini</keyname><forenames>Arash</forenames></author><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author><author><keyname>Bostan</keyname><forenames>Emrah</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Bayesian Estimation for Continuous-Time Sparse Stochastic Processes</title><categories>cs.LG</categories><comments>To appear in IEEE TSP</comments><doi>10.1109/TSP.2012.2226446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider continuous-time sparse stochastic processes from which we have
only a finite number of noisy/noiseless samples. Our goal is to estimate the
noiseless samples (denoising) and the signal in-between (interpolation
problem).
  By relying on tools from the theory of splines, we derive the joint a priori
distribution of the samples and show how this probability density function can
be factorized. The factorization enables us to tractably implement the maximum
a posteriori and minimum mean-square error (MMSE) criteria as two statistical
approaches for estimating the unknowns. We compare the derived statistical
methods with well-known techniques for the recovery of sparse signals, such as
the $\ell_1$ norm and Log ($\ell_1$-$\ell_0$ relaxation) regularization
methods. The simulation results show that, under certain conditions, the
performance of the regularization techniques can be very close to that of the
MMSE estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5403</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5403</id><created>2012-10-19</created><authors><author><keyname>Schwarte</keyname><forenames>Andreas</forenames></author><author><keyname>Haase</keyname><forenames>Peter</forenames></author><author><keyname>Schmidt</keyname><forenames>Michael</forenames></author><author><keyname>Hose</keyname><forenames>Katja</forenames></author><author><keyname>Schenkel</keyname><forenames>Ralf</forenames></author></authors><title>An Experience Report of Large Scale Federations</title><categories>cs.DB</categories><acm-class>H.2.3; H.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an experimental study of large-scale RDF federations on top of the
Bio2RDF data sources, involving 29 data sets with more than four billion RDF
triples deployed in a local federation. Our federation is driven by FedX, a
highly optimized federation mediator for Linked Data. We discuss design
decisions, technical aspects, and experiences made in setting up and optimizing
the Bio2RDF federation, and present an exhaustive experimental evaluation of
the federation scenario. In addition to a controlled setting with local
federation members, we study implications arising in a hybrid setting, where
local federation members interact with remote federation members exhibiting
higher network latency. The outcome demonstrates the feasibility of federated
semantic data management in general and indicates remaining bottlenecks and
research opportunities that shall serve as a guideline for future work in the
area of federated semantic data processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5424</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5424</id><created>2012-10-19</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Balasubramanian</keyname><forenames>Shantharam</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author><author><keyname>Kompella</keyname><forenames>Sastry</forenames></author></authors><title>Implementation of Distributed Time Exchange Based Cooperative Forwarding</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted in 2012 Military Communications Conference</comments><journal-ref>Military Communications Conference 2012 (Page 1-6)</journal-ref><doi>10.1109/MILCOM.2012.6415742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design and implement time exchange (TE) based cooperative
forwarding where nodes use transmission time slots as incentives for relaying.
We focus on distributed joint time slot exchange and relay selection in the sum
goodput maximization of the overall network. We formulate the design objective
as a mixed integer nonlinear programming (MINLP) problem and provide a
polynomial time distributed solution of the MINLP. We implement the designed
algorithm in the software defined radio enabled USRP nodes of the ORBIT indoor
wireless testbed. The ORBIT grid is used as a global control plane for exchange
of control information between the USRP nodes. Experimental results suggest
that TE can significantly increase the sum goodput of the network. We also
demonstrate the performance of a goodput optimization algorithm that is
proportionally fair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5443</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5443</id><created>2012-10-19</created><authors><author><keyname>van Renesse</keyname><forenames>Robbert</forenames></author><author><keyname>Johansen</keyname><forenames>H&#xe5;vard</forenames></author><author><keyname>Naigaonkar</keyname><forenames>Nihar</forenames></author><author><keyname>Johansen</keyname><forenames>Dag</forenames></author></authors><title>Secure Abstraction with Code Capabilities</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose embedding executable code fragments in cryptographically protected
capabilities to enable flexible discretionary access control in cloud-like
computing infrastructures. We are developing this as part of a sports analytics
application that runs on a federation of public and enterprise clouds. The
capability mechanism is implemented completely in user space. Using a novel
combination of X.509 certificates and Javscript code, the capabilities support
restricted delegation, confinement, revocation, and rights amplification for
secure abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5454</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5454</id><created>2012-10-19</created><authors><author><keyname>Guirguis</keyname><forenames>Mina</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>Stuck in Traffic (SiT) Attacks: A Framework for Identifying Stealthy
  Attacks that Cause Traffic Congestion</title><categories>cs.NI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in wireless technologies have enabled many new applications
in Intelligent Transportation Systems (ITS) such as collision avoidance,
cooperative driving, congestion avoidance, and traffic optimization. Due to the
vulnerable nature of wireless communication against interference and
intentional jamming, ITS face new challenges to ensure the reliability and the
safety of the overall system. In this paper, we expose a class of stealthy
attacks -- Stuck in Traffic (SiT) attacks -- that aim to cause congestion by
exploiting how drivers make decisions based on smart traffic signs. An attacker
mounting a SiT attack solves a Markov Decision Process problem to find
optimal/suboptimal attack policies in which he/she interferes with a
well-chosen subset of signals that are based on the state of the system. We
apply Approximate Policy Iteration (API) algorithms to derive potent attack
policies. We evaluate their performance on a number of systems and compare them
to other attack policies including random, myopic and DoS attack policies. The
generated policies, albeit suboptimal, are shown to significantly outperform
other attack policies as they maximize the expected cumulative reward from the
standpoint of the attacker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5470</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5470</id><created>2012-10-19</created><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>The DoF of Network MIMO with Backhaul Delays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of downlink precoding for Network (multi-cell) MIMO
networks where Transmitters (TXs) are provided with imperfect Channel State
Information (CSI). Specifically, each TX receives a delayed channel estimate
with the delay being specific to each channel component. This model is
particularly adapted to the scenarios where a user feeds back its CSI to its
serving base only as it is envisioned in future LTE networks. We analyze the
impact of the delay during the backhaul-based CSI exchange on the rate
performance achieved by Network MIMO. We highlight how delay can dramatically
degrade system performance if existing precoding methods are to be used. We
propose an alternative robust beamforming strategy which achieves the maximal
performance, in DoF sense. We verify by simulations that the theoretical DoF
improvement translates into a performance increase at finite Signal-to-Noise
Ratio (SNR) as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5474</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5474</id><created>2012-10-19</created><authors><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Disentangling Factors of Variation via Generative Entangling</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we propose a novel model family with the objective of learning to
disentangle the factors of variation in data. Our approach is based on the
spike-and-slab restricted Boltzmann machine which we generalize to include
higher-order interactions among multiple latent variables. Seen from a
generative perspective, the multiplicative interactions emulates the entangling
of factors of variation. Inference in the model can be seen as disentangling
these generative factors. Unlike previous attempts at disentangling latent
factors, the proposed model is trained using no supervised information
regarding the latent factors. We apply our model to the task of facial
expression classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5484</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5484</id><created>2012-10-19</created><authors><author><keyname>Escalona</keyname><forenames>Francisco</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>Hamiltonian Tetrahedralizations with Steiner Points</title><categories>cs.CG</categories><comments>A conference version of this paper appeared in EuroCG' 07</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of $n$ points in 3-dimensional space. A tetrahedralization
$\mathcal{T}$ of $S$ is a set of interior disjoint tetrahedra with vertices on
$S$, not containing points of $S$ in their interior, and such that their union
is the convex hull of $S$. Given $\mathcal{T}$, $D_\mathcal{T}$ is defined as
the graph having as vertex set the tetrahedra of $\mathcal{T}$, two of which
are adjacent if they share a face. We say that $\mathcal{T}$ is Hamiltonian if
$D_\mathcal{T}$ has a Hamiltonian path. Let $m$ be the number of convex hull
vertices of $S$. We prove that by adding at most $\lfloor \frac{m-2}{2}
\rfloor$ Steiner points to interior of the convex hull of $S$, we can obtain a
point set that admits a Hamiltonian tetrahedralization. An $O(m^{3/2}) + O(n
\log n)$ time algorithm to obtain these points is given. We also show that all
point sets with at most 20 convex hull points admit a Hamiltonian
tetrahedralization without the addition of any Steiner points. Finally we
exhibit a set of 84 points that does not admit a Hamiltonian tetrahedralization
in which all tetrahedra share a vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5486</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5486</id><created>2012-10-19</created><updated>2012-11-11</updated><authors><author><keyname>Ameta</keyname><forenames>Juhi</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>A Lightweight Stemmer for Gujarati</title><categories>cs.CL</categories><comments>In Proceedings of 46th Annual Convention of Computer Society of India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gujarati is a resource poor language with almost no language processing tools
being available. In this paper we have shown an implementation of a rule based
stemmer of Gujarati. We have shown the creation of rules for stemming and the
richness in morphology that Gujarati possesses. We have also evaluated our
results by verifying it with a human expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5500</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5500</id><created>2012-10-19</created><authors><author><keyname>Soto</keyname><forenames>Marta</forenames></author><author><keyname>Gonz&#xe1;lez-Fern&#xe1;ndez</keyname><forenames>Yasser</forenames></author><author><keyname>Ochoa</keyname><forenames>Alberto</forenames></author></authors><title>Modeling with Copulas and Vines in Estimation of Distribution Algorithms</title><categories>cs.NE stat.ME</categories><journal-ref>Investigaci\'on Operacional, 36(1), 1-23</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is studying the use of copulas and vines in the
optimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are
built around the multivariate product and normal copulas, and other two are
based on pair-copula decomposition of vine models. Empirically we study the
effect of both marginal distributions and dependence structure separately, and
show that both aspects play a crucial role in the success of the optimization.
The results show that the use of copulas and vines opens new opportunities to a
more appropriate modeling of search distributions in EDAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5502</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5502</id><created>2012-10-18</created><updated>2012-11-26</updated><authors><author><keyname>Geissmann</keyname><forenames>Quentin</forenames></author></authors><title>OpenCFU, a New Free and Open-Source Software to Count Cell Colonies and
  Other Circular Objects</title><categories>q-bio.QM cs.CV</categories><doi>10.1371/journal.pone.0054072</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Counting circular objects such as cell colonies is an important source of
information for biologists. Although this task is often time-consuming and
subjective, it is still predominantly performed manually. The aim of the
present work is to provide a new tool to enumerate circular objects from
digital pictures and video streams. Here, I demonstrate that the created
program, OpenCFU, is very robust, accurate and fast. In addition, it provides
control over the processing parameters and is implemented in an in- tuitive and
modern interface. OpenCFU is a cross-platform and open-source software freely
available at http://opencfu.sourceforge.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5503</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5503</id><created>2012-10-19</created><authors><author><keyname>Xia</keyname><forenames>Ping</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Downlink Coordinated Multi-Point with Overhead Modeling in Heterogeneous
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated multi-point (CoMP) communication is attractive for heterogeneous
cellular networks (HCNs) for interference reduction. However, previous
approaches to CoMP face two major hurdles in HCNs. First, they usually ignore
the inter-cell overhead messaging delay, although it results in an irreducible
performance bound. Second, they consider the grid or Wyner model for base
station locations, which is not appropriate for HCN BS locations which are
numerous and haphazard. Even for conventional macrocell networks without
overlaid small cells, SINR results are not tractable in the grid model nor
accurate in the Wyner model. To overcome these hurdles, we develop a novel
analytical framework which includes the impact of overhead delay for CoMP
evaluation in HCNs. This framework can be used for a class of CoMP schemes
without user data sharing. As an example, we apply it to downlink CoMP
zero-forcing beamforming (ZFBF), and see significant divergence from previous
work. For example, we show that CoMP ZFBF does not increase throughput when the
overhead channel delay is larger than 60% of the channel coherence time. We
also find that, in most cases, coordinating with only one other cell is nearly
optimum for downlink CoMP ZFBF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5515</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5515</id><created>2012-10-19</created><authors><author><keyname>Mtibaa</keyname><forenames>Sabri</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>Quality of Service Support on High Level Petri-Net Based Model for
  Dynamic Configuration of Web Service Composition</title><categories>cs.SE cs.SY</categories><comments>International Journal of Computer Science and Information Security;
  IJCSIS September 2012, Vol. 10 No. 9. arXiv admin note: substantial text
  overlap with arXiv:1210.5374</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web services are widely used thanks to their features of universal
interoperability between software assets, platform independent and
loose-coupled. Web services composition is one of the most challenging topics
in service computing area. In this paper, an approach based on High Level
Petri-Net model as dynamic configuration schema of web services composition is
proposed to achieve self adaptation to run-time environment and self management
of composite web services. For composite service based applications, in
addition to functional requirements, quality of service properties should be
considered. This paper presents and proves some quality of service formulas in
context of web service composition. Based on this model and the quality of
service properties, a suitable configuration with optimal quality of service
can be selected in dynamic way to reach the goal of automatic service
composition. The correctness of the approach is proved by a simulation results
and corresponding analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5516</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5516</id><created>2012-10-19</created><authors><author><keyname>Mtibaa</keyname><forenames>Sabri</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>Managing Changes in Citizen-Centric Healthcare Service Platform using
  High Level Petri Net</title><categories>cs.SE cs.SY</categories><comments>IJACSA, The Science and Information Organization(SAI), Volume 3 Issue
  8 August 2012. arXiv admin note: substantial text overlap with
  arXiv:1210.5374</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The healthcare organizations are facing a number of daunting challenges
pushing systems to deal with requirements changes and benefit from modern
technologies and telecom capabilities. Systems evolution through extension of
the existing information technology infrastructure becomes one of the most
challenging aspects of healthcare and the adaptation to changes is a must. The
paper presents a change management framework for a citizen-centric healthcare
service platform. A combination between Petri nets model to handle changes and
reconfigurable Petri nets model to react to these changes are introduced to
fulfill healthcare goals. Thanks to this management framework model,
consistency and correctness of a healthcare processes in the presence of
frequent changes can be checked and guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5517</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5517</id><created>2012-10-19</created><authors><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Design of English-Hindi Translation Memory for Efficient Translation</title><categories>cs.CL</categories><comments>Proceedings of National Conference in Recent Advances in Computer
  Engineering, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing parallel corpora is an important and a difficult activity for
Machine Translation. This requires manual annotation by Human Translators.
Translating same text again is a useless activity. There are tools available to
implement this for European Languages, but no such tool is available for Indian
Languages. In this paper we present a tool for Indian Languages which not only
provides automatic translations of the previously available translation but
also provides multiple translations, in cases where a sentence has multiple
translations, in ranked list of suggestive translations for a sentence.
Moreover this tool also lets translators have global and local saving options
of their work, so that they may share it with others, which further lightens
the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5539</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5539</id><created>2012-10-19</created><updated>2013-03-18</updated><authors><author><keyname>Harper</keyname><forenames>Marc</forenames></author><author><keyname>Fryer</keyname><forenames>Dashiell E. A.</forenames></author></authors><title>Stability of Evolutionary Dynamics on Time Scales</title><categories>math.DS cs.IT math.IT q-bio.PE</categories><msc-class>91A22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine incentive, adaptive, and time-scale dynamics to study
multipopulation dynamics on the simplex equipped with a large class of
Riemmanian metrics, simultaneously generalizing and extending many dynamics
commonly studied in dynamic game theory and evolutionary dynamics. Each
population has its own geometry, method of adaptation (incentive), and
time-scale (discrete, continuous, and others). Using an information-theoretic
measure of distance we give a widely-applicable Lyapunov result for the
dynamic. We include a wealth of examples leading up to and beyond the main
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5541</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5541</id><created>2012-10-19</created><authors><author><keyname>Ruijgrok</keyname><forenames>Matthijs</forenames></author></authors><title>A single-item continuous double auction game</title><categories>cs.GT</categories><comments>37 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A double auction game with an infinite number of buyers and sellers is
introduced. All sellers posses one unit of a good, all buyers desire to buy one
unit. Each seller and each buyer has a private valuation of the good. The
distribution of the valuations define supply and demand functions. One unit of
the good is auctioned. At successive, discrete time instances, a player is
randomly selected to make a bid (buyer) or an ask (seller). When the maximum of
the bids becomes larger than the minimum of the asks, a transaction occurs and
the auction is closed. The players have to choose the value of their bid or ask
before the auction starts and use this value when they are selected. Assuming
that the supply and demand functions are known, expected profits as functions
of the strategies are derived, as well as expected transaction prices. It is
shown that for linear supply and demand functions, there exists at most one
Bayesian Nash equilibrium. Competitive behaviour is not an equilibrium of the
game. For linear supply and demand functions, the sum of the expected profit of
the sellers and the buyers is the same for the Bayesian Nash equilibrium and
the market where players behave competitively. Connections are made with the
ZI-C traders model and the $k$-double auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5543</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5543</id><created>2012-10-19</created><authors><author><keyname>Chen</keyname><forenames>Changbo</forenames></author><author><keyname>Maza</keyname><forenames>Marc Moreno</forenames></author></authors><title>An Incremental Algorithm for Computing Cylindrical Algebraic
  Decompositions</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an incremental algorithm for computing cylindrical
algebraic decompositions. The algorithm consists of two parts: computing a
complex cylindrical tree and refining this complex tree into a cylindrical tree
in real space. The incrementality comes from the first part of the algorithm,
where a complex cylindrical tree is constructed by refining a previous complex
cylindrical tree with a polynomial constraint. We have implemented our
algorithm in Maple. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5544</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5544</id><created>2012-10-19</created><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Online Learning in Decentralized Multiuser Resource Sharing Problems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the general scenario of resource sharing in a
decentralized system when the resource rewards/qualities are time-varying and
unknown to the users, and using the same resource by multiple users leads to
reduced quality due to resource sharing. Firstly, we consider a
user-independent reward model with no communication between the users, where a
user gets feedback about the congestion level in the resource it uses.
Secondly, we consider user-specific rewards and allow costly communication
between the users. The users have a cooperative goal of achieving the highest
system utility. There are multiple obstacles in achieving this goal such as the
decentralized nature of the system, unknown resource qualities, communication,
computation and switching costs. We propose distributed learning algorithms
with logarithmic regret with respect to the optimal allocation. Our logarithmic
regret result holds under both i.i.d. and Markovian reward models, as well as
under communication, computation and switching costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5552</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5552</id><created>2012-10-19</created><authors><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author></authors><title>Quickest Change Detection</title><categories>math.ST cs.IT math.IT math.OC math.PR stat.AP stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting changes in the statistical properties of a
stochastic system and time series arises in various branches of science and
engineering. It has a wide spectrum of important applications ranging from
machine monitoring to biomedical signal processing. In all of these
applications the observations being monitored undergo a change in distribution
in response to a change or anomaly in the environment, and the goal is to
detect the change as quickly as possibly, subject to false alarm constraints.
In this chapter, two formulations of the quickest change detection problem,
Bayesian and minimax, are introduced, and optimal or asymptotically optimal
solutions to these formulations are discussed. Then some generalizations and
extensions of the quickest change detection problem are described. The chapter
is concluded with a discussion of applications and open issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5560</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5560</id><created>2012-10-19</created><authors><author><keyname>Mola-Velasco</keyname><forenames>Santiago M.</forenames></author></authors><title>Wikipedia Vandalism Detection Through Machine Learning: Feature Review
  and New Proposals: Lab Report for PAN at CLEF 2010</title><categories>cs.IR cs.AI</categories><comments>Published in CLEF 2010 LABs and Workshops, Notebook Papers, 22-23
  September 2010, Padua, Italy. 2010, ISBN 978-88-904810-0-0. First position at
  the 1st International Competition on Wikipedia Vandalism Detection (PAN @
  CLEF 2010)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Wikipedia is an online encyclopedia that anyone can edit. In this open model,
some people edits with the intent of harming the integrity of Wikipedia. This
is known as vandalism. We extend the framework presented in (Potthast, Stein,
and Gerling, 2008) for Wikipedia vandalism detection. In this approach, several
vandalism indicating features are extracted from edits in a vandalism corpus
and are fed to a supervised learning algorithm. The best performing classifiers
were LogitBoost and Random Forest. Our classifier, a Random Forest, obtained an
AUC of 0.92236, ranking in the first place of the PAN'10 Wikipedia vandalism
detection task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5561</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5561</id><created>2012-10-19</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author></authors><title>On the independent subsets of powers of paths and cycles</title><categories>cs.DM math.CO</categories><comments>9 pages, 4 figures</comments><msc-class>68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this work we provide a formula for the number of edges
of the Hasse diagram of the independent subsets of the h-th power of a path
ordered by inclusion. For h=1 such a value is the number of edges of a
Fibonacci cube. We show that, in general, the number of edges of the diagram is
obtained by convolution of a Fibonacci-like sequence with itself.
  In the second part we consider the case of cycles. We evaluate the number of
edges of the Hasse diagram of the independent subsets of the h-th power of a
cycle ordered by inclusion. For h=1, and n&gt;1, such a value is the number of
edges of a Lucas cube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5581</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5581</id><created>2012-10-20</created><authors><author><keyname>Tsai</keyname><forenames>Chia-Chi</forenames></author><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Huang</keyname><forenames>Wei-Jie</forenames></author><author><keyname>Shan</keyname><forenames>Man-Kwan</forenames></author></authors><title>Hidden Trends in 90 Years of Harvard Business Review</title><categories>cs.CL cs.DL cs.IR</categories><comments>6 pages, 14 figures, Proceedings of 2012 International Conference on
  Technologies and Applications of Artificial Intelligence</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we demonstrate and discuss results of our mining the abstracts
of the publications in Harvard Business Review between 1922 and 2012.
Techniques for computing n-grams, collocations, basic sentiment analysis, and
named-entity recognition were employed to uncover trends hidden in the
abstracts. We present findings about international relationships, sentiment in
HBR's abstracts, important international companies, influential technological
inventions, renown researchers in management theories, US presidents via
chronological analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5594</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5594</id><created>2012-10-20</created><updated>2012-12-11</updated><authors><author><keyname>Spurek</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Cross-Entropy Clustering</title><categories>cs.IT math.IT</categories><journal-ref>Pattern Recognition, Volume 47, Issue 9, September 2014, 3046-3059</journal-ref><doi>10.1016/j.patcog.2014.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a cross-entropy clustering (CEC) theory which finds the optimal
number of clusters by automatically removing groups which carry no information.
Moreover, our theory gives simple and efficient criterion to verify cluster
validity.
  Although CEC can be build on an arbitrary family of densities, in the most
important case of Gaussian CEC:
  {\em -- the division into clusters is affine invariant;
  -- the clustering will have the tendency to divide the data into
ellipsoid-type shapes;
  -- the approach is computationally efficient as we can apply Hartigan
approach.}
  We study also with particular attention clustering based on the Spherical
Gaussian densities and that of Gaussian densities with covariance $s \I$. In
the letter case we show that with $s$ converging to zero we obtain the
classical k-means clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5599</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5599</id><created>2012-10-20</created><updated>2012-10-28</updated><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames><affiliation>Angela</affiliation></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames><affiliation>Angela</affiliation></author><author><keyname>Jun</keyname><forenames>Ying</forenames><affiliation>Angela</affiliation></author><author><keyname>Zhang</keyname></author></authors><title>Pragmatic Physical Layer Encryption for Achieving Perfect Secrecy</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in simulations</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Conventionally, secrecy is achieved using cryptographic techniques beyond the
physical layer. Recent studies raise the interest of performing encryption
within the physical layer by exploiting some unique features of the physical
wireless channel. Following this spirit, we present a novel physical layer
encryption (PLE) scheme that randomizes the radio signal using a secret key
extracted from the wireless channel under the assumption of channel
reciprocity. Specifically, we propose to jointly design the encryption function
and the secret-key generation method. On one hand, we establish a sufficient
and necessary condition for the encryption function to achieve perfect secrecy.
Based on that, several candidate encryption functions are proposed and
compared. We show that, given the secret key available to the legitimate users,
perfect secrecy can be achieved without compromising the capability of the
communication channel. On the other hand, we study the practical design of the
secret-key generation method based on the channel reciprocity. We show that, by
introducing marginal system overhead, the key agreement between the legitimate
users can be done with a high success probability. The performance advantages
of the proposed PLE method is verified through comparisons against other
existing PLE methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5614</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5614</id><created>2012-10-20</created><authors><author><keyname>Deng</keyname><forenames>Na</forenames></author><author><keyname>Zhang</keyname><forenames>Sihai</forenames></author><author><keyname>Zhou</keyname><forenames>Wuyang</forenames></author><author><keyname>Zhu</keyname><forenames>Jinkang</forenames></author></authors><title>A Stochastic Geometry Approach to Energy Efficiency in Relay-Assisted
  Cellular Networks</title><categories>cs.NI</categories><comments>6 pages, 5 figures, accepted by IEEE Globecom'12. arXiv admin note:
  text overlap with arXiv:1108.1257 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though cooperative relaying is believed to be a promising technology to
improve the energy efficiency of cellular networks, the relays' static power
consumption might worsen the energy efficiency therefore can not be neglected.
In this paper, we focus on whether and how the energy efficiency of cellular
networks can be improved via relays. Based on the spatial Poisson point
process, an analytical model is proposed to evaluate the energy efficiency of
relay-assisted cellular networks. With the aid of the technical tools of
stochastic geometry, we derive the distributions of
signal-to-interference-plus-noise ratios (SINRs) and mean achievable rates of
both non-cooperative users and cooperative users. The energy efficiency
measured by &quot;bps/Hz/W&quot; is expressed subsequently. These established expressions
are amenable to numerical evaluation and corroborated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5626</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5626</id><created>2012-10-20</created><updated>2013-07-06</updated><authors><author><keyname>Karahanoglu</keyname><forenames>Nazim Burak</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Compressed Sensing Signal Recovery via Forward-Backward Pursuit</title><categories>cs.IT math.IT</categories><comments>accepted for publication in Digital Signal Processing</comments><journal-ref>Digital Signal Processing 23 (2013), pp. 1539-1548</journal-ref><doi>10.1016/j.dsp.2013.05.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovery of sparse signals from compressed measurements constitutes an l0
norm minimization problem, which is unpractical to solve. A number of sparse
recovery approaches have appeared in the literature, including l1 minimization
techniques, greedy pursuit algorithms, Bayesian methods and nonconvex
optimization techniques among others. This manuscript introduces a novel two
stage greedy approach, called the Forward-Backward Pursuit (FBP). FBP is an
iterative approach where each iteration consists of consecutive forward and
backward stages. The forward step first expands the support estimate by the
forward step size, while the following backward step shrinks it by the backward
step size. The forward step size is larger than the backward step size, hence
the initially empty support estimate is expanded at the end of each iteration.
Forward and backward steps are iterated until the residual power of the
observation vector falls below a threshold. This structure of FBP does not
necessitate the sparsity level to be known a priori in contrast to the Subspace
Pursuit or Compressive Sampling Matching Pursuit algorithms. FBP recovery
performance is demonstrated via simulations including recovery of random sparse
signals with different nonzero coefficient distributions in noisy and
noise-free scenarios in addition to the recovery of a sparse image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5631</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5631</id><created>2012-10-20</created><updated>2013-01-04</updated><authors><author><keyname>Nguyen</keyname><forenames>Jennifer</forenames></author><author><keyname>Zhu</keyname><forenames>Mu</forenames></author></authors><title>Content-boosted Matrix Factorization Techniques for Recommender Systems</title><categories>stat.ML cs.LG</categories><journal-ref>Statistical Analysis and Data Mining, Vol. 6, pp. 286 - 301,
  August 2013</journal-ref><doi>10.1002/sam.11184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many businesses are using recommender systems for marketing outreach.
Recommendation algorithms can be either based on content or driven by
collaborative filtering. We study different ways to incorporate content
information directly into the matrix factorization approach of collaborative
filtering. These content-boosted matrix factorization algorithms not only
improve recommendation accuracy, but also provide useful insights about the
contents, as well as make recommendations more easily interpretable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5644</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5644</id><created>2012-10-20</created><authors><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Philipp</forenames></author><author><keyname>Koltun</keyname><forenames>Vladlen</forenames></author></authors><title>Efficient Inference in Fully Connected CRFs with Gaussian Edge
  Potentials</title><categories>cs.CV cs.AI cs.LG</categories><comments>NIPS 2011</comments><journal-ref>Advances in Neural Information Processing Systems 24 (2011)
  109-117</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art techniques for multi-class image segmentation and
labeling use conditional random fields defined over pixels or image regions.
While region-level models often feature dense pairwise connectivity,
pixel-level models are considerably larger and have only permitted sparse graph
structures. In this paper, we consider fully connected CRF models defined on
the complete set of pixels in an image. The resulting graphs have billions of
edges, making traditional inference algorithms impractical. Our main
contribution is a highly efficient approximate inference algorithm for fully
connected CRF models in which the pairwise edge potentials are defined by a
linear combination of Gaussian kernels. Our experiments demonstrate that dense
connectivity at the pixel level substantially improves segmentation and
labeling accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5648</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5648</id><created>2012-10-20</created><updated>2012-10-28</updated><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>New NP-hardness results for 3-Coloring and 2-to-1 Label Cover</title><categories>cs.CC</categories><comments>Corrected typos. arXiv admin note: substantial text overlap with
  arXiv:1204.5666</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that given a 3-colorable graph, it is NP-hard to find a 3-coloring
with $(16/17 + \eps)$ of the edges bichromatic. In a related result, we show
that given a satisfiable instance of the 2-to-1 Label Cover problem, it is
NP-hard to find a $(23/24 + \eps)$-satisfying assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5653</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5653</id><created>2012-10-20</created><authors><author><keyname>Bandyopadhyay</keyname><forenames>Prof. Samir K.</forenames></author><author><keyname>Datta</keyname><forenames>Biswajita</forenames></author><author><keyname>Roy</keyname><forenames>Sudipta</forenames></author></authors><title>Identifications of concealed weapon in a Human Body</title><categories>cs.CV</categories><comments>6 pages, International Journal of Scientific &amp; Engineering Research
  (ISSN 2229-5518) 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The detection of weapons concealed underneath a person cloths is very much
important to the improvement of the security of the public as well as the
safety of public assets like airports, buildings and railway stations etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5658</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5658</id><created>2012-10-20</created><authors><author><keyname>Pelayo</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Warren</keyname><forenames>Michael A.</forenames></author></authors><title>Homotopy type theory and Voevodsky's univalent foundations</title><categories>math.LO cs.LO math.AT</categories><comments>48 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent discoveries have been made connecting abstract homotopy theory and the
field of type theory from logic and theoretical computer science. This has
given rise to a new field, which has been christened &quot;homotopy type theory&quot;. In
this direction, Vladimir Voevodsky observed that it is possible to model type
theory using simplicial sets and that this model satisfies an additional
property, called the Univalence Axiom, which has a number of striking
consequences. He has subsequently advocated a program, which he calls univalent
foundations, of developing mathematics in the setting of type theory with the
Univalence Axiom and possibly other additional axioms motivated by the
simplicial set model. Because type theory possesses good computational
properties, this program can be carried out in a computer proof assistant. In
this paper we give an introduction to homotopy type theory in Voevodsky's
setting, paying attention to both theoretical and practical issues. In
particular, the paper serves as an introduction to both the general ideas of
homotopy type theory as well as to some of the concrete details of Voevodsky's
work using the well-known proof assistant Coq. The paper is written for a
general audience of mathematicians with basic knowledge of algebraic topology;
the paper does not assume any preliminary knowledge of type theory, logic, or
computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5659</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5659</id><created>2012-10-20</created><authors><author><keyname>Bauer</keyname><forenames>Sebastian S.</forenames></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Juhl</keyname><forenames>Line</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Thrane</keyname><forenames>Claus</forenames></author></authors><title>Weighted Modal Transition Systems</title><categories>cs.LO</categories><comments>Submitted to Formal Methods in System Design</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specification theories as a tool in model-driven development processes of
component-based software systems have recently attracted a considerable
attention. Current specification theories are however qualitative in nature,
and therefore fragile in the sense that the inevitable approximation of systems
by models, combined with the fundamental unpredictability of hardware
platforms, makes it difficult to transfer conclusions about the behavior, based
on models, to the actual system. Hence this approach is arguably unsuited for
modern software systems. We propose here the first specification theory which
allows to capture quantitative aspects during the refinement and implementation
process, thus leveraging the problems of the qualitative setting.
  Our proposed quantitative specification framework uses weighted modal
transition systems as a formal model of specifications. These are labeled
transition systems with the additional feature that they can model optional
behavior which may or may not be implemented by the system. Satisfaction and
refinement is lifted from the well-known qualitative to our quantitative
setting, by introducing a notion of distances between weighted modal transition
systems. We show that quantitative versions of parallel composition as well as
quotient (the dual to parallel composition) inherit the properties from the
Boolean setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5660</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5660</id><created>2012-10-20</created><updated>2012-11-02</updated><authors><author><keyname>Fang</keyname><forenames>Dong</forenames></author><author><keyname>Burr</keyname><forenames>Alister</forenames></author></authors><title>Linear Physical-layer Network Coding in Galois Field for Rayleigh fading
  2-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>There is a a crucial error for the conditons of generating linear
  combination. The corresponding simulations are thus incorrect. The draft
  should be withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel linear physicallayer network coding (LPNC)
for Rayleigh fading 2-way relay channels (2-WRC). Rather than the simple
modulo-2 (bit-XOR) operation, the relay directly maps the superimposed signal
of the two users into the linear network coded combination in GF(2^2) by
multiplying the user data by properly selected generator matrix. We derive the
constellation constrained capacities for LPNC and 5QAM denoise-and forward
(5QAM-DNF) [2] and further explicitly characterize the capacity difference
between LPNC and 5QAM-DNF. Based on our analysis and simulation, we highlight
that without employing the irregular 5QAM mapping and sacrificing the spectral
efficiency, our LPNC in GF(2^2) is superior to 5QAM-DNF scheme in low SNR
regime while they achieve equal performance in the the moderate-to-high SNR
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5664</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5664</id><created>2012-10-20</created><authors><author><keyname>Zadeh</keyname><forenames>Reza Bosagh</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author></authors><title>Characterizing Properties for Q-Clustering</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We uniquely characterize two members of the Q-Clustering family in an
axiomatic framework. We introduce properties that use known tree constructions
for the purpose of characterization. To characterize the Max-Sum clustering
algorithm, we use the Gomory-Hu construction, and to characterize
Single-Linkage, we use the Maximum Spanning Tree. Although at first glance it
seems these properties are `obviously' all that are necessary to characterize
Max-Sum and Single-Linkage, we show that this is not the case, by investigating
how subsets of properties interact. We conclude by proposing additions to the
taxonomy of clustering paradigms currently in use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5670</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5670</id><created>2012-10-20</created><authors><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Dzifcak</keyname><forenames>Juraj</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marcos A.</forenames></author><author><keyname>Gottesman</keyname><forenames>Aaron</forenames></author></authors><title>Typed Answer Set Programming and Inverse Lambda Algorithms</title><categories>cs.AI cs.LO cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our broader goal is to automatically translate English sentences into
formulas in appropriate knowledge representation languages as a step towards
understanding and thus answering questions with respect to English text. Our
focus in this paper is on the language of Answer Set Programming (ASP). Our
approach to translate sentences to ASP rules is inspired by Montague's use of
lambda calculus formulas as meaning of words and phrases. With ASP as the
target language the meaning of words and phrases are ASP-lambda formulas. In an
earlier work we illustrated our approach by manually developing a dictionary of
words and their ASP-lambda formulas. However such an approach is not scalable.
In this paper our focus is on two algorithms that allow one to construct
ASP-lambda formulas in an inverse manner. In particular the two algorithms take
as input two lambda-calculus expressions G and H and compute a lambda-calculus
expression F such that F with input as G, denoted by F@G, is equal to H; and
similarly G@F = H. We present correctness and complexity results about these
algorithms. To do that we develop the notion of typed ASP-lambda calculus
theories and their orders and use it in developing the completeness results.
(To appear in Theory and Practice of Logic Programming.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5677</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5677</id><created>2012-10-20</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Weinstein</keyname><forenames>Amit</forenames></author></authors><title>Local Correction with Constant Error Rate</title><categories>cs.CC cs.DS cs.IT math.IT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function f of n variables is said to be q-locally correctable if,
given a black-box access to a function g which is &quot;close&quot; to an isomorphism
f_sigma(x)=f_sigma(x_1, ..., x_n) = f(x_sigma(1), ..., x_sigma(n)) of f, we can
compute f_sigma(x) for any x in {0,1}^n with good probability using q queries
to g. It is known that degree d polynomials are O(2^d)-locally correctable, and
that most k-juntas are O(k log k)-locally correctable, where the closeness
parameter, or more precisely the distance between g and f_sigma, is required to
be exponentially small (in d and k respectively).
  In this work we relax the requirement for the closeness parameter by allowing
the distance between the functions to be a constant. We first investigate the
family of juntas, and show that almost every k-junta is O(k log^2 k)-locally
correctable for any distance epsilon &lt; 0.001. A similar result is shown for the
family of partially symmetric functions, that is functions which are
indifferent to any reordering of all but a constant number of their variables.
For both families, the algorithms provided here use non-adaptive queries and
are applicable to most but not all functions of each family (as it is shown to
be impossible to locally correct all of them).
  Our approach utilizes the measure of symmetric influence introduced in the
recent analysis of testing partial symmetry of functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5693</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5693</id><created>2012-10-21</created><authors><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames><affiliation>LTCI</affiliation></author><author><keyname>De Arazoza</keyname><forenames>Hector</forenames><affiliation>MATCOM, LPP</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>LTCI</affiliation></author><author><keyname>Tran</keyname><forenames>Viet Chi</forenames><affiliation>LPP, CMAP</affiliation></author></authors><title>Hierarchical clustering for graph visualization</title><categories>stat.AP cs.SI physics.soc-ph</categories><comments>6 pages</comments><proxy>ccsd</proxy><journal-ref>European Symposium on Artificial Neural Networks (ESANN 2011),
  Bruges : Belgium (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a graph visualization methodology based on hierarchical
maximal modularity clustering, with interactive and significant coarsening and
refining possibilities. An application of this method to HIV epidemic analysis
in Cuba is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5694</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5694</id><created>2012-10-21</created><authors><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames><affiliation>LTCI</affiliation></author><author><keyname>De Arazoza</keyname><forenames>Hector</forenames><affiliation>MATCOM, LPP</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>LTCI</affiliation></author><author><keyname>Tran</keyname><forenames>Viet Chi</forenames><affiliation>LPP, CMAP</affiliation></author></authors><title>Visual Mining of Epidemic Networks</title><categories>stat.AP cs.SI physics.soc-ph</categories><comments>8 pages</comments><proxy>ccsd</proxy><journal-ref>International Work Conference on Artificial Neural Networks,
  Torremolinos : Spain (2011)</journal-ref><doi>10.1007/978-3-642-21498-1_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how an interactive graph visualization method based on maximal
modularity clustering can be used to explore a large epidemic network. The
visual representation is used to display statistical tests results that expose
the relations between the propagation of HIV in a sexual contact network and
the sexual orientation of the patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5701</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5701</id><created>2012-10-21</created><authors><author><keyname>Francis</keyname><forenames>Mathew</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Stacho</keyname><forenames>Juraj</forenames></author></authors><title>Obstructions to chordal circular-arc graphs of small independence number</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A blocking quadruple (BQ) is a quadruple of vertices of a graph such that any
two vertices of the quadruple either miss (have no neighbours on) some path
connecting the remaining two vertices of the quadruple, or are connected by
some path missed by the remaining two vertices. This is akin to the notion of
asteroidal triple used in the classical characterization of interval graphs by
Lekkerkerker and Boland. We show that a circular-arc graph cannot have a
blocking quadruple. We also observe that the absence of blocking quadruples is
not in general sufficient to guarantee that a graph is a circular-arc graph.
Nonetheless, it can be shown to be sufficient for some special classes of
graphs, such as those investigated by Bonomo et al. In this note, we focus on
chordal graphs, and study the relationship between the structure of chordal
graphs and the presence/absence of blocking quadruples. Our contribution is
two-fold. Firstly, we provide a forbidden induced subgraph characterization of
chordal graphs without blocking quadruples. In particular, we observe that all
the forbidden subgraphs are variants of the subgraphs forbidden for interval
graphs. Secondly, we show that the absence of blocking quadruples is sufficient
to guarantee that a chordal graph with no independent set of size five is a
circular-arc graph. In our proof we use a novel geometric approach,
constructing a circular-arc representation by traversing around a carefully
chosen clique tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5706</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5706</id><created>2012-10-21</created><updated>2012-11-17</updated><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author><author><keyname>Guo</keyname><forenames>Lankun</forenames></author><author><keyname>Wang</keyname><forenames>Chunyong</forenames></author></authors><title>The construction of characteristic matrixes of dynamic coverings using
  an incremental approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The covering approximation space evolves in time due to the explosion of the
information, and the characteristic matrixes of coverings viewed as an
effective approach to approximating the concept should update with time for
knowledge discovery. This paper further investigates the construction of
characteristic matrixes without running the matrix acquisition algorithm
repeatedly. First, we present two approaches to computing the characteristic
matrixes of the covering with lower time complexity. Then, we investigate the
construction of the characteristic matrixes of the dynamic covering using the
incremental approach. We mainly address the characteristic matrix updating from
three aspects: the variations of elements in the covering, the immigration and
emigration of objects and the changes of attribute values. Afterwards, several
illustrative examples are employed to show that the proposed approach can
effectively compute the characteristic matrixes of the dynamic covering for
approximations of concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5725</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5725</id><created>2012-10-21</created><updated>2012-12-11</updated><authors><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Coding for the Lee and Manhattan Metrics with Weighing Matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has two goals. The first one is to discuss good codes for packing
problems in the Lee and Manhattan metrics. The second one is to consider
weighing matrices for some of these coding problems. Weighing matrices were
considered as building blocks for codes in the Hamming metric in various
constructions. In this paper we will consider mainly two types of weighing
matrices, namely conference matrices and Hadamard matrices, to construct codes
in the Lee (and Manhattan) metric. We will show that these matrices have some
desirable properties when considered as generator matrices for codes in these
metrics. Two related packing problems will be considered. The first is to find
good codes for error-correction (i.e. dense packings of Lee spheres). The
second is to transform the space in a way that volumes are preserved and each
Lee sphere (or conscribed cross-polytope), in the space, will be transformed to
a shape inscribed in a small cube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5729</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5729</id><created>2012-10-21</created><authors><author><keyname>Hung</keyname><forenames>Hao-Hsiang</forenames></author></authors><title>Survival Network Design of Doubling Dimension Metrics</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We investigate the Minimum Weight 2-Edge-Connected Spanning Subgraph (2-ECSS)
problem in an arbitrary metric space of doubling dimension and show a
polynomial time randomized $(1+\epsilon)$-approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5732</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5732</id><created>2012-10-21</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>Developing ICC Profile Using Gray Level Control In Offset Printing
  Process</title><categories>cs.CV</categories><comments>4 Pages, 3 figures, 1 Tables, International Journal of Computer
  Science and Network Security, Volume 12, No 10, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In prepress department RGB image has to be converted to CMYK image. To
control that amount of black, cyan, magenta and yellow has to be controlled by
using color separation method. Graycolor separation method is selected to
control the amounts of these colors because it increase the quality of printing
also. A single printer used for printing the same image on different paper also
results in different printed images. To remove this problem a different ICC
profile based on gray level control is developedand a sheet offset printer is
calibrated using that profile and a subjective evaluation shows satisfactory
results for different quality papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5734</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5734</id><created>2012-10-21</created><updated>2013-12-10</updated><authors><author><keyname>Mayer</keyname><forenames>M. Cialdea</forenames></author></authors><title>Tableaux for multi-modal hybrid logic with binders, transitive relations
  and relation hierarchies</title><categories>cs.LO math.LO</categories><comments>This paper has been withdrawn by the author because it is superseded
  by a new one: A Proof Procedure for Hybrid Logic with Binders, Transitivity
  and Relation Hierarchies(extended version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper, a tableau calculus has been presented, which constitute
a decision procedure for hybrid logic with the converse and global modalities
and a restricted use of the binder. This work extends such a calculus to
multi-modal logic with transitive relations and relation inclusion assertions.
  The separate addition of either transitive relations or relation hierarchies
to the considered decidable fragment of multi-modal hybrid logic can easily be
shown to stay decidable, by resorting to results already proved in the
literature. However, such results do not directly allow for concluding whether
the logic including both features is still decidable. The existence of a
terminating, sound and complete calculus for the considered logic proves that
the addition of transitive relations and relation hierarchies to such an
expressive decidable fragment of hybrid logic yields a decidable logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5751</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5751</id><created>2012-10-21</created><authors><author><keyname>Delpech</keyname><forenames>Estelle</forenames><affiliation>LINA</affiliation></author><author><keyname>Daille</keyname><forenames>B&#xe9;atrice</forenames><affiliation>LINA</affiliation></author><author><keyname>Morin</keyname><forenames>Emmanuel</forenames><affiliation>LINA</affiliation></author><author><keyname>Lemaire</keyname><forenames>Claire</forenames></author></authors><title>Extraction of domain-specific bilingual lexicon from comparable corpora:
  compositional translation and ranking</title><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.2400</comments><proxy>ccsd</proxy><journal-ref>COLING 2012, Mumbai : India (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method for extracting translations of morphologically
constructed terms from comparable corpora. The method is based on compositional
translation and exploits translation equivalences at the morpheme-level, which
allows for the generation of &quot;fertile&quot; translations (translation pairs in which
the target term has more words than the source term). Ranking methods relying
on corpus-based and translation-based features are used to select the best
candidate translation. We obtain an average precision of 91% on the Top1
candidate translation. The method was tested on two language pairs
(English-French and English-German) and with a small specialized comparable
corpora (400k words per language).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5752</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5752</id><created>2012-10-21</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Optimal Linear Transceiver Designs for Cognitive Two-Way Relay Networks</title><categories>cs.IT math.IT</categories><comments>31 pages 12 figures, Accepted by TSP</comments><doi>10.1109/TSP.2012.2226452</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studies a cooperative cognitive radio network where two primary
users (PUs) exchange information with the help of a secondary user (SU) that is
equipped with multiple antennas and in return, the SU superimposes its own
messages along with the primary transmission. The fundamental problem in the
considered network is the design of transmission strategies at the secondary
node. It involves three basic elements: first, how to split the power for
relaying the primary signals and for transmitting the secondary signals;
second, what two-way relay strategy should be used to assist the bidirectional
communication between the two PUs; third, how to jointly design the primary and
secondary transmit precoders. This work aims to address this problem by
proposing a transmission framework of maximizing the achievable rate of the SU
while maintaining the rate requirements of the two PUs. Three well-known and
practical two-way relay strategies are considered: amplify-and-forward (AF),
bit level XOR based decode-and-forward (DF-XOR) and symbol level superposition
coding based DF (DF-SUP). For each relay strategy, although the design problem
is non-convex, we find the optimal solution by using certain transformation
techniques and optimization tools such as semidefinite programming (SDP) and
second-order cone programming (SOCP). Closed-form solutions are also obtained
under certain conditions. Simulation results show that when the rate
requirements of the two PUs are symmetric, by using the DF-XOR strategy and
applying the proposed optimal precoding, the SU requires the least power for
relaying and thus reserves the most power to transmit its own signal. In the
asymmetric scenario, on the other hand, the DF-SUP strategy with the
corresponding optimal precoding is the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5755</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5755</id><created>2012-10-21</created><authors><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Eigenvalue Based Sensing and SNR Estimation for Cognitive Radio in
  Presence of Noise Correlation</title><categories>cs.IT cs.ET math.IT</categories><comments>23 pages, 8 figures</comments><msc-class>Info Theory</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herein, we present a detailed analysis of an eigenvalue based sensing
technique in the presence of correlated noise in the context of a Cognitive
Radio (CR). We use a Standard Condition Number (SCN) based decision statistic
based on asymptotic Random Matrix Theory (RMT) for decision process. Firstly,
the effect of noise correlation on eigenvalue based Spectrum Sensing (SS) is
studied analytically under both the noise only and the signal plus noise
hypotheses. Secondly, new bounds for the SCN are proposed for achieving
improved sensing in correlated noise scenarios. Thirdly, the performance of
Fractional Sampling (FS) based SS is studied and a method for determining the
operating point for the FS rate in terms of sensing performance and complexity
is suggested. Finally, an SNR estimation technique based on the maximum
eigenvalue of the received signal's covariance matrix is proposed. It is shown
that proposed SCN-based threshold improves sensing performance in the presence
of correlated noise and SNRs upto 0 dB can be reliably estimated without the
knowledge of noise variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5774</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5774</id><created>2012-10-21</created><updated>2012-11-02</updated><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Fast Routing Table Construction Using Small Messages</title><categories>cs.DC</categories><comments>40 pages, 2 figures, extended abstract submitted to STOC'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a distributed randomized algorithm computing approximate
distances and routes that approximate shortest paths. Let n denote the number
of nodes in the graph, and let HD denote the hop diameter of the graph, i.e.,
the diameter of the graph when all edges are considered to have unit weight.
Given 0 &lt; eps &lt;= 1/2, our algorithm runs in weak-O(n^(1/2 + eps) + HD)
communication rounds using messages of O(log n) bits and guarantees a stretch
of O(eps^(-1) log eps^(-1)) with high probability. This is the first
distributed algorithm approximating weighted shortest paths that uses small
messages and runs in weak-o(n) time (in graphs where HD in weak-o(n)). The time
complexity nearly matches the lower bounds of weak-Omega(sqrt(n) + HD) in the
small-messages model that hold for stateless routing (where routing decisions
do not depend on the traversed path) as well as approximation of the weigthed
diameter. Our scheme replaces the original identifiers of the nodes by labels
of size O(log eps^(-1) log n). We show that no algorithm that keeps the
original identifiers and runs for weak-o(n) rounds can achieve a
polylogarithmic approximation ratio.
  Variations of our techniques yield a number of fast distributed approximation
algorithms solving related problems using small messages. Specifically, we
present algorithms that run in weak-O(n^(1/2 + eps) + HD) rounds for a given 0
&lt; eps &lt;= 1/2, and solve, with high probability, the following problems:
  - O(eps^(-1))-approximation for the Generalized Steiner Forest (the running
time in this case has an additive weak-O(t^(1 + 2eps)) term, where t is the
number of terminals);
  - O(eps^(-2))-approximation of weighted distances, using node labels of size
O(eps^(-1) log n) and weak-O(n^(eps)) bits of memory per node;
  - O(eps^(-1))-approximation of the weighted diameter;
  - O(eps^(-3))-approximate shortest paths using the labels 1,...,n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5783</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5783</id><created>2012-10-21</created><authors><author><keyname>Silva</keyname><forenames>Josep</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Spain</affiliation></author><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames><affiliation>IMT Institute for Advanced Studies Lucca, Italy</affiliation></author></authors><title>Proceedings 8th International Workshop on Automated Specification and
  Verification of Web Systems</title><categories>cs.SE cs.FL cs.NI</categories><comments>EPTCS 98, 2012</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.98</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the final and revised versions of the papers presented
at the 8th International Workshop on Automated Specification and Verification
of Web Systems (WWV 2012). The workshop was held in Stockholm, Sweden, on June
16, 2012, as part of DisCoTec 2012.
  WWV is a yearly workshop that aims at providing an interdisciplinary forum to
facilitate the cross-fertilization and the advancement of hybrid methods that
exploit concepts and tools drawn from Rule-based programming, Software
engineering, Formal methods and Web-oriented research. WWV has a reputation for
being a lively, friendly forum for presenting and discussing work in progress.
The proceedings have been produced after the symposium to allow the authors to
incorporate the feedback gathered during the event in the published papers.
  All papers submitted to the workshop were reviewed by at least three Program
Committee members or external referees. The Program Committee held an
electronic discussion leading to the acceptance of all papers for presentation
at the workshop. In addition to the presentation of the contributed papers, the
scientific programme included the invited talks by two outstanding speakers:
Rocco De Nicola (IMT, Institute for Advanced Studies Lucca, Italy) and Jos\`e
Luiz Fiadeiro (Royal Holloway, United Kingdom).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5786</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5786</id><created>2012-10-21</created><authors><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Optimizing Tile Concentrations to Minimize Errors and Time for DNA Tile
  Self-Assembly Systems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA tile self-assembly has emerged as a rich and promising primitive for
nano-technology. This paper studies the problems of minimizing assembly time
and error rate by changing the tile concentrations because changing the tile
concentrations is easy to implement in actual lab experiments. We prove that
setting the concentration of tile $T_i$ proportional to the square root of
$N_i$ where $N_i$ is the number of times $T_i$ appears outside the seed
structure in the final assembled shape minimizes the rate of growth errors for
rectilinear tile systems. We also show that the same concentrations minimize
the expected assembly time for a feasible class of tile systems. Moreover, for
general tile systems, given tile concentrations, we can approximate the
expected assembly time with high accuracy and probability by running only a
polynomial number of simulations in the size of the target shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5802</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5802</id><created>2012-10-22</created><updated>2012-10-30</updated><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Gebremedhin</keyname><forenames>Assefaw H.</forenames></author><author><keyname>Patwary</keyname><forenames>Md. Mostofa Ali</forenames></author></authors><title>What if CLIQUE were fast? Maximum Cliques in Information Networks and
  Strong Components in Temporal Networks</title><categories>cs.SI cs.DC cs.DM physics.soc-ph</categories><msc-class>05C69, 05C85, 91D30</msc-class><acm-class>G.2.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact maximum clique finders have progressed to the point where we can
investigate cliques in million-node social and information networks, as well as
find strongly connected components in temporal networks. We use one such finder
to study a large collection of modern networks emanating from biological,
social, and technological domains. We show inter-relationships between maximum
cliques and several other common network properties, including network density,
maximum core, and number of triangles. In temporal networks, we find that the
largest temporal strong components have around 20-30% of the vertices of the
entire network. These components represent groups of highly communicative
individuals. In addition, we discuss and improve the performance and utility of
the maximum clique finder itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5813</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5813</id><created>2012-10-22</created><authors><author><keyname>Xiang</keyname><forenames>Zhengzheng</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Coordinated Multicast Beamforming in Multicell Networks</title><categories>cs.IT math.IT</categories><comments>10pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study physical layer multicasting in multicell networks where each base
station, equipped with multiple antennas, transmits a common message using a
single beamformer to multiple users in the same cell. We investigate two
coordinated beamforming designs: the quality-of-service (QoS) beamforming and
the max-min SINR (signal-to-interference-plus-noise ratio) beamforming. The
goal of the QoS beamforming is to minimize the total power consumption while
guaranteeing that received SINR at each user is above a predetermined
threshold. We present a necessary condition for the optimization problem to be
feasible. Then, based on the decomposition theory, we propose a novel
decentralized algorithm to implement the coordinated beamforming with limited
information sharing among different base stations. The algorithm is guaranteed
to converge and in most cases it converges to the optimal solution. The max-min
SINR (MMS) beamforming is to maximize the minimum received SINR among all users
under per-base station power constraints. We show that the MMS problem and a
weighted peak-power minimization (WPPM) problem are inverse problems. Based on
this inversion relationship, we then propose an efficient algorithm to solve
the MMS problem in an approximate manner. Simulation results demonstrate
significant advantages of the proposed multicast beamforming algorithms over
conventional multicasting schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5814</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5814</id><created>2012-10-22</created><authors><author><keyname>Xiang</keyname><forenames>Zhengzheng</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Robust Beamforming for Wireless Information and Power Transmission</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures; IEEE Wireless Communications Letters 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we study the robust beamforming problem for the multi-antenna
wireless broadcasting system with simultaneous information and power
transmission, under the assumption of imperfect channel state information (CSI)
at the transmitter. Following the worst-case deterministic model, our objective
is to maximize the worst-case harvested energy for the energy receiver while
guaranteeing that the rate for the information receiver is above a threshold
for all possible channel realizations. Such problem is nonconvex with infinite
number of constraints. Using certain transformation techniques, we convert this
problem into a relaxed semidefinite programming problem (SDP) which can be
solved efficiently. We further show that the solution of the relaxed SDP
problem is always rank-one. This indicates that the relaxation is tight and we
can get the optimal solution for the original problem. Simulation results are
presented to validate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5826</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5826</id><created>2012-10-22</created><updated>2013-03-20</updated><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanis&#x142;aw P.</forenames></author></authors><title>New Computational Upper Bounds for Ramsey Numbers R(3,k)</title><categories>math.CO cs.DM</categories><comments>28 pages (includes a lot of tables); added improved lower bound for
  R(3,11); added some notes</comments><msc-class>05C55, 05C30, 68R10</msc-class><journal-ref>Electron. J. Comb. 20(1) (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using computational techniques we derive six new upper bounds on the
classical two-color Ramsey numbers: R(3,10) &lt;= 42, R(3,11) &lt;= 50, R(3,13) &lt;=
68, R(3,14) &lt;= 77, R(3,15) &lt;= 87, and R(3,16) &lt;= 98. All of them are
improvements by one over the previously best known bounds.
  Let e(3,k,n) denote the minimum number of edges in any triangle-free graph on
n vertices without independent sets of order k. The new upper bounds on R(3,k)
are obtained by completing the computation of the exact values of e(3,k,n) for
all n with k &lt;= 9 and for all n &lt;= 33 for k = 10, and by establishing new lower
bounds on e(3,k,n) for most of the open cases for 10 &lt;= k &lt;= 15. The
enumeration of all graphs witnessing the values of e(3,k,n) is completed for
all cases with k &lt;= 9. We prove that the known critical graph for R(3,9) on 35
vertices is unique up to isomorphism. For the case of R(3,10), first we
establish that R(3,10) = 43 if and only if e(3,10,42) = 189, or equivalently,
that if R(3,10) = 43 then every critical graph is regular of degree 9. Then,
using computations, we disprove the existence of the latter, and thus show that
R(3,10) &lt;= 42.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5830</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5830</id><created>2012-10-22</created><updated>2015-10-11</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>SIERRA, DI-ENS</affiliation></author><author><keyname>Lerasle</keyname><forenames>Matthieu</forenames><affiliation>JAD</affiliation></author></authors><title>Choice of V for V-Fold Cross-Validation in Least-Squares Density
  Estimation</title><categories>math.ST cs.LG stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies V-fold cross-validation for model selection in
least-squares density estimation. The goal is to provide theoretical grounds
for choosing V in order to minimize the least-squares loss of the selected
estimator. We first prove a non-asymptotic oracle inequality for V-fold
cross-validation and its bias-corrected version (V-fold penalization). In
particular, this result implies that V-fold penalization is asymptotically
optimal in the nonparametric case. Then, we compute the variance of V-fold
cross-validation and related criteria, as well as the variance of key
quantities for model selection performance. We show that these variances depend
on V like 1+4/(V-1), at least in some particular cases, suggesting that the
performance increases much from V=2 to V=5 or 10, and then is almost constant.
Overall, this can explain the common advice to take V=5---at least in our
setting and when the computational power is limited---, as supported by some
simulation experiments. An oracle inequality and exact formulas for the
variance are also proved for Monte-Carlo cross-validation, also known as
repeated cross-validation, where the parameter V is replaced by the number B of
random splits of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5839</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5839</id><created>2012-10-22</created><updated>2012-10-29</updated><authors><author><keyname>Bostan</keyname><forenames>Emrah</forenames></author><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author><author><keyname>Nilchian</keyname><forenames>Masih</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Sparse Stochastic Processes and Discretization of Linear Inverse
  Problems</title><categories>cs.IT math.IT</categories><comments>26 pages, 5 figures</comments><msc-class>92C55</msc-class><doi>10.1109/TIP.2013.2255305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel statistically-based discretization paradigm and derive a
class of maximum a posteriori (MAP) estimators for solving ill-conditioned
linear inverse problems. We are guided by the theory of sparse stochastic
processes, which specifies continuous-domain signals as solutions of linear
stochastic differential equations. Accordingly, we show that the class of
admissible priors for the discretized version of the signal is confined to the
family of infinitely divisible distributions. Our estimators not only cover the
well-studied methods of Tikhonov and $\ell_1$-type regularizations as
particular cases, but also open the door to a broader class of
sparsity-promoting regularization schemes that are typically nonconvex. We
provide an algorithm that handles the corresponding nonconvex problems and
illustrate the use of our formalism by applying it to deconvolution, MRI, and
X-ray tomographic reconstruction problems. Finally, we compare the performance
of estimators associated with models of increasing sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5840</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5840</id><created>2012-10-22</created><authors><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Supervised Learning with Similarity Functions</title><categories>cs.LG stat.ML</categories><comments>To appear in the proceedings of NIPS 2012, 30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of general supervised learning when data can only be
accessed through an (indefinite) similarity function between data points.
Existing work on learning with indefinite kernels has concentrated solely on
binary/multi-class classification problems. We propose a model that is generic
enough to handle any supervised learning task and also subsumes the model
previously proposed for classification. We give a &quot;goodness&quot; criterion for
similarity functions w.r.t. a given supervised learning task and then adapt a
well-known landmarking technique to provide efficient algorithms for supervised
learning using &quot;good&quot; similarity functions. We demonstrate the effectiveness of
our model on three important super-vised learning problems: a) real-valued
regression, b) ordinal regression and c) ranking where we show that our method
guarantees bounded generalization error. Furthermore, for the case of
real-valued regression, we give a natural goodness definition that, when used
in conjunction with a recent result in sparse vector recovery, guarantees a
sparse predictor with bounded generalization error. Finally, we report results
of our learning algorithms on regression and ordinal regression tasks using
non-PSD similarity functions and demonstrate the effectiveness of our
algorithms, especially that of the sparse landmark selection algorithm that
achieves significantly higher accuracies than the baseline methods while
offering reduced computational costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5844</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5844</id><created>2012-10-22</created><updated>2014-03-20</updated><authors><author><keyname>Chierchia</keyname><forenames>Giovanni</forenames></author><author><keyname>Pustelnik</keyname><forenames>Nelly</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Pesquet-Popescu</keyname><forenames>B&#xe9;atrice</forenames></author></authors><title>Epigraphical splitting for solving constrained convex formulations of
  inverse problems with proximal tools</title><categories>cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a proximal approach to deal with a class of convex variational
problems involving nonlinear constraints. A large family of constraints, proven
to be effective in the solution of inverse problems, can be expressed as the
lower level set of a sum of convex functions evaluated over different, but
possibly overlapping, blocks of the signal. For such constraints, the
associated projection operator generally does not have a simple form. We
circumvent this difficulty by splitting the lower level set into as many
epigraphs as functions involved in the sum. A closed half-space constraint is
also enforced, in order to limit the sum of the introduced epigraphical
variables to the upper bound of the original lower level set. In this paper, we
focus on a family of constraints involving linear transforms of distance
functions to a convex set or $\ell_{1,p}$ norms with $p\in \{1,2,\infty\}$. In
these cases, the projection onto the epigraph of the involved function has a
closed form expression.
  The proposed approach is validated in the context of image restoration with
missing samples, by making use of constraints based on Non-Local Total
Variation. Experiments show that our method leads to significant improvements
in term of convergence speed over existing algorithms for solving similar
constrained problems. A second application to a pulse shape design problem is
provided in order to illustrate the flexibility of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5859</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5859</id><created>2012-10-22</created><authors><author><keyname>Bayraktar</keyname><forenames>Ertugrul</forenames></author><author><keyname>Bilge</keyname><forenames>Ayse Humeyra</forenames></author></authors><title>Determination the Parameters of Markowitz Portfolio Optimization Model</title><categories>q-fin.PM cs.CE q-fin.ST</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this study is the determination of the optimal length of
the historical data for the estimation of statistical parameters in Markowitz
Portfolio Optimization. We present a trading simulation using Markowitz method,
for a portfolio consisting of foreign currency exchange rates and selected
assets from the Istanbul Stock Exchange ISE 30, over the period 2001-2009. In
the simulation, the expected returns and the covariance matrix are computed
from historical data observed for past n days and the target returns are chosen
as multiples of the return of the market index. The trading strategy is to buy
a stock if the simulation resulted in a feasible solution and sell the stock
after exactly m days, independently from the market conditions. The actual
returns are computed for n and m being equal to 21, 42, 63, 84 and 105 days and
we have seen that the best return is obtained when the observation period is 2
or 3 times the investment period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5863</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5863</id><created>2012-10-22</created><updated>2013-08-04</updated><authors><author><keyname>Araujo</keyname><forenames>Carlos</forenames></author><author><keyname>Dejter</keyname><forenames>Italo J.</forenames></author><author><keyname>Horak</keyname><forenames>Peter</forenames></author></authors><title>A Generalization of Lee Codes</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>17 pages, 13 figures; Designs, Codes and Cryptography 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a problem in computer architecture we introduce a notion of the
perfect distance-dominating set, PDDS, in a graph. PDDSs constitute a
generalization of perfect Lee codes, diameter perfect codes, as well as other
codes and dominating sets. In this paper we initiate a systematic study of
PDDSs. PDDSs related to the application will be constructed and the
non-existence of some PDDSs will be shown. In addition, an extension of the
long-standing Golomb-Welch conjecture, in terms of PDDS, will be stated. We
note that all constructed PDDSs are lattice-like which is a very important
feature from the practical point of view as in this case decoding algorithms
tend to be much simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5873</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5873</id><created>2012-10-22</created><authors><author><keyname>Akinduko</keyname><forenames>A. A.</forenames></author><author><keyname>Mirkes</keyname><forenames>E. M.</forenames></author></authors><title>Initialization of Self-Organizing Maps: Principal Components Versus
  Random Initialization. A Case Study</title><categories>stat.ML cs.LG</categories><comments>18 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The performance of the Self-Organizing Map (SOM) algorithm is dependent on
the initial weights of the map. The different initialization methods can
broadly be classified into random and data analysis based initialization
approach. In this paper, the performance of random initialization (RI) approach
is compared to that of principal component initialization (PCI) in which the
initial map weights are chosen from the space of the principal component.
Performance is evaluated by the fraction of variance unexplained (FVU).
Datasets were classified into quasi-linear and non-linear and it was observed
that RI performed better for non-linear datasets; however the performance of
PCI approach remains inconclusive for quasi-linear datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5879</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5879</id><created>2012-10-22</created><updated>2013-05-15</updated><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author><author><keyname>Monteil</keyname><forenames>Thierry</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>Symmetric Determinantal Representations in Characteristic 2</title><categories>cs.CC math.RA</categories><comments>24 pages, 3 figures</comments><msc-class>12705, 15A15, 11T55</msc-class><journal-ref>Linear Algebra and Its Applications 439(5), pp. 1364-1381, 2013</journal-ref><doi>10.1016/j.laa.2013.04.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies Symmetric Determinantal Representations (SDR) in
characteristic 2, that is the representation of a multivariate polynomial P by
a symmetric matrix M such that P=det(M), and where each entry of M is either a
constant or a variable.
  We first give some sufficient conditions for a polynomial to have an SDR. We
then give a non-trivial necessary condition, which implies that some
polynomials have no SDR, answering a question of Grenet et al.
  A large part of the paper is then devoted to the case of multilinear
polynomials. We prove that the existence of an SDR for a multilinear polynomial
is equivalent to the existence of a factorization of the polynomial in certain
quotient rings. We develop some algorithms to test the factorizability in these
rings and use them to find SDRs when they exist. Altogether, this gives us
polynomial-time algorithms to factorize the polynomials in the quotient rings
and to build SDRs. We conclude by describing the case of Alternating
Determinantal Representations in any characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5888</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5888</id><created>2012-10-22</created><authors><author><keyname>Janicki</keyname><forenames>Artur</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Steganalysis of Transcoding Steganography</title><categories>cs.CR cs.MM</categories><comments>13 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TranSteg (Trancoding Steganography) is a fairly new IP telephony
steganographic method that functions by compressing overt (voice) data to make
space for the steganogram by means of transcoding. It offers high
steganographic bandwidth, retains good voice quality and is generally harder to
detect than other existing VoIP steganographic methods. In TranSteg, after the
steganogram reaches the receiver, the hidden information is extracted and the
speech data is practically restored to what was originally sent. This is a huge
advantage compared with other existing VoIP steganographic methods, where the
hidden data can be extracted and removed but the original data cannot be
restored because it was previously erased due to a hidden data insertion
process. In this paper we address the issue of steganalysis of TranSteg.
Various TranSteg scenarios and possibilities of warden(s) localization are
analyzed with regards to the TranSteg detection. A steganalysis method based on
MFCC (Mel-Frequency Cepstral Coefficients) parameters and GMMs (Gaussian
Mixture Models) was developed and tested for various overt/covert codec pairs
in a single warden scenario with double transcoding. The proposed method
allowed for efficient detection of some codec pairs (e.g., G.711/G.729), whilst
some others remained more resistant to detection (e.g., iLBC/AMR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5898</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5898</id><created>2012-10-22</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Jin</keyname><forenames>Guantao</forenames></author><author><keyname>Liu</keyname><forenames>Qingfeng</forenames></author><author><keyname>Chiu</keyname><forenames>Wei-Yun</forenames></author><author><keyname>Yu</keyname><forenames>Yih-Soong</forenames></author></authors><title>Some Chances and Challenges in Applying Language Technologies to
  Historical Studies in Chinese</title><categories>cs.CL cs.DL cs.IR</categories><comments>15 pages, 9 figures, 2 tables; partially appeared in the Proceedings
  of the Third International Conference of Digital Archives and Digital
  Humanities</comments><journal-ref>International Journal of Computational Linguistics and Chinese
  Language Processing, 16(1-2), 27-46, 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We report applications of language technology to analyzing historical
documents in the Database for the Study of Modern Chinese Thoughts and
Literature (DSMCTL). We studied two historical issues with the reported
techniques: the conceptualization of &quot;huaren&quot; (Chinese people) and the attempt
to institute constitutional monarchy in the late Qing dynasty. We also discuss
research challenges for supporting sophisticated issues using our experience
with DSMCTL, the Database of Government Officials of the Republic of China, and
the Dream of the Red Chamber. Advanced techniques and tools for lexical,
syntactic, semantic, and pragmatic processing of language information, along
with more thorough data collection, are needed to strengthen the collaboration
between historians and computer scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5902</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5902</id><created>2012-10-22</created><authors><author><keyname>Bertschinger</keyname><forenames>Nils</forenames></author><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author><author><keyname>Olbrich</keyname><forenames>Eckehard</forenames></author><author><keyname>Jost</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Shared Information -- New Insights and Problems in Decomposing
  Information in Complex Systems</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><msc-class>62B10, 94A17</msc-class><acm-class>H.1.1</acm-class><journal-ref>Proceedings of the European Conference on Complex Systems 2012 /
  T. Gilbert... (eds.). Springer, 2013. - P. 251-269 (Springer proceedings in
  complexity)</journal-ref><doi>10.1007/978-3-319-00395-5_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can the information that a set ${X_{1},...,X_{n}}$ of random variables
contains about another random variable $S$ be decomposed? To what extent do
different subgroups provide the same, i.e. shared or redundant, information,
carry unique information or interact for the emergence of synergistic
information?
  Recently Williams and Beer proposed such a decomposition based on natural
properties for shared information. While these properties fix the structure of
the decomposition, they do not uniquely specify the values of the different
terms. Therefore, we investigate additional properties such as strong symmetry
and left monotonicity. We find that strong symmetry is incompatible with the
properties proposed by Williams and Beer. Although left monotonicity is a very
natural property for an information measure it is not fulfilled by any of the
proposed measures.
  We also study a geometric framework for information decompositions and ask
whether it is possible to represent shared information by a family of posterior
distributions.
  Finally, we draw connections to the notions of shared knowledge and common
knowledge in game theory. While many people believe that independent variables
cannot share information, we show that in game theory independent agents can
have shared knowledge, but not common knowledge. We conclude that intuition and
heuristic arguments do not suffice when arguing about information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5908</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5908</id><created>2012-10-22</created><updated>2013-03-01</updated><authors><author><keyname>Farnsworth</keyname><forenames>Keith D.</forenames></author><author><keyname>Nelson</keyname><forenames>John</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Living is information processing: from molecules to global systems</title><categories>cs.IT math.IT physics.bio-ph q-bio.OT</categories><comments>28 pages, 2 figures</comments><acm-class>H.1.1; J.3</acm-class><journal-ref>Acta Biotheoretica, 61(2):203-222. 2013</journal-ref><doi>10.1007/s10441-013-9179-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the concept that life is an informational phenomenon, at every
level of organisation, from molecules to the global ecological system.
According to this thesis: (a) living is information processing, in which memory
is maintained by both molecular states and ecological states as well as the
more obvious nucleic acid coding; (b) this information processing has one
overall function - to perpetuate itself; and (c) the processing method is
filtration (cognition) of, and synthesis of, information at lower levels to
appear at higher levels in complex systems (emergence). We show how information
patterns, are united by the creation of mutual context, generating persistent
consequences, to result in `functional information'. This constructive process
forms arbitrarily large complexes of information, the combined effects of which
include the functions of life. Molecules and simple organisms have already been
measured in terms of functional information content; we show how quantification
may be extended to each level of organisation up to the ecological. In terms of
a computer analogy, life is both the data and the program and its biochemical
structure is the way the information is embodied. This idea supports the
seamless integration of life at all scales with the physical universe. The
innovation reported here is essentially to integrate these ideas, basing
information on the `general definition' of information, rather than simply the
statistics of information, thereby explaining how functional information
operates throughout life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5912</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5912</id><created>2012-10-11</created><updated>2014-07-25</updated><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author><author><keyname>Mishra</keyname><forenames>Priyadarsini</forenames></author><author><keyname>Adhikary</keyname><forenames>M. C.</forenames></author><author><keyname>Kumar</keyname><forenames>Sunit</forenames></author></authors><title>Image Encryption Using Fibonacci-Lucas Transformation</title><categories>cs.CR</categories><comments>International Journal on Cryptography and Information Security
  (IJCIS),Vol.2, No.3, September 2012, Pp. 131- 141 (11 Pages), 6 figures.
  http://airccse.org/journal/ijcis/current2012.html</comments><msc-class>68U10</msc-class><doi>10.5121/ijcis.2012.2312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret communication techniques are of great demand since last 3000 years due
to the need of information security and confidentiality at various levels of
communication such as while communicating confidential personal data, medical
data of patients, defence and intelligence information of countries, data
related to examinations etc. With advancements in image processing research,
Image encryption and Steganographic techniques have gained popularity over
other forms of hidden communication techniques during the last few decades and
a number of image encryption models are suggested by various researchers from
time to time. In this paper, we are suggesting a new image encryption model
based on Fibonacci and Lucas series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5913</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5913</id><created>2012-10-05</created><authors><author><keyname>Adeyemi</keyname><forenames>Ikuesan R.</forenames></author><author><keyname>Ithnin</keyname><forenames>Norafida Bt</forenames></author></authors><title>Bio-Thentic Card: Authentication concept for RFID Card</title><categories>cs.CR</categories><comments>(IJCSIS) International Journal of Computer Science and Information
  Security 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio frequency identification (RFID) is a technology that employs basic
identifier of an object embedded in a chip, transmitted via radio wave, for
identification. An RFID Card responds to query or interrogation irrespective of
&quot;Who&quot; holds the Card; like a key to a door. Since an attacker can possess the
card, access to such object can therefore be easily compromised. This security
breach is classified as an unauthorized use of Card, and it forms the bedrock
for RFID Card compromise especially in access control. As an on-card
authentication mechanism, this research proposed a concept termed Bio-Thentic
Card, which can be adopted to prevent this single point of failure of RFID
Card. The Bio-Thentic Card was fabricated, tested and assessed in line with the
known threats, and attacks; and it was observed to proffer substantive solution
to unauthorized use of RFID Card vulnerability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5917</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5917</id><created>2012-10-22</created><authors><author><keyname>Nicolas</keyname><forenames>Charbel</forenames></author><author><keyname>Marot</keyname><forenames>Michel</forenames></author></authors><title>Dynamic Link adaptation Based on Coexistence-Fingerprint Detection for
  WSN</title><categories>cs.NI</categories><comments>Ad Hoc Networking Workshop (Med-Hoc-Net), 2012 The 11th Annual
  Mediterranean</comments><doi>10.1109/MedHocNet.2012.6257128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating in the ISM band, the wireless sensor network (WSN) risks being
interfered by other concurrent networks. Our concerns are the technologies that
do not perform listening before transmission such as Bluetooth, and the ones
that do not detect other technologies due to their channel sensing techniques
like WIFI. To overcome this issue a WSN node should be able to identify the
presence of such technologies. This will allow deducing the characteristics of
the generated traffic of these technologies, and thus the behavior of the
channel can be predicted. These predictions would help to trigger adequate
reactions as to avoid or synchronize with the concurrent net- works. Many works
exist on link adaptation, but they concern blind adaptations which are
unintelligent and solve momentarily the problem that may reappear over time.
  In this paper, we perform several experiments on a real testbed to categorize
the model of the bit errors in corrupted received packets. These experiments
are performed under different conditions of channel noise and interferences.
This allows us to identify each corruption pattern as a fingerprint for the
interfering technology. Then we propose the mechanism FIM to identify on the
fly the source of the corruption. With an implementation on &quot;Tmote Sky&quot; motes
using Tinyos1.x, We demonstrate the use of FIM for link adaptation in a
coexistence environment. Our mechanism led to throughput improvements of
87%-100% depending on the transmission rate and channel quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5932</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5932</id><created>2012-10-22</created><updated>2013-01-20</updated><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Physical Layer Network Coding for the K-user Multiple Access Relay
  Channel</title><categories>cs.IT math.IT</categories><comments>More Simulation results added, 12 pages, 10 figures. arXiv admin
  note: substantial text overlap with arXiv:1210.0490</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Physical layer Network Coding (PNC) scheme is proposed for the $K$-user
wireless Multiple Access Relay Channel (MARC), in which $K$ source nodes
transmit their messages to the destination node $D$ with the help of a relay
node $R.$ The proposed PNC scheme involves two transmission phases: (i) Phase 1
during which the source nodes transmit, the relay node and the destination node
receive and (ii) Phase 2 during which the source nodes and the relay node
transmit, and the destination node receives. At the end of Phase 1, the relay
node decodes the messages of the source nodes and during Phase 2 transmits a
many-to-one function of the decoded messages. Wireless networks in which the
relay node decodes, suffer from loss of diversity order if the decoder at the
destination is not chosen properly. A novel decoder is proposed for the PNC
scheme, which offers the maximum possible diversity order of $2,$ for a proper
choice of certain parameters and the network coding map. Specifically, the
network coding map used at the relay is chosen to be a $K$-dimensional Latin
Hypercube, in order to ensure the maximum diversity order of $2.$ Also, it is
shown that the proposed decoder can be implemented by a fast decoding
algorithm. Simulation results presented for the 3-user MARC show that the
proposed scheme offers a large gain over the existing scheme for the $K$-user
MARC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5935</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5935</id><created>2012-10-22</created><authors><author><keyname>Scherer</keyname><forenames>Gabriel</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>R&#xe9;my</keyname><forenames>Didier</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>GADT meet Subtyping</title><categories>cs.PL</categories><comments>No. RR-8114 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While generalized abstract datatypes (GADT) are now considered
well-understood, adding them to a language with a notion of subtyping comes
with a few surprises. What does it mean for a GADT parameter to be covariant?
The answer turns out to be quite subtle. It involves fine-grained properties of
the subtyping relation that raise interesting design questions. We allow
variance annotations in GADT definitions, study their soundness, and present a
sound and complete algorithm to check them. Our work may be applied to
real-world ML-like languages with explicit subtyping such as OCaml, or to
languages with general subtyping constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5936</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5936</id><created>2012-10-22</created><authors><author><keyname>Camus</keyname><forenames>Benjamin</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Siebert</keyname><forenames>Julien</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Bourjot</keyname><forenames>Christine</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Chevrier</keyname><forenames>Vincent</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Mod\'elisation multi-niveaux dans AA4MM</title><categories>cs.MA</categories><proxy>ccsd</proxy><journal-ref>Journ\'ees Francophones sur les Syst\`emes Multi-Agents (2012)
  43-52</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose to represent a multi-level phenomenon as a set of
interacting models. This perspective makes the levels of representation and
their relationships explicit. To deal with coherence, causality and
coordination issues between models, we rely on AA4MM, a metamodel dedicated to
such a representation. We illustrate our proposal and we show the interest of
our approach on a flocking phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5940</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5940</id><created>2012-10-22</created><updated>2012-10-27</updated><authors><author><keyname>Guskov</keyname><forenames>G. K.</forenames></author><author><keyname>Solov'eva</keyname><forenames>F. I.</forenames></author></authors><title>Properties of perfect transitive binary codes of length 15 and extended
  perfect transitive binary codes of length 16</title><categories>math.CO cs.DM cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some properties of perfect transitive binary codes of length 15 and extended
perfect transitive binary codes of length 16 are presented for reference
purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5941</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5941</id><created>2012-10-22</created><authors><author><keyname>Veerappan</keyname><forenames>J.</forenames><affiliation>Department of Electronics and Communication Engineering, Sethu Institute of Technology, Viruthunagar, Tamil Nadu, India</affiliation></author><author><keyname>Pitchammal</keyname><forenames>G.</forenames><affiliation>Anna university, Chennai, Tamil nadu, India</affiliation></author></authors><title>Multilayer image watermarking scheme for providing high security</title><categories>cs.CR cs.MM</categories><comments>6 pages, 6 figures, one table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The main theme of this application is to provide an algorithm color image
watermark to manage the attacks such as rotation, scaling and translation. In
the existing watermarking algorithms, those exploited robust features are more
or less related to the pixel position, so they cannot be more robust against
the attacks. In order to solve this problem this application focus on certain
parameters rather than the pixel position for watermarking. Two statistical
features such as the histogram shape and the mean of Gaussian filtered
low-frequency component of images are taken for this proposed application to
make the watermarking algorithm robust to attacks and also AES technique is
used to provide higher security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5946</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5946</id><created>2012-10-22</created><authors><author><keyname>Maieli</keyname><forenames>Roberto</forenames></author></authors><title>Bipolar Proof Nets for MALL</title><categories>cs.LO</categories><comments>Proceedings of the &quot;Proof, Computation, Complexity&quot; International
  Workshop, 17-18 August 2012, University of Copenhagen, Denmark</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a computation paradigm based on a concurrent and
incremental construction of proof nets (de-sequentialized or graphical proofs)
of the pure multiplicative and additive fragment of Linear Logic, a resources
conscious refinement of Classical Logic. Moreover, we set a correspon- dence
between this paradigm and those more pragmatic ones inspired to transactional
or distributed systems. In particular we show that the construction of additive
proof nets can be interpreted as a model for super-ACID (or co-operative)
transactions over distributed transactional systems (typi- cally,
multi-databases).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5955</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5955</id><created>2012-10-22</created><updated>2013-02-25</updated><authors><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Farias</keyname><forenames>Pablo M. S.</forenames></author><author><keyname>de Souza</keyname><forenames>Cr&#xed;ston P.</forenames></author></authors><title>Insertion and Sorting in a Sequence of Numbers Minimizing the Maximum
  Sum of a Contiguous Subsequence</title><categories>cs.DS cs.CC cs.DM</categories><comments>This paper has been submitted for journal publication</comments><doi>10.1016/j.jda.2013.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a sequence of $n \geq 0$ real numbers. A subsequence of $A$ is a
sequence of contiguous elements of $A$. A \emph{maximum scoring subsequence} of
$A$ is a subsequence with largest sum of its elements, which can be found in
O(n) time by Kadane's dynamic programming algorithm. We consider in this paper
two problems involving maximal scoring subsequences of a sequence. Both of
these problems arise in the context of buffer memory minimization in computer
networks. The first one, which is called {\sc Insertion in a Sequence with
Scores (ISS)}, consists in inserting a given real number $x$ in $A$ in such a
way to minimize the sum of a maximum scoring subsequence of the resulting
sequence, which can be easily done in $O(n^2)$ time by successively applying
Kadane's algorithm to compute the maximum scoring subsequence of the resulting
sequence corresponding to each possible insertion position for $x$. We show in
this paper that the ISS problem can be solved in linear time and space with a
more specialized algorithm. The second problem we consider in this paper is the
{\sc Sorting a Sequence by Scores (SSS)} one, stated as follows: find a
permutation $A'$ of $A$ that minimizes the sum of a maximum scoring
subsequence. We show that the SSS problem is strongly NP-Hard and give a
2-approximation algorithm for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5965</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5965</id><created>2012-10-22</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>Classification Analysis Of Authorship Fiction Texts in The Space Of
  Semantic Fields</title><categories>cs.CL</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of naive Bayesian classifier (NB) and the classifier by the k nearest
neighbors (kNN) in classification semantic analysis of authors' texts of
English fiction has been analysed. The authors' works are considered in the
vector space the basis of which is formed by the frequency characteristics of
semantic fields of nouns and verbs. Highly precise classification of authors'
texts in the vector space of semantic fields indicates about the presence of
particular spheres of author's idiolect in this space which characterizes the
individual author's style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5968</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5968</id><created>2012-10-22</created><updated>2015-12-22</updated><authors><author><keyname>Petrovi&#x107;</keyname><forenames>Tomislav</forenames></author></authors><title>A pair of universal sequence-set betting strategies</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the sequence-set betting game, a generalization of An. A.
Muchnik's non-monotonic betting game. Instead of successively partitioning the
infinite binary strings by their value of a bit at a chosen position, as in the
non-monotonic game, the player is allowed to partition the strings into any two
clopen sets with equal measure. We show that, while there is no single
computable sequence-set betting strategy that predicts all non-Martin-L\&quot;of
random strings, we can construct two strategies such that every
non-Martin-L\&quot;of random string is predicted by at least one of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5973</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5973</id><created>2012-10-10</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Kolo</keyname><forenames>J. G.</forenames></author><author><keyname>Olumide</keyname><forenames>I.</forenames></author></authors><title>A Simple and Reliable Touch Sensitive Security System</title><categories>cs.CR</categories><comments>17 Pages, 7 Figures, Journal Publication</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), vol. 4, issue 5, pp. 149-169, 2012</journal-ref><doi>10.5121/ijnsa.2012.4512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research focuses on detection of unauthorized access to residential and
commercial buildings when the residents are far away from the access gate of
the house. The system is a simple and reliable touch activated security system
and uses sensor technology to revolutionize the standards of living. The system
provides a best solution to most of the problems faced by house owners in their
daily life. Due to its simple electronic components nature, it is more
adaptable and cost-effective. The system is divided into three units; the power
supply unit which employs the use of both DC battery and mains supply to ensure
constant power supply to the circuit, the trigger unit which is responsible for
activating the alarm unit and designed to have much time and period and
moderate sensitivity in order to reduce the rate of false alarm, and the alarm
amplitude unit which main function is to produce amplitude alarm sound when
triggered by the trigger unit with the aim of producing a large audible sound
that can alert the entire neighborhood or scare an intruder away. The design of
the system was achieved by considering some factors such as economy,
availability of components and research materials, efficiency, compatibility
and portability and also durability in the design process. The performance of
the system after test met design specifications. This system works on the
principle of touch sensor. The general operation of the system and performance
is dependent on the presence of an intruder entering through the door and
touching any part of the door. The overall system was constructed and tested
and it work perfectly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5974</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5974</id><created>2012-10-22</created><updated>2012-10-23</updated><authors><author><keyname>Castillo-Andreu</keyname><forenames>H&#xe9;ctor</forenames></author></authors><title>An MML-based tool for evaluating the complexity of (stochastic) logic
  theories</title><categories>cs.LO cs.PL</categories><comments>MsC Thesis</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Theory evaluation is a key problem in many areas: machine learning,
scientific discovery, inverse engineering, decision making, software
engineering, design, human sciences, etc. If we have a set of theories that are
able to explain the same set of phenomena, we need a criterion to choose which
one is best. There are, of course, many possible criteria. Model simplicity is
one of the most common criteria in theory evaluation. The Minimum Message
Length (MML) is a solid approach to evaluate theories relative to a given
evidence or data. Theories can be expressed in specific or general
(Turing-complete) languages. First-order logic, and logic programming in
particular, is a Turing-complete language. Evaluating the simplicity of a
theory or program described in a Turing-complete language is much more
difficult than just counting the number of lines or bits. It is, in fact, the
problem of calculating its Kolmogorov complexity, which is uncomputable. Few
works in the literature have been able to present accurate and effective
approximations for a Turing-complete language. In this work, we present the
first general MML coding scheme for logic programs. With this scheme, we can
quantify the bits of information required to code (or send) a theory, a set of
data or the same data given the theory. As a realization of the above-mentioned
schemes, we present a software tool which is able to code and evaluate a set of
alternative (stochastic) theories (programs) against a set of examples. We
illustrate the application of the tool to a variety of non-probabilistic and
probabilistic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5975</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5975</id><created>2012-10-10</created><authors><author><keyname>Frankie</keyname><forenames>Tasha</forenames></author><author><keyname>Hughes</keyname><forenames>Gordon</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Ken</forenames></author></authors><title>Solid State Disk Object-Based Storage with Trim Commands</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model of NAND flash SSD utilization and write
amplification when the ATA/ATAPI SSD Trim command is incorporated into
object-based storage under a variety of user workloads, including a uniform
random workload with objects of fixed size and a uniform random workload with
objects of varying sizes. We first summarize the existing models for write
amplification in SSDs for workloads with and without the Trim command, then
propose an alteration of the models that utilizes a framework of object-based
storage. The utilization of objects and pages in the SSD is derived, with the
analytic results compared to simulation. Finally, the effect of objects on
write amplification and its computation is discussed along with a potential
application to optimization of SSD usage through object storage metadata
servers that allocate object classes of distinct object size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5980</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5980</id><created>2012-10-22</created><authors><author><keyname>Furche</keyname><forenames>Tim</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Grasso</keyname><forenames>Giovanni</forenames></author><author><keyname>Guo</keyname><forenames>Xiaonan</forenames></author><author><keyname>Orsi</keyname><forenames>Giorgio</forenames></author><author><keyname>Schallhart</keyname><forenames>Christian</forenames></author></authors><title>The Ontological Key: Automatically Understanding and Integrating Forms
  to Access the Deep Web</title><categories>cs.DB</categories><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forms are our gates to the web. They enable us to access the deep content of
web sites. Automatic form understanding provides applications, ranging from
crawlers over meta-search engines to service integrators, with a key to this
content. Yet, it has received little attention other than as component in
specific applications such as crawlers or meta-search engines. No comprehensive
approach to form understanding exists, let alone one that produces rich models
for semantic services or integration with linked open data.
  In this paper, we present OPAL, the first comprehensive approach to form
understanding and integration. We identify form labeling and form
interpretation as the two main tasks involved in form understanding. On both
problems OPAL pushes the state of the art: For form labeling, it combines
features from the text, structure, and visual rendering of a web page. In
extensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern
web forms OPAL outperforms previous approaches for form labeling by a
significant margin. For form interpretation, OPAL uses a schema (or ontology)
of forms in a given domain. Thanks to this domain schema, it is able to produce
nearly perfect (more than 97 percent accuracy in the evaluation domains) form
interpretations. Yet, the effort to produce a domain schema is very low, as we
provide a Datalog-based template language that eases the specification of such
schemata and a methodology for deriving a domain schema largely automatically
from an existing domain ontology. We demonstrate the value of the form
interpretations in OPAL through a light-weight form integration system that
successfully translates and distributes master queries to hundreds of forms
with no error, yet is implemented with only a handful translation rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5984</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5984</id><created>2012-10-22</created><authors><author><keyname>Furche</keyname><forenames>Tim</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Grasso</keyname><forenames>Giovanni</forenames></author><author><keyname>Orsi</keyname><forenames>Giorgio</forenames></author><author><keyname>Schallhart</keyname><forenames>Christian</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author></authors><title>AMBER: Automatic Supervision for Multi-Attribute Extraction</title><categories>cs.DB</categories><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extraction of multi-attribute objects from the deep web is the bridge
between the unstructured web and structured data. Existing approaches either
induce wrappers from a set of human-annotated pages or leverage repeated
structures on the page without supervision. What the former lack in automation,
the latter lack in accuracy. Thus accurate, automatic multi-attribute object
extraction has remained an open challenge.
  AMBER overcomes both limitations through mutual supervision between the
repeated structure and automatically produced annotations. Previous approaches
based on automatic annotations have suffered from low quality due to the
inherent noise in the annotations and have attempted to compensate by exploring
multiple candidate wrappers. In contrast, AMBER compensates for this noise by
integrating repeated structure analysis with annotation-based induction: The
repeated structure limits the search space for wrapper induction, and
conversely, annotations allow the repeated structure analysis to distinguish
noise from relevant data. Both, low recall and low precision in the annotations
are mitigated to achieve almost human quality (more than 98 percent)
multi-attribute object extraction.
  To achieve this accuracy, AMBER needs to be trained once for an entire
domain. AMBER bootstraps its training from a small, possibly noisy set of
attribute instances and a few unannotated sites of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5987</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5987</id><created>2012-10-22</created><authors><author><keyname>Caccioli</keyname><forenames>Fabio</forenames></author><author><keyname>Shrestha</keyname><forenames>Munik</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Farmer</keyname><forenames>J. Doyne</forenames></author></authors><title>Stability analysis of financial contagion due to overlapping portfolios</title><categories>q-fin.GN cs.SI physics.soc-ph q-fin.RM</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common asset holdings are widely believed to have been the primary vector of
contagion in the recent financial crisis. We develop a network approach to the
amplification of financial contagion due to the combination of overlapping
portfolios and leverage, and we show how it can be understood in terms of a
generalized branching process. By studying a stylized model we estimate the
circumstances under which systemic instabilities are likely to occur as a
function of parameters such as leverage, market crowding, diversification, and
market impact. Although diversification may be good for individual
institutions, it can create dangerous systemic effects, and as a result
financial contagion gets worse with too much diversification. Under our model
there is a critical threshold for leverage; below it financial networks are
always stable, and above it the unstable region grows as leverage increases.
The financial system exhibits &quot;robust yet fragile&quot; behavior, with regions of
the parameter space where contagion is rare but catastrophic whenever it
occurs. Our model and methods of analysis can be calibrated to real data and
provide simple yet powerful tools for macroprudential stress testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.5991</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.5991</id><created>2012-10-22</created><updated>2013-03-29</updated><authors><author><keyname>Karahanoglu</keyname><forenames>Nazim Burak</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Online Recovery Guarantees and Analytical Results for OMP</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal Matching Pursuit (OMP) is a simple, yet empirically competitive
algorithm for sparse recovery. Recent developments have shown that OMP
guarantees exact recovery of K-sparse signals with K or more than K iterations
if the observation matrix satisfies the restricted isometry property (RIP) with
some conditions. We develop RIP-based online guarantees for recovery of a
K-sparse signal with more than K OMP iterations. Though these guarantees cannot
be generalized to all sparse signals a priori, we show that they can still hold
online when the state-of-the-art K-step recovery guarantees fail. In addition,
we present bounds on the number of correct and false indices in the support
estimate for the derived condition to be less restrictive than the K-step
guarantees. Under these bounds, this condition guarantees exact recovery of a
K-sparse signal within 3K/2 iterations, which is much less than the number of
steps required for the state-of-the-art exact recovery guarantees with more
than K steps. Moreover, we present phase transitions of OMP in comparison to
basis pursuit and subspace pursuit, which are obtained after extensive recovery
simulations involving different sparse signal types. Finally, we empirically
analyse the number of false indices in the support estimate, which indicates
that these do not violate the developed upper bound in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6001</identifier>
 <datestamp>2013-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6001</id><created>2012-10-22</created><updated>2013-06-07</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author><author><keyname>Mary</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>Reducing statistical time-series problems to binary classification</title><categories>cs.LG stat.ML</categories><comments>In proceedings of NIPS 2012, pp. 2069-2077</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how binary classification methods developed to work on i.i.d. data
can be used for solving statistical problems that are seemingly unrelated to
classification and concern highly-dependent time series. Specifically, the
problems of time-series clustering, homogeneity testing and the three-sample
problem are addressed. The algorithms that we construct for solving these
problems are based on a new metric between time-series distributions, which can
be evaluated using binary classification methods. Universal consistency of the
proposed algorithms is proven under most general assumptions. The theoretical
results are illustrated with experiments on synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6024</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6024</id><created>2012-10-23</created><authors><author><keyname>Borcea</keyname><forenames>Liliana</forenames></author><author><keyname>Callaghan</keyname><forenames>Thomas</forenames></author><author><keyname>Papanicolaou</keyname><forenames>George</forenames></author></authors><title>Motion Estimation and Imaging of Complex Scenes with Synthetic Aperture
  Radar</title><categories>math.NA cs.IT math.IT</categories><comments>34 pages, 23 figures</comments><doi>10.1088/0266-5611/29/5/054011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study synthetic aperture radar (SAR) imaging and motion estimation of
complex scenes consisting of stationary and moving targets. We use the classic
SAR setup with a single antenna emitting signals and receiving the echoes from
the scene. The known motion estimation methods for such setups work only in
simple cases, with one or a few targets in the same motion. We propose to
extend the applicability of these methods to complex scenes, by complementing
them with a data pre-processing step intended to separate the echoes from the
stationary targets and the moving ones. We present two approaches. The first is
an iteration designed to subtract the echoes from the stationary targets one by
one. It estimates the location of each stationary target from a preliminary
image, and then uses it to define a filter that removes its echo from the data.
The second approach is based on the robust principle component analysis (PCA)
method. The key observation is that with appropriate pre-processing and
windowing, the discrete samples of the stationary target echoes form a low rank
matrix, whereas the samples of a few moving target echoes form a high rank
sparse matrix. The robust PCA method is designed to separate the low rank from
the sparse part, and thus can be used for the SAR data separation. We present a
brief analysis of the two methods and explain how they can be combined to
improve the data separation for extended and complex imaging scenes. We also
assess the performance of the methods with extensive numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6044</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6044</id><created>2012-10-22</created><updated>2013-03-14</updated><authors><author><keyname>Lucas</keyname><forenames>Andrew</forenames></author><author><keyname>Lee</keyname><forenames>Ching Hua</forenames></author></authors><title>Multistable binary decision making on networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>v3: mostly published version; v2: fixed minor textual errors; 21
  pages, 8 figures, 1 table</comments><journal-ref>Physical Review E87 (2013) 032806</journal-ref><doi>10.1103/PhysRevE.87.032806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple model for a binary decision making process on a graph,
motivated by modeling social decision making with cooperative individuals. The
model is similar to a random field Ising model or fiber bundle model, but with
key differences on heterogeneous networks. For many types of disorder and
interactions between the nodes, we predict discontinuous phase transitions with
mean field theory which are largely independent of network structure. We show
how these phase transitions can also be understood by studying microscopic
avalanches, and describe how network structure enhances fluctuations in the
distribution of avalanches. We suggest theoretically the existence of a
&quot;glassy&quot; spectrum of equilibria associated with a typical phase, even on
infinite graphs, so long as the first moment of the degree distribution is
finite. This behavior implies that the model is robust against noise below a
certain scale, and also that phase transitions can switch from discontinuous to
continuous on networks with too few edges. Numerical simulations suggest that
our theory is accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6052</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6052</id><created>2012-10-22</created><authors><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>Leveraging Peer Centrality in the Design of Socially-Informed
  Peer-to-Peer Systems</title><categories>cs.SI cs.DC</categories><comments>18 double-column IEEE journal pages, 14 figures, shorter version
  submitted to IEEE Transactions in Parallel and Distributed Systems for review</comments><acm-class>C.2.4; E.1; D.4.7; H.3.3; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social applications mine user social graphs to improve performance in search,
provide recommendations, allow resource sharing and increase data privacy. When
such applications are implemented on a peer-to-peer (P2P) architecture, the
social graph is distributed on the P2P system: the traversal of the social
graph translates into a socially-informed routing in the peer-to-peer layer. In
this work we introduce the model of a projection graph that is the result of
decentralizing a social graph onto a peer-to-peer network. We focus on three
social network metrics: degree, node betweenness and edge betweenness
centrality and analytically formulate the relation between metrics in the
social graph and in the projection graph. Through experimental evaluation on
real networks, we demonstrate that when mapping user communities of sizes up to
50-150 users on each peer, the association between the properties of the social
graph and the projection graph is high, and thus the properties of the
(dynamic) projection graph can be inferred from the properties of the (slower
changing) social graph. Furthermore, we demonstrate with two application
scenarios on large-scale social networks the usability of the projection graph
in designing social search applications and unstructured P2P overlays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6070</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6070</id><created>2012-10-22</created><updated>2013-06-10</updated><authors><author><keyname>Pan</keyname><forenames>Wei</forenames></author><author><keyname>Ghoshal</keyname><forenames>Gourab</forenames></author><author><keyname>Krumme</keyname><forenames>Coco</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Urban characteristics attributable to density-driven tie formation</title><categories>physics.soc-ph cs.SI</categories><comments>Early version of this paper was presented in NetSci 2012 as a
  contributed talk in June 2012. An improved version of this paper is published
  in Nature Communications in June 2013. It has 14 pages and 5 figures</comments><journal-ref>W Pan et al., Urban characteristics attributable to density-driven
  tie formation, Nature Communications 4 1961, (2003)</journal-ref><doi>10.1038/ncomms2961</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by empirical evidence on the interplay between geography,
population density and societal interaction, we propose a generative process
for the evolution of social structure in cities. Our analytical and simulation
results predict both super-linear scaling of social tie density and information
flow as a function of the population. We demonstrate that our model provides a
robust and accurate fit for the dependency of city characteristics with city
size, ranging from individual-level dyadic interactions (number of
acquaintances, volume of communication) to population-level variables
(contagious disease rates, patenting activity, economic productivity and crime)
without the need to appeal to modularity, specialization, or hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6076</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6076</id><created>2012-10-19</created><authors><author><keyname>Mtibaa</keyname><forenames>Sabri</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>An Automated Petri-Net Based Approach for Change Management in
  Distributed Telemedicine Environment</title><categories>cs.SE cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1210.5516</comments><journal-ref>Journal of Telecommunications, Volume 15, Issue 1, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The worldwide healthcare industry is facing a number of daunting challenges
which are forcing healthcare systems worldwide to adapt and transform, and will
ultimately completely redefine the way they do business and deliver care for
patients. In this paper, we present a distributed telemedicine environement
reaping from both the benefits of Service Oriented Approach (SOA) and the
strong telecoms capabilities. We propose an automated approach to handle
changes in a distributed telemedicine environement. A combined Petri nets model
to handle changes and Reconfigurable Petri nets model to react to these changes
are used to fulfill telemedicine functional and non functional requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6082</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6082</id><created>2012-10-22</created><authors><author><keyname>Churchill</keyname><forenames>Richard L.</forenames></author></authors><title>Interplay: Dispersed Activation in Neural Networks</title><categories>cs.NE q-bio.NC</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a multi-point stimulation of a Hebbian neural network
with investigation of the interplay between the stimulus waves through the
neurons of the network. Equilibrium of the resulting memory is achieved for
recall of specific memory data at a rate faster than single point stimulus. The
interplay of the intersecting stimuli appears to parallel the clarification
process of recall in biological systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6095</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6095</id><created>2012-10-22</created><authors><author><keyname>Akoum</keyname><forenames>Salam</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Interference Coordination: Random Clustering and Adaptive Limited
  Feedback</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2238933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference coordination improves data rates and reduces outages in cellular
networks. Accurately evaluating the gains of coordination, however, is
contingent upon using a network topology that models realistic cellular
deployments. In this paper, we model the base stations locations as a Poisson
point process to provide a better analytical assessment of the performance of
coordination. Since interference coordination is only feasible within clusters
of limited size, we consider a random clustering process where cluster stations
are located according to a random point process and groups of base stations
associated with the same cluster coordinate. We assume channel knowledge is
exchanged among coordinating base stations, and we analyze the performance of
interference coordination when channel knowledge at the transmitters is either
perfect or acquired through limited feedback. We apply intercell interference
nulling (ICIN) to coordinate interference inside the clusters. The feasibility
of ICIN depends on the number of antennas at the base stations. Using tools
from stochastic geometry, we derive the probability of coverage and the average
rate for a typical mobile user. We show that the average cluster size can be
optimized as a function of the number of antennas to maximize the gains of
ICIN. To minimize the mean loss in rate due to limited feedback, we propose an
adaptive feedback allocation strategy at the mobile users. We show that
adapting the bit allocation as a function of the signals' strength increases
the achievable rate with limited feedback, compared to equal bit partitioning.
Finally, we illustrate how this analysis can help solve network design problems
such as identifying regions where coordination provides gains based on average
cluster size, number of antennas, and number of feedback bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6110</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6110</id><created>2012-10-22</created><authors><author><keyname>Lampropoulos</keyname><forenames>Leonidas</forenames><affiliation>National Technical University of Athens</affiliation></author><author><keyname>Sagonas</keyname><forenames>Konstantinos</forenames><affiliation>National Technical University of Athens, Uppsala University, Sweden</affiliation></author></authors><title>Automatic WSDL-guided Test Case Generation for PropEr Testing of Web
  Services</title><categories>cs.SE cs.NI</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783</comments><proxy>EPTCS</proxy><acm-class>D.2.5; H.3.5</acm-class><journal-ref>EPTCS 98, 2012, pp. 3-16</journal-ref><doi>10.4204/EPTCS.98.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With web services already being key ingredients of modern web systems,
automatic and easy-to-use but at the same time powerful and expressive testing
frameworks for web services are increasingly important. Our work aims at fully
automatic testing of web services: ideally the user only specifies properties
that the web service is expected to satisfy, in the form of input-output
relations, and the system handles all the rest. In this paper we present in
detail the component which lies at the heart of this system: how the WSDL
specification of a web service is used to automatically create test case
generators that can be fed to PropEr, a property-based testing tool, to create
structurally valid random test cases for its operations and check its
responses. Although the process is fully automatic, our tool optionally allows
the user to easily modify its output to either add semantic information to the
generators or write properties that test for more involved functionality of the
web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6111</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6111</id><created>2012-10-22</created><authors><author><keyname>Almendros-Jim&#xe9;nez</keyname><forenames>Jes&#xfa;s M.</forenames><affiliation>University of Almeria</affiliation></author><author><keyname>Iribarne</keyname><forenames>Luis</forenames><affiliation>University of Almeria</affiliation></author></authors><title>Model Validation in Ontology Based Transformations</title><categories>cs.LO cs.SE</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783</comments><proxy>EPTCS</proxy><acm-class>D.2.2;D.3.2;</acm-class><journal-ref>EPTCS 98, 2012, pp. 17-30</journal-ref><doi>10.4204/EPTCS.98.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model Driven Engineering (MDE) is an emerging approach of software
engineering. MDE emphasizes the construction of models from which the
implementation should be derived by applying model transformations. The
Ontology Definition Meta-model (ODM) has been proposed as a profile for UML
models of the Web Ontology Language (OWL). In this context, transformations of
UML models can be mapped into ODM/OWL transformations. On the other hand, model
validation is a crucial task in model transformation. Meta-modeling permits to
give a syntactic structure to source and target models. However, semantic
requirements have to be imposed on source and target models. A given
transformation will be sound when source and target models fulfill the
syntactic and semantic requirements. In this paper, we present an approach for
model validation in ODM based transformations. Adopting a logic programming
based transformational approach we will show how it is possible to transform
and validate models. Properties to be validated range from structural and
semantic requirements of models (pre and post conditions) to properties of the
transformation (invariants). The approach has been applied to a well-known
example of model transformation: the Entity-Relationship (ER) to Relational
Model (RM) transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6112</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6112</id><created>2012-10-22</created><authors><author><keyname>Smith</keyname><forenames>James</forenames></author></authors><title>The Jasper Framework: Towards a Platform Independent, Formal Treatment
  of Web Programming</title><categories>cs.SE cs.LO cs.PL</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783. Added doi references where
  possible</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 98, 2012, pp. 31-45</journal-ref><doi>10.4204/EPTCS.98.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Jasper, a web programming framework which allows web
applications to be developed in an essentially platform indepedent manner and
which is also suited to a formal treatment. It outlines Jasper conceptually and
shows how Jasper is implemented on several commonplace platforms. It also
introduces the Jasper Music Store, a web application powered by Jasper and
implemented on each of these platforms. And it briefly describes a formal
treatment and outlines the tools and languages planned that will allow this
treatment to be automated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6113</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6113</id><created>2012-10-22</created><authors><author><keyname>L&#xf3;pez</keyname><forenames>Sergio</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author><author><keyname>Silva</keyname><forenames>Josep</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author><author><keyname>Insa</keyname><forenames>David</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author></authors><title>Using the DOM Tree for Content Extraction</title><categories>cs.IR</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 98, 2012, pp. 46-59</journal-ref><doi>10.4204/EPTCS.98.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main information of a webpage is usually mixed between menus,
advertisements, panels, and other not necessarily related information; and it
is often difficult to automatically isolate this information. This is precisely
the objective of content extraction, a research area of widely interest due to
its many applications. Content extraction is useful not only for the final
human user, but it is also frequently used as a preprocessing stage of
different systems that need to extract the main content in a web document to
avoid the treatment and processing of other useless information. Other
interesting application where content extraction is particularly used is
displaying webpages in small screens such as mobile phones or PDAs. In this
work we present a new technique for content extraction that uses the DOM tree
of the webpage to analyze the hierarchical relations of the elements in the
webpage. Thanks to this information, the technique achieves a considerable
recall and precision. Using the DOM structure for content extraction gives us
the benefits of other approaches based on the syntax of the webpage (such as
characters, words and tags), but it also gives us a very precise information
regarding the related components in a block, thus, producing very cohesive
blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6114</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6114</id><created>2012-10-22</created><authors><author><keyname>Michaux</keyname><forenames>Jonathan</forenames><affiliation>T&#xe9;l&#xe9;com ParisTech</affiliation></author><author><keyname>Najm</keyname><forenames>Elie</forenames><affiliation>T&#xe9;l&#xe9;com ParisTech</affiliation></author><author><keyname>Fantechi</keyname><forenames>Alessandro</forenames><affiliation>Universit&#xe0; degli Studi di Firenze</affiliation></author></authors><title>Adding Sessions to BPEL</title><categories>cs.PL</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 98, 2012, pp. 60-76</journal-ref><doi>10.4204/EPTCS.98.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By considering an essential subset of the BPEL orchestration language, we
define SeB, a session based style of this subset. We discuss the formal
semantics of SeB and we present its main properties. We use a new approach to
address the formal semantics, based on a translation into so-called control
graphs. Our semantics handles control links and addresses the static semantics
that prescribes the valid usage of variables. We also provide the semantics of
collections of networked services.
  Relying on these semantics, we define precisely what is meant by interaction
safety, paving the way to the formal analysis of safe interactions between BPEL
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6115</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6115</id><created>2012-10-22</created><authors><author><keyname>Rauf</keyname><forenames>Irum</forenames><affiliation>&#xc5;bo Akademi</affiliation></author><author><keyname>Khan</keyname><forenames>Ali Hanzala</forenames><affiliation>&#xc5;bo Akademi</affiliation></author><author><keyname>Porres</keyname><forenames>Ivan</forenames><affiliation>&#xc5;bo Akademi</affiliation></author></authors><title>Analyzing Consistency of Behavioral REST Web Service Interfaces</title><categories>cs.SE</categories><comments>In Proceedings WWV 2012, arXiv:1210.5783</comments><proxy>EPTCS</proxy><acm-class>Design; Languages; Verification</acm-class><journal-ref>EPTCS 98, 2012, pp. 77-92</journal-ref><doi>10.4204/EPTCS.98.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  REST web services can offer complex operations that do more than just simply
creating, retrieving, updating and deleting information from a database. We
have proposed an approach to design the interfaces of behavioral REST web
services by defining a resource and a behavioral model using UML. In this paper
we discuss the consistency between the resource and behavioral models that
represent service states using state invariants. The state invariants are
defined as predicates over resources and describe what are the valid state
configurations of a behavioral model. If a state invariant is unsatisfiable
then there is no valid state configuration containing the state and there is no
service that can implement the service interface. We also show how we can use
reasoning tools to determine the consistency between these design models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6118</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6118</id><created>2012-10-22</created><authors><author><keyname>Wijs</keyname><forenames>Anton</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Bo&#x161;na&#x10d;ki</keyname><forenames>Dragan</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Edelkamp</keyname><forenames>Stefan</forenames><affiliation>University of Bremen</affiliation></author></authors><title>Proceedings First Workshop on GRAPH Inspection and Traversal Engineering</title><categories>cs.DS cs.DM cs.LO</categories><proxy>EPTCS</proxy><acm-class>D.1.3; D.2.4; G.2.2; I.2.8</acm-class><journal-ref>EPTCS 99, 2012</journal-ref><doi>10.4204/EPTCS.99</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are the proceedings of the First Workshop on GRAPH Inspection and
Traversal Engineering (GRAPHITE 2012), which took place on April 1, 2012 in
Tallinn, Estonia, as a satellite event of the 15th European Joint Conferences
on Theory and Practice of Software (ETAPS 2012).
  The topic of the GRAPHITE workshop is graph search in all its forms in
computer science. Graph search algorithms tend to have common characteristics,
such as duplicate state detection, independent of their application domain.
Over the past few years, it has been shown that the scalability of such
algorithms can be dramatically improved by using, e.g., external memory, by
exploiting parallel architectures, such as clusters, multi-core CPUs, and
graphics processing units, and by using heuristics to guide the search. The
goal of this event is to gather scientists from different communities, such as
model checking, artificial intelligence planning, game playing, and algorithm
engineering, who do research on graph search algorithms, such that awareness of
each others' work is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6119</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6119</id><created>2012-10-22</created><authors><author><keyname>Cabarle</keyname><forenames>Francis George C.</forenames></author><author><keyname>Bu&#xf1;o</keyname><forenames>Kelvin C.</forenames></author><author><keyname>Adorna</keyname><forenames>Henry N.</forenames></author></authors><title>Time After Time: Notes on Delays In Spiking Neural P Systems</title><categories>cs.NE cs.DC cs.ET</categories><comments>11 pages, 9 figures, 4 lemmas, 1 theorem, preprint of Workshop on
  Computation: Theory and Practice 2012 at DLSU, Manila together with UP
  Diliman, DLSU, Tokyo Institute of Technology, and Osaka university</comments><msc-class>97P20</msc-class><acm-class>F.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking Neural P systems, SNP systems for short, are biologically inspired
computing devices based on how neurons perform computations. SNP systems use
only one type of symbol, the spike, in the computations. Information is encoded
in the time differences of spikes or the multiplicity of spikes produced at
certain times. SNP systems with delays (associated with rules) and those
without delays are two of several Turing complete SNP system variants in
literature. In this work we investigate how restricted forms of SNP systems
with delays can be simulated by SNP systems without delays. We show the
simulations for the following spike routing constructs: sequential, iteration,
join, and split.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6122</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6122</id><created>2012-10-23</created><authors><author><keyname>Munaga</keyname><forenames>Hazarath</forenames></author><author><keyname>Jarugumalli</keyname><forenames>Venkata</forenames></author></authors><title>Performance Evaluation: Ball-Tree and KD-Tree in the Context of MST</title><categories>cs.PF</categories><comments>4 pages</comments><journal-ref>http://link.springer.com/chapter/10.1007%2F978-3-642-32573-1_38?LI=true 2012</journal-ref><doi>10.1007/978-3-642-32573-1_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a days many algorithms are invented or being inventing to find the
solution for Euclidean Minimum Spanning Tree, EMST, problem, as its
applicability is increasing in much wide range of fields containing spatial or
spatio temporal data viz. astronomy which consists of millions of spatial data.
To solve this problem, we are presenting a technique by adopting the dual tree
algorithm for finding efficient EMST and experimented on a variety of real time
and synthetic datasets. This paper presents the observed experimental
observations and the efficiency of the dual tree framework, in the context of
kdtree and ball tree on spatial datasets of different dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6123</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6123</id><created>2012-10-23</created><authors><author><keyname>Wang</keyname><forenames>Dao-Shun</forenames></author><author><keyname>Song</keyname><forenames>Tao</forenames></author><author><keyname>Dong</keyname><forenames>Lin</forenames></author><author><keyname>Yang</keyname><forenames>Ching-Nung</forenames></author></authors><title>Optimal Contrast Greyscale Visual Cryptography Schemes with Reversing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual cryptography scheme (VCS) is an encryption technique that utilizes
human visual system in recovering secret image and it does not require any
complex calculation. However, the contrast of the reconstructed image could be
quite low. A number of reversing-based VCSs (or VCSs with reversing) (RVCS)
have been proposed for binary secret images, allowing participants to perform a
reversing operation on shares (or shadows). This reversing operation can be
easily implemented by current copy machines. Some existing traditional VCS
schemes without reversing (nRVCS) can be extended to RVCS with the same pixel
expansion for binary image, and the RVCS can achieve ideal contrast,
significantly higher than that of the corresponding nRVCS. In the application
of greyscale VCS, the contrast is much lower than that of the binary cases.
Therefore, it is more desirable to improve the contrast in the greyscale image
reconstruction. However, when greyscale images are involved, one cannot take
advantage of this reversing operation so easily. Many existing greyscale nRVCS
cannot be directly extended to RVCS. In this paper, we first give a new
greyscale nRVCS with minimum pixel expansion and propose an optimal-contrast
greyscale RVCS (GRVCS) by using basis matrices of perfect black nRVCS. Also, we
propose an optimal GRVCS even though the basis matrices are not perfect black.
Finally, we design an optimal-contrast GRVCS with minimum number of shares held
by each participant. The proposed schemes can satisfy different user
requirement, previous RVCSs for binary images can be viewed as special cases in
the schemes proposed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6128</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6128</id><created>2012-10-23</created><authors><author><keyname>Sharma</keyname><forenames>Tarun Kumar</forenames></author><author><keyname>Pant</keyname><forenames>Millie</forenames></author><author><keyname>Singh</keyname><forenames>V. P.</forenames></author></authors><title>Improved Local Search in Artificial Bee Colony using Golden Section
  Search</title><categories>cs.AI cs.CE</categories><comments>6 Pages, Journal of Engineering (JOE), World Science Publisher 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Artificial bee colony (ABC), an optimization algorithm is a recent addition
to the family of population based search algorithm. ABC has taken its
inspiration from the collective intelligent foraging behavior of honey bees. In
this study we have incorporated golden section search mechanism in the
structure of basic ABC to improve the global convergence and prevent to stick
on a local solution. The proposed variant is termed as ILS-ABC. Comparative
numerical results with the state-of-art algorithms show the performance of the
proposal when applied to the set of unconstrained engineering design problems.
The simulated results show that the proposed variant can be successfully
applied to solve real life problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6134</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6134</id><created>2012-10-23</created><authors><author><keyname>Vyavahare</keyname><forenames>Pooja</forenames></author><author><keyname>Limaye</keyname><forenames>Nutan</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>In-Network Estimation of Frequency Moments</title><categories>cs.NI cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the problem of estimating functions of distributed data using a
distributed algorithm over a network. The extant literature on computing
functions in distributed networks such as wired and wireless sensor networks
and peer-to-peer networks deals with computing linear functions of the
distributed data when the alphabet size of the data values is small, O(1). We
describe a distributed randomized algorithm to estimate a class of non-linear
functions of the distributed data which is over a large alphabet. We consider
three types of networks: point-to-point networks with gossip based
communication, random planar networks in the connectivity regime and random
planar networks in the percolating regime both of which use the slotted Aloha
communication protocol. For each network type, we estimate the scaled $k$-th
frequency moments, for $k \geq 2$. Specifically, for every $k \geq 2,$ we give
a distributed randomized algorithm that computes, with probability
$(1-\delta),$ an $\epsilon$-approximation of the scaled $k$-th frequency
moment, $F_k/N^k$, using time $O(M^{1-\frac{1}{k-1}} T)$ and
$O(M^{1-\frac{1}{k-1}} \log N \log (\delta^{-1})/\epsilon^2)$ bits of
transmission per communication step. Here, $N$ is the number of nodes in the
network, $T$ is the information spreading time and $M=o(N)$ is the alphabet
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6142</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6142</id><created>2012-10-23</created><authors><author><keyname>Min</keyname><forenames>Yong</forenames></author><author><keyname>Ge</keyname><forenames>Ying</forenames></author><author><keyname>Jin</keyname><forenames>Xiaogang</forenames></author><author><keyname>Chang</keyname><forenames>Jie</forenames></author></authors><title>Cooperating epidemics of foodborne diseases with diverse trade networks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>25 pages, 5 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The frequent outbreak of severe foodborne diseases warns of a potential
threat that the global trade networks could spread fatal pathogens. The global
trade network is a typical overlay network, which compounds multiple standalone
trade networks representing the transmission of a single product and connecting
the same set of countries and territories through their own set of trade
interactions. Although the epidemic dynamic implications of overlay networks
have been debated in recent studies, some general answers for the overlay of
multiple and diverse standalone networks remain elusive, especially the
relationship between the heterogeneity and diversity of a set of standalone
networks and the behavior of the overlay network. In this paper, we establish a
general analysis framework for multiple overlay networks based on diversity
theory. The framework could reveal the critical epidemic mechanisms beyond
overlay processes. Applying the framework to global trade networks, we found
that, although the distribution of connectivity of standalone trade networks
was highly heterogeneous, epidemic behavior on overlay networks is more
dependent on cooperation among standalone trade networks rather than on a few
high-connectivity networks as the general property of complex systems with
heterogeneous distribution. Moreover, the analysis of overlay trade networks
related to 7 real pathogens also suggested that epidemic behavior is not
controlled by high-connectivity goods but that the actual compound mode of
overlay trade networks plays a critical role in spreading pathogens. Finally,
we study the influence of cooperation mechanisms on the stability of overlay
networks and on the control of global epidemics. The framework provides a
general tool to study different problems on overlay networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6147</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6147</id><created>2012-10-23</created><authors><author><keyname>Pandolfi</keyname><forenames>Luciano</forenames></author></authors><title>Traction, deformation and velocity of deformation in a viscoelastic
  string</title><categories>math-ph cs.SY math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a viscoelastic string whose deformation is
controlled at one end. We study the relations and the controllability of the
couples traction/velocity and traction/deformation and we show that the first
couple behaves very like as in the purely elastic case, while new phenomena
appears when studying the couple of the traction and the deformation. Namely,
while traction and velocity are independent (for large time), traction and
deformation are related at each time but the relation is not so strict. In fact
we prove that an arbitrary number of &quot;Fourier&quot; components of the traction and,
independently, of the deformation can be assigned at any time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6154</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6154</id><created>2012-10-23</created><updated>2012-10-26</updated><authors><author><keyname>Gutierrez-Corea</keyname><forenames>Federico-Vladimir</forenames><affiliation>UNI</affiliation></author><author><keyname>Urrutia-Zambrana</keyname><forenames>Adolfo-Javier</forenames><affiliation>UNI</affiliation></author></authors><title>Propuesta de sistema GeoInform\'atico con representaci\'on de escenarios
  para auxiliar en la nueva metodolog\'ia propuesta por INETER y la UNI para el
  estudio a gran escala de la vulnerabilidad y da\~nos debido a sismos en las
  edificaciones</title><categories>cs.OH</categories><comments>Published date: 02/07/2007</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A GIS based software is presented which permits the estimation of seismic
vulnerability and the presentation of results in digital maps for single
houses, groups of buildings, parts of settlements or even complete towns.
Nicaragua is a country with a high seismic activity. The assessment of seismic
vulnerability requires the execution of distinct tasks, e.g. recollection of
field data, integration of data from the municipal cadastre, reprocessing or
screening to test the reliability of the data, definition of calculation of
vulnerability functions, calculation of vulnerability for single objects as
houses or buildings, calculation of mean vulnerability for certain areas as
barrios or squares. In order to reduce time and effort to be spent with several
unspecialized tools and procedures, an integrated software system was created,
the user of which has not to care about separate software tools for each part
of the process. The main advantage of the software is the combination of
Geographical Information System (GIS) with the logics that surrounds the
specific methodologies of seismic vulnerability index, index of damages and
presentation of results. The new software uses a connection with an external
centralized Enterprise Data Base which stores all the input information and
calculation results and which is automatically synchronized for the
presentation of results using GIS. The cadastral information contains data on
the constructive type of the house, dimensions, year of construction, type of
walls, roof, number of inhabitants, etc.. The system also allows to present
damage scenarios for specific seismic events with given hypocenter and
magnitude. The documentation of the software serves as a guide for students
working on object oriented software engineering by using unified modeling
language (UML) and software logic architecture (3-tiers).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6157</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6157</id><created>2012-10-23</created><authors><author><keyname>Dutta</keyname><forenames>Vibekananda</forenames></author><author><keyname>Kesswani</keyname><forenames>Dr Nishtha</forenames></author><author><keyname>Gahalot</keyname><forenames>Deepti</forenames></author></authors><title>Novel Architecture for 3D model in virtual communities from detected
  face</title><categories>cs.CV</categories><comments>7 pages</comments><journal-ref>http://www.ijascse.in/publications-2012--2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper we suggest how to extract a face from an image, modify
it, characterize it in terms of high-level properties, and apply it to the
creation of a personalized avatar. In this research work we tested, we
implemented the algorithm on several hundred facial images, including many
taken under uncontrolled acquisition conditions, and found to exhibit
satisfactory performance for immediate practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6168</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6168</id><created>2012-10-23</created><updated>2013-01-15</updated><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>Accelerating Iterative Detection for Spatially Coupled Systems by
  Collaborative Training</title><categories>cs.IT math.IT</categories><comments>accepted for publication on IEEE Commun. Lett</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a novel method for accelerating iterative detection for
spatially coupled (SC) systems. An SC system is constructed by one-dimensional
coupling of many subsystems, which are classified into training and propagation
parts. An irregular structure is introduced into the subsystems in the training
part so that information in that part can be detected successfully. The
obtained reliable information may spread over the whole system via the
subsystems in the propagation part. In order to allow the subsystems in the
training part to collaborate, shortcuts between them are created to accelerate
iterative detection for that part. As an example of SC systems, SC
code-division multiple-access (CDMA) systems are considered. Density Evolution
for the SC CDMA systems shows that the proposed method can provide a
significant reduction in the number of iterations for highly loaded systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6176</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6176</id><created>2012-10-23</created><updated>2013-05-01</updated><authors><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>New algorithms for binary jumbled pattern matching</title><categories>cs.DS</categories><doi>10.1016/j.ipl.2013.04.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pattern $P$ and a text $T$, both strings over a binary alphabet, the
binary jumbled string matching problem consists in telling whether any
permutation of $P$ occurs in $T$. The indexed version of this problem, i.e.,
preprocessing a string to efficiently answer such permutation queries, is hard
and has been studied in the last few years. Currently the best bounds for this
problem are $O(n^2/\log^2 n)$ (with O(n) space and O(1) query time) and
$O(r^2\log r)$ (with O(|L|) space and $O(\log|L|)$ query time), where $r$ is
the length of the run-length encoding of $T$ and $|L| = O(n)$ is the size of
the index. In this paper we present new results for this problem. Our first
result is an alternative construction of the index by Badkobeh et al. that
obtains a trade-off between the space and the time complexity. It has
$O(r^2\log k + n/k)$ complexity to build the index, $O(\log k)$ query time, and
uses $O(n/k + |L|)$ space, where $k$ is a parameter. The second result is an
$O(n^2 \log^2 w / w)$ algorithm (with O(n) space and O(1) query time), based on
word-level parallelism where $w$ is the word size in bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6179</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6179</id><created>2012-10-23</created><updated>2012-10-24</updated><authors><author><keyname>Frid</keyname><forenames>Anna E.</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca</forenames></author></authors><title>On minimal factorizations of words as products of palindromes</title><categories>math.CO cs.DM</categories><comments>13 pages, 1 figure, preliminary version reported at Journ\'ees
  Montoises 2012</comments><msc-class>68R15, 05D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite word u, we define its palindromic length |u|_{pal} to be the
least number n such that u=v_1v_2... v_n with each v_i a palindrome. We address
the following open question: Does there exist an infinite non ultimately
periodic word w and a positive integer P such that |u|_{pal}&lt;P for each factor
u of w? We give a partial answer to this question by proving that if an
infinite word w satisfies the so-called (k,l)-condition for some k and l, then
for each positive integer P there exists a factor u of w whose palindromic
length |u|_{pal}&gt;P. In particular, the result holds for all the k-power-free
words and for the Sierpinski word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6192</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6192</id><created>2012-10-23</created><authors><author><keyname>Misra</keyname><forenames>Rachita</forenames></author><author><keyname>ray</keyname><forenames>Kasturika B</forenames></author></authors><title>Textural Approach to Palmprint Identification</title><categories>cs.CV cs.CR cs.GR</categories><comments>9 pages</comments><journal-ref>http://www.ijascse.in/publications-2012--2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometrics which use of human physiological characteristics for identifying
an individual is now a widespread method of identification and authentication.
Biometric identification is a technology which uses several image processing
techniques and describes the general procedure for identification and
verification using feature extraction, storage and matching from the digitized
image of biometric characters such as Finger Print, Face, Iris or Palm Print.
The current paper uses palm print biometrics. Here we have presented an
identification approach using textural properties of palm print images. The
elegance of the method is that the conventional edge detection technique is
extended to suitably describe the texture features. In this technique all the
characteristics of the palm such as principal lines, edges and wrinkles are
considered with equal importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6197</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6197</id><created>2012-10-23</created><authors><author><keyname>Boopathi</keyname><forenames>Marx</forenames></author></authors><title>Game Theory in Oligopoly</title><categories>cs.GT q-fin.GN</categories><comments>6 pages</comments><journal-ref>http://www.ijascse.in/publications-2012--2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game theory techniques are used to find the equilibrium of a market. Game
theory refers to the ways in which strategic interactions among economic agents
produce outcomes with respect to the preferences (or utilities) of those
agents, where the outcomes in question might have been intended by none of the
agents. The oligopolistic market structures are taken and how game theory
applies to them is explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6198</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6198</id><created>2012-10-23</created><updated>2012-10-29</updated><authors><author><keyname>Oliva</keyname><forenames>Gabriele</forenames></author><author><keyname>Panzieri</keyname><forenames>Stefano</forenames></author><author><keyname>Pascucci</keyname><forenames>Federica</forenames></author><author><keyname>Setola</keyname><forenames>Roberto</forenames></author></authors><title>Network Localization by Shadow Edges</title><categories>cs.SY cs.NI</categories><comments>preprint submitted to 2013 European Control Conference, July 17-19
  2013, Zurich, Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization is a fundamental task for sensor networks. Traditional network
construction approaches allow to obtain localized networks requiring the nodes
to be at least tri-connected (in 2D), i.e., the communication graph needs to be
globally rigid. In this paper we exploit, besides the information on the
neighbors sensed by each robot/sensor, also the information about the lack of
communication among nodes. The result is a framework where the nodes are
required to be bi-connected and the communication graph has to be rigid. This
is possible considering a novel typology of link, namely Shadow Edges, that
account for the lack of communication among nodes and allow to reduce the
uncertainty associated to the position of the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6202</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6202</id><created>2012-10-23</created><authors><author><keyname>Dalf&#xf3;</keyname><forenames>C.</forenames></author><author><keyname>Fiol</keyname><forenames>M. A.</forenames></author></authors><title>The (\Delta,D) and (\Delta,N) problems for New Amsterdam and Manhattan
  digraphs</title><categories>math.CO cs.DM</categories><msc-class>05C20, 05C12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a quasi-complete solution of the (\Delta,N) problem for two
well-known families of digraphs used as good models for large interconnection
networks. In our study we also relate both families, the New Amsterdam and
Manhattan digraphs, with the double-step graphs (or circulant graphs with
degree two).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6209</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6209</id><created>2012-10-23</created><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Characteristic of partition-circuit matroid through approximation number</title><categories>cs.AI</categories><comments>12 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough set theory is a useful tool to deal with uncertain, granular and
incomplete knowledge in information systems. And it is based on equivalence
relations or partitions. Matroid theory is a structure that generalizes linear
independence in vector spaces, and has a variety of applications in many
fields. In this paper, we propose a new type of matroids, namely,
partition-circuit matroids, which are induced by partitions. Firstly, a
partition satisfies circuit axioms in matroid theory, then it can induce a
matroid which is called a partition-circuit matroid. A partition and an
equivalence relation on the same universe are one-to-one corresponding, then
some characteristics of partition-circuit matroids are studied through rough
sets. Secondly, similar to the upper approximation number which is proposed by
Wang and Zhu, we define the lower approximation number. Some characteristics of
partition-circuit matroids and the dual matroids of them are investigated
through the lower approximation number and the upper approximation number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6230</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6230</id><created>2012-10-23</created><updated>2012-10-25</updated><authors><author><keyname>Ludue&#xf1;a</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Gros</keyname><forenames>Claudius</forenames></author></authors><title>A Self-Organized Neural Comparator</title><categories>q-bio.NC cond-mat.dis-nn cs.NE</categories><journal-ref>G. A. Ludue\~na and C. Gros, A self-organized neural comparator,
  Neural Computation, 25, pp 1006 (2013)</journal-ref><doi>10.1162/NECO_a_00424</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning algorithms need generally the possibility to compare several streams
of information. Neural learning architectures hence need a unit, a comparator,
able to compare several inputs encoding either internal or external
information, like for instance predictions and sensory readings. Without the
possibility of comparing the values of prediction to actual sensory inputs,
reward evaluation and supervised learning would not be possible.
  Comparators are usually not implemented explicitly, necessary comparisons are
commonly performed by directly comparing one-to-one the respective activities.
This implies that the characteristics of the two input streams (like size and
encoding) must be provided at the time of designing the system.
  It is however plausible that biological comparators emerge from
self-organizing, genetically encoded principles, which allow the system to
adapt to the changes in the input and in the organism.
  We propose an unsupervised neural circuitry, where the function of input
comparison emerges via self-organization only from the interaction of the
system with the respective inputs, without external influence or supervision.
  The proposed neural comparator adapts, unsupervised, according to the
correlations present in the input streams. The system consists of a multilayer
feed-forward neural network which follows a local output minimization
(anti-Hebbian) rule for adaptation of the synaptic weights.
  The local output minimization allows the circuit to autonomously acquire the
capability of comparing the neural activities received from different neural
populations, which may differ in the size of the population and in the neural
encoding used. The comparator is able to compare objects never encountered
before in the sensory input streams and to evaluate a measure of their
similarity, even when differently encoded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6231</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6231</id><created>2012-10-22</created><authors><author><keyname>Abdessamad</keyname><forenames>Benlafkih</forenames></author><author><keyname>Salah-ddine</keyname><forenames>Krit</forenames></author><author><keyname>Mohamed</keyname><forenames>Chafik Elidrissi</forenames></author></authors><title>Designing a High Efficiency Pulse Width Modulation Step-Down DC/DC
  Converter for Mobile Phone Applications</title><categories>cs.OH</categories><comments>7 pages, IJCSI International Journal of Computer Science Issues, Vol.
  9, Issue 5, No 3, September 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents the design and analysis of a high efficiency, PWM
(Pulse-Width-Modulation) Buck converters for mobile phone applications. The
steady-state and average-value models for the proposed converter are developed
and simulated. A practical design approach which aims at systematizing the
procedure for the selection of the control parameters is introduced. The
switching losses are reduced by using soft switching, additionally, a simple
analog and digital form of the controller for practical realization is
provided. It is found that this controller adopts a structure similar to the
conventional PWM voltage mode controller. The proposed circuit uses a
current-mode control and a voltage-to-pulse converter for the PWM. The circuit,
fabricated using a 0.18-{\mu}m CMOS technology, reaches a peak load regulation
of 20 mV/V and line regulation of 0.5 mV/V at Current load equal 300 mA. The
used 10{\mu}H inductance and 22{\mu}F capacitor and requires clock and
Vref/Vramp input of 1,23V.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6234</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6234</id><created>2012-10-23</created><authors><author><keyname>Focke</keyname><forenames>C.</forenames></author><author><keyname>Bothe</keyname><forenames>D.</forenames></author><author><keyname>Kuschel</keyname><forenames>M.</forenames></author><author><keyname>Sommerfeld</keyname><forenames>M.</forenames></author></authors><title>Experiments and Direct Numerical Simulations of binary collisions of
  miscible liquid droplets with different viscosities</title><categories>physics.flu-dyn cs.CE</categories><comments>12th Triennial International Conference on Liquid Atomization and
  Spray Systems, Heidelberg, Germany, September 2-6, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary droplet collisions are of importance in a variety of practical
applications comprising dispersed two-phase flows. The background of our
research is the prediction of properties of particulate products formed in
spray processes. To gain a more thorough understanding of the elementary
sub-processes inside a spray, experiments and direct numerical simulations of
binary droplet collisions are used. The aim of these investigations is to
develop semi-analytical descriptions for the outcome of droplet collisions.
Such collision models can then be employed as closure terms for scale-reduced
simulations. In the present work we focus on the collision of droplets of
different liquids. These kinds of collisions take place in every spray drying
process when droplets with different solids contents collide in recirculation
zones. A new experimental method has been developed allowing for high spatial
and time resolved recordings via Laser-induced fluorescence. The results
obtained with the proposed method will be compared with DNS simulations. The
viscosities of the droplets are different whereas the interfacial tension and
density are equal. The liquids are miscible and no surface tension is acting
between the two liquids. Our intention is to discover elementary phenomena
caused by the viscosity ratio of the droplets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6241</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6241</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Transforming Monitoring Structures with Resilient Encoders. Application
  to Repeated Games</title><categories>cs.IT cs.GT math.IT</categories><comments>Springer, Dynamic Games and Applications, 2012</comments><doi>10.1007/s13235-012-0058-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important feature of a dynamic game is its monitoring structure namely,
what the players effectively see from the played actions. We consider games
with arbitrary monitoring structures. One of the purposes of this paper is to
know to what extent an encoder, who perfectly observes the played actions and
sends a complementary public signal to the players, can establish perfect
monitoring for all the players. To reach this goal, the main technical problem
to be solved at the encoder is to design a source encoder which compresses the
action profile in the most concise manner possible. A special feature of this
encoder is that the multi-dimensional signal (namely, the action profiles) to
be encoded is assumed to comprise a component whose probability distribution is
not known to the encoder and the decoder has a side information (the private
signals received by the players when the encoder is off). This new framework
appears to be both of game-theoretical and information-theoretical interest. In
particular, it is useful for designing certain types of encoders that are
resilient to single deviations and provide an equilibrium utility region in the
proposed setting; it provides a new type of constraints to compress an
information source (i.e., a random variable). Regarding the first aspect, we
apply the derived result to the repeated prisoner's dilemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6242</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6242</id><created>2012-10-23</created><authors><author><keyname>Wiese</keyname><forenames>Lena</forenames></author></authors><title>Enhancing Algebraic Query Relaxation with Semantic Similarity</title><categories>cs.DB</categories><comments>Appeared in Proceedings of IADIS Information Systems 2012 (10-12
  March 2012, Berlin, Germany)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative database systems support a database user by searching for answers
that are closely related to his query and hence are informative answers. Common
operators to relax the user query are Dropping Condition, Anti-Instantiation
and Goal Replacement. In this article, we provide an algebraic version of these
operators. Moreover we propose some heuristics to assign a degree of similarity
to each tuple of an answer table; this degree can help the user to determine
whether this answer is relevant for him or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6266</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6266</id><created>2012-10-23</created><authors><author><keyname>Sampath</keyname><forenames>Rahul S.</forenames></author><author><keyname>Philip</keyname><forenames>Bobby</forenames></author><author><keyname>Allu</keyname><forenames>Srikanth</forenames></author><author><keyname>Simunovic</keyname><forenames>Srdjan</forenames></author></authors><title>Recursive Schur Decomposition</title><categories>math.NA cs.DC cs.NA</categories><msc-class>65F08, 65F10, 65N55, 65Y05, 68W10, 68W15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a parallel recursive algorithm based on
multi-level domain decomposition that can be used as a precondtioner to a
Krylov subspace method to solve sparse linear systems of equations arising from
the discretization of partial differential equations (PDEs). We tested the
effectiveness of the algorithm on several PDEs using different number of
sub-domains (ranging from 8 to 32768) and various problem sizes (ranging from
about 2000 to over a billion degrees of freedom). We report the results from
these tests; the results show that the algorithm scales very well with the
number of sub-domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6267</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6267</id><created>2012-10-23</created><authors><author><keyname>Isikman</keyname><forenames>Arif Onder</forenames><affiliation>Chalmers University of Technology</affiliation></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames><affiliation>Chalmers University of Technology</affiliation></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames><affiliation>Chalmers University of Technology</affiliation></author></authors><title>Phase Noise Estimation for Uncoded/Coded SISO and MIMO Systems</title><categories>cs.IT math.IT</categories><report-no>MS Thesis no: EX061/2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-ideal oscillators both at the transmitter and the receiver introduces
time varying phase noise which interacts with the transmitted data in a
non-linear fashion. Phase noise becomes a detrimental problem and needs to be
estimated and compensated. In this thesis receiver algorithms are derived and
evaluated to mitigate the effects of the phase noise in digital communication
systems.
  In Chapter 3 phase noise estimation in single-input single-output (SISO)
systems is investigated. First, a hard decision directed extended Kalman filter
(EKF) is applied to an uncoded system. Next, an iterative receiver algorithm
performing code-aided turbo synchronization is derived using the expectation
maximization (EM) framework for a coded system. Two soft-decision directed
estimators in the literature based on Kalman filtering are evaluated. Low
density parity check (LDPC) codes are proposed to calculate marginal a
posteriori probabilities and to construct soft decision symbols. Error rate
performance of both estimators are compared through simulations.
  In Chapter 4 phase noise estimation in multi-input multi-output (MIMO)
systems is investigated. First, a low complexity hard decision directed EKF is
applied to an uncoded system. Next, a new receiver algorithm based on the EM
framework for joint estimation and detection in coded MIMO systems is proposed.
A low complexity soft decision directed extended Kalman filter and smoother
(EKFS) that tracks the phase noise parameters over a frame is proposed in order
to carry out the maximization step. The proposed EKFS based approach is
combined with an iterative detector that utilizes bit interleaved coded
modulation and employs LDPC codes. Finally, simulation results confirm that the
error rate performance of the proposed EM-based approach is close to the
scenario of perfect knowledge of phase noise at low-to-medium signal-to-noise
ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6272</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6272</id><created>2012-10-23</created><updated>2013-04-24</updated><authors><author><keyname>Schroeder</keyname><forenames>Rebeca</forenames></author><author><keyname>Mello</keyname><forenames>Ronaldo Santos</forenames></author><author><keyname>Hara</keyname><forenames>Carmem Satie</forenames></author></authors><title>Affinity-based XML Fragmentation</title><categories>cs.DB</categories><comments>This paper has been withdrawn by the author due to its recent
  publication in the conference site</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we tackle the fragmentation problem for highly distributed
databases. In such an environment, a suitable fragmentation strategy may
provide scalability and availability by minimizing distributed transactions. We
propose an approach for XML fragmentation that takes as input both the
application's expected workload and a storage threshold, and produces as output
an XML fragmentation schema. Our workload-aware method aims to minimize the
execution of distributed transactions by packing up related data in a small set
of fragments. We present experiments that compare alternative fragmentation
schemas, showing that the one produced by our technique provides a
finer-grained result and better system throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6275</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6275</id><created>2012-10-23</created><updated>2012-10-24</updated><authors><author><keyname>Marynowski</keyname><forenames>Jo&#xe3;o Eugenio</forenames></author></authors><title>Ambiente de Planejamento Ip\^e</title><categories>cs.AI</categories><comments>MSc dissertation involving Artificial Intelligence, Planning, Petri
  Net, Plangraph, Intelig\^encia Artificial, Planejamento, Redes de Petri e
  Grafo de Planos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the systems that implements algorithms for the
planning problem in Artificial Intelligence, called planners, with especial
attention to the planners based on the plan graph. We analyze the problem of
comparing the performance of the different algorithms and we propose an
environment for the development and analysis of planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6284</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6284</id><created>2012-10-23</created><authors><author><keyname>Giarrusso</keyname><forenames>Paolo G.</forenames></author><author><keyname>Ostermann</keyname><forenames>Klaus</forenames></author><author><keyname>Eichberg</keyname><forenames>Michael</forenames></author><author><keyname>Mitschke</keyname><forenames>Ralf</forenames></author><author><keyname>Rendel</keyname><forenames>Tillmann</forenames></author><author><keyname>K&#xe4;stner</keyname><forenames>Christian</forenames></author></authors><title>Reify Your Collection Queries for Modularity and Speed!</title><categories>cs.PL cs.DB</categories><comments>20 pages</comments><acm-class>H.2.3; D.1.1; D.1.5</acm-class><journal-ref>Proceedings of the 12th annual international conference on
  Aspect-oriented software development (AOSD '13), 2013. ACM, New York, NY,
  USA, 1-12</journal-ref><doi>10.1145/2451436.2451438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity and efficiency are often contradicting requirements, such that
programers have to trade one for the other. We analyze this dilemma in the
context of programs operating on collections. Performance-critical code using
collections need often to be hand-optimized, leading to non-modular, brittle,
and redundant code. In principle, this dilemma could be avoided by automatic
collection-specific optimizations, such as fusion of collection traversals,
usage of indexing, or reordering of filters. Unfortunately, it is not obvious
how to encode such optimizations in terms of ordinary collection APIs, because
the program operating on the collections is not reified and hence cannot be
analyzed.
  We propose SQuOpt, the Scala Query Optimizer--a deep embedding of the Scala
collections API that allows such analyses and optimizations to be defined and
executed within Scala, without relying on external tools or compiler
extensions. SQuOpt provides the same &quot;look and feel&quot; (syntax and static typing
guarantees) as the standard collections API. We evaluate SQuOpt by
re-implementing several code analyses of the Findbugs tool using SQuOpt, show
average speedups of 12x with a maximum of 12800x and hence demonstrate that
SQuOpt can reconcile modularity and efficiency in real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6286</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6286</id><created>2012-10-23</created><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author></authors><title>A one-bit swap object using test-and-sets and a max register</title><categories>cs.DC cs.DS</categories><report-no>YALEU/DCS/TR-1464</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a linearizable, wait-free implementation of a one-bit swap object
from a single max register and an unbounded array of test-and-set bits. Each
swap operation takes at most three steps. Using standard randomized
constructions, the max register and test-and-set bits can be replaced by
read-write registers, at the price of raising the cost of a swap operation to
an expected O(max(log n, min(log t, n))) steps, where t is the number of times
the swap object has previously changed its value and n is the number of
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6287</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6287</id><created>2012-10-23</created><updated>2012-10-26</updated><authors><author><keyname>Curtin</keyname><forenames>Ryan R.</forenames></author><author><keyname>Ram</keyname><forenames>Parikshit</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>Fast Exact Max-Kernel Search</title><categories>cs.DS cs.IR cs.LG</categories><comments>Under submission in SIAM Data Mining conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide applicability of kernels makes the problem of max-kernel search
ubiquitous and more general than the usual similarity search in metric spaces.
We focus on solving this problem efficiently. We begin by characterizing the
inherent hardness of the max-kernel search problem with a novel notion of
directional concentration. Following that, we present a method to use an $O(n
\log n)$ algorithm to index any set of objects (points in $\Real^\dims$ or
abstract objects) directly in the Hilbert space without any explicit feature
representations of the objects in this space. We present the first provably
$O(\log n)$ algorithm for exact max-kernel search using this index. Empirical
results for a variety of data sets as well as abstract objects demonstrate up
to 4 orders of magnitude speedup in some cases. Extensions for approximate
max-kernel search are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6292</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6292</id><created>2012-10-23</created><updated>2014-02-06</updated><authors><author><keyname>Mart&#xed;nez-P&#xe9;rez</keyname><forenames>&#xc1;lvaro</forenames></author></authors><title>A density-sensitive hierarchical clustering method</title><categories>cs.LG</categories><comments>25 pages, 14 figures</comments><msc-class>62H30, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a hierarchical clustering method: $\alpha$-unchaining single
linkage or $SL(\alpha)$. The input of this algorithm is a finite metric space
and a certain parameter $\alpha$. This method is sensitive to the density of
the distribution and offers some solution to the so called chaining effect. We
also define a modified version, $SL^*(\alpha)$, to treat the chaining through
points or small blocks. We study the theoretical properties of these methods
and offer some theoretical background for the treatment of chaining effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6293</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6293</id><created>2012-10-23</created><authors><author><keyname>Curtin</keyname><forenames>Ryan R.</forenames></author><author><keyname>Cline</keyname><forenames>James R.</forenames></author><author><keyname>Slagle</keyname><forenames>N. P.</forenames></author><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Ram</keyname><forenames>Parikshit</forenames></author><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>MLPACK: A Scalable C++ Machine Learning Library</title><categories>cs.MS cs.CV cs.LG</categories><comments>Submitted to JMLR MLOSS (http://jmlr.csail.mit.edu/mloss/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning
library released in late 2011 offering both a simple, consistent API accessible
to novice users and high performance and flexibility to expert users by
leveraging modern features of C++. MLPACK provides cutting-edge algorithms
whose benchmarks exhibit far better performance than other leading machine
learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available
at http://www.mlpack.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6321</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6321</id><created>2012-10-23</created><updated>2013-03-23</updated><authors><author><keyname>Hisano</keyname><forenames>Ryohei</forenames></author><author><keyname>Sornette</keyname><forenames>Didier</forenames></author><author><keyname>Mizuno</keyname><forenames>Takayuki</forenames></author><author><keyname>Ohnishi</keyname><forenames>Takaaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Tsutomu</forenames></author></authors><title>High quality topic extraction from business news explains abnormal
  financial market volatility</title><categories>stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST</categories><comments>The previous version of this article included an error. This is a
  revised version</comments><doi>10.1371/journal.pone.0064846</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the mutual relationships between information flows and social
activity in society today is one of the cornerstones of the social sciences. In
financial economics, the key issue in this regard is understanding and
quantifying how news of all possible types (geopolitical, environmental,
social, financial, economic, etc.) affect trading and the pricing of firms in
organized stock markets. In this article, we seek to address this issue by
performing an analysis of more than 24 million news records provided by
Thompson Reuters and of their relationship with trading activity for 206 major
stocks in the S&amp;P US stock index. We show that the whole landscape of news that
affect stock price movements can be automatically summarized via simple
regularized regressions between trading activity and news information pieces
decomposed, with the help of simple topic modeling techniques, into their
&quot;thematic&quot; features. Using these methods, we are able to estimate and quantify
the impacts of news on trading. We introduce network-based visualization
techniques to represent the whole landscape of news information associated with
a basket of stocks. The examination of the words that are representative of the
topic distributions confirms that our method is able to extract the significant
pieces of information influencing the stock market. Our results show that one
of the most puzzling stylized fact in financial economies, namely that at
certain times trading volumes appear to be &quot;abnormally large,&quot; can be partially
explained by the flow of news. In this sense, our results prove that there is
no &quot;excess trading,&quot; when restricting to times when news are genuinely novel
and provide relevant financial information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6334</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6334</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Resilient Source Coding</title><categories>cs.IT cs.GT math-ph math.IT math.MP</categories><journal-ref>Proc. of the 5th International Conference on Network Games,
  Control and Optimization (NetGCooP), 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a source coding theorem for multi-dimensional information
signals when, at a given instant, the distribution associated with one
arbitrary component of the signal to be compressed is not known and a side
information is available at the destination. This new framework appears to be
both of information-theoretical and game-theoretical interest: it provides a
new type of constraints to compress an information source; it is useful for
designing certain types of mediators in games and characterize utility regions
for games with signals. Regarding the latter aspect, we apply the derived
source coding theorem to the prisoner's dilemma and the battle of the sexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6338</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6338</id><created>2012-10-22</created><authors><author><keyname>Stolfi</keyname><forenames>Guido</forenames><affiliation>University of Sao Paulo</affiliation></author></authors><title>A Ternary Digital to Analog Converter with High Power Output and 170-dB
  Dynamic Range</title><categories>cs.AR</categories><comments>4 pages, 10 figures</comments><msc-class>94C99</msc-class><acm-class>B.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prototype of a very high dynamic range 32-bits Digital to Analog Converter
(DAC) was designed and built for the purpose of direct auditory stimulus
generation. It provides signals from less than 100 nV up to 50 Watts peak power
output, driving a 32-Ohms earphone or speaker. The use of ternary cells makes
possible a 170 dB dynamic range that is basically limited by thermal noise
only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6341</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6341</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>An Achievable Rate Region for the Broadcast Wiretap Channel with
  Asymmetric Side Information</title><categories>cs.IT cs.GT math.IT</categories><journal-ref>Proc. of the 49th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton), 2011</journal-ref><doi>10.1109/Allerton.2011.6120151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication scenario under consideration in this paper corresponds to a
multiuser channel with side information and consists of a broadcast channel
with two legitimate receivers and an eavesdropper. Mainly, the results obtained
are as follows. First, an achievable rate region is provided for the (general)
case of discrete-input discrete-output channels, generalizing existing results.
Second, the obtained theorem is used to derive achievable transmission rates
for two practical cases of Gaussian channels. It is shown that known
perturbations can enlarge the rate region of broadcast wiretap channels with
side information and having side information at the decoder as well can
increase the secrecy rate of channels with side information. Third, we
establish for the first time an explicit connection between multiuser channels
and observation structures in dynamic games. In this respect, we show how to
exploit the proved achievability theorem (discrete case) to derive a
communication-compatible upper bound on the minmax level of a player.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6365</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6365</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>The price of re-establishing perfect, almost perfect or public
  monitoring in games with arbitrary monitoring</title><categories>cs.IT cs.GT math.IT</categories><comments>Proc. of the 4th ACM International Workshop on Game Theory in
  Communication Networks, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes a connection between the notion of observation (or
monitoring) structure in game theory and the one of communication channels in
Shannon theory. One of the objectives is to know under which conditions an
arbitrary monitoring structure can be transformed into a more pertinent
monitoring structure. To this end, a mediator is added to the game. The
objective of the mediator is to choose a signalling scheme that allows the
players to have perfect, almost perfect or public monitoring and all of this,
at a minimum cost in terms of signalling. Graph coloring, source coding, and
channel coding are exploited to deal with these issues. A wireless power
control game is used to illustrate these notions but the applicability of the
provided results and, more importantly, the framework of transforming
monitoring structures go much beyond this example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6366</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6366</id><created>2012-10-23</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Singer</keyname><forenames>Michael F.</forenames></author></authors><title>On the Summability of Bivariate Rational Functions</title><categories>math.CO cs.SC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present criteria for deciding whether a bivariate rational function in two
variables can be written as a sum of two (q-)differences of bivariate rational
functions. Using these criteria, we show how certain double sums can be
evaluated, first, in terms of single sums and, finally, in terms of values of
special functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6367</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6367</id><created>2012-10-23</created><updated>2014-07-23</updated><authors><author><keyname>Brandao</keyname><forenames>Fernando G. S. L.</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Quantum de Finetti Theorems under Local Measurements with Applications</title><categories>quant-ph cs.CC</categories><comments>39 pages, no figure. v2: changes to references and other minor
  improvements. v3: added some explanations, mostly about Theorem 1 and
  Conjecture 5. STOC version. v4, v5. small improvements and fixes</comments><journal-ref>Proc. of the 45th ACM Symposium on theory of computing (STOC
  2013), pp. 861-870</journal-ref><doi>10.1145/2488608.2488718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum de Finetti theorems are a useful tool in the study of correlations in
quantum multipartite states. In this paper we prove two new quantum de Finetti
theorems, both showing that under tests formed by local measurements one can
get a much improved error dependence on the dimension of the subsystems. We
also obtain similar results for non-signaling probability distributions. We
give the following applications of the results:
  We prove the optimality of the Chen-Drucker protocol for 3-SAT, under the
exponential time hypothesis.
  We show that the maximum winning probability of free games can be estimated
in polynomial time by linear programming. We also show that 3-SAT with m
variables can be reduced to obtaining a constant error approximation of the
maximum winning probability under entangled strategies of O(m^{1/2})-player
one-round non-local games, in which the players communicate O(m^{1/2}) bits all
together.
  We show that the optimization of certain polynomials over the hypersphere can
be performed in quasipolynomial time in the number of variables n by
considering O(log(n)) rounds of the Sum-of-Squares (Parrilo/Lasserre) hierarchy
of semidefinite programs. As an application to entanglement theory, we find a
quasipolynomial-time algorithm for deciding multipartite separability.
  We consider a result due to Aaronson -- showing that given an unknown n qubit
state one can perform tomography that works well for most observables by
measuring only O(n) independent and identically distributed (i.i.d.) copies of
the state -- and relax the assumption of having i.i.d copies of the state to
merely the ability to select subsystems at random from a quantum multipartite
state.
  The proofs of the new quantum de Finetti theorems are based on information
theory, in particular on the chain rule of mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6370</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6370</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>&quot;To sense&quot; or &quot;not to sense&quot; in energy-efficient power control games</title><categories>cs.GT cs.IT math.IT</categories><comments>Proc. of the 2nd International Conference on Game Theory for Network
  (GAMENETS), 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network of cognitive transmitters is considered. Each transmitter has to
decide his power control policy in order to maximize energy-efficiency of his
transmission. For this, a transmitter has two actions to take. He has to decide
whether to sense the power levels of the others or not (which corresponds to a
finite sensing game), and to choose his transmit power level for each block
(which corresponds to a compact power control game). The sensing game is shown
to be a weighted potential game and its set of correlated equilibria is
studied. Interestingly, it is shown that the general hybrid game where each
transmitter can jointly choose the hybrid pair of actions (to sense or not to
sense, transmit power level) leads to an outcome which is worse than the one
obtained by playing the sensing game first, and then playing the power control
game. This is an interesting Braess-type paradox to be aware of for
energy-efficient power control in cognitive networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6378</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6378</id><created>2012-10-23</created><authors><author><keyname>Ermakov</keyname><forenames>Sergei M.</forenames></author><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Efficient algorithms for tandem queueing system simulation</title><categories>math.NA cs.DC</categories><comments>7 pages, 1 figure</comments><msc-class>68U20 (Primary) 90B22, 65Y05, 65Y20 (Secondary)</msc-class><journal-ref>Applied Mathematics Letters, Volume 7, Issue 6, 1994, Pages 39-43</journal-ref><doi>10.1016/0893-9659(94)90092-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serial and parallel algorithms for simulation of tandem queueing systems with
infinite buffers are presented, and their performance is examined. It is shown
that the algorithms which are based on a simple computational procedure involve
low time and memory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6379</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6379</id><created>2012-10-23</created><updated>2012-11-15</updated><authors><author><keyname>Bravetti</keyname><forenames>Mario</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>Di Giusto</keyname><forenames>Cinzia</forenames><affiliation>INRIA Rhone Alpes</affiliation></author><author><keyname>Perez</keyname><forenames>Jorge A</forenames><affiliation>FCT New University of Lisbon</affiliation></author><author><keyname>Zavattaro</keyname><forenames>Gianluigi</forenames><affiliation>University of Bologna</affiliation></author></authors><title>Adaptable processes</title><categories>cs.LO cs.PL</categories><proxy>LMCS</proxy><acm-class>D.2.4; F.3.1; F.3.2; F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (November
  19, 2012) lmcs:982</journal-ref><doi>10.2168/LMCS-8(4:13)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the concept of adaptable processes as a way of overcoming the
limitations that process calculi have for describing patterns of dynamic
process evolution. Such patterns rely on direct ways of controlling the
behavior and location of running processes, and so they are at the heart of the
adaptation capabilities present in many modern concurrent systems. Adaptable
processes have a location and are sensible to actions of dynamic update at
runtime; this allows to express a wide range of evolvability patterns for
concurrent processes. We introduce a core calculus of adaptable processes and
propose two verification problems for them: bounded and eventual adaptation.
While the former ensures that the number of consecutive erroneous states that
can be traversed during a computation is bound by some given number k, the
latter ensures that if the system enters into a state with errors then a state
without errors will be eventually reached. We study the (un)decidability of
these two problems in several variants of the calculus, which result from
considering dynamic and static topologies of adaptable processes as well as
different evolvability patterns. Rather than a specification language, our
calculus intends to be a basis for investigating the fundamental properties of
evolvable processes and for developing richer languages with evolvability
capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6382</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6382</id><created>2012-10-23</created><updated>2013-02-22</updated><authors><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Borcea</keyname><forenames>Cristian</forenames></author><author><keyname>Murphy</keyname><forenames>Robin</forenames></author></authors><title>Data Survivability in Networks of Mobile Robots in Urban Disaster
  Environments</title><categories>cs.RO cs.DC cs.NI</categories><comments>16 double-column pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile multi-robot teams deployed for monitoring or search-and-rescue
missions in urban disaster areas can greatly improve the quality of vital data
collected on-site. Analysis of such data can identify hazards and save lives.
Unfortunately, such real deployments at scale are cost prohibitive and robot
failures lead to data loss. Moreover, scaled-down deployments do not capture
significant levels of interaction and communication complexity. To tackle this
problem, we propose novel mobility and failure generation frameworks that allow
realistic simulations of mobile robot networks for large scale disaster
scenarios. Furthermore, since data replication techniques can improve the
survivability of data collected during the operation, we propose an adaptive,
scalable data replication technique that achieves high data survivability with
low overhead. Our technique considers the anticipated robot failures and robot
heterogeneity to decide how aggressively to replicate data. In addition, it
considers survivability priorities, with some data requiring more effort to be
saved than others. Using our novel simulation generation frameworks, we compare
our adaptive technique with flooding and broadcast-based replication techniques
and show that for failure rates of up to 60% it ensures better data
survivability with lower communication costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6384</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6384</id><created>2012-10-23</created><authors><author><keyname>Coutinho</keyname><forenames>Rafaelli de C.</forenames></author><author><keyname>Drummond</keyname><forenames>L&#xfa;cia M. A.</forenames></author><author><keyname>Frota</keyname><forenames>Yuri</forenames></author></authors><title>A Distributed Transportation Simplex Applied to a Content Distribution
  Network Problem</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Content Distribution Network (CDN) can be defined as an overlay system that
replicates copies of contents at multiple points of a network, close to the
final users, with the objective of improving data access. CDN technology is
widely used for the distribution of large-sized contents, like in video
streaming. In this paper we address the problem of finding the best server for
each customer request in CDNs, in order to minimize the overall cost. We
consider the problem as a transportation problem and a distributed algorithm is
proposed to solve it. The algorithm is composed of two independent phases: a
distributed heuristic finds an initial solution that may be later improved by a
distributed transportation simplex algorithm. It is compared with the
sequential version of the transportation simplex and with an auction-based
distributed algorithm. Computational experiments carried out on a set of
instances adapted from the literature revealed that our distributed approach
has a performance similar to its sequential counterpart, in spite of not
requiring global information about the contents requests. Moreover, the results
also showed that the new method outperforms the based-auction distributed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6385</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6385</id><created>2012-10-23</created><authors><author><keyname>Liu</keyname><forenames>Guodong</forenames></author><author><keyname>Li</keyname><forenames>Zhihua</forenames></author><author><keyname>Lin</keyname><forenames>Yuefeng</forenames></author><author><keyname>John</keyname><forenames>Bino</forenames></author></authors><title>Automated family-based naming of small RNAs for next generation
  sequencing data using a modified MD5-digest algorithm</title><categories>cs.CR q-bio.GN</categories><comments>12 pages, 1 main table, 3 supplementary information tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed NameMyGene, a web tool and a stand alone program to easily
generate putative family-based names for small RNA sequences so that
laboratories can easily organize, analyze, and observe patterns from, the
massive amount of data generated by next-generation sequencers. NameMyGene,
also applicable to other emerging methods such as RNA-Seq, and Chip-Seq, solely
uses the input small RNA sequence and does not require any additional data such
as other sequence data sets. The web server and software is freely available
(http://www.johnlab.org/NameMyGene) and is based on Java to ensure platform
independency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6390</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6390</id><created>2012-10-23</created><updated>2012-10-30</updated><authors><author><keyname>Dagand</keyname><forenames>Pierre-Evariste</forenames></author><author><keyname>McBride</keyname><forenames>Conor</forenames></author></authors><title>Elaborating Inductive Definitions</title><categories>cs.PL</categories><comments>32 pages, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an elaboration of inductive definitions down to a universe of
datatypes. The universe of datatypes is an internal presentation of strictly
positive families within type theory. By elaborating an inductive definition --
a syntactic artifact -- to its code -- its semantics -- we obtain an
internalized account of inductives inside the type theory itself: we claim that
reasoning about inductive definitions could be carried in the type theory, not
in the meta-theory as it is usually the case. Besides, we give a formal
specification of that elaboration process. It is therefore amenable to formal
reasoning too. We prove the soundness of our translation and hint at its
correctness with respect to Coq's Inductive definitions.
  The practical benefits of this approach are numerous. For the type theorist,
this is a small step toward bootstrapping, ie. implementing the inductive
fragment in the type theory itself. For the programmer, this means better
support for generic programming: we shall present a lightweight deriving
mechanism, entirely definable by the programmer and therefore not requiring any
extension to the type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6395</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6395</id><created>2012-10-23</created><authors><author><keyname>Taluja</keyname><forenames>Pawandeep S.</forenames></author><author><keyname>Hughes</keyname><forenames>Brian L.</forenames></author></authors><title>Diversity Limits of Compact Broadband Multi-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Journal on Selected Areas in Communications
  (Large-Scale Multiple Antenna Wireless Systems)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to support multiple antennas on compact wireless devices,
transceivers are often designed with matching networks that compensate for
mutual coupling. Some works have suggested that when optimal matching is
applied to such a system, performance at the center frequency can be improved
at the expense of an apparent reduction in the system bandwidth. This paper
addresses the question of how coupling impacts bandwidth in the context of
circular arrays. It will be shown that mutual coupling creates eigen-modes
(virtual antennas) with diverse frequency responses, using the standard
matching techniques. We shall also demonstrate how common communications
techniques such as Diversity-OFDM would need to be optimized in order to
compensate for these effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6398</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6398</id><created>2012-10-23</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Implicit cooperation in distributed energy-efficient networks</title><categories>cs.GT cs.IT cs.NI math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1007.5004</comments><journal-ref>Proc. 4th International Symposium on Communications, Control and
  Signal Processing (ISCCSP), 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of cooperation in distributed wireless networks of
selfish and free transmitters aiming at maximizing their energy-efficiency. The
strategy of each transmitter consists in choosing his power control (PC)
policy. Two scenarios are considered: the case where transmitters can update
their power levels within time intervals less than the channel coherence time
(fast PC) and the case where it is updated only once per time interval (slow
PC). One of our objectives is to show how cooperation can be stimulated without
assuming cooperation links between the transmitters but only by repeating the
corresponding PC game and by signals from the receiver. In order to design
efficient PC policies, standard and stochastic repeated games are respectively
exploited to analyze the fast and slow PC problems. In the first case a
cooperation plan between transmitters, that is both efficient and relies on
mild information assumptions, is proposed. In the second case, the region of
equilibrium utilities is derived from very recent and powerful results in game
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6411</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6411</id><created>2012-10-23</created><authors><author><keyname>Beckmann</keyname><forenames>Andreas</forenames><affiliation>Goethe-Universit&#xe4;t Frankfurt</affiliation></author><author><keyname>Fedorowicz</keyname><forenames>Jaroslaw</forenames><affiliation>Goethe-Universit&#xe4;t Frankfurt</affiliation></author><author><keyname>Keller</keyname><forenames>J&#xf6;rg</forenames><affiliation>FernUniversit&#xe4;t in Hagen</affiliation></author><author><keyname>Meyer</keyname><forenames>Ulrich</forenames><affiliation>Goethe-Universit&#xe4;t Frankfurt</affiliation></author></authors><title>A structural analysis of the A5/1 state transition graph</title><categories>cs.DC cs.CR cs.DS</categories><comments>In Proceedings GRAPHITE 2012, arXiv:1210.6118</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 99, 2012, pp. 5-19</journal-ref><doi>10.4204/EPTCS.99.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe efficient algorithms to analyze the cycle structure of the graph
induced by the state transition function of the A5/1 stream cipher used in GSM
mobile phones and report on the results of the implementation. The analysis is
performed in five steps utilizing HPC clusters, GPGPU and external memory
computation. A great reduction of this huge state transition graph of 2^64
nodes is achieved by focusing on special nodes in the first step and removing
leaf nodes that can be detected with limited effort in the second step. This
step does not break the overall structure of the graph and keeps at least one
node on every cycle. In the third step the nodes of the reduced graph are
connected by weighted edges. Since the number of nodes is still huge an
efficient bitslice approach is presented that is implemented with NVIDIA's CUDA
framework and executed on several GPUs concurrently. An external memory
algorithm based on the STXXL library and its parallel pipelining feature
further reduces the graph in the fourth step. The result is a graph containing
only cycles that can be further analyzed in internal memory to count the number
and size of the cycles. This full analysis which previously would take months
can now be completed within a few days and allows to present structural results
for the full graph for the first time. The structure of the A5/1 graph deviates
notably from the theoretical results for random mappings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6412</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6412</id><created>2012-10-23</created><authors><author><keyname>Cormie-Bowins</keyname><forenames>Elise</forenames></author></authors><title>A Comparison of Sequential and GPU Implementations of Iterative Methods
  to Compute Reachability Probabilities</title><categories>cs.DC cs.LO</categories><comments>In Proceedings GRAPHITE 2012, arXiv:1210.6118</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 99, 2012, pp. 20-34</journal-ref><doi>10.4204/EPTCS.99.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing reachability probabilities: given a
Markov chain, an initial state of the Markov chain, and a set of goal states of
the Markov chain, what is the probability of reaching any of the goal states
from the initial state? This problem can be reduced to solving a linear
equation Ax = b for x, where A is a matrix and b is a vector. We consider two
iterative methods to solve the linear equation: the Jacobi method and the
biconjugate gradient stabilized (BiCGStab) method. For both methods, a
sequential and a parallel version have been implemented. The parallel versions
have been implemented on the compute unified device architecture (CUDA) so that
they can be run on a NVIDIA graphics processing unit (GPU). From our
experiments we conclude that as the size of the matrix increases, the CUDA
implementations outperform the sequential implementations. Furthermore, the
BiCGStab method performs better than the Jacobi method for dense matrices,
whereas the Jacobi method does better for sparse ones. Since the reachability
probabilities problem plays a key role in probabilistic model checking, we also
compared the implementations for matrices obtained from a probabilistic model
checker. Our experiments support the conjecture by Bosnacki et al. that the
Jacobi method is superior to Krylov subspace methods, a class to which the
BiCGStab method belongs, for probabilistic model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6413</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6413</id><created>2012-10-23</created><authors><author><keyname>Zambon</keyname><forenames>Eduardo</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Rensink</keyname><forenames>Arend</forenames><affiliation>University of Twente</affiliation></author></authors><title>Graph Subsumption in Abstract State Space Exploration</title><categories>cs.LO</categories><comments>In Proceedings GRAPHITE 2012, arXiv:1210.6118</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 99, 2012, pp. 35-49</journal-ref><doi>10.4204/EPTCS.99.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the extension of an existing method for abstract
graph-based state space exploration, called neighbourhood abstraction, with a
reduction technique based on subsumption. Basically, one abstract state
subsumes another when it covers more concrete states; in such a case, the
subsumed state need not be included in the state space, thus giving a
reduction. We explain the theory and especially also report on a number of
experiments, which show that subsumption indeed drastically reduces both the
state space and the resources (time and memory) needed to compute it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6414</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6414</id><created>2012-10-23</created><authors><author><keyname>Kant</keyname><forenames>Gijs</forenames></author><author><keyname>van de Pol</keyname><forenames>Jaco</forenames></author></authors><title>Efficient Instantiation of Parameterised Boolean Equation Systems to
  Parity Games</title><categories>cs.LO cs.GT</categories><comments>In Proceedings GRAPHITE 2012, arXiv:1210.6118</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 99, 2012, pp. 50-65</journal-ref><doi>10.4204/EPTCS.99.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameterised Boolean Equation Systems (PBESs) are sequences of Boolean fixed
point equations with data variables, used for, e.g., verification of modal
mu-calculus formulae for process algebraic specifications with data.
  Solving a PBES is usually done by instantiation to a Parity Game and then
solving the game. Practical game solvers exist, but the instantiation step is
the bottleneck.
  We enhance the instantiation in two steps. First, we transform the PBES to a
Parameterised Parity Game (PPG), a PBES with each equation either conjunctive
or disjunctive. Then we use LTSmin, that offers transition caching, efficient
storage of states and both distributed and symbolic state space generation, for
generating the game graph. To that end we define a language module for LTSmin,
consisting of an encoding of variables with parameters into state vectors, a
grouped transition relation and a dependency matrix to indicate the
dependencies between parts of the state vector and transition groups.
  Benchmarks on some large case studies, show that the method speeds up the
instantiation significantly and decreases memory usage drastically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6415</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6415</id><created>2012-10-23</created><authors><author><keyname>Edelkamp</keyname><forenames>Stefan</forenames></author><author><keyname>Kissmann</keyname><forenames>Peter</forenames></author><author><keyname>Torralba</keyname><forenames>&#xc1;lvaro</forenames></author></authors><title>Lex-Partitioning: A New Option for BDD Search</title><categories>cs.AI</categories><comments>In Proceedings GRAPHITE 2012, arXiv:1210.6118</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 99, 2012, pp. 66-82</journal-ref><doi>10.4204/EPTCS.99.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the exploration of large state spaces, symbolic search using binary
decision diagrams (BDDs) can save huge amounts of memory and computation time.
State sets are represented and modified by accessing and manipulating their
characteristic functions. BDD partitioning is used to compute the image as the
disjunction of smaller subimages.
  In this paper, we propose a novel BDD partitioning option. The partitioning
is lexicographical in the binary representation of the states contained in the
set that is represented by a BDD and uniform with respect to the number of
states represented. The motivation of controlling the state set sizes in the
partitioning is to eventually bridge the gap between explicit and symbolic
search.
  Let n be the size of the binary state vector. We propose an O(n) ranking and
unranking scheme that supports negated edges and operates on top of precomputed
satcount values. For the uniform split of a BDD, we then use unranking to
provide paths along which we partition the BDDs. In a shared BDD representation
the efforts are O(n). The algorithms are fully integrated in the CUDD library
and evaluated in strongly solving general game playing benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6423</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6423</id><created>2012-10-23</created><authors><author><keyname>Fouladgar</keyname><forenames>Ali Mohammad</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>On the Transfer of Information and Energy in Multi-User Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, Accepted for publication in IEEE Communication
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of joint transfer of information and energy for wireless links
has been recently investigated in light of emerging applications such as RFID
and body area networks. Specifically, recent work has shown that the additional
requirements of providing sufficient energy to the receiver significantly
affects the design of the optimal communication strategy. In contrast to most
previous works, this letter focuses on baseline multi-user systems, namely
multiple access and multi-hop channels, and demonstrates that energy transfer
constraints call for additional coordination among distributed nodes of a
wireless network. The analysis is carried out using information theoretic
tools, and specific examples are worked out to illustrate the main conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6441</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6441</id><created>2012-10-24</created><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Lim</keyname><forenames>Hoon Wei</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Su</keyname><forenames>Le</forenames></author><author><keyname>Wang</keyname><forenames>Huaxiong</forenames></author></authors><title>Anonymous and Adaptively Secure Revocable IBE with Constant Size Public
  Parameters</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Identity-Based Encryption (IBE) systems, key revocation is non-trivial.
This is because a user's identity is itself a public key. Moreover, the private
key corresponding to the identity needs to be obtained from a trusted key
authority through an authenticated and secrecy protected channel. So far, there
exist only a very small number of revocable IBE (RIBE) schemes that support
non-interactive key revocation, in the sense that the user is not required to
interact with the key authority or some kind of trusted hardware to renew her
private key without changing her public key (or identity). These schemes are
either proven to be only selectively secure or have public parameters which
grow linearly in a given security parameter. In this paper, we present two
constructions of non-interactive RIBE that satisfy all the following three
attractive properties: (i) proven to be adaptively secure under the Symmetric
External Diffie-Hellman (SXDH) and the Decisional Linear (DLIN) assumptions;
(ii) have constant-size public parameters; and (iii) preserve the anonymity of
ciphertexts---a property that has not yet been achieved in all the current
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6447</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6447</id><created>2012-10-24</created><authors><author><keyname>Yashvir</keyname><forenames>S.</forenames></author><author><keyname>Prakash</keyname><forenames>Om</forenames></author></authors><title>Disk Scheduling: Selection of Algorithm</title><categories>cs.OS</categories><comments>9 pages; http://www.ijascse.in/publications-2012--2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to take some aspects of disk scheduling and
scheduling algorithms. The disk scheduling is discussed with a sneak peak in
general and selection of algorithm in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6450</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6450</id><created>2012-10-24</created><authors><author><keyname>Shareef</keyname><forenames>Amjed</forenames></author></authors><title>Short Report on: Possible directions to Auctions with Cryptographic
  pre-play</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In auction theory, cryptography has been used to achieve anonymity of the
participants, security and privacy of the bids, secure computation and to
simulate mediator (auctioneer). Auction theory focuses on revenue and
Cryptography focuses on security and privacy. Involving Cryptography at base
level, to enhance revenue gives entirely new perspective and insight to Auction
theory, thereby achieving the core goals of auction theory. In this report, we
try to investigate an interesting field of study in Auction Theory using
Cryptographic primitives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6456</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6456</id><created>2012-10-24</created><updated>2012-11-18</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Kushnir</keyname><forenames>Duncan</forenames></author><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author></authors><title>Interactive Overlay Maps for US Patent (USPTO) Data Based on
  International Patent Classifications (IPC)</title><categories>cs.DL</categories><comments>Scientometrics (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on the development of an interface to the US Patent and Trademark
Office (USPTO) that allows for the mapping of patent portfolios as overlays to
basemaps constructed from citation relations among all patents contained in
this database during the period 1976-2011. Both the interface and the data are
in the public domain; the freeware programs VOSViewer and/or Pajek can be used
for the visualization. These basemaps and overlays can be generated at both the
3-digit and 4-digit levels of the International Patent Classifications (IPC) of
the World Intellectual Property Organization (WIPO). The basemaps can provide a
stable mental framework for analysts to follow developments over searches for
different years, which can be animated. The full flexibility of the advanced
search engines of USPTO are available for generating sets of patents and/or
patent applications which can thus be visualized and compared. This instrument
allows for addressing questions about technological distance, diversity in
portfolios, and animating the developments of both technologies and
technological capacities of organizations over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6459</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6459</id><created>2012-10-24</created><authors><author><keyname>Gillespie</keyname><forenames>Neil I.</forenames></author></authors><title>A note on binary completely regular codes with large minimum distance</title><categories>math.CO cs.IT math.IT</categories><comments>4 pages</comments><msc-class>94B05, 94C30</msc-class><journal-ref>Discrete Mathematics, Volume 313, Issue 14, 28 July 2013, Pages
  1532-1534</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify all binary error correcting completely regular codes of length
$n$ with minimum distance $\delta&gt;n/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6465</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6465</id><created>2012-10-24</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Winzen</keyname><forenames>Carola</forenames></author></authors><title>Black-Box Complexity: Breaking the $O(n \log n)$ Barrier of LeadingOnes</title><categories>cs.DS cs.NE</categories><comments>12 pages, to appear in the Proc. of Artificial Evolution 2011, LNCS
  7401, Springer, 2012. For the unrestricted black-box complexity of
  LeadingOnes there is now a tight $\Theta(n \log\log n)$ bound, cf.
  http://eccc.hpi-web.de/report/2012/087/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the unrestricted black-box complexity of the $n$-dimensional
XOR- and permutation-invariant LeadingOnes function class is $O(n \log (n) /
\log \log n)$. This shows that the recent natural looking $O(n\log n)$ bound is
not tight.
  The black-box optimization algorithm leading to this bound can be implemented
in a way that only 3-ary unbiased variation operators are used. Hence our bound
is also valid for the unbiased black-box complexity recently introduced by
Lehre and Witt (GECCO 2010). The bound also remains valid if we impose the
additional restriction that the black-box algorithm does not have access to the
objective values but only to their relative order (ranking-based black-box
complexity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6488</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6488</id><created>2012-10-24</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>A New Identification Framework For Off-Line Computation of
  Moving-Horizon Observers</title><categories>cs.SY</categories><comments>To Appear in IEEE Transactions on Automatic Control (This version is
  the second revision)</comments><journal-ref>IEEE Transactions on Automatic Control. Vol 58, Issue 7, pp.
  1877-1882, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new nonlinear identification framework is proposed to
address the issue of off-line computation of moving-horizon observer estimate.
The proposed structure merges the advantages of nonlinear approximators with
the efficient computation of constrained quadratic programming problems. A
bound on the estimation error is proposed and the efficiency of the resulting
scheme is illustrated using two state estimation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6494</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6494</id><created>2012-10-24</created><updated>2012-11-09</updated><authors><author><keyname>Usatyuk</keyname><forenames>Vasiliy</forenames></author></authors><title>Short review of lattice basis reduction types and his applications
  (Russian)</title><categories>cs.DM math.GR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article presets a review of lattice lattice basis reduction types. Paper
contains the main five types of lattice basis reduction: size reduced (weak
Hermit), c-reduced, Lovasz condition, Hermit-Korkin-Zolotarev, Minkowski
reduced. The article provides references to applications in: information theory
(decoding of coding group in MIMO), calculus (minimize of the positive
quadratic form), complexity theory and cryptanalysis of Merkle-Hellman
cryptography (solving subset sum problems), algebra and control theory(solving
system of linear diophantine equation), compiler theory (lattice based memory
allocation), synthesize cryptographic and cryptanalysis in lattice based
cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6497</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6497</id><created>2012-10-24</created><authors><author><keyname>Li</keyname><forenames>Daifeng</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Shuai</keyname><forenames>Xin</forenames></author><author><keyname>Sun</keyname><forenames>Golden Guo-zheng</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Luo</keyname><forenames>Zhipeng</forenames></author><author><keyname>Zhang</keyname><forenames>Jingwei</forenames></author><author><keyname>Zhang</keyname><forenames>Guo</forenames></author></authors><title>Topic-Level Opinion Influence Model(TOIM): An Investigation Using
  Tencent Micro-Blogging</title><categories>cs.SI cs.CY cs.LG</categories><comments>PLOS ONE Manuscript Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining user opinion from Micro-Blogging has been extensively studied on the
most popular social networking sites such as Twitter and Facebook in the U.S.,
but few studies have been done on Micro-Blogging websites in other countries
(e.g. China). In this paper, we analyze the social opinion influence on
Tencent, one of the largest Micro-Blogging websites in China, endeavoring to
unveil the behavior patterns of Chinese Micro-Blogging users. This paper
proposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously
incorporates topic factor and social direct influence in a unified
probabilistic framework. Based on TOIM, two topic level opinion influence
propagation and aggregation algorithms are developed to consider the indirect
influence: CP (Conservative Propagation) and NCP (None Conservative
Propagation). Users' historical social interaction records are leveraged by
TOIM to construct their progressive opinions and neighbors' opinion influence
through a statistical learning process, which can be further utilized to
predict users' future opinions on some specific topics. To evaluate and test
this proposed model, an experiment was designed and a sub-dataset from Tencent
Micro-Blogging was used. The experimental results show that TOIM outperforms
baseline methods on predicting users' opinion. The applications of CP and NCP
have no significant differences and could significantly improve recall and
F1-measure of TOIM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6502</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6502</id><created>2012-10-24</created><authors><author><keyname>Usatyuk</keyname><forenames>Vasiliy</forenames></author></authors><title>Block Korkin-Zolotarev algorithm generalization and their practical
  implementation (Russian)</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a practical algorithm for block Korkin-Zolotarev reduction, a
concept introduced by Schnorr, using CPU arbitrary length Householder
QR-decomposition for orthogonalization and double precision OpenCL GPU
Finke-Post shortest vector enumeration. Empirical tests was used on random
lattices in the sense of Goldstein and Mayer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6508</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6508</id><created>2012-10-24</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>An algebraic approach to project schedule development under precedence
  constraints</title><categories>math.OC cs.SY</categories><comments>21 pages, 10 figures. ISSN: 2074-1278</comments><msc-class>68M20 (Primary) 15A80, 90B35 (Secondary)</msc-class><journal-ref>International Journal of Applied Mathematics and Informatics,
  2012. Vol. 6, no. 2, pp. 92-100</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to schedule development in project management is developed within
the framework of idempotent algebra. The approach offers a way to represent
precedence relationships among activities in projects as linear vector
equations in terms of an idempotent semiring. As a result, many issues in
project scheduling reduce to solving computational problems in the idempotent
algebra setting, including linear equations and eigenvalue-eigenvector
problems. The solutions to the problems are given in a compact vector form that
provides the basis for the development of efficient computation procedures and
related software applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6510</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6510</id><created>2012-10-24</created><authors><author><keyname>Cordier</keyname><forenames>St&#xe9;phane</forenames><affiliation>MAPMO</affiliation></author></authors><title>A measure of similarity between scientific journals and of diversity of
  a list of publications</title><categories>cs.DL cs.IR physics.soc-ph</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this note is to propose a definition of the scientific diversity
and corollarly, a measure of the &quot;interdisciplinarity&quot; of collaborations. With
respect to previous studies, the proposed approach consists of 2 steps : first,
the definition of similarity between journals and second, these similarities
are used to characterize the homogeneity (or, on the contrary the diversity) of
a publication list (that can be for one individual or a team).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6511</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6511</id><created>2012-10-24</created><authors><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Olteanu</keyname><forenames>Madalina</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rynkiewicz</keyname><forenames>Joseph</forenames><affiliation>SAMM</affiliation></author><author><keyname>Villa-Vialaneix</keyname><forenames>Nathalie</forenames><affiliation>SAMM</affiliation></author></authors><title>Neural Networks for Complex Data</title><categories>cs.NE cs.LG stat.ML</categories><proxy>ccsd</proxy><journal-ref>K\&quot;unstliche Intelligenz 26, 4 (2012) 373-380</journal-ref><doi>10.1007/s13218-012-0207-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks are simple and efficient machine learning tools.
Defined originally in the traditional setting of simple vector data, neural
network models have evolved to address more and more difficulties of complex
real world problems, ranging from time evolving data to sophisticated data
structures such as graphs and functions. This paper summarizes advances on
those themes from the last decade, with a focus on results obtained by members
of the SAMM team of Universit\'e Paris 1
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6539</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6539</id><created>2012-10-24</created><updated>2013-03-18</updated><authors><author><keyname>Hamann</keyname><forenames>Heiko</forenames></author></authors><title>Towards Swarm Calculus: Urn Models of Collective Decisions and Universal
  Properties of Swarm Performance</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods of general applicability are searched for in swarm intelligence with
the aim of gaining new insights about natural swarms and to develop design
methodologies for artificial swarms. An ideal solution could be a `swarm
calculus' that allows to calculate key features of swarms such as expected
swarm performance and robustness based on only a few parameters. To work
towards this ideal, one needs to find methods and models with high degrees of
generality. In this paper, we report two models that might be examples of
exceptional generality. First, an abstract model is presented that describes
swarm performance depending on swarm density based on the dichotomy between
cooperation and interference. Typical swarm experiments are given as examples
to show how the model fits to several different results. Second, we give an
abstract model of collective decision making that is inspired by urn models.
The effects of positive feedback probability, that is increasing over time in a
decision making system, are understood by the help of a parameter that controls
the feedback based on the swarm's current consensus. Several applicable
methods, such as the description as Markov process, calculation of splitting
probabilities, mean first passage times, and measurements of positive feedback,
are discussed and applications to artificial and natural swarms are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6568</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6568</id><created>2012-10-24</created><authors><author><keyname>Furma&#xf1;czyk</keyname><forenames>Hanna</forenames></author><author><keyname>Kubale</keyname><forenames>Marek</forenames></author><author><keyname>Mkrtchyan</keyname><forenames>Vahan V.</forenames></author></authors><title>Equitable Colorings of Corona Multiproducts of Graphs</title><categories>cs.DM math.CO</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is equitably $k$-colorable if its vertices can be partitioned into
$k$ independent sets in such a way that the number of vertices in any two sets
differ by at most one. The smallest $k$ for which such a coloring exists is
known as the equitable chromatic number of $G$ and denoted $\chi_{=}(G)$. It is
known that this problem is NP-hard in general case and remains so for corona
graphs. In &quot;Equitable colorings of Cartesian products of graphs&quot; (2012) Lin and
Chang studied equitable coloring of Cartesian products of graphs. In this paper
we consider the same model of coloring in the case of corona products of
graphs. In particular, we obtain some results regarding the equitable chromatic
number for $l$-corona product $G \circ ^l H$, where $G$ is an equitably 3- or
4-colorable graph and $H$ is an $r$-partite graph, a path, a cycle or a
complete graph. Our proofs are constructive in that they lead to polynomial
algorithms for equitable coloring of such graph products provided that there is
given an equitable coloring of $G$. Moreover, we confirm Equitable Coloring
Conjecture for corona products of such graphs. This paper extends our results
from \cite{hf}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6578</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6578</id><created>2012-10-24</created><updated>2014-02-01</updated><authors><author><keyname>Sigalov</keyname><forenames>Daniel</forenames></author><author><keyname>Michaeli</keyname><forenames>Tomer</forenames></author><author><keyname>Oshman</keyname><forenames>Yaakov</forenames></author></authors><title>LMMSE Filtering in Feedback Systems with White Random Modes: Application
  to Tracking in Clutter</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized state space representation of dynamical systems with random
modes switching according to a white random process is presented. The new
formulation includes a term, in the dynamics equation, that depends on the most
recent linear minimum mean squared error (LMMSE) estimate of the state. This
can model the behavior of a feedback control system featuring a state
estimator. The measurement equation is allowed to depend on the previous LMMSE
estimate of the state, which can represent the fact that measurements are
obtained from a validation window centered about the predicted measurement and
not from the entire surveillance region. The LMMSE filter is derived for the
considered problem. The approach is demonstrated in the context of target
tracking in clutter and is shown to be competitive with several popular
nonlinear methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6581</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6581</id><created>2012-10-24</created><authors><author><keyname>Bansal</keyname><forenames>N.</forenames></author><author><keyname>Pendavingh</keyname><forenames>R. A.</forenames></author><author><keyname>van der Pol</keyname><forenames>J. G.</forenames></author></authors><title>An entropy argument for counting matroids</title><categories>math.CO cs.IT math.IT</categories><comments>Short note, 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how a direct application of Shearers' Lemma gives an almost optimum
bound on the number of matroids on $n$ elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6621</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6621</id><created>2012-10-24</created><updated>2013-05-06</updated><authors><author><keyname>Hoepman</keyname><forenames>Jaap-Henk</forenames></author></authors><title>Privacy Design Strategies</title><categories>cs.CY cs.CR</categories><comments>12 pages, 3 figures, 1 table. Presented at the Privacy Law Scholars
  Conference (PLSC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define the notion of a privacy design strategy. These
strategies help IT architects to support privacy by design early in the
software development life cycle, during concept development and analysis. Using
current data protection legislation as point of departure we derive the
following eight privacy design strategies: minimise, hide, separate, aggregate,
inform, control, enforce, and demonstrate. The strategies also provide a useful
classification of privacy design patterns and the underlying privacy enhancing
technologies. We therefore believe that these privacy design strategies are not
only useful when designing privacy friendly systems, but also helpful when
evaluating the privacy impact of existing IT systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6624</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6624</id><created>2012-10-24</created><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author></authors><title>Advanced Automata Minimization</title><categories>cs.FL cs.DS</categories><comments>15 pages</comments><report-no>EDI-INF-RR-1414</report-no><msc-class>68Q45</msc-class><acm-class>D.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm to reduce the size of nondeterministic
Buchi word automata, while retaining their language. Additionally, we describe
methods to solve PSPACE-complete automata problems like universality,
equivalence and inclusion for much larger instances (1-3 orders of magnitude)
than before. This can be used to scale up applications of automata in formal
verification tools and decision procedures for logical theories. The algorithm
is based on new transition pruning techniques. These use criteria based on
combinations of backward and forward trace inclusions. Since these relations
are themselves PSPACE-complete, we describe methods to compute good
approximations of them in polynomial time. Extensive experiments show that the
average-case complexity of our algorithm scales quadratically. The size
reduction of the automata depends very much on the class of instances, but our
algorithm consistently outperforms all previous techniques by a wide margin. We
tested our algorithm on Buchi automata derived from LTL-formulae, many classes
of random automata and automata derived from mutual exclusion protocols, and
compared its performance to the well-known automata tool GOAL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6631</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6631</id><created>2012-10-24</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Risk-driven migration and the collective-risk social dilemma</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>8 two-column pages, 7 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 86 (2012) 036101</journal-ref><doi>10.1103/PhysRevE.86.036101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A collective-risk social dilemma implies that personal endowments will be
lost if contributions to the common pool within a group are too small. Failure
to reach the collective target thus has dire consequences for all group
members, independently of their strategies. Wanting to move away from
unfavorable locations is therefore all but surprising. Inspired by these
observations, we here propose and study a collective-risk social dilemma where
players are allowed to move if the collective failure becomes too probable.
More precisely, this so-called risk-driven migration is launched depending on
the difference between the actual contributions and the declared target.
Mobility therefore becomes an inherent property that is utilized in an entirely
self-organizing manner. We show that under these assumptions cooperation is
promoted much more effectively than under the action of manually determined
migration rates. For the latter, we in fact identify parameter regions where
the evolution of cooperation is incredibly inhibited. Moreover, we find
unexpected spatial patterns where cooperators that do not form compact clusters
outperform those that do, and where defectors are able to utilize strikingly
different ways of invasion. The presented results support the recently revealed
importance of percolation for the successful evolution of public cooperation,
while at the same time revealing surprisingly simple ways of self-organization
towards socially desirable states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6636</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6636</id><created>2012-10-24</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Informaticology: combining Computer Science, Data Science, and Fiction
  Science</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an intention to remedy current complications with Dutch
terminology concerning informatics, the term informaticology is positioned to
denote an academic counterpart of informatics where informatics is conceived of
as a container for a coherent family of practical disciplines ranging from
computer engineering and software engineering to network technology, data
center management, information technology, and information management in a
broad sense.
  Informaticology escapes from the limitations of instrumental objectives and
the perspective of usage that both restrict the scope of informatics. That is
achieved by including fiction science in informaticology and by ranking fiction
science on equal terms with computer science and data science, and framing (the
study of) game design, evelopment, assessment and distribution, ranging from
serious gaming to entertainment gaming, as a chapter of fiction science. A
suggestion for the scope of fiction science is specified in some detail.
  In order to illustrate the coherence of informaticology thus conceived, a
potential application of fiction to the ontology of instruction sequences and
to software quality assessment is sketched, thereby highlighting a possible
role of fiction (science) within informaticology but outside gaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6646</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6646</id><created>2012-10-24</created><updated>2013-08-07</updated><authors><author><keyname>Garcia</keyname><forenames>Hector J.</forenames></author><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author><author><keyname>Cross</keyname><forenames>Andrew W.</forenames></author></authors><title>Efficient Inner-product Algorithm for Stabilizer States</title><categories>cs.ET cs.CG cs.DS quant-ph</categories><comments>14 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale quantum computation is likely to require massive quantum error
correction (QEC). QEC codes and circuits are described via the stabilizer
formalism, which represents stabilizer states by keeping track of the operators
that preserve them. Such states are obtained by stabilizer circuits (consisting
of CNOT, Hadamard and Phase only) and can be represented compactly on
conventional computers using Omega(n^2) bits, where n is the number of qubits.
Although techniques for the efficient simulation of stabilizer circuits have
been studied extensively, techniques for efficient manipulation of stabilizer
states are not currently available. To this end, we design new algorithms for:
(i) obtaining canonical generators for stabilizer states, (ii) obtaining
canonical stabilizer circuits, and (iii) computing the inner product between
stabilizer states. Our inner-product algorithm takes O(n^3) time in general,
but observes quadratic behavior for many practical instances relevant to QECC
(e.g., GHZ states). We prove that each n-qubit stabilizer state has exactly
4(2^n - 1) nearest-neighbor stabilizer states, and verify this claim
experimentally using our algorithms. We design techniques for representing
arbitrary quantum states using stabilizer frames and generalize our algorithms
to compute the inner product between two such frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6649</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6649</id><created>2012-10-25</created><authors><author><keyname>Gall&#xe9;</keyname><forenames>Roberto Baena</forenames></author><author><keyname>N&#xfa;&#xf1;ez</keyname><forenames>Jorge</forenames></author><author><keyname>Gladysz</keyname><forenames>Szymon</forenames></author></authors><title>Extended object reconstruction in adaptive-optics imaging: the
  multiresolution approach</title><categories>astro-ph.IM cs.CV math.NA</categories><comments>In revision in Astronomy &amp; Astrophysics. 19 pages, 13 figures</comments><doi>10.1051/0004-6361/201219489</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the application of multiresolution transforms, such as wavelets
(WT) and curvelets (CT), to the reconstruction of images of extended objects
that have been acquired with adaptive optics (AO) systems. Such multichannel
approaches normally make use of probabilistic tools in order to distinguish
significant structures from noise and reconstruction residuals. Furthermore, we
aim to check the historical assumption that image-reconstruction algorithms
using static PSFs are not suitable for AO imaging. We convolve an image of
Saturn taken with the Hubble Space Telescope (HST) with AO PSFs from the 5-m
Hale telescope at the Palomar Observatory and add both shot and readout noise.
Subsequently, we apply different approaches to the blurred and noisy data in
order to recover the original object. The approaches include multi-frame blind
deconvolution (with the algorithm IDAC), myopic deconvolution with
regularization (with MISTRAL) and wavelets- or curvelets-based static PSF
deconvolution (AWMLE and ACMLE algorithms). We used the mean squared error
(MSE) and the structural similarity index (SSIM) to compare the results. We
discuss the strengths and weaknesses of the two metrics. We found that CT
produces better results than WT, as measured in terms of MSE and SSIM.
Multichannel deconvolution with a static PSF produces results which are
generally better than the results obtained with the myopic/blind approaches
(for the images we tested) thus showing that the ability of a method to
suppress the noise and to track the underlying iterative process is just as
critical as the capability of the myopic/blind approaches to update the PSF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6673</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6673</id><created>2012-10-24</created><updated>2013-11-02</updated><authors><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author><author><keyname>Stehl&#xe9;</keyname><forenames>Damien</forenames></author></authors><title>Semantically Secure Lattice Codes for the Gaussian Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Information Theory, Sept. 2012; revised,
  Oct. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new scheme of wiretap lattice coding that achieves semantic
security and strong secrecy over the Gaussian wiretap channel. The key tool in
our security proof is the flatness factor which characterizes the convergence
of the conditional output distributions corresponding to different messages and
leads to an upper bound on the information leakage. We not only introduce the
notion of secrecy-good lattices, but also propose the {flatness factor} as a
design criterion of such lattices. Both the modulo-lattice Gaussian channel and
the genuine Gaussian channel are considered. In the latter case, we propose a
novel secrecy coding scheme based on the discrete Gaussian distribution over a
lattice, which achieves the secrecy capacity to within a half nat under mild
conditions. No \textit{a priori} distribution of the message is assumed, and no
dither is used in our proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6685</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6685</id><created>2012-10-24</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Distributed Optimization: Convergence Conditions from a Dynamical System
  Perspective</title><categories>cs.SY cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the fundamental properties of distributed minimization of
a sum of functions with each function only known to one node, and a
pre-specified level of node knowledge and computational capacity. We define the
optimization information each node receives from its objective function, the
neighboring information each node receives from its neighbors, and the
computational capacity each node can take advantage of in controlling its
state. It is proven that there exist a neighboring information way and a
control law that guarantee global optimal consensus if and only if the solution
sets of the local objective functions admit a nonempty intersection set for
fixed strongly connected graphs. Then we show that for any tolerated error, we
can find a control law that guarantees global optimal consensus within this
error for fixed, bidirectional, and connected graphs under mild conditions. For
time-varying graphs, we show that optimal consensus can always be achieved as
long as the graph is uniformly jointly strongly connected and the nonempty
intersection condition holds. The results illustrate that nonempty intersection
for the local optimal solution sets is a critical condition for successful
distributed optimization for a large class of algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6705</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6705</id><created>2012-10-24</created><updated>2014-08-19</updated><authors><author><keyname>Ali</keyname><forenames>Mortuza</forenames></author><author><keyname>Murshed</keyname><forenames>Manzur</forenames></author></authors><title>Modified Rice-Golomb Code for Predictive Coding of Integers with
  Real-valued Predictions</title><categories>cs.IT math.IT</categories><comments>A revised and extended version has been submitted to Digital Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rice-Golomb codes are widely used in practice to encode integer-valued
prediction residuals. However, in lossless coding of audio, image, and video,
specially those involving linear predictors, the predictions are from the real
domain. In this paper, we have modified and extended the Rice-Golomb code so
that it can operate at fractional precision to efficiently exploit the
real-valued predictions. Coding at arbitrarily small precision allows the
residuals to be modeled with the Laplace distribution instead of its discrete
counterpart, namely the two-sided geometric distribution (TSGD). Unlike the
Rice-Golomb code, which maps equally probable opposite-signed residuals to
different integers, the proposed coding scheme is symmetric in the sense that,
at arbitrarily small precision, it assigns codewords of equal length to equally
probable residual intervals. The symmetry of both the Laplace distribution and
the code facilitates the analysis of the proposed coding scheme to determine
the average code-length and the optimal value of the associated coding
parameter. Experimental results demonstrate that the proposed scheme, by making
efficient use of real-valued predictions, achieves better compression as
compared to the conventional scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6707</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6707</id><created>2012-10-24</created><authors><author><keyname>Coviello</keyname><forenames>Emanuele</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert R. G.</forenames></author></authors><title>Clustering hidden Markov models with variational HEM</title><categories>cs.LG cs.CV stat.ML</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hidden Markov model (HMM) is a widely-used generative model that copes
with sequential data, assuming that each observation is conditioned on the
state of a hidden Markov chain. In this paper, we derive a novel algorithm to
cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed
algorithm i) clusters a given collection of HMMs into groups of HMMs that are
similar, in terms of the distributions they represent, and ii) characterizes
each group by a &quot;cluster center&quot;, i.e., a novel HMM that is representative for
the group, in a manner that is consistent with the underlying generative model
of the HMM. To cope with intractable inference in the E-step, the HEM algorithm
is formulated as a variational optimization problem, and efficiently solved for
the HMM case by leveraging an appropriate variational approximation. The
benefits of the proposed algorithm, which we call variational HEM (VHEM), are
demonstrated on several tasks involving time-series data, such as hierarchical
clustering of motion capture sequences, and automatic annotation and retrieval
of music and of online hand-writing data, showing improvements over current
methods. In particular, our variational HEM algorithm effectively leverages
large amounts of data when learning annotation models by using an efficient
hierarchical estimation procedure, which reduces learning times and memory
requirements, while improving model robustness through better regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6712</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6712</id><created>2012-10-24</created><authors><author><keyname>Chen</keyname><forenames>Hung-Hsun</forenames></author><author><keyname>Hu</keyname><forenames>Wen-Guei</forenames></author><author><keyname>Lai</keyname><forenames>De-Jan</forenames></author><author><keyname>Lin</keyname><forenames>Song-Sun</forenames></author></authors><title>Decidability of plane edge coloring with three colors</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This investigation studies the decidability problem of plane edge coloring
with three symbols. In the edge coloring (or Wang tiles) of a plane, unit
squares with colored edges that have one of $p$ colors are arranged side by
side such that the touching edges of the adjacent tiles have the same colors.
Given a basic set $B$ of Wang tiles, the decision problem is to find an
algorithm to determine whether or not $\Sigma(B)\neq\emptyset$, where
$\Sigma(B)$ is the set of all global patterns on $\mathbb{Z}^{2}$ that can be
constructed from the Wang tiles in $B$.
  When $p\geq 5$, the problem is known to be undecidable. When $p=2$, the
problem is decidable. This study proves that when $p=3$, the problem is also
decidable. $\mathcal{P}(B)$ is the set of all periodic patterns on
$\mathbb{Z}^{2}$ that can be generated by the tiles in $B$. If
$\mathcal{P}(B)\neq\emptyset$, then $B$ has a subset $B'$ of minimal cycle
generators such that $\mathcal{P}(B')\neq\emptyset$ and
$\mathcal{P}(B&quot;)=\emptyset$ for $B&quot;\subsetneqq B'$. This study demonstrates
that the set $\mathcal{C}(3)$ of all minimal cycle generators contains
$787,605$ members that can be classified into $2,906$ equivalence classes.
$\mathcal{N}(3)$ is the set of all maximal non-cycle generators: if $B\in
\mathcal{N}(3)$, then $\mathcal{P}(B)=\emptyset$ and
$\mathcal{P}(\tilde{B})\neq\emptyset$ for $\tilde{B}\supsetneqq B$. The problem
is shown to be decidable by proving that $B\in \mathcal{N}(3)$ implies
$\Sigma(B)=\emptyset$. Consequently, $\Sigma(B)\neq\emptyset$ if and only if
$\mathcal{P}(B)\neq\emptyset$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6719</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6719</id><created>2012-10-24</created><authors><author><keyname>Muramatsu</keyname><forenames>Jun</forenames></author><author><keyname>Miyake</keyname><forenames>Shigeki</forenames></author></authors><title>Construction of Multiple Access Channel Codes Based on Hash Property</title><categories>cs.IT math.IT</categories><comments>This paper has been presented in part at Proc. 2011 IEEE Internal
  Symposium on Information Theory and submitted to IEEE Transactions on
  Information Theory. 39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to introduce the construction of codes for a general
discrete stationary memoryless multiple access channel based on the the notion
of the hash property. Since an ensemble of sparse matrices has a hash property,
we can use sparse matrices for code construction. Our approach has a potential
advantage compared to the conventional random coding because it is expected
that we can use some approximation algorithms by using the sparse structure of
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6722</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6722</id><created>2012-10-24</created><updated>2012-10-31</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author></authors><title>Feng-Rao decoding of primary codes</title><categories>cs.IT math.IT</categories><comments>elsarticle.cls, 23 pages, no figure. Version 3 added citations to the
  works by I.M. Duursma and R. Pellikaan</comments><msc-class>94B65 (Primary) 94B35 (Secondary)</msc-class><journal-ref>Finite Fields and Their Applications, vol. 23, pp. 35-52,
  September 2013</journal-ref><doi>10.1016/j.ffa.2013.03.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Feng-Rao bound for dual codes and a similar bound by
Andersen and Geil [H.E. Andersen and O. Geil, Evaluation codes from order
domain theory, Finite Fields Appl., 14 (2008), pp. 92-123] for primary codes
are consequences of each other. This implies that the Feng-Rao decoding
algorithm can be applied to decode primary codes up to half their designed
minimum distance. The technique applies to any linear code for which
information on well-behaving pairs is available. Consequently we are able to
decode efficiently a large class of codes for which no non-trivial decoding
algorithm was previously known. Among those are important families of
multivariate polynomial codes. Matsumoto and Miura in [R. Matsumoto and S.
Miura, On the Feng-Rao bound for the L-construction of algebraic geometry
codes, IEICE Trans. Fundamentals, E83-A (2000), pp. 926-930] (See also [P.
Beelen and T. H{\o}holdt, The decoding of algebraic geometry codes, in Advances
in algebraic geometry codes, pp. 49-98]) derived from the Feng-Rao bound a
bound for primary one-point algebraic geometric codes and showed how to decode
up to what is guaranteed by their bound. The exposition by Matsumoto and Miura
requires the use of differentials which was not needed in [Andersen and Geil
2008]. Nevertheless we demonstrate a very strong connection between Matsumoto
and Miura's bound and Andersen and Geil's bound when applied to primary
one-point algebraic geometric codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6724</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6724</id><created>2012-10-24</created><authors><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Aguiar</keyname><forenames>A. Pedro</forenames></author></authors><title>A Structured Systems Approach for Optimal Actuator-Sensor Placement in
  Linear Time-Invariant Systems</title><categories>cs.SY cs.MA math.OC</categories><comments>8 pages, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the actuator/sensor allocation problem for linear
time invariant (LTI) systems. Given the structure of an autonomous linear
dynamical system, the goal is to design the structure of the input matrix
(commonly denoted by $B$) such that the system is structurally controllable
with the restriction that each input be dedicated, i.e., it can only control
directly a single state variable. We provide a methodology that addresses this
design question: specifically, we determine the minimum number of dedicated
inputs required to ensure such structural controllability, and characterize,
and characterizes all (when not unique) possible configurations of the
\emph{minimal} input matrix $B$. Furthermore, we show that the proposed
solution methodology incurs \emph{polynomial complexity} in the number of state
variables. By duality, the solution methodology may be readily extended to the
structural design of the corresponding minimal output matrix (commonly denoted
by $C$) that ensures structural observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6730</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6730</id><created>2012-10-24</created><authors><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author></authors><title>Measure What Should be Measured: Progress and Challenges in Compressive
  Sensing</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2012.2224518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is compressive sensing overrated? Or can it live up to our expectations? What
will come after compressive sensing and sparsity? And what has Galileo Galilei
got to do with it? Compressive sensing has taken the signal processing
community by storm. A large corpus of research devoted to the theory and
numerics of compressive sensing has been published in the last few years.
Moreover, compressive sensing has inspired and initiated intriguing new
research directions, such as matrix completion. Potential new applications
emerge at a dazzling rate. Yet some important theoretical questions remain
open, and seemingly obvious applications keep escaping the grip of compressive
sensing. In this paper I discuss some of the recent progress in compressive
sensing and point out key challenges and opportunities as the area of
compressive sensing and sparse representations keeps evolving. I also attempt
to assess the long-term impact of compressive sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6732</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6732</id><created>2012-10-24</created><updated>2012-10-27</updated><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>Hildebrand</keyname><forenames>Robert</forenames></author><author><keyname>Koeppe</keyname><forenames>Matthias</forenames></author></authors><title>Equivariant Perturbation in Gomory and Johnson's Infinite Group Problem.
  II. The Unimodular Two-Dimensional Case</title><categories>math.OC cs.DM</categories><comments>23 pages. arXiv admin note: text overlap with arXiv:1206.2079</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for testing the extremality of a large class of minimal
valid functions for the two-dimensional infinite group problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6738</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6738</id><created>2012-10-25</created><updated>2014-05-02</updated><authors><author><keyname>Paisley</keyname><forenames>John</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Nested Hierarchical Dirichlet Processes</title><categories>stat.ML cs.LG</categories><comments>To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence, Special Issue on Bayesian Nonparametrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical
topic modeling. The nHDP is a generalization of the nested Chinese restaurant
process (nCRP) that allows each word to follow its own path to a topic node
according to a document-specific distribution on a shared tree. This alleviates
the rigid, single-path formulation of the nCRP, allowing a document to more
easily express thematic borrowings as a random effect. We derive a stochastic
variational inference algorithm for the model, in addition to a greedy subtree
selection method for each document, which allows for efficient inference using
massive collections of text documents. We demonstrate our algorithm on 1.8
million documents from The New York Times and 3.3 million documents from
Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6740</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6740</id><created>2012-10-25</created><authors><author><keyname>Xue</keyname><forenames>Feng</forenames></author></authors><title>An upper bound on relaying over capacity based on channel simulation</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, 21 pages, 6
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The upper bound on the capacity of a 3-node discrete memoryless relay channel
is considered, where a source X wants to send information to destination Y with
the help of a relay Z. Y and Z are independent given X, and the link from Z to
Y is lossless with rate $R_0$. A new inequality is introduced to upper-bound
the capacity when the encoding rate is beyond the capacities of both individual
links XY and XZ. It is based on generalization of the blowing-up lemma, linking
conditional entropy to decoding error, and channel simulation, to the case with
side information. The achieved upper-bound is strictly better than the
well-known cut-set bound in several cases when the latter is $C_{XY}+R_0$, with
$C_{XY}$ being the channel capacity between X and Y. One particular case is
when the channel is statistically degraded, i.e., either Y is a statistically
degraded version of Z with respect to X, or Z is a statistically degraded
version of Y with respect to X. Moreover in this case, the bound is shown to be
explicitly computable. The binary erasure channel is analyzed in detail and
evaluated numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6746</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6746</id><created>2012-10-25</created><authors><author><keyname>Mahmud</keyname><forenames>Hossain</forenames></author><author><keyname>Amin</keyname><forenames>Ashfaq Mahmood</forenames></author><author><keyname>Ali</keyname><forenames>Mohammed Eunus</forenames></author><author><keyname>Hashem</keyname><forenames>Tanzima</forenames></author></authors><title>Shared Execution of Path Queries on Road Networks</title><categories>cs.DB</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of mobile technologies and the proliferation of map-based
applications have enabled a user to access a wide variety of services that
range from information queries to navigation systems. Due to the popularity of
map-based applications among the users, the service provider often requires to
answer a large number of simultaneous queries. Thus, processing queries
efficiently on spatial networks (i.e., road networks) have become an important
research area in recent years. In this paper, we focus on path queries that
find the shortest path between a source and a destination of the user. In
particular, we address the problem of finding the shortest paths for a large
number of simultaneous path queries in road networks. Traditional systems that
consider one query at a time are not suitable for many applications due to high
computational and service costs. These systems cannot guarantee required
response time in high load conditions. We propose an efficient group based
approach that provides a practical solution with reduced cost. The key concept
for our approach is to group queries that share a common travel path and then
compute the shortest path for the group. Experimental results show that our
approach is on an average ten times faster than the traditional approach in
return of sacrificing the accuracy by 0.5% in the worst case, which is
acceptable for most of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6764</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6764</id><created>2012-10-25</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Universal decoding for arbitrary channels relative to a given class of
  decoding metrics</title><categories>cs.IT math.IT</categories><comments>25 pages; Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of universal decoding for arbitrary unknown channels
in the random coding regime. For a given random coding distribution and a given
class of metric decoders, we propose a generic universal decoder whose average
error probability is, within a sub-exponential multiplicative factor, no larger
than that of the best decoder within this class of decoders. Since the optimum,
maximum likelihood (ML) decoder of the underlying channel is not necessarily
assumed to belong to the given class of decoders, this setting suggests a
common generalized framework for: (i) mismatched decoding, (ii) universal
decoding for a given family of channels, and (iii) universal coding and
decoding for deterministic channels using the individual-sequence approach. The
proof of our universality result is fairly simple, and it is demonstrated how
some earlier results on universal decoding are obtained as special cases. We
also demonstrate how our method extends to more complicated scenarios, like
incorporation of noiseless feedback, and the multiple access channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6766</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6766</id><created>2012-10-25</created><authors><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author><author><keyname>Golbabaee</keyname><forenames>Mohammad</forenames></author><author><keyname>Bourlard</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Structured Sparsity Models for Multiparty Speech Recovery from
  Reverberant Recordings</title><categories>cs.LG cs.SD</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the multi-party speech recovery problem through modeling the
acoustic of the reverberant chambers. Our approach exploits structured sparsity
models to perform room modeling and speech recovery. We propose a scheme for
characterizing the room acoustic from the unknown competing speech sources
relying on localization of the early images of the speakers by sparse
approximation of the spatial spectra of the virtual sources in a free-space
model. The images are then clustered exploiting the low-rank structure of the
spectro-temporal components belonging to each source. This enables us to
identify the early support of the room impulse response function and its unique
map to the room geometry. To further tackle the ambiguity of the reflection
ratios, we propose a novel formulation of the reverberation model and estimate
the absorption coefficients through a convex optimization exploiting joint
sparsity model formulated upon spatio-spectral sparsity of concurrent speech
representation. The acoustic parameters are then incorporated for separating
individual speech signals through either structured sparse recovery or inverse
filtering the acoustic channels. The experiments conducted on real data
recordings demonstrate the effectiveness of the proposed approach for
multi-party speech recovery and recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6777</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6777</id><created>2012-10-25</created><authors><author><keyname>Rodrigues</keyname><forenames>Miguel</forenames></author></authors><title>Multiple-antenna fading coherent channels with arbitrary inputs:
  Characterization and optimization of the reliable information transmission
  rate</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the constrained capacity of multiple-antenna fading coherent
channels, where the receiver knows the channel state but the transmitter knows
only the channel distribution, driven by arbitrary equiprobable discrete inputs
in a regime of high signal-to-noise ratio (${\sf snr}$). In particular, we
capitalize on intersections between information theory and estimation theory to
conceive expansions to the average minimum-mean squared error (MMSE) and the
average mutual information, which leads to an expansion of the constrained
capacity, that capture well their behavior in the asymptotic regime of high
${\sf snr}$. We use the expansions to study the constrained capacity of various
multiple-antenna fading coherent channels, including Rayleigh fading models,
Ricean fading models and antenna-correlated models. The analysis unveils in
detail the impact of the number of transmit and receive antennas, transmit and
receive antenna correlation, line-of-sight components and the geometry of the
signalling scheme on the reliable information transmission rate. We also use
the expansions to design key system elements, such as power allocation and
precoding schemes, as well as to design space-time signalling schemes for
multiple-antenna fading coherent channels. Simulations results demonstrate that
the expansions lead to very sharp designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6780</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6780</id><created>2012-10-25</created><updated>2013-05-14</updated><authors><author><keyname>Dreier</keyname><forenames>Jannik</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Lafourcade</keyname><forenames>Pascal</forenames><affiliation>VERIMAG - IMAG</affiliation></author></authors><title>Brandt's Fully Private Auction Protocol Revisited</title><categories>cs.CR cs.GT</categories><comments>Africacrypt 2013, Le Caire : Egypt (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auctions have a long history, having been recorded as early as 500 B.C.
Nowadays, electronic auctions have been a great success and are increasingly
used. Many cryptographic protocols have been proposed to address the various
security requirements of these electronic transactions, in particular to ensure
privacy. Brandt developed a protocol that computes the winner using homomorphic
operations on a distributed ElGamal encryption of the bids. He claimed that it
ensures full privacy of the bidders, i.e. no information apart from the winner
and the winning price is leaked. We first show that this protocol -- when using
malleable interactive zero-knowledge proofs -- is vulnerable to attacks by
dishonest bidders. Such bidders can manipulate the publicly available data in a
way that allows the seller to deduce all participants' bids. Additionally we
discuss some issues with verifiability as well as attacks on non-repudiation,
fairness and the privacy of individual bidders exploiting authentication
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6800</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6800</id><created>2012-10-25</created><updated>2012-11-05</updated><authors><author><keyname>Buffenoir</keyname><forenames>Eric</forenames></author><author><keyname>Bourdon</keyname><forenames>Isabelle</forenames></author></authors><title>Reconciling complex organizations and data management: the Panopticon
  paradigm</title><categories>cs.CY cs.SI</categories><comments>19 pages (including extended bibliography)</comments><report-no>L2C:12-186</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These last years, main IT companies have build software solutions and change
management plans promoting data quality management within organizations
concerned by the enhancement of their business intelligence system. These
offers are closely similar data governance schemes based on a common paradigm
called Master Data Management. These schemes appear generally inappropriate to
the context of complex extended organizations. On the other hand, the
community-based data governance schemes have shown their own efficiency to
contribute to the reliability of data in digital social networks, as well as
their ability to meet user expectations. After a brief analysis of the very
specific constraints weighting on extended organization s data governance, and
of peculiarities of monitoring and regulatory processes associated to
management control and IT within these, we propose a new scheme inspired by
Foucaldian analysis on governmentality: the Panopticon data governance
paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6815</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6815</id><created>2012-10-25</created><updated>2012-10-25</updated><authors><author><keyname>Lecomte</keyname><forenames>Thierry</forenames></author><author><keyname>Burdy</keyname><forenames>Lilian</forenames></author><author><keyname>Leuschel</keyname><forenames>Michael</forenames></author></authors><title>Formally Checking Large Data Sets in the Railways</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents industrial experience of validating large data sets
against specification written using the B / Event-B mathematical language and
the ProB model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6819</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6819</id><created>2012-10-25</created><updated>2013-04-07</updated><authors><author><keyname>Li</keyname><forenames>Lingxiang</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author></authors><title>On Feasibility of Generalized Interference Alignment with Partial
  Interference Cancelation</title><categories>cs.IT math.IT</categories><comments>No comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new IA strategy which is referred to as &quot;Partial Interference
Cancelation-based Interference Alignment&quot; (PIC-IA). Unlike the conventional IA
strategy, PIC-IA does not strive to eliminate interference from all users.
Instead, it aims to remove the most significant interference signals. This
PIC-IA strategy generalizes the conventional IA concept by addressing partial,
instead of complete, interference cancelation. The feasibility of this new
strategy is studied in this paper. Our results show that for a symmetric,
single-stream system with $N_t$ transmit antennas and $N_r$ receive antennas,
the PIC-IA is feasible when the number of significant interference signals to
be removed at each receiver is no more than $N_t+N_r-2$, no matter how many
users are in the network. This is in sharp contrast to the conventional IA
whose feasibility is severely limited by the number of users $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6853</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6853</id><created>2012-10-25</created><updated>2014-05-21</updated><authors><author><keyname>Ben-Tal</keyname><forenames>Aharon</forenames></author><author><keyname>Nemirovski</keyname><forenames>Arkadi</forenames></author></authors><title>On solving large scale polynomial convex problems by randomized
  first-order algorithms</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most attractive recent approaches to processing well-structured
large-scale convex optimization problems is based on smooth convex-concave
saddle point reformu-lation of the problem of interest and solving the
resulting problem by a fast First Order saddle point method utilizing
smoothness of the saddle point cost function. In this paper, we demonstrate
that when the saddle point cost function is polynomial, the precise gra-dients
of the cost function required by deterministic First Order saddle point
algorithms and becoming prohibitively computationally expensive in the
extremely large-scale case, can be replaced with incomparably cheaper
computationally unbiased random estimates of the gradients. We show that for
large-scale problems with favourable geometry, this randomization accelerates,
progressively as the sizes of the problem grow, the solution process. This
extends significantly previous results on acceleration by randomization, which,
to the best of our knowledge, dealt solely with bilinear saddle point problems.
We illustrate our theoretical findings by instructive and encouraging numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6855</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6855</id><created>2012-10-25</created><authors><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Nov&#xe1;k</keyname><forenames>Peter</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>P&#x11b;chou&#x10d;ek</keyname><forenames>Michal</forenames></author></authors><title>Asynchronous Decentralized Algorithm for Space-Time Cooperative
  Pathfinding</title><categories>cs.AI cs.DC cs.RO</categories><journal-ref>Spatio-Temporal Dynamics (STeDy 2012). Editors: Mehul Bhatt, Hans
  Guesgen, and Ernest Davis. Workshop Proceedings of the European Conference on
  Articial Intelligence (ECAI 2012), Montpellier, France</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cooperative pathfinding is a multi-agent path planning problem where a group
of vehicles searches for a corresponding set of non-conflicting space-time
trajectories. Many of the practical methods for centralized solving of
cooperative pathfinding problems are based on the prioritized planning
strategy. However, in some domains (e.g., multi-robot teams of unmanned aerial
vehicles, autonomous underwater vehicles, or unmanned ground vehicles) a
decentralized approach may be more desirable than a centralized one due to
communication limitations imposed by the domain and/or privacy concerns.
  In this paper we present an asynchronous decentralized variant of prioritized
planning ADPP and its interruptible version IADPP. The algorithm exploits the
inherent parallelism of distributed systems and allows for a speed up of the
computation process. Unlike the synchronized planning approaches, the algorithm
allows an agent to react to updates about other agents' paths immediately and
invoke its local spatio-temporal path planner to find the best trajectory, as
response to the other agents' choices. We provide a proof of correctness of the
algorithms and experimentally evaluate them on synthetic domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6857</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6857</id><created>2012-10-25</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Petit</keyname><forenames>Barbara</forenames></author></authors><title>The Geometry of Types (Long Version)</title><categories>cs.LO cs.PL</categories><comments>27 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that time complexity analysis of higher-order functional programs can
be effectively reduced to an arguably simpler (although computationally
equivalent) verification problem, namely checking first-order inequalities for
validity. This is done by giving an efficient inference algorithm for linear
dependent types which, given a PCF term, produces in output both a linear
dependent type and a cost expression for the term, together with a set of proof
obligations. Actually, the output type judgement is derivable iff all proof
obligations are valid. This, coupled with the already known relative
completeness of linear dependent types, ensures that no information is lost,
i.e., that there are no false positives or negatives. Moreover, the procedure
reflects the difficulty of the original problem: simple PCF terms give rise to
sets of proof obligations which are easy to solve. The latter can then be put
in a format suitable for automatic or semi-automatic verification by external
solvers. Ongoing experimental evaluation has produced encouraging results,
which are briefly presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6883</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6883</id><created>2012-10-25</created><updated>2012-11-05</updated><authors><author><keyname>Neff</keyname><forenames>Jessica G.</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Kappler</keyname><forenames>Karolin</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author><author><keyname>Arag&#xf3;n</keyname><forenames>Pablo</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author></authors><title>Jointly they edit: examining the impact of community identification on
  political interaction in Wikipedia</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>33 pages, 5 figures</comments><journal-ref>PLoS ONE 8(4): e60584 (2013)</journal-ref><doi>10.1371/journal.pone.0060584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In their 2005 study, Adamic and Glance coined the memorable phrase &quot;divided
they blog&quot;, referring to a trend of cyberbalkanization in the political
blogosphere, with liberal and conservative blogs tending to link to other blogs
with a similar political slant, and not to one another. As political discussion
and activity increasingly moves online, the power of framing political
discourses is shifting from mass media to social media. Continued examination
of political interactions online is critical, and we extend this line of
research by examining the activities of political users within the Wikipedia
community. First, we examined how users in Wikipedia choose to display (or not
to display) their political affiliation. Next, we more closely examined the
patterns of cross-party interaction and community participation among those
users proclaiming a political affiliation. In contrast to previous analyses of
other social media, we did not find strong trends indicating a preference to
interact with members of the same political party within the Wikipedia
community. Our results indicate that users who proclaim their political
affiliation within the community tend to proclaim their identity as a
&quot;Wikipedian&quot; even more loudly. It seems that the shared identity of &quot;being
Wikipedian&quot; may be strong enough to triumph over other potentially divisive
facets of personal identity, such as political affiliation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6891</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6891</id><created>2012-10-24</created><authors><author><keyname>Phua</keyname><forenames>Clifton</forenames></author><author><keyname>Cao</keyname><forenames>Hong</forenames></author><author><keyname>Gomes</keyname><forenames>Jo&#xe3;o B&#xe1;rtolo</forenames></author><author><keyname>Nguyen</keyname><forenames>Minh Nhut</forenames></author></authors><title>Predicting Near-Future Churners and Win-Backs in the Telecommunications
  Industry</title><categories>cs.CE cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this work, we presented the strategies and techniques that we have
developed for predicting the near-future churners and win-backs for a telecom
company. On a large-scale and real-world database containing customer profiles
and some transaction data from a telecom company, we first analyzed the data
schema, developed feature computation strategies and then extracted a large set
of relevant features that can be associated with the customer churning and
returning behaviors. Our features include both the original driver factors as
well as some derived features. We evaluated our features on the imbalance
corrected dataset, i.e. under-sampled dataset and compare a large number of
existing machine learning tools, especially decision tree-based classifiers,
for predicting the churners and win-backs. In general, we find RandomForest and
SimpleCart learning algorithms generally perform well and tend to provide us
with highly competitive prediction performance. Among the top-15 driver factors
that signal the churn behavior, we find that the service utilization, e.g. last
two months' download and upload volume, last three months' average upload and
download, and the payment related factors are the most indicative features for
predicting if churn will happen soon. Such features can collectively tell
discrepancies between the service plans, payments and the dynamically changing
utilization needs of the customers. Our proposed features and their
computational strategy exhibit reasonable precision performance to predict
churn behavior in near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6893</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6893</id><created>2012-10-25</created><authors><author><keyname>Madelaine</keyname><forenames>Florent</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>On the complexity of the model checking problem</title><categories>cs.LO cs.CC</categories><acm-class>F.2.2; F.4.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model checking problem for various fragments of first-order logic has
attracted much attention over the last two decades: in particular, for the
primitive positive and the positive Horn fragments, which are better known as
the constraint satisfaction problem and the quantified constraint satisfaction
problem, respectively. These two fragments are in fact the only ones for which
there is currently no known complexity classification. All other syntactic
fragments can be easily classified, either directly or using Schaefer's
dichotomy theorems for SAT and QSAT, with the exception of the positive
equality free fragment. This outstanding fragment can also be classified and
enjoys a tetrachotomy: according to the model, the corresponding model checking
problem is either tractable, NP-complete, co-NP-complete or Pspace-complete.
Moreover, the complexity drop is always witnessed by a generic solving
algorithm which uses quantifier relativisation. Furthermore, its complexity is
characterised by algebraic means: the presence or absence of specific
surjective hyper-operations among those that preserve the model characterise
the complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6908</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6908</id><created>2012-10-25</created><updated>2014-06-30</updated><authors><author><keyname>Disanto</keyname><forenames>Filippo</forenames></author><author><keyname>Wiehe</keyname><forenames>Thomas</forenames></author></authors><title>On the sub-permutations of pattern avoiding permutations</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a deep connection between permutations and trees. Certain
sub-structures of permutations, called sub-permutations, bijectively map to
sub-trees of binary increasing trees. This opens a powerful tool set to study
enumerative and probabilistic properties of sub-permutations and to investigate
the relationships between 'local' and 'global' features using the concept of
pattern avoidance. First, given a pattern {\mu}, we study how the avoidance of
{\mu} in a permutation {\pi} affects the presence of other patterns in the
sub-permutations of {\pi}. More precisely, considering patterns of length 3, we
solve instances of the following problem: given a class of permutations K and a
pattern {\mu}, we ask for the number of permutations $\pi \in Av_n(\mu)$ whose
sub-permutations in K satisfy certain additional constraints on their size.
Second, we study the probability for a generic pattern to be contained in a
random permutation {\pi} of size n without being present in the
sub-permutations of {\pi} generated by the entry $1 \leq k \leq n$. These
theoretical results can be useful to define efficient randomized pattern-search
procedures based on classical algorithms of pattern-recognition, while the
general problem of pattern-search is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6910</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6910</id><created>2012-10-25</created><authors><author><keyname>Foukalas</keyname><forenames>F.</forenames></author><author><keyname>Karetsos</keyname><forenames>G. T.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>G. K.</forenames></author></authors><title>Adaptive Modulation in OSA-based Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Opportunistic spectrum access is based on channel state information and can
lead to important performance improvements for the underlying communication
systems. On the other hand adaptive modulation is also based on channel state
information and can achieve increased transmission rates in fading channels. In
this work we propose the combination of adaptive modulation with opportunistic
spectrum access and we study the anticipated effects on the performance of
wireless communication systems in terms of achieved spectral efficiency and
power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6912</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6912</id><created>2012-10-25</created><authors><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Manocha</keyname><forenames>Sahil</forenames></author><author><keyname>Atluri</keyname><forenames>Gowtham</forenames></author><author><keyname>Kumar</keyname><forenames>Vipin</forenames></author></authors><title>Enhancing the functional content of protein interaction networks</title><categories>q-bio.MN cs.CE cs.LG q-bio.GN stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein interaction networks are a promising type of data for studying
complex biological systems. However, despite the rich information embedded in
these networks, they face important data quality challenges of noise and
incompleteness that adversely affect the results obtained from their analysis.
Here, we explore the use of the concept of common neighborhood similarity
(CNS), which is a form of local structure in networks, to address these issues.
Although several CNS measures have been proposed in the literature, an
understanding of their relative efficacies for the analysis of interaction
networks has been lacking. We follow the framework of graph transformation to
convert the given interaction network into a transformed network corresponding
to a variety of CNS measures evaluated. The effectiveness of each measure is
then estimated by comparing the quality of protein function predictions
obtained from its corresponding transformed network with those from the
original network. Using a large set of S. cerevisiae interactions, and a set of
136 GO terms, we find that several of the transformed networks produce more
accurate predictions than those obtained from the original network. In
particular, the $HC.cont$ measure proposed here performs particularly well for
this task. Further investigation reveals that the two major factors
contributing to this improvement are the abilities of CNS measures, especially
$HC.cont$, to prune out noisy edges and introduce new links between
functionally related proteins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6917</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6917</id><created>2012-10-25</created><authors><author><keyname>Ben-Sasson</keyname><forenames>Eli</forenames></author><author><keyname>Ron-Zewi</keyname><forenames>Noga</forenames></author><author><keyname>Tulsiani</keyname><forenames>Madhur</forenames></author><author><keyname>Wolf</keyname><forenames>Julia</forenames></author></authors><title>Sampling-based proofs of almost-periodicity results and algorithmic
  applications</title><categories>cs.DM cs.DS math.CO</categories><comments>28 pages</comments><msc-class>11B30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new combinatorial proofs of known almost-periodicity results for
sumsets of sets with small doubling in the spirit of Croot and Sisask, whose
almost-periodicity lemma has had far-reaching implications in additive
combinatorics. We provide an alternative (and L^p-norm free) point of view,
which allows for proofs to easily be converted to probabilistic algorithms that
decide membership in almost-periodic sumsets of dense subsets of F_2^n.
  As an application, we give a new algorithmic version of the quasipolynomial
Bogolyubov-Ruzsa lemma recently proved by Sanders. Together with the results by
the last two authors, this implies an algorithmic version of the quadratic
Goldreich-Levin theorem in which the number of terms in the quadratic Fourier
decomposition of a given function is quasipolynomial in the error parameter,
compared with an exponential dependence previously proved by the authors. It
also improves the running time of the algorithm to have quasipolynomial
dependence instead of an exponential one.
  We also give an application to the problem of finding large subspaces in
sumsets of dense sets. Green showed that the sumset of a dense subset of F_2^n
contains a large subspace. Using Fourier analytic methods, Sanders proved that
such a subspace must have dimension bounded below by a constant times the
density times n. We provide an alternative (and L^p norm-free) proof of a
comparable bound, which is analogous to a recent result of Croot, Laba and
Sisask in the integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6918</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6918</id><created>2012-10-25</created><authors><author><keyname>Levit</keyname><forenames>Vadim</forenames></author><author><keyname>Tankus</keyname><forenames>David</forenames></author></authors><title>Well-Covered Graphs Without Cycles of Lengths 4, 5 and 6</title><categories>cs.DM math.CO</categories><comments>11 pages, 2 figures</comments><msc-class>05C69 (Primary) 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is well-covered if all its maximal independent sets are of the same
cardinality. Assume that a weight function w is defined on its vertices. Then G
is w-well-covered if all maximal independent sets are of the same weight. For
every graph G, the set of weight functions w such that G is w-well-covered is a
vector space. Given an input graph G without cycles of length 4, 5, and 6, we
characterize polynomially the vector space of weight functions w for which G is
w-well-covered. Let B be an induced complete bipartite subgraph of G on vertex
sets of bipartition B_{X} and B_{Y}. Assume that there exists an independent
set S such that both the union of S and B_{X} and the union of S and B_{Y} are
maximal independent sets of G. Then B is a generating subgraph of G, and it
produces the restriction w(B_{X})=w(B_{Y}). It is known that for every weight
function w, if G is w-well-covered, then the above restriction is satisfied. In
the special case, where B_{X}={x} and B_{Y}={y}, we say that xy is a relating
edge. Recognizing relating edges and generating subgraphs is an NP-complete
problem. However, we provide a polynomial algorithm for recognizing generating
subgraphs of an input graph without cycles of length 5, 6 and 7. We also
present a polynomial algorithm for recognizing relating edges in an input graph
without cycles of length 5 and 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6923</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6923</id><created>2012-10-25</created><authors><author><keyname>Pat</keyname><forenames>Ankit</forenames></author></authors><title>On Construction of a Class of Orthogonal Arrays (Thesis)</title><categories>cs.DM math.CO</categories><comments>Master's Thesis, Indian Institute of Technology (IIT) - Kharagpur</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for the construction of orthogonal arrays. The
algorithm makes use of the Kronecker Product operator in association with unit
column vectors to generate new orthogonal arrays from existing orthogonal
arrays. The effectiveness of the proposed algorithm lies in the fact that it
works well with any linear seed orthogonal array without imposing any
constraints on the strength or the number of levels. The resulting orthogonal
array has the same strength as the seed orthogonal array. We also discuss the
proof of correctness of the algorithm. In the Results section we provide a list
of new orthogonal arrays generated using this algorithm, that are currently not
present in the libraries of orthogonal arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6927</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6927</id><created>2012-10-25</created><updated>2013-12-20</updated><authors><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Farina</keyname><forenames>Marcello</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>Plug-and-Play Model Predictive Control based on robust control invariant
  sets</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a linear system represented by a coupling graph
between subsystems and propose a distributed control scheme capable to
guarantee asymptotic stability and satisfaction of constraints on system inputs
and states. Most importantly, as in Riverso et al., 2012 our design procedure
enables plug-and-play (PnP) operations, meaning that (i) the addition or
removal of subsystems triggers the design of local controllers associated to
successors to the subsystem only and (ii) the synthesis of a local controller
for a subsystem requires information only from predecessors of the subsystem
and it can be performed using only local computational resources. Our method
hinges on local tube MPC controllers based on robust control invariant sets and
it advances the PnP design procedure proposed in Riverso et al., 2012 in
several directions. Quite notably, using recent results in the computation of
robust control invariant sets, we show how critical steps in the design of a
local controller can be solved through linear programming. Finally, an
application of the proposed control design procedure to frequency control in
power networks is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6954</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6954</id><created>2012-10-25</created><updated>2013-08-06</updated><authors><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Optimal Locally Repairable and Secure Codes for Distributed Storage
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to go beyond resilience into the study of security and
local-repairability for distributed storage systems (DSS). Security and
local-repairability are both important as features of an efficient storage
system, and this paper aims to understand the trade-offs between resilience,
security, and local-repairability in these systems. In particular, this paper
first investigates security in the presence of colluding eavesdroppers, where
eavesdroppers are assumed to work together in decoding stored information.
Second, the paper focuses on coding schemes that enable optimal local repairs.
It further brings these two concepts together, to develop locally repairable
coding schemes for DSS that are secure against eavesdroppers.
  The main results of this paper include: a. An improved bound on the secrecy
capacity for minimum storage regenerating codes, b. secure coding schemes that
achieve the bound for some special cases, c. a new bound on minimum distance
for locally repairable codes, d. code construction for locally repairable codes
that attain the minimum distance bound, and e. repair-bandwidth-efficient
locally repairable codes with and without security constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6956</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6956</id><created>2012-10-25</created><updated>2013-03-09</updated><authors><author><keyname>Baayen</keyname><forenames>Jorn H.</forenames></author></authors><title>Vortexje - An Open-Source Panel Method for Co-Simulation</title><categories>cs.CE physics.flu-dyn</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the use of the 3-dimensional panel method for dynamical
system simulation. Specifically, the advantages and disadvantages of model
exchange versus co-simulation of the aerodynamics and the dynamical system
model are discussed. Based on a trade-off analysis, a set of recommendations
for a panel method implementation and for a co-simulation environment is
proposed. These recommendations are implemented in a C++ library, offered
on-line under an open source license. This code is validated against XFLR5, and
its suitability for co-simulation is demonstrated with an example of a tethered
wing, i.e, a kite. The panel method implementation and the co-simulation
environment are shown to be able to solve this stiff problem in a stable
fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6962</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6962</id><created>2012-10-25</created><updated>2013-03-24</updated><authors><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Quantum-to-classical rate distortion coding</title><categories>quant-ph cs.IT math.IT</categories><comments>21 pages, 3 png figures, accepted for publication in Journal of
  Mathematical Physics</comments><journal-ref>Journal of Mathematical Physics 54, 042201 (2013)</journal-ref><doi>10.1063/1.4798396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a theory of quantum-to-classical rate distortion coding. In this
setting, a sender Alice has many copies of a quantum information source. Her
goal is to transmit classical information about the source, obtained by
performing a measurement on it, to a receiver Bob, up to some specified level
of distortion. We derive a single-letter formula for the minimum rate of
classical communication needed for this task. We also evaluate this rate in the
case in which Bob has some quantum side information about the source. Our
results imply that, in general, Alice's best strategy is a non-classical one,
in which she performs a collective measurement on successive outputs of the
source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.6963</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.6963</id><created>2012-10-25</created><updated>2014-06-21</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Lavaee</keyname><forenames>Rahman</forenames></author><author><keyname>Menton</keyname><forenames>Curtis</forenames></author></authors><title>Schulze and Ranked-Pairs Voting are Fixed-Parameter Tractable to Bribe,
  Manipulate, and Control</title><categories>cs.GT cs.DS cs.MA</categories><report-no>URCS-TR-2012-982</report-no><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schulze and ranked-pairs elections have received much attention recently, and
the former has quickly become a quite widely used election system. For many
cases these systems have been proven resistant to bribery, control, or
manipulation, with ranked pairs being particularly praised for being NP-hard
for all three of those. Nonetheless, the present paper shows that with respect
to the number of candidates, Schulze and ranked-pairs elections are
fixed-parameter tractable to bribe, control, and manipulate: we obtain uniform,
polynomial-time algorithms whose degree does not depend on the number of
candidates. We also provide such algorithms for some weighted variants of these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7002</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7002</id><created>2012-10-25</created><authors><author><keyname>Hamou</keyname><forenames>Mohamed</forenames></author><author><keyname>Amine</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Lokbani</keyname><forenames>Ahmed Chaouki</forenames></author></authors><title>A Biomimetic Approach Based on Immune Systems for Classification of
  Unstructured Data</title><categories>cs.AI</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the results of unstructured data clustering in this
case a textual data from Reuters 21578 corpus with a new biomimetic approach
using immune system. Before experimenting our immune system, we digitalized
textual data by the n-grams approach. The novelty lies on hybridization of
n-grams and immune systems for clustering. The experimental results show that
the recommended ideas are promising and prove that this method can solve the
text clustering problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7009</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7009</id><created>2012-10-25</created><authors><author><keyname>Iwen</keyname><forenames>Mark</forenames></author><author><keyname>Santosa</keyname><forenames>Fadil</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>A symbol-based algorithm for decoding bar codes</title><categories>math.NA cs.IT math.IT math.OC</categories><comments>24 pages, 12 figures</comments><msc-class>44A35, 94A12, 65K05, 94A08, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of decoding a bar code from a signal measured with
a hand-held laser-based scanner. Rather than formulating the inverse problem as
one of binary image reconstruction, we instead incorporate the symbology of the
bar code into the reconstruction algorithm directly, and search for a sparse
representation of the UPC bar code with respect to this known dictionary. Our
approach significantly reduces the degrees of freedom in the problem, allowing
for accurate reconstruction that is robust to noise and unknown parameters in
the scanning device. We propose a greedy reconstruction algorithm and provide
robust reconstruction guarantees. Numerical examples illustrate the
insensitivity of our symbology-based reconstruction to both imprecise model
parameters and noise on the scanned measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7014</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7014</id><created>2012-10-25</created><updated>2012-11-07</updated><authors><author><keyname>Hashemi</keyname><forenames>Jordan</forenames></author><author><keyname>Spina</keyname><forenames>Thiago Vallin</forenames></author><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Esler</keyname><forenames>Amy</forenames></author><author><keyname>Morellas</keyname><forenames>Vassilios</forenames></author><author><keyname>Papanikolopoulos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Computer vision tools for the non-invasive assessment of autism-related
  behavioral markers</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The early detection of developmental disorders is key to child outcome,
allowing interventions to be initiated that promote development and improve
prognosis. Research on autism spectrum disorder (ASD) suggests behavioral
markers can be observed late in the first year of life. Many of these studies
involved extensive frame-by-frame video observation and analysis of a child's
natural behavior. Although non-intrusive, these methods are extremely
time-intensive and require a high level of observer training; thus, they are
impractical for clinical and large population research purposes. Diagnostic
measures for ASD are available for infants but are only accurate when used by
specialists experienced in early diagnosis. This work is a first milestone in a
long-term multidisciplinary project that aims at helping clinicians and general
practitioners accomplish this early detection/measurement task automatically.
We focus on providing computer vision tools to measure and identify ASD
behavioral markers based on components of the Autism Observation Scale for
Infants (AOSI). In particular, we develop algorithms to measure three critical
AOSI activities that assess visual attention. We augment these AOSI activities
with an additional test that analyzes asymmetrical patterns in unsupported
gait. The first set of algorithms involves assessing head motion by tracking
facial features, while the gait analysis relies on joint foreground
segmentation and 2D body pose estimation in video. We show results that provide
insightful knowledge to augment the clinician's behavioral observations
obtained from real in-clinic assessments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7030</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7030</id><created>2012-10-25</created><authors><author><keyname>Silva</keyname><forenames>Renato</forenames></author></authors><title>Lessons Learned/Sharing the Experience of Developing a Metro System Case
  Study</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document we share the experiences gained throughout the development
of a metro system case study. The model is constructed in Event-B using its
respective tool set, the Rodin platform. Starting from requirements, adding
more details to the model in a stepwise manner through refinement, we identify
some keys points and available plugins necessary for modelling large systems
(requirement engineering, decomposition, generic instantiation, among others),
which ones are lacking plus strengths and weaknesses of the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7032</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7032</id><created>2012-10-25</created><authors><author><keyname>Troubitsyna</keyname><forenames>Elena</forenames></author></authors><title>Dependability-Explicit Engineering with Event-B: Overview of Recent
  Achievements</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-B has been actively used within the EU Deploy project to model
dependable systems from various application domains. As a result, we have
created a number of formal approaches to explicitly reason about dependability
in the refinement process. In this paper we overview the work on formal
engineering of dependable systems carried out in the Deploy project. We outline
our approaches to integrating safety analysis into the development process,
modelling fault tolerant systems and probabilistic dependability evaluation. We
discuss achievements and challenges in development of dependable systems within
the Event-B framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7034</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7034</id><created>2012-10-25</created><authors><author><keyname>Edmunds</keyname><forenames>Andrew</forenames></author><author><keyname>Butler</keyname><forenames>Michael</forenames></author><author><keyname>Colley</keyname><forenames>John</forenames></author></authors><title>Building on the DEPLOY Legacy: Code Generation and Simulation</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The RODIN, and DEPLOY projects laid solid foundations for further
theoretical, and practical (methodological and tooling) advances with Event-B.
Our current interest is the co-simulation of cyber-physical systems using
Event-B. Using this approach we aim to simulate various features of the
environment separately, in order to exercise deployable code. This paper has
two contributions, the first is the extension of the code generation work of
DEPLOY, where we add the ability to generate code from Event-B state-machine
diagrams. The second describes how we may use code, generated from
state-machines, to simulate the environment, and simulate concurrently
executing state-machines, in a single task. We show how we can instrument the
code to guide the simulation, by controlling the relative rate that
non-deterministic transitions are traversed in the simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7035</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7035</id><created>2012-10-25</created><authors><author><keyname>Pereverzeva</keyname><forenames>Inna</forenames></author><author><keyname>Troubitsyna</keyname><forenames>Elena</forenames></author><author><keyname>Laibinis</keyname><forenames>Linas</forenames></author></authors><title>Development of Fault Tolerant MAS with Cooperative Error Recovery by
  Refinement in Event-B</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing fault tolerance mechanisms for multi-agent systems is a notoriously
difficult task. In this paper we present an approach to formal development of a
fault tolerant multi-agent system by refinement in Event-B. We demonstrate how
to formally specify cooperative error recovery and dynamic reconfiguration in
Event-B. Moreover, we discuss how to express and verify essential properties of
a fault tolerant multi-agent system while refining it. The approach is
illustrated by a case study - a multi-robotic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7036</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7036</id><created>2012-10-25</created><authors><author><keyname>Kobayashi</keyname><forenames>Tsutomu</forenames></author><author><keyname>Honiden</keyname><forenames>Shinichi</forenames></author></authors><title>Towards Refinement Strategy Planning for Event-B</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-B is a formal approach oriented to system modeling and analysis. It
supports refinement mechanism that enables stepwise modeling and verification
of a system. By using refinement, the complexity of verification can be spread
and mitigated. In common development using Event-B, a specification written in
a natural language is examined before modeling in order to plan the modeling
and refinement strategy. After that, starting from a simple abstract model,
concrete models in several different abstraction levels are constructed by
gradually introducing complex structures and concepts. Although users of
Event-B have to plan how to abstract the specification for the construction of
each model, guidelines for such a planning have not been suggested.
Specifically, some elements in a model often require that other elements are
included in the model because of semantics constraints of Event-B. As such
requirements introduces many elements at once, non-experts of Event-B often
make refinement rough though rough refinement does not mitigate the complexity
of verification well. In response to the problem, a method is proposed to plan
what models are constructed in each abstraction level. The method calculates
plans that mitigate the complexity well considering the semantics constraints
of Event-B and the relationships between elements in a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7038</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7038</id><created>2012-10-25</created><authors><author><keyname>Oji</keyname><forenames>Reza</forenames></author><author><keyname>Tajeripour</keyname><forenames>Farshad</forenames></author></authors><title>Full Object Boundary Detection by Applying Scale Invariant Features in a
  Region Merging Segmentation Algorithm</title><categories>cs.CV cs.AI</categories><comments>10 pages - 7 figures</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA) (2012) Volume 3, Number 5, pp: 41-50</journal-ref><doi>10.5121/ijaia.2012.3504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is a fundamental task in computer vision and has many
applications in image processing. This paper proposes a new approach for object
detection by applying scale invariant feature transform (SIFT) in an automatic
segmentation algorithm. SIFT is an invariant algorithm respect to scale,
translation and rotation. The features are very distinct and provide stable
keypoints that can be used for matching an object in different images. At
first, an object is trained with different aspects for finding best keypoints.
The object can be recognized in the other images by using achieved keypoints.
Then, a robust segmentation algorithm is used to detect the object with full
boundary based on SIFT keypoints. In segmentation algorithm, a merging role is
defined to merge the regions in image with the assistance of keypoints. The
results show that the proposed approach is reliable for object detection and
can extract object boundary well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7039</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7039</id><created>2012-10-25</created><authors><author><keyname>Badeau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Doche-Petit</keyname><forenames>Marielle</forenames></author></authors><title>Formal Data Validation with Event-B</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a verification and validation activity performed in an
industrial context, to validate configuration data of a metro CBTC system by
creating a formal B model of these configuration data and of their properties.
A double tool chain is used to safely check whether a certain given input of
configuration data fulfill its properties. One tool is based on some Rodin and
open source plug-ins and the other tool is based on ProB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7043</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7043</id><created>2012-10-25</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Huemer</keyname><forenames>Clemens</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>Empty Monochromatic Simplices</title><categories>math.CO cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a $k$-colored (finite) set of $n$ points in $\mathbb{R}^d$, $d\geq
3$, in general position, that is, no {$(d + 1)$} points of $S$ lie in a common
$(d - 1)$}-dimensional hyperplane. We count the number of empty monochromatic
$d$-simplices determined by $S$, that is, simplices which have only points from
one color class of $S$ as vertices and no points of $S$ in their interior. For
$3 \leq k \leq d$ we provide a lower bound of $\Omega(n^{d-k+1+2^{-d}})$ and
strengthen this to $\Omega(n^{d-2/3})$ for $k=2$. On the way we provide various
results on triangulations of point sets in $\mathbb{R}^d$. In particular, for
any constant dimension $d\geq3$, we prove that every set of $n$ points ($n$
sufficiently large), in general position in $\mathbb{R}^d$, admits a
triangulation with at least $dn+\Omega(\log n)$ simplices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7044</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7044</id><created>2012-10-25</created><authors><author><keyname>Oggier</keyname><forenames>Frederique</forenames></author><author><keyname>Sethuraman</keyname><forenames>B. A.</forenames></author></authors><title>Quotients of Orders in Cyclic Algebras and Space-Time Codes</title><categories>cs.IT math.IT</categories><msc-class>11S45 (Primary) 11T71, 94B40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $F$ be a number field with ring of integers $\Oc_F$ and $\Dc$ a division
$F$-algebra with a maximal cyclic subfield $K$. We study rings occurring as
quotients of a natural $\Oc_F$-order $\Lambda$ in $\Dc$ by two-sided ideals. We
reduce the problem to studying the ideal structure of $\Lambda/\qf^s\Lambda$,
where $\qf$ is a prime ideal in $\Oc_F$, $s\geq 1$. We study the case where
$\qf$ remains unramified in $K$, both when $s=1$ and $s&gt;1$. This work is
motivated by its applications to space-time coded modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7047</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7047</id><created>2012-10-25</created><authors><author><keyname>Li</keyname><forenames>Daifeng</forenames></author><author><keyname>Luo</keyname><forenames>Zhipeng</forenames></author><author><keyname>Sun</keyname><forenames>Golden Guo-zheng</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Jingwei</forenames></author></authors><title>User-level Weibo Recommendation incorporating Social Influence based on
  Semi-Supervised Algorithm</title><categories>cs.SI cs.CY cs.LG</categories><comments>to be sumitted in JASIST</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tencent Weibo, as one of the most popular micro-blogging services in China,
has attracted millions of users, producing 30-60 millions of weibo (similar as
tweet in Twitter) daily. With the overload problem of user generate content,
Tencent users find it is more and more hard to browse and find valuable
information at the first time. In this paper, we propose a Factor Graph based
weibo recommendation algorithm TSI-WR (Topic-Level Social Influence based Weibo
Recommendation), which could help Tencent users to find most suitable
information. The main innovation is that we consider both direct and indirect
social influence from topic level based on social balance theory. The main
advantages of adopting this strategy are that it could first build a more
accurate description of latent relationship between two users with weak
connections, which could help to solve the data sparsity problem; second
provide a more accurate recommendation for a certain user from a wider range.
Other meaningful contextual information is also combined into our model, which
include: Users profile, Users influence, Content of weibos, Topic information
of weibos and etc. We also design a semi-supervised algorithm to further reduce
the influence of data sparisty. The experiments show that all the selected
variables are important and the proposed model outperforms several baseline
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7053</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7053</id><created>2012-10-26</created><updated>2013-04-14</updated><authors><author><keyname>Than</keyname><forenames>Khoat</forenames></author><author><keyname>Ho</keyname><forenames>Tu Bao</forenames></author></authors><title>Managing sparsity, time, and quality of inference in topic models</title><categories>stat.ML cs.AI cs.CV stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference is an integral part of probabilistic topic models, but is often
non-trivial to derive an efficient algorithm for a specific model. It is even
much more challenging when we want to find a fast inference algorithm which
always yields sparse latent representations of documents. In this article, we
introduce a simple framework for inference in probabilistic topic models,
denoted by FW. This framework is general and flexible enough to be easily
adapted to mixture models. It has a linear convergence rate, offers an easy way
to incorporate prior knowledge, and provides us an easy way to directly trade
off sparsity against quality and time. We demonstrate the goodness and
flexibility of FW over existing inference methods by a number of tasks.
Finally, we show how inference in topic models with nonconjugate priors can be
done efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7054</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7054</id><created>2012-10-26</created><authors><author><keyname>Zhang</keyname><forenames>Youwei</forenames></author><author><keyname>Ghaoui</keyname><forenames>Laurent El</forenames></author></authors><title>Large-Scale Sparse Principal Component Analysis with Application to Text
  Data</title><categories>stat.ML cs.LG math.OC</categories><comments>Appeared in the proceedings of NIPS 2011; The Neural Information
  Processing Systems Conference (NIPS), Granada, Spain, December 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse PCA provides a linear combination of small number of features that
maximizes variance across data. Although Sparse PCA has apparent advantages
compared to PCA, such as better interpretability, it is generally thought to be
computationally much more expensive. In this paper, we demonstrate the
surprising fact that sparse PCA can be easier than PCA in practice, and that it
can be reliably applied to very large data sets. This comes from a rigorous
feature elimination pre-processing result, coupled with the favorable fact that
features in real-life data typically have exponentially decreasing variances,
which allows for many features to be eliminated. We introduce a fast block
coordinate ascent algorithm with much better computational complexity than the
existing first-order ones. We provide experimental results obtained on text
corpora involving millions of documents and hundreds of thousands of features.
These results illustrate how Sparse PCA can help organize a large corpus of
text data in a user-interpretable way, providing an attractive alternative
approach to topic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7056</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7056</id><created>2012-10-26</created><authors><author><keyname>Lu</keyname><forenames>Zhongqi</forenames></author><author><keyname>Zhong</keyname><forenames>Erheng</forenames></author><author><keyname>Zhao</keyname><forenames>Lili</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Pan</keyname><forenames>Weike</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>Selective Transfer Learning for Cross Domain Recommendation</title><categories>cs.LG cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering (CF) aims to predict users' ratings on items
according to historical user-item preference data. In many real-world
applications, preference data are usually sparse, which would make models
overfit and fail to give accurate predictions. Recently, several research works
show that by transferring knowledge from some manually selected source domains,
the data sparseness problem could be mitigated. However for most cases, parts
of source domain data are not consistent with the observations in the target
domain, which may misguide the target domain model building. In this paper, we
propose a novel criterion based on empirical prediction error and its variance
to better capture the consistency across domains in CF settings. Consequently,
we embed this criterion into a boosting framework to perform selective
knowledge transfer. Comparing to several state-of-the-art methods, we show that
our proposed selective transfer learning framework can significantly improve
the accuracy of rating prediction tasks on several real-world recommendation
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7057</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7057</id><created>2012-10-26</created><authors><author><keyname>Bahmani</keyname><forenames>Bahman</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Shinde</keyname><forenames>Rajendra</forenames></author></authors><title>Efficient Distributed Locality Sensitive Hashing</title><categories>cs.DC</categories><comments>A short version of this paper will appear in CIKM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed frameworks are gaining increasingly widespread use in
applications that process large amounts of data. One important example
application is large scale similarity search, for which Locality Sensitive
Hashing (LSH) has emerged as the method of choice, specially when the data is
high-dimensional. At its core, LSH is based on hashing the data points to a
number of buckets such that similar points are more likely to map to the same
buckets. To guarantee high search quality, the LSH scheme needs a rather large
number of hash tables. This entails a large space requirement, and in the
distributed setting, with each query requiring a network call per hash bucket
look up, this also entails a big network load. The Entropy LSH scheme proposed
by Panigrahy significantly reduces the number of required hash tables by
looking up a number of query offsets in addition to the query itself. While
this improves the LSH space requirement, it does not help with (and in fact
worsens) the search network efficiency, as now each query offset requires a
network call. In this paper, focusing on the Euclidian space under $l_2$ norm
and building up on Entropy LSH, we propose the distributed Layered LSH scheme,
and prove that it exponentially decreases the network cost, while maintaining a
good load balance between different machines. Our experiments also verify that
our scheme results in a significant network traffic reduction that brings about
large runtime improvement in real world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7070</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7070</id><created>2012-10-26</created><updated>2012-11-02</updated><authors><author><keyname>Bagon</keyname><forenames>Shai</forenames></author><author><keyname>Galun</keyname><forenames>Meirav</forenames></author></authors><title>A Multiscale Framework for Challenging Discrete Optimization</title><categories>cs.CV cs.LG math.OC stat.ML</categories><comments>5 pages, 1 figure, To appear in NIPS Workshop on Optimization for
  Machine Learning (December 2012). Camera-ready version. Fixed typos,
  acknowledgements added</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Current state-of-the-art discrete optimization methods struggle behind when
it comes to challenging contrast-enhancing discrete energies (i.e., favoring
different labels for neighboring variables). This work suggests a multiscale
approach for these challenging problems. Deriving an algebraic representation
allows us to coarsen any pair-wise energy using any interpolation in a
principled algebraic manner. Furthermore, we propose an energy-aware
interpolation operator that efficiently exposes the multiscale landscape of the
energy yielding an effective coarse-to-fine optimization scheme. Results on
challenging contrast-enhancing energies show significant improvement over
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7101</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7101</id><created>2012-10-26</created><updated>2012-10-29</updated><authors><author><keyname>Burnay</keyname><forenames>Corentin</forenames></author><author><keyname>Jureta</keyname><forenames>Ivan</forenames></author><author><keyname>Faulkner</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Influence of Context on Decision Making during Requirements Elicitation</title><categories>cs.SE cs.LO</categories><comments>appears in Proceedings of the 4th International Workshop on
  Acquisition, Representation and Reasoning with Contextualized Knowledge
  (ARCOE), 2012, Montpellier, France, held at the European Conference on
  Artificial Intelligence (ECAI-12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements engineers should strive to get a better insight into decision
making processes. During elicitation of requirements, decision making
influences how stakeholders communicate with engineers, thereby affecting the
engineers' understanding of requirements for the future information system.
Empirical studies issued from Artificial Intelligence offer an adequate
groundwork to understand how decision making is influenced by some particular
contextual factors. However, no research has gone into the validation of such
empirical studies in the process of collecting needs of the future system's
users. As an answer, the paper empirically studies factors, initially
identified by AI literature, that influence decision making and communication
during requirements elicitation. We argue that the context's structure of the
decision should be considered as a cornerstone to adequately study how
stakeholders decide to communicate or not a requirement. The paper proposes a
context framework to categorize former factors into specific families, and
support the engineers during the elicitation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7102</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7102</id><created>2012-10-26</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>Harivinod</keyname><forenames>N.</forenames></author><author><keyname>Kumari</keyname><forenames>M. Sharmila</forenames></author><author><keyname>Holla</keyname><forenames>K. Raghurama</forenames></author></authors><title>3D Face Recognition using Significant Point based SULD Descriptor</title><categories>cs.CV</categories><doi>10.1109/ICRTIT.2011.5972443</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this work, we present a new 3D face recognition method based on Speeded-Up
Local Descriptor (SULD) of significant points extracted from the range images
of faces. The proposed model consists of a method for extracting distinctive
invariant features from range images of faces that can be used to perform
reliable matching between different poses of range images of faces. For a given
3D face scan, range images are computed and the potential interest points are
identified by searching at all scales. Based on the stability of the interest
point, significant points are extracted. For each significant point we compute
the SULD descriptor which consists of vector made of values from the convolved
Haar wavelet responses located on concentric circles centred on the significant
point, and where the amount of Gaussian smoothing is proportional to the radii
of the circles. Experimental results show that the newly proposed method
provides higher recognition rate compared to other existing contemporary models
developed for 3D face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7123</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7123</id><created>2012-10-26</created><authors><author><keyname>Bos</keyname><forenames>Arie</forenames></author></authors><title>Index notation of grid graphs</title><categories>cs.CG</categories><comments>12 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By defining grids as graphs, geometric graphs can be represented in a very
concise way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7126</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7126</id><created>2012-10-26</created><authors><author><keyname>Ben-Ner</keyname><forenames>Moria</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author></authors><title>On Numbers of Pseudo-Triangulations</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the maximum numbers of pseudo-triangulations and pointed
pseudo-triangulations that can be embedded over a specific set of points in the
plane or contained in a specific triangulation.
  We derive the bounds $O(5.45^N)$ and $\Omega (2.41^N)$ for the maximum number
of pointed pseudo-triangulations that can be contained in a specific
triangulation over a set of $N$ points. For the number of all
pseudo-triangulations contained in a triangulation we derive the bounds
$O^*(6.54^N)$ and $\Omega (3.30^N)$. We also prove that $O^*(89.1^N)$ pointed
pseudo-triangulations can be embedded over any specific set of $N$ points in
the plane, and at most $120^N$ general pseudo-triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7136</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7136</id><created>2012-10-26</created><authors><author><keyname>P&#xe9;choux</keyname><forenames>Romain</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Synthesis of sup-interpretations: a survey</title><categories>cs.CC</categories><comments>(2012)</comments><proxy>ccsd</proxy><doi>10.1016/j.tcs.2012.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we survey the complexity of distinct methods that allow the
programmer to synthesize a sup-interpretation, a function providing an upper-
bound on the size of the output values computed by a program. It consists in a
static space analysis tool without consideration of the time consumption.
Although clearly related, sup-interpretation is independent from termination
since it only provides an upper bound on the terminating computations. First,
we study some undecidable properties of sup-interpretations from a theoretical
point of view. Next, we fix term rewriting systems as our computational model
and we show that a sup-interpretation can be obtained through the use of a
well-known termination technique, the polynomial interpretations. The drawback
is that such a method only applies to total functions (strongly normalizing
programs). To overcome this problem we also study sup-interpretations through
the notion of quasi-interpretation. Quasi-interpretations also suffer from a
drawback that lies in the subterm property. This property drastically restricts
the shape of the considered functions. Again we overcome this problem by
introducing a new notion of interpretations mainly based on the dependency
pairs method. We study the decidability and complexity of the
sup-interpretation synthesis problem for all these three tools over sets of
polynomials. Finally, we take benefit of some previous works on termination and
runtime complexity to infer sup-interpretations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7137</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7137</id><created>2012-10-26</created><authors><author><keyname>Ycart</keyname><forenames>Bernard</forenames><affiliation>LJK</affiliation></author></authors><title>Alberti's letter counts</title><categories>math.HO cs.CL</categories><proxy>ccsd</proxy><journal-ref>Literary and Linguistic Computing (2013) 10.1093/llc/fqt034</journal-ref><doi>10.1093/llc/fqt034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four centuries before modern statistical linguistics was born, Leon Battista
Alberti (1404--1472) compared the frequency of vowels in Latin poems and
orations, making the first quantified observation of a stylistic difference
ever. Using a corpus of 20 Latin texts (over 5 million letters), Alberti's
observations are statistically assessed. Letter counts prove that poets used
significantly more a's, e's, and y's, whereas orators used more of the other
vowels. The sample sizes needed to justify the assertions are studied, and
proved to be within reach for Alberti's scholarship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7138</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7138</id><created>2012-10-26</created><authors><author><keyname>Anquetil</keyname><forenames>Nicolas</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Laval</keyname><forenames>Jannik</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Legacy Software Restructuring: Analyzing a Concrete Case</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>Proceedings of the 15th European Conference on Software
  Maintenance and Reengineering (CSMR'11) (2011) 279--286</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software re-modularization is an old preoccupation of reverse engineering
research. The advantages of a well structured or modularized system are well
known. Yet after so much time and efforts, the field seems unable to come up
with solutions that make a clear difference in practice. Recently, some
researchers started to question whether some basic assumptions of the field
were not overrated. The main one consists in evaluating the
high-cohesion/low-coupling dogma with metrics of unknown relevance. In this
paper, we study a real structuring case (on the Eclipse platform) to try to
better understand if (some) existing metrics would have helped the software
engineers in the task. Results show that the cohesion and coupling metrics used
in the experiment did not behave as expected and would probably not have helped
the maintainers reach there goal. We also measured another possible
restructuring which is to decrease the number of cyclic dependencies between
modules. Again, the results did not meet expectations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7154</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7154</id><created>2012-10-26</created><authors><author><keyname>Lambrix</keyname><forenames>Patrick</forenames></author><author><keyname>Dragisic</keyname><forenames>Zlatan</forenames></author><author><keyname>Ivanova</keyname><forenames>Valentina</forenames></author></authors><title>Get my pizza right: Repairing missing is-a relations in ALC ontologies
  (extended version)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increased use of ontologies in semantically-enabled applications,
the issue of debugging defects in ontologies has become increasingly important.
These defects can lead to wrong or incomplete results for the applications.
Debugging consists of the phases of detection and repairing. In this paper we
focus on the repairing phase of a particular kind of defects, i.e. the missing
relations in the is-a hierarchy. Previous work has dealt with the case of
taxonomies. In this work we extend the scope to deal with ALC ontologies that
can be represented using acyclic terminologies. We present algorithms and
discuss a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7156</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7156</id><created>2012-10-26</created><updated>2013-03-13</updated><authors><author><keyname>Checco</keyname><forenames>Alessandro</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author></authors><title>Learning-Based Constraint Satisfaction With Sensing Restrictions</title><categories>cs.NI</categories><acm-class>C.2.1; G.2.2; E.1; F.1.1</acm-class><doi>10.1109/JSTSP.2013.2251604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider graph-coloring problems, an important subset of
general constraint satisfaction problems that arise in wireless resource
allocation. We constructively establish the existence of fully decentralized
learning-based algorithms that are able to find a proper coloring even in the
presence of strong sensing restrictions, in particular sensing asymmetry of the
type encountered when hidden terminals are present. Our main analytic
contribution is to establish sufficient conditions on the sensing behaviour to
ensure that the solvers find satisfying assignments with probability one. These
conditions take the form of connectivity requirements on the induced sensing
graph. These requirements are mild, and we demonstrate that they are commonly
satisfied in wireless allocation tasks. We argue that our results are of
considerable practical importance in view of the prevalence of both
communication and sensing restrictions in wireless resource allocation
problems. The class of algorithms analysed here requires no message-passing
whatsoever between wireless devices, and we show that they continue to perform
well even when devices are only able to carry out constrained sensing of the
surrounding radio environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7171</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7171</id><created>2012-10-26</created><authors><author><keyname>Kasa</keyname><forenames>Gentian</forenames></author></authors><title>Hypercomputation: Towards an extension of the classical notion of
  Computability?</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this thesis is to make an analysis of the concept of
Hypercomputation and of some hypermachines. This thesis is separated in three
main parts. We start in the first chapter with an analysis of the concept of
Classical Computability with the Turing Machine and the Church-Turing thesis as
a main reference and afterwards, in the second chapter, we continue with an
analysis of hypercomputation and some hypermachines. Attention is given to the
possible physical realization of these machines and their usefulness. In the
third chapter a superficial introduction to Quantum Computing is made and a
brief analysis to a quantum hypercomputational model, Tien D. Kieu's Adiabatic
Quantum Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7190</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7190</id><created>2012-10-26</created><updated>2015-04-16</updated><authors><author><keyname>Marshall</keyname><forenames>Kyle</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>Subspace Fuzzy Vault</title><categories>cs.IT cs.CR math.IT</categories><comments>to appear in Springer Lecture Notes in Electrical Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy vault is a scheme providing secure authentication based on fuzzy
matching of sets. A major application is the use of biometric features for
authentication, whereby unencrypted storage of these features is not an option
because of security concerns. While there is still ongoing research around the
practical implementation of such schemes, we propose and analyze here an
alternative construction based on subspace codes. This offers some advantages
in terms of security, as an eventual discovery of the key does not provide an
obvious access to the features. Crucial for an efficient implementation are the
computational complexity and the choice of good code parameters. The parameters
depend on the particular application, e.g. the biometric feature to be stored
and the rate one wants to allow for false acceptance. The developed theory is
closely linked to constructions of subspace codes studied in the area of random
network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7253</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7253</id><created>2012-10-26</created><updated>2012-11-04</updated><authors><author><keyname>Perera</keyname><forenames>K. K. K. R.</forenames></author><author><keyname>Mizoguchi</keyname><forenames>Yoshihiro</forenames></author></authors><title>Bipartition of graphs based on the normalized cut and spectral methods</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this paper, we survey results that are associated with
three types of Laplacian matrices:difference, normalized, and signless. We
derive eigenvalue and eigenvector formulaes for paths and cycles using
circulant matrices and present an alternative proof for finding eigenvalues of
the adjacency matrix of paths and cycles using Chebyshev polynomials. Even
though each results is separately well known, we unite them, and provide
uniform proofs in a simple manner. The main objective of this study is to solve
the problem of finding graphs, on which spectral clustering methods and
normalized cuts produce different partitions. First, we derive a formula for a
minimum normalized cut for graph classes such as paths, cycles, complete
graphs, double-trees, cycle cross paths, and some complex graphs like lollipop
graph $LP_{n,m}$, roach type graph $R_{n,k}$, and weighted path $P_{n,k}$.
Next, we provide characteristic polynomials of the normalized Laplacian
matrices ${\mathcal L}(P_{n,k})$ and ${\mathcal L}(R_{n,k})$. Then, we present
counter example graphs based on $R_{n,k}$, on which spectral methods and
normalized cuts produce different clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7269</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7269</id><created>2012-10-26</created><updated>2012-11-16</updated><authors><author><keyname>Groshaus</keyname><forenames>Marina</forenames></author><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author><author><keyname>Terlisky</keyname><forenames>Pablo</forenames></author></authors><title>The star and biclique coloring and choosability problems</title><categories>cs.DM</categories><comments>33 pages, 8 figures</comments><msc-class>05C15, 05C85</msc-class><doi>10.7155/jgaa.00326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A biclique of a graph G is an induced complete bipartite graph. A star of G
is a biclique contained in the closed neighborhood of a vertex. A star
(biclique) k-coloring of G is a k-coloring of G that contains no monochromatic
maximal stars (bicliques). Similarly, for a list assignment L of G, a star
(biclique) L-coloring is an L-coloring of G in which no maximal star (biclique)
is monochromatic. If G admits a star (biclique) L-coloring for every k-list
assignment L, then G is said to be star (biclique) k-choosable. In this article
we study the computational complexity of the star and biclique coloring and
choosability problems. Specifically, we prove that the star (biclique)
k-coloring and k-choosability problems are \Sigma_2^p-complete and
\Pi_3^p-complete for k &gt; 2, respectively, even when the input graph contains no
induced C_4 or K_{k+2}. Then, we study all these problems in some related
classes of graphs, including H-free graphs for every H on three vertices,
graphs with restricted diamonds, split graphs, threshold graphs, and net-free
block graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7282</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7282</id><created>2012-10-26</created><authors><author><keyname>Bishop</keyname><forenames>Robert</forenames></author><author><keyname>Micheletto</keyname><forenames>Ruggero</forenames></author></authors><title>The Hangulphabet: A Descriptive Alphabet</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the Hangulphabet, a new writing system that should prove
useful in a number of contexts. Using the Hangulphabet, a user can instantly
see voicing, manner and place of articulation of any phoneme found in human
language. The Hangulphabet places consonant graphemes on a grid with the x-axis
representing the place of articulation and the y-axis representing manner of
articulation. Each individual grapheme contains radicals from both axes where
the points intersect. The top radical represents manner of articulation where
the bottom represents place of articulation. A horizontal line running through
the middle of the bottom radical represents voicing. For vowels, place of
articulation is located on a grid that represents the position of the tongue in
the mouth. This grid is similar to that of the IPA vowel chart (International
Phonetic Association, 1999). The difference with the Hangulphabet being the
trapezoid representing the vocal apparatus is on a slight tilt. Place of
articulation for a vowel is represented by a breakout figure from the grid.
This system can be used as an alternative to the International Phonetic
Alphabet (IPA) or as a complement to it. Beginning students of linguistics may
find it particularly useful. A Hangulphabet font has been created to facilitate
switching between the Hangulphabet and the IPA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7283</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7283</id><created>2012-10-26</created><authors><author><keyname>Basin</keyname><forenames>David</forenames></author><author><keyname>F&#xfc;rst</keyname><forenames>Andreas</forenames></author><author><keyname>Hoang</keyname><forenames>Thai Son</forenames></author><author><keyname>Miyazaki</keyname><forenames>Kunihiko</forenames></author><author><keyname>Sato</keyname><forenames>Naoto</forenames></author></authors><title>Abstract Data Types in Event-B - An Application of Generic Instantiation</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating formal methods into industrial practice is a challenging task.
Often, different kinds of expertise are required within the same development.
On the one hand, there are domain engineers who have specific knowledge of the
system under development. On the other hand, there are formal methods experts
who have experience in rigorously specifying and reasoning about formal
systems. Coordination between these groups is important for taking advantage of
their expertise. In this paper, we describe our approach of using generic
instantiation to facilitate this coordination. In particular, generic
instantiation enables a separation of concerns between the different parties
involved in developing formal systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7292</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7292</id><created>2012-10-27</created><updated>2012-11-20</updated><authors><author><keyname>Messner</keyname><forenames>Matthias</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Bramas</keyname><forenames>B&#xe9;renger</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Coulaud</keyname><forenames>Olivier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>Optimized M2L Kernels for the Chebyshev Interpolation based Fast
  Multipole Method</title><categories>cs.NA cs.CE cs.MS math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast multipole method (FMM) for asymptotically smooth kernel functions
(1/r, 1/r^4, Gauss and Stokes kernels, radial basis functions, etc.) based on a
Chebyshev interpolation scheme has been introduced in [Fong et al., 2009]. The
method has been extended to oscillatory kernels (e.g., Helmholtz kernel) in
[Messner et al., 2012]. Beside its generality this FMM turns out to be
favorable due to its easy implementation and its high performance based on
intensive use of highly optimized BLAS libraries. However, one of its
bottlenecks is the precomputation of the multiple-to-local (M2L) operator, and
its higher number of floating point operations (flops) compared to other FMM
formulations. Here, we present several optimizations for that operator, which
is known to be the costliest FMM operator. The most efficient ones do not only
reduce the precomputation time by a factor up to 340 but they also speed up the
matrix-vector product. We conclude with comparisons and numerical validations
of all presented optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7295</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7295</id><created>2012-10-27</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author><author><keyname>Abed</keyname><forenames>Eyad H.</forenames></author></authors><title>Analysis and Control of Period-Doubling Bifurcation in Buck Converters
  Using Harmonic Balance</title><categories>cs.SY math.DS nlin.CD</categories><comments>Published in the International Journal of Latin American Applied
  Research, 31(3), pp. 149-156, Jul. 2001, Special theme issue: Bifurcation
  Control: Methodologies and Applications, In Honor of the 65th Birthday of
  Professor Leon O. Chua</comments><journal-ref>International Journal of Latin American Applied Research, 31(3),
  pp. 149-156, Jul. 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Period doubling bifurcation in buck converters is studied by using the
harmonic balance method. A simple dynamic model of a buck converter in
continuous conduction mode under voltage mode or current mode control is
derived. This model consists of the feedback connection of a linear system and
a nonlinear one. An exact harmonic balance analysis is used to obtain a
necessary and sufficient condition for a period doubling bifurcation to occur.
If such a bifurcation occurs, the analysis also provides information on its
exact location. Using the condition for bifurcation, a feedforward control is
designed to eliminate the period doubling bifurcation. This results in a wider
range of allowed source voltage, and also in improved line regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7325</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7325</id><created>2012-10-27</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Aulchenko</keyname><forenames>Yurii</forenames><affiliation>Institute of Cytology and Genetics SD RAS</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Solving Sequences of Generalized Least-Squares Problems on
  Multi-threaded Architectures</title><categories>cs.MS cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized linear mixed-effects models in the context of genome-wide
association studies (GWAS) represent a formidable computational challenge: the
solution of millions of correlated generalized least-squares problems, and the
processing of terabytes of data. We present high performance in-core and
out-of-core shared-memory algorithms for GWAS: By taking advantage of
domain-specific knowledge, exploiting multi-core parallelism, and handling data
efficiently, our algorithms attain unequalled performance. When compared to
GenABEL, one of the most widely used libraries for GWAS, on a 12-core processor
we obtain 50-fold speedups. As a consequence, our routines enable genome
studies of unprecedented size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7335</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7335</id><created>2012-10-27</created><updated>2014-06-23</updated><authors><author><keyname>Bettencourt</keyname><forenames>Lu&#xed;s M. A.</forenames></author><author><keyname>Samaniego</keyname><forenames>Horacio</forenames></author><author><keyname>Youn</keyname><forenames>HyeJin</forenames></author></authors><title>Professional diversity and the productivity of cities</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>Press embargo in place until publication</comments><journal-ref>Scientific Reports (2014) 4: 5393</journal-ref><doi>10.1038/srep05393</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The relationships between diversity, productivity and scale determine much of
the structure and robustness of complex biological and social systems. While
arguments for the link between specialization and productivity are common,
diversity has often been invoked as a hedging strategy, allowing systems to
evolve in response to environmental change. Despite their general appeal, these
arguments have not typically produced quantitative predictions for optimal
levels of functional diversity consistent with observations. One important
reason why these relationships have resisted formalization is the idiosyncratic
nature of diversity measures, which depend on given classification schemes.
Here, we address these issues by analyzing the statistics of professions in
cities and show how their probability distribution takes a universal
scale-invariant form, common to all cities, obtained in the limit of infinite
resolution of given taxonomies. We propose a model that generates the form and
parameters of this distribution via the introduction of new occupations at a
rate leading to individual specialization subject to the preservation of access
to overall function via their ego social networks. This perspective unifies
ideas about the importance of network structure in ecology and of innovation as
a recombinatory process with economic concepts of productivity gains obtained
through the division and coordination of labor, stimulated by scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7341</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7341</id><created>2012-10-27</created><updated>2013-02-27</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author></authors><title>Subset Codes for Packet Networks</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><msc-class>94B60</msc-class><journal-ref>IEEE Commun. Lett., vol. 17, no. 4, pp. 729-732, Apr. 2013</journal-ref><doi>10.1109/LCOMM.2013.022713.122397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a coding-theoretic framework for message
transmission over packet-switched networks. Network is modeled as a channel
which can induce packet errors, deletions, insertions, and out of order
delivery of packets. The proposed approach can be viewed as an extension of the
one introduced by Koetter and Kschischang for networks based on random linear
network coding. Namely, while their framework is based on subspace codes and
designed for networks in which network nodes perform random linear combining of
the packets, ours is based on the so-called subset codes, and is designed for
networks employing routing in network nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7349</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7349</id><created>2012-10-27</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Edwards</keyname><forenames>Katherine</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author></authors><title>Edge-colouring seven-regular planar graphs</title><categories>cs.DM math.CO</categories><comments>23 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.1176</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conjecture due to the fourth author states that every $d$-regular planar
multigraph can be $d$-edge-coloured, provided that for every odd set $X$ of
vertices, there are at least $d$ edges between $X$ and its complement. For $d =
3$ this is the four-colour theorem, and the conjecture has been proved for all
$d\le 8$, by various authors. In particular, two of us proved it when $d=7$;
and then three of us proved it when $d=8$. The methods used for the latter give
a proof in the $d=7$ case that is simpler than the original, and we present it
here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7350</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7350</id><created>2012-10-27</created><authors><author><keyname>Mishne</keyname><forenames>Gilad</forenames></author><author><keyname>Dalton</keyname><forenames>Jeff</forenames></author><author><keyname>Li</keyname><forenames>Zhenghua</forenames></author><author><keyname>Sharma</keyname><forenames>Aneesh</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author></authors><title>Fast Data in the Era of Big Data: Twitter's Real-Time Related Query
  Suggestion Architecture</title><categories>cs.IR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the architecture behind Twitter's real-time related query
suggestion and spelling correction service. Although these tasks have received
much attention in the web search literature, the Twitter context introduces a
real-time &quot;twist&quot;: after significant breaking news events, we aim to provide
relevant results within minutes. This paper provides a case study illustrating
the challenges of real-time data processing in the era of &quot;big data&quot;. We tell
the story of how our system was built twice: our first implementation was built
on a typical Hadoop-based analytics stack, but was later replaced because it
did not meet the latency requirements necessary to generate meaningful
real-time results. The second implementation, which is the system deployed in
production, is a custom in-memory processing engine specifically designed for
the task. This experience taught us that the current typical usage of Hadoop as
a &quot;big data&quot; platform, while great for experimentation, is not well suited to
low-latency processing, and points the way to future work on data analytics
platforms that can handle &quot;big&quot; as well as &quot;fast&quot; data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7362</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7362</id><created>2012-10-27</created><updated>2012-11-07</updated><authors><author><keyname>Bagon</keyname><forenames>Shai</forenames></author></authors><title>Discrete Energy Minimization, beyond Submodularity: Applications and
  Approximations</title><categories>cs.CV cs.LG math.OC stat.ML</categories><comments>Doctoral dissertation, Weizmann Institute of Science. Under the
  supervision of Prof. Michal Irani and Dr Meirav Galun Corrected typos.
  Citation added</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this thesis I explore challenging discrete energy minimization problems
that arise mainly in the context of computer vision tasks. This work motivates
the use of such &quot;hard-to-optimize&quot; non-submodular functionals, and proposes
methods and algorithms to cope with the NP-hardness of their optimization.
Consequently, this thesis revolves around two axes: applications and
approximations. The applications axis motivates the use of such
&quot;hard-to-optimize&quot; energies by introducing new tasks. As the energies become
less constrained and structured one gains more expressive power for the
objective function achieving more accurate models. Results show how
challenging, hard-to-optimize, energies are more adequate for certain computer
vision applications. To overcome the resulting challenging optimization tasks
the second axis of this thesis proposes approximation algorithms to cope with
the NP-hardness of the optimization. Experiments show that these new methods
yield good results for representative challenging problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7375</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7375</id><created>2012-10-27</created><updated>2014-06-25</updated><authors><author><keyname>Chandrasekhar</keyname><forenames>Arun G.</forenames></author><author><keyname>Jackson</keyname><forenames>Matthew O.</forenames></author></authors><title>Tractable and Consistent Random Graph Models</title><categories>physics.soc-ph cs.SI</categories><comments>60 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a general class of network formation models, Statistical
Exponential Random Graph Models (SERGMs), that nest standard exponential random
graph models (ERGMs) as a special case. We provide the first general results on
when these models' (including ERGMs) parameters estimated from the observation
of a single network are consistent (i.e., become accurate as the number of
nodes grows). Next, addressing the problem that standard techniques of
estimating ERGMs have been shown to have exponentially slow mixing times for
many specifications, we show that by reformulating network formation as a
distribution over the space of sufficient statistics instead of the space of
networks, the size of the space of estimation can be greatly reduced, making
estimation practical and easy. We also develop a related, but distinct, class
of models that we call subgraph generation models (SUGMs) that are useful for
modeling sparse networks and whose parameter estimates are also directly and
easily estimable, consistent, and asymptotically normally distributed. Finally,
we show how choice-based (strategic) network formation models can be written as
SERGMs and SUGMs, and apply our models and techniques to network data from
rural Indian villages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7385</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7385</id><created>2012-10-27</created><authors><author><keyname>Khalid</keyname><forenames>Omer</forenames></author><author><keyname>Sheikh</keyname><forenames>Arsalaan</forenames></author><author><keyname>Copy</keyname><forenames>Brice</forenames></author></authors><title>Optimizing Infrastructures for Testing Using Virtualization</title><categories>cs.DC cs.NI</categories><comments>13th International Conference on Accelerator and Large Experimental
  Physics Control Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization technology and cloud computing have brought a paradigm shift
in the way we utilize, deploy and manage computer resources. They allow fast
deployment of multiple operating system as containers on physical ma- chines
which can be either discarded after use or check- pointed for later
re-deployment. At European Organization for Nuclear Research (CERN), we have
been using virtualization technology to quickly setup virtual machines for our
developers with preconfigured software to enable them to quickly test/deploy a
new version of a software patch for a given application. This paper reports
both on the techniques that have been used to setup a private cloud on a
commodity hardware and also presents the optimization techniques we used to
remove deployment specific performance bottlenecks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7397</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7397</id><created>2012-10-28</created><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author><author><keyname>Lee</keyname><forenames>Tong H.</forenames></author></authors><title>Optimal Sensor Placement for Target Localization and Tracking in 2D and
  3D</title><categories>math.OC cs.SY</categories><doi>10.1080/00207179.2013.792606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analytically characterizes optimal sensor placements for target
localization and tracking in 2D and 3D. Three types of sensors are considered:
bearing-only, range-only, and received-signal-strength. The optimal placement
problems of the three sensor types are formulated as an identical parameter
optimization problem and consequently analyzed in a unified framework. Recently
developed frame theory is applied to the optimality analysis. We prove
necessary and sufficient conditions for optimal placements in 2D and 3D. A
number of important analytical properties of optimal placements are further
explored. In order to verify the analytical analysis, we present a gradient
control law that can numerically construct generic optimal placements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7399</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7399</id><created>2012-10-28</created><authors><author><keyname>nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>One-Step Quantized Network Coding for Near Sparse Gaussian Messages</title><categories>cs.IT math.IT</categories><comments>Submitted for IEEE 2013 International Conference on Acoustics, Speech
  and Signal Processing (ICASSP13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, mathematical bases for non-adaptive joint source network
coding of correlated messages in a Bayesian scenario are studied. Specifically,
we introduce one-step Quantized Network Coding (QNC), which is a hybrid
combination of network coding and packet forwarding for transmission. Motivated
by the work on Bayesian compressed sensing, we derive theoretical guarantees on
robust recovery in a one-step QNC scenario. Our mathematical derivations for
Gaussian messages express the opportunity of distributed compression by using
one-step QNC, as a simplified version of QNC scenario. Our simulation results
show an improvement in terms of quality-delay performance over routing based
packet forwarding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7401</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7401</id><created>2012-10-28</created><authors><author><keyname>Yi</keyname><forenames>Huiyue</forenames></author></authors><title>Joint Doppler frequency shift compensation and data detection method
  using 2-D unitary ESPRIT algorithm for SIMO-OFDM railway communication
  systems</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a joint Doppler frequency shift compensation and
data detection method using 2-D unitary ESPRIT algorithm for SIMO-OFDM railway
communication systems over fast time-varying sparse multipath channels. By
creating the spatio-temporal array data matrix utilizing the ISI-free part of
the CP (cyclic prefix), we first propose a novel algorithm for obtaining
auto-paired joint DOA and Doppler frequency shift estimates of all paths via
2-D unitary ESPRIT algorithm. Thereafter, based on the obtained estimates, a
joint Doppler frequency shift compensation and data detection method is
developed. This method consists of three parts: (a) the received signal is
spatially filtered to get the signal corresponding to each path, and the signal
corresponding to each path is compensated for the Doppler frequency shift in
time domain, (b) the Doppler frequency shift-compensated signals of all paths
are summed together, and (c) the desired information is detected by performing
FFT on the summed signal after excluding the CP. Moreover, we prove that the
channel matrix becomes time-invariant after Doppler frequency shift
compensation and the ICI is effectively avoided. Finally, simulation results
are presented to demonstrate the performance of the proposed method and compare
it with the conventional method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7403</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7403</id><created>2012-10-28</created><authors><author><keyname>Bhavsar</keyname><forenames>Arnav</forenames></author></authors><title>Resolution Enhancement of Range Images via Color-Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a method for super-resolution of range images. Our approach
leverages the interpretation of LR image as sparse samples on the HR grid.
Based on this interpretation, we demonstrate that our recently reported
approach, which reconstructs dense range images from sparse range data by
exploiting a registered colour image, can be applied for the task of resolution
enhancement of range images. Our method only uses a single colour image in
addition to the range observation in the super-resolution process. Using the
proposed approach, we demonstrate super-resolution results for large factors
(e.g. 4) with good localization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7410</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7410</id><created>2012-10-28</created><updated>2012-10-30</updated><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Peng</keyname><forenames>Kemao</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author><author><keyname>Lee</keyname><forenames>Tong H.</forenames></author></authors><title>Distributed Control of Angle-constrained Circular Formations using
  Bearing-only Measurements</title><categories>cs.SY math.OC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies distributed formation control of multiple agents in the
plane using bearing-only measurements. It is assumed that each agent only
measures the local bearings of their neighbor agents. The target formation
considered in this paper is a circular formation, where each agent has exactly
two neighbors. In the target formation, the angle subtended at each agent by
their two neighbors is specified. We propose a distributed control law that
stabilizes angle-constrained target formations merely using local bearing
measurements. The stability of the target formation is analyzed based on
Lyapunov approaches. We present a unified proof to show that our control law
not only can ensure local exponential stability but also can give local
finite-time stability. The exponential or finite-time stability can be easily
switched by tuning a parameter in the control law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7417</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7417</id><created>2012-10-28</created><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author><author><keyname>Oskouei</keyname><forenames>Hamid R. Dalili</forenames></author></authors><title>Cryptanalysis of a Public-key Cryptosystem Using Lattice Basis Reduction
  Algorithm</title><categories>cs.CR cs.DM</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 1, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a new attack against Hwang et al.'s cryptosystem. This
cryptosystem uses a super-increasing sequence as private key and the authors
investigate a new algorithm called permutation combination algorithm to enhance
density of knapsack to avoid the low-density attack. Sattar J. Aboud [Aboud j.
Sattar, &quot;An improved knapsack public key cryptography system&quot;, International
Journal of Internet Technology and Secured Transactions, Vol.3 (3), pp.310-319,
2011] used Shamir's attack on the basic Merkle-Hellman cryptosystem to break
this cryptosystem.
  In this paper, we introduce a direct attack against Hwang et al.'s
cryptosystem based on Lattice basis reduction algorithms. By computing
complexity of propose attack, we show that unlike Aboud's cryptanalysis, our
cryptanalysis is more efficient and practicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7420</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7420</id><created>2012-10-28</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Majumdar</keyname><forenames>Anirudha</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>Complexity of Ten Decision Problems in Continuous Time Dynamical Systems</title><categories>math.OC cs.CC cs.SY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for continuous time dynamical systems described by polynomial
differential equations of modest degree (typically equal to three), the
following decision problems which arise in numerous areas of systems and
control theory cannot have a polynomial time (or even pseudo-polynomial time)
algorithm unless P=NP: local attractivity of an equilibrium point, stability of
an equilibrium point in the sense of Lyapunov, boundedness of trajectories,
convergence of all trajectories in a ball to a given equilibrium point,
existence of a quadratic Lyapunov function, invariance of a ball, invariance of
a quartic semialgebraic set under linear dynamics, local collision avoidance,
and existence of a stabilizing control law. We also extend our earlier
NP-hardness proof of testing local asymptotic stability for polynomial vector
fields to the case of trigonometric differential equations of degree four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7422</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7422</id><created>2012-10-28</created><authors><author><keyname>Pintea</keyname><forenames>Camelia-M.</forenames></author><author><keyname>Pop</keyname><forenames>Petrica C.</forenames></author></authors><title>Sensor networks security based on sensitive robots agents. A conceptual
  model</title><categories>cs.MA</categories><comments>5 pages</comments><msc-class>68T05, 91B69</msc-class><journal-ref>Advances in Intelligent Systems and Computing, 189:47-56, 2013,
  Springer</journal-ref><doi>10.1007/978-3-642-33018-6_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent systems are currently applied to solve complex problems. The
security of networks is an eloquent example of a complex and difficult problem.
A new model-concept Hybrid Sensitive Robot Metaheuristic for Intrusion
Detection is introduced in the current paper. The proposed technique could be
used with machine learning based intrusion detection techniques. The new model
uses the reaction of virtual sensitive robots to different stigmergic variables
in order to keep the tracks of the intruders when securing a sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7427</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7427</id><created>2012-10-28</created><authors><author><keyname>Rubensson</keyname><forenames>Emanuel H.</forenames></author><author><keyname>Rudberg</keyname><forenames>Elias</forenames></author></authors><title>Chunks and Tasks: a programming model for parallelization of dynamic
  algorithms</title><categories>cs.DC cs.SE</categories><comments>This manuscript was submitted to Parallel Computing (Elsevier) for
  the special issue devoted to the conference Parallel Matrix Algorithms and
  Applications (PMAA 2012). A presentation of this work was given at PMAA 2012
  on June 29, 2012</comments><acm-class>D.1.3; D.2.2</acm-class><journal-ref>Parallel Comput. 40 (2014) 328-343</journal-ref><doi>10.1016/j.parco.2013.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Chunks and Tasks, a parallel programming model built on
abstractions for both data and work. The application programmer specifies how
data and work can be split into smaller pieces, chunks and tasks, respectively.
The Chunks and Tasks library maps the chunks and tasks to physical resources.
In this way we seek to combine user friendliness with high performance. An
application programmer can express a parallel algorithm using a few simple
building blocks, defining data and work objects and their relationships. No
explicit communication calls are needed; the distribution of both work and data
is handled by the Chunks and Tasks library. This makes efficient implementation
of complex applications that require dynamic distribution of work and data
easier. At the same time, Chunks and Tasks imposes restrictions on data access
and task dependencies that facilitates the development of high performance
parallel back ends. We discuss the fundamental abstractions underlying the
programming model, as well as performance and fault resilience considerations.
We also present a pilot C++ library implementation for clusters of multicore
machines and demonstrate its performance for sparse blocked matrix-matrix
multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7443</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7443</id><created>2012-10-28</created><authors><author><keyname>Arkoudogiannis</keyname><forenames>Konstantinos S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Christos E.</forenames></author><author><keyname>Koutsouvelis</keyname><forenames>Konstantinos V.</forenames></author></authors><title>A Better Understanding of the Performance of Rate-1/2 Binary Turbo Codes
  that Use Odd-Even Interleavers</title><categories>cs.IT math.IT</categories><journal-ref>IEEE CSNDSP 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effects of the odd-even constraint - as an interleaver design criterion -
on the performance of rate-1/2 binary turbo codes are revisited. According to
the current understanding, its adoption is favored because it makes the
information bits be uniformly protected, each one by its own parity bit. In
this paper, we provide instances that contradict this point of view suggesting
for a different explanation of the constraint's behavior, in terms of distance
spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7461</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7461</id><created>2012-10-28</created><authors><author><keyname>de Souza</keyname><forenames>C&#xe9;sar Roberto</forenames></author><author><keyname>Pizzolato</keyname><forenames>Ednaldo Brigante</forenames></author><author><keyname>Anjo</keyname><forenames>Mauro dos Santos</forenames></author></authors><title>Recognizing Static Signs from the Brazilian Sign Language: Comparing
  Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines
  and Artificial Neural Networks</title><categories>cs.CV cs.LG stat.ML</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore and detail our experiments in a
high-dimensionality, multi-class image classification problem often found in
the automatic recognition of Sign Languages. Here, our efforts are directed
towards comparing the characteristics, advantages and drawbacks of creating and
training Support Vector Machines disposed in a Directed Acyclic Graph and
Artificial Neural Networks to classify signs from the Brazilian Sign Language
(LIBRAS). We explore how the different heuristics, hyperparameters and
multi-class decision schemes affect the performance, efficiency and ease of use
for each classifier. We provide hyperparameter surface maps capturing accuracy
and efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects of
heuristics when training ANNs with Resilient Backpropagation. We report
statistically significant results using Cohen's Kappa statistic for contingency
tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7463</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7463</id><created>2012-10-28</created><authors><author><keyname>de Souza</keyname><forenames>C&#xe9;sar Roberto</forenames></author></authors><title>A Tutorial on Principal Component Analysis with the Accord.NET Framework</title><categories>cs.SE stat.CO</categories><comments>35 pages, Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document aims to clarify frequent questions on using the Accord.NET
Framework to perform statistical analyses. Here, we reproduce all steps of the
famous Lindsay's Tutorial on Principal Component Analysis, in an attempt to
give the reader a complete hands-on overview on the framework's basics while
also discussing some of the results and sources of divergence between the
results generated by Accord.NET and by other software packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7467</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7467</id><created>2012-10-28</created><authors><author><keyname>Ghiasian</keyname><forenames>Ali</forenames></author><author><keyname>Omoomi</keyname><forenames>Behnaz</forenames></author><author><keyname>Saidi</keyname><forenames>Hossein</forenames></author></authors><title>A generalization of line graphs via link scheduling in wireless networks</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In single channel wireless networks, concurrent transmission at different
links may interfere with each other. To improve system throughput, a scheduling
algorithm is necessary to choose a subset of links at each time slot for data
trasmission. Throughput optimal link scheduling discipline in such a wireless
network is generally an NP-hard problem. In this paper, we develop a poylnomial
time algorithm for link scheduling problem provided that network conflict graph
is line multigraph. (i.e. line graph for which its root graph is multigraph).
This result can be a guideline for network designers to plan the topology of a
stationary wireless network such that the required conditions hold and then the
throughput optimal algorithm can be run in a much less time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7468</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7468</id><created>2012-10-28</created><updated>2013-07-24</updated><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Link Scheduling in Amplify-and-Forward Cooperative Wireless Networks</title><categories>cs.NI</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we are concerned with the problem of link scheduling for
throughput maximization in wireless networks that employ a cooperative amplify
and forward (AF) protocol. To address this problem first we define the
signal-to-interference plus noise ratio (SINR) expression for the complete
cooperative AF-based transmission. Next, we formulate the problem of link
scheduling as a mixed integer linear program (MILP) that uses as a constraint
the developed SINR expression. The proposed formulation is motivated by the
observation that the aggregate interference that affects a single cooperative
transmission can be decomposed into two separate SINR constraints. Results for
the optimal solution and a polynomial time approximation algorithm are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7473</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7473</id><created>2012-10-28</created><authors><author><keyname>Ilic</keyname><forenames>Velimir M.</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir S.</forenames></author></authors><title>Comments on &quot;Nonextensive Entropies derived from Form Invariance of
  Pseudoadditivity&quot;</title><categories>cs.IT math-ph math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Suyari has defined nonextensive information content measure with
unique class of functions which satisfies certain set of axioms. Nonextensive
entropy is then defined as the appropriate expectation value of nonextensive
information content [H. Suyari, Phys. Rev E 65 066118 (2002)]. In this comment
we show that the class of functions determined by Suyari's axioms is actually
wider than the one given by Suyari and we determine the class. Particularly, an
information content corresponding to Havrda-Charvat entropy satisfies Suyari's
axioms and does not belong to the class given by Suyari but belongs to our
class. Moreover, some of the conditions from Suyari's set of axioms are
redundant, and some of them can be replaced with more intuitive weaker ones. We
give a modification of Suyari's axiomatic system with these weaker assumptions
and define the corresponding information content measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7482</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7482</id><created>2012-10-28</created><authors><author><keyname>Ebrahim</keyname><forenames>Nader Ale</forenames></author><author><keyname>Ahmed</keyname><forenames>Shamsuddin</forenames></author><author><keyname>Taha</keyname><forenames>Zahari</forenames></author></authors><title>Modified Stage-Gate: A Conceptual Model of Virtual Product Development
  Process</title><categories>cs.OH</categories><comments>24 pages</comments><msc-class>90B50, 90C29, 90C31, 91A35, 91B06</msc-class><acm-class>D.2.9; K.1</acm-class><journal-ref>African Journal of Marketing Management, 1(9) (2009) 211-219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today s dynamic marketplace, manufacturing companies are under strong
pressure to introduce new products for long-term survival with their
competitors. Nevertheless, every company cannot cope up progressively or
immediately with the market requirements due to knowledge dynamics being
experienced in the competitive milieu. Increased competition and reduced
product life cycles put force upon companies to develop new products faster. In
response to these pressing needs, there should be some new approach compatible
in flexible circumstances. This paper presents a solution based on the popular
Stage-Gate system, which is closely linked with virtual team approach. Virtual
teams can provide a platform to advance the knowledge-base in a company and
thus to reduce time-to-market. This article introduces conceptual product
development architecture under a virtual team umbrella. The paper describes all
the major aspects of new product development (NPD), NPD process and its
relationship with virtual teams, Stage-Gate system finally presents a modified
Stage-Gate system to cope up with the changing needs. It also provides the
guidelines for the successful implementation of virtual teams in new product
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7493</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7493</id><created>2012-10-28</created><authors><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Koupparis</keyname><forenames>Charalambos</forenames></author></authors><title>Non-commutative Digital Signatures</title><categories>cs.CR math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work is to survey several digital signatures proposed
in the last decade using non-commutative groups and rings and propose a digital
signature using non-commutative groups and analyze its security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7495</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7495</id><created>2012-10-28</created><updated>2016-02-27</updated><authors><author><keyname>Mizraji</keyname><forenames>Eduardo</forenames></author></authors><title>Illustrating a neural model of logic computations: The case of Sherlock
  Holmes' old maxim</title><categories>q-bio.NC cs.AI</categories><comments>Corrected version with new references</comments><msc-class>92B20, 68T05, 68T37</msc-class><journal-ref>THEORIA 31/1 (2016): 7-25</journal-ref><doi>10.1387/theoria.13959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural languages can express some logical propositions that humans are able
to understand. We illustrate this fact with a famous text that Conan Doyle
attributed to Holmes: 'It is an old maxim of mine that when you have excluded
the impossible, whatever remains, however improbable, must be the truth'. This
is a subtle logical statement usually felt as an evident truth. The problem we
are trying to solve is the cognitive reason for such a feeling. We postulate
here that we accept Holmes' maxim as true because our adult brains are equipped
with neural modules that naturally perform modal logical computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7498</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7498</id><created>2012-10-28</created><updated>2013-04-11</updated><authors><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Percolation on interacting, antagonistic networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>12 pages, 4 figures</comments><journal-ref>J. Stat. Mech. (2013) P05005</journal-ref><doi>10.1088/1742-5468/2013/05/P05005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, new results on percolation of interdependent networks have shown
that the percolation transition can be first order. In this paper we show that,
when considering antagonistic interactions between interacting networks, the
percolation process might present a bistability of the equilibrium solution. To
this end, we introduce antagonistic interactions for which the functionality,
or activity, of a node in a network is incompatible with the functionality, of
the linked nodes in the other interacting networks. In particular, we study the
percolation transition in two interacting networks with purely antagonistic
interaction and different topology. For two antagonistic Poisson networks of
different average degree we found a large region in the phase diagram in which
there is a bistability of the steady state solutions of the percolation
process, i.e. we can find that either one of the two networks might percolate.
For two antagonistic scale-free networks we found that there is a region in the
phase diagram in which, despite the antagonistic interactions, both networks
are percolating. Finally we characterize the rich phase diagram of the
percolation problems on two antagonistic networks, the first one of the two
being a Poisson network and the second one being a scale-free network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7506</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7506</id><created>2012-10-28</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Convolutional Compressed Sensing Using Deterministic Sequences</title><categories>cs.IT cs.MM math.IT</categories><comments>A major overhaul of the withdrawn paper Orthogonal symmetric Toeplitz
  matrices for compressed sensing: Statistical isometry property</comments><doi>10.1109/TSP.2012.2229994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new class of circulant matrices built from deterministic
sequences is proposed for convolution-based compressed sensing (CS). In
contrast to random convolution, the coefficients of the underlying filter are
given by the discrete Fourier transform of a deterministic sequence with good
autocorrelation. Both uniform recovery and non-uniform recovery of sparse
signals are investigated, based on the coherence parameter of the proposed
sensing matrices. Many examples of the sequences are investigated, particularly
the Frank-Zadoff-Chu (FZC) sequence, the \textit{m}-sequence and the Golay
sequence. A salient feature of the proposed sensing matrices is that they can
not only handle sparse signals in the time domain, but also those in the
frequency and/or or discrete-cosine transform (DCT) domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7515</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7515</id><created>2012-10-28</created><authors><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Wolf</keyname><forenames>Jack K.</forenames></author></authors><title>Rewriting Codes for Flash Memories</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:0905.1512</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flash memory is a non-volatile computer memory comprising blocks of cells,
wherein each cell can take on q different values or levels. While increasing
the cell level is easy, reducing the level of a cell can be accomplished only
by erasing an entire block. Since block erasures are highly undesirable, coding
schemes - known as floating codes (or flash codes) and buffer codes - have been
designed in order to maximize the number of times that information stored in a
flash memory can be written (and re-written) prior to incurring a block
erasure.
  An (n,k,t)q flash code C is a coding scheme for storing k information bits in
$n$ cells in such a way that any sequence of up to t writes can be accommodated
without a block erasure. The total number of available level transitions in n
cells is n(q-1), and the write deficiency of C, defined as \delta(C) =
n(q-1)-t, is a measure of how close the code comes to perfectly utilizing all
these transitions. In this paper, we show a construction of flash codes with
write deficiency O(qk\log k) if q \geq \log_2k, and at most O(k\log^2 k)
otherwise.
  An (n,r,\ell,t)q buffer code is a coding scheme for storing a buffer of r
\ell-ary symbols such that for any sequence of t symbols it is possible to
successfully decode the last r symbols that were written. We improve upon a
previous upper bound on the maximum number of writes t in the case where there
is a single cell to store the buffer. Then, we show how to improve a
construction by Jiang et al. that uses multiple cells, where n\geq 2r.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7533</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7533</id><created>2012-10-28</created><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Joint Viterbi Decoding and Decision Feedback Equalization for Monobit
  Digital Receivers</title><categories>cs.IT math.IT</categories><comments>6 pages, 8 figures, submitted to IEEE ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ultra-wideband (UWB) communication systems with impulse radio (IR)
modulation, the bandwidth is usually 1GHz or more. To process the received
signal digitally, high sampling rate analog-digital-converters (ADC) are
required. Due to the high complexity and large power consumption, monobit ADC
is appropriate. The optimal monobit receiver has been derived. But it is not
efficient to combat intersymbol interference (ISI). Decision feedback
equalization (DFE) is an effect way dealing with ISI. In this paper, we
proposed a algorithm that combines Viterbi decoding and DFE together for
monobit receivers. In this way, we suppress the impact of ISI effectively, thus
improving the bit error rate (BER) performance. By state expansion, we achieve
better performance. The simulation results show that the algorithm has about
1dB SNR gain compared to separate demodulation and decoding method and 1dB loss
compared to the BER performance in the channel without ISI. Compare to the full
resolution detection in fading channel without ISI, it has 3dB SNR loss after
state expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7539</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7539</id><created>2012-10-28</created><authors><author><keyname>Ganapathy</keyname><forenames>Harish</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Dimitrov</keyname><forenames>Nedialko B.</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author></authors><title>Feedback Allocation For OFDMA Systems With Slow Frequency-domain
  Scheduling</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2012.2218243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of allocating limited feedback resources across multiple
users in an orthogonal-frequency-division-multiple-access downlink system with
slow frequency-domain scheduling. Many flavors of slow frequency-domain
scheduling (e.g., persistent scheduling, semi-persistent scheduling), that
adapt user-sub-band assignments on a slower time-scale, are being considered in
standards such as 3GPP Long-Term Evolution. In this paper, we develop a
feedback allocation algorithm that operates in conjunction with any arbitrary
slow frequency-domain scheduler with the goal of improving the throughput of
the system. Given a user-sub-band assignment chosen by the scheduler, the
feedback allocation algorithm involves solving a weighted sum-rate maximization
at each (slow) scheduling instant. We first develop an optimal
dynamic-programming-based algorithm to solve the feedback allocation problem
with pseudo-polynomial complexity in the number of users and in the total
feedback bit budget. We then propose two approximation algorithms with
complexity further reduced, for scenarios where the problem exhibits additional
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7543</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7543</id><created>2012-10-28</created><authors><author><keyname>Ganapathy</keyname><forenames>Harish</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Exploiting Sparse Dynamics For Bandwidth Reduction In Cooperative
  Sensing Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Signal Processing</comments><doi>10.1109/TSP.2013.2260336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a significant interest in developing cooperative
sensing systems for certain types of wireless applications. In such systems, a
group of sensing nodes periodically collect measurements about the signals
being observed in the given geographical region and transmit these measurements
to a central node, which in turn processes this information to recover the
signals. For example, in cognitive radio networks, the signals of interest are
those generated by the primary transmitters and the sensing nodes are the
secondary users. In such networks, it is critically important to be able to
reliably determine the presence or absence of primary transmitters in order to
avoid causing interference. The standard approach to transmit these
measurements from sensor the nodes to the fusion center has been to use
orthogonal channels. Such an approach quickly places a burden on the
control-channel-capacity of the network that would scale linearly in the number
of cooperating sensing nodes. In this paper, we show that as long as one
condition is satisfied: the dynamics of the observed signals are sparse, i.e.,
the observed signals do not change their values very rapidly in relation to the
time-scale at which the measurements are collected, we can significantly reduce
the control bandwidth of the system while achieving the full (linear) bandwidth
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7559</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7559</id><created>2012-10-29</created><updated>2014-11-13</updated><authors><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Tensor decompositions for learning latent variable models</title><categories>cs.LG math.NA stat.ML</categories><journal-ref>Journal of Machine Learning Research, 15(Aug):2773-2832, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a computationally and statistically efficient parameter
estimation method for a wide class of latent variable models---including
Gaussian mixture models, hidden Markov models, and latent Dirichlet
allocation---which exploits a certain tensor structure in their low-order
observable moments (typically, of second- and third-order). Specifically,
parameter estimation is reduced to the problem of extracting a certain
(orthogonal) decomposition of a symmetric tensor derived from the moments; this
decomposition can be viewed as a natural generalization of the singular value
decomposition for matrices. Although tensor decompositions are generally
intractable to compute, the decomposition of these specially structured tensors
can be efficiently obtained by a variety of approaches, including power
iterations and maximization approaches (similar to the case of matrices). A
detailed analysis of a robust tensor power method is provided, establishing an
analogue of Wedin's perturbation theorem for the singular vectors of matrices.
This implies a robust and computationally tractable estimation approach for
several popular latent variable models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7591</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7591</id><created>2012-10-29</created><authors><author><keyname>Kochkarev</keyname><forenames>B. S.</forenames></author></authors><title>About one class polynomial problems with not polynomial certificates</title><categories>math.GM cs.DM</categories><msc-class>05C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build a class of polynomial problems with not polynomial certificates. The
parameter concerning which are defined efficiency of corresponding algorithms
is the number $n$ of elements of the set has used at construction of
combinatory objects (families of subsets) with necessary properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7599</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7599</id><created>2012-10-29</created><updated>2014-09-27</updated><authors><author><keyname>Zubrinic</keyname><forenames>Krunoslav</forenames></author><author><keyname>Kalpic</keyname><forenames>Damir</forenames></author><author><keyname>Milicevic</keyname><forenames>Mario</forenames></author></authors><title>The automatic creation of concept maps from documents written using
  morphologically rich languages</title><categories>cs.IR cs.AI cs.CL</categories><comments>ISSN 0957-4174</comments><journal-ref>Expert Systems with Applications, Volume 39, Issue 16, 2012, Pages
  12709-12718</journal-ref><doi>10.1016/j.eswa.2012.04.065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept map is a graphical tool for representing knowledge. They have been
used in many different areas, including education, knowledge management,
business and intelligence. Constructing of concept maps manually can be a
complex task; an unskilled person may encounter difficulties in determining and
positioning concepts relevant to the problem area. An application that
recommends concept candidates and their position in a concept map can
significantly help the user in that situation. This paper gives an overview of
different approaches to automatic and semi-automatic creation of concept maps
from textual and non-textual sources. The concept map mining process is
defined, and one method suitable for the creation of concept maps from
unstructured textual sources in highly inflected languages such as the Croatian
language is described in detail. Proposed method uses statistical and data
mining techniques enriched with linguistic tools. With minor adjustments, that
method can also be used for concept map mining from textual sources in other
morphologically rich languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7600</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7600</id><created>2012-10-29</created><authors><author><keyname>Meng</keyname><forenames>Lingzhong</forenames></author></authors><title>Study on the Availability Prediction of the Reconfigurable Networked
  Software System</title><categories>cs.MA cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes multi-agent based availability prediction approach for
the reconfigurable networked software system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7605</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7605</id><created>2012-10-29</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>List-coloring embedded graphs</title><categories>cs.DS math.CO</categories><comments>14 pages, 0 figures, accepted to SODA'13</comments><msc-class>05C15 (Primary) 05C85 (Secondary)</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any fixed surface Sigma of genus g, we give an algorithm to decide
whether a graph G of girth at least five embedded in Sigma is colorable from an
assignment of lists of size three in time O(|V(G)|). Furthermore, we can allow
a subgraph (of any size) with at most s components to be precolored, at the
expense of increasing the time complexity of the algorithm to
O(|V(G)|^{K(g+s)+1}) for some absolute constant K; in both cases, the
multiplicative constant hidden in the O-notation depends on g and s. This also
enables us to find such a coloring when it exists. The idea of the algorithm
can be applied to other similar problems, e.g., 5-list-coloring of graphs on
surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7620</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7620</id><created>2012-10-29</created><authors><author><keyname>Bilotta</keyname><forenames>Stefano</forenames></author><author><keyname>Grazzini</keyname><forenames>Elisabetta</forenames></author><author><keyname>Pergola</keyname><forenames>Elisa</forenames></author><author><keyname>Pinzani</keyname><forenames>Renzo</forenames></author></authors><title>Generation of binary words avoiding alternating patterns</title><categories>cs.DM math.CO</categories><comments>15 pages with figures</comments><msc-class>05A15, 05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an algorithm to generate binary words with no more
0's than 1's having a fixed number of 1's and avoiding the pattern $(10)^j1$
for any fixed $j \geq 1$. We will prove that this generation is exhaustive,
that is, all such binary words are generated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7621</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7621</id><created>2012-10-29</created><authors><author><keyname>Deza</keyname><forenames>Antoine</forenames></author><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author><author><keyname>Xie</keyname><forenames>Feng</forenames></author></authors><title>Computational Lower Bounds for Colourful Simplicial Depth</title><categories>math.CO cs.CG</categories><comments>8 pages</comments><msc-class>52A35, 05D05, 52B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The colourful simplicial depth problem in dimension d is to find a
configuration of (d+1) sets of (d+1) points such that the origin is contained
in the convex hull of each set (colour) but contained in a minimal number of
colourful simplices generated by taking one point from each set. A construction
attaining d^2+1 simplices is known, and is conjectured to be minimal. This has
been confirmed up to d=3, however the best known lower bound for d at least 4
is ((d+1)^2)/2.
  A promising method to improve this lower bound is to look at combinatorial
octahedral systems generated by such configurations. The difficulty to
employing this approach is handling the many symmetric configurations that
arise. We propose a table of invariants which exclude many of partial
configurations, and use this to improve the lower bound in dimension 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7624</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7624</id><created>2012-10-29</created><authors><author><keyname>Chalotra</keyname><forenames>Vivek</forenames><affiliation>AIE</affiliation></author><author><keyname>Bhasin</keyname><forenames>Anju</forenames></author><author><keyname>Gupta</keyname><forenames>Anik</forenames></author><author><keyname>Sambyal</keyname><forenames>Sanjeev Singh</forenames></author></authors><title>HEP Analysis Facility An Approach to Grid Computing</title><categories>cs.DC</categories><comments>4 pages, 4 figures, Indian Engineering Congress 2011, Bangalore,
  India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HEP Analysis Facility is a cluster designed and implemented in Scientific
Linux Cern 5.5 to grant High Energy Physics researchers one place where they
can go to undertake a particular task or to provide a parallel processing
architecture in which CPU resources are shared across a network and all
machines function as one large supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7626</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7626</id><created>2012-10-29</created><authors><author><keyname>Chalotra</keyname><forenames>Vivek</forenames></author><author><keyname>Bhasin</keyname><forenames>Anju</forenames></author><author><keyname>Gupta</keyname><forenames>Anik</forenames></author><author><keyname>Sambyal</keyname><forenames>Sanjeev Singh</forenames></author></authors><title>Hep Cluster First Step Towards Grid Computing</title><categories>cs.DC</categories><comments>7th JK Science Congress 2011, Jammu, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HEP Cluster is designed and implemented in Scientific Linux Cern 5.5 to grant
High Energy Physics researchers one place where they can go to undertake a
particular task or to provide a parallel processing architecture in which CPU
resources are shared across a network and all machines function as one large
supercomputer. It gives physicists a facility to access computers and data,
transparently, without having to consider location, operating system, account
administration, and other details. By using this facility researchers can
process their jobs much faster than the stand alone desktop systems. Keywords:
Cluster, Network, Storage, Parallel Computing &amp; Gris.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7631</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7631</id><created>2012-10-29</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>The fortresses of Ejin: an example of outlining a site from satellite
  images</title><categories>cs.CV</categories><comments>Keywords: Satellite Imagery, Image processing, GIS, fortresses, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From 1960's to 1970's, the Chinese Army built some fortified artificial
hills. Some of them are located in the Inner Mongolia, Western China. These
large fortresses are surrounded by moats. For some of them it is still possible
to see earthworks, trenches and ditches, the planning of which could have a
symbolic meaning. We can argue this result form their digital outlining,
obtained after an image processing of satellite images, based on edge
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7638</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7638</id><created>2012-10-29</created><updated>2013-11-12</updated><authors><author><keyname>Wang</keyname><forenames>Zhi Jie</forenames></author><author><keyname>Yao</keyname><forenames>Bin</forenames></author></authors><title>Finding Achievable Region among Line-segment Obstacles in the Plane</title><categories>cs.DS cs.CG</categories><comments>10 pages</comments><acm-class>F.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a new class of problem, called finding achievable region
(FAR). Let $O$ be a set of $n$ disjoint obstacles in $R^2$, $M$ be a moving
object. Let $s$ and $l$ denote the starting point and maximum path length of
the moving object $M$, respectively. Given a point $p$ in $R^2$, we say the
point $p$ is achievable for $M$ such that $\pi(s,p)\leq l$, where $\pi(\cdot)$
denotes the shortest path length in the presence of obstacles. The FAR problem
is to find a region $\mathscr R$ such that, for any point $p\in R^2$, if it is
achievable for $M$, then $p\in \mathscr R$; otherwise, $p\notin \mathscr R$.
The FAR problem has a variety of applications ranging from location-based
service, probabilistic answer on uncertain data, to moving target search. In
this paper, we restrict our attention to the case of line-segment obstacles. To
tackle this problem, we develop three algorithms. We first present a
simpler-version algorithm for the sake of intuition. Its basic idea is to
reduce our problem to computing the union of a set of circular visibility
regions (CVRs). In order to obtain the CVRs, we adopt the spatial pruning
mechanism and incorporate the idea of the rotational plane sweep. Furthermore,
for most CVRs, the circles used to construct them are unavailable beforehand.
We adopt the visibility graph technique to obtain those circles. This algorithm
takes $O(n^3)$ time. By analysing its dominant steps, we break through its
bottleneck by using the short path map (SPM) technique to obtain those circles
(unavailable beforehand), yielding an $O(n^2\log n)$ algorithm. Owing to the
finding above, the third algorithm also uses the SPM technique. It however,
does not continue to construct the CVRs. Instead, it directly traverses each
region of the SPM to trace the boundaries, the final algorithm obtains the
$O(n\log n)$ worst case upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7641</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7641</id><created>2012-10-29</created><authors><author><keyname>de Rugy-Altherre</keyname><forenames>Nicolas</forenames></author></authors><title>A Dichotomy Theorem for Homomorphism Polynomials</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we show a dichotomy theorem for the complexity of
polynomial evaluation. We associate to each graph H a polynomial that encodes
all graphs of a fixed size homomorphic to H. We show that this family is
computable by arithmetic circuits in constant depth if H has a loop or no edge
and that it is hard otherwise (i.e., complete for VNP, the arithmetic class
related to #P). We also demonstrate the hardness over the rational field of cut
eliminator, a polynomial defined by B\&quot;urgisser which is known to be neither VP
nor VNP-complete in the field of two elements, if VP is not equal to VNP (VP is
the class of polynomials computable by arithmetic circuit of polynomial size).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7650</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7650</id><created>2012-10-29</created><authors><author><keyname>Ibrahim</keyname><forenames>Heba Ezzat</forenames></author><author><keyname>Badr</keyname><forenames>Sherif M.</forenames></author><author><keyname>Shaheen</keyname><forenames>Mohamed A.</forenames></author></authors><title>Adaptive Layered Approach using Machine Learning Techniques with Gain
  Ratio for Intrusion Detection Systems</title><categories>cs.CR</categories><comments>7 pages</comments><journal-ref>(IJCA)International Journal of Computer Applications, Volume 56,
  No.7, 2012, 10-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion Detection System (IDS) has increasingly become a crucial issue for
computer and network systems. Optimizing performance of IDS becomes an
important open problem which receives more and more attention from the research
community. In this work, A multi-layer intrusion detection model is designed
and developed to achieve high efficiency and improve the detection and
classification rate accuracy .we effectively apply Machine learning techniques
(C5 decision tree, Multilayer Perceptron neural network and Na\&quot;ive Bayes)
using gain ratio for selecting the best features for each layer as to use
smaller storage space and get higher Intrusion detection performance. Our
experimental results showed that the proposed multi-layer model using C5
decision tree achieves higher classification rate accuracy, using feature
selection by Gain Ratio, and less false alarm rate than MLP and na\&quot;ive Bayes.
Using Gain Ratio enhances the accuracy of U2R and R2L for the three machine
learning techniques (C5, MLP and Na\&quot;ive Bayes) significantly. MLP has high
classification rate when using the whole 41 features in Dos and Probe layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7653</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7653</id><created>2012-10-29</created><authors><author><keyname>Bartnicki</keyname><forenames>Tomasz</forenames></author><author><keyname>Miechowicz</keyname><forenames>Zofia</forenames></author></authors><title>Total Game Coloring of Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total variant of well known graph coloring game is considered. We determine
exact values of total game chromatic number for some classes of graphs and show
show the strategie for first player to win the game. We also show relation
between total game coloring number and game coloring index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7656</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7656</id><created>2012-10-29</created><authors><author><keyname>Naor</keyname><forenames>Assaf</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>Efficient rounding for the noncommutative Grothendieck inequality</title><categories>cs.DS</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Grothendieck inequality has applications to the design of
approximation algorithms for NP-hard optimization problems. We show that an
algorithmic interpretation may also be given for a noncommutative
generalization of the Grothendieck inequality due to Pisier and Haagerup. Our
main result, an efficient rounding procedure for this inequality, leads to a
constant-factor polynomial time approximation algorithm for an optimization
problem which generalizes the Cut Norm problem of Frieze and Kannan, and is
shown here to have additional applications to robust principle component
analysis and the orthogonal Procrustes problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7657</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7657</id><created>2012-10-29</created><authors><author><keyname>Zippo</keyname><forenames>Antonio Giuliano</forenames></author></authors><title>Text Classification with Compression Algorithms</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns a comparison of SVM kernel methods in text categorization
tasks. In particular I define a kernel function that estimates the similarity
between two objects computing by their compressed lengths. In fact, compression
algorithms can detect arbitrarily long dependencies within the text strings.
Data text vectorization looses information in feature extractions and is highly
sensitive by textual language. Furthermore, these methods are language
independent and require no text preprocessing. Moreover, the accuracy computed
on the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than
Gaussian, linear and polynomial kernels. The method limits are represented by
computational time complexity of the Gram matrix and by very poor performance
on non-textual datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7659</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7659</id><created>2012-10-29</created><updated>2013-01-23</updated><authors><author><keyname>Ellerman</keyname><forenames>David</forenames></author></authors><title>The Objective Indefiniteness Interpretation of Quantum Mechanics</title><categories>quant-ph cs.IT math.IT math.LO physics.hist-ph</categories><msc-class>81P05, 81P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The common-sense view of reality is expressed logically in Boolean subset
logic (each element is either definitely in or not in a subset, i.e., either
definitely has or does not have a property). But quantum mechanics does not
agree with this &quot;properties all the way down&quot; picture of micro-reality. Are
there other coherent alternative views of reality? A logic of partitions, dual
to the Boolean logic of subsets (partitions are dual to subsets), was recently
developed along with a logical version of information theory. In view of the
subset-partition duality, partition logic is the alternative to Boolean subset
logic and thus it abstractly describes the alternative dual view of
micro-reality. Perhaps QM is compatible with this dual view? Indeed, when the
mathematics of partitions using sets is &quot;lifted&quot; from sets to vector spaces,
then it yields the mathematics and relations of quantum mechanics. Thus the
vision of micro-reality abstractly characterized by partition logic matches
that described by quantum mechanics. The key concept explicated by partition
logic is the old idea of &quot;objective indefiniteness&quot; (emphasized by Shimony).
Thus partition logic, logical information theory, and the lifting program
provide the back story so that the old idea then yields the objective
indefiniteness interpretation of quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7669</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7669</id><created>2012-10-29</created><authors><author><keyname>Maknikar</keyname><forenames>Pooja</forenames></author></authors><title>Performance Evaluation of Different Techniques for texture
  Classification</title><categories>cs.CV</categories><comments>Performance evaluation of Wavelet transform and Co-occurrence matrix
  method was done using energy as extracted feature on the basis of Time
  complexity and accuracy basis; pp.353-361,2012</comments><doi>10.5121/csit.2012.2434</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Texture is the term used to characterize the surface of a given object or
phenomenon and is an important feature used in image processing and pattern
recognition. Our aim is to compare various Texture analyzing methods and
compare the results based on time complexity and accuracy of classification.
The project describes texture classification using Wavelet Transform and Co
occurrence Matrix. Comparison of features of a sample texture with database of
different textures is performed. In wavelet transform we use the Haar, Symlets
and Daubechies wavelets. We find that, thee Haar wavelet proves to be the most
efficient method in terms of performance assessment parameters mentioned above.
Comparison of Haar wavelet and Co-occurrence matrix method of classification
also goes in the favor of Haar. Though the time requirement is high in the
later method, it gives excellent results for classification accuracy except if
the image is rotated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7678</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7678</id><created>2012-10-19</created><authors><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author></authors><title>Plagiarism Detection: Keeping Check on Misuse of Intellectual Property</title><categories>cs.OH</categories><comments>Proceedings of National Conference on Recent Advances in Computer
  Engineering, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, Plagiarism has become a menace. Every journal editor or conference
organizers has to deal with this problem. Simply Copying or rephrasing of text
without giving due credit to the original author has become more common. This
is considered to be an Intellectual Property Theft. We are developing a
Plagiarism Detection Tool which would deal with this problem. In this paper we
discuss the common tools available to detect plagiarism and their short comings
and the advantages of our tool over these tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7683</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7683</id><created>2012-10-29</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Computing Petaflops over Terabytes of Data: The Case of Genome-Wide
  Association Studies</title><categories>cs.MS cs.CE cs.PF q-bio.GN q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scientific and engineering applications, one has to solve not one but
a sequence of instances of the same problem. Often times, the problems in the
sequence are linked in a way that allows intermediate results to be reused. A
characteristic example for this class of applications is given by the
Genome-Wide Association Studies (GWAS), a widely spread tool in computational
biology. GWAS entails the solution of up to trillions ($10^{12}$) of correlated
generalized least-squares problems, posing a daunting challenge: the
performance of petaflops ($10^{15}$ floating-point operations) over terabytes
of data.
  In this paper, we design an algorithm for performing GWAS on multi-core
architectures. This is accomplished in three steps. First, we show how to
exploit the relation among successive problems, thus reducing the overall
computational complexity. Then, through an analysis of the required data
transfers, we identify how to eliminate any overhead due to input/output
operations. Finally, we study how to decompose computation into tasks to be
distributed among the available cores, to attain high performance and
scalability. With our algorithm, a GWAS that currently requires the use of a
supercomputer may now be performed in matter of hours on a single multi-core
node.
  The discussion centers around the methodology to develop the algorithm rather
than the specific application. We believe the paper contributes valuable
guidelines of general applicability for computational scientists on how to
develop and optimize numerical algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7684</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7684</id><created>2012-10-29</created><authors><author><keyname>Farzad</keyname><forenames>Babak</forenames></author><author><keyname>Karimi</keyname><forenames>Majid</forenames></author></authors><title>Square-Root Finding Problem In Graphs, A Complete Dichotomy Theorem</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph G is the square of graph H if two vertices x,y have an edge in G if and
only if x,y are of distance at most two in H. Given H it is easy to compute its
square H^2. Determining if a given graph G is the square of some graph is not
easy in general. Motwani and Sudan proved that it is NP-complete to determine
if a given graph G is the square of some graph. The graph introduced in their
reduction is a graph that contains many triangles and is relatively dense.
Farzad et al. proved the NP-completeness for finding a square root for girth 4
while they gave a polynomial time algorithm for computing a square root of
girth at least six. Adamaszek and Adamaszek proved that if a graph has a square
root of girth six then this square root is unique up to isomorphism. In this
paper we consider the characterization and recognition problem of graphs that
are square of graphs of girth at least five. We introduce a family of graphs
with exponentially many non-isomorphic square roots, and as the main result of
this paper we prove that the square root finding problem is NP-complete for
square roots of girth five. This proof is providing the complete dichotomy
theorem for square root problem in terms of the girth of the square roots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7686</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7686</id><created>2012-10-29</created><authors><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Murawski</keyname><forenames>Andrzej S.</forenames></author></authors><title>Bisimilarity of Pushdown Systems is Nonelementary</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two pushdown systems, the bisimilarity problem asks whether they are
bisimilar. While this problem is known to be decidable our main result states
that it is nonelementary, improving EXPTIME-hardness, which was the previously
best known lower bound for this problem. Our lower bound result holds for
normed pushdown systems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7711</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7711</id><created>2012-10-29</created><authors><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Torr&#xe9;sani</keyname><forenames>Bruno</forenames></author></authors><title>Refined support and entropic uncertainty inequalities</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized versions of the entropic (Hirschman-Beckner) and support
(Elad-Bruckstein) uncertainty principle are presented for frames
representations. Moreover, a sharpened version of the support inequality has
been obtained by introducing a generalization of the coherence. In the finite
dimensional case and under certain conditions, minimizers of this inequalities
are given as constant functions on their support. In addition, $\ell^p$-norms
inequalities are introduced as byproducts of the entropic inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7719</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7719</id><created>2012-10-29</created><authors><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Robustness, Canalyzing Functions and Systems Design</title><categories>math.PR cs.SY</categories><comments>20 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1110.1338</comments><msc-class>93B51</msc-class><journal-ref>Theory in Biosciences 133 (2), 2014, p. 63-78</journal-ref><doi>10.1007/s12064-013-0186-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a notion of robustness of a Markov kernel that describes a system of
several input random variables and one output random variable. Robustness
requires that the behaviour of the system does not change if one or several of
the input variables are knocked out. If the system is required to be robust
against too many knockouts, then the output variable cannot distinguish
reliably between input states and must be independent of the input. We study
how many input states the output variable can distinguish as a function of the
required level of robustness.
  Gibbs potentials allow a mechanistic description of the behaviour of the
system after knockouts. Robustness imposes structural constraints on these
potentials. We show that interaction families of Gibbs potentials allow to
describe robust systems.
  Given a distribution of the input random variables and the Markov kernel
describing the system, we obtain a joint probability distribution. Robustness
implies a number of conditional independence statements for this joint
distribution. The set of all probability distributions corresponding to robust
systems can be decomposed into a finite union of components, and we find
parametrizations of the components. The decomposition corresponds to a primary
decomposition of the conditional independence ideal and can be derived from
more general results about generalized binomial edge ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7752</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7752</id><created>2012-10-29</created><updated>2013-09-11</updated><authors><author><keyname>Alexeev</keyname><forenames>Boris</forenames></author><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Phase retrieval with polarization</title><categories>cs.IT math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many areas of imaging science, it is difficult to measure the phase of
linear measurements. As such, one often wishes to reconstruct a signal from
intensity measurements, that is, perform phase retrieval. In this paper, we
provide a novel measurement design which is inspired by interferometry and
exploits certain properties of expander graphs. We also give an efficient phase
retrieval procedure, and use recent results in spectral graph theory to produce
a stable performance guarantee which rivals the guarantee for PhaseLift in
[Candes et al. 2011]. We use numerical simulations to illustrate the
performance of our phase retrieval procedure, and we compare reconstruction
error and runtime with a common alternating-projections-type procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7756</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7756</id><created>2012-10-29</created><authors><author><keyname>Paterson</keyname><forenames>Maura B.</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author><author><keyname>Upadhyay</keyname><forenames>Jalaj</forenames></author></authors><title>A coding theory foundation for the analysis of general unconditionally
  secure proof-of-retrievability schemes for cloud storage</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been considerable recent interest in &quot;cloud storage&quot; wherein a user
asks a server to store a large file. One issue is whether the user can verify
that the server is actually storing the file, and typically a
challenge-response protocol is employed to convince the user that the file is
indeed being stored correctly. The security of these schemes is phrased in
terms of an extractor which will recover or retrieve the file given any
&quot;proving algorithm&quot; that has a sufficiently high success probability.
  This paper treats proof-of-retrievability schemes in the model of
unconditional security, where an adversary has unlimited computational power.
In this case retrievability of the file can be modelled as error-correction in
a certain code. We provide a general analytical framework for such schemes that
yields exact (non-asymptotic) reductions that precisely quantify conditions for
extraction to succeed as a function of the success probability of a proving
algorithm, and we apply this analysis to several archetypal schemes. In
addition, we provide a new methodology for the analysis of keyed POR schemes in
an unconditionally secure setting, and use it to prove the security of a
modified version of a scheme due to Shacham and Waters under a slightly
restricted attack model, thus providing the first example of a keyed POR scheme
with unconditional security. We also show how classical statistical techniques
can be used to evaluate whether the responses of the prover are accurate enough
to permit successful extraction. Finally, we prove a new lower bound on storage
and communication complexity of POR schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7774</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7774</id><created>2012-10-26</created><updated>2013-03-25</updated><authors><author><keyname>Kristensen</keyname><forenames>Mads Ruben Burgdorff</forenames></author><author><keyname>Lund</keyname><forenames>Simon Andreas Frimann</forenames></author><author><keyname>Blum</keyname><forenames>Troels</forenames></author><author><keyname>Vinter</keyname><forenames>Brian</forenames></author></authors><title>cphVB: A System for Automated Runtime Optimization and Parallelization
  of Vectorized Applications</title><categories>cs.PL cs.DC</categories><journal-ref>Proceedings of The 11th Python In Science Conference (SciPy 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern processor architectures, in addition to having still more cores, also
require still more consideration to memory-layout in order to run at full
capacity. The usefulness of most languages is deprecating as their
abstractions, structures or objects are hard to map onto modern processor
architectures efficiently.
  The work in this paper introduces a new abstract machine framework, cphVB,
that enables vector oriented high-level programming languages to map onto a
broad range of architectures efficiently. The idea is to close the gap between
high-level languages and hardware optimized low-level implementations. By
translating high-level vector operations into an intermediate vector bytecode,
cphVB enables specialized vector engines to efficiently execute the vector
operations.
  The primary success parameters are to maintain a complete abstraction from
low-level details and to provide efficient code execution across different,
modern, processors. We evaluate the presented design through a setup that
targets multi-core CPU architectures. We evaluate the performance of the
implementation using Python implementations of well-known algorithms: a jacobi
solver, a kNN search, a shallow water simulation and a synthetic stencil
simulation. All demonstrate good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7784</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7784</id><created>2012-10-29</created><authors><author><keyname>Crnkovic</keyname><forenames>Gordana Dodig</forenames></author><author><keyname>Giovagnoli</keyname><forenames>Raffaela</forenames></author></authors><title>Computing Nature: A Network of Networks of Concurrent Information
  Processes</title><categories>cs.GL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This text presents the research field of natural/unconventional computing as
it appears in the book COMPUTING NATURE. The articles discussed consist a
selection of works from the Symposium on Natural Computing at AISB-IACAP
(British Society for the Study of Artificial Intelligence and the Simulation of
Behaviour and The International Association for Computing and Philosophy) World
Congress 2012, held at the University of Birmingham, celebrating Turing
centenary. The COMPUTING NATURE is about nature considered as the totality of
physical existence, the universe. By physical we mean all phenomena, objects
and processes, that are possible to detect either directly by our senses or via
instruments. Historically, there have been many ways of describing the universe
(cosmic egg, cosmic tree, theistic universe, mechanistic universe) while a
particularly prominent contemporary approach is computational universe, as
discussed in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7788</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7788</id><created>2012-10-29</created><authors><author><keyname>Nascimento</keyname><forenames>Marcelo Zanchetta do</forenames></author><author><keyname>Batista</keyname><forenames>Val&#xe9;rio Ramos</forenames></author><author><keyname>Coimbra</keyname><forenames>Wendhel Raffa</forenames></author></authors><title>An interactive programme for Steiner trees</title><categories>cs.CG</categories><comments>9 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a fully written programmed code with a supervised method for
generating Steiner trees. Our choice of the programming language, and the use
of well-known theorems from Geometry and Complex Analysis, allowed this method
to be implemented with only 747 lines of effective source code. This eases the
understanding and the handling of this beta version for future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7789</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7789</id><created>2012-10-29</created><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Architectural innovation: A game-theoretic approach</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we view the Internet under a game-theoretic lens in an effort
to explain and overcome the Internet's innovation slump. Game Theory is used to
model Internet environments as problems of technological competition toward the
end of understanding their emergent phenomena and the evolutionary forces that
shape them. However, our results extend beyond understanding the Internet
architecture toward helping the Internet population achieve socially desirable
outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7828</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7828</id><created>2012-10-29</created><authors><author><keyname>Fronczak</keyname><forenames>Agata</forenames></author></authors><title>Exponential random graph models</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><journal-ref>Chapter in Encyclopedia of Social Network Analysis and Mining, R.
  Alhajj, J. Rokne (Eds.), Springer-Verlag, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, exponential random graphs (ERGs) are among the most widely-studied
network models. Different analytical and numerical techniques for ERG have been
developed that resulted in the well-established theory with true predictive
power. An excellent basic discussion of exponential random graphs addressed to
social science students and researchers is given in [Anderson et al.,
1999][Robins et al., 2007]. This essay is intentionally designed to be more
theoretical in comparison with the well-known primers just mentioned. Given the
interdisciplinary character of the new emerging science of complex networks,
the essay aims to give a contribution upon which network scientists and
practitioners, who represent different research areas, could build a common
area of understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7837</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7837</id><created>2012-10-29</created><updated>2014-08-17</updated><authors><author><keyname>Mondal</keyname><forenames>Santanu</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Scheduling Under Fading and Partial Channel Information</title><categories>math.OC cs.NI</categories><comments>22 pages, 3 figures; Added IEEE Journal submission notice</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a scheduler for the downlink of a wireless channel when only
partial channel-state information is available at the scheduler. We
characterize the network stability region and provide two throughput-optimal
scheduling policies. We also derive a deterministic bound on the mean packet
delay in the network. Finally, we provide a throughput-optimal policy for the
network under QoS constraints when real-time and rate-guaranteed data traffic
may be present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7844</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7844</id><created>2012-10-29</created><updated>2014-10-29</updated><authors><author><keyname>Elphick</keyname><forenames>Clive</forenames></author><author><keyname>Wocjan</keyname><forenames>Pawel</forenames></author></authors><title>Unified spectral bounds on the chromatic number</title><categories>math.CO cs.DM quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the best known results in spectral graph theory is the following lower
bound on the chromatic number due to Alan Hoffman, where mu_1 and mu_n are
respectively the maximum and minimum eigenvalues of the adjacency matrix: chi
&gt;= 1 + mu_1 / (- mu_n). We recently generalised this bound to include all
eigenvalues of the adjacency matrix.
  In this paper, we further generalize these results to include all eigenvalues
of the adjacency, Laplacian and signless Laplacian matrices. The various known
bounds are also unified by considering the normalized adjacency matrix, and
examples are cited for which the new bounds outperform known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7858</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7858</id><created>2012-10-29</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>Solving Linear System of Equations Via A Convex Hull Algorithm</title><categories>cs.NA cs.CG math.NA</categories><comments>15 pages, 3 figures</comments><msc-class>65F05, 65F10, 34A30, 90C05, 52A20, 52B55</msc-class><acm-class>G.1.3; G.1.6; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new iterative algorithms for solving a square linear system $Ax=b$
in dimension $n$ by employing the {\it Triangle Algorithm} \cite{kal12}, a
fully polynomial-time approximation scheme for testing if the convex hull of a
finite set of points in a Euclidean space contains a given point. By converting
$Ax=b$ into a convex hull problem and solving via the Triangle Algorithm,
together with a {\it sensitivity theorem}, we compute in $O(n^2\epsilon^{-2})$
arithmetic operations an approximate solution satisfying $\Vert Ax_\epsilon - b
\Vert \leq \epsilon \rho$, where $\rho= \max \{\Vert a_1 \Vert,..., \Vert a_n
\Vert, \Vert b \Vert \}$, and $a_i$ is the $i$-th column of $A$. In another
approach we apply the Triangle Algorithm incrementally, solving a sequence of
convex hull problems while repeatedly employing a {\it distance duality}. The
simplicity and theoretical complexity bounds of the proposed algorithms,
requiring no structural restrictions on the matrix $A$, suggest their potential
practicality, offering alternatives to the existing exact and iterative
methods, especially for large scale linear systems. The assessment of
computational performance however is the subject of future experimentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7859</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7859</id><created>2012-10-29</created><authors><author><keyname>Narayanan</keyname><forenames>Prashant</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Stochastic Games on a Multiple Access Channel</title><categories>cs.SY cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a scenario where N users try to access a common base station.
Associated with each user is its channel state and a finite queue which varies
with time. Each user chooses his power and the admission control variable in a
dynamic manner so as to maximize his expected throughput. The throughput of
each user is a function of the actions and states of all users. The scenario
considers the situation where each user knows his channel and buffer state but
is unaware of the states and actions taken by the other users. We consider the
scenario when each user is saturated (i.e., always has a packet to transmit) as
well as the case when each user is unsaturated. We formulate the problem as a
Markov game and show connections with strategic form games. We then consider
various throughput functions associated with the multiple user channel and
provide algorithms for finding these equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7870</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7870</id><created>2012-10-29</created><authors><author><keyname>Lu</keyname><forenames>Yuan</forenames></author><author><keyname>Duel-Hallen</keyname><forenames>Alexandra</forenames></author></authors><title>Channel-Adaptive Sensing Strategy for Cognitive Radio Ad Hoc Networks</title><categories>cs.NI</categories><comments>6 pages, 8 figures, CCNC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cognitive Radio (CR) ad hoc networks, secondary users (SU) attempt to
utilize valuable spectral resources without causing significant interference to
licensed primary users (PU). While there is a large body of research on
spectrum opportunity detection, exploitation, and adaptive transmission in CR,
most existing approaches focus only on avoiding PU activity when making sensing
decisions. Since the myopic sensing strategy results in congestion and poor
throughput, several collision-avoidance sensing approaches were investigated in
the literature. However, they provide limited improvement. A channel-aware
myopic sensing strategy that adapts the reward to the fading channel state
information (CSI) of the SU link is proposed. This CSI varies over the CR
spectrum and from one SU pair to another due to multipath and shadow fading,
thus randomizing sensing decisions and increasing the network throughput. The
proposed joint CSI adaptation at the medium access control (MAC) and physical
layers provides large throughput gain over randomized sensing strategies and/or
conventional adaptive transmission methods. The performance of the proposed
CSI-aided sensing strategy is validated for practical network scenarios and
demonstrated to be robust to CSI mismatch, sensing errors, and spatial channel
correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7889</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7889</id><created>2012-10-29</created><authors><author><keyname>Ebrahim</keyname><forenames>Nader Ale</forenames></author><author><keyname>Ahmed</keyname><forenames>Shamsuddin</forenames></author><author><keyname>Rashid</keyname><forenames>Salwa Hanim Abdul</forenames></author><author><keyname>Taha</keyname><forenames>Zahari</forenames></author><author><keyname>Wazed</keyname><forenames>M. A.</forenames></author></authors><title>Virtual Collaborative R&amp;D Teams in Malaysia Manufacturing SMEs</title><categories>cs.OH</categories><comments>4 pages</comments><msc-class>90B50, 90C29, 90C31, 91A35, 91B06</msc-class><acm-class>D.2.9; K.1</acm-class><journal-ref>(2012). Virtual Collaborative R&amp;D Teams in Malaysia Manufacturing
  SMEs. Advanced Materials Research, 433-440, 1653-1659</journal-ref><doi>10.4028/www.scientific.net/AMR.433-440.1653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results of empirical research conducted during March
to September 2009. The study focused on the influence of virtual research and
development teams within Malaysian manufacturing small and medium sized
enterprises (SMEs). The specific objective of the study is better understanding
of the application of collaborative technologies in business, to find the
effective factors to assist SMEs to remain competitive in the future. The paper
stresses to find an answer for a question Is there any relationship between
company size, Internet connection facility and virtuality?. The survey data
shows SMEs are now technologically capable of performing the virtual
collaborative team, but the infrastructure usage is less. SMEs now have the
necessary technology to begin the implementation process of collaboration tools
to reduce research and development time, costs and increase productivity. So,
the manager of R and D should take the potentials of virtual teams into
account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7906</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7906</id><created>2012-10-30</created><authors><author><keyname>Mustafa</keyname><forenames>Atta Ul</forenames></author><author><keyname>Murtaza</keyname><forenames>Ghulam</forenames></author></authors><title>Synthesis-by-analysis of BCH Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a technique to blindly synthesize the generator
polynomial of BCH codes. The proposed technique involves finding Greatest
Common Divisor (GCD) among different codewords and block lengths. Based on this
combinatorial GCD calculation, correlation values are found. For a valid block
length, the iterative GCD calculation results either into generator polynomial
or some of its higher order multiples. These higher order polynomials are
factorized under modulo-2 operation, and one of the resulting factors is always
the generator polynomial which further increases the correlation value. The
resulting correlation plot for different polynomials shows very high values for
correct block length and valid generator polynomial. Knowing the valid block
length and generator polynomial, all other parameters including number of
parity-check digits (n-k), minimum distance dmin and error correcting
capability t are readily exposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7913</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7913</id><created>2012-10-30</created><authors><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Interleaved equivalence of categories of persistence modules</title><categories>math.AT cs.CG math.CT</categories><comments>9 pages</comments><msc-class>16D90, 06A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that an equivalence of categories using
$\varepsilon$-interleavings as a fundamental component exists between the model
of persistence modules as graded modules over a polynomial ring and the model
of persistence modules as modules over the total order of the real numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7917</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7917</id><created>2012-10-30</created><authors><author><keyname>Pavlyshenko</keyname><forenames>Bohdan</forenames></author></authors><title>The Model of Semantic Concepts Lattice For Data Mining Of Microblogs</title><categories>cs.CL cs.IR</categories><comments>In Ukrainian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of semantic concept lattice for data mining of microblogs has been
proposed in this work. It is shown that the use of this model is effective for
the semantic relations analysis and for the detection of associative rules of
key words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7919</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7919</id><created>2012-10-30</created><updated>2013-10-16</updated><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>Tree t-spanners in Outerplanar Graphs via Supply Demand Partition</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tree t-spanner of an unweighted graph G is a spanning tree T such that for
every two vertices their distance in T is at most t times their distance in G.
Given an unweighted graph G and a positive integer t as input, the tree
t-spanner problem is to compute a tree t-spanner of G if one exists. This
decision problem is known to be NP-complete even in the restricted class of
unweighted planar graphs. We present a linear-time reduction from tree
t-spanner in outerplanar graphs to the supply-demand tree partition problem.
Based on this reduction, we obtain a linear-time algorithm to solve tree
t-spanner in outerplanar graphs. Consequently, we show that the minimum value
of t for which an input outerplanar graph on n vertices has a tree t-spanner
can be found in O(n log n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7931</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7931</id><created>2012-10-30</created><authors><author><keyname>Mat&#xfa;&#x161;</keyname><forenames>Franti&#x161;ek</forenames></author></authors><title>Polymatroids and polyquantoids</title><categories>cs.IT cs.CR math.CO math.IT</categories><journal-ref>\emph{Proceedings of WUPES 2012}, Sept.\ 12--15, Mari\'ansk\'e
  L\'azn\`i, Czech Republic, 126--136</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When studying entropy functions of multivariate probability distributions,
polymatroids and matroids emerge. Entropy functions of pure multiparty quantum
states give rise to analogous notions, called here polyquantoids and quantoids.
Polymatroids and polyquantoids are related via linear mappings and duality.
Quantum secret sharing schemes that are ideal are described by selfdual
matroids. Expansions of integer polyquantoids to quantoids are studied and
linked to that of polymatroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7935</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7935</id><created>2012-10-30</created><authors><author><keyname>Chalotra</keyname><forenames>Vivek</forenames></author><author><keyname>Bhasin</keyname><forenames>Anju</forenames></author><author><keyname>Gupta</keyname><forenames>Anik</forenames></author><author><keyname>Sambyal</keyname><forenames>Sanjeev Singh</forenames></author><author><keyname>Mahajan</keyname><forenames>Sanjay</forenames></author></authors><title>Energy Efficient Algorithms and Power Consumption Techniques in High
  Performance Computing</title><categories>cs.DC</categories><comments>5 pages, 4 figures &amp; Presented at 8th JKSC,Srinagar, J&amp;K, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Performance Computing is an internet based computing which makes
computer infrastructure and services available to the user for research
purpose. However, an important issue which needs to be resolved before High
Performance Computing Cluster with large pool of servers gain widespread
acceptance is the design of data centers with less energy consumption. It is
only possible when servers produce less heat and consume less power. Systems
reliability decreases with increase in temperature due to heat generation
caused by large power consumption as computing in high temperature is more
error-prone. Here in this paper our approach is to design and implement a high
performance cluster for high-end research in the High Energy Physics stream.
This involves the usage of fine grained power gating technique in
microprocessors and energy efficient algorithms that reduce the overall running
cost of the data center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7940</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7940</id><created>2012-10-30</created><authors><author><keyname>Kazakopoulos</keyname><forenames>Pavlos</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author></authors><title>Transmission of information via the non-linear Scroedinger equation: The
  random Gaussian input case</title><categories>cs.IT math.IT nlin.SI</categories><report-no>PHYSCOM report July 2008. Research funded through a Marie-Curie
  Mobility research Grant (2006-2008)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion of demand for ultra-high information transmission rates over
the last decade has necessitated the usage of increasingly high light
intensities for fiber optical transmissions. As a result, the fiber
non-linearities need to be treated non-perturbatively. Similar analyses in the
past have focused on the effects of non-linearities on existing transmission
technologies, e.g. WDM. In this paper we take advantage of the fact that, under
certain assumptions, light transmission through optical fibers can be described
using the non-linear Schroedinger equation, which is exactly integrable. As a
particular example, we show that in the low Gaussian noise limit, the Gaussian
input distribution has a higher mutual information than the transmission using
WDM over the same available bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7942</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7942</id><created>2012-10-30</created><updated>2012-12-18</updated><authors><author><keyname>Babinkostova</keyname><forenames>L.</forenames></author><author><keyname>Bombardier</keyname><forenames>K. W.</forenames></author><author><keyname>Cole</keyname><forenames>M. M.</forenames></author><author><keyname>Morrell</keyname><forenames>T. A.</forenames></author><author><keyname>Scott</keyname><forenames>C. B.</forenames></author></authors><title>Algebraic properties of generalized Rijndael-like ciphers</title><categories>math.GR cs.CR math.CO</categories><comments>22 pages; Prelim04</comments><msc-class>11T71, 14G50, 20B05, 20B30, 94A60</msc-class><acm-class>D.4.6; E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide conditions under which the set of Rijndael functions considered as
permutations of the state space and based on operations of the finite field
$\GF (p^k)$ ($p\geq 2$ a prime number) is not closed under functional
composition. These conditions justify using a sequential multiple encryption to
strengthen the AES (Rijndael block cipher with specific block sizes) in case
AES became practically insecure. In Sparr and Wernsdorf (2008), R. Sparr and R.
Wernsdorf provided conditions under which the group generated by the
Rijndael-like round functions based on operations of the finite field $\GF
(2^k)$ is equal to the alternating group on the state space. In this paper we
provide conditions under which the group generated by the Rijndael-like round
functions based on operations of the finite field $\GF (p^k)$ ($p\geq 2$) is
equal to the symmetric group or the alternating group on the state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7954</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7954</id><created>2012-10-30</created><updated>2013-04-28</updated><authors><author><keyname>Chawachat</keyname><forenames>Jakarin</forenames></author><author><keyname>Fakcharoenphol</keyname><forenames>Jittat</forenames></author></authors><title>A simpler load-balancing algorithm for range-partitioned data in
  peer-to-peer systems</title><categories>cs.DC cs.NI</categories><comments>(21 pages, 3 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random hashing is a standard method to balance loads among nodes in
Peer-to-Peer networks. However, hashing destroys locality properties of object
keys, the critical properties to many applications, more specifically, those
that require range searching. To preserve a key order while keeping loads
balanced, Ganesan, Bawa and Garcia-Molina proposed a load-balancing algorithm
that supports both object insertion and deletion that guarantees a ratio of
4.237 between the maximum and minimum loads among nodes in the network using
constant amortized costs. However, their algorithm is not straightforward to
implement in real networks because it is recursive. Their algorithm mostly uses
local operations with global max-min load information. In this work, we present
a simple non-recursive algorithm using essentially the same primitive
operations as in Ganesan {\em et al.}'s work. We prove that for insertion and
deletion, our algorithm guarantees a constant max-min load ratio of 7.464 with
constant amortized costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7956</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7956</id><created>2012-10-30</created><authors><author><keyname>Achkar</keyname><forenames>Roger</forenames></author><author><keyname>Owayjan</keyname><forenames>Michel</forenames></author></authors><title>Implementation of a Vision System for a Landmine Detecting Robot Using
  Artificial Neural Network</title><categories>cs.NE cs.CV</categories><comments>20 pages, 14 figures, 4 tables</comments><msc-class>68T45</msc-class><acm-class>I.2.6; I.5.1</acm-class><journal-ref>Achkar R, Owayjan M. Implementation of a Vision System for a
  Landmine Detecting Robot Using Artificial Neural Network. International
  Journal of Artificial Intelligence &amp; Applications (IJAIA), Vol 3, No. 5, pp.
  73-92, September 2012</journal-ref><doi>10.5121/ijaia.2012.3507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Landmines, specifically anti-tank mines, cluster bombs, and unexploded
ordnance form a serious problem in many countries. Several landmine sweeping
techniques are used for minesweeping. This paper presents the design and the
implementation of the vision system of an autonomous robot for landmines
localization. The proposed work develops state-of-the-art techniques in digital
image processing for pre-processing captured images of the contaminated area.
After enhancement, Artificial Neural Network (ANN) is used in order to
identify, recognize and classify the landmines' make and model. The
Back-Propagation algorithm is used for training the network. The proposed work
proved to be able to identify and classify different types of landmines under
various conditions (rotated landmine, partially covered landmine) with a
success rate of up to 90%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7959</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7959</id><created>2012-10-30</created><authors><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author></authors><title>Algorithm Selection for Combinatorial Search Problems: A Survey</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Algorithm Selection Problem is concerned with selecting the best
algorithm to solve a given problem on a case-by-case basis. It has become
especially relevant in the last decade, as researchers are increasingly
investigating how to identify the most suitable existing algorithm for solving
a problem instead of developing new algorithms. This survey presents an
overview of this work focusing on the contributions made in the area of
combinatorial search problems, where Algorithm Selection techniques have
achieved significant performance improvements. We unify and organise the vast
literature according to criteria that determine Algorithm Selection systems in
practice. The comprehensive classification of approaches identifies and
analyses the different directions from which Algorithm Selection has been
approached. This paper contrasts and compares different methods for solving the
problem as well as ways of using these solutions. It closes by identifying
directions of current and future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7961</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7961</id><created>2012-10-30</created><updated>2013-05-14</updated><authors><author><keyname>Hansen</keyname><forenames>Johan P.</forenames></author></authors><title>Osculating Spaces of Varieties and Linear Network Codes</title><categories>math.AG cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general theory to obtain good linear network codes utilizing the
osculating nature of algebraic varieties. In particular, we obtain from the
osculating spaces of Veronese varieties explicit families of equidimensional
vector spaces, in which any pair of distinct vector spaces intersects in the
same dimension.
  Linear network coding transmits information in terms of a basis of a vector
space and the information is received as a basis of a possible altered vector
space. Ralf Koetter and Frank R. Kschischang introduced a metric on the set af
vector spaces and showed that a minimal distance decoder for this metric
achieves correct decoding if the dimension of the intersection of the
transmitted and received vector space is sufficiently large.
  The obtained osculating spaces of Veronese varieties are equidistant in the
above metric. The parameters of the resulting linear network codes are
determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7970</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7970</id><created>2012-10-30</created><authors><author><keyname>Lenzner</keyname><forenames>Pascal</forenames></author></authors><title>Greedy Selfish Network Creation</title><categories>cs.GT cs.DS</categories><comments>28 pages, 8 figures. An extended abstract of this work was accepted
  at WINE'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze greedy equilibria (GE) for the well-known model of
selfish network creation by Fabrikant et al.[PODC'03]. GE are interesting for
two reasons: (1) they model outcomes found by agents which prefer smooth
adaptations over radical strategy-changes, (2) GE are outcomes found by agents
which do not have enough computational resources to play optimally. In the
model of Fabrikant et al. agents correspond to Internet Service Providers which
buy network links to improve their quality of network usage. It is known that
computing a best response in this model is NP-hard. Hence, poly-time agents are
likely not to play optimally. But how good are networks created by such agents?
We answer this question for very simple agents. Quite surprisingly, naive
greedy play suffices to create remarkably stable networks. Specifically, we
show that in the SUM version, where agents attempt to minimize their average
distance to all other agents, GE capture Nash equilibria (NE) on trees and that
any GE is in 3-approximate NE on general networks. For the latter we also
provide a lower bound of 3/2 on the approximation ratio. For the MAX version,
where agents attempt to minimize their maximum distance, we show that any
GE-star is in 2-approximate NE and any GE-tree having larger diameter is in
6/5-approximate NE. Both bounds are tight. We contrast these positive results
by providing a linear lower bound on the approximation ratio for the MAX
version on general networks in GE. This result implies a locality gap of
$\Omega(n)$ for the metric min-max facility location problem, where n is the
number of clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.7985</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.7985</id><created>2012-10-30</created><authors><author><keyname>Steane</keyname><forenames>Andrew M.</forenames></author></authors><title>Threat, support and dead edges in the Shannon game</title><categories>math.CO cs.DM</categories><comments>18 pages; 11 figures</comments><report-no>SG01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notions of captured/lost vertices and dead edges in the Shannon game
(Shannon switching game on nodes) are examined using graph theory. Simple
methods are presented for identifying some dead edges and some captured sets of
vertices, thus simplifying the (computationally hard) problem of analyzing the
game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8011</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8011</id><created>2012-10-30</created><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Singh</keyname><forenames>Rishideep</forenames></author></authors><title>Reusability Framework for Cloud Computing</title><categories>cs.SE</categories><comments>9 Pages including 8 figures, Published by IJCER</comments><report-no>AF02601690177</report-no><msc-class>68N30</msc-class><acm-class>D.2.13</acm-class><journal-ref>International Journal Of Computational Engineering Research
  (ijceronline.com) Vol. 2 Issue. 6, 2012, 169-177</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Cloud based development is a challenging task for several software
engineering projects, especially for those which needs development with
reusability. Present time of cloud computing is allowing new professional
models for using the software development. The expected upcoming trend of
computing is assumed to be this cloud computing because of speed of application
deployment, shorter time to market, and lower cost of operation. Until Cloud Co
mputing Reusability Model is considered a fundamental capability, the speed of
developing services is very slow. Th is paper spreads cloud computing with
component based development named Cloud Co mputing Reusability Model (CCR) and
enable reusability in cloud computing. In this paper Cloud Co mputing
Reusability Model has been proposed. The model has been validated by Cloudsim
an d experimental result shows that reusability based cloud computing approach
is effective in minimizing cost and time to market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8014</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8014</id><created>2012-10-30</created><updated>2013-01-01</updated><authors><author><keyname>Labadens</keyname><forenames>Marc</forenames></author><author><keyname>Pomar&#xe8;de</keyname><forenames>Daniel</forenames></author><author><keyname>Chapon</keyname><forenames>Damien</forenames></author><author><keyname>Teyssier</keyname><forenames>Romain</forenames></author><author><keyname>Bournaud</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Renaud</keyname><forenames>Florent</forenames></author><author><keyname>Grandjouan</keyname><forenames>Nicolas</forenames></author></authors><title>Volume Rendering of AMR Simulations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution simulations often rely on the Adaptive Mesh Resolution (AMR)
technique to optimize memory consumption versus attainable precision. While
this technique allows for dramatic improvements in terms of computing
performance, the analysis and visualization of its data outputs remain
challenging. The lack of effective volume renderers for the octree-based AMR
used by the RAMSES simulation program has led to the development of the
solutions presented in this paper. Two custom algorithms are discussed, based
on the splatting and the ray-casting techniques. Their usage is illustrated in
the context of the visualization of a high-resolution, 6000-processor
simulation of a Milky Way-like galaxy. Performance obtained in terms of memory
management and parallelism speedup are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8024</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8024</id><created>2012-10-30</created><updated>2014-03-10</updated><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Emergent Spiking in Non-Ideal Memristor Networks</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>29 pages</comments><acm-class>B.3.1; B.5.2; C.1.3; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors have uses as artificial synapses and perform well in this role in
simulations with artificial spiking neurons. Our experiments show that
memristor networks natively spike and can exhibit emergent oscillations and
bursting spikes. Networks of near-ideal memristors exhibit behaviour similar to
a single memristor and combine in circuits like resistors do. Spiking is more
likely when filamentary memristors are used or the circuits have a higher
degree of compositional complexity (i.e. a larger number of anti-series or
anti-parallel interactions). 3-memristor circuits with the same memristor
polarity (low compositional complexity) are stabilised and do not show spiking
behaviour. 3-memristor circuits with anti-series and/or anti-parallel
compositions show richer and more complex dynamics than 2-memristor spiking
circuits. We show that the complexity of these dynamics can be quantified by
calculating (using partial auto-correlation functions) the minimum order
auto-regression function that could fit it. We propose that these oscillations
and spikes may be similar phenomena to brainwaves and neural spike trains and
suggest that these behaviours can be used to perform neuromorphic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8025</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8025</id><created>2012-10-18</created><authors><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author><author><keyname>Lam</keyname><forenames>Ka Chun</forenames></author><author><keyname>Wong</keyname><forenames>Tsz Wai</forenames></author><author><keyname>Gu</keyname><forenames>XianFeng</forenames></author></authors><title>Beltrami Representation and its applications to texture map and video
  compression</title><categories>cs.MM cs.GR math.DG</categories><comments>30 pages, 23 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Surface parameterizations and registrations are important in computer
graphics and imaging, where 1-1 correspondences between meshes are computed. In
practice, surface maps are usually represented and stored as 3D coordinates
each vertex is mapped to, which often requires lots of storage memory. This
causes inconvenience in data transmission and data storage. To tackle this
problem, we propose an effective algorithm for compressing surface
homeomorphisms using Fourier approximation of the Beltrami representation. The
Beltrami representation is a complex-valued function defined on triangular
faces of the surface mesh with supreme norm strictly less than 1. Under
suitable normalization, there is a 1-1 correspondence between the set of
surface homeomorphisms and the set of Beltrami representations. Hence, every
bijective surface map is associated with a unique Beltrami representation.
Conversely, given a Beltrami representation, the corresponding bijective
surface map can be exactly reconstructed using the Linear Beltrami Solver
introduced in this paper. Using the Beltrami representation, the surface
homeomorphism can be easily compressed by Fourier approximation, without
distorting the bijectivity of the map. The storage memory can be effectively
reduced, which is useful for many practical problems in computer graphics and
imaging. In this paper, we proposed to apply the algorithm to texture map
compression and video compression. With our proposed algorithm, the storage
requirement for the texture properties of a textured surface can be
significantly reduced. Our algorithm can further be applied to compressing
motion vector fields for video compression, which effectively improve the
compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8030</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8030</id><created>2012-10-30</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Budav&#xe1;ri</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Fluke</keyname><forenames>Christopher</forenames></author><author><keyname>Gray</keyname><forenames>Norman</forenames></author><author><keyname>Mann</keyname><forenames>Robert G</forenames></author><author><keyname>O'Mullane</keyname><forenames>William</forenames></author><author><keyname>Wicenec</keyname><forenames>Andreas</forenames></author><author><keyname>Wise</keyname><forenames>Michael</forenames></author></authors><title>Astronomy and Computing: a New Journal for the Astronomical Computing
  Community</title><categories>astro-ph.IM cs.DL</categories><comments>5 pages, no figures; editorial for first edition of journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce \emph{Astronomy and Computing}, a new journal for the growing
population of people working in the domain where astronomy overlaps with
computer science and information technology. The journal aims to provide a new
communication channel within that community, which is not well served by
current journals, and to help secure recognition of its true importance within
modern astronomy. In this inaugural editorial, we describe the rationale for
creating the journal, outline its scope and ambitions, and seek input from the
community in defining in detail how the journal should work towards its
high-level goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8031</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8031</id><created>2012-10-27</created><authors><author><keyname>Campbell</keyname><forenames>John O.</forenames></author></authors><title>Bayesian inference and the world mind</title><categories>cs.OH</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge is a central concept within both Bayesian inference and the
mathematical and philosophical program of logic and semiotics initiated by
Charles Sanders Peirce and further developed by George Spencer-Brown and Louis
Kauffman. The latter school is more philosophical than is usual with the
practitioners of Bayesian inference and claims the existence of a world mind.
When these two disciplines inform each other semiotics is provided with
mathematical mechanism and Bayesian inference is seen to be closely related to
the act of distinction, the fundamental basis of logic in the work of
Spencer-Brown. This hybridization also suggests a definition for knowledge
within Bayesian inference; a definition that has been curiously lacking. Given
that Darwinian processes are physical implementations of Bayesian inference and
are utilized within numerous scientific theories across a wide range of
disciplines as mechanisms for the creation and evolution of knowledge we may
view the conjunction of these theories, within universal Darwinism, as
descriptive of a world mind. Placing the world mind in this context provides
detailed support from the scientific literature and goes some way to refute the
charges of mysticism which have been leveled at the semiotic approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8055</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8055</id><created>2012-10-26</created><authors><author><keyname>Mandal</keyname><forenames>Sudhindu Bikash</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Sur-Kolay</keyname><forenames>Susmita</forenames></author></authors><title>A Synthesis Method for Quaternary Quantum Logic Circuits</title><categories>cs.OH quant-ph</categories><comments>10 pages</comments><journal-ref>Progress in VLSI Design and Test Lecture Notes in Computer Science
  Volume 7373, 2012, pp 270-280</journal-ref><doi>10.1007/978-3-642-31494-0_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis of quaternary quantum circuits involves basic quaternary gates and
logic operations in the quaternary quantum domain. In this paper, we propose
new projection operations and quaternary logic gates for synthesizing
quaternary logic functions. We also demonstrate the realization of the proposed
gates using basic quantum quaternary operations. We then employ our synthesis
method to design of quaternary adder and some benchmark circuits. Our results
in terms of circuit cost, are better than the existing works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8072</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8072</id><created>2012-10-30</created><authors><author><keyname>Struski</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Strict localization of eigenvectors and eigenvalues</title><categories>cs.NA math.NA</categories><msc-class>65F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we show and implement a simple and effcient method to
strictly locate eigenvectors and eigenvalues of a given matrix, based on the
modified cone condition. As a consequence we can also effectively localize
zeros of complex polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8083</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8083</id><created>2012-10-30</created><authors><author><keyname>Marro</keyname><forenames>Giovanni</forenames></author></authors><title>A Note on the Dimensions of the Structural Invariant Subspaces of the
  Discrete-Time Singular Hamiltonian Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structural invariant subspaces of the discrete-time singular Hamiltonian
system are used in 1] to give an analytic nonrecursive expression of all the
admissible trajectories. A deeper insight into the features of these subspaces,
particularly focused on the dimensionality issue, is the object of this note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8092</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8092</id><created>2012-10-30</created><authors><author><keyname>Kordy</keyname><forenames>Barbara</forenames></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames></author><author><keyname>Schweitzer</keyname><forenames>Patrick</forenames></author></authors><title>Quantitative Questions on Attack-Defense Trees</title><categories>cs.CR</categories><comments>technical report including formal pruning and additional figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attack-defense trees are a novel methodology for graphical security modeling
and assessment. The methodology includes visual, intuitive tree models whose
analysis is supported by a rigorous mathematical formalism. Both, the intuitive
and the formal components of the approach can be used for quantitative analysis
of attack-defense scenarios. In practice, we use intuitive questions to ask
about aspects of scenarios we are interested in. Formally, a computational
procedure, defined with the help of attribute domains and a bottom-up
algorithm, is applied to derive the corresponding numerical values.
  This paper bridges the gap between the intuitive and the formal way of
quantitatively assessing attack-defense scenarios. We discuss how to properly
specify a question, so that it can be answered unambiguously. Given a well
specified question, we then show how to derive an appropriate attribute domain
which constitutes the corresponding formal model. Since any attack tree is in
particular an attack-defense tree, our analysis is also an advancement of the
attack tree methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8099</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8099</id><created>2012-10-30</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>An Atypical Survey of Typical-Case Heuristic Algorithms</title><categories>cs.CC cs.AI cs.DS</categories><comments>This article is currently scheduled to appear in the December 2012
  issue of SIGACT News</comments><report-no>URCS-TR-2012-984</report-no><acm-class>F.1.3; I.2.8; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heuristic approaches often do so well that they seem to pretty much always
give the right answer. How close can heuristic algorithms get to always giving
the right answer, without inducing seismic complexity-theoretic consequences?
This article first discusses how a series of results by Berman, Buhrman,
Hartmanis, Homer, Longpr\'{e}, Ogiwara, Sch\&quot;{o}ening, and Watanabe, from the
early 1970s through the early 1990s, explicitly or implicitly limited how well
heuristic algorithms can do on NP-hard problems. In particular, many desirable
levels of heuristic success cannot be obtained unless severe, highly unlikely
complexity class collapses occur. Second, we survey work initiated by Goldreich
and Wigderson, who showed how under plausible assumptions deterministic
heuristics for randomized computation can achieve a very high frequency of
correctness. Finally, we consider formal ways in which theory can help explain
the effectiveness of heuristics that solve NP-hard problems in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8101</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8101</id><created>2012-10-30</created><updated>2013-01-21</updated><authors><author><keyname>Abreu</keyname><forenames>M.</forenames></author><author><keyname>Labbate</keyname><forenames>D.</forenames></author><author><keyname>Rizzi</keyname><forenames>R.</forenames></author><author><keyname>Sheehan</keyname><forenames>J.</forenames></author></authors><title>Odd 2-factored snarks</title><categories>math.CO cs.DM</categories><journal-ref>Eur. J. Combin. 36 (2014) 460--472</journal-ref><doi>10.1016/j.ejc.2013.09.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em snark} is a cubic cyclically 4-edge connected graph with edge
chromatic number four and girth at least five. We say that a graph $G$ is {\em
odd 2-factored} if for each 2-factor F of G each cycle of F is odd.
  In this paper, we present a method for constructing odd 2--factored snarks.
In particular, we construct two families of odd 2-factored snarks that disprove
a conjecture by some of the authors. Moreover, we approach the problem of
characterizing odd 2-factored snarks furnishing a partial characterization of
cyclically 4-edge connected odd 2-factored snarks. Finally, we pose a new
conjecture regarding odd 2-factored snarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8113</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8113</id><created>2012-10-30</created><authors><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Vaner</keyname><forenames>Michal</forenames></author></authors><title>A note on planar partial 3-trees</title><categories>cs.DM math.CO</categories><comments>6 pages, 3 figures. To be published (not decided where yet)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It implicitly follows from the work of [Colbourn, El-Mallah: On two dual
classes of planar graphs. Discrete Mathematics 80(1): 21-40 (1990)] that every
planar partial 3-tree is a subgraph of a planar 3-tree. This fact has already
enabled to prove a couple of results for planar partial 3-trees by induction on
the structure of the underlying planar 3-tree completion. We provide an
explicit proof of this observation and strengthen it by showing that one can
keep the plane drawing of the input graph unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8114</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8114</id><created>2012-10-30</created><updated>2015-06-17</updated><authors><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>Polynomial-time solutions of computational problems in
  noncommutative-algebraic cryptography</title><categories>cs.CR math.GR math.RT</categories><comments>Minor changes. Now published</comments><journal-ref>Journal of Cryptology 28 (2015), 601--622</journal-ref><doi>10.1007/s00145-013-9170-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the \emph{linear centralizer method}, and use it to devise a
provable polynomial time solution of the Commutator Key Exchange Problem, the
computational problem on which, in the passive adversary model, the security of
the Anshel--Anshel--Goldfeld 1999 \emph{Commutator} key exchange protocol is
based. We also apply this method to the computational problem underlying the
\emph{Centralizer} key exchange protocol, introduced by Shpilrain and Ushakov
in 2006.
  This is the first provable polynomial time cryptanalysis of the Commutator
key exchange protocol, hitherto the most important key exchange protocol in the
realm of noncommutative-algebraic cryptography, and the first cryptanalysis (of
any kind) of the Centralizer key exchange protocol. Unlike earlier
cryptanalyses of the Commutator key exchange protocol, our cryptanalyses cannot
be foiled by changing the distributions used in the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8116</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8116</id><created>2012-10-30</created><authors><author><keyname>Lim</keyname><forenames>Fabian</forenames></author><author><keyname>Stojanovic</keyname><forenames>Vladimir Marko</forenames></author></authors><title>On U-Statistics and Compressed Sensing I: Non-Asymptotic Average-Case
  Analysis</title><categories>cs.IT math.IT</categories><comments>12 pages. 3 pages supplementary material. Submitted to IEEE Trans.
  Signal Processing</comments><doi>10.1109/TSP.2013.2247598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hoeffding's U-statistics model combinatorial-type matrix parameters
(appearing in CS theory) in a natural way. This paper proposes using these
statistics for analyzing random compressed sensing matrices, in the
non-asymptotic regime (relevant to practice). The aim is to address certain
pessimisms of &quot;worst-case&quot; restricted isometry analyses, as observed by both
Blanchard &amp; Dossal, et. al.
  We show how U-statistics can obtain &quot;average-case&quot; analyses, by relating to
statistical restricted isometry property (StRIP) type recovery guarantees.
However unlike standard StRIP, random signal models are not required; the
analysis here holds in the almost sure (probabilistic) sense. For
Gaussian/bounded entry matrices, we show that both l1-minimization and LASSO
essentially require on the order of k \cdot [\log((n-k)/u) + \sqrt{2(k/n)
\log(n/k)}] measurements to respectively recover at least 1-5u fraction, and
1-4u fraction, of the signals. Noisy conditions are considered. Empirical
evidence suggests our analysis to compare well to Donoho &amp; Tanner's recent
large deviation bounds for l0/l1-equivalence, in the regime of block lengths
1000-3000 with high undersampling (50-150 measurements); similar system sizes
are found in recent CS implementation.
  In this work, it is assumed throughout that matrix columns are independently
sampled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8117</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8117</id><created>2012-10-30</created><authors><author><keyname>Lim</keyname><forenames>Fabian</forenames></author><author><keyname>Stojanovic</keyname><forenames>Vladimir</forenames></author></authors><title>On U-Statistics and Compressed Sensing II: Non-Asymptotic Worst-Case
  Analysis</title><categories>cs.IT math.IT</categories><comments>12 pages. Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2255041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In another related work, U-statistics were used for non-asymptotic
&quot;average-case&quot; analysis of random compressed sensing matrices. In this
companion paper the same analytical tool is adopted differently - here we
perform non-asymptotic &quot;worst-case&quot; analysis.
  Simple union bounds are a natural choice for &quot;worst-case&quot; analyses, however
their tightness is an issue (and questioned in previous works). Here we focus
on a theoretical U-statistical result, which potentially allows us to prove
that these union bounds are tight. To our knowledge, this kind of (powerful)
result is completely new in the context of CS. This general result applies to a
wide variety of parameters, and is related to (Stein-Chen) Poisson
approximation. In this paper, we consider i) restricted isometries, and ii)
mutual coherence. For the bounded case, we show that k-th order restricted
isometry constants have tight union bounds, when the measurements m =
\mathcal{O}(k (1 + \log(n/k))). Here we require the restricted isometries to
grow linearly in k, however we conjecture that this result can be improved to
allow them to be fixed. Also, we show that mutual coherence (with the standard
estimate \sqrt{(4\log n)/m}) have very tight union bounds.
  For coherence, the normalization complicates general discussion, and we
consider only Gaussian and Bernoulli cases here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8123</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8123</id><created>2012-10-30</created><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Limited-Capacity Many-To-Many Point Matching in One Dimension</title><categories>cs.CG cs.DS</categories><comments>20pages, 13figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two point sets S and T, in a many-to-many matching between S and T each
point in S is assigned to one or more points in T and vice versa. A
generalization of the many-to-many matching problem is the limited capacity
many-to-many matching problem, where the number of points that can be matched
to each point, that is the capacity of each point, is limited. In this paper,
we provide an O(n^2) time algorithm for the one dimensional minimum-cost
limited capacity many-to-many matching problem, where |S| + |T| = n. Our
algorithm improves the best previous time complexity of O(k(n^2)), that in
which k is the largest capacity of the points in the union of S and T. In this
problem, both S and T lie on the real line and the cost of matching s in S to t
in T is equal to the distance between s and t.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8124</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8124</id><created>2012-10-30</created><authors><author><keyname>Dhahri</keyname><forenames>Habib</forenames></author><author><keyname>Alimi</keyname><forenames>Mohamed Adel</forenames></author></authors><title>Hierarchical Learning Algorithm for the Beta Basis Function Neural
  Network</title><categories>cs.NE cs.AI</categories><journal-ref>Third International Conference on Systems, Signals &amp; Device, March
  21-24, 2005 , Sousse, Tunisia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a two-level learning method for the design of the Beta
Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the
upper level to construct BBFNN, while the key learning parameters :the width,
the centers and the Beta form are optimised using the gradient algorithm at the
lower level. In order to demonstrate the effectiveness of this hierarchical
learning algorithm HLABBFNN, we need to validate our algorithm for the
approximation of non-linear function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8129</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8129</id><created>2012-10-30</created><updated>2012-11-19</updated><authors><author><keyname>Narang</keyname><forenames>Sunil K.</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Compact Support Biorthogonal Wavelet Filterbanks for Arbitrary
  Undirected Graphs</title><categories>cs.IT cs.DC math.IT</categories><comments>Submitted for review in IEEE TSP</comments><doi>10.1109/TSP.2013.2273197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent work, we proposed the design of perfect reconstruction
orthogonal wavelet filterbanks, called graph- QMF, for arbitrary undirected
weighted graphs. In that formulation we first designed &quot;one-dimensional&quot;
two-channel filterbanks on bipartite graphs, and then extended them to
&quot;multi-dimensional&quot; separable two-channel filterbanks for arbitrary graphs via
a bipartite subgraph decomposition. We specifically designed wavelet filters
based on the spectral decomposition of the graph, and stated necessary and
sufficient conditions for a two-channel graph filter-bank on bipartite graphs
to provide aliasing-cancellation, perfect reconstruction and orthogonal set of
basis (orthogonality). While, the exact graph-QMF designs satisfy all the above
conditions, they are not exactly k-hop localized on the graph. In this paper,
we relax the condition of orthogonality to design a biorthogonal pair of
graph-wavelets that can have compact spatial spread and still satisfy the
perfect reconstruction conditions. The design is analogous to the standard
Cohen-Daubechies-Feauveau's (CDF) construction of factorizing a maximally-flat
Daubechies half-band filter. Preliminary results demonstrate that the proposed
filterbanks can be useful for both standard signal processing applications as
well as for signals defined on arbitrary graphs.
  Note: Code examples from this paper are available at
http://biron.usc.edu/wiki/index.php/Graph Filterbanks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8139</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8139</id><created>2012-10-30</created><authors><author><keyname>Provan</keyname><forenames>J. Scott</forenames></author><author><keyname>Brazil</keyname><forenames>Marcus</forenames></author><author><keyname>Thomas</keyname><forenames>Doreen</forenames></author><author><keyname>Weng</keyname><forenames>Jia F.</forenames></author></authors><title>Minimum Opaque Covers for Polygonal Regions</title><categories>cs.CG</categories><msc-class>68U05</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Opaque Cover Problem (OCP), also known as the Beam Detector Problem, is
the problem of finding, for a set S in Euclidean space, the minimum-length set
F which intersects every straight line passing through S. In spite of its
simplicity, the problem remains remarkably intractable. The aim of this paper
is to establish a framework and fundamental results for minimum opaque covers
where S is a polygonal region in two-dimensional space. We begin by giving some
general results about opaque covers, and describe the close connection that the
OCP has with the Point Goalie Problem. We then consider properties of graphical
solutions to the OCP when S is a convex polygonal region in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8165</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8165</id><created>2012-10-30</created><authors><author><keyname>Srivastava</keyname><forenames>Madhur</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Non-uniform Quantization of Detail Components in Wavelet Transformed
  Image for Lossy JPEG2000 Compression</title><categories>cs.MM</categories><comments>5 pages, 3 figures, conference</comments><journal-ref>International Conference on Pattern Recognition Applications and
  Methods (ICPRAM 2013)</journal-ref><doi>10.5220/0004333706040607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces the idea of non-uniform quantization in the detail
components of wavelet transformed image. It argues that most of the
coefficients of horizontal, vertical and diagonal components lie near to zeros
and the coefficients representing large differences are few at the extreme ends
of histogram. Therefore, this paper advocates need for variable step size
quantization scheme which preserves the edge information at the edge of
histogram and removes redundancy with the minimal number of quantized values.
To support the idea, preliminary results are provided using a non-uniform
quantization algorithm. We believe that successful implementation of
non-uniform quantization in detail components in JPEG-2000 still image standard
will improve image quality and compression efficiency with lesser number of
quantized values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8174</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8174</id><created>2012-10-30</created><updated>2012-11-02</updated><authors><author><keyname>Gargouri</keyname><forenames>Yassine</forenames></author><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author><author><keyname>Gingras</keyname><forenames>Yves</forenames></author><author><keyname>Brody</keyname><forenames>Tim</forenames></author><author><keyname>Carr</keyname><forenames>Les</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Testing the Finch Hypothesis on Green OA Mandate Ineffectiveness</title><categories>cs.DL</categories><comments>6 pages, 1 table, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We have now tested the Finch Committee's Hypothesis that Green Open Access
Mandates are ineffective in generating deposits in institutional repositories.
With data from ROARMAP on institutional Green OA mandates and data from ROAR on
institutional repositories, we show that deposit number and rate is
significantly correlated with mandate strength (classified as 1-12): The
stronger the mandate, the more the deposits. The strongest mandates generate
deposit rates of 70%+ within 2 years of adoption, compared to the un-mandated
deposit rate of 20%. The effect is already detectable at the national level,
where the UK, which has the largest proportion of Green OA mandates, has a
national OA rate of 35%, compared to the global baseline of 25%. The conclusion
is that, contrary to the Finch Hypothesis, Green Open Access Mandates do have a
major effect, and the stronger the mandate, the stronger the effect (the Liege
ID/OA mandate, linked to research performance evaluation, being the strongest
mandate model). RCUK (as well as all universities, research institutions and
research funders worldwide) would be well advised to adopt the strongest Green
OA mandates and to integrate institutional and funder mandates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8176</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8176</id><created>2012-10-30</created><authors><author><keyname>Urriza</keyname><forenames>Paulo</forenames></author><author><keyname>Rebeiz</keyname><forenames>Eric</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Eigenvalue-based Cyclostationary Spectrum Sensing Using Multiple
  Antennas</title><categories>cs.PF cs.IT math.IT stat.AP</categories><comments>6 pages, 6 figures, accepted to IEEE GLOBECOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a signal-selective spectrum sensing method for
cognitive radio networks and specifically targeted for receivers with
multiple-antenna capability. This method is used for detecting the presence or
absence of primary users based on the eigenvalues of the cyclic covariance
matrix of received signals. In particular, the cyclic correlation significance
test is used to detect a specific signal-of-interest by exploiting knowledge of
its cyclic frequencies. The analytical threshold for achieving constant false
alarm rate using this detection method is presented, verified through
simulations, and shown to be independent of both the number of samples used and
the noise variance, effectively eliminating the dependence on accurate noise
estimation. The proposed method is also shown, through numerical simulations,
to outperform existing multiple-antenna cyclostationary-based spectrum sensing
algorithms under a quasi-static Rayleigh fading channel, in both spatially
correlated and uncorrelated noise environments. The algorithm also has
significantly lower computational complexity than these other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8181</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8181</id><created>2012-10-30</created><authors><author><keyname>Hoven</keyname><forenames>Jeroen van den</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Gianotti</keyname><forenames>Fosca</forenames></author><author><keyname>Christen</keyname><forenames>Markus</forenames></author></authors><title>FuturICT - The Road towards Ethical ICT</title><categories>cs.CY cs.DL</categories><comments>arXiv admin note: text overlap with arXiv:1012.0178</comments><doi>10.1140/epjst/e2012-01691-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasive use of information and communication technology (ICT) in modern
societies enables countless opportunities for individuals, institutions,
businesses and scientists, but also raises difficult ethical and social
problems. In particular, ICT helped to make societies more complex and thus
harder to understand, which impedes social and political interventions to avoid
harm and to increase the common good. To overcome this obstacle, the
large-scale EU flagship proposal FuturICT intends to create a platform for
accessing global human knowledge as a public good and instruments to increase
our understanding of the information society by making use of ICT-based
research. In this contribution, we outline the ethical justification for such
an endeavor. We argue that the ethical issues raised by FuturICT research
projects overlap substantially with many of the known ethical problems emerging
from ICT use in general. By referring to the notion of Value Sensitive Design,
we show for the example of privacy how this core value of responsible ICT can
be protected in pursuing research in the framework of FuturICT. In addition, we
discuss further ethical issues and outline the institutional design of FuturICT
allowing to address them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8182</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8182</id><created>2012-10-30</created><updated>2013-01-10</updated><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Discovering Social Circles in Ego Networks</title><categories>cs.SI physics.soc-ph</categories><comments>30 pages, 10 figures, extended version of our NIPS 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People's personal social networks are big and cluttered, and currently there
is no good way to automatically organize them. Social networking sites allow
users to manually categorize their friends into social circles (e.g. 'circles'
on Google+, and 'lists' on Facebook and Twitter), however they are laborious to
construct and must be updated whenever a user's network grows. In this paper,
we study the novel task of automatically identifying users' social circles. We
pose this task as a multi-membership node clustering problem on a user's
ego-network, a network of connections between her friends. We develop a model
for detecting circles that combines network structure as well as user profile
information. For each circle we learn its members and the circle-specific user
profile similarity metric. Modeling node membership to multiple circles allows
us to detect overlapping as well as hierarchically nested circles. Experiments
show that our model accurately identifies circles on a diverse set of data from
Facebook, Google+, and Twitter, for all of which we obtain hand-labeled
ground-truth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8184</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8184</id><created>2012-10-30</created><authors><author><keyname>Ray</keyname><forenames>J.</forenames></author><author><keyname>Pinar</keyname><forenames>A.</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>A stopping criterion for Markov chains when generating independent
  random graphs</title><categories>cs.SI cs.DM physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov chains are convenient means of generating realizations of networks
with a given (joint or otherwise) degree distribution, since they simply
require a procedure for rewiring edges. The major challenge is to find the
right number of steps to run such a chain, so that we generate truly
independent samples. Theoretical bounds for mixing times of these Markov chains
are too large to be practically useful. Practitioners have no useful guide for
choosing the length, and tend to pick numbers fairly arbitrarily. We give a
principled mathematical argument showing that it suffices for the length to be
proportional to the number of desired number of edges. We also prescribe a
method for choosing this proportionality constant. We run a series of
experiments showing that the distributions of common graph properties converge
in this time, providing empirical evidence for our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8188</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8188</id><created>2012-10-30</created><updated>2013-04-02</updated><authors><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author><author><keyname>Kumar</keyname><forenames>K. Suresh</forenames></author></authors><title>Relative Value Iteration for Stochastic Differential Games</title><categories>math.OC cs.SY</categories><msc-class>93E15, 93E20 (Primary) 60J25, 60J60, 90C40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study zero-sum stochastic differential games with player dynamics governed
by a nondegenerate controlled diffusion process. Under the assumption of
uniform stability, we establish the existence of a solution to the Isaac's
equation for the ergodic game and characterize the optimal stationary
strategies. The data is not assumed to be bounded, nor do we assume geometric
ergodicity. Thus our results extend previous work in the literature. We also
study a relative value iteration scheme that takes the form of a parabolic
Isaac's equation. Under the hypothesis of geometric ergodicity we show that the
relative value iteration converges to the elliptic Isaac's equation as time
goes to infinity. We use these results to establish convergence of the relative
value iteration for risk-sensitive control problems under an asymptotic
flatness assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8189</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8189</id><created>2012-10-30</created><updated>2013-01-29</updated><authors><author><keyname>Raggi</keyname><forenames>Miguel</forenames></author></authors><title>Forbidden Configurations: Finding the number predicted by the
  Anstee-Sali Conjecture is NP-hard</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let F be a hypergraph and let forb(m,F) denote the maximum number of edges a
hypergraph with m vertices can have if it doesn't contain F as a subhypergraph.
A conjecture of Anstee and Sali predicts the asymptotic behaviour of forb(m,F)
for fixed F. In this paper we prove that even finding this predicted asymptotic
behaviour is an NP-hard problem, meaning that if the Anstee-Sali conjecture
were true, finding the asymptotics of forb(m,F) would be NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8191</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8191</id><created>2012-10-30</created><updated>2012-11-13</updated><authors><author><keyname>Eraslan</keyname><forenames>Eren</forenames></author><author><keyname>Daneshrad</keyname><forenames>Babak</forenames></author><author><keyname>Lou</keyname><forenames>Chung-Yu</forenames></author></authors><title>Performance Indicator for MIMO MMSE Receivers in the Presence of Channel
  Estimation Error</title><categories>cs.PF cs.IT cs.NI math.IT</categories><comments>4 pages, 3 figures. Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the derivation of post-processing SNR for
Minimum-Mean-Squared-Error (MMSE) receivers with imperfect channel estimates,
and show that it is an accurate indicator of the error rate performance of MIMO
systems in the presence of channel estimation error. Simulation results show
the tightness of the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8193</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8193</id><created>2012-10-30</created><authors><author><keyname>Lucatero</keyname><forenames>Carlos Rodr&#xed;guez</forenames></author><author><keyname>Alarc&#xf3;n</keyname><forenames>Luis</forenames></author><author><keyname>Jaquez</keyname><forenames>Roberto Bernal</forenames></author><author><keyname>Schaum</keyname><forenames>Alexander</forenames></author></authors><title>Decision dynamics in complex networks subject to mass media and social
  contact transmission mechanisms</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 8 figures, submitted to Advances in Complex Systems</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The dynamics of decisions in complex networks is studied within a Markov
process framework using numerical simulations combined with mathematical
insight into the process mechanisms. A mathematical discrete-time model is
derived based on a set of basic assumptions on the convincing mechanisms
associated to two opinions. The model is analyzed with respect to multiplicity
of critical points, illustrating in this way the main behavior to be expected
in the network. Particular interest is focussed on the effect of social network
and exogenous mass media-based influences on the decision behavior. A set of
numerical simulation results is provided illustrating how these mechanisms
impact the final decision results. The analysis reveals (i) the presence of
fixed-point multiplicity (with a maximum of four different fixed points),
multistability, and sensitivity with respect to process parameters, and (ii)
that mass media have a strong impact on the decision behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8194</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8194</id><created>2012-10-30</created><updated>2014-11-17</updated><authors><author><keyname>Acharya</keyname><forenames>Anish</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author></authors><title>Extending the Concept of Analog Butterworth Filter for Fractional Order
  Systems</title><categories>cs.SY</categories><comments>21 pages, 5 figures</comments><journal-ref>Signal Processing, Volume 94, Pages 409-420, January 2014</journal-ref><doi>10.1016/j.sigpro.2013.07.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the design of Fractional Order (FO) Butterworth filter in
complex w-plane (w=sq; q being any real number) considering the presence of
under-damped, hyper-damped, ultra-damped poles. This is the first attempt to
design such fractional Butterworth filters in complex w-plane instead of
complex s-plane, as conventionally done for integer order filters. Firstly, the
concept of fractional derivatives and w-plane stability of linear fractional
order systems are discussed. Detailed mathematical formulation for the design
of fractional Butterworth-like filter (FBWF) in w-plane is then presented.
Simulation examples are given along with a practical example to design the FO
Butterworth filter with given specifications in frequency domain to show the
practicability of the proposed formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8196</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8196</id><created>2012-10-30</created><authors><author><keyname>Pakhira</keyname><forenames>Anindya</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Acharya</keyname><forenames>Anish</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Saha</keyname><forenames>Suman</forenames></author></authors><title>Optimized Quality Factor of Fractional Order Analog Filters with
  Band-Pass and Band-Stop Characteristics</title><categories>cs.SY math.OC</categories><comments>6 pages, 13 figures; 2012 Third International Conference on
  Computing, Communication and Networking Technologies (ICCCNT'12), July 2012,
  Coimbatore</comments><doi>10.1109/ICCCNT.2012.6396000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional order (FO) filters have been investigated in this paper, with
band-pass (BP) and band-stop (BS) characteristics, which can not be achieved
with conventional integer order filters with orders lesser then two. The
quality factors for symmetric and asymmetric magnitude response have been
optimized using real coded Genetic Algorithm (GA) for a user specified center
frequency. Parametric influence of the FO filters on the magnitude response is
also illustrated with credible numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8197</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8197</id><created>2012-10-30</created><updated>2013-01-05</updated><authors><author><keyname>Routh</keyname><forenames>Avijit</forenames></author><author><keyname>Das</keyname><forenames>Sourav</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author></authors><title>Stabilization Based Networked Predictive Controller Design for Switched
  Plants</title><categories>cs.SY math.OC</categories><comments>6 pages, 4 figures</comments><journal-ref>Computing Communication &amp; Networking Technologies (ICCCNT), 2012
  Third International Conference on, July 2012, Coimbatore</journal-ref><doi>10.1109/ICCCNT.2012.6396001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stabilizing state feedback controller has been designed in this paper for a
switched DC motor plant, controlled over communication network. The switched
system formulation for the networked control system (NCS) with additional
switching in a plant parameter along with the switching due to random packet
losses, have been formulated as few set of non-strict Linear Matrix
Inequalities (LMIs). In order to solve non-strict LMIs using standard LMI
solver and to design the stabilizing state feedback controller, the Cone
Complementary Linearization (CCL) technique has been adopted. Simulation
studies have been carried out for a DC motor plant, operating at two different
sampling times with random switching in the moment of inertia, representing
sudden jerks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8220</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8220</id><created>2012-10-30</created><updated>2012-11-27</updated><authors><author><keyname>Gibson</keyname><forenames>Travis E.</forenames></author><author><keyname>Annaswamy</keyname><forenames>Anuradha M.</forenames></author><author><keyname>Lavretsky</keyname><forenames>Eugene</forenames></author></authors><title>Closed-loop Reference Models for Output-Feedback Adaptive Systems</title><categories>math.OC cs.SY nlin.AO</categories><comments>v1 Submitted to European Control Conference 2013, v2 Typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Closed-loop reference models have recently been proposed for states
accessible adaptive systems. They have been shown to have improved transient
response over their open loop counter parts. The results in the states
accessible case are extended to single input single output plants of arbitrary
relative degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8223</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8223</id><created>2012-10-30</created><authors><author><keyname>Wanless</keyname><forenames>Ian M.</forenames></author><author><keyname>Zhang</keyname><forenames>Xiande</forenames></author></authors><title>On the Existence of Retransmission Permutation Arrays</title><categories>math.CO cs.IT math.IT</categories><comments>8 pages</comments><msc-class>05B15</msc-class><journal-ref>Discrete Appl. Math. 161 (2013), 2772-2777</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate retransmission permutation arrays (RPAs) that are motivated by
applications in overlapping channel transmissions. An RPA is an $n\times n$
array in which each row is a permutation of ${1, ..., n}$, and for $1\leq i\leq
n$, all $n$ symbols occur in each $i\times\lceil\frac{n}{i}\rceil$ rectangle in
specified corners of the array. The array has types 1, 2, 3 and 4 if the stated
property holds in the top left, top right, bottom left and bottom right
corners, respectively. It is called latin if it is a latin square. We show that
for all positive integers $n$, there exists a type-$1,2,3,4$ $\RPA(n)$ and a
type-1,2 latin $\RPA(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8229</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8229</id><created>2012-10-31</created><authors><author><keyname>K.</keyname><forenames>Jnanamurthy H.</forenames></author><author><keyname>V.</keyname><forenames>Vishesh H.</forenames></author><author><keyname>Jain</keyname><forenames>Vishruth</forenames></author><author><keyname>Kumar</keyname><forenames>Preetham</forenames></author><author><keyname>Pai</keyname><forenames>Radhika M.</forenames></author></authors><title>Top Down Approach to find Maximal Frequent Item Sets using Subset
  Creation</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Association rule has been an area of active research in the field of
knowledge discovery. Data mining researchers had improved upon the quality of
association rule mining for business development by incorporating influential
factors like value (utility), quantity of items sold (weight) and more for the
mining of association patterns. In this paper, we propose an efficient approach
to find maximal frequent itemset first. Most of the algorithms in literature
used to find minimal frequent item first, then with the help of minimal
frequent itemsets derive the maximal frequent itemsets. These methods consume
more time to find maximal frequent itemsets. To overcome this problem, we
propose a navel approach to find maximal frequent itemset directly using the
concepts of subsets. The proposed method is found to be efficient in finding
maximal frequent itemsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8234</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8234</id><created>2012-10-31</created><updated>2012-12-10</updated><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>The hyperbolic Voronoi diagram in arbitrary dimension</title><categories>cs.CG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that in the Klein projective ball model of hyperbolic space, the
hyperbolic Voronoi diagram is affine and amounts to clip a corresponding power
diagram, requiring however algebraic arithmetic. By considering the
lesser-known Beltrami hemisphere model of hyperbolic geometry, we overcome the
arithmetic limitations of Klein construction. Finally, we characterize the
bisectors and geodesics in the other Poincar\' e upper half-space, the
Poincar\'e ball, and the Lorentz hyperboloid models, and discusses on
degenerate cases for which the dual hyperbolic Delaunay complex is not a
triangulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8242</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8242</id><created>2012-10-31</created><authors><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author></authors><title>Pipelined Workflow in Hybrid MPI/Pthread runtime for External Memory
  Graph Construction</title><categories>cs.DB cs.DC</categories><comments>12 pages</comments><acm-class>C.2.4; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph construction from a given set of edges is a data-intensive operator
that appears in social network analysis, ontology enabled databases, and, other
analytics processing. The operator represents an edge list to compressed sparse
row (CSR) representation (or sometimes in adjacency list, or as clustered
B-Tree storage). In this work, we show how to scale CSR construction to massive
scale on SSD-enabled supercomputers such as Gordon using pipelined processing.
We develop several abstraction and operations for external memory and parallel
edge list and integer array processing that are utilized towards building a
scalable algorithm for creating CSR representation.
  Our experiments demonstrate that this scheme is four to six times faster than
currently available implementation. Moreover, our scheme can handle up to 8
billion edges (128GB) by using external memory as compared to prior schemes
where performance degrades considerably for edge list size 26 million and
beyond.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8253</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8253</id><created>2012-10-31</created><authors><author><keyname>Guskov</keyname><forenames>George K.</forenames></author><author><keyname>Mogilnykh</keyname><forenames>Ivan Yu.</forenames></author><author><keyname>Solov'eva</keyname><forenames>Faina I.</forenames></author></authors><title>Ranks of propelinear perfect binary codes</title><categories>math.CO cs.IT math.IT</categories><msc-class>94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proven that for any numbers n=2^m-1, m &gt;= 4 and r, such that n -
log(n+1)&lt;= r &lt;= n excluding n = r = 63, n = 127, r in {126,127} and n = r =
2047 there exists a propelinear perfect binary code of length n and rank r.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8260</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8260</id><created>2012-10-31</created><updated>2013-03-13</updated><authors><author><keyname>Massar</keyname><forenames>Marc</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author></authors><title>Mean Field Theory of Dynamical Systems Driven by External Signals</title><categories>nlin.CD cond-mat.dis-nn cs.AI</categories><comments>7 pages, 6 figures</comments><journal-ref>Physical Review E 87, 042809 (2013)</journal-ref><doi>10.1103/PhysRevE.87.042809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical systems driven by strong external signals are ubiquituous in nature
and engineering. Here we study &quot;echo state networks&quot;, networks of a large
number of randomly connected nodes, which represent a simple model of a neural
network, and have important applications in machine learning. We develop a mean
field theory of echo state networks. The dynamics of the network is captured by
the evolution law, similar to a logistic map, for a single collective variable.
When the network is driven by many independent external signals, this
collective variable reaches a steady state. But when the network is driven by a
single external signal, the collective variable is nonstationnary but can be
characterised by its time averaged distribution. The predictions of the mean
field theory, including the value of the largest Lyaponuov exponent, are
compared with the numerical integration of the equations of motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8262</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8262</id><created>2012-10-31</created><authors><author><keyname>Rebagliati</keyname><forenames>Nicola</forenames></author><author><keyname>Sol&#xe9;-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>Pelillo</keyname><forenames>Marcello</forenames></author><author><keyname>Serratosa</keyname><forenames>Francesc</forenames></author></authors><title>On the Relation Between the Common Labelling and the Median Graph</title><categories>cs.CV</categories><comments>12 pages, 2 figures. Published in Structural, Syntactic, And
  Statistical Pattern Recognition Lecture Notes in Computer Science, 2012. The
  original publication is available at
  http://www.springerlink.com/content/e524g4483g146383/</comments><journal-ref>Lecture Notes in Computer Science, 2012, Volume 7626/2012, 107-115</journal-ref><doi>10.1007/978-3-642-34166-31_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In structural pattern recognition, given a set of graphs, the computation of
a Generalized Median Graph is a well known problem. Some methods approach the
problem by assuming a relation between the Generalized Median Graph and the
Common Labelling problem. However, this relation has still not been formally
proved. In this paper, we analyse such relation between both problems. The main
result proves that the cost of the common labelling upper-bounds the cost of
the median with respect to the given set. In addition, we show that the two
problems are equivalent in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8267</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8267</id><created>2012-10-31</created><authors><author><keyname>Yardi</keyname><forenames>Arti D.</forenames></author><author><keyname>Vijayakumaran</keyname><forenames>Saravanan</forenames></author></authors><title>Detecting Linear Block Codes in Noise using the GLRT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of distinguishing the noisy codewords
of a known binary linear block code from a random bit sequence. We propose to
use the generalized likelihood ratio test (GLRT) to solve this problem. We also
give a formula to find approximate number of codewords required and compare our
results with an existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8270</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8270</id><created>2012-10-31</created><authors><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author></authors><title>Non-associative public-key cryptography</title><categories>cs.CR math.GR</categories><comments>32 pages</comments><msc-class>20N02 (Primary) 20F36 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalized Anshel-Anshel-Goldfeld (AAG) key establishment
protocol (KEP) for magmas. This leads to the foundation of non-associative
public-key cryptography (PKC), generalizing the concept of non-commutative PKC.
We show that left selfdistributive systems appear in a natural special case of
a generalized AAG-KEP for magmas, and we propose, among others instances,
concrete realizations using $f$-conjugacy in groups and shifted conjugacy in
braid groups. We discuss the advantages of our schemes compared with the
classical AAG-KEP based on conjugacy in braid groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8291</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8291</id><created>2012-10-31</created><authors><author><keyname>Chen</keyname><forenames>Huanhuan</forenames></author><author><keyname>Tino</keyname><forenames>Peter</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author><author><keyname>Rodan</keyname><forenames>Ali</forenames></author></authors><title>Learning in the Model Space for Fault Diagnosis</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of large scaled sensor networks facilitates the collection of
large amounts of real-time data to monitor and control complex engineering
systems. However, in many cases the collected data may be incomplete or
inconsistent, while the underlying environment may be time-varying or
un-formulated. In this paper, we have developed an innovative cognitive fault
diagnosis framework that tackles the above challenges. This framework
investigates fault diagnosis in the model space instead of in the signal space.
Learning in the model space is implemented by fitting a series of models using
a series of signal segments selected with a rolling window. By investigating
the learning techniques in the fitted model space, faulty models can be
discriminated from healthy models using one-class learning algorithm. The
framework enables us to construct fault library when unknown faults occur,
which can be regarded as cognitive fault isolation. This paper also
theoretically investigates how to measure the pairwise distance between two
models in the model space and incorporates the model distance into the learning
algorithm in the model space. The results on three benchmark applications and
one simulated model for the Barcelona water distribution network have confirmed
the effectiveness of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8293</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8293</id><created>2012-10-31</created><updated>2012-11-22</updated><authors><author><keyname>Nagarjun</keyname><forenames>K. P.</forenames></author><author><keyname>Sivaranjani</keyname><forenames>S.</forenames></author><author><keyname>Koshy</keyname><forenames>George</forenames></author></authors><title>Lyapunov Control of Quantum Systems with Applications to Quantum
  Computing</title><categories>cs.SY math.OC</categories><comments>A final version will be uploaded with significant expansions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the design of complex quantum systems like ion traps for quantum
computing, it is usually desired to stabilize a particular system state or make
the system state track a desired trajectory. Several control theoretical
approaches based on feedback seem attractive to solve such problems. But the
uncertain dynamics introduced by measurement on quantum systems makes the
synthesis of feedback control laws very complicated. Although we have not
explicitly modeled the change in system dynamics due to measurement (we have
assumed weak measurements), this is a first step towards a more detailed
analysis and closed-loop feedback design. Here, we present a Lyapunov-based
control approach on the lines of that developed by Mirrahimi, Rouchon, Turnici
(2005). The states are assumed to be obtained from weak measurements. The
Lyapunov control technique has not been applied to realistic quantum systems so
far. We have extended and applied the technique to two realistic physical
systems - the quantum harmonic oscillator and the n-qubit system. We also
propose to extend this concept to ion traps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8296</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8296</id><created>2012-10-31</created><updated>2015-09-06</updated><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Chen</keyname><forenames>Han-Fu</forenames></author></authors><title>Parameter Estimation of Switched Hammerstein Systems</title><categories>cs.SY math.OC</categories><comments>16 pages, 3 figures; Accepted for publication by Acta Mathematicae
  Applicatae Sinica (http://link.springer.com/journal/10255)</comments><msc-class>93E12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the parameter estimation problem of the
Single-Input-Single-Output (SISO) switched Hammerstein system. Suppose that the
switching law is arbitrary but can be observed online. All subsystems are
parameterized and the Recursive Least Squares (RLS) algorithm is applied to
estimate their parameters. To overcome the difficulty caused by coupling of
data from different subsystems, the concept &quot;intrinsic switch&quot; is introduced.
Two cases are considered: i) The input is taken to be a sequence of independent
identically distributed (i.i.d.) random variables when identification is the
only purpose; ii) A diminishingly excited signal is superimposed on the control
when the adaptive control law is given. The strong consistency of the estimates
in both cases is established and a simulation example is given to verify the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8302</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8302</id><created>2012-10-31</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Marra</keyname><forenames>Vincenzo</forenames></author></authors><title>The logical content of triangular bases of fuzzy sets in {\L}ukasiewicz
  infinite-valued logic</title><categories>cs.LO math.LO</categories><comments>Preprint submitted to Fuzzy Sets and Systems</comments><msc-class>03B50, 03B52</msc-class><doi>10.1016/j.fss.2013.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuing to pursue a research direction that we already explored in
connection with G\&quot;odel-Dummett logic and Ruspini partitions, we show here that
{\L}ukasiewicz logic is able to express the notion of pseudo-triangular basis
of fuzzy sets, a mild weakening of the standard notion of triangular basis. En
route to our main result we obtain an elementary, logic-independent
characterisation of triangular bases of fuzzy sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8303</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8303</id><created>2012-10-31</created><updated>2013-03-07</updated><authors><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author></authors><title>Dominator Tree Certification and Independent Spanning Trees</title><categories>cs.DS</categories><comments>Rewritten abstract and introduction. Added references</comments><acm-class>D.3.4; E.1; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does one verify that the output of a complicated program is correct? One
can formally prove that the program is correct, but this may be beyond the
power of existing methods. Alternatively one can check that the output produced
for a particular input satisfies the desired input-output relation, by running
a checker on the input-output pair. Then one only needs to prove the
correctness of the checker. But for some problems even such a checker may be
too complicated to formally verify. There is a third alternative: augment the
original program to produce not only an output but also a correctness
certificate, with the property that a very simple program (whose correctness is
easy to prove) can use the certificate to verify that the input-output pair
satisfies the desired input-output relation.
  We consider the following important instance of this general question: How
does one verify that the dominator tree of a flow graph is correct? Existing
fast algorithms for finding dominators are complicated, and even verifying the
correctness of a dominator tree in the absence of additional information seems
complicated. We define a correctness certificate for a dominator tree, show how
to use it to easily verify the correctness of the tree, and show how to augment
fast dominator-finding algorithms so that they produce a correctness
certificate. We also relate the dominator certificate problem to the problem of
finding independent spanning trees in a flow graph, and we develop algorithms
to find such trees. All our algorithms run in linear time. Previous algorithms
apply just to the special case of only trivial dominators, and they take at
least quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8318</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8318</id><created>2012-10-31</created><authors><author><keyname>Chennamma</keyname><forenames>H. R.</forenames></author><author><keyname>Rangarajan</keyname><forenames>Lalitha</forenames></author></authors><title>Mugshot Identification from Manipulated Facial Images</title><categories>cs.CV cs.MM</categories><comments>8 pages, 5 figures, 1 table, journal. arXiv admin note: substantial
  text overlap with arXiv:1106.4907</comments><journal-ref>International Journal of Machine Intelligence, Volume 4, Issue 1,
  pp. 407, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Editing on digital images is ubiquitous. Identification of deliberately
modified facial images is a new challenge for face identification system. In
this paper, we address the problem of identification of a face or person from
heavily altered facial images. In this face identification problem, the input
to the system is a manipulated or transformed face image and the system reports
back the determined identity from a database of known individuals. Such a
system can be useful in mugshot identification in which mugshot database
contains two views (frontal and profile) of each criminal. We considered only
frontal view from the available database for face identification and the query
image is a manipulated face generated by face transformation software tool
available online. We propose SIFT features for efficient face identification in
this scenario. Further comparative analysis has been given with well known
eigenface approach. Experiments have been conducted with real case images to
evaluate the performance of both methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8326</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8326</id><created>2012-10-31</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>General BER Expression for One-Dimensional Constellations</title><categories>cs.IT math.IT</categories><comments>To appear in the Proceedings of the IEEE Global Communications
  Conference (GLOBECOM) 2012. Remark 3 modified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel general ready-to-use bit-error rate (BER) expression for
one-dimensional constellations is developed. The BER analysis is performed for
bit patterns that form a labeling. The number of patterns for equally spaced
M-PAM constellations with different BER is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8338</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8338</id><created>2012-10-31</created><updated>2014-04-08</updated><authors><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Goldhirsh</keyname><forenames>Yonatan</forenames></author><author><keyname>Matsliah</keyname><forenames>Arie</forenames></author></authors><title>On the Power of Conditional Samples in Distribution Testing</title><categories>cs.DS cs.CC math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define and examine the power of the {\em
conditional-sampling} oracle in the context of distribution-property testing.
The conditional-sampling oracle for a discrete distribution $\mu$ takes as
input a subset $S \subset [n]$ of the domain, and outputs a random sample $i
\in S$ drawn according to $\mu$, conditioned on $S$ (and independently of all
prior samples). The conditional-sampling oracle is a natural generalization of
the ordinary sampling oracle in which $S$ always equals $[n]$.
  We show that with the conditional-sampling oracle, testing uniformity,
testing identity to a known distribution, and testing any label-invariant
property of distributions is easier than with the ordinary sampling oracle. On
the other hand, we also show that for some distribution properties the
sample-complexity remains near-maximal even with conditional sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8339</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8339</id><created>2012-10-31</created><authors><author><keyname>Gawron</keyname><forenames>Piotr</forenames></author><author><keyname>Kurzyk</keyname><forenames>Dariusz</forenames></author><author><keyname>Pucha&#x142;a</keyname><forenames>Zbigniew</forenames></author></authors><title>A model for quantum queue</title><categories>quant-ph cs.OH</categories><comments>14 pages, 7 figures</comments><journal-ref>Int. J. Quantum Inform. 11, 1350023 (2013)</journal-ref><doi>10.1142/S0219749913500238</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an extension of Discrete Time Markov Chain queueing model to the
quantum domain by use of Discrete Time Quantum Markov Chain. We introduce
methods for numerical analysis of such models. Using this tools we show that
quantum model behaves fundamentally differently from the classical one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8353</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8353</id><created>2012-10-31</created><authors><author><keyname>H&#xe4;usler</keyname><forenames>Chris</forenames></author><author><keyname>Susemihl</keyname><forenames>Alex</forenames></author></authors><title>Temporal Autoencoding Restricted Boltzmann Machine</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much work has been done refining and characterizing the receptive fields
learned by deep learning algorithms. A lot of this work has focused on the
development of Gabor-like filters learned when enforcing sparsity constraints
on a natural image dataset. Little work however has investigated how these
filters might expand to the temporal domain, namely through training on natural
movies. Here we investigate exactly this problem in established temporal deep
learning algorithms as well as a new learning paradigm suggested here, the
Temporal Autoencoding Restricted Boltzmann Machine (TARBM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8365</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8365</id><created>2012-10-29</created><authors><author><keyname>Chang</keyname><forenames>M.</forenames></author><author><keyname>Hung</keyname><forenames>L.</forenames></author><author><keyname>Kloks</keyname><forenames>T.</forenames></author><author><keyname>Peng</keyname><forenames>S.</forenames></author></authors><title>On the threshold-width of graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GG-width of a class of graphs GG is defined as follows. A graph G has
GG-width k if there are k independent sets N1,...,Nk in G such that G can be
embedded into a graph H in GG such that for every edge e in H which is not an
edge in G, there exists an i such that both endpoints of e are in Ni. For the
class TH of threshold graphs we show that TH-width is NP-complete and we
present fixed-parameter algorithms. We also show that for each k, graphs of
TH-width at most k are characterized by a finite collection of forbidden
induced subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8368</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8368</id><created>2012-10-31</created><updated>2013-03-17</updated><authors><author><keyname>B&#xfc;rgisser</keyname><forenames>Peter</forenames></author><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author></authors><title>Explicit Lower Bounds via Geometric Complexity Theory</title><categories>cs.CC math.RT</categories><comments>10 pages, 2 figures</comments><msc-class>68Q17, 20C30, 14L24</msc-class><acm-class>F.1.3; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the lower bound R(M_m) \geq 3/2 m^2 - 2 on the border rank of m x m
matrix multiplication by exhibiting explicit representation theoretic
(occurence) obstructions in the sense of the geometric complexity theory (GCT)
program. While this bound is weaker than the one recently obtained by Landsberg
and Ottaviani, these are the first significant lower bounds obtained within the
GCT program. Behind the proof is the new combinatorial concept of obstruction
designs, which encode highest weight vectors in Sym^d\otimes^3(C^n)^* and
provide new insights into Kronecker coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8375</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8375</id><created>2012-10-31</created><updated>2013-09-14</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>Cryptanalysis of a New Knapsack Type Public-Key Cryptosystem</title><categories>cs.CR</categories><comments>International Conference on Applied Mathematics and Computer
  Sciences, Rio de Janeiro, Brazil, March 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Hwang et al. introduced a knapsack type public-key cryptosystem.
They proposed a new algorithm called permutation combination algorithm. By
exploiting this algorithm, they attempt to increase the density of knapsack to
avoid the low-density attack.
  We show that this cryptosystem is not secure, as it based on basic
Merkel-Hellman knapsack cryptosystem and because of the superincreasing
structure, we can use shamir's attack on the basic Merkel-Hellman knapsack to
break this cryptosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8378</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8378</id><created>2012-10-31</created><updated>2012-11-05</updated><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M. S.</forenames></author></authors><title>Development of a Dual Sensor Heat Control System</title><categories>cs.SY</categories><comments>16 pages, 10 figures, journal article</comments><journal-ref>International Journal of Instrumentation and Control Systems
  (IJICS), vol. 2(4), pp. 11-26, 2012</journal-ref><doi>10.5121/ijics.2012.2402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convenience and safeguarding our home appliances have become an important
issue when dealing with an advancement and growth of an economy. This research
focuses on the design and construction of a Dual Sensor heat-monitoring system.
The circuit works by monitoring temperature from an external input and
comparing the temperature level with that of a preset temperature value. The
power output of the circuit is cut off or switched OFF or an alarm is triggered
ON if the temperature of the external input is equal to or, greater than the
preset temperature value. The methodology involves the application of linear
precision temperature sensors i.e., they generate a voltage that is directly
proportional to the temperature. Basically the system is constructed using
temperature sensors and comparators. The system is powered using a 12V power
supply. The results of the tests showed that the power output of the circuit is
switched OFF hence switching OFF the heating device or an alarm is triggered ON
when the device exceeded a preset temperature level. The general operation of
the system and performance is dependent on the temperature difference between
the preset temperature value and external temperature intended to be monitored.
The overall system was tested and found perfectly functional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8385</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8385</id><created>2012-10-31</created><authors><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Steunebrink</keyname><forenames>Bas R.</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>First Experiments with PowerPlay</title><categories>cs.AI cs.LG</categories><comments>13 pages, 6 figures. Extends preliminary work presented at
  ICDL-EpiRob 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Like a scientist or a playing child, PowerPlay not only learns new skills to
solve given problems, but also invents new interesting problems by itself. By
design, it continually comes up with the fastest to find, initially novel, but
eventually solvable tasks. It also continually simplifies or compresses or
speeds up solutions to previous tasks. Here we describe first experiments with
PowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a
general computational problem solving architecture. Its connection weights can
encode arbitrary, self-delimiting, halting or non-halting programs affecting
both environment (through effectors) and internal states encoding abstractions
of event sequences. Our PowerPlay-driven SLIM RNN learns to become an
increasingly general solver of self-invented problems, continually adding new
problem solving procedures to its growing skill repertoire. Extending a recent
conference paper, we identify interesting, emerging, developmental stages of
our open-ended system. We also show how it automatically self-modularizes,
frequently re-using code for previously invented skills, always trying to
invent novel tasks that can be quickly validated because they do not require
too many weight changes affecting too many previous tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8386</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8386</id><created>2012-10-31</created><updated>2012-11-15</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>Grammar-Based Construction of Indexes for Binary Jumbled Pattern
  Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how, given a straight-line program with $g$ rules for a binary string
$B$ of length $n$, in $O(g^{2 / 3} n^{4 / 3})$ time we can build a linear-space
index such that, given $m$ and $c$, in O(1) time we can determine whether there
is a substring of $B$ with length $m$ containing exactly $c$ copies of 1. If we
use $O(n \log n)$ space for the index, then we can list all such substrings
using $O(m)$ time per substring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8398</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8398</id><created>2012-10-31</created><authors><author><keyname>Hosangadi</keyname><forenames>Sandeep</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>An Alignment Algorithm for Sequences</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new alignment algorithm for sequences that can be used
for determination of deletions and substitutions. It provides several solutions
out of which the best one can be chosen on the basis of minimization of gaps or
other considerations. The algorithm does not use similarity tables and it
performs aspects of both global and local alignment. The algorithm is compared
with other sequence alignment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8400</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8400</id><created>2012-10-31</created><authors><author><keyname>Sun</keyname><forenames>John Z.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K.</forenames></author></authors><title>Distributed Quantization Networks</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. on Communications, vol. 61, no. 9, pp. 3931-3942,
  September 2013</journal-ref><doi>10.1109/TCOMM.2013.071813.120833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several key results in distributed source coding offer the intuition that
little improvement in compression can be gained from intersensor communication
when the information is coded in long blocks. However, when sensors are
restricted to code their observations in small blocks (e.g., 1), intelligent
collaboration between sensors can greatly reduce distortion. For networks where
sensors are allowed to &quot;chat&quot; using a side channel that is unobservable at the
fusion center, we provide asymptotically-exact characterization of distortion
performance and optimal quantizer design in the high-resolution
(low-distortion) regime using a framework called distributed functional scalar
quantization (DFSQ). The key result is that chatting can dramatically improve
performance even when intersensor communication is at very low rate, especially
if the fusion center desires fidelity of a nonlinear computation applied to
source realizations rather than fidelity in representing the sources
themselves. We also solve the rate allocation problem when communication links
have heterogeneous costs and provide a detailed example to demonstrate the
theoretical and practical gains from chatting. This example for maximum
computation gives insight on the gap between chatting and distributed networks,
and how to optimize the intersensor communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8418</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8418</id><created>2012-10-25</created><authors><author><keyname>P.</keyname><forenames>Kirana Kumara</forenames><affiliation>Indian Institute of Science, Bangalore, India</affiliation></author></authors><title>Demonstrating the Usefulness of CAELinux for Computer Aided Engineering
  using an Example of the Three Dimensional Reconstruction of a Pig Liver</title><categories>cs.MS physics.bio-ph physics.comp-ph physics.med-ph</categories><comments>10 pages, six *.JPG figures, uses html.sty, to be published in
  International Journal of Advancements in Technology (ISSN : 0976-4860)</comments><journal-ref>International Journal of Advancements in Technology Vol. 3 No. 4
  (December 2012) pp. 301-309</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CAELinux is a Linux distribution which is bundled with free software packages
related to Computer Aided Engineering (CAE). The free software packages include
software that can build a three dimensional solid model, programs that can mesh
a geometry, software for carrying out Finite Element Analysis (FEA), programs
that can carry out image processing etc. Present work has two goals: 1) To give
a brief description of CAELinux 2) To demonstrate that CAELinux could be useful
for Computer Aided Engineering, using an example of the three dimensional
reconstruction of a pig liver from a stack of CT-scan images. One can note that
instead of using CAELinux, using commercial software for reconstructing the
liver would cost a lot of money. One can also note that CAELinux is a free and
open source operating system and all software packages that are included in the
operating system are also free. Hence one can conclude that CAELinux could be a
very useful tool in application areas like surgical simulation which require
three dimensional reconstructions of biological organs. Also, one can see that
CAELinux could be a very useful tool for Computer Aided Engineering, in
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8421</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8421</id><created>2012-10-31</created><updated>2014-05-08</updated><authors><author><keyname>Jelenkovi&#x107;</keyname><forenames>Predrag R.</forenames></author><author><keyname>Skiani</keyname><forenames>Evangelia D.</forenames></author></authors><title>Distribution of the Number of Retransmissions of Bounded Documents</title><categories>cs.PF math.PR</categories><comments>32 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retransmission-based failure recovery represents a primary approach in
existing communication networks that guarantees data delivery in the presence
of channel failures. Recent work has shown that, when data sizes have infinite
support, retransmissions can cause long (-tailed) delays even if all traffic
and network characteristics are light-tailed. In this paper we investigate the
practically important case of bounded data units 0 &lt;= L_b &lt;= b under the
condition that the hazard functions of the distributions of data sizes and
channel statistics are proportional. To this end, we provide an explicit and
uniform characterization of the entire body of the retransmission distribution
Pr[N_b &gt; n] in both n and b. Our main discovery is that this distribution can
be represented as the product of a power law and Gamma distribution. This
rigorous approximation clearly demonstrates the coupling of a power law
distribution, dominating the main body, and the Gamma distribution, determining
the exponential tail. Our results are validated via simulation experiments and
can be useful for designing retransmission-based systems with the required
performance characteristics. From a broader perspective, this study applies to
any other system, e.g., computing, where restart mechanisms are employed after
a job processing failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8433</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8433</id><created>2012-10-31</created><authors><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Green Cellular Wireless Networks: Where to Begin?</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional cellular wireless networks were designed with the purpose of
providing high throughput for the user and high capacity for the service
provider, without any provisions of energy efficiency. As a result, these
networks have an enormous Carbon footprint. In this note, we describe the
sources of the inefficiencies in such networks. First we quantify how much
Carbon footprint such networks generate. We also discuss how much more mobile
traffic is expected to increase so that this Carbon footprint will even
increase tremendously more. We then discuss specific sources of inefficiency
and potential sources of improvement at the physical layer as well as higher
layers of the communication protocol hierarchy. In particular, considering that
most of the energy inefficiency in wireless cellular networks is at the base
stations, we discuss multi-tier networks and point to the potential of
exploiting mobility patterns in order to use base station energy judiciously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8436</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8436</id><created>2012-10-31</created><authors><author><keyname>Kamvar</keyname><forenames>Maryam</forenames></author><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author></authors><title>Optimal size, freshness and time-frame for voice search vocabulary</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate how to optimize the vocabulary for a voice
search language model. The metric we optimize over is the out-of-vocabulary
(OoV) rate since it is a strong indicator of user experience. In a departure
from the usual way of measuring OoV rates, web search logs allow us to compute
the per-session OoV rate and thus estimate the percentage of users that
experience a given OoV rate. Under very conservative text normalization, we
find that a voice search vocabulary consisting of 2 to 2.5 million words
extracted from 1 week of search query data will result in an aggregate OoV rate
of 1%; at that size, the same OoV rate will also be experienced by 90% of
users. The number of words included in the vocabulary is a stable indicator of
the OoV rate. Altering the freshness of the vocabulary or the duration of the
time window over which the training data is gathered does not significantly
change the OoV rate. Surprisingly, a significantly larger vocabulary
(approximately 10 million words) is required to guarantee OoV rates below 1%
for 95% of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8439</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8439</id><created>2012-10-31</created><updated>2014-04-03</updated><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Near Optimal Leader Election in Multi-Hop Radio Networks</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present distributed randomized leader election protocols for multi-hop
radio networks that elect a leader in almost the same time $T_{BC}$ required
for broadcasting a message. For the setting without collision detection, our
algorithm runs with high probability in $O(D \log \frac{n}{D} + \log^3 n)
\min\{\log\log n,\log \frac{n}{D}\}$ rounds on any $n$-node network with
diameter $D$. Since $T_{BC} = \Theta(D \log \frac{n}{D} + \log^2 n)$ is a lower
bound, our upper bound is optimal up to a factor of at most $\log \log n$ and
the extra $\log n$ factor on the additive term. This algorithm is furthermore
the first $O(n)$ time algorithm for this setting.
  Our algorithms improve over a 25 year old simulation approach of Bar-Yehuda,
Goldreich and Itai with a $O(T_{BC} \log n)$ running time: In 1987 they
designed a fast broadcast protocol and subsequently in 1989 they showed how it
can be used to simulate one round of a single-hop network that has collision
detection in $T_{BC}$ time. The prime application of this simulation was to
simulate Willards single-hop leader election protocol, which elects a leader in
$O(\log n)$ rounds with high probability and $O(\log \log n)$ rounds in
expectation. While it was subsequently shown that Willards bounds are tight, it
was unclear whether the simulation approach is optimal. Our results break this
barrier and essentially remove the logarithmic slowdown over the broadcast time
$T_{BC}$ by going away from the simulation approach.
  We also give a distributed randomized leader election algorithm for the
setting with collision detection that runs in $O(D + \log n \log \log n) \cdot
\min\{\log \log n, \log \frac{n}{D}\}$ rounds. This round complexity is optimal
up to $O(\log \log n)$ factors and improves over a deterministic algorithm that
requires $\Theta(n)$ rounds independently of the diameter $D$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8440</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8440</id><created>2012-10-31</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Bikel</keyname><forenames>Dan</forenames></author><author><keyname>Shugrina</keyname><forenames>Maria</forenames></author><author><keyname>Nguyen</keyname><forenames>Patrick</forenames></author><author><keyname>Kumar</keyname><forenames>Shankar</forenames></author></authors><title>Large Scale Language Modeling in Automatic Speech Recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models have been proven quite beneficial for a variety of
automatic speech recognition tasks in Google. We summarize results on Voice
Search and a few YouTube speech transcription tasks to highlight the impact
that one can expect from increasing both the amount of training data, and the
size of the language model estimated from such data. Depending on the task,
availability and amount of training data used, language model size and amount
of work and care put into integrating them in the lattice rescoring step we
observe reductions in word error rate between 6% and 10% relative, for systems
on a wide range of operating points between 17% and 52% word error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8441</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8441</id><created>2012-10-31</created><authors><author><keyname>Koyuncu</keyname><forenames>Erdem</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Very Low-Rate Variable-Length Channel Quantization for Minimum Outage
  Probability</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify a practical vector quantizer design problem where any
fixed-length quantizer (FLQ) yields non-zero distortion at any finite rate,
while there is a variable-length quantizer (VLQ) that can achieve zero
distortion with arbitrarily low rate. The problem arises in a $t \times 1$
multiple-antenna fading channel where we would like to minimize the channel
outage probability by employing beamforming via quantized channel state
information at the transmitter (CSIT). It is well-known that in such a
scenario, finite-rate FLQs cannot achieve the full-CSIT (zero distortion)
outage performance. We construct VLQs that can achieve the full-CSIT
performance with finite rate. In particular, with $P$ denoting the power
constraint of the transmitter, we show that the necessary and sufficient VLQ
rate that guarantees the full-CSIT performance is $\Theta(1/P)$. We also
discuss several extensions (e.g. to precoding) of this result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.8442</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.8442</id><created>2012-10-31</created><updated>2013-01-27</updated><authors><author><keyname>Shao</keyname><forenames>Louis Yuanlong</forenames></author></authors><title>Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On
  Boltzmann Machines</title><categories>cs.AI cs.NE q-bio.NC stat.ML</categories><comments>Submitted to International Conference of Learning Representation
  (ICLR) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One conjecture in both deep learning and classical connectionist viewpoint is
that the biological brain implements certain kinds of deep networks as its
back-end. However, to our knowledge, a detailed correspondence has not yet been
set up, which is important if we want to bridge between neuroscience and
machine learning. Recent researches emphasized the biological plausibility of
Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally
plausible settings, the whole network is capable of representing any Boltzmann
machine and performing a semi-stochastic Bayesian inference algorithm lying
between Gibbs sampling and variational inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0020</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0020</id><created>2012-10-31</created><updated>2015-01-06</updated><authors><author><keyname>Woods</keyname><forenames>Kevin</forenames></author></authors><title>Presburger arithmetic, rational generating functions, and
  quasi-polynomials</title><categories>math.CO cs.LO math.LO</categories><comments>revised, including significant additions explaining computational
  complexity results. To appear in Journal of Symbolic Logic. Extended abstract
  in ICALP 2013. 17 pages</comments><msc-class>05A15, 52C07, 03B25</msc-class><acm-class>F.4.3; G.2.1; F.2.2</acm-class><journal-ref>Journal of Symbolic Logic 80 (2015), 433-449</journal-ref><doi>10.1017/jsl.2015.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presburger arithmetic is the first-order theory of the natural numbers with
addition (but no multiplication). We characterize sets that can be defined by a
Presburger formula as exactly the sets whose characteristic functions can be
represented by rational generating functions; a geometric characterization of
such sets is also given. In addition, if p=(p_1,...,p_n) are a subset of the
free variables in a Presburger formula, we can define a counting function g(p)
to be the number of solutions to the formula, for a given p. We show that every
counting function obtained in this way may be represented as, equivalently,
either a piecewise quasi-polynomial or a rational generating function. Finally,
we translate known computational complexity results into this setting and
discuss open directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0025</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0025</id><created>2012-10-31</created><updated>2014-06-21</updated><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author><author><keyname>Petej</keyname><forenames>Ivan</forenames></author></authors><title>Venn-Abers predictors</title><categories>cs.LG stat.ML</categories><comments>18 pages; to appear in the UAI 2014 Proceedings</comments><report-no>OCM07</report-no><msc-class>68T05, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues study, both theoretical and empirical, of the method of
Venn prediction, concentrating on binary prediction problems. Venn predictors
produce probability-type predictions for the labels of test objects which are
guaranteed to be well calibrated under the standard assumption that the
observations are generated independently from the same distribution. We give a
simple formalization and proof of this property. We also introduce Venn-Abers
predictors, a new class of Venn predictors based on the idea of isotonic
regression, and report promising empirical results both for Venn-Abers
predictors and for their more computationally efficient simplified version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0028</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0028</id><created>2012-10-31</created><authors><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Yan</keyname><forenames>Rong</forenames></author><author><keyname>Raina</keyname><forenames>Rajat</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Understanding the Interaction between Interests, Conversations and
  Friendships in Facebook</title><categories>cs.SI cs.LG stat.ML</categories><report-no>CMU-ML-12-109</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore salient questions about user interests,
conversations and friendships in the Facebook social network, using a novel
latent space model that integrates several data types. A key challenge of
studying Facebook's data is the wide range of data modalities such as text,
network links, and categorical labels. Our latent space model seamlessly
combines all three data modalities over millions of users, allowing us to study
the interplay between user friendships, interests, and higher-order
network-wide social trends on Facebook. The recovered insights not only answer
our initial questions, but also reveal surprising facts about user interests in
the context of Facebook's ecosystem. We also confirm that our results are
significant with respect to evidential information from the study subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0053</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0053</id><created>2012-10-31</created><updated>2013-03-10</updated><authors><author><keyname>Shuman</keyname><forenames>David I</forenames></author><author><keyname>Narang</keyname><forenames>Sunil K.</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>The Emerging Field of Signal Processing on Graphs: Extending
  High-Dimensional Data Analysis to Networks and Other Irregular Domains</title><categories>cs.DM cs.LG cs.SI</categories><comments>To appear in the IEEE Signal Processing Magazine</comments><doi>10.1109/MSP.2012.2235192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applications such as social, energy, transportation, sensor, and neuronal
networks, high-dimensional data naturally reside on the vertices of weighted
graphs. The emerging field of signal processing on graphs merges algebraic and
spectral graph theoretic concepts with computational harmonic analysis to
process such signals on graphs. In this tutorial overview, we outline the main
challenges of the area, discuss different ways to define graph spectral
domains, which are the analogues to the classical frequency domain, and
highlight the importance of incorporating the irregular structures of graph
data domains when processing signals on graphs. We then review methods to
generalize fundamental operations such as filtering, translation, modulation,
dilation, and downsampling to the graph setting, and survey the localized,
multiscale transforms that have been proposed to efficiently extract
information from high-dimensional data on graphs. We conclude with a brief
discussion of open issues and possible extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0055</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0055</id><created>2012-10-31</created><authors><author><keyname>Sarhrouni</keyname><forenames>Elkebir</forenames></author><author><keyname>Hammouch</keyname><forenames>Ahmed</forenames></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames></author></authors><title>Dimensionality Reduction and Classification Feature Using Mutual
  Information Applied to Hyperspectral Images: A Wrapper Strategy Algorithm
  Based on Minimizing the Error Probability Using the Inequality of Fano</title><categories>cs.CV</categories><comments>12 page, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1210.0528, arXiv:1210.0052</comments><journal-ref>Applied Mathematical Sciences, Vol. 6, 2012, no. 102, 5073 - 5084</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In the feature classification domain, the choice of data affects widely the
results. For the Hyperspectral image, the bands dont all contain the
information; some bands are irrelevant like those affected by various
atmospheric effects, see Figure.4, and decrease the classification accuracy.
And there exist redundant bands to complicate the learning system and product
incorrect prediction [14]. Even the bands contain enough information about the
scene they may can't predict the classes correctly if the dimension of space
images, see Figure.3, is so large that needs many cases to detect the
relationship between the bands and the scene (Hughes phenomenon) [10]. We can
reduce the dimensionality of hyperspectral images by selecting only the
relevant bands (feature selection or subset selection methodology), or
extracting, from the original bands, new bands containing the maximal
information about the classes, using any functions, logical or numerical
(feature extraction methodology) [11][9]. Here we focus on the feature
selection using mutual information. Hyperspectral images have three advantages
regarding the multispectral images [6],
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0056</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0056</id><created>2012-10-31</created><updated>2012-11-02</updated><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author></authors><title>Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone
  Programming</title><categories>math.OC cs.LG math.NA stat.CO stat.ML</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider $l_0$ regularized convex cone programming problems.
In particular, we first propose an iterative hard thresholding (IHT) method and
its variant for solving $l_0$ regularized box constrained convex programming.
We show that the sequence generated by these methods converges to a local
minimizer. Also, we establish the iteration complexity of the IHT method for
finding an $\epsilon$-local-optimal solution. We then propose a method for
solving $l_0$ regularized convex cone programming by applying the IHT method to
its quadratic penalty relaxation and establish its iteration complexity for
finding an $\epsilon$-approximate local minimizer. Finally, we propose a
variant of this method in which the associated penalty parameter is dynamically
updated, and show that every accumulation point is a local minimizer of the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0059</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0059</id><created>2012-10-31</created><authors><author><keyname>Linahan</keyname><forenames>Jeff</forenames></author></authors><title>Improving the Numerical Robustness of Sphere Swept Collision Detection</title><categories>cs.CG</categories><comments>7 pages, 4 figures</comments><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses improvements to the numerical robustness of the
algorithm described in Kasper Fauerby's &quot;Improved Collision Detection and
Response.&quot; The algorithm addresses a common collision detection query: a moving
sphere or ellipsoid vs. a set of motionless triangles. In its current form, the
algorithm allows the sphere to penetrate the triangles. The sphere also
displays &quot;jittering&quot; behavior when colliding with certain geometry. Most of
these problems are the product of insufficient attention to numerical
robustness, the focus of this paper. Motivated by the importance of numerical
robustness in collision detection code, this paper addresses these problems in
detail and proposes efficient solutions to them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0071</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0071</id><created>2012-10-31</created><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Randomness and Non-determinism</title><categories>cs.CC cs.CR cs.IT math.IT</categories><comments>1992 talk at ASL meeting</comments><journal-ref>Journal of Symbolic Logic, 58(3) pp. 1102-1103, 1993</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponentiation makes the difference between the bit-size of this line and the
number (&lt;&lt; 2^{300}) of particles in the known Universe. The expulsion of
exponential time algorithms from Computer Theory in the 60's broke its
umbilical cord from Mathematical Logic. It created a deep gap between
deterministic computation and -- formerly its unremarkable tools -- randomness
and non-determinism. Little did we learn in the past decades about the power of
either of these two basic &quot;freedoms&quot; of computation, but some vague pattern is
emerging in relationships between them. The pattern of similar techniques
instrumental for quite different results in this area seems even more
interesting. Ideas like multilinear and low-degree multivariate polynomials,
Fourier transformation over low-periodic groups seem very illuminating. The
talk surveyed some recent results. One of them, given in a stronger form than
previously published, is described below.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0074</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0074</id><created>2012-10-31</created><authors><author><keyname>Rudnick</keyname><forenames>Alex</forenames></author></authors><title>Transition-Based Dependency Parsing With Pluggable Classifiers</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In principle, the design of transition-based dependency parsers makes it
possible to experiment with any general-purpose classifier without other
changes to the parsing algorithm. In practice, however, it often takes
substantial software engineering to bridge between the different
representations used by two software packages. Here we present extensions to
MaltParser that allow the drop-in use of any classifier conforming to the
interface of the Weka machine learning package, a wrapper for the TiMBL
memory-based learner to this interface, and experiments on multilingual
dependency parsing with a variety of classifiers. While earlier work had
suggested that memory-based learners might be a good choice for low-resource
parsing scenarios, we cannot support that hypothesis in this work. We observed
that support-vector machines give better parsing performance than the
memory-based learner, regardless of the size of the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0086</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0086</id><created>2012-11-01</created><authors><author><keyname>Ahadpour</keyname><forenames>Sodeif</forenames></author><author><keyname>Majidpour</keyname><forenames>Mahdiyeh</forenames></author><author><keyname>Sadra</keyname><forenames>Yaser</forenames></author></authors><title>Public key Steganography Using Discrete Cross-Coupled Chaotic Maps</title><categories>cs.CR cs.MM nlin.CD</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By cross-coupling two logistic maps a novel method is proposed for the public
key steganography in JPEG image. Chaotic maps entail high complexity in the
used algorithm for embedding secret data in a medium. In this paper, discrete
cross- coupled chaotic maps are used to specifying the location of the
different parts of the secret data in the image. Modifying JPEG format during
compressing and decompressing, and also using public key enhanced difficulty of
the algorithm. Simulation results show that in addition to excessive capacity,
this method has high robustness and resistance against hackers and can be
applicable in secret communication. Also the PSNR value is high compared to the
other works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0090</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0090</id><created>2012-11-01</created><authors><author><keyname>Ahadpour</keyname><forenames>Sodeif</forenames></author><author><keyname>Sadra</keyname><forenames>Yaser</forenames></author></authors><title>A Chaos-based Image Encryption Scheme using Chaotic Coupled Map Lattices</title><categories>cs.CR</categories><comments>4 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1201.1449, arXiv:1112.3791</comments><journal-ref>International Journal of Computer Applications 49(2):15-18, July
  2012. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/7599-0311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, information security essential in various arenas like
internet communication, multimedia systems, medical imaging, tele-medicine and
military communication. However, most of them faced with some problems such as
the lack of robustness and security. In this letter, after reviewing the main
points of the chaotic trigonometric maps and the coupled map lattices, we
introduce the scheme of chaos-based image encryption based on coupled map lat
tices. The scheme decreases periodic effect of the ergodic dynamical systems in
the chaos-based image encryption. To evaluate the security of the encrypted
image of this scheme, the key space analysis, the correlation of two adjacent
pixels and differential attack were performed. This scheme tries to improve the
problem of failure of encryption such as small key space and level of security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0122</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0122</id><created>2012-11-01</created><updated>2013-02-25</updated><authors><author><keyname>Beelen</keyname><forenames>Peter</forenames></author><author><keyname>H&#xf8;holdt</keyname><forenames>Tom</forenames></author><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Wu</keyname><forenames>Yingquan</forenames></author></authors><title>On Rational-Interpolation Based List-Decoding and List-Decoding Binary
  Goppa Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions of Information Theory</comments><doi>10.1109/TIT.2013.2243800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the Wu list-decoding algorithm for Generalised Reed-Solomon (GRS)
codes by using Gr\&quot;obner bases over modules and the Euclidean algorithm (EA) as
the initial algorithm instead of the Berlekamp-Massey algorithm (BMA). We
present a novel method for constructing the interpolation polynomial fast. We
give a new application of the Wu list decoder by decoding irreducible binary
Goppa codes up to the binary Johnson radius. Finally, we point out a connection
between the governing equations of the Wu algorithm and the Guruswami-Sudan
algorithm (GSA), immediately leading to equality in the decoding range and a
duality in the choice of parameters needed for decoding, both in the case of
GRS codes and in the case of Goppa codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0135</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0135</id><created>2012-11-01</created><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Sampling and Reconstruction of Spatial Fields using Mobile Sensors</title><categories>cs.MM cs.CV cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing May 2012; revised
  Oct 2012</comments><doi>10.1109/TSP.2013.2247599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial sampling is traditionally studied in a static setting where static
sensors scattered around space take measurements of the spatial field at their
locations. In this paper we study the emerging paradigm of sampling and
reconstructing spatial fields using sensors that move through space. We show
that mobile sensing offers some unique advantages over static sensing in
sensing time-invariant bandlimited spatial fields. Since a moving sensor
encounters such a spatial field along its path as a time-domain signal, a
time-domain anti-aliasing filter can be employed prior to sampling the signal
received at the sensor. Such a filtering procedure, when used by a
configuration of sensors moving at constant speeds along equispaced parallel
lines, leads to a complete suppression of spatial aliasing in the direction of
motion of the sensors. We analytically quantify the advantage of using such a
sampling scheme over a static sampling scheme by computing the reduction in
sampling noise due to the filter. We also analyze the effects of non-uniform
sensor speeds on the reconstruction accuracy. Using simulation examples we
demonstrate the advantages of mobile sampling over static sampling in practical
problems.
  We extend our analysis to sampling and reconstruction schemes for monitoring
time-varying bandlimited fields using mobile sensors. We demonstrate that in
some situations we require a lower density of sensors when using a mobile
sensing scheme instead of the conventional static sensing scheme. The exact
advantage is quantified for a problem of sampling and reconstructing an audio
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0156</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0156</id><created>2012-11-01</created><updated>2014-08-07</updated><authors><author><keyname>Cetin</keyname><forenames>Uzay</forenames></author><author><keyname>Bingol</keyname><forenames>Haluk O.</forenames></author></authors><title>Attention Competition with Advertisement</title><categories>cs.SI nlin.AO physics.soc-ph</categories><journal-ref>Physical Review E, 90, 032801, 2014</journal-ref><doi>10.1103/PhysRevE.90.032801</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the new digital age, information is available in large quantities. Since
information consumes primarily the attention of its recipients, the scarcity of
attention is becoming the main limiting factor. In this study, we investigate
the impact of advertisement pressure on a cultural market where consumers have
a limited attention capacity. A model of competition for attention is developed
and investigated analytically and by simulation. Advertisement is found to be
much more effective when attention capacity of agents is extremely scarce. We
have observed that the market share of the advertised item improves if dummy
items are introduced to the market while the strength of the advertisement is
kept constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0157</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0157</id><created>2012-11-01</created><authors><author><keyname>Gorain</keyname><forenames>Barun</forenames></author><author><keyname>Mandal</keyname><forenames>Partha Sarathi</forenames></author></authors><title>Optimal Covering with Mobile Sensors in an Unbounded Region</title><categories>cs.DC cs.DS</categories><comments>7 pages, submitted in the Eighth International Conference on Wireless
  Communication and Sensor Networks (WCSN-2012)</comments><acm-class>C.2.4; H.5.3; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering a bounded region with minimum number of homogeneous sensor nodes is
a NP-complete problem \cite{Li09}. In this paper we have proposed an {\it id}
based distributed algorithm for optimal coverage in an unbounded region. The
proposed algorithm guarantees maximum spreading in $O(\sqrt{n})$ rounds without
creating any coverage hole. The algorithm executes in synchronous rounds
without exchanging any message.
  We have also explained how our proposed algorithm can achieve optimal energy
consumption and handle random sensor node deployment for optimal spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0169</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0169</id><created>2012-11-01</created><authors><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author></authors><title>Multi-Stratum Networks: toward a unified model of on-line identities</title><categories>cs.SI physics.soc-ph</categories><comments>Extended version of paper presented at ASONAM 2011 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the reasons behind the success of Social Network Analysis is its
simple and general graph model made of nodes (representing individuals) and
ties. However, when we focus on our daily on-line experience we must confront a
more complex scenario: people inhabitate several on-line spaces interacting to
several communities active on various technological infrastructures like
Twitter, Facebook, YouTube or FourSquare and with distinct social objectives.
This constitutes a complex network of interconnected networks where users'
identities are spread and where information propagates navigating through
different communities and social platforms. In this article we introduce a
model for this layered scenario that we call multi-stratum network. Through a
theoretical discussion and the analysis of real-world data we show how not only
focusing on a single network may provide a very partial understanding of the
role of its users, but also that considering all the networks separately may
not reveal the information contained in the whole multi-stratum model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0175</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0175</id><created>2012-11-01</created><authors><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>Harmonious Hilbert curves and other extradimensional space-filling
  curves</title><categories>cs.CG</categories><comments>40 pages, 10 figures, pseudocode included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new way of generalizing Hilbert's two-dimensional
space-filling curve to arbitrary dimensions. The new curves, called harmonious
Hilbert curves, have the unique property that for any d' &lt; d, the d-dimensional
curve is compatible with the d'-dimensional curve with respect to the order in
which the curves visit the points of any d'-dimensional axis-parallel space
that contains the origin. Similar generalizations to arbitrary dimensions are
described for several variants of Peano's curve (the original Peano curve, the
coil curve, the half-coil curve, and the Meurthe curve). The d-dimensional
harmonious Hilbert curves and the Meurthe curves have neutral orientation: as
compared to the curve as a whole, arbitrary pieces of the curve have each of d!
possible rotations with equal probability. Thus one could say these curves are
`statistically invariant' under rotation---unlike the Peano curves, the coil
curves, the half-coil curves, and the familiar generalization of Hilbert curves
by Butz and Moore.
  In addition, prompted by an application in the construction of R-trees, this
paper shows how to construct a 2d-dimensional generalized Hilbert or Peano
curve that traverses the points of a certain d-dimensional diagonally placed
subspace in the order of a given d-dimensional generalized Hilbert or Peano
curve.
  Pseudocode is provided for comparison operators based on the curves presented
in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0176</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0176</id><created>2012-11-01</created><authors><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Montesi</keyname><forenames>Danilo</forenames></author></authors><title>Joining relations under discrete uncertainty</title><categories>cs.DB</categories><comments>database join operator, uncertain relations with discrete
  uncertainty, algorithms and experimental evaluation (28 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and experimentally compare alternative algorithms
to join uncertain relations. Different algorithms are based on specific
principles, e.g., sorting, indexing, or building intermediate relational tables
to apply traditional approaches. As a consequence their performance is affected
by different features of the input data, and each algorithm is shown to be more
efficient than the others in specific cases. In this way statistics explicitly
representing the amount and kind of uncertainty in the input uncertain
relations can be used to choose the most efficient algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0177</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0177</id><created>2012-11-01</created><authors><author><keyname>Hung</keyname><forenames>Hao-Hsiang</forenames></author></authors><title>Improved Time Complexity of Bandwidth Approximation in Dense Graphs</title><categories>cs.DS math.CO</categories><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Given a graph $G=(V, E)$ and and a proper labeling $f$ from $V$ to $\{1, ...,
n\}$, we define $B(f)$ as the maximum absolute difference between $f(u)$ and
$f(v)$ where $(u,v)\in E$. The bandwidth of $G$ is the minimum $B(f)$ for all
$f$. Say $G$ is $\delta$-dense if its minimum degree is $\delta n$. In this
paper, we investigate the trade-off between the approximation ratio and the
time complexity of the classical approach of Karpinski {et al}.\cite{Karpin97},
and present a faster randomized algorithm for approximating the bandwidth of
$\delta$-dense graphs. In particular, by removing the polylog factor of the
time complexity required to enumerate all possible placements for balls to
bins, we reduce the time complexity from $O(n^6\cdot (\log n)^{O(1)})$ to
$O(n^{4+o(1)})$. In advance, we reformulate the perfect matching phase of the
algorithm with a maximum flow problem of smaller size and reduce the time
complexity to $O(n^2\log\log n)$. We also extend the graph classes could be
applied by the original approach: we show that the algorithm remains polynomial
time as long as $\delta$ is $O({(\log\log n)}^2 / {\log n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0191</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0191</id><created>2012-10-25</created><authors><author><keyname>Ristic</keyname><forenames>Branko</forenames></author><author><keyname>Sherrah</keyname><forenames>Jamie</forenames></author><author><keyname>Garc&#xed;a-Fern&#xe1;ndez</keyname><forenames>&#xc1;ngel F.</forenames></author></authors><title>Performance Evaluation of Random Set Based Pedestrian Tracking
  Algorithms</title><categories>cs.CV</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper evaluates the error performance of three random finite set based
multi-object trackers in the context of pedestrian video tracking. The
evaluation is carried out using a publicly available video dataset of 4500
frames (town centre street) for which the ground truth is available. The input
to all pedestrian tracking algorithms is an identical set of head and body
detections, obtained using the Histogram of Oriented Gradients (HOG) detector.
The tracking error is measured using the recently proposed OSPA metric for
tracks, adopted as the only known mathematically rigorous metric for measuring
the distance between two sets of tracks. A comparative analysis is presented
under various conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0210</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0210</id><created>2012-11-01</created><authors><author><keyname>Selvaraj</keyname><forenames>Sathiya Keerthi</forenames></author><author><keyname>Sellamanickam</keyname><forenames>Sundararajan</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author></authors><title>Extension of TSVM to Multi-Class and Hierarchical Text Classification
  Problems With General Losses</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transductive SVM (TSVM) is a well known semi-supervised large margin learning
method for binary text classification. In this paper we extend this method to
multi-class and hierarchical classification problems. We point out that the
determination of labels of unlabeled examples with fixed classifier weights is
a linear programming problem. We devise an efficient technique for solving it.
The method is applicable to general loss functions. We demonstrate the value of
the new method using large margin loss on a number of multi-class and
hierarchical classification datasets. For maxent loss we show empirically that
our method is better than expectation regularization/constraint and posterior
regularization methods, and competitive with the version of entropy
regularization method which uses label constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0224</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0224</id><created>2012-11-01</created><authors><author><keyname>Etcheverry</keyname><forenames>Lorena</forenames></author><author><keyname>Vaisman</keyname><forenames>Alejandro A.</forenames></author></authors><title>Views over RDF Datasets: A State-of-the-Art and Open Challenges</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Views on RDF datasets have been discussed in several works, nevertheless
there is no consensus on their definition nor the requirements they should
fulfill. In traditional data management systems, views have proved to be useful
in different application scenarios such as data integration, query answering,
data security, and query modularization.
  In this work we have reviewed existent work on views over RDF datasets, and
discussed the application of existent view definition mechanisms to four
scenarios in which views have proved to be useful in traditional (relational)
data management systems. To give a framework for the discussion we provided a
definition of views over RDF datasets, an issue over which there is no
consensus so far. We finally chose the three proposals closer to this
definition, and analyzed them with respect to four selected goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0235</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0235</id><created>2012-11-01</created><authors><author><keyname>Scott</keyname><forenames>Alex</forenames></author><author><keyname>Jeavons</keyname><forenames>Peter</forenames></author><author><keyname>Xu</keyname><forenames>Lei</forenames></author></authors><title>Feedback from nature: an optimal distributed algorithm for maximal
  independent set selection</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximal Independent Set selection is a fundamental problem in distributed
computing. A novel probabilistic algorithm for this problem has recently been
proposed by Afek et al, inspired by the study of the way that developing cells
in the fly become specialised. The algorithm they propose is simple and robust,
but not as efficient as previous approaches: the expected time complexity is
O(log^2 n). Here we first show that the approach of Afek et al cannot achieve
better efficiency than this across all networks, no matter how the probability
values are chosen. However, we then propose a new algorithm that incorporates
another important feature of the biological system: adapting the probabilities
used at each node based on local feedback from neighbouring nodes. Our new
algorithm retains all the advantages of simplicity and robustness, but also
achieves the optimal efficiency of O(log n) expected time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0242</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0242</id><created>2012-11-01</created><updated>2015-09-21</updated><authors><author><keyname>Lopes</keyname><forenames>Bruno</forenames></author><author><keyname>Englander</keyname><forenames>Cec&#xed;lia</forenames></author><author><keyname>Lobo</keyname><forenames>Fernanda</forenames></author><author><keyname>Cruz</keyname><forenames>Marcela</forenames></author></authors><title>Revisiting the proof theory of Classical S4</title><categories>cs.LO</categories><comments>10 pages</comments><msc-class>03B45</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1965 Dag Prawitz presented an extension of Gentzen-type systems of Natural
Deduction to modal concepts of S4. Maria da Paz Medeiros showed in 2006 that
the proof of normalisation for classical S4 does not hold and proposed a new
proof of normalisation for a logically equivalent system, the system NS4.
However two problems in the proof of the critical lemma used by Medeiros in her
proof were pointed out by Yuuki Andou in 2009. This paper presents a proof of
the critical lemma, resulting in a proof of normalisation for NS4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0243</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0243</id><created>2012-11-01</created><authors><author><keyname>Li</keyname><forenames>Shi</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Approximating $k$-Median via Pseudo-Approximation</title><categories>cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approximation algorithm for $k$-median that achieves an
approximation guarantee of
  $1+\sqrt{3}+\epsilon$, improving upon the decade-old ratio of $3+\epsilon$.
Our approach is based on two components, each of which, we believe, is of
independent interest.
  First, we show that in order to give an $\alpha$-approximation algorithm for
$k$-median, it is sufficient to give a \emph{pseudo-approximation algorithm}
that finds an $\alpha$-approximate solution by opening $k+O(1)$ facilities.
This is a rather surprising result as there exist instances for which opening
$k+1$ facilities may lead to a significant smaller cost than if only $k$
facilities were opened.
  Second, we give such a pseudo-approximation algorithm with $\alpha=
1+\sqrt{3}+\epsilon$. Prior to our work, it was not even known whether opening
$k + o(k)$ facilities would help improve the approximation ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0270</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0270</id><created>2012-11-01</created><updated>2013-04-22</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author></authors><title>Time-Space Trade-Offs for Longest Common Extensions</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in the proceedings of
  the 23rd Annual Symposium on Combinatorial Pattern Matching (CPM 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the longest common extension (LCE) problem, that is, preprocess a
string $T$ into a compact data structure that supports fast LCE queries. An LCE
query takes a pair $(i,j)$ of indices in $T$ and returns the length of the
longest common prefix of the suffixes of $T$ starting at positions $i$ and $j$.
We study the time-space trade-offs for the problem, that is, the space used for
the data structure vs. the worst-case time for answering an LCE query. Let $n$
be the length of $T$. Given a parameter $\tau$, $1 \leq \tau \leq n$, we show
how to achieve either $O(\infrac{n}{\sqrt{\tau}})$ space and $O(\tau)$ query
time, or $O(\infrac{n}{\tau})$ space and $O(\tau \log({|\LCE(i,j)|}/{\tau}))$
query time, where $|\LCE(i,j)|$ denotes the length of the LCE returned by the
query. These bounds provide the first smooth trade-offs for the LCE problem and
almost match the previously known bounds at the extremes when $\tau=1$ or
$\tau=n$. We apply the result to obtain improved bounds for several
applications where the LCE problem is the computational bottleneck, including
approximate string matching and computing palindromes. We also present an
efficient technique to reduce LCE queries on two strings to one string.
Finally, we give a lower bound on the time-space product for LCE data
structures in the non-uniform cell probe model showing that our second
trade-off is nearly optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0290</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0290</id><created>2012-11-01</created><updated>2013-07-09</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Fernandez-Granda</keyname><forenames>Carlos</forenames></author></authors><title>Super-Resolution from Noisy Data</title><categories>cs.IT math.IT math.NA</categories><comments>20 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the recovery of a superposition of point sources from
noisy bandlimited data. In the fewest possible words, we only have information
about the spectrum of an object in a low-frequency band bounded by a certain
cut-off frequency and seek to obtain a higher resolution estimate by
extrapolating the spectrum up to a higher frequency. We show that as long as
the sources are separated by twice the inverse of the cut-off frequency,
solving a simple convex program produces a stable estimate in the sense that
the approximation error between the higher-resolution reconstruction and the
truth is proportional to the noise level times the square of the
super-resolution factor (SRF), which is the ratio between the desired high
frequency and the cut-off frequency of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0297</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0297</id><created>2012-11-01</created><updated>2013-12-04</updated><authors><author><keyname>Lekeas</keyname><forenames>Paraskevas V.</forenames></author></authors><title>A Note on Circular Arc Online Coloring using First Fit</title><categories>cs.DS cs.DM math.CO</categories><comments>9 pages, 3 figures, Figure 3 corrected from previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Raman (2007), using a column construction technique it is proved that
every interval graph can be colored online with First Fit with at most $8w(G)$
colors, where $w(G)$ is the size of the maximum clique of $G$. Since the column
construction can not be adapted to circular arc graphs we give a different
proof to establish an upper bound of $9w(G)$ for online coloring a circular arc
graph $G$ with the First Fit algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0303</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0303</id><created>2012-11-01</created><authors><author><keyname>Lorenz</keyname><forenames>Andy</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author></authors><title>Non-redundant random generation algorithms for weighted context-free
  languages</title><categories>cs.FL cs.DM cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1012.4560</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the non-redundant random generation of $k$ words of length $n$ in
a context-free language. Additionally, we want to avoid a predefined set of
words. We study a rejection-based approach, whose worst-case time complexity is
shown to grow exponentially with $k$ for some specifications and in the limit
case of a coupon collector. We propose two algorithms respectively based on the
recursive method and on an unranking approach. We show how careful
implementations of these algorithms allow for a non-redundant generation of $k$
words of length $n$ in $\mathcal{O}(k\cdot n\cdot \log{n})$ arithmetic
operations, after a precomputation of $\Theta(n)$ numbers. The overall
complexity is therefore dominated by the generation of $k$ words, and the
non-redundancy comes at a negligible cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0307</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0307</id><created>2012-11-01</created><authors><author><keyname>Chevrier</keyname><forenames>Joel</forenames></author><author><keyname>Madani</keyname><forenames>Laya</forenames></author><author><keyname>Ledenmat</keyname><forenames>Simon</forenames></author><author><keyname>Bsiesy</keyname><forenames>Ahmad</forenames></author></authors><title>Teaching Classical Mechanics using Smartphones</title><categories>physics.ed-ph cs.HC</categories><comments>Presented at WCPE 2012 Istanbul</comments><doi>10.1119/1.4818381</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a personal computer and a smartphone, iMecaProf is a software that
provides a complete teaching environment for practicals associated to a
Classical Mechanics course. iMecaProf proposes a visual, real time and
interactive representation of data transmitted by a smartphone using the
formalism of Classical Mechanics. Using smartphones is more than using a set of
sensors. iMecaProf shows students that important concepts of physics they here
learn, are necessary to control daily life smartphone operations. This is
practical introduction to mechanical microsensors that are nowadays a key
technology in advanced trajectory control. First version of iMecaProf can be
freely downloaded. It will be tested this academic year in Universit\'e Joseph
Fourier (Grenoble, France)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0313</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0313</id><created>2012-11-01</created><authors><author><keyname>Urriza</keyname><forenames>Paulo</forenames></author><author><keyname>Rebeiz</keyname><forenames>Eric</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic
  Correlation Significance Test</title><categories>cs.PF stat.AP</categories><comments>26 pages, 8 figures, submitted to IEEE JSAC: Cognitive Radio Series.
  arXiv admin note: substantial text overlap with arXiv:1210.8176</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and analyze a spectrum sensing method based on
cyclostationarity specifically targeted for receivers with multiple antennas.
This detection method is used for determining the presence or absence of
primary users in cognitive radio networks based on the eigenvalues of the
cyclic covariance matrix of received signals. In particular, the cyclic
correlation significance test is used to detect a specific signal-of-interest
by exploiting knowledge of its cyclic frequencies. Analytical expressions for
the probability of detection and probability of false-alarm under both
spatially uncorrelated or spatially correlated noise are derived and verified
by simulation. The detection performance in a Rayleigh flat-fading environment
is found and verified through simulations. One of the advantages of the
proposed method is that the detection threshold is shown to be independent of
both the number of samples and the noise covariance, effectively eliminating
the dependence on accurate noise estimation. The proposed method is also shown
to provide higher detection probability and better robustness to noise
uncertainty than existing multiple-antenna cyclostationary-based spectrum
sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0320</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0320</id><created>2012-11-01</created><authors><author><keyname>Al-Rfou'</keyname><forenames>Rami</forenames></author><author><keyname>Jannen</keyname><forenames>William</forenames></author><author><keyname>Patwardhan</keyname><forenames>Nikhil</forenames></author></authors><title>TrackMeNot-so-good-after-all</title><categories>cs.IR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TrackMeNot is a Firefox plugin with laudable intentions - protecting your
privacy. By issuing a customizable stream of random search queries on its
users' behalf, TrackMeNot surmises that enough search noise will prevent its
users' true query profiles from being discerned. However, we find that
clustering queries by semantic relatedness allows us to disentangle a
nontrivial subset of true user queries from TrackMeNot issued noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0330</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0330</id><created>2012-11-01</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Improved rank bounds for design matrices and a new proof of Kelly's
  theorem</title><categories>math.CO cs.CC cs.CG</categories><msc-class>52C35, 05B20</msc-class><acm-class>F.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the rank of complex sparse matrices in which the supports of
different columns have small intersections. The rank of these matrices, called
design matrices, was the focus of a recent work by Barak et. al. (BDWY11) in
which they were used to answer questions regarding point configurations. In
this work we derive near-optimal rank bounds for these matrices and use them to
obtain asymptotically tight bounds in many of the geometric applications. As a
consequence of our improved analysis, we also obtain a new, linear algebraic,
proof of Kelly's theorem, which is the complex analog of the Sylvester-Gallai
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0331</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0331</id><created>2012-11-01</created><authors><author><keyname>Ai</keyname><forenames>Albert</forenames></author><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Sylvester-Gallai type theorems for approximate collinearity</title><categories>math.CO cs.CC cs.CG cs.DM</categories><msc-class>52C35, 05B20</msc-class><acm-class>F.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study questions in incidence geometry where the precise position of points
is `blurry' (e.g. due to noise, inaccuracy or error). Thus lines are replaced
by narrow tubes, and more generally affine subspaces are replaced by their
small neighborhood. We show that the presence of a sufficiently large number of
approximately collinear triples in a set of points in d dimensional complex
space implies that the points are close to a low dimensional affine subspace.
This can be viewed as a stable variant of the Sylvester-Gallai theorem and its
extensions.
  Building on the recently found connection between Sylvester-Gallai type
theorems and complex Locally Correctable Codes (LCCs), we define the new notion
of stable LCCs, in which the (local) correction procedure can also handle small
perturbations in the euclidean metric. We prove that such stable codes with
constant query complexity do not exist. No impossibility results were known in
any such local setting for more than 2 queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0358</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0358</id><created>2012-11-01</created><updated>2013-03-22</updated><authors><author><keyname>Damianou</keyname><forenames>Andreas C.</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Deep Gaussian Processes</title><categories>stat.ML cs.LG math.PR</categories><comments>9 pages, 8 figures. Appearing in Proceedings of the 16th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2013</comments><msc-class>60G15, 58E30</msc-class><acm-class>G.3; G.1.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a
deep belief network based on Gaussian process mappings. The data is modeled as
the output of a multivariate GP. The inputs to that Gaussian process are then
governed by another GP. A single layer model is equivalent to a standard GP or
the GP latent variable model (GP-LVM). We perform inference in the model by
approximate variational marginalization. This results in a strict lower bound
on the marginal likelihood of the model which we use for model selection
(number of layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient descent for
optimization. Our fully Bayesian treatment allows for the application of deep
models even when data is scarce. Model selection by our variational bound shows
that a five layer hierarchy is justified even when modelling a digit data set
containing only 150 examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0361</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0361</id><created>2012-11-01</created><authors><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Park</keyname><forenames>Jae Young</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Sketched SVD: Recovering Spectral Features from Compressive Measurements</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a streaming data model in which n sensors observe individual
streams of data, presented in a turnstile model. Our goal is to analyze the
singular value decomposition (SVD) of the matrix of data defined implicitly by
the stream of updates. Each column i of the data matrix is given by the stream
of updates seen at sensor i. Our approach is to sketch each column of the
matrix, forming a &quot;sketch matrix&quot; Y, and then to compute the SVD of the sketch
matrix. We show that the singular values and right singular vectors of Y are
close to those of X, with small relative error. We also believe that this bound
is of independent interest in non-streaming and non-distributed data collection
settings.
  Assuming that the data matrix X is of size Nxn, then with m linear
measurements of each column of X, we obtain a smaller matrix Y with dimensions
mxn. If m = O(k \epsilon^{-2} (log(1/\epsilon) + log(1/\delta)), where k
denotes the rank of X, then with probability at least 1-\delta, the singular
values \sigma'_j of Y satisfy the following relative error result
  (1-\epsilon)^(1/2)&lt;= \sigma'_j/\sigma_j &lt;= (1 + \epsilon)^(1/2) as compared
to the singular values \sigma_j of the original matrix X. Furthermore, the
right singular vectors v'_j of Y satisfy
  ||v_j-v_j'||_2 &lt;= min(sqrt{2},
(\epsilon\sqrt{1+\epsilon})/(\sqrt{1-\epsilon}) max_{i\neq j}
(\sqrt{2}\sigma_i\sigma_j)/(min_{c\in[-1,1]}(|\sigma^2_i-\sigma^2_j(1+c\epsilon)|)))
as compared to the right singular vectors v_j of X. We apply this result to
obtain a streaming graph algorithm to approximate the eigenvalues and
eigenvectors of the graph Laplacian in the case where the graph has low rank
(many connected components).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0377</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0377</id><created>2012-11-02</created><authors><author><keyname>Tiwari</keyname><forenames>Rajesh Kumar</forenames></author><author><keyname>Sahoo</keyname><forenames>Gadadhar</forenames></author></authors><title>Some New Methodologies for Image Hiding using Steganographic Techniques</title><categories>cs.CR cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security and memory management are the major demands for electronics devices
like ipods, cell phones, pmps, iphones and digital cameras. In this paper, we
have suggested a high level of security mechanism by considering the concept of
steganography along with the principle of cryptography. Four different methods
that can save a considerable amount of memory space have been discussed. Based
on these methods, we have constructed secured stego image creator and secured
multi image viewer in Microsoft platform so as to provide high level of
security and using less memory space for storage of image files in the above
said electronic devices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0381</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0381</id><created>2012-11-02</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>The use of percentiles and percentile rank classes in the analysis of
  bibliometric data: Opportunities and limits</title><categories>cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Percentiles have been established in bibliometrics as an important
alternative to mean-based indicators for obtaining a normalized citation impact
of publications. Percentiles have a number of advantages over standard
bibliometric indicators used frequently: for example, their calculation is not
based on the arithmetic mean which should not be used for skewed bibliometric
data. This study describes the opportunities and limits and the advantages and
disadvantages of using percentiles in bibliometrics. We also address problems
in the calculation of percentiles and percentile rank classes for which there
is not (yet) a satisfactory solution. It will be hard to compare the results of
different percentile-based studies with each other unless it is clear that the
studies were done with the same choices for percentile calculation and rank
assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0390</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0390</id><created>2012-11-02</created><authors><author><keyname>Allahbakhsh</keyname><forenames>Mohammad</forenames></author><author><keyname>Ignjatovic</keyname><forenames>Aleksandar</forenames></author></authors><title>Rating through Voting: An Iterative Method for Robust Rating</title><categories>cs.IR cs.HC cs.SI</categories><comments>12 pages, 5 figures</comments><doi>10.1109/TPDS.2013.215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce an iterative voting algorithm and then use it to
obtain a rating method which is very robust against collusion attacks as well
as random and biased raters. Unlike the previous iterative methods, our method
is not based on comparing submitted evaluations to an approximation of the
final rating scores, and it entirely decouples credibility assessment of the
cast evaluations from the ranking itself. The convergence of our algorithm
relies on the existence of a fixed point of a continuous mapping which is also
a stationary point of a constrained optimization objective. We have implemented
and tested our rating method using both simulated data as well as real world
data. In particular, we have applied our method to movie evaluations obtained
from MovieLens and compared our results with IMDb and Rotten Tomatoes movie
rating sites. Not only are the ratings provided by our system very close to
IMDb rating scores, but when we differ from the IMDb ratings, the direction of
such differences is essentially always towards the ratings provided by the
critics in Rotten Tomatoes. Our tests demonstrate high efficiency of our
method, especially for very large online rating systems, for which trust
management is both of the highest importance and one of the most challenging
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0391</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0391</id><created>2012-11-02</created><updated>2013-08-26</updated><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author></authors><title>Below All Subsets for Some Permutational Counting Problems</title><categories>cs.DS</categories><comments>Corrected several technical errors, added comment on how to use the
  algorithm for ATSP, and changed title slightly to a more adequate one</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the two problems of computing the permanent of an $n\times n$
matrix of $\operatorname{poly}(n)$-bit integers and counting the number of
Hamiltonian cycles in a directed $n$-vertex multigraph with
$\operatorname{exp}(\operatorname{poly}(n))$ edges can be reduced to relatively
few smaller instances of themselves. In effect we derive the first
deterministic algorithms for these two problems that run in $o(2^n)$ time in
the worst case. Classic $\operatorname{poly}(n)2^n$ time algorithms for the two
problems have been known since the early 1960's. Our algorithms run in
$2^{n-\Omega(\sqrt{n/\log n})}$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0415</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0415</id><created>2012-11-02</created><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Capacity and Security of Heterogeneous Distributed Storage Systems</title><categories>cs.DC cs.IT cs.NI math.IT</categories><comments>7 pages, 2 figures</comments><msc-class>68P30</msc-class><journal-ref>IEEE JSAC, December 2013, Volume: 31, Issue: 12, Pages: 2701 -
  2709</journal-ref><doi>10.1109/JSAC.2013.131210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity of heterogeneous distributed storage systems under
repair dynamics. Examples of these systems include peer-to-peer storage clouds,
wireless, and Internet caching systems. Nodes in a heterogeneous system can
have different storage capacities and different repair bandwidths. We give
lower and upper bounds on the system capacity. These bounds depend on either
the average resources per node, or on a detailed knowledge of the node
characteristics. Moreover, we study the case in which nodes may be compromised
by an eavesdropper, and give bounds on the system secrecy capacity. One
implication of our results is that symmetric repair maximizes the capacity of a
homogeneous system, which justifies the model widely used in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0418</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0418</id><created>2012-11-02</created><authors><author><keyname>Gr&#x16b;z&#x12b;tis</keyname><forenames>Normunds</forenames></author><author><keyname>Ne&#x161;pore</keyname><forenames>Gunta</forenames></author><author><keyname>Saul&#x12b;te</keyname><forenames>Baiba</forenames></author></authors><title>Verbalizing Ontologies in Controlled Baltic Languages</title><categories>cs.CL cs.AI</categories><journal-ref>Frontiers in Artificial Intelligence and Applications, Vol. 219,
  IOS Press, 2010, pp. 187-194</journal-ref><doi>10.3233/978-1-60750-641-6-187</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlled natural languages (mostly English-based) recently have emerged as
seemingly informal supplementary means for OWL ontology authoring, if compared
to the formal notations that are used by professional knowledge engineers. In
this paper we present by examples controlled Latvian language that has been
designed to be compliant with the state of the art Attempto Controlled English.
We also discuss relation with controlled Lithuanian language that is being
designed in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0424</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0424</id><created>2012-11-02</created><authors><author><keyname>Zang</keyname><forenames>Zhaoxiang</forenames></author><author><keyname>Li</keyname><forenames>Dehua</forenames></author><author><keyname>Wang</keyname><forenames>Junying</forenames></author></authors><title>Learning classifier systems with memory condition to solve non-Markov
  problems</title><categories>cs.NE cs.AI</categories><comments>34 pages, 15 figures, 1 table</comments><doi>10.1007/s00500-014-1357-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the family of Learning Classifier Systems, the classifier system XCS has
been successfully used for many applications. However, the standard XCS has no
memory mechanism and can only learn optimal policy in Markov environments,
where the optimal action is determined solely by the state of current sensory
input. In practice, most environments are partially observable environments on
agent's sensation, which are also known as non-Markov environments. Within
these environments, XCS either fails, or only develops a suboptimal policy,
since it has no memory. In this work, we develop a new classifier system based
on XCS to tackle this problem. It adds an internal message list to XCS as the
memory list to record input sensation history, and extends a small number of
classifiers with memory conditions. The classifier's memory condition, as a
foothold to disambiguate non-Markov states, is used to sense a specified
element in the memory list. Besides, a detection method is employed to
recognize non-Markov states in environments, to avoid these states controlling
over classifiers' memory conditions. Furthermore, four sets of different
complex maze environments have been tested by the proposed method. Experimental
results show that our system is one of the best techniques to solve partially
observable environments, compared with some well-known classifier systems
proposed for these environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0439</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0439</id><created>2012-11-02</created><authors><author><keyname>Ashton</keyname><forenames>Simon R. F.</forenames></author><author><keyname>Sollich</keyname><forenames>Peter</forenames></author></authors><title>Learning curves for multi-task Gaussian process regression</title><categories>cs.LG cond-mat.dis-nn stat.ML</categories><comments>9 pages, to appear in Advances in Neural Information Processing
  Systems 25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the average case performance of multi-task Gaussian process (GP)
regression as captured in the learning curve, i.e. the average Bayes error for
a chosen task versus the total number of examples $n$ for all tasks. For GP
covariances that are the product of an input-dependent covariance function and
a free-form inter-task covariance matrix, we show that accurate approximations
for the learning curve can be obtained for an arbitrary number of tasks $T$. We
use these to study the asymptotic learning behaviour for large $n$.
Surprisingly, multi-task learning can be asymptotically essentially useless, in
the sense that examples from other tasks help only when the degree of
inter-task correlation, $\rho$, is near its maximal value $\rho=1$. This effect
is most extreme for learning of smooth target functions as described by e.g.
squared exponential kernels. We also demonstrate that when learning many tasks,
the learning curves separate into an initial phase, where the Bayes error on
each task is reduced down to a plateau value by &quot;collective learning&quot; even
though most tasks have not seen examples, and a final decay that occurs once
the number of examples is proportional to the number of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0447</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0447</id><created>2012-11-02</created><authors><author><keyname>Du</keyname><forenames>Wei</forenames></author><author><keyname>Liao</keyname><forenames>Yongjun</forenames></author><author><keyname>Geurts</keyname><forenames>and Pierre</forenames></author><author><keyname>Leduc</keyname><forenames>Guy</forenames></author></authors><title>Ordinal Rating of Network Performance and Inference by Matrix Completion</title><categories>cs.NI cs.LG</categories><comments>submitted to the Passive and Active Measurement Conference (PAM),
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the large-scale acquisition of end-to-end network
performance. We made two distinct contributions: ordinal rating of network
performance and inference by matrix completion. The former reduces measurement
costs and unifies various metrics which eases their processing in applications.
The latter enables scalable and accurate inference with no requirement of
structural information of the network nor geometric constraints. By combining
both, the acquisition problem bears strong similarities to recommender systems.
This paper investigates the applicability of various matrix factorization
models used in recommender systems. We found that the simple regularized matrix
factorization is not only practical but also produces accurate results that are
beneficial for peer selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0463</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0463</id><created>2012-11-02</created><authors><author><keyname>Seamone</keyname><forenames>Ben</forenames></author><author><keyname>Stevens</keyname><forenames>Brett</forenames></author></authors><title>Sequence variations of the 1-2-3 Conjecture and irregularity strength</title><categories>math.CO cs.DM</categories><comments>Accepted to Discrete Mathematics and Theoretical Computer Science</comments><msc-class>05C15 (Primary) 05C78 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Karonski, Luczak, and Thomason (2004) conjectured that, for any connected
graph G on at least three vertices, there exists an edge weighting from {1,2,3}
such that adjacent vertices receive different sums of incident edge weights.
Bartnicki, Grytczuk, and Niwcyk (2009) made a stronger conjecture, that each
edge's weight may be chosen from an arbitrary list of size 3 rather than
{1,2,3}. We examine a variation of these conjectures, where each vertex is
coloured with a sequence of edge weights. Such a colouring relies on an
ordering of the graph's edges, and so two variations arise -- one where we may
choose any ordering of the edges and one where the ordering is fixed. In the
former case, we bound the list size required for any graph. In the latter, we
obtain a bound on list sizes for graphs with sufficiently large minimum degree.
We also extend our methods to a list variation of irregularity strength, where
each vertex receives a distinct sequence of edge weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0479</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0479</id><created>2012-11-02</created><updated>2013-01-21</updated><authors><author><keyname>B&#xe4;ckstr&#xf6;m</keyname><forenames>Christer</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Parameterized Complexity and Kernel Bounds for Hard Planning Problems</title><categories>cs.DS cs.AI</categories><comments>This is the full version of a paper that will appear in the Proc. of
  CIAC 2013</comments><journal-ref>Proceedings of CIAC 2013, LNCS 7878, pp. 13-24, 2013</journal-ref><doi>10.1007/978-3-642-38233-8_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The propositional planning problem is a notoriously difficult computational
problem. Downey et al. (1999) initiated the parameterized analysis of planning
(with plan length as the parameter) and B\&quot;ackstr\&quot;om et al. (2012) picked up
this line of research and provided an extensive parameterized analysis under
various restrictions, leaving open only one stubborn case. We continue this
work and provide a full classification. In particular, we show that the case
when actions have no preconditions and at most $e$ postconditions is
fixed-parameter tractable if $e\leq 2$ and W[1]-complete otherwise. We show
fixed-parameter tractability by a reduction to a variant of the Steiner Tree
problem; this problem has been shown fixed-parameter tractable by Guo et al.
(2007). If a problem is fixed-parameter tractable, then it admits a
polynomial-time self-reduction to instances whose input size is bounded by a
function of the parameter, called the kernel. For some problems, this function
is even polynomial which has desirable computational implications. Recent
research in parameterized complexity has focused on classifying fixed-parameter
tractable problems on whether they admit polynomial kernels or not. We revisit
all the previously obtained restrictions of planning that are fixed-parameter
tractable and show that none of them admits a polynomial kernel unless the
polynomial hierarchy collapses to its third level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0498</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0498</id><created>2012-11-02</created><authors><author><keyname>Al-Rfou'</keyname><forenames>Rami</forenames></author></authors><title>Detecting English Writing Styles For Non-native Speakers</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing writing styles of non-native speakers is a challenging task. In
this paper, we analyze the comments written in the discussion pages of the
English Wikipedia. Using learning algorithms, we are able to detect native
speakers' writing style with an accuracy of 74%. Given the diversity of the
English Wikipedia users and the large number of languages they speak, we
measure the similarities among their native languages by comparing the
influence they have on their English writing style. Our results show that
languages known to have the same origin and development path have similar
footprint on their speakers' English writing style. To enable further studies,
the dataset we extracted from Wikipedia will be made available publicly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0501</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0501</id><created>2012-11-01</created><updated>2014-04-30</updated><authors><author><keyname>Costello</keyname><forenames>Fintan</forenames></author><author><keyname>Watts</keyname><forenames>Paul</forenames></author></authors><title>Surprisingly Rational: Probability theory plus noise explains biases in
  judgment</title><categories>physics.data-an cs.AI stat.AP</categories><comments>64 pages. Final preprint version. In press, Psychological Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The systematic biases seen in people's probability judgments are typically
taken as evidence that people do not reason about probability using the rules
of probability theory, but instead use heuristics which sometimes yield
reasonable judgments and sometimes systematic biases. This view has had a major
impact in economics, law, medicine, and other fields; indeed, the idea that
people cannot reason with probabilities has become a widespread truism. We
present a simple alternative to this view, where people reason about
probability according to probability theory but are subject to random variation
or noise in the reasoning process. In this account the effect of noise is
cancelled for some probabilistic expressions: analysing data from two
experiments we find that, for these expressions, people's probability judgments
are strikingly close to those required by probability theory. For other
expressions this account produces systematic deviations in probability
estimates. These deviations explain four reliable biases in human probabilistic
reasoning (conservatism, subadditivity, conjunction and disjunction fallacies).
These results suggest that people's probability judgments embody the rules of
probability theory, and that biases in those judgments are due to the effects
of random noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0515</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0515</id><created>2012-11-02</created><authors><author><keyname>Iglesias</keyname><forenames>Jennifer</forenames></author><author><keyname>Ince</keyname><forenames>Nathaniel</forenames></author><author><keyname>Loh</keyname><forenames>Po-Shen</forenames></author></authors><title>Computing with voting trees</title><categories>math.CO cs.DM</categories><comments>13 pages</comments><msc-class>05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical paradox of social choice theory asserts that there is no fair
way to deterministically select a winner in an election among more than two
candidates; the only definite collective preferences are between individual
pairs of candidates. Combinatorially, one may summarize this information with a
graph-theoretic tournament on N vertices (one per candidate), placing an edge
from U to V if U would beat V in an election between only those two candidates
(no ties are permitted). One well-studied procedure for selecting a winner is
to specify a complete binary tree whose leaves are labeled by the candidates,
and evaluate it by running pairwise elections between the pairs of leaves,
sending the winners to successive rounds of pairwise elections which ultimately
terminate with a single winner. This structure is called a voting tree.
  Much research has investigated which functions on tournaments are computable
in this way. Fischer, Procaccia, and Samorodnitsky quantitatively studied the
computability of the Copeland rule, which returns a vertex of maximum
out-degree in the given tournament. Perhaps surprisingly, the best previously
known voting tree could only guarantee a returned out-degree of at least log_2
N, despite the fact that every tournament has a vertex of degree at least
(N-1)/2. In this paper, we present three constructions, the first of which
substantially improves this guarantee to \Theta(sqrt{N}). The other two
demonstrate the richness of the voting tree universe, with a tree that resists
manipulation, and a tree which implements arithmetic modulo three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0517</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0517</id><created>2012-11-02</created><authors><author><keyname>Dharmawansa</keyname><forenames>Prathapasinghe</forenames></author><author><keyname>McKay</keyname><forenames>Matthew</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author></authors><title>Distributions of Demmel and Related Condition Numbers</title><categories>math.ST cs.CC cs.NA stat.TH</categories><comments>To appear in SIAM Journal on Matrix Analysis and Applications (SIMAX)</comments><msc-class>15B52, 65F35, 15A18, 62H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random matrix $\mathbf{A}\in\mathbb{C}^{m\times n}$ ($m \geq n$)
containing independent complex Gaussian entries with zero mean and unit
variance, and let $0&lt;\lambda_1\leq \lambda_{2}\leq ...\leq \lambda_n&lt;\infty$
denote the eigenvalues of $\mathbf{A}^{*}\mathbf{A}$ where $(\cdot)^*$
represents conjugate-transpose. This paper investigates the distribution of the
random variables $\frac{\sum_{j=1}^n \lambda_j}{\lambda_k}$, for $k = 1$ and $k
= 2$. These two variables are related to certain condition number metrics,
including the so-called Demmel condition number, which have been shown to arise
in a variety of applications. For both cases, we derive new exact expressions
for the probability densities, and establish the asymptotic behavior as the
matrix dimensions grow large. In particular, it is shown that as $n$ and $m$
tend to infinity with their difference fixed, both densities scale on the order
of $n^3$. After suitable transformations, we establish exact expressions for
the asymptotic densities, obtaining simple closed-form expressions in some
cases. Our results generalize the work of Edelman on the Demmel condition
number for the case $m = n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0518</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0518</id><created>2012-11-01</created><authors><author><keyname>Campbell</keyname><forenames>Ellsworth</forenames></author><author><keyname>Salath&#xe9;</keyname><forenames>Marcel</forenames></author></authors><title>Complex social contagion makes networks more vulnerable to disease
  outbreaks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>10 pages, 2 paneled figures, 2 supplemental figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network analysis is now widely used to investigate the dynamics of
infectious disease spread from person to person. Vaccination dramatically
disrupts the disease transmission process on a contact network, and indeed,
sufficiently high vaccination rates can disrupt the process to such an extent
that disease transmission on the network is effectively halted. Here, we build
on mounting evidence that health behaviors - such as vaccination, and refusal
thereof - can spread through social networks through a process of complex
contagion that requires social reinforcement. Using network simulations that
model both the health behavior and the infectious disease spread, we find that
under otherwise identical conditions, the process by which the health behavior
spreads has a very strong effect on disease outbreak dynamics. This variability
in dynamics results from differences in the topology within susceptible
communities that arise during the health behavior spreading process, which in
turn depends on the topology of the overall social network. Our findings point
to the importance of health behavior spread in predicting and controlling
disease outbreaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0519</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0519</id><created>2012-11-02</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Route 20, autobahn 7 and Physarum polycephalum: Approximating longest
  roads in USA and Germany with slime mould on 3D terrains</title><categories>nlin.PS cs.ET nlin.AO</categories><journal-ref>IEEE Trans Cybernetics 20 March 2013 Issue 99</journal-ref><doi>10.1109/TCYB.2013.2248359</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acellular slime mould Physarum polycephalum is a monstrously large single
cell visible by an unaided eye. It shows sophisticated behavioural traits in
foraging for nutrients and developing an optimal transport network of
protoplasmic tubes spanning sources of nutrients. The slime mould sufficiently
approximates man-made transport networks on a flat substrate. Does slime mould
imitate man-made transport networks on three-dimensional terrain as well as it
does on a flat substrate? We simplified the problem to approximation of a
single transport route. In laboratory experiments with 3D Nylon terrains of USA
and Germany we imitated development of route 20, the longest road in USA, and
autobahn 7, the longest national motorway in Europe. We found that slime mould
builds longer transport routes on 3D terrains, comparing to flat agar plates
yet sufficiently approximates man-made transport routes studied. We discuss how
nutrients placed in destination sites affect performance of slime mould, how
the mould navigates around elevations, and offer an automaton model of slime
mould which might explain variability of the protoplasmic routes constructed.
We also display results of scoping experiments with slime mould on 3D terrains
of Russia and United Kingdom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0524</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0524</id><created>2012-11-02</created><authors><author><keyname>Lampis</keyname><forenames>Michael</forenames></author></authors><title>Local Improvement Gives Better Expanders</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been known that random regular graphs are with high probability
good expanders. This was first established in the 1980s by Bollob\'as by
directly calculating the probability that a set of vertices has small expansion
and then applying the union bound.
  In this paper we improve on this analysis by relying on a simple high-level
observation: if a graph contains a set of vertices with small expansion then it
must also contain such a set of vertices that is locally optimal, that is, a
set whose expansion cannot be made smaller by exchanging a vertex from the set
with one from the set's complement. We show that the probability that a set of
vertices satisfies this additional property is significantly smaller. Thus,
after again applying the union bound, we obtain improved lower bounds on the
expansion of random $\Delta$-regular graphs for $\Delta\ge 4$. In fact, the
gains from this analysis increase as $\Delta$ grows, a fact we explain by
extending our technique to general $\Delta$. Thus, in the end we obtain an
improvement not only for some small special cases but on the general asymptotic
bound on the expansion of $\Delta$-regular graphs given by Bollob\'as.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0557</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0557</id><created>2012-11-02</created><authors><author><keyname>Schkufza</keyname><forenames>Eric</forenames></author><author><keyname>Sharma</keyname><forenames>Rahul</forenames></author><author><keyname>Aiken</keyname><forenames>Alex</forenames></author></authors><title>Stochastic Superoptimization</title><categories>cs.PF cs.PL</categories><comments>To appear in ASPLOS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the loop-free, binary superoptimization task as a stochastic
search problem. The competing constraints of transformation correctness and
performance improvement are encoded as terms in a cost function, and a Markov
Chain Monte Carlo sampler is used to rapidly explore the space of all possible
programs to find one that is an optimization of a given target program.
Although our method sacrifices com- pleteness, the scope of programs we are
able to reason about, and the quality of the programs we produce, far exceed
those of existing superoptimizers. Beginning from binaries com- piled by llvm
-O0 for 64-bit X86, our prototype implemen- tation, STOKE, is able to produce
programs which either match or outperform the code sequences produced by gcc
with full optimizations enabled, and, in some cases, expert handwritten
assembly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0575</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0575</id><created>2012-11-02</created><authors><author><keyname>Bharucha</keyname><forenames>Zubin</forenames></author><author><keyname>Calvanese</keyname><forenames>Emilio</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Feki</keyname><forenames>Afef</forenames></author><author><keyname>De Domenico</keyname><forenames>Antonio</forenames></author><author><keyname>Galindo-Serrano</keyname><forenames>Ana</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Kwan</keyname><forenames>Raymond</forenames></author><author><keyname>Liu</keyname><forenames>Jimin</forenames></author><author><keyname>L&#xf3;pez-P&#xe9;rez</keyname><forenames>David</forenames></author><author><keyname>Maqbool</keyname><forenames>Massod</forenames></author><author><keyname>Peng</keyname><forenames>Ying</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir</forenames></author><author><keyname>de la Roche</keyname><forenames>Guillaume</forenames></author><author><keyname>Uygungelen</keyname><forenames>Serkan</forenames></author><author><keyname>Valcarce</keyname><forenames>Alvaro</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Small Cell Deployments: Recent Advances and Research Challenges</title><categories>cs.NI</categories><comments>19 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the outcomes of the 5th International Workshop on
Femtocells held at King's College London, UK, on the 13th and 14th of February,
2012.The workshop hosted cutting-edge presentations about the latest advances
and research challenges in small cell roll-outs and heterogeneous cellular
networks. This paper provides some cutting edge information on the developments
of Self-Organizing Networks (SON) for small cell deployments, as well as
related standardization supports on issues such as carrier aggregation (CA),
Multiple-Input-Multiple-Output (MIMO) techniques, and enhanced Inter-Cell
Interference Coordination (eICIC), etc. Furthermore, some recent efforts on
issues such as energy-saving as well as Machine Learning (ML) techniques on
resource allocation and multi-cell cooperation are described. Finally, current
developments on simulation tools and small cell deployment scenarios are
presented. These topics collectively represent the current trends in small cell
deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0582</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0582</id><created>2012-11-02</created><authors><author><keyname>Kl&#xf6;ckner</keyname><forenames>Andreas</forenames></author><author><keyname>Warburton</keyname><forenames>Timothy</forenames></author><author><keyname>Hesthaven</keyname><forenames>Jan S.</forenames></author></authors><title>High-Order Discontinuous Galerkin Methods by GPU Metaprogramming</title><categories>cs.MS math.NA</categories><comments>To appear as part of &quot;GPU Solutions to Multi-scale Problems in
  Science and Engineering&quot;, http://books.google.com/books?vid=9783642164040</comments><journal-ref>ISBN 9783642164040, Springer, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discontinuous Galerkin (DG) methods for the numerical solution of partial
differential equations have enjoyed considerable success because they are both
flexible and robust: They allow arbitrary unstructured geometries and easy
control of accuracy without compromising simulation stability. In a recent
publication, we have shown that DG methods also adapt readily to execution on
modern, massively parallel graphics processors (GPUs). A number of qualities of
the method contribute to this suitability, reaching from locality of reference,
through regularity of access patterns, to high arithmetic intensity. In this
article, we illuminate a few of the more practical aspects of bringing DG onto
a GPU, including the use of a Python-based metaprogramming infrastructure that
was created specifically to support DG, but has found many uses across all
disciplines of computational science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0587</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0587</id><created>2012-11-02</created><updated>2012-11-21</updated><authors><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Partition Tree Weighting</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Partition Tree Weighting technique, an efficient
meta-algorithm for piecewise stationary sources. The technique works by
performing Bayesian model averaging over a large class of possible partitions
of the data into locally stationary segments. It uses a prior, closely related
to the Context Tree Weighting technique of Willems, that is well suited to data
compression applications. Our technique can be applied to any coding
distribution at an additional time and space cost only logarithmic in the
sequence length. We provide a competitive analysis of the redundancy of our
method, and explore its application in a variety of settings. The order of the
redundancy and the complexity of our algorithm matches those of the best
competitors available in the literature, and the new algorithm exhibits a
superior complexity-performance trade-off in our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0589</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0589</id><created>2012-11-02</created><authors><author><keyname>Lyons</keyname><forenames>Russell</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author></authors><title>Sharp Bounds on Random Walk Eigenvalues via Spectral Embedding</title><categories>math.PR cs.DM cs.DS math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral embedding of graphs uses the top k eigenvectors of the random walk
matrix to embed the graph into R^k. The primary use of this embedding has been
for practical spectral clustering algorithms [SM00,NJW02]. Recently, spectral
embedding was studied from a theoretical perspective to prove higher order
variants of Cheeger's inequality [LOT12,LRTV12].
  We use spectral embedding to provide a unifying framework for bounding all
the eigenvalues of graphs. For example, we show that for any finite graph with
n vertices and all k &gt;= 2, the k-th largest eigenvalue is at most
1-Omega(k^3/n^3), which extends the only other such result known, which is for
k=2 only and is due to [LO81]. This upper bound improves to 1-Omega(k^2/n^2) if
the graph is regular. We generalize these results, and we provide sharp bounds
on the spectral measure of various classes of graphs, including
vertex-transitive graphs and infinite graphs, in terms of specific graph
parameters like the volume growth.
  As a consequence, using the entire spectrum, we provide (improved) upper
bounds on the return probabilities and mixing time of random walks with
considerably shorter and more direct proofs. Our work introduces spectral
embedding as a new tool in analyzing reversible Markov chains. Furthermore,
building on [Lyo05], we design a local algorithm to approximate the number of
spanning trees of massive graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0592</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0592</id><created>2012-11-03</created><authors><author><keyname>Saboohi</keyname><forenames>Hadi</forenames></author><author><keyname>Kareem</keyname><forenames>Sameem Abdul</forenames></author></authors><title>Requirements of a Recovery Solution for Failure of Composite Web
  Services</title><categories>cs.SE</categories><comments>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.3,
  No.4, October 2012</comments><acm-class>H.3.5</acm-class><doi>10.5121/ijwest.2012.3402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web services are building blocks of interoperable systems. Composing Web
services makes the processes capable of doing complex tasks. Composite services
may fail during their execution which can be diagnosed by a mediator. The
mediator adapts the structure so that the failure is recovered. Moreover,
future executions should avoid the situation or organize a strategy to repair
the structure with a minimum delay. In this paper the failure reasons of a
composite service are reviewed. Furthermore, the requirements of a solution for
recovery of a system from a failure are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0602</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0602</id><created>2012-11-03</created><authors><author><keyname>Zhao</keyname><forenames>Jie</forenames></author><author><keyname>Zheng</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Tian</keyname><forenames>Hua</forenames></author></authors><title>Segmentation of ultrasound images of thyroid nodule for assisting fine
  needle aspiration cytology</title><categories>cs.CV</categories><comments>15pages,13figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The incidence of thyroid nodule is very high and generally increases with the
age. Thyroid nodule may presage the emergence of thyroid cancer. The thyroid
nodule can be completely cured if detected early. Fine needle aspiration
cytology is a recognized early diagnosis method of thyroid nodule. There are
still some limitations in the fine needle aspiration cytology, and the
ultrasound diagnosis of thyroid nodule has become the first choice for
auxiliary examination of thyroid nodular disease. If we could combine medical
imaging technology and fine needle aspiration cytology, the diagnostic rate of
thyroid nodule would be improved significantly. The properties of ultrasound
will degrade the image quality, which makes it difficult to recognize the edges
for physicians. Image segmentation technique based on graph theory has become a
research hotspot at present. Normalized cut (Ncut) is a representative one,
which is suitable for segmentation of feature parts of medical image. However,
how to solve the normalized cut has become a problem, which needs large memory
capacity and heavy calculation of weight matrix. It always generates over
segmentation or less segmentation which leads to inaccurate in the
segmentation. The speckle noise in B ultrasound image of thyroid tumor makes
the quality of the image deteriorate. In the light of this characteristic, we
combine the anisotropic diffusion model with the normalized cut in this paper.
After the enhancement of anisotropic diffusion model, it removes the noise in
the B ultrasound image while preserves the important edges and local details.
This reduces the amount of computation in constructing the weight matrix of the
improved normalized cut and improves the accuracy of the final segmentation
results. The feasibility of the method is proved by the experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0606</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0606</id><created>2012-11-03</created><updated>2012-12-29</updated><authors><author><keyname>Vyalyi</keyname><forenames>Mikhail N.</forenames></author></authors><title>On complexity of regular realizability problems</title><categories>cs.CC cs.FL</categories><comments>Submitted to Problems of Information Transmission. Corrected and
  extended version, main results are the same</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular realizability (RR) problem is testing nonemptiness of intersection
of some fixed language (filter) with given regular language. We study here
complexity of RR problems. It appears that for any language L there exists RR
problem equivalent to L under disjunctive reductions on nondeterministic log
space. It implies that for any level of polynomial hierarchy there exists
complete RR problem under polynomial reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0611</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0611</id><created>2012-11-03</created><updated>2013-03-27</updated><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Matrix approach to rough sets through vector matroids over a field</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough sets were proposed to deal with the vagueness and incompleteness of
knowledge in information systems. There are may optimization issues in this
field such as attribute reduction. Matroids generalized from matrices are
widely used in optimization. Therefore, it is necessary to connect matroids
with rough sets. In this paper, we take field into consideration and introduce
matrix to study rough sets through vector matroids. First, a matrix
representation of an equivalence relation is proposed, and then a matroidal
structure of rough sets over a field is presented by the matrix. Second, the
properties of the matroidal structure including circuits, bases and so on are
studied through two special matrix solution spaces, especially null space.
Third, over a binary field, we construct an equivalence relation from matrix
null space, and establish an algebra isomorphism from the collection of
equivalence relations to the collection of sets, which any member is a family
of the minimal non-empty sets that are supports of members of null space of a
binary dependence matrix. In a word, matrix provides a new viewpoint to study
rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0613</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0613</id><created>2012-11-03</created><updated>2012-12-17</updated><authors><author><keyname>Sarhrouni</keyname><forenames>ELkebir</forenames></author><author><keyname>Hammouch</keyname><forenames>Ahmed</forenames></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames></author></authors><title>Application of Symmetric Uncertainty and Mutual Information to
  Dimensionality Reduction and Classification of Hyperspectral Images</title><categories>cs.CV</categories><comments>14 pages, 7 Figure, 2 Tables, Paper keywords: Hyperspectral images,
  Classification, Feature Selection, Mutual information, Redundancy. arXiv
  admin note: text overlap with arXiv:1210.0052, arXiv:1211.0055</comments><journal-ref>International Journal of Engineering and Technology (IJET)
  VOL:4({\deg}5).P. 268--276. 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Remote sensing is a technology to acquire data for disatant substances,
necessary to construct a model knowledge for applications as classification.
Recently Hyperspectral Images (HSI) becomes a high technical tool that the main
goal is to classify the point of a region. The HIS is more than a hundred
bidirectional measures, called bands (or simply images), of the same region
called Ground Truth Map (GT). But some bands are not relevant because they are
affected by different atmospheric effects; others contain redundant
information; and high dimensionality of HSI features make the accuracy of
classification lower. All these bands can be important for some applications;
but for the classification a small subset of these is relevant. The problematic
related to HSI is the dimensionality reduction. Many studies use mutual
information (MI) to select the relevant bands. Others studies use the MI
normalized forms, like Symmetric Uncertainty, in medical imagery applications.
In this paper we introduce an algorithm based also on MI to select relevant
bands and it apply the Symmetric Uncertainty coefficient to control redundancy
and increase the accuracy of classification. This algorithm is feature
selection tool and a Filter strategy. We establish this study on HSI AVIRIS
92AV3C. This is an effectiveness, and fast scheme to control redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0616</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0616</id><created>2012-11-03</created><updated>2014-05-10</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>The complexity of learning halfspaces using generalized linear methods</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many popular learning algorithms (E.g. Regression, Fourier-Transform based
algorithms, Kernel SVM and Kernel ridge regression) operate by reducing the
problem to a convex optimization problem over a vector space of functions.
These methods offer the currently best approach to several central problems
such as learning half spaces and learning DNF's. In addition they are widely
used in numerous application domains. Despite their importance, there are still
very few proof techniques to show limits on the power of these algorithms.
  We study the performance of this approach in the problem of (agnostically and
improperly) learning halfspaces with margin $\gamma$. Let $\mathcal{D}$ be a
distribution over labeled examples. The $\gamma$-margin error of a hyperplane
$h$ is the probability of an example to fall on the wrong side of $h$ or at a
distance $\le\gamma$ from it. The $\gamma$-margin error of the best $h$ is
denoted $\mathrm{Err}_\gamma(\mathcal{D})$. An $\alpha(\gamma)$-approximation
algorithm receives $\gamma,\epsilon$ as input and, using i.i.d. samples of
$\mathcal{D}$, outputs a classifier with error rate $\le
\alpha(\gamma)\mathrm{Err}_\gamma(\mathcal{D}) + \epsilon$. Such an algorithm
is efficient if it uses $\mathrm{poly}(\frac{1}{\gamma},\frac{1}{\epsilon})$
samples and runs in time polynomial in the sample size.
  The best approximation ratio achievable by an efficient algorithm is
$O\left(\frac{1/\gamma}{\sqrt{\log(1/\gamma)}}\right)$ and is achieved using an
algorithm from the above class. Our main result shows that the approximation
ratio of every efficient algorithm from this family must be $\ge
\Omega\left(\frac{1/\gamma}{\mathrm{poly}\left(\log\left(1/\gamma\right)\right)}\right)$,
essentially matching the best known upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0618</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0618</id><created>2012-11-03</created><updated>2014-07-02</updated><authors><author><keyname>Spencer</keyname><forenames>Joel</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author><author><keyname>Xu</keyname><forenames>Kuang</forenames></author></authors><title>Queuing with future information</title><categories>math.PR cs.NI cs.PF</categories><comments>Published in at http://dx.doi.org/10.1214/13-AAP973 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP973</report-no><journal-ref>Annals of Applied Probability 2014, Vol. 24, No. 5, 2091-2142</journal-ref><doi>10.1214/13-AAP973</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an admissions control problem, where a queue with service rate $1-p$
receives incoming jobs at rate $\lambda\in(1-p,1)$, and the decision maker is
allowed to redirect away jobs up to a rate of $p$, with the objective of
minimizing the time-average queue length. We show that the amount of
information about the future has a significant impact on system performance, in
the heavy-traffic regime. When the future is unknown, the optimal average queue
length diverges at rate $\sim\log_{1/(1-p)}\frac{1}{1-\lambda}$, as $\lambda\to
1$. In sharp contrast, when all future arrival and service times are revealed
beforehand, the optimal average queue length converges to a finite constant,
$(1-p)/p$, as $\lambda\to1$. We further show that the finite limit of $(1-p)/p$
can be achieved using only a finite lookahead window starting from the current
time frame, whose length scales as $\mathcal{O}(\log\frac{1}{1-\lambda})$, as
$\lambda\to1$. This leads to the conjecture of an interesting duality between
queuing delay and the amount of information about the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0620</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0620</id><created>2012-11-03</created><authors><author><keyname>Nezakatolhoseini</keyname><forenames>Majid</forenames></author><author><keyname>Taherkhani</keyname><forenames>Mohammad Amin</forenames></author></authors><title>A Framework For Performance Evaluation Of ASIPS In Network-Based IDS</title><categories>cs.NI</categories><comments>13 pages, 3 figures, International Journal of Network Security &amp; Its
  Applications (IJNSA), Vol.4, No.5, September 2012</comments><doi>10.5121/ijnsa.2012.4504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays efficient usage of high-tech security tools and appliances is
considered as an important criterion for security improvement of computer
networks. Based on this assumption, Intrusion Detection and Prevention Systems
(IDPS) have key role for applying the defense in depth strategy. In this
situation, by increasing network bandwidth in addition to increasing number of
threats, Network-based IDPSes have been faced with performance challenge for
processing of huge traffic in the networks. A general solution for this
bottleneck is exploitation of efficient hardware architectures for performance
improvement of IDPS. In this paper a framework for analysis and performance
evaluation of application specific instruction set processors is presented for
usage in application of attack detection in Networkbased Intrusion Detection
Systems(NIDS). By running this framework as a security application on V850,
OR1K, MIPS32, ARM7TDMI and PowerPC32 microprocessors, their performance has
been evaluated and analyzed. For performance improvement, the compiler
optimization levels are employed and at the end; base on O2 optimization level
a new combination of optimization flags is presented. The experiments show that
the framework results 18.10% performance improvements for pattern matching on
ARM7TDMI microprocessors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0632</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0632</id><created>2012-11-03</created><updated>2013-01-22</updated><authors><author><keyname>Ouyang</keyname><forenames>Hua</forenames></author><author><keyname>He</keyname><forenames>Niao</forenames></author><author><keyname>Gray</keyname><forenames>Alexander</forenames></author></authors><title>Stochastic ADMM for Nonsmooth Optimization</title><categories>cs.LG math.OC stat.ML</categories><comments>A short version of this paper appears in the 5th NIPS Workshop on
  Optimization for Machine Learning, Lake Tahoe, Nevada, USA, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a stochastic setting for optimization problems with nonsmooth
convex separable objective functions over linear equality constraints. To solve
such problems, we propose a stochastic Alternating Direction Method of
Multipliers (ADMM) algorithm. Our algorithm applies to a more general class of
nonsmooth convex functions that does not necessarily have a closed-form
solution by minimizing the augmented function directly. We also demonstrate the
rates of convergence for our algorithm under various structural assumptions of
the stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\log
t/t)$ for strongly convex functions. Compared to previous literature, we
establish the convergence rate of ADMM algorithm, for the first time, in terms
of both the objective value and the feasibility violation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0645</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0645</id><created>2012-11-03</created><authors><author><keyname>Oliva</keyname><forenames>Pietro</forenames></author></authors><title>Project G.N.O.S.I.S.: Geographical Network Of Synoptic Information
  System</title><categories>cs.OH</categories><comments>3 pages, Proposal for future project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Everybody knows how much synoptic maps are useful today. An excellent example
above all is Google Earth: its simplicity and friendly interface allows every
user to have the Earth maps ready in just one simple layout; nevertheless a
crucial dimension is missing in Google Earth: the time. This doesn't mean we
simply aim to add history to Google Earth (though it could be already a nice
goal): the main idea behind GNOSIS project is to produce applications to &quot;dress
up&quot; the Globe with a set of skin-maps representing the most various different
kind of histories like the evolution of geology, genetics, agriculture,
ethnology, linguistics, musicology, metallurgy and so forth, in time. It may be
interesting in the near future to have such a possibility to watch on the map
the positions and movements of the armies during the battles of Waterloo or
Thermopylae, the spreading of the cultivation of corn in time, the rise and
fall of Roman Empire or the diffusion of Smallpox together with the spread of a
religion, a specific dialect, the early pottery techniques or the natural
resources available to pre-Columbian civilizations on a Google-Earth-map-like,
that is to say to have at one's hand the ultimate didactic-enciclopedic tool.
To do so we foresee the use of a general-purpose intermediate/high level
programming language, possibly object-oriented such C++ or Java.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0651</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0651</id><created>2012-11-03</created><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Non-Malleable Condensers for Arbitrary Min-Entropy, and Almost Optimal
  Protocols for Privacy Amplification</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1112.1045</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the problem of privacy amplification with an active adversary has
received a lot of attention. Given a shared n-bit weak random source X with
min-entropy k and a security parameter s, the main goal is to construct an
explicit 2-round privacy amplification protocol that achieves entropy loss
O(s). Dodis and Wichs \cite{DW09} showed that optimal protocols can be achieved
by constructing explicit non-malleable extractors. However, the best known
explicit non-malleable extractor only achieves k=0.49n \cite{Li12b} and
evidence in \cite{Li12b} suggests that constructing explicit non-malleable
extractors for smaller min-entropy may be hard. In an alternative approach, Li
\cite{Li12} introduced the notion of a non-malleable condenser and showed that
explicit non-malleable condensers also give optimal privacy amplification
protocols.
  In this paper, we give the first construction of non-malleable condensers for
arbitrary min-entropy. Using our construction, we obtain a 2-round privacy
amplification protocol with optimal entropy loss for security parameter up to
s=\Omega(\sqrt{k}). This is the first protocol that simultaneously achieves
optimal round complexity and optimal entropy loss for arbitrary min-entropy k.
We also generalize this result to obtain a protocol that runs in O(s/\sqrt{k})
rounds with optimal entropy loss, for security parameter up to s=\Omega(k).
This significantly improves the protocol in \cite{ckor}. Finally, we give a
better non-malleable condenser for linear min-entropy, and in this case obtain
a 2-round protocol with optimal entropy loss for security parameter up to
s=\Omega(k), which improves the entropy loss and communication complexity of
the protocol in \cite{Li12b}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0654</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0654</id><created>2012-11-03</created><updated>2013-01-02</updated><authors><author><keyname>Adam</keyname><forenames>Elie M.</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author></authors><title>On Threshold Models over Finite Networks</title><categories>cs.DM cs.GT cs.SI</categories><comments>49 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model for cascade effects over finite networks based on a
deterministic binary linear threshold model. Our starting point is a networked
coordination game where each agent's payoff is the sum of the payoffs coming
from pairwise interactions with each of the neighbors. We first establish that
the best response dynamics in this networked game is equivalent to the linear
threshold dynamics with heterogeneous thresholds over the agents. While the
previous literature has studied such linear threshold models under the
assumption that each agent may change actions at most once, a study of best
response dynamics in such networked games necessitates an analysis that allows
for multiple switches in actions. In this paper, we develop such an analysis
and construct a combinatorial framework to understand the behavior of the
model. To this end, we establish that the agents behavior cycles among
different actions in the limit and provide three sets of results.
  We first characterize the limiting behavioral properties of the dynamics. We
determine the length of the limit cycles and reveal bounds on the time steps
required to reach such cycles for different network structures. We then study
the complexity of decision/counting problems that arise within the context.
Specifically, we consider the tractability of counting the number of limit
cycles and fixed-points, and deciding the reachability of action profiles. We
finally propose a measure of network resilience that captures the nature of the
involved dynamics. We prove bounds and investigate the resilience of different
network structures under this measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0656</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0656</id><created>2012-11-03</created><authors><author><keyname>Levine</keyname><forenames>G. C.</forenames></author><author><keyname>Caravan</keyname><forenames>B.</forenames></author><author><keyname>Cerise</keyname><forenames>J. E.</forenames></author></authors><title>Electoral Susceptibility</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI stat.AP</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the United States electoral system, a candidate is elected indirectly by
winning a majority of electoral votes cast by individual states, the election
usually being decided by the votes cast by a small number of &quot;swing states&quot;
where the two candidates historically have roughly equal probabilities of
winning. The effective value of a swing state in deciding the election is
determined not only by the number of its electoral votes but by the frequency
of its appearance in the set of winning partitions of the electoral college.
Since the electoral vote values of swing states are not identical, the presence
or absence of a state in a winning partition is generally correlated with the
frequency of appearance of other states and, hence, their effective values. We
quantify the effective value of states by an {\sl electoral susceptibility},
$\chi_j$, the variation of the winning probability with the &quot;cost&quot; of changing
the probability of winning state $j$. We study $\chi_j$ for realistic data
accumulated for the 2012 U.S. presidential election and for a simple model with
a Zipf's law type distribution of electoral votes. In the latter model we show
that the susceptibility for small states is largest in &quot;one-sided&quot; electoral
contests and smallest in close contests. We draw an analogy to models of
entropically driven interactions in poly-disperse colloidal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0658</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0658</id><created>2012-11-04</created><authors><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author></authors><title>On the Non-existence of Lattice Tilings by Quasi-crosses</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study necessary conditions for the existence of lattice tilings of $\R^n$
by quasi-crosses. We prove non-existence results, and focus in particular on
the two smallest unclassified shapes, the $(3,1,n)$-quasi-cross and the
$(3,2,n)$-quasi-cross. We show that for dimensions $n\leq 250$, apart from the
known constructions, there are no lattice tilings of $\R^n$ by
$(3,1,n)$-quasi-crosses except for ten remaining cases, and no lattice tilings
of $\R^n$ by $(3,2,n)$-quasi-crosses except for eleven remaining cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0660</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0660</id><created>2012-11-04</created><authors><author><keyname>Kamada</keyname><forenames>Yukihiro</forenames></author><author><keyname>Miyasaki</keyname><forenames>Kiyonori</forenames></author></authors><title>Generation of Two-Layer Monotonic Functions</title><categories>cs.NE</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of implementing a class of functions with particular conditions
by using monotonic multilayer functions is considered. A genetic algorithm is
used to create monotonic functions of a certain class, and these are
implemented with two-layer monotonic functions. The existence of a solution to
the given problem suggests that from two monotone functions, a monotonic
function with the same dimensions can be created. A new algorithm based on the
genetic algorithm is proposed, which easily implemented two-layer monotonic
functions of a specific class for up to six variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0665</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0665</id><created>2012-11-04</created><authors><author><keyname>Koiran</keyname><forenames>Pascal</forenames><affiliation>LIP</affiliation></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>Hidden cliques and the certification of the restricted isometry property</title><categories>cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1103.4984</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a technique for finding sparse solutions to
underdetermined linear systems. This technique relies on properties of the
sensing matrix such as the restricted isometry property. Sensing matrices that
satisfy this property with optimal parameters are mainly obtained via
probabilistic arguments. Deciding whether a given matrix satisfies the
restricted isometry property is a non-trivial computational problem. Indeed, we
show in this paper that restricted isometry parameters cannot be approximated
in polynomial time within any constant factor under the assumption that the
hidden clique problem is hard. Moreover, on the positive side we propose an
improvement on the brute-force enumeration algorithm for checking the
restricted isometry property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0673</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0673</id><created>2012-11-04</created><updated>2013-09-17</updated><authors><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Hassan</keyname><forenames>Dina S. M.</forenames></author><author><keyname>Fahmy</keyname><forenames>Hossam M. A.</forenames></author></authors><title>RCA: Efficient Connected Dominated Clustering Algorithm for Mobile Ad
  Hoc Networks</title><categories>cs.NI</categories><comments>Submited by error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering of mobile ad hoc networks is a largely growing field. The
perceived benefits of clustering are comprehensively analyzed in open
literature. This paper considers the development of a new
connected-dominated-set clustering algorithm called Ring Clustering Algorithm
(RCA). RCA is a heuristic algorithm that groups mobile nodes in a network into
rings. Each ring consists of three ring-nodes. The priority of a ring is
determined according to a new parameter, the ring degree. This paper presents
the proof that the maximum number of rings that can be formed by RCA in any
disk area equals the maximum number of independent nodes that create
non-overlapping circles in a corresponding area. Moreover, RCA has achieved a
fixed approximation ratio, which is 5.146 and O(n) for both time and message
complexities. Thus, RCA algorithm outperforms the current-best CDS algorithms
that are investigated in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0689</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0689</id><created>2012-11-04</created><authors><author><keyname>Glauner</keyname><forenames>Patrick O.</forenames></author></authors><title>Enhancing Invenio Digital Library With An External Relevance Ranking
  Engine</title><categories>cs.IR cs.DL</categories><comments>70 pages, 34 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invenio is a comprehensive web-based free digital library software suite
originally developed at CERN. In order to improve its information retrieval and
word similarity ranking capabilities, the goal of this thesis is to enhance
Invenio by bridging it with modern external information retrieval systems. In
the first part a comparison of various information retrieval systems such as
Solr and Xapian is made. In the second part a system-independent bridge for
word similarity ranking is designed and implemented. Subsequently, Solr and
Xapian are integrated in Invenio via adapters to the bridge. In the third part
scalability tests are performed. Finally, a future outlook is briefly
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0704</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0704</id><created>2012-11-04</created><authors><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Dynamic Frequency Management in 802.11-based Multi-Radio Wireless
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient channel selection is essential in 802.11 mesh deployments, for
minimizing contention and interference among co-channel devices and thereby
supporting a plurality of QoS-sensitive applications. A few protocols have been
proposed for frequency allocation in such networks, however they do not address
the problem end-to-end. In this paper, we present a general formulation of the
channel selection problem taking into account the performance of both
mesh-access and mesh-backhaul. Moreover, we propose ARACHNE, a routing-aware
channel selection protocol for wireless mesh networks. ARACHNE is distributed
in nature, and motivated by our measurements on a wireless testbed. The main
novelty of our protocol comes from adopting a metric that captures the
end-to-end link loads across different routes in the network. ARACHNE
prioritizes the assignment of low-interference channels to links that (a) need
to serve high-load aggregate traffic and/or (b) already suffer significant
levels of contention and interference. Our protocol takes into account the
number of potential interfaces (radios) per device, and allocates these
interfaces in a manner that efficiently utilizes the available channel
capacity. We evaluate ARACHNE through extensive, tracedriven simulations and we
show that approaches the optimal channel selection. We observe that our
protocol improves the total network throughput, as compared to three other
representative channel allocation approaches in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0709</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0709</id><created>2012-11-04</created><authors><author><keyname>Callahan</keyname><forenames>Devon</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Nielsen</keyname><forenames>Jeffrey</forenames></author><author><keyname>Johnson</keyname><forenames>Anthony N.</forenames></author></authors><title>Shaping Operations to Attack Robust Terror Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Security organizations often attempt to disrupt terror or insurgent networks
by targeting &quot;high value targets&quot; (HVT's). However, there have been numerous
examples that illustrate how such networks are able to quickly re-generate
leadership after such an operation. Here, we introduce the notion of a
&quot;shaping&quot; operation in which the terrorist network is first targeted for the
purpose of reducing its leadership re-generation ability before targeting
HVT's. We look to conduct shaping by maximizing the network-wide degree
centrality through node removal. We formally define this problem and prove
solving it is NP-Complete. We introduce a mixed integer-linear program that
solves this problem exactly as well as a greedy heuristic for more practical
use. We implement the greedy heuristic and found in examining five real-world
terrorist networks that removing only 12% of nodes can increase the
network-wide centrality between 17% and 45%. We also show our algorithm can
scale to large social networks of 1,133 nodes and 5,541 edges on commodity
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0713</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0713</id><created>2012-11-04</created><authors><author><keyname>Herchi</keyname><forenames>Hatem</forenames></author><author><keyname>Abdessalem</keyname><forenames>Wahiba Ben</forenames></author></authors><title>From user requirements to UML class diagram</title><categories>cs.SE</categories><comments>International Conference on Computer Related Knowledge</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transition from user requirements to UML diagrams is a difficult task for
the designer especially when he handles large texts expressing these needs.
Modeling class Diagram must be performed frequently, even during the
development of a simple application. This paper proposes an approach to
facilitate class diagram extraction from textual requirements using NLP
techniques and domain ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0716</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0716</id><created>2012-11-04</created><authors><author><keyname>Achour</keyname><forenames>Hadhemi</forenames></author><author><keyname>Abdessalem</keyname><forenames>Wahiba Ben</forenames></author></authors><title>An Evaluation of Arabic Language Learning Websites</title><categories>cs.OH</categories><comments>International Conference on Education and E-Learning Innovations</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a result of ICT development and the increasingly growing use of the
Internet in particular, practices of language teaching and learning are about
to evolve significantly. Our study focuses on the Arabic language, and aims to
explore and evaluate Arabic language learning websites. To reach these goals,
we propose in a first step, to define an evaluation model, based on a set of
criteria for assessing the quality of websites dedicated to teaching and
learning Arabic. We subsequently apply our model on a set of Arabic sites
available on the web and give an assessment of these web sites. We finally
discuss their strengths and limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0719</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0719</id><created>2012-11-04</created><updated>2013-02-22</updated><authors><author><keyname>Latora</keyname><forenames>Vito</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Panzarasa</keyname><forenames>Pietro</forenames></author></authors><title>Social cohesion, structural holes, and a tale of two measures</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 3 figures</comments><journal-ref>J. Stat. Phys. 151 (3-4), 745 (2013)</journal-ref><doi>10.1007/s10955-013-0722-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the social sciences, the debate over the structural foundations of social
capital has long vacillated between two positions on the relative benefits
associated with two types of social structures: closed structures, rich in
third-party relationships, and open structures, rich in structural holes and
brokerage opportunities. In this paper, we engage with this debate by focusing
on the measures typically used for formalising the two conceptions of social
capital: clustering and effective size. We show that these two measures are
simply two sides of the same coin, as they can be expressed one in terms of the
other through a simple functional relation. Building on this relation, we then
attempt to reconcile closed and open structures by proposing a new measure,
Simmelian brokerage, that captures opportunities of brokerage between otherwise
disconnected cohesive groups of contacts. Implications of our findings for
research on social capital and complex networks are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0721</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0721</id><created>2012-11-04</created><updated>2014-07-09</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author></authors><title>Superlinear advantage for exact quantum algorithms</title><categories>quant-ph cs.CC</categories><comments>20 pages, v6: small number of small corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum algorithm is exact if, on any input data, it outputs the correct
answer with certainty (probability 1). A key question is: how big is the
advantage of exact quantum algorithms over their classical counterparts:
deterministic algorithms. For total Boolean functions in the query model, the
biggest known gap was just a factor of 2: PARITY of N inputs bits requires $N$
queries classically but can be computed with N/2 queries by an exact quantum
algorithm.
  We present the first example of a Boolean function f(x_1, ..., x_N) for which
exact quantum algorithms have superlinear advantage over the deterministic
algorithms. Any deterministic algorithm that computes our function must use N
queries but an exact quantum algorithm can compute it with O(N^{0.8675...})
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0722</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0722</id><created>2012-11-04</created><updated>2013-09-22</updated><authors><author><keyname>Bar-Ilan</keyname><forenames>Omer</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Sub-Nyquist Radar via Doppler Focusing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of a monostatic pulse-Doppler radar transceiver
trying to detect targets, sparsely populated in the radar's unambiguous
time-frequency region. Several past works employ compressed sensing (CS)
algorithms to this type of problem, but either do not address sample rate
reduction, impose constraints on the radar transmitter, propose CS recovery
methods with prohibitive dictionary size, or perform poorly in noisy
conditions. Here we describe a sub-Nyquist sampling and recovery approach
called Doppler focusing which addresses all of these problems: it performs low
rate sampling and digital processing, imposes no restrictions on the
transmitter, and uses a CS dictionary with size which does not increase with
increasing number of pulses P. Furthermore, in the presence of noise, Doppler
focusing enjoys an SNR increase which scales linearly with P, obtaining good
detection performance even at SNRs as low as -25dB. The recovery is based on
the Xampling framework, which allows reducing the number of samples needed to
accurately represent the signal, directly in the analog-to-digital conversion
process. After sampling, the entire digital recovery process is performed on
the low rate samples without having to return to the Nyquist rate. Finally, our
approach can be implemented in hardware using a previously suggested Xampling
prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0728</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0728</id><created>2012-11-04</created><authors><author><keyname>Turitsyn</keyname><forenames>K. S.</forenames></author><author><keyname>Kaplunovich</keyname><forenames>P. A.</forenames></author></authors><title>Fast Algorithm for N-2 Contingency Problem</title><categories>physics.soc-ph cs.SI math-ph math.MP</categories><comments>HICCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel selection algorithm for N-2 contingency analysis problem.
The algorithm is based on the iterative bounding of line outage distribution
factors and successive pruning of the set of contingency pair candidates. The
selection procedure is non-heuristic, and is certified to identify all events
that lead to thermal constraints violations in DC approximation. The complexity
of the algorithm is O(N^2) comparable to the complexity of N-1 contingency
problem. We validate and test the algorithm on the Polish grid network with
around 3000 lines. For this test case two iterations of the pruning procedure
reduce the total number of candidate pairs by a factor of almost 1000 from 5
millions line pairs to only 6128.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0729</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0729</id><created>2012-11-04</created><updated>2014-09-13</updated><authors><author><keyname>Wang</keyname><forenames>Zhi Jie</forenames></author><author><keyname>Lin</keyname><forenames>Xiao</forenames></author><author><keyname>Fang</keyname><forenames>Mei-e</forenames></author><author><keyname>Yao</keyname><forenames>Bin</forenames></author><author><keyname>Guan</keyname><forenames>Haibing</forenames></author><author><keyname>Guo</keyname><forenames>Minyi</forenames></author></authors><title>RE2L: An Efficient Output-sensitive Algorithm for Computing Boolean
  Operation on Circular-arc Polygons</title><categories>cs.DS cs.CG cs.GR</categories><comments>12 pages</comments><acm-class>E.1; I.3.5; I.3.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The boundaries of \textit{conic polygons} consist of conic segments or second
degree curves. The conic polygon has two degenerate or special cases: the
linear polygon and the circular-arc polygon. The natural problem --- boolean
operation on linear polygons, has been \textit{well} studied. Surprisingly,
(almost) no article \textit{focuses on} the problem of boolean operation on
circular-arc polygons, which actually can also find many applications, implying
that if there is a targeted solution for boolean operation on circular-arc
polygons, which should be favourable for potential users. In this article, we
devise a concise data structure, and then develop a targeted algorithm called
R{\scriptsize E2L}. Our method is surprisingly simple, easy-to-implement but
without loss of efficiency. Given two circular-arc polygons with $m$ and $n$
edges respectively, we prove that the proposed method runs in $O(m+n+(l+k)\log
l)$ time, using $O(m+n+l+k)$ space, where $k$ is the number of intersections,
and $l$ is the number of related edges. The experimental results show our
proposed algorithm is significantly faster than the ones that are by directly
appealing to the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0730</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0730</id><created>2012-11-04</created><authors><author><keyname>Tawfeeq</keyname><forenames>Mohammed Ali</forenames></author></authors><title>Intelligent Algorithm for Optimum Solutions Based on the Principles of
  Bat Sonar</title><categories>cs.NE</categories><comments>9 pages, 16 figures, 7 tables; (IJCSIS) International Journal of
  Computer Science and Information Security,Vol. 10, No. 10, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new intelligent algorithm that can solve the problems
of finding the optimum solution in the state space among which the desired
solution resides. The algorithm mimics the principles of bat sonar in finding
its targets. The algorithm introduces three search approaches. The first search
approach considers a single sonar unit (SSU) with a fixed beam length and a
single starting point. In this approach, although the results converge toward
the optimum fitness, it is not guaranteed to find the global optimum solution
especially for complex problems; it is satisfied with finding 'acceptably good'
solutions to these problems. The second approach considers multisonar units
(MSU) working in parallel in the same state space. Each unit has its own
starting point and tries to find the optimum solution. In this approach the
probability that the algorithm converges toward the optimum solution is
significantly increased. It is found that this approach is suitable for complex
functions and for problems of wide state space. In the third approach, a single
sonar unit with a moment (SSM) is used in order to handle the problem of
convergence toward a local optimum rather than a global optimum. The momentum
term is added to the length of the transmitted beams. This will give the chance
to find the best fitness in a wider range within the state space. In this paper
a comparison between the proposed algorithm and genetic algorithm (GA) has been
made. It showed that both of the algorithms can catch approximately the optimum
solutions for all of the testbed functions except for the function that has a
local minimum, in which the proposed algorithm's result is much better than
that of the GA algorithm. On the other hand, the comparison showed that the
required execution time to obtain the optimum solution using the proposed
algorithm is much less than that of the GA algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0736</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0736</id><created>2012-11-04</created><authors><author><keyname>Norwell</keyname><forenames>Arron</forenames></author></authors><title>A Threshold For Clusters in Real-World Random Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages with 12 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent empirical work [Leskovec2009] has suggested the existence of a size
threshold for the existence of clusters within many real-world networks. We
give the first proof that this clustering size threshold exists within a
real-world random network model, and determine the asymptotic value at which it
occurs.
  More precisely, we choose the Community Guided Attachment (CGA) random
network model of Leskovek, Kleinberg, and Faloutsos [Leskovec2005]. The model
is non-uniform and contains self-similar communities, and has been shown to
have many properties of real-world networks. To capture the notion of
clustering, we follow Mishra et. al. [Mishra2007], who defined a type of
clustering for real-world networks: an (\alpha,\beta)-cluster is a set that is
both internally dense (to the extent given by the parameter \beta), and
externally sparse (to the extent given by the parameter \alpha) . With this
definition of clustering, we show the existence of a size threshold of (\ln
n)^{1/2} for the existence of clusters in the CGA model. For all \epsilon&gt;0,
a.a.s. clusters larger than (\ln n)^{1/2-\epsilon} exist, whereas a.a.s.
clusters larger than (\ln n)^{1/2+\epsilon} do not exist. Moreover, we show a
size bound on the existence of small, constant-size clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0737</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0737</id><created>2012-11-04</created><updated>2013-06-24</updated><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author></authors><title>Optimal Information-Theoretic Wireless Location Verification</title><categories>cs.IT cs.CR math.IT</categories><comments>Corrected typos and introduced new threat models</comments><doi>10.1109/TVT.2014.2302022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new Location Verification System (LVS) focussed on network-based
Intelligent Transport Systems and vehicular ad hoc networks. The algorithm we
develop is based on an information-theoretic framework which uses the received
signal strength (RSS) from a network of base-stations and the claimed position.
Based on this information we derive the optimal decision regarding the
verification of the user's location. Our algorithm is optimal in the sense of
maximizing the mutual information between its input and output data. Our
approach is based on the practical scenario in which a non-colluding malicious
user some distance from a highway optimally boosts his transmit power in an
attempt to fool the LVS that he is on the highway. We develop a practical
threat model for this attack scenario, and investigate in detail the
performance of the LVS in terms of its input/output mutual information. We show
how our LVS decision rule can be implemented straightforwardly with a
performance that delivers near-optimality under realistic threat conditions,
with information-theoretic optimality approached as the malicious user moves
further from the highway. The practical advantages our new
information-theoretic scheme delivers relative to more traditional Bayesian
verification frameworks are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0749</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0749</id><created>2012-11-04</created><authors><author><keyname>Hidayah</keyname><forenames>Indriana</forenames></author><author><keyname>Syahrina</keyname><forenames>Alvi</forenames></author><author><keyname>Permanasari</keyname><forenames>Adhistya Erna</forenames></author></authors><title>Student Modeling using Case-Based Reasoning in Conventional Learning
  System</title><categories>cs.AI cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional face-to-face classrooms are still the main learning system
applied in Indonesia. In assisting such conventional learning towards an
optimal learning, formative evaluations are needed to monitor the progress of
the class. This task can be very hard when the size of the class is large.
Hence, this research attempted to create a classroom monitoring system based on
student data of Department of Electrical Engineering and Information
Technology. In order to achieve the goal, a student modeling using Case-Based
Reasoning was proposed. A generic student model based on a framework was
developed. The model represented student knowledge of a subject. The result
showed that the system was able to store and retrieve student data for
suggestion of the current situation and formative evaluation for one of the
subject in the Department.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0750</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0750</id><created>2012-11-04</created><updated>2012-11-13</updated><authors><author><keyname>Josellis</keyname><forenames>Frank</forenames></author><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The Lusternik-Schnirelmann theorem for graphs</title><categories>math.AT cs.DM math.GN</categories><comments>29 pages, 7 figures. Main results unchanged but cat(G) had not yet
  been homotopy invariant. 3 more references, smaller typos and a figure
  correction</comments><msc-class>55M30, 58E05, 05C75, 05C10, 57M15, 57Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the discrete Lusternik-Schnirelmann theorem telling that tcat(G)
less or equal to crit(G) for a general simple graph G=(V,E). It relates the
minimal number tcat(G) of in G contractible graphs covering G, with crit(G),
the minimal number of critical points which an injective function f on the
vertex set V can have. We also prove that the cup length cup(G) is less or
equal to tcat(G) which is valid also for any finite simple graph. If cat(G) is
the minimal tcat(H) among all graphs H homotopic to G and cri(G) is the minimal
crit(H) among all graphs H homotopic to G, we get a relation between three
homotopy invariants: an algebraic quantity (cup), a topological quantity (cat)
and an analytic quantity (cri).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0752</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0752</id><created>2012-11-04</created><updated>2012-11-17</updated><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author></authors><title>Faster Approximation of Max Flow for Directed Graphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I extend the methods in &quot;Electrical Flows, Laplacian Systems, and Faster
Approximation of Maximum Flow in Undirected Graphs, with Paul Christiano,
Jonathan Kelner, Daniel Spielman, and Shang-Hua Teng&quot; to directed graphs with a
variation of the framework in the paper, which lead to an algorithm that
approximately solves the directed max flow problem in nearly-linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0757</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0757</id><created>2012-11-04</created><authors><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Zhang</keyname><forenames>Yuqian</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Efficient Point-to-Subspace Query in $\ell^1$: Theory and Applications
  in Computer Vision</title><categories>stat.ML cs.CV stat.AP</categories><comments>To appear in NIPS workshop on big learning, 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Motivated by vision tasks such as robust face and object recognition, we
consider the following general problem: given a collection of low-dimensional
linear subspaces in a high-dimensional ambient (image) space and a query point
(image), efficiently determine the nearest subspace to the query in $\ell^1$
distance. We show in theory that Cauchy random embedding of the objects into
significantly-lower-dimensional spaces helps preserve the identity of the
nearest subspace with constant probability. This offers the possibility of
efficiently selecting several candidates for accurate search. We sketch
preliminary experiments on robust face and digit recognition to corroborate our
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0779</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0779</id><created>2012-11-05</created><updated>2013-05-25</updated><authors><author><keyname>Chen</keyname><forenames>Junting</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Large Deviation Delay Analysis of Queue-Aware Multi-user MIMO Systems
  with Multi-timescale Mobile-Driven Feedback</title><categories>cs.SY cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 16, pp.
  4067-4076, 2013</journal-ref><doi>10.1109/TSP.2013.2265681</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user multi-input-multi-output (MU-MIMO) systems transmit data to
multiple users simultaneously using the spatial degrees of freedom with user
feedback channel state information (CSI). Most of the existing literatures on
the reduced feedback user scheduling focus on the throughput performance and
the user queueing delay is usually ignored. As the delay is very important for
real-time applications, a low feedback queue-aware user scheduling algorithm is
desired for the MU-MIMO system. This paper proposed a two-stage queue-aware
user scheduling algorithm, which consists of a queue-aware mobile-driven
feedback filtering stage and a SINR-based user scheduling stage, where the
feedback filtering policy is obtained from the solution of an optimization
problem. We evaluate the queueing performance of the proposed scheduling
algorithm by using the sample path large deviation analysis. We show that the
large deviation decay rate for the proposed algorithm is much larger than that
of the CSI-only user scheduling algorithm. The numerical results also
demonstrate that the proposed algorithm performs much better than the CSI-only
algorithm requiring only a small amount of feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0801</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0801</id><created>2012-11-05</created><authors><author><keyname>Yuan</keyname><forenames>Ming</forenames></author></authors><title>Discussion: Latent variable graphical model selection via convex
  optimization</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS979 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS979</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 1968-1972</journal-ref><doi>10.1214/12-AOS979</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of &quot;Latent variable graphical model selection via convex
optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0806</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0806</id><created>2012-11-05</created><authors><author><keyname>Lauritzen</keyname><forenames>Steffen</forenames></author><author><keyname>Meinshausen</keyname><forenames>Nicolai</forenames></author></authors><title>Discussion: Latent variable graphical model selection via convex
  optimization</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS980 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS980</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 1973-1977</journal-ref><doi>10.1214/12-AOS980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of &quot;Latent variable graphical model selection via convex
optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0808</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0808</id><created>2012-11-05</created><authors><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Discussion: Latent variable graphical model selection via convex
  optimization</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS981 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS981</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 1978-1983</journal-ref><doi>10.1214/12-AOS981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of &quot;Latent variable graphical model selection via convex
optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0817</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0817</id><created>2012-11-05</created><authors><author><keyname>Cand&#xe9;s</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Discussion: Latent variable graphical model selection via convex
  optimization</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1001 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1001</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 1997-2004</journal-ref><doi>10.1214/12-AOS1001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of &quot;Latent variable graphical model selection via convex
optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0820</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0820</id><created>2012-11-05</created><authors><author><keyname>Jeong</keyname><forenames>Hwancheol</forenames></author><author><keyname>Kim</keyname><forenames>Sunghoon</forenames></author><author><keyname>Lee</keyname><forenames>Weonjong</forenames></author><author><keyname>Myung</keyname><forenames>Seok-Ho</forenames></author></authors><title>Performance of SSE and AVX Instruction Sets</title><categories>hep-lat cs.PF</categories><comments>7 pages, 5 figures, 4 tables, Contribution to proceedings of the 30th
  International Symposium on Lattice Field Theory (Lattice 2012), June 24-29,
  2012</comments><journal-ref>PoS (LATTICE 2012) 249</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SSE (streaming SIMD extensions) and AVX (advanced vector extensions) are SIMD
(single instruction multiple data streams) instruction sets supported by recent
CPUs manufactured in Intel and AMD. This SIMD programming allows parallel
processing by multiple cores in a single CPU. Basic arithmetic and data
transfer operations such as sum, multiplication and square root can be
processed simultaneously. Although popular compilers such as GNU compilers and
Intel compilers provide automatic SIMD optimization options, one can obtain
better performance by a manual SIMD programming with proper optimization: data
packing, data reuse and asynchronous data transfer. In particular, linear
algebraic operations of vectors and matrices can be easily optimized by the
SIMD programming. Typical calculations in lattice gauge theory are composed of
linear algebraic operations of gauge link matrices and fermion vectors, and so
can adopt the manual SIMD programming to improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0834</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0834</id><created>2012-11-05</created><updated>2012-11-21</updated><authors><author><keyname>D&#x119;bowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>On Hidden Markov Processes with Infinite Excess Entropy</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><msc-class>60J10, 94A17, 37A25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate stationary hidden Markov processes for which mutual
information between the past and the future is infinite. It is assumed that the
number of observable states is finite and the number of hidden states is
countably infinite. Under this assumption, we show that the block mutual
information of a hidden Markov process is upper bounded by a power law
determined by the tail index of the hidden state distribution. Moreover, we
exhibit three examples of processes. The first example, considered previously,
is nonergodic and the mutual information between the blocks is bounded by the
logarithm of the block length. The second example is also nonergodic but the
mutual information between the blocks obeys a power law. The third example
obeys the power law and is ergodic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0835</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0835</id><created>2012-11-05</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Rejoinder: Latent variable graphical model selection via convex
  optimization</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1020 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1020</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 4, 2005-2013</journal-ref><doi>10.1214/12-AOS1020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rejoinder to &quot;Latent variable graphical model selection via convex
optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0865</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0865</id><created>2012-11-05</created><updated>2013-02-26</updated><authors><author><keyname>Stump</keyname><forenames>Aaron</forenames><affiliation>University of Iowa</affiliation></author><author><keyname>Kimmell</keyname><forenames>Garrin</forenames><affiliation>University of Iowa</affiliation></author><author><keyname>Zantema</keyname><forenames>Hans</forenames><affiliation>TU Eindhoven</affiliation></author><author><keyname>Omar</keyname><forenames>Ruba El Haj</forenames><affiliation>University of Iowa</affiliation></author></authors><title>A Rewriting View of Simple Typing</title><categories>cs.PL</categories><proxy>LMCS</proxy><acm-class>D.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  27, 2013) lmcs:936</journal-ref><doi>10.2168/LMCS-9(1:4)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how a recently developed view of typing as small-step
abstract reduction, due to Kuan, MacQueen, and Findler, can be used to recast
the development of simple type theory from a rewriting perspective. We show how
standard meta-theoretic results can be proved in a completely new way, using
the rewriting view of simple typing. These meta-theoretic results include
standard type preservation and progress properties for simply typed lambda
calculus, as well as generalized versions where typing is taken to include both
abstract and concrete reduction. We show how automated analysis tools developed
in the term-rewriting community can be used to help automate the proofs for
this meta-theory. Finally, we show how to adapt a standard proof of
normalization of simply typed lambda calculus, for the rewriting approach to
typing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0872</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0872</id><created>2012-11-05</created><authors><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Mendelson</keyname><forenames>Shahar</forenames></author></authors><title>Phase Retrieval: Stability and Recovery Guarantees</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stability and uniqueness in real phase retrieval problems over
general input sets. Specifically, we assume the data consists of noisy
quadratic measurements of an unknown input x in R^n that lies in a general set
T and study conditions under which x can be stably recovered from the
measurements. In the noise-free setting we derive a general expression on the
number of measurements needed to ensure that a unique solution can be found in
a stable way, that depends on the set T through a natural complexity parameter.
This parameter can be computed explicitly for many sets T of interest. For
example, for k-sparse inputs we show that O(k\log(n/k)) measurements are
needed, and when x can be any vector in R^n, O(n) measurements suffice. In the
noisy case, we show that if one can find a value for which the empirical risk
is bounded by a given, computable constant (that depends on the set T), then
the error with respect to the true input is bounded above by an another,
closely related complexity parameter of the set. By choosing an appropriate
number N of measurements, this bound can be made arbitrarily small, and it
decays at a rate faster than N^{-1/2+\delta} for any \delta&gt;0. In particular,
for k-sparse vectors stable recovery is possible from O(k\log(n/k)\log k) noisy
measurements, and when x can be any vector in R^n, O(n \log n) noisy
measurements suffice. We also show that the complexity parameter for the
quadratic problem is the same as the one used for analyzing stability in linear
measurements under very general conditions. Thus, no substantial price has to
be paid in terms of stability if there is no knowledge of the phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0877</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0877</id><created>2012-11-05</created><updated>2013-03-22</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Differential Privacy for the Analyst via Private Equilibrium Computation</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new mechanisms for answering exponentially many queries from multiple
analysts on a private database, while protecting differential privacy both for
the individuals in the database and for the analysts. That is, our mechanism's
answer to each query is nearly insensitive to changes in the queries asked by
other analysts. Our mechanism is the first to offer differential privacy on the
joint distribution over analysts' answers, providing privacy for data analysts
even if the other data analysts collude or register multiple accounts. In some
settings, we are able to achieve nearly optimal error rates (even compared to
mechanisms which do not offer analyst privacy), and we are able to extend our
techniques to handle non-linear queries. Our analysis is based on a novel view
of the private query-release problem as a two-player zero-sum game, which may
be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0879</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0879</id><created>2012-11-05</created><authors><author><keyname>Shi</keyname><forenames>Yanshan</forenames></author></authors><title>Comparing K-Nearest Neighbors and Potential Energy Method in
  classification problem. A case study using KNN applet by E.M. Mirkes and real
  life benchmark data sets</title><categories>stat.ML cs.LG</categories><comments>23 pages, 27 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  K-nearest neighbors (KNN) method is used in many supervised learning
classification problems. Potential Energy (PE) method is also developed for
classification problems based on its physical metaphor. The energy potential
used in the experiments are Yukawa potential and Gaussian Potential. In this
paper, I use both applet and MATLAB program with real life benchmark data to
analyze the performances of KNN and PE method in classification problems. The
results show that in general, KNN and PE methods have similar performance. In
particular, PE with Yukawa potential has worse performance than KNN when the
density of the data is higher in the distribution of the database. When the
Gaussian potential is applied, the results from PE and KNN have similar
behavior. The indicators used are correlation coefficients and information
gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0886</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0886</id><created>2012-11-05</created><authors><author><keyname>Lance</keyname><forenames>Brent J.</forenames></author><author><keyname>Kerick</keyname><forenames>Scott E.</forenames></author><author><keyname>Ries</keyname><forenames>Anthony J.</forenames></author><author><keyname>Oie</keyname><forenames>Kelvin S.</forenames></author><author><keyname>McDowell</keyname><forenames>Kaleb</forenames></author></authors><title>Brain Computer Interface Technologies in the Coming Decades</title><categories>cs.HC cs.ET</categories><comments>41 pages, 3 figures</comments><acm-class>H.1.2; J.3</acm-class><journal-ref>Proceedings of the IEEE, vol.100, no.Special Centennial Issue,
  pp.1585-1599, May 13 2012</journal-ref><doi>10.1109/JPROC.2012.2184830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0889</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0889</id><created>2012-11-02</created><updated>2013-05-04</updated><authors><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Feng</keyname><forenames>Yang</forenames></author></authors><title>APPLE: Approximate Path for Penalized Likelihood Estimators</title><categories>stat.ML cs.LG</categories><comments>24 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high-dimensional data analysis, penalized likelihood estimators are shown
to provide superior results in both variable selection and parameter
estimation. A new algorithm, APPLE, is proposed for calculating the Approximate
Path for Penalized Likelihood Estimators. Both the convex penalty (such as
LASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.
The APPLE efficiently computes the solution path for the penalized likelihood
estimator using a hybrid of the modified predictor-corrector method and the
coordinate-descent algorithm. APPLE is compared with several well-known
packages via simulation and analysis of two gene expression data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0895</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0895</id><created>2012-11-05</created><authors><author><keyname>Bras-Amor&#xf3;s</keyname><forenames>Maria</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>Pedro A.</forenames></author><author><keyname>Vico-Oton</keyname><forenames>Albert</forenames></author></authors><title>Nonhomogeneous patterns on numerical semigroups</title><categories>math.NT cs.DM math.AC</categories><msc-class>20M14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterns on numerical semigroups are multivariate linear polynomials, and
they are said to be admissible if there exists a numerical semigroup such that
evaluated at any nonincreasing sequence of elements of the semigroup gives
integers belonging to the semigroup. In a first approach, only homogeneous
patterns where analized. In this contribution we study conditions for an
eventually non-homogeneous pattern to be admissible, and particularize this
study to the case the independent term of the pattern is a multiple of the
multiplicity of the semigroup. Moreover, for the so called strongly admissible
patterns, the set of numerical semigroups admitting these patterns with fixed
multiplicity $m$ form an $m$-variety, which allows us to represent this set in
a tree and to describe minimal sets of generators of the semigroups in the
variety with respect to the pattern. Furthermore, we characterize strongly
admissible patterns having a finite associated tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0897</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0897</id><created>2012-11-05</created><authors><author><keyname>Cady</keyname><forenames>Field</forenames></author></authors><title>An Elementary Derivation of Mean Wait Time in Polling Systems</title><categories>cs.SY math.PR</categories><comments>6 pages, generalizes previous work, elementary treatment of classic
  problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polling systems are a well-established subject in queueing theory. However,
their formal treatments generally rely heavily on relatively sophisticated
theoretical tools, such as moment generating functions and Laplace transforms,
and solutions often require the solution of large systems of equations. We show
that, if you are willing to only have the average waiting of a system time
rather than higher moments, it can found through an elementary derivation based
only on algebra and some well-known properties of Poisson processes. Our result
is simple enough to be easily used in real-world applications, and the
simplicity of our derivation makes it ideal for pedagogical purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0906</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0906</id><created>2012-11-05</created><updated>2013-10-26</updated><authors><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Xu</keyname><forenames>Lin</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>Algorithm Runtime Prediction: Methods &amp; Evaluation</title><categories>cs.AI cs.LG cs.PF stat.ML</categories><comments>51 pages, 13 figures, 8 tables. Added references, feature cost, and
  experiments with subsets of features; reworded Sections 1&amp;2</comments><msc-class>68T20</msc-class><acm-class>I.2.8; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perhaps surprisingly, it is possible to predict how long an algorithm will
take to run on a previously unseen input, using machine learning techniques to
build a model of the algorithm's runtime as a function of problem-specific
instance features. Such models have important applications to algorithm
analysis, portfolio-based algorithm selection, and the automatic configuration
of parameterized algorithms. Over the past decade, a wide variety of techniques
have been studied for building such models. Here, we describe extensions and
improvements of existing models, new families of models, and -- perhaps most
importantly -- a much more thorough treatment of algorithm parameters as model
inputs. We also comprehensively describe new and existing features for
predicting algorithm runtime for propositional satisfiability (SAT), travelling
salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate
these innovations through the largest empirical analysis of its kind, comparing
to a wide range of runtime modelling techniques from the literature. Our
experiments consider 11 algorithms and 35 instance distributions; they also
span a very wide range of SAT, MIP, and TSP instances, with the least
structured having been generated uniformly at random and the most structured
having emerged from real industrial applications. Overall, we demonstrate that
our new models yield substantially better runtime predictions than previous
approaches in terms of their generalization to new problem instances, to new
algorithms from a parameterized space, and to both simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0938</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0938</id><created>2012-11-05</created><updated>2012-11-11</updated><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author><author><keyname>Cheong</keyname><forenames>Michelle</forenames></author><author><keyname>Laik</keyname><forenames>Ma Nang</forenames></author><author><keyname>Shung</keyname><forenames>Koo Ping</forenames></author></authors><title>US Presidential Election 2012 Prediction using Census Corrected Twitter
  Model</title><categories>stat.AP cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  US Presidential Election 2012 has been a very tight race between the two key
candidates. There were intense battle between the two key candidates. The
election reflects the sentiment of the electorate towards the achievements of
the incumbent President Obama. The campaign lasted several months and the
effects can be felt in the internet and twitter. The presidential debates
injected new vigor in the challenger's campaign and successfully captured the
electorate of several states posing a threat to the incumbent's position. Much
of the sentiment in the election has been captured in the online discussions.
In this paper, we will be using the original model described in Choy et. al.
(2011) using twitter data to forecast the next US president.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0951</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0951</id><created>2012-11-05</created><authors><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Decoding Delay Minimization in Inter-Session Network Coding</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intra-session network coding has been shown to offer significant gains in
terms of achievable throughput and delay in settings where one source
multicasts data to several clients. In this paper, we consider a more general
scenario where multiple sources transmit data to sets of clients and study the
benefits of inter-session network coding, when network nodes have the
opportunity to combine packets from different sources. In particular, we
propose a novel framework for optimal rate allocation in inter-session network
coding systems. We formulate the problem as the minimization of the average
decoding delay in the client population and solve it with a gradient-based
stochastic algorithm. Our optimized inter-session network coding solution is
evaluated in different network topologies and compared with basic intra-session
network coding solutions. Our results show the benefits of proper coding
decisions and effective rate allocation for lowering the decoding delay when
the network is used by concurrent multicast sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0952</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0952</id><created>2012-11-05</created><updated>2014-01-11</updated><authors><author><keyname>Clarkson</keyname><forenames>Kenneth L.</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Self-improving Algorithms for Coordinate-Wise Maxima and Convex Hulls</title><categories>cs.CG cs.DS</categories><comments>39 pages, 17 figures; thoroughly revised presentation; preliminary
  versions appeared at SODA 2010 and SoCG 2012</comments><journal-ref>SIAM Journal on Computing (SICOMP), 43(2), 2014, pp. 617-653</journal-ref><doi>10.1137/12089702X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the coordinate-wise maxima and the convex hull of a planar point set
are probably the most classic problems in computational geometry. We consider
these problems in the self-improving setting. Here, we have $n$ distributions
$\mathcal{D}_1, \ldots, \mathcal{D}_n$ of planar points. An input point set
$(p_1, \ldots, p_n)$ is generated by taking an independent sample $p_i$ from
each $\mathcal{D}_i$, so the input is distributed according to the product
$\mathcal{D} = \prod_i \mathcal{D}_i$. A self-improving algorithm repeatedly
gets inputs from the distribution $\mathcal{D}$ (which is a priori unknown),
and it tries to optimize its running time for $\mathcal{D}$. The algorithm uses
the first few inputs to learn salient features of the distribution
$\mathcal{D}$, before it becomes fine-tuned to $\mathcal{D}$. Let
$\text{OPTMAX}_\mathcal{D}$ (resp. $\text{OPTCH}_\mathcal{D}$) be the expected
depth of an \emph{optimal} linear comparison tree computing the maxima (resp.
convex hull) for $\mathcal{D}$. Our maxima algorithm eventually achieves
expected running time $O(\text{OPTMAX}_\mathcal{D} + n)$. Furthermore, we give
a self-improving algorithm for convex hulls with expected running time
$O(\text{OPTCH}_\mathcal{D} + n\log\log n)$.
  Our results require new tools for understanding linear comparison trees. In
particular, we convert a general linear comparison tree to a restricted version
that can then be related to the running time of our algorithms. Another
interesting feature is an interleaved search procedure to determine the
likeliest point to be extremal with minimal computation. This allows our
algorithms to be competitive with the optimal algorithm for $\mathcal{D}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0954</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0954</id><created>2012-11-05</created><authors><author><keyname>Lopez-Ramos</keyname><forenames>Luis M.</forenames><affiliation>contact author</affiliation></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames><affiliation>contact author</affiliation></author><author><keyname>Ramos</keyname><forenames>Javier</forenames></author></authors><title>Jointly Optimal Sensing and Resource Allocation for Multiuser Overlay
  Cognitive Radios</title><categories>cs.NI cs.IT cs.SY math.IT</categories><comments>This work is supported by the Spanish Ministry of Science, under FPU
  Grant AP2010-1050. This paper has been submitted for publication to the IEEE
  Journal on Selected Areas in Communications. Parts of this paper were
  presented at CROWNCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful deployment of cognitive radios requires efficient sensing of the
spectrum and dynamic adaptation of the available resources according to the
sensed (imperfect) information. While most works design these two tasks
separately, in this paper we address them jointly. In particular, we
investigate an overlay cognitive radio with multiple secondary users that
access orthogonally a set of frequency bands originally devoted to primary
users. The schemes are designed to minimize the cost of sensing, maximize the
performance of the secondary users (weighted sum rate), and limit the
probability of interfering the primary users. The joint design is addressed
using dynamic programming and nonlinear optimization techniques. A two-step
strategy that first finds the optimal resource allocation for any sensing
scheme and then uses that solution as input to solve for the optimal sensing
policy is implemented. The two-step strategy is optimal, gives rise to
intuitive optimal policies, and entails a computational complexity much lower
than that required to solve the original formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0957</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0957</id><created>2012-10-22</created><authors><author><keyname>Sharma</keyname><forenames>Tarun Kumar</forenames></author><author><keyname>Pant</keyname><forenames>Millie</forenames></author><author><keyname>Singh</keyname><forenames>V. P.</forenames></author></authors><title>Adaptive Bee Colony in an Artificial Bee Colony for Solving Engineering
  Design Problems</title><categories>cs.CE q-bio.QM</categories><comments>Advances in Mechanical Engineering and its Applications (AMEA), 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A wide range of engineering design problems have been solved by the
algorithms that simulates collective intelligence in swarms of birds or
insects. The Artificial Bee Colony or ABC is one of the recent additions to the
class of swarm intelligence based algorithms that mimics the foraging behavior
of honey bees. ABC consists of three groups of bees namely employed, onlooker
and scout bees. In ABC, the food locations represent the potential candidate
solution. In the present study an attempt is made to generate the population of
food sources (Colony Size) adaptively and the variant is named as A-ABC. A-ABC
is further enhanced to improve convergence speed and exploitation capability,
by employing the concept of elitism, which guides the bees towards the best
food source. This enhanced variant is called E-ABC. The proposed algorithms are
validated on a set of standard benchmark problems with varying dimensions taken
from literature and on five engineering design problems. The numerical results
are compared with the basic ABC and three recent variant of ABC. Numerically
and statistically simulated results illustrate that the proposed method is very
efficient and competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0963</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0963</id><created>2012-11-02</created><authors><author><keyname>Allahbakhsh</keyname><forenames>Mohammad</forenames></author><author><keyname>Ignjatovic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Beheshti</keyname><forenames>Seyed-Mehdi-Reza</forenames></author><author><keyname>Foo</keyname><forenames>Norman</forenames></author><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author></authors><title>Detecting, Representing and Querying Collusion in Online Rating Systems</title><categories>cs.CR cs.HC cs.IR</categories><comments>22 pages, 6 figures</comments><report-no>UNSW-CSE-TR-201220</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online rating systems are subject to malicious behaviors mainly by posting
unfair rating scores. Users may try to individually or collaboratively promote
or demote a product. Collaborating unfair rating 'collusion' is more damaging
than individual unfair rating. Although collusion detection in general has been
widely studied, identifying collusion groups in online rating systems is less
studied and needs more investigation. In this paper, we study impact of
collusion in online rating systems and asses their susceptibility to collusion
attacks. The proposed model uses a frequent itemset mining algorithm to detect
candidate collusion groups. Then, several indicators are used for identifying
collusion groups and for estimating how damaging such colluding groups might
be. Also, we propose an algorithm for finding possible collusive subgroup
inside larger groups which are not identified as collusive. The model has been
implemented and we present results of experimental evaluation of our
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0967</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0967</id><created>2012-11-05</created><updated>2014-02-21</updated><authors><author><keyname>Terraf</keyname><forenames>Pedro S&#xe1;nchez</forenames></author></authors><title>Bisimilarity is not Borel</title><categories>math.LO cs.LO</categories><comments>20 pages, 1 figure; proof of Sigma_1^1 completeness added with
  extended comments. I acknowledge careful reading by the referees. Major
  changes in Introduction, Conclusion, and motivation for NLMP. Proof for Lemma
  22 added, simpler proofs for Lemma 17 and Theorem 30. Added references. Part
  of this work was presented at Dagstuhl Seminar 12411 on Coalgebraic Logics</comments><msc-class>03B70, 03E15, 28A05</msc-class><acm-class>F.4.1; F.1.2</acm-class><doi>10.1017/S0960129515000535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the relation of bisimilarity between countable labelled
transition systems is $\Sigma_1^1$-complete (hence not Borel), by reducing the
set of non-wellorders over the natural numbers continuously to it.
  This has an impact on the theory of probabilistic and nondeterministic
processes over uncountable spaces, since logical characterizations of
bisimilarity (as, for instance, those based on the unique structure theorem for
analytic spaces) require a countable logic whose formulas have measurable
semantics. Our reduction shows that such a logic does not exist in the case of
image-infinite processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0969</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0969</id><created>2012-11-05</created><updated>2013-08-03</updated><authors><author><keyname>Akin</keyname><forenames>Ethan</forenames></author></authors><title>Stable Cooperative Solutions for the Iterated Prisoner's Dilemma</title><categories>math.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the iterated Prisoner's Dilemma, there exist Markov strategies which
solve the problem when we restrict attention to the long term average payoff.
When used by both players these assure the cooperative payoff for each of them.
Neither player can benefit by moving unilaterally any other strategy, i.e.
these are Nash equilibria. In addition, if a player uses instead an alternative
which decreases the opponent's payoff below the cooperative level, then his own
payoff is decreased as well. Thus, if we limit attention to the long term
payoff, these \emph{good strategies} effectively stabilize cooperative
behavior. We characterize these good strategies and analyze their role in
evolutionary dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0970</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0970</id><created>2012-11-05</created><updated>2013-06-26</updated><authors><author><keyname>Mesty&#xe1;n</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Early Prediction of Movie Box Office Success based on Wikipedia Activity
  Big Data</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>13 pages, Including Supporting Information, 7 Figures, Download the
  dataset from: http://wwm.phy.bme.hu/SupplementaryDataS1.zip</comments><journal-ref>PLoS ONE 8(8): e71226 (2013)</journal-ref><doi>10.1371/journal.pone.0071226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use of socially generated &quot;big data&quot; to access information about collective
states of the minds in human societies has become a new paradigm in the
emerging field of computational social science. A natural application of this
would be the prediction of the society's reaction to a new product in the sense
of popularity and adoption rate. However, bridging the gap between &quot;real time
monitoring&quot; and &quot;early predicting&quot; remains a big challenge. Here we report on
an endeavor to build a minimalistic predictive model for the financial success
of movies based on collective activity data of online users. We show that the
popularity of a movie can be predicted much before its release by measuring and
analyzing the activity level of editors and viewers of the corresponding entry
to the movie in Wikipedia, the well-known online encyclopedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0975</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0975</id><created>2012-11-05</created><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Beyond Worst-Case Analysis in Private Singular Vector Computation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider differentially private approximate singular vector computation.
Known worst-case lower bounds show that the error of any differentially private
algorithm must scale polynomially with the dimension of the singular vector. We
are able to replace this dependence on the dimension by a natural parameter
known as the coherence of the matrix that is often observed to be significantly
smaller than the dimension both theoretically and empirically. We also prove a
matching lower bound showing that our guarantee is nearly optimal for every
setting of the coherence parameter. Notably, we achieve our bounds by giving a
robust analysis of the well-known power iteration algorithm, which may be of
independent interest. Our algorithm also leads to improvements in worst-case
settings and to better low-rank approximations in the spectral norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0978</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0978</id><created>2012-11-05</created><updated>2014-01-22</updated><authors><author><keyname>Garnero</keyname><forenames>Valentin</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author></authors><title>A linear kernel for planar total dominating set</title><categories>cs.DS</categories><comments>20 pages, 11 figures</comments><msc-class>05C85, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A &quot;total dominating set&quot; of a graph G=(V,E) is a subset D of V such that
every vertex in V is adjacent to some vertex in D. Finding a total dominating
set of minimum size is NP-complete on planar graphs and W[2]-complete on
general graphs when parameterized by the solution size. By the meta-theorem of
Bodlaender et al. [FOCS 2009], it follows that there exists a linear kernel for
Total Dominating Set on graphs of bounded genus. Nevertheless, it is not clear
how such a kernel can be effectively constructed, and how to obtain explicit
reduction rules with reasonably small constants. Following the approach of
Alber et al. [J. ACM 2004], we provide an explicit linear kernel for Total
Dominating Set on planar graphs. This result complements several known
constructive linear kernels on planar graphs for other domination problems such
as Dominating Set, Edge Dominating Set, Efficient Dominating Set, or Connected
Dominating Set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0985</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0985</id><created>2012-11-05</created><updated>2013-10-11</updated><authors><author><keyname>Geng</keyname><forenames>Quan</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Interactive Interference Alignment</title><categories>cs.IT math.IT</categories><comments>16 pages (two-column), 7 figures. Part of this work was presented in
  Information Theory Workshop 2012 (ITW), Lausanne</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study interference channels (IFC) where interaction among sources and
destinations is enabled, e.g., both sources and destinations can talk to each
other using full-duplex radios. The interaction can come in two ways: 1) {\em
In-band interaction:} sources and destinations can transmit and listen in the
same channel simultaneously, enabling interaction. 2) {\em out-of-band
interaction:} destinations talk back to the sources on an out-of-band channel,
possible from white-space channels. The flexibility afforded by interaction
among sources and destinations allows for the derivation of interference
alignment (IA) strategies that have desirable &quot;engineering properties&quot;:
insensitivity to the rationality or irrationality of channel parameters, small
block lengths and finite SNR operations. We show that for several classes of
interference channels the interactive interference alignment scheme can achieve
the optimal degrees of freedom. In particular, we show the {\em first simple
scheme} (having finite block length, for channels having no diversity) for
$K=3,4$ that can achieve the optimal degrees of freedom of $\frac{K}{2}$ even
after accounting for the cost of interaction. We also give simulation results
on the finite SNR performance of interactive alignment under some settings.
  On the technical side, we show using a Gr\&quot;{o}bner basis argument that in a
general network potentially utilizing cooperation and feedback, the optimal
degrees of freedom under linear schemes of a fixed block length is the same for
channel coefficients with probability 1. Furthermore, a numerical method to
estimate this value is also presented. These tools have potentially wider
utility in studying other wireless networks as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0986</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0986</id><created>2012-11-05</created><authors><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>New constructions of RIP matrices with fast multiplication and fewer
  rows</title><categories>cs.DS cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing, the &quot;restricted isometry property&quot; (RIP) is a
sufficient condition for the efficient reconstruction of a nearly k-sparse
vector x in C^d from m linear measurements Phi x. It is desirable for m to be
small, and for Phi to support fast matrix-vector multiplication. In this work,
we give a randomized construction of RIP matrices Phi in C^{m x d}, preserving
the L_2 norms of all k-sparse vectors with distortion 1+eps, where the
matrix-vector multiply Phi x can be computed in nearly linear time. The number
of rows m is on the order of eps^{-2}klog dlog^2(klog d). Previous analyses of
constructions of RIP matrices supporting fast matrix-vector multiplies, such as
the sampled discrete Fourier matrix, required m to be larger by roughly a log k
factor.
  Supporting fast matrix-vector multiplication is useful for iterative recovery
algorithms which repeatedly multiply by Phi or Phi^*. Furthermore, our
construction, together with a connection between RIP matrices and the
Johnson-Lindenstrauss lemma in [Krahmer-Ward, SIAM. J. Math. Anal. 2011],
implies fast Johnson-Lindenstrauss embeddings with asymptotically fewer rows
than previously known.
  Our approach is a simple twist on previous constructions. Rather than
choosing the rows for the embedding matrix to be rows sampled from some larger
structured matrix (such as the discrete Fourier transform or a random circulant
matrix), we instead choose each row of the embedding matrix to be a linear
combination of a small number of rows of the original matrix, with random sign
flips as coefficients. The main tool in our analysis is a recent bound for the
supremum of certain types of Rademacher chaos processes in
[Krahmer-Mendelson-Rauhut, arXiv:1207.0235].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0995</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0995</id><created>2012-11-05</created><authors><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>Sparsity Lower Bounds for Dimensionality Reducing Maps</title><categories>cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give near-tight lower bounds for the sparsity required in several
dimensionality reducing linear maps. First, consider the JL lemma which states
that for any set of n vectors in R there is a matrix A in R^{m x d} with m =
O(eps^{-2}log n) such that mapping by A preserves pairwise Euclidean distances
of these n vectors up to a 1 +/- eps factor. We show that there exists a set of
n vectors such that any such matrix A with at most s non-zero entries per
column must have s = Omega(eps^{-1}log n/log(1/eps)) as long as m &lt;
O(n/log(1/eps)). This bound improves the lower bound of Omega(min{eps^{-2},
eps^{-1}sqrt{log_m d}}) by [Dasgupta-Kumar-Sarlos, STOC 2010], which only held
against the stronger property of distributional JL, and only against a certain
restricted class of distributions. Meanwhile our lower bound is against the JL
lemma itself, with no restrictions. Our lower bound matches the sparse
Johnson-Lindenstrauss upper bound of [Kane-Nelson, SODA 2012] up to an
O(log(1/eps)) factor.
  Next, we show that any m x n matrix with the k-restricted isometry property
(RIP) with constant distortion must have at least Omega(klog(n/k)) non-zeroes
per column if the number of the rows is the optimal value m = O(klog (n/k)),
and if k &lt; n/polylog n. This improves the previous lower bound of Omega(min{k,
n/m}) by [Chandar, 2010] and shows that for virtually all k it is impossible to
have a sparse RIP matrix with an optimal number of rows.
  Lastly, we show that any oblivious distribution over subspace embedding
matrices with 1 non-zero per column and preserving all distances in a d
dimensional-subspace up to a constant factor with constant probability must
have at least Omega(d^2) rows. This matches one of the upper bounds in
[Nelson-Nguyen, 2012] and shows the impossibility of obtaining the best of both
of constructions in that work, namely 1 non-zero per column and \~O(d) rows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.0996</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.0996</id><created>2012-11-05</created><updated>2013-04-17</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Kanade</keyname><forenames>Varun</forenames></author></authors><title>Learning using Local Membership Queries</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model of membership query (MQ) learning, where the
learning algorithm is restricted to query points that are \emph{close} to
random examples drawn from the underlying distribution. The learning model is
intermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where
the queries are allowed to be arbitrary points).
  Membership query algorithms are not popular among machine learning
practitioners. Apart from the obvious difficulty of adaptively querying
labelers, it has also been observed that querying \emph{unnatural} points leads
to increased noise from human labelers (Lang and Baum, 1992). This motivates
our study of learning algorithms that make queries that are close to examples
generated from the data distribution.
  We restrict our attention to functions defined on the $n$-dimensional Boolean
hypercube and say that a membership query is local if its Hamming distance from
some example in the (random) training data is at most $O(\log(n))$. We show the
following results in this model:
  (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$
is polynomial time learnable under a large class of \emph{locally smooth}
distributions using $O(\log(n))$-local queries. This class also includes the
class of $O(\log(n))$-depth decision trees.
  (ii) The class of polynomial-sized decision trees is polynomial time
learnable under product distributions using $O(\log(n))$-local queries.
  (iii) The class of polynomial size DNF formulas is learnable under the
uniform distribution using $O(\log(n))$-local queries in time
$n^{O(\log(\log(n)))}$.
  (iv) In addition we prove a number of results relating the proposed model to
the traditional PAC model and the PAC+MQ model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1001</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1001</id><created>2012-11-05</created><updated>2012-11-06</updated><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author></authors><title>Majority is Stablest : Discrete and SoS</title><categories>cs.CC math.PR</categories><acm-class>F.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Majority is Stablest Theorem has numerous applications in hardness of
approximation and social choice theory. We give a new proof of the Majority is
Stablest Theorem by induction on the dimension of the discrete cube. Unlike the
previous proof, it uses neither the &quot;invariance principle&quot; nor Borell's result
in Gaussian space. The new proof is general enough to include all previous
variants of majority is stablest such as &quot;it ain't over until it's over&quot; and
&quot;Majority is most predictable&quot;. Moreover, the new proof allows us to derive a
proof of Majority is Stablest in a constant level of the Sum of Squares
hierarchy.This implies in particular that Khot-Vishnoi instance of Max-Cut does
not provide a gap instance for the Lasserre hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1002</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1002</id><created>2012-11-05</created><authors><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>OSNAP: Faster numerical linear algebra algorithms via sparser subspace
  embeddings</title><categories>cs.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An &quot;oblivious subspace embedding (OSE)&quot; given some parameters eps,d is a
distribution D over matrices B in R^{m x n} such that for any linear subspace W
in R^n with dim(W) = d it holds that Pr_{B ~ D}(forall x in W ||B x||_2 in (1
+/- eps)||x||_2) &gt; 2/3 We show an OSE exists with m = O(d^2/eps^2) and where
every B in the support of D has exactly s=1 non-zero entries per column. This
improves previously best known bound in [Clarkson-Woodruff, arXiv:1207.6365].
Our quadratic dependence on d is optimal for any OSE with s=1 [Nelson-Nguyen,
2012]. We also give two OSE's, which we call Oblivious Sparse
Norm-Approximating Projections (OSNAPs), that both allow the parameter settings
m = \~O(d/eps^2) and s = polylog(d)/eps, or m = O(d^{1+gamma}/eps^2) and
s=O(1/eps) for any constant gamma&gt;0. This m is nearly optimal since m &gt;= d is
required simply to no non-zero vector of W lands in the kernel of B. These are
the first constructions with m=o(d^2) to have s=o(d). In fact, our OSNAPs are
nothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson,
SODA 2012]. Our analyses all yield OSE's that are sampled using either
O(1)-wise or O(log d)-wise independent hash functions, which provides some
efficiency advantages over previous work for turnstile streaming applications.
Our main result is essentially a Bai-Yin type theorem in random matrix theory
and is likely to be of independent interest: i.e. we show that for any U in
R^{n x d} with orthonormal columns and random sparse B, all singular values of
BU lie in [1-eps, 1+eps] with good probability.
  Plugging OSNAPs into known algorithms for numerical linear algebra problems
such as approximate least squares regression, low rank approximation, and
approximating leverage scores implies faster algorithms for all these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1035</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1035</id><created>2012-11-02</created><authors><author><keyname>Bingol</keyname><forenames>Haluk O.</forenames></author><author><keyname>Basar</keyname><forenames>Omer</forenames></author></authors><title>Asymmetries of Men and Women in Selecting Partner</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates human dynamics in a large online dating site with
3,000 new users daily who stay in the system for 3 months on the average. The
daily activity is also quite large such as 500,000 massage transactions, 5,000
photo uploads, and 20,000 votes.
  The data investigated has 276, 210 male and 483, 963 female users. Based on
the activity that they made, there are clear distinctions between men and women
in their pattern of behavior. Men prefer lower, women prefer higher
qualifications in their partner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1036</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1036</id><created>2012-11-05</created><authors><author><keyname>Kelk</keyname><forenames>David</forenames></author><author><keyname>Devine</keyname><forenames>David</forenames></author></authors><title>A Scienceographic Comparison of Physics Papers from the arXiv and viXra
  Archives</title><categories>cs.DL physics.soc-ph</categories><comments>10 pages, 7 tables</comments><acm-class>H.3.7; J.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  arXiv is an e-print repository of papers in physics, computer science, and
biology, amongst others. viXra is a newer repository of e-prints on similar
topics. Scienceography is the study of the writing of science. In this work we
perform a scienceographic comparison of a selection of papers from the physics
section of each archive. We provide the first study of the viXra archive and
describe key differences on how science is written by these communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1041</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1041</id><created>2012-11-05</created><updated>2013-12-03</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>Algorithms and Hardness for Robust Subspace Recovery</title><categories>cs.CC cs.DS cs.IT cs.LG math.IT</categories><comments>Appeared in Proceedings of COLT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fundamental problem in unsupervised learning called
\emph{subspace recovery}: given a collection of $m$ points in $\mathbb{R}^n$,
if many but not necessarily all of these points are contained in a
$d$-dimensional subspace $T$ can we find it? The points contained in $T$ are
called {\em inliers} and the remaining points are {\em outliers}. This problem
has received considerable attention in computer science and in statistics. Yet
efficient algorithms from computer science are not robust to {\em adversarial}
outliers, and the estimators from robust statistics are hard to compute in high
dimensions.
  Are there algorithms for subspace recovery that are both robust to outliers
and efficient? We give an algorithm that finds $T$ when it contains more than a
$\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimator
is both easy to compute and well-behaved when there are a constant fraction of
outliers. We prove that it is Small Set Expansion hard to find $T$ when the
fraction of errors is any larger, thus giving evidence that our estimator is an
{\em optimal} compromise between efficiency and robustness.
  As it turns out, this basic problem has a surprising number of connections to
other areas including small set expansion, matroid theory and functional
analysis that we make use of here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1043</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1043</id><created>2012-11-05</created><authors><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>Soft (Gaussian CDE) regression models and loss functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression, unlike classification, has lacked a comprehensive and effective
approach to deal with cost-sensitive problems by the reuse (and not a
re-training) of general regression models. In this paper, a wide variety of
cost-sensitive problems in regression (such as bids, asymmetric losses and
rejection rules) can be solved effectively by a lightweight but powerful
approach, consisting of: (1) the conversion of any traditional one-parameter
crisp regression model into a two-parameter soft regression model, seen as a
normal conditional density estimator, by the use of newly-introduced enrichment
methods; and (2) the reframing of an enriched soft regression model to new
contexts by an instance-dependent optimisation of the expected loss derived
from the conditional normal distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1044</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1044</id><created>2012-11-05</created><updated>2013-07-30</updated><authors><author><keyname>Noori</keyname><forenames>Moslem</forenames></author><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author><author><keyname>Ardakani</keyname><forenames>Masoud</forenames></author></authors><title>Low-Latency Data Sharing in Erasure Multi-Way Relay Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>The paper has been accepted for publication in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an erasure multi-way relay channel (EMWRC) in which several users
share their data through a relay over erasure links. Assuming no feedback
channel between the users and the relay, we first identify the challenges for
designing a data sharing scheme over an EMWRC. Then, to overcome these
challenges, we propose practical low-latency and low-complexity data sharing
schemes based on fountain coding. Later, we introduce the notion of end-to-end
erasure rate (EEER) and analytically derive it for the proposed schemes. EEER
is then used to calculate the achievable rate and transmission overhead of the
proposed schemes. Using EEER and computer simulations, the achievable rates and
transmission overhead of our proposed schemes are compared with the ones of
one-way relaying. This comparison implies that when the number of users and the
channel erasure rates are not large, our proposed schemes outperform one-way
relaying. We also find an upper bound on the achievable rates of EMWRC and
observe that depending on the number of users and channel erasure rates, our
proposed solutions can perform very close to this bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1056</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1056</id><created>2012-11-05</created><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>How Robust are Linear Sketches to Adaptive Inputs?</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear sketches are powerful algorithmic tools that turn an n-dimensional
input into a concise lower-dimensional representation via a linear
transformation. Such sketches have seen a wide range of applications including
norm estimation over data streams, compressed sensing, and distributed
computing. In almost any realistic setting, however, a linear sketch faces the
possibility that its inputs are correlated with previous evaluations of the
sketch. Known techniques no longer guarantee the correctness of the output in
the presence of such correlations. We therefore ask: Are linear sketches
inherently non-robust to adaptively chosen inputs? We give a strong affirmative
answer to this question. Specifically, we show that no linear sketch
approximates the Euclidean norm of its input to within an arbitrary
multiplicative approximation factor on a polynomial number of adaptively chosen
inputs. The result remains true even if the dimension of the sketch is d = n -
o(n) and the sketch is given unbounded computation time. Our result is based on
an algorithm with running time polynomial in d that adaptively finds a
distribution over inputs on which the sketch is incorrect with constant
probability. Our result implies several corollaries for related problems
including lp-norm estimation and compressed sensing. Notably, we resolve an
open problem in compressed sensing regarding the feasibility of l2/l2-recovery
guarantees in the presence of computationally bounded adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1073</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1073</id><created>2012-11-05</created><updated>2012-11-26</updated><authors><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Computational and Statistical Tradeoffs via Convex Relaxation</title><categories>math.ST cs.IT math.IT math.OC stat.TH</categories><doi>10.1073/pnas.1302293110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern data analysis, one is frequently faced with statistical inference
problems involving massive datasets. Processing such large datasets is usually
viewed as a substantial computational challenge. However, if data are a
statistician's main resource then access to more data should be viewed as an
asset rather than as a burden. In this paper we describe a computational
framework based on convex relaxation to reduce the computational complexity of
an inference procedure when one has access to increasingly larger datasets.
Convex relaxation techniques have been widely used in theoretical computer
science as they give tractable approximation algorithms to many computationally
intractable tasks. We demonstrate the efficacy of this methodology in
statistical estimation in providing concrete time-data tradeoffs in a class of
denoising problems. Thus, convex relaxation offers a principled approach to
exploit the statistical gains from larger datasets to reduce the runtime of
inference algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1079</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1079</id><created>2012-11-05</created><updated>2014-10-09</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ozlen</keyname><forenames>Melih</forenames></author></authors><title>A fast branching algorithm for unknot recognition with experimental
  polynomial-time behaviour</title><categories>math.GT cs.CG math.OC</categories><comments>29 pages, 18 figures; v2: restructured into a full journal version;
  v3: minor revisions, more experimentation, more detail in the proof of Lemma
  10. To appear in Mathematical Programming</comments><msc-class>Primary 57M25, 90C57, Secondary 90C05</msc-class><acm-class>F.2.2; G.1.6; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a major unsolved problem as to whether unknot recognition - that is,
testing whether a given closed loop in R^3 can be untangled to form a plain
circle - has a polynomial time algorithm. In practice, trivial knots (which can
be untangled) are typically easy to identify using fast simplification
techniques, whereas non-trivial knots (which cannot be untangled) are more
resistant to being conclusively identified as such. Here we present the first
unknot recognition algorithm which is always conclusive and, although
exponential time in theory, exhibits a clear polynomial time behaviour under
exhaustive experimentation even for non-trivial knots.
  The algorithm draws on techniques from both topology and integer / linear
programming, and highlights the potential for new applications of techniques
from mathematical programming to difficult problems in low-dimensional
topology. The exhaustive experimentation covers all 2977 non-trivial prime
knots with &lt;= 12 crossings. We also adapt our techniques to the important
topological problems of 3-sphere recognition and the prime decomposition of
3-manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1080</identifier>
 <datestamp>2013-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1080</id><created>2012-11-05</created><authors><author><keyname>Broadbent</keyname><forenames>Anne</forenames></author><author><keyname>Gutoski</keyname><forenames>Gus</forenames></author><author><keyname>Stebila</keyname><forenames>Douglas</forenames></author></authors><title>Quantum one-time programs</title><categories>quant-ph cs.CR</categories><comments>62 pages, 5 figures</comments><msc-class>81P94, 94A60</msc-class><acm-class>E.3; F.1</acm-class><journal-ref>Advances in Cryptology -- Proc. CRYPTO 2013, LNCS vol. 8043, pp.
  344-360, Springer</journal-ref><doi>10.1007/978-3-642-40084-1_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-time programs are modelled after a black box that allows a single
evaluation of a function, and then self-destructs. Because software can, in
principle, be copied, general one-time programs exists only in the hardware
token model: it has been shown that any function admits a one-time program as
long as we assume access to physical devices called one-time memories. Quantum
information, with its well-known property of no-cloning, would, at first
glance, prevent the basic copying attack for classical programs. We show that
this intuition is false: one-time programs for both classical and quantum maps,
based solely on quantum information, do not exist, even with computational
assumptions. We complement this strong impossibility proof by an equally strong
possibility result: assuming the same basic one-time memories as used for
classical one-time programs, we show that every quantum map has a quantum
one-time program that is secure in the universal composability framework. Our
construction relies on a new, simpler quantum authentication scheme and
corresponding mechanism for computing on authenticated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1082</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1082</id><created>2012-11-05</created><updated>2013-04-26</updated><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Long</keyname><forenames>Philip M.</forenames></author></authors><title>Active and passive learning of linear separators under log-concave
  distributions</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide new results concerning label efficient, polynomial time, passive
and active learning of linear separators. We prove that active learning
provides an exponential improvement over PAC (passive) learning of homogeneous
linear separators under nearly log-concave distributions. Building on this, we
provide a computationally efficient PAC algorithm with optimal (up to a
constant factor) sample complexity for such problems. This resolves an open
question concerning the sample complexity of efficient PAC algorithms under the
uniform distribution in the unit ball. Moreover, it provides the first bound
for a polynomial-time PAC algorithm that is tight for an interesting infinite
class of hypothesis functions under a general and natural class of
data-distributions, providing significant progress towards a longstanding open
question.
  We also provide new bounds for active and passive learning in the case that
the data might not be linearly separable, both in the agnostic case and and
under the Tsybakov low-noise condition. To derive our results, we provide new
structural results for (nearly) log-concave distributions, which might be of
independent interest as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1107</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1107</id><created>2012-11-05</created><authors><author><keyname>Roul</keyname><forenames>R. K.</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>An effective web document clustering for information retrieval</title><categories>cs.IR</categories><comments>11 Pages, 2 figures</comments><report-no>IJCSMR, 2012, Vol. 1, No. 3, p. 481</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The size of web has increased exponentially over the past few years with
thousands of documents related to a subject available to the user. With this
much amount of information available, it is not possible to take the full
advantage of the World Wide Web without having a proper framework to search
through the available data. This requisite organization can be done in many
ways. In this paper we introduce a combine approach to cluster the web pages
which first finds the frequent sets and then clusters the documents. These
frequent sets are generated by using Frequent Pattern growth technique. Then by
applying Fuzzy C- Means algorithm on it, we found clusters having documents
which are highly related and have similar features. We used Gensim package to
implement our approach because of its simplicity and robust nature. We have
compared our results with the combine approach of (Frequent Pattern growth,
K-means) and (Frequent Pattern growth, Cosine_Similarity). Experimental results
show that our approach is more efficient then the above two combine approach
and can handles more efficiently the serious limitation of traditional Fuzzy
C-Means algorithm, which is sensitiveto initial centroid and the number of
clusters to be formed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1109</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1109</id><created>2012-11-05</created><authors><author><keyname>Kane</keyname><forenames>Daniel</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>A PRG for Lipschitz Functions of Polynomials with Applications to
  Sparsest Cut</title><categories>cs.CC cs.DS math.FA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give improved pseudorandom generators (PRGs) for Lipschitz functions of
low-degree polynomials over the hypercube. These are functions of the form
psi(P(x)), where P is a low-degree polynomial and psi is a function with small
Lipschitz constant. PRGs for smooth functions of low-degree polynomials have
received a lot of attention recently and play an important role in constructing
PRGs for the natural class of polynomial threshold functions. In spite of the
recent progress, no nontrivial PRGs were known for fooling Lipschitz functions
of degree O(log n) polynomials even for constant error rate. In this work, we
give the first such generator obtaining a seed-length of (log
n)\tilde{O}(d^2/eps^2) for fooling degree d polynomials with error eps.
Previous generators had an exponential dependence on the degree.
  We use our PRG to get better integrality gap instances for sparsest cut, a
fundamental problem in graph theory with many applications in graph
optimization. We give an instance of uniform sparsest cut for which a powerful
semi-definite relaxation (SDP) first introduced by Goemans and Linial and
studied in the seminal work of Arora, Rao and Vazirani has an integrality gap
of exp(\Omega((log log n)^{1/2})). Understanding the performance of the
Goemans-Linial SDP for uniform sparsest cut is an important open problem in
approximation algorithms and metric embeddings and our work gives a
near-exponential improvement over previous lower bounds which achieved a gap of
\Omega(log log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1119</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1119</id><created>2012-11-06</created><authors><author><keyname>Dabhi</keyname><forenames>Vipul K.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Sanjay</forenames></author></authors><title>A Survey on Techniques of Improving Generalization Ability of Genetic
  Programming Solutions</title><categories>cs.NE</categories><doi>10.1007/s11047-014-9416-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of empirical modeling using Genetic Programming (GP), it is
important to evolve solution with good generalization ability. Generalization
ability of GP solutions get affected by two important issues: bloat and
over-fitting. We surveyed and classified existing literature related to
different techniques used by GP research community to deal with these issues.
We also point out limitation of these techniques, if any. Moreover, the
classification of different bloat control approaches and measures for bloat and
over-fitting are also discussed. We believe that this work will be useful to GP
practitioners in following ways: (i) to better understand concepts of
generalization in GP (ii) comparing existing bloat and over-fitting control
techniques and (iii) selecting appropriate approach to improve generalization
ability of GP evolved solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1121</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1121</id><created>2012-11-06</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Numerical Schemes for Nonlinear Predictor Feedback</title><categories>math.OC cs.SY</categories><comments>29 pages, 1 Figure, submitted to Mathematics of Control, Signals and
  Systems for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementation is a common problem with feedback laws with distributed
delays. This paper focuses on a specific aspect of the implementation problem
for predictor-based feedback laws: the problem of the approximation of the
predictor mapping. It is shown that the numerical approximation of the
predictor mapping by means of a numerical scheme in conjunction with a hybrid
feedback law that uses sampled measurements, can be used for the global
stabilization of all forward complete nonlinear systems that are globally
asymptotically stabilizable and locally exponentially stabilizable in the
delay-free case. Special results are provided for the linear time invariant
case. Explicit formulae are provided for the estimation of the parameters of
the resulting hybrid control scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1123</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1123</id><created>2012-11-06</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author></authors><title>Feedback Stabilization Methods for the Solution of Nonlinear Programming
  Problems</title><categories>math.OC cs.SY math.DS</categories><comments>18 pages, 1 Figure, submitted to the Journal of Optimization Theory
  and Applications for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we show that given a nonlinear programming problem, it is
possible to construct a family of dynamical systems defined on the feasible set
of the given problem, so that: (a) the equilibrium points are the unknown
critical points of the problem, (b) each dynamical system admits the objective
function of the problem as a Lyapunov function, and (c) explicit formulae are
available without involving the unknown critical points of the problem. The
construction of the family of dynamical systems is based on the Control
Lyapunov Function methodology, which is used in mathematical control theory for
the construction of stabilizing feedback. The knowledge of a dynamical system
with the previously mentioned properties allows the construction of algorithms
which guarantee global convergence to the set of the critical points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1125</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1125</id><created>2012-11-06</created><updated>2012-12-30</updated><authors><author><keyname>Arnon-Friedman</keyname><forenames>Rotem</forenames></author><author><keyname>Ta-Shma</keyname><forenames>Amnon</forenames></author></authors><title>Limits of privacy amplification against non-signalling memory attacks</title><categories>quant-ph cs.CR</categories><comments>12 pages, 5 figures</comments><journal-ref>Phys. Rev. A 86, 062333 (2012)</journal-ref><doi>10.1103/PhysRevA.86.062333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of privacy amplification, in which Alice holds some partially secret
information with respect to an adversary Eve and wishes to distill it until it
is completely secret, is known to be solvable almost optimally both in the
classical and quantum world. Unfortunately, when considering an adversary who
is only limited by non-signalling constraints such a statement cannot be made
in general. We here prove that under the natural assumptions of time-ordered
non-signalling system, which allow past subsystems to signal future subsystems
(using the device's memory for example), super-polynomial privacy amplification
by any hashing is impossible. This is in great relevance when considering
practical device independent key distribution protocols which assume a
super-quantum adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1127</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1127</id><created>2012-11-06</created><authors><author><keyname>Rodner</keyname><forenames>Erik</forenames></author></authors><title>Visual Transfer Learning: Informal Introduction and Literature Overview</title><categories>cs.CV cs.LG</categories><comments>part of my PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer learning techniques are important to handle small training sets and
to allow for quick generalization even from only a few examples. The following
paper is the introduction as well as the literature overview part of my thesis
related to the topic of transfer learning for visual recognition problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1136</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1136</id><created>2012-11-06</created><authors><author><keyname>Malathi</keyname><forenames>S.</forenames></author><author><keyname>Sridhar</keyname><forenames>S.</forenames></author></authors><title>Estimation of Effort in Software Cost Analysis for Heterogenous Dataset
  using Fuzzy Analogy</title><categories>cs.SE</categories><comments>5 pages,5 figures</comments><journal-ref>Journal of IEEE Transactions on Software Engineering,2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the significant objectives of software engineering community is to use
effective and useful models for precise calculation of effort in software cost
estimation. The existing techniques cannot handle the dataset having
categorical variables efficiently including the commonly used analogy method.
Also, the project attributes of cost estimation are measured in terms of
linguistic values whose imprecision leads to confusion and ambiguity while
explaining the process. There are no definite set of models which can
efficiently handle the dataset having categorical variables and endure the
major hindrances such as imprecision and uncertainty without taking the
classical intervals and numeric value approaches. In this paper, a new approach
based on fuzzy logic, linguistic quantifiers and analogy based reasoning is
proposed to enhance the performance of the effort estimation in software
projects dealing with numerical and categorical data. The performance of this
proposed method illustrates that there is a realistic validation of the results
while using historical heterogeneous dataset. The results were analyzed using
the Mean Magnitude Relative Error (MMRE) and indicates that the proposed method
can produce more explicable results than the methods which are in vogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1137</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1137</id><created>2012-11-06</created><authors><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Ting</keyname><forenames>See Ho</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Wireless Compressive Sensing for Energy Harvesting Sensor Nodes</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, Submitted to the IEEE Trans. on Sig. Proc</comments><doi>10.1109/TSP.2013.2271480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the scenario in which multiple sensors send spatially correlated
data to a fusion center (FC) via independent Rayleigh-fading channels with
additive noise. Assuming that the sensor data is sparse in some basis, we show
that the recovery of this sparse signal can be formulated as a compressive
sensing (CS) problem. To model the scenario in which the sensors operate with
intermittently available energy that is harvested from the environment, we
propose that each sensor transmits independently with some probability, and
adapts the transmit power to its harvested energy. Due to the probabilistic
transmissions, the elements of the equivalent sensing matrix are not Gaussian.
Besides, since the sensors have different energy harvesting rates and different
sensor-to-FC distances, the FC has different receive signal-to-noise ratios
(SNRs) for each sensor. This is referred to as the inhomogeneity of SNRs. Thus,
the elements of the sensing matrix are also not identically distributed. For
this unconventional setting, we provide theoretical guarantees on the number of
measurements for reliable and computationally efficient recovery, by showing
that the sensing matrix satisfies the restricted isometry property (RIP), under
reasonable conditions. We then compute an achievable system delay under an
allowable mean-squared-error (MSE). Furthermore, using techniques from large
deviations theory, we analyze the impact of inhomogeneity of SNRs on the
so-called k-restricted eigenvalues, which governs the number of measurements
required for the RIP to hold. We conclude that the number of measurements
required for the RIP is not sensitive to the inhomogeneity of SNRs, when the
number of sensors n is large and the sparsity of the sensor data (signal) k
grows slower than the square root of n. Our analysis is corroborated by
extensive numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1138</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1138</id><created>2012-11-06</created><updated>2016-01-21</updated><authors><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Motion Planning for Continuous Time Stochastic Processes: A Dynamic
  Programming Approach</title><categories>math.OC cs.SY math.PR</categories><doi>10.1109/TAC.2015.2500638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study stochastic motion planning problems which involve a controlled
process, with possibly discontinuous sample paths, visiting certain subsets of
the state-space while avoiding others in a sequential fashion. For this
purpose, we first introduce two basic notions of motion planning, and then
establish a connection to a class of stochastic optimal control problems
concerned with sequential stopping times. A weak dynamic programming principle
(DPP) is then proposed, which characterizes the set of initial states that
admit a control enabling the process to execute the desired maneuver with
probability no less than some pre-specified value. The proposed DPP comprises
auxiliary value functions defined in terms of discontinuous payoff functions. A
concrete instance of the use of this novel DPP in the case of diffusion
processes is also presented. In this case, we establish that the aforementioned
set of initial states can be characterized as the level set of a discontinuous
viscosity solution to a sequence of partial differential equations, for which
the first one has a known boundary condition, while the boundary conditions of
the subsequent ones are determined by the solutions to the preceding steps.
Finally, the generality and flexibility of the theoretical results are
illustrated on an example involving biological switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1140</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1140</id><created>2012-11-06</created><authors><author><keyname>Attaccalite</keyname><forenames>C.</forenames></author><author><keyname>Barland</keyname><forenames>S.</forenames></author></authors><title>Trends in condensed matter physics: is research going faster and faster?</title><categories>physics.soc-ph cond-mat.other cs.DL</categories><journal-ref>Journal of Unsolved Questions, 3, 1, Articles 1-4, (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study research trends in condensed matter physics. Trends
are analyzed by means of the the number of publications in the different
sub-fields as function of the years. We found that many research topics have a
similar behavior with an initial fast growth and a next slower exponential
decay. We derived a simple model to describe this behavior and built up some
predictions for future trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1146</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1146</id><created>2012-11-06</created><authors><author><keyname>Goni-Moreno</keyname><forenames>Angel</forenames></author><author><keyname>Amos</keyname><forenames>Martyn</forenames></author></authors><title>Discrete modelling of bacterial conjugation dynamics</title><categories>cs.MA physics.bio-ph q-bio.CB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In bacterial populations, cells are able to cooperate in order to yield
complex collective functionalities. Interest in population-level cellular
behaviour is increasing, due to both our expanding knowledge of the underlying
biological principles, and the growing range of possible applications for
engineered microbial consortia. Researchers in the field of synthetic biology -
the application of engineering principles to living systems - have, for
example, recently shown how useful decision-making circuits may be distributed
across a bacterial population. The ability of cells to interact through small
signalling molecules (a mechanism known as it quorum sensing) is the basis for
the majority of existing engineered systems. However, horizontal gene transfer
(or conjugation) offers the possibility of cells exchanging messages (using
DNA) that are much more information-rich. The potential of engineering this
conjugation mechanism to suit specific goals will guide future developments in
this area. Motivated by a lack of computational models for examining the
specific dynamics of conjugation, we present a simulation framework for its
further study. We present an agent-based model for conjugation dynamics, with
realistic handling of physical forces. Our framework combines the management of
intercellular interactions together with simulation of intracellular genetic
networks, to provide a general-purpose platform. We validate our simulations
against existing experimental data, and then demonstrate how the emergent
mixing patterns of multi-strain populations can affect conjugation dynamics.
Our model of conjugation, based on a probability distribution, may be easily
tuned to correspond to the behaviour of different cell types. Simulation code
and movies are available at http://code.google.com/p/discus/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1149</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1149</id><created>2012-11-06</created><updated>2013-03-19</updated><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Yuan</keyname><forenames>Wen</forenames></author></authors><title>Stochastic Combinatorial Optimization via Poisson Approximation</title><categories>cs.DS</categories><comments>42 pages, 1 figure, Preliminary version appears in the Proceeding of
  the 45th ACM Symposium on the Theory of Computing (STOC13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study several stochastic combinatorial problems, including the expected
utility maximization problem, the stochastic knapsack problem and the
stochastic bin packing problem. A common technical challenge in these problems
is to optimize some function of the sum of a set of random variables. The
difficulty is mainly due to the fact that the probability distribution of the
sum is the convolution of a set of distributions, which is not an easy
objective function to work with. To tackle this difficulty, we introduce the
Poisson approximation technique. The technique is based on the Poisson
approximation theorem discovered by Le Cam, which enables us to approximate the
distribution of the sum of a set of random variables using a compound Poisson
distribution.
  We first study the expected utility maximization problem introduced recently
[Li and Despande, FOCS11]. For monotone and Lipschitz utility functions, we
obtain an additive PTAS if there is a multidimensional PTAS for the
multi-objective version of the problem, strictly generalizing the previous
result.
  For the stochastic bin packing problem (introduced in [Kleinberg, Rabani and
Tardos, STOC97]), we show there is a polynomial time algorithm which uses at
most the optimal number of bins, if we relax the size of each bin and the
overflow probability by eps.
  For stochastic knapsack, we show a 1+eps-approximation using eps extra
capacity, even when the size and reward of each item may be correlated and
cancelations of items are allowed. This generalizes the previous work [Balghat,
Goel and Khanna, SODA11] for the case without correlation and cancelation. Our
algorithm is also simpler. We also present a factor 2+eps approximation
algorithm for stochastic knapsack with cancelations. the current known
approximation factor of 8 [Gupta, Krishnaswamy, Molinaro and Ravi, FOCS11].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1154</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1154</id><created>2012-11-06</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The validation of (advanced) bibliometric indicators through peer
  assessments: A comparative study using data from InCites and F1000</title><categories>cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data of F1000 provide us with the unique opportunity to investigate the
relationship between peers' ratings and bibliometric metrics on a broad and
comprehensive data set with high-quality ratings. F1000 is a post-publication
peer review system of the biomedical literature. The comparison of metrics with
peer evaluation has been widely acknowledged as a way of validating metrics.
Based on the seven indicators offered by InCites, we analyzed the validity of
raw citation counts (Times Cited, 2nd Generation Citations, and 2nd Generation
Citations per Citing Document), normalized indicators (Journal Actual/Expected
Citations, Category Actual/Expected Citations, and Percentile in Subject Area),
and a journal based indicator (Journal Impact Factor). The data set consists of
125 papers published in 2008 and belonging to the subject category cell biology
or immunology. As the results show, Percentile in Subject Area achieves the
highest correlation with F1000 ratings; we can assert that for further three
other indicators (Times Cited, 2nd Generation Citations, and Category
Actual/Expected Citations) the 'true' correlation with the ratings reaches at
least a medium effect size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1158</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1158</id><created>2012-11-06</created><updated>2013-02-21</updated><authors><author><keyname>Gabra</keyname><forenames>Hany N.</forenames></author><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Hoda K.</forenames></author></authors><title>Data Mining Based Technique for IDS Alerts Classification</title><categories>cs.CR</categories><journal-ref>International Journal of Electronic Commerce Studies Vol.5, No.1 ,
  pp.1-6, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion detection systems (IDSs) have become a widely used measure for
security systems. The main problem for those systems results is the irrelevant
alerts on those results. We will propose a data mining based method for
classification to distinguish serious alerts and irrelevant one with a
performance of 99.9% which is better in comparison with the other recent data
mining methods that have reached the performance of 97%. A ranked alerts list
also created according to alerts importance to minimize human interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1172</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1172</id><created>2012-11-06</created><authors><author><keyname>Hoang</keyname><forenames>Thai Son</forenames></author></authors><title>Proof Hints for Event-B</title><categories>cs.SE cs.LO</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive proofs are often considered as costs of formal modelling
activity. In an incremental development environment such as the Rodin platform
for Event-B, information from proof attempts is important input for adapting
the model. This paper considers the idea of using interactive proofs to
&quot;improve&quot; the model, in particular, to convert them into automatic ones. We
propose to lift some essential proof information from the interactive proofs
into the model as what we called proof hints. In particular, proof hints are
not only for the purpose of proofs: it helps to understand the formal models
better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1188</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1188</id><created>2012-11-06</created><authors><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Mavrodiev</keyname><forenames>Pavlin</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author></authors><title>How can social herding enhance cooperation?</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a system in which N agents have to decide between two strategies
\theta_i (i \in 1... N), for defection or cooperation, when interacting with
other n agents (either spatial neighbors or randomly chosen ones). After each
round, they update their strategy responding nonlinearly to two different
information sources: (i) the payoff a_i(\theta_i, f_i) received from the
strategic interaction with their n counterparts, (ii) the fraction f_i of
cooperators in this interaction. For the latter response, we assume social
herding, i.e. agents adopt their strategy based on the frequencies of the
different strategies in their neighborhood, without taking into account the
consequences of this decision. We note that f_i already determines the payoff,
so there is no additional information assumed. A parameter \zeta defines to
what level agents take the two different information sources into account. For
the strategic interaction, we assume a Prisoner's Dilemma game, i.e. one in
which defection is the evolutionary stable strategy. However, if the additional
dimension of social herding is taken into account, we find instead a stable
outcome where cooperators are the majority. By means of agent-based computer
simulations and analytical investigations, we evaluate the critical conditions
for this transition towards cooperation. We find that, in addition to a high
degree of social herding, there has to be a nonlinear response to the fraction
of cooperators. We argue that the transition to cooperation in our model is
based on less information, i.e. on agents which are not informed about the
payoff matrix, and therefore rely on just observing the strategy of others, to
adopt it. By designing the right mechanisms to respond to this information, the
transition to cooperation can be remarkably enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1234</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1234</id><created>2012-11-06</created><updated>2013-03-01</updated><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Nejati</keyname><forenames>Hamid</forenames></author></authors><title>A Framework for Investigating the Performance of Chaotic-Map Truly
  Random Number Generators</title><categories>cs.IT math.DS math.IT</categories><comments>IEEE Transactions on Circuits and Systems-II: Express Briefs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we approximate the hidden Markov model of chaotic-map truly
random number generators (TRNGs) and describe its fundamental limits based on
the approximate entropy-rate of the underlying bit-generation process. We
demonstrate that entropy-rate plays a key role in the performance and
robustness of chaotic-map TRNGs, which must be taken into account in the
circuit design optimization. We further derive optimality conditions for
post-processing units that extract truly random bits from a raw-RNG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1242</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1242</id><created>2012-11-01</created><authors><author><keyname>Isah</keyname><forenames>Haruna</forenames></author></authors><title>Information and Communication Technology in Combating Counterfeit Drugs</title><categories>cs.OH</categories><journal-ref>International Journal of Engineering and Technology Volume 2 No.
  9, September, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pharma frauds are on the rise, counterfeit drugs are giving sleepless nights
to patients, pharmaceutical companies and governments. The laws prohibiting the
sales of counterfeit drugs cannot succeed without technological interventions.
Several analytical techniques and tools including spectroscopy, holograms,
barcoding, differentiated packing, radio frequency identification,
fingerprints, hyperspectral imaging etc. have been employed over the years in
combating this menace; however this challenge is becoming more sophisticated
with the evolution of the World Wide Web and online pharmacies. This paper
presents a review on the contribution of Information and Communication
Technology (ICT) as a drug counterfeit countermeasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1250</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1250</id><created>2012-11-06</created><authors><author><keyname>Kang</keyname><forenames>Jaewook</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Detection-Directed Sparse Estimation using Bayesian Hypothesis Test and
  Belief Propagation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a sparse recovery algorithm called
detection-directed (DD) sparse estimation using Bayesian hypothesis test (BHT)
and belief propagation (BP). In this framework, we consider the use of
sparse-binary sensing matrices which has the tree-like property and the
sampled-message approach for the implementation of BP.
  The key idea behind the proposed algorithm is that the recovery takes
DD-estimation structure consisting of two parts: support detection and signal
value estimation. BP and BHT perform the support detection, and an MMSE
estimator finds the signal values using the detected support set. The proposed
algorithm provides noise-robustness against measurement noise beyond the
conventional MAP approach, as well as a solution to remove quantization effect
by the sampled-message based BP independently of memory size for the message
sampling.
  We explain how the proposed algorithm can have the aforementioned
characteristics via exemplary discussion. In addition, our experiments validate
such superiority of the proposed algorithm, compared to recent algorithms under
noisy setup. Interestingly the experimental results show that performance of
the proposed algorithm approaches that of the oracle estimator as SNR becomes
higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1252</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1252</id><created>2012-10-16</created><authors><author><keyname>Hossain</keyname><forenames>Md. Ali</forenames></author><author><keyname>Ahsan-Ul-Ambia</keyname></author><author><keyname>Aktaruzzaman</keyname><forenames>Md.</forenames></author><author><keyname>Khan</keyname><forenames>Md. Ahaduzzaman</forenames></author></authors><title>Implementation of Radon Transformation for Electrical Impedance
  Tomography (EIT)</title><categories>cs.CV</categories><comments>12 pages</comments><journal-ref>International Journal of Information Sciences and Techniques
  (IJIST) Vol.2, No.5, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radon Transformation is generally used to construct optical image (like CT
image) from the projection data in biomedical imaging. In this paper, the
concept of Radon Transformation is implemented to reconstruct Electrical
Impedance Topographic Image (conductivity or resistivity distribution) of a
circular subject. A parallel resistance model of a subject is proposed for
Electrical Impedance Topography(EIT) or Magnetic Induction Tomography(MIT). A
circular subject with embedded circular objects is segmented into equal width
slices from different angles. For each angle, Conductance and Conductivity of
each slice is calculated and stored in an array. A back projection method is
used to generate a two-dimensional image from one-dimensional projections. As a
back projection method, Inverse Radon Transformation is applied on the
calculated conductance and conductivity to reconstruct two dimensional images.
These images are compared to the target image. In the time of image
reconstruction, different filters are used and these images are compared with
each other and target image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1255</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1255</id><created>2012-11-06</created><authors><author><keyname>Zippo</keyname><forenames>Antonio G.</forenames></author><author><keyname>Gelsomino</keyname><forenames>Giuliana</forenames></author><author><keyname>Nencini</keyname><forenames>Sara</forenames></author><author><keyname>Biella</keyname><forenames>Gabriele E. M.</forenames></author></authors><title>Handwritten digit recognition by bio-inspired hierarchical networks</title><categories>cs.LG cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human brain processes information showing learning and prediction
abilities but the underlying neuronal mechanisms still remain unknown.
Recently, many studies prove that neuronal networks are able of both
generalizations and associations of sensory inputs. In this paper, following a
set of neurophysiological evidences, we propose a learning framework with a
strong biological plausibility that mimics prominent functions of cortical
circuitries. We developed the Inductive Conceptual Network (ICN), that is a
hierarchical bio-inspired network, able to learn invariant patterns by
Variable-order Markov Models implemented in its nodes. The outputs of the
top-most node of ICN hierarchy, representing the highest input generalization,
allow for automatic classification of inputs. We found that the ICN clusterized
MNIST images with an error of 5.73% and USPS images with an error of 12.56%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1265</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1265</id><created>2012-11-06</created><authors><author><keyname>d'Angelo</keyname><forenames>Emmanuel</forenames></author><author><keyname>jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Alahi</keyname><forenames>Alexandre</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>From Bits to Images: Inversion of Local Binary Descriptors</title><categories>cs.CV cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local Binary Descriptors are becoming more and more popular for image
matching tasks, especially when going mobile. While they are extensively
studied in this context, their ability to carry enough information in order to
infer the original image is seldom addressed.
  In this work, we leverage an inverse problem approach to show that it is
possible to directly reconstruct the image content from Local Binary
Descriptors. This process relies on very broad assumptions besides the
knowledge of the pattern of the descriptor at hand. This generalizes previous
results that required either a prior learning database or non-binarized
features.
  Furthermore, our reconstruction scheme reveals differences in the way
different Local Binary Descriptors capture and encode image information. Hence,
the potential applications of our work are multiple, ranging from privacy
issues caused by eavesdropping image keypoints streamed by mobile devices to
the design of better descriptors through the visualization and the analysis of
their geometric content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1276</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1276</id><created>2012-11-06</created><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Time-bounded Reachability for Hybrid Automata: Complexity and Fixpoints</title><categories>cs.LO</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study thetime-bounded reachability problem for rectangular
hybrid automata with non-negative rates (RHA+). This problem was recently shown
to be decidable [Brihaye et al, ICALP11] (even though the unbounded
reachability problem for even very simple classes of hybrid automata is
well-known to be undecidable). However, [Brihaye et al, ICALP11] does not
provide a precise characterisation of the complexity of the time-bounded
reachability problem. The contribution of the present paper is threefold.
First, we provide a new NExpTime algorithm to solve the timed-bounded
reachability problem on RHA+. This algorithm improves on the one of [Brihaye et
al, ICALP11] by at least one exponential. Second, we show that this new
algorithm is optimal, by establishing a matching lower bound: time-bounded
reachability for RHA+ is therefore NExpTime-complete. Third, we extend these
results in a practical direction, by showing that we can effectively compute
fixpoints that characterise the sets of states that are reachable (resp.
co-reachable) within T time units from a given starting state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1279</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1279</id><created>2012-11-06</created><authors><author><keyname>Khatua</keyname><forenames>Sunirmal</forenames></author><author><keyname>Mukherjee</keyname><forenames>Nandini</forenames></author></authors><title>Application-centric Resource Provisioning for Amazon EC2 Spot Instances</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In late 2009, Amazon introduced spot instances to offer their unused
resources at lower cost with reduced reliability. Amazon's spot instances allow
customers to bid on unused Amazon EC2 capacity and run those instances for as
long as their bid exceeds the current spot price. The spot price changes
periodically based on supply and demand, and customers whose bids exceed it
gain access to the available spot instances. Customers may expect their
services at lower cost with spot instances compared to on-demand or reserved.
However the reliability is compromised since the instances(IaaS) providing the
service(SaaS) may become unavailable at any time without any notice to the
customer. Checkpointing and migration schemes are of great use to cope with
such situation. In this paper we study various checkpointing schemes that can
be used with spot instances. Also we device some algorithms for checkpointing
scheme on top of application-centric resource provisioning framework that
increase the reliability while reducing the cost significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1299</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1299</id><created>2012-11-06</created><updated>2015-01-06</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Downey</keyname><forenames>Rodney G.</forenames></author><author><keyname>Fellows</keyname><forenames>Michael R.</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Rosamond</keyname><forenames>Frances A.</forenames></author></authors><title>Myhill-Nerode methods for hypergraphs</title><categories>cs.DM cs.DS cs.FL math.CO</categories><comments>A preliminary version of this article appeared in the proceedings of
  ISAAC 2013. This extended and revised version contains the full proof
  details, more figures, and corollaries to make the application of the
  Myhill-Nerode theorem for hypergraphs easier in an algorithmic setting.
  Moreover, it provides a fix to the proof of the Myhill-Nerode theorem for
  graphs in the books of Downey and Fellows</comments><msc-class>68Q45</msc-class><acm-class>F.1.1; F.2.2; F.4.2; G.2.1; G.2.3</acm-class><journal-ref>Algorithmica 73(4):696-729, 2015</journal-ref><doi>10.1007/s00453-015-9977-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an analog of the Myhill-Nerode methods from formal language theory
for hypergraphs and use it to derive the following results for two NP-hard
hypergraph problems:
  * We provide an algorithm for testing whether a hypergraph has cutwidth at
most k that runs in linear time for constant k. In terms of parameterized
complexity theory, the problem is fixed-parameter linear parameterized by k.
  * We show that it is not expressible in monadic second-order logic whether a
hypergraph has bounded (fractional, generalized) hypertree width. The proof
leads us to conjecture that, in terms of parameterized complexity theory, these
problems are W[1]-hard parameterized by the incidence treewidth (the treewidth
of the incidence graph).
  Thus, in the form of the Myhill-Nerode theorem for hypergraphs, we obtain a
method to derive linear-time algorithms and to obtain indicators for
intractability for hypergraph problems parameterized by incidence treewidth.
  In an appendix, we point out an error and a fix to the proof of the
Myhill-Nerode theorem for graphs in Downey and Fellow's book on parameterized
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1301</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1301</id><created>2012-11-06</created><authors><author><keyname>Goc</keyname><forenames>Daniel</forenames></author><author><keyname>Mousavi</keyname><forenames>Hamoon</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>On the Number of Unbordered Factors</title><categories>cs.FL cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We illustrate a general technique for enumerating factors of k-automatic
sequences by proving a conjecture on the number f(n) of unbordered factors of
the Thue-Morse sequence. We show that f(n) &lt;= n for n &gt;= 4 and that f(n) = n
infinitely often. We also give examples of automatic sequences having exactly 2
unbordered factors of every length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1302</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1302</id><created>2012-11-06</created><updated>2014-05-07</updated><authors><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author></authors><title>Calculating Kolmogorov Complexity from the Output Frequency
  Distributions of Small Turing Machines</title><categories>cs.IT cs.CC math.IT nlin.PS</categories><comments>26 pages, 9 figures, 8 tables. Additional material can be found at
  the Algorithmic Nature Group website at http://www.algorithmicnature.org. An
  Online Algorithmic Complexity Calculator implementing this technique and
  making the data available to the research community is accessible at
  http://www.complexitycalculator.com. Corresponding author: HZ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing on various notions from theoretical computer science, we present a
novel numerical approach, motivated by the notion of algorithmic probability,
to the problem of approximating the Kolmogorov-Chaitin complexity of short
strings. The method is an alternative to the traditional lossless compression
algorithms, which it may complement, the two being serviceable for different
string lengths. We provide a thorough analysis for all $\sum_{n=1}^{11} 2^n$
binary strings of length $n&lt;12$ and for most strings of length $12\leq n
\leq16$ by running all $\sim 2.5 \times 10^{13}$ Turing machines with 5 states
and 2 symbols ($8\times 22^9$ with reduction techniques) using the most
standard formalism of Turing machines, used in for example the Busy Beaver
problem. We address the question of stability and error estimation, the
sensitivity of the continued application of the method for wider coverage and
better accuracy, and provide statistical evidence suggesting robustness. As
with compression algorithms, this work promises to deliver a range of
applications, and to provide insight into the question of complexity
calculation of finite (and short) strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1319</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1319</id><created>2012-11-06</created><authors><author><keyname>Kozma</keyname><forenames>Laszlo</forenames></author><author><keyname>Moran</keyname><forenames>Shay</forenames></author></authors><title>Shattering, Graph Orientations, and Connectivity</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a connection between two seemingly disparate fields: VC-theory and
graph theory. This connection yields natural correspondences between
fundamental concepts in VC-theory, such as shattering and VC-dimension, and
well-studied concepts of graph theory related to connectivity, combinatorial
optimization, forbidden subgraphs, and others.
  In one direction, we use this connection to derive results in graph theory.
Our main tool is a generalization of the Sauer-Shelah Lemma. Using this tool we
obtain a series of inequalities and equalities related to properties of
orientations of a graph. Some of these results appear to be new, for others we
give new and simple proofs.
  In the other direction, we present new illustrative examples of
shattering-extremal systems - a class of set-systems in VC-theory whose
understanding is considered by some authors to be incomplete. These examples
are derived from properties of orientations related to distances and flows in
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1325</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1325</id><created>2012-11-06</created><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>Composable and Efficient Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of efficient mechanism design with guaranteed good
properties even when players participate in multiple different mechanisms
simultaneously or sequentially. We define the class of smooth mechanisms,
related to smooth games defined by Roughgarden, that can be thought of as
mechanisms that generate approximately market clearing prices. We show that
smooth mechanisms result in high quality outcome in equilibrium both in the
full information setting and in the Bayesian setting with uncertainty about
participants, as well as in learning outcomes. Our main result is to show that
such mechanisms compose well: smoothness locally at each mechanism implies
efficiency globally.
  For mechanisms where good performance requires that bidders do not bid above
their value, we identify the notion of a weakly smooth mechanism. Weakly smooth
mechanisms, such as the Vickrey auction, are approximately efficient under the
no-overbidding assumption. Similar to smooth mechanisms, weakly smooth
mechanisms behave well in composition, and have high quality outcome in
equilibrium (assuming no overbidding) both in the full information setting and
in the Bayesian setting, as well as in learning outcomes.
  In most of the paper we assume participants have quasi-linear valuations. We
also extend some of our results to settings where participants have budget
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1328</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1328</id><created>2012-11-06</created><updated>2013-09-30</updated><authors><author><keyname>Urry</keyname><forenames>Matthew</forenames></author><author><keyname>Sollich</keyname><forenames>Peter</forenames></author></authors><title>Random walk kernels and learning curves for Gaussian process regression
  on random graphs</title><categories>stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG</categories><journal-ref>JMLR(14):1801-1835 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning on graphs, guided by kernels that encode similarity
between vertices. Our focus is on random walk kernels, the analogues of squared
exponential kernels in Euclidean spaces. We show that on large, locally
treelike, graphs these have some counter-intuitive properties, specifically in
the limit of large kernel lengthscales. We consider using these kernels as
covariance matrices of e.g.\ Gaussian processes (GPs). In this situation one
typically scales the prior globally to normalise the average of the prior
variance across vertices. We demonstrate that, in contrast to the Euclidean
case, this generically leads to significant variation in the prior variance
across vertices, which is undesirable from the probabilistic modelling point of
view. We suggest the random walk kernel should be normalised locally, so that
each vertex has the same prior variance, and analyse the consequences of this
by studying learning curves for Gaussian process regression. Numerical
calculations as well as novel theoretical predictions for the learning curves
using belief propagation make it clear that one obtains distinctly different
probabilistic models depending on the choice of normalisation. Our method for
predicting the learning curves using belief propagation is significantly more
accurate than previous approximations and should become exact in the limit of
large random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1332</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1332</id><created>2012-11-06</created><authors><author><keyname>Jahandideh</keyname><forenames>Hossein</forenames></author><author><keyname>Namvar</keyname><forenames>Mehrzad</forenames></author></authors><title>Use of PSO in Parameter Estimation of Robot Dynamics; Part Two:
  Robustness</title><categories>cs.RO</categories><comments>6 pages, 1 figure, 5 tables, Published in the International
  Conference on System Theory, Control and Computing 2012 (IEEE) Proceedings,
  to be indexed in IEEExplore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the robustness of the PSO-based approach to
parameter estimation of robot dynamics presented in Part One. We have made
attempts to make the PSO method more robust by experimenting with potential
cost functions. The simulated system is a cylindrical robot; through
simulation, the robot is excited, samples are taken, error is added to the
samples, and the noisy samples are used for estimating the robot parameters
through the presented method. Comparisons are made with the least squares,
total least squares, and robust least squares methods of estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1335</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1335</id><created>2012-11-06</created><authors><author><keyname>Jahandideh</keyname><forenames>Hossein</forenames></author><author><keyname>Nooranidoost</keyname><forenames>Mohammad</forenames></author><author><keyname>Enghiad</keyname><forenames>Behnam</forenames></author><author><keyname>Hajimirzakhani</keyname><forenames>Armin</forenames></author></authors><title>Ball Striking Algorithm for a 3 DOF Ping-Pong Playing Robot Based on
  Particle Swarm Optimization</title><categories>cs.RO</categories><comments>6 pages, 6 figures, 1 table, Published in the International
  Conference on System Theory, Control and Computing 2012 (IEEE) Proceedings,
  to be indexed in IEEExplore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper illustrates how a 3 degrees of freedom, Cartesian robot can be
given the task of playing ping pong against a human player. We present an
algorithm based on particle swarm optimization for the robot to calculate when
and how to hit an approaching ball. Simulation results are shown to depict the
effectiveness of our approach. Although emphasis is placed on sending the ball
to a desired point on the ping pong table, it is shown that our method may be
adjusted to meet the requirements of a variety of ball hitting strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1339</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1339</id><created>2012-11-06</created><authors><author><keyname>Jahandideh</keyname><forenames>Hossein</forenames></author><author><keyname>Namvar</keyname><forenames>Mehrzad</forenames></author></authors><title>Use of PSO in Parameter Estimation of Robot Dynamics; Part One: No Need
  for Parameterization</title><categories>cs.RO</categories><comments>6 pages, 7 tables, 3 figures published in the International
  Conference on System Theory, Control and Computing 2012 (IEEE) conference
  proceedings, to be indexed in IEEEXplore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline procedures for estimating parameters of robot dynamics are
practically based on the parameterized inverse dynamic model. In this paper, we
present a novel approach to parameter estimation of robot dynamics which
removes the necessity of parameterization (i.e. finding the minimum number of
parameters from which the dynamics can be calculated through a linear model
with respect to these parameters). This offline approach is based on a simple
and powerful swarm intelligence tool: the particle swarm optimization (PSO). In
this paper, we discuss and validate the method through simulated experiments.
In Part Two we analyze our method in terms of robustness and compare it to
robust analytical methods of estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1340</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1340</id><created>2012-11-06</created><authors><author><keyname>Vashkevich</keyname><forenames>Maxim</forenames></author><author><keyname>Petrovsky</keyname><forenames>Alexander</forenames></author></authors><title>Derivation of fast DCT algorithms using algebraic technique based on
  Galois theory</title><categories>cs.DS</categories><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an algebraic technique for derivation of fast discrete
cosine transform (DCT) algorithms. The technique is based on the algebraic
signal processing theory (ASP). In ASP a DCT associates with a polynomial
algebra C[x]/p(x). A fast algorithm is obtained as a stepwise decomposition of
C[x]/p(x). In order to reveal the connection between derivation of fast DCT
algorithms and Galois theory we define polynomial algebra over the field of
rational numbers Q instead of complex C. The decomposition of Q[x]/p(x)
requires the extension of the base field Q to splitting field E of polynomial
p(x). Galois theory is used to find intermediate subfields L_i in which
polynomial p(x) is factored. Based on this factorization fast DCT algorithm is
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1343</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1343</id><created>2012-11-06</created><updated>2015-03-17</updated><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Sulzbach</keyname><forenames>Henning</forenames></author></authors><title>The dual tree of a recursive triangulation of the disk</title><categories>math.PR cs.DM math.CO</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOP894 the Annals of
  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOP-AOP894</report-no><journal-ref>Annals of Probability 2015, Vol. 43, 738-781</journal-ref><doi>10.1214/13-AOP894</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recursive lamination of the disk, one tries to add chords one after
another at random; a chord is kept and inserted if it does not intersect any of
the previously inserted ones. Curien and Le Gall [Ann. Probab. 39 (2011)
2224-2270] have proved that the set of chords converges to a limit
triangulation of the disk encoded by a continuous process $\mathscr{M}$. Based
on a new approach resembling ideas from the so-called contraction method in
function spaces, we prove that, when properly rescaled, the planar dual of the
discrete lamination converges almost surely in the Gromov-Hausdorff sense to a
limit real tree $\mathscr{T}$, which is encoded by $\mathscr{M}$. This confirms
a conjecture of Curien and Le Gall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1345</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1345</id><created>2012-11-06</created><updated>2012-11-24</updated><authors><author><keyname>Vasic</keyname><forenames>Bata</forenames></author></authors><title>Ordered Statistics Vertex Extraction and Tracing Algorithm (OSVETA)</title><categories>cs.CG cs.MM</categories><comments>Accepted for publishing and Copyright transfered to Advances in
  Electrical and Computer Engineering, November 23th 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for identifying vertices from three dimensional (3D)
meshes that are most important for a geometric shape creation. Extracting such
a set of vertices from a 3D mesh is important in applications such as digital
watermarking, but also as a component of optimization and triangulation. In the
first step, the Ordered Statistics Vertex Extraction and Tracing Algorithm
(OSVETA) estimates precisely the local curvature, and most important
topological features of mesh geometry. Using the vertex geometric importance
ranking, the algorithm traces and extracts a vector of vertices, ordered by
decreasing index of importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1361</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1361</id><created>2012-11-06</created><authors><author><keyname>H&#xe9;bert-Dufresne</keyname><forenames>Laurent</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author><author><keyname>Dub&#xe9;</keyname><forenames>Louis J.</forenames></author></authors><title>On the constrained growth of complex critical systems</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 7 figures; prepared for the 2nd International Conference on
  Complex Sciences: Theory and Applications (Santa Fe)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical, or scale independent, systems are so ubiquitous, that gaining
theoretical insights on their nature and properties has many direct
repercussions in social and natural sciences. In this report, we start from the
simplest possible growth model for critical systems and deduce constraints in
their growth : the well-known preferential attachment principle, and, mainly, a
new law of temporal scaling. We then support our scaling law with a number of
calculations and simulations of more complex theoretical models : critical
percolation, self-organized criticality and fractal growth. Perhaps more
importantly, the scaling law is also observed in a number of empirical systems
of quite different nature : prose samples, artistic and scientific
productivity, citation networks, and the topology of the Internet. We believe
that these observations pave the way towards a general and analytical framework
for predicting the growth of complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1364</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1364</id><created>2012-11-06</created><updated>2015-09-30</updated><authors><author><keyname>Young</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author><author><keyname>H&#xe9;bert-Dufresne</keyname><forenames>Laurent</forenames></author><author><keyname>Dub&#xe9;</keyname><forenames>Louis J.</forenames></author></authors><title>A shadowing problem in the detection of overlapping communities: lifting
  the resolution limit through a cascading procedure</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>14 pages, 12 figures + supporting information (5 pages, 6 tables, 3
  figures)</comments><journal-ref>PLoS ONE 10(10): e0140133 (2015)</journal-ref><doi>10.1371/journal.pone.0140133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is the process of assigning nodes and links in
significant communities (e.g. clusters, function modules) and its development
has led to a better understanding of complex networks. When applied to sizable
networks, we argue that most detection algorithms correctly identify prominent
communities, but fail to do so across multiple scales. As a result, a
significant fraction of the network is left uncharted. We show that this
problem stems from larger or denser communities overshadowing smaller or
sparser ones, and that this effect accounts for most of the undetected
communities and unassigned links. We propose a generic cascading approach to
community detection that circumvents the problem. Using real and artificial
network datasets with three widely used community detection algorithms, we show
how a simple cascading procedure allows for the detection of the missing
communities. This work highlights a new detection limit of community structure,
and we hope that our approach can inspire better community detection
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1410</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1410</id><created>2012-11-06</created><authors><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Reed</keyname><forenames>Bruce A.</forenames></author></authors><title>A short proof that $\chi$ can be bounded $\epsilon$ away from $\Delta+1$
  towards $\omega$</title><categories>cs.DM math.CO</categories><comments>10 pages. arXiv admin note: text overlap with arXiv:0911.1741</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1998 the second author proved that there is an $\epsilon&gt;0$ such that
every graph satisfies $\chi \leq \lceil
(1-\epsilon)(\Delta+1)+\epsilon\omega\rceil$. The first author recently proved
that any graph satisfying $\omega &gt; \frac 23(\Delta+1)$ contains a stable set
intersecting every maximum clique. In this note we exploit the latter result to
give a much shorter, simpler proof of the former. We include, as a certificate
of simplicity, an appendix that proves all intermediate results with the
exception of Hall's Theorem, Brooks' Theorem, the Lov\'asz Local Lemma, and
Talagrand's Inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1411</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1411</id><created>2012-11-06</created><updated>2014-01-13</updated><authors><author><keyname>Vervoort</keyname><forenames>Louis</forenames></author></authors><title>Hidden Variable Theories: Arguments for a Paradigm Shift</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually the 'hidden variables' of Bell's theorem are supposed to describe the
pair of Bell particles. Here a semantic shift is proposed, namely to attach the
hidden variables to a stochastic medium or field in which the particles move.
It appears that under certain conditions one of the premises of Bell's theorem,
namely 'measurement independence', is not satisfied for such 'background-based'
theories, even if these only involve local interactions. Such theories
therefore do not fall under the restriction of Bell's no-go theorem. A simple
version of such background-based models are Ising models, which we investigate
here in the classical and quantum regime. We also propose to test
background-based models by a straightforward extension of existing experiments.
The present version corrects an error in the preceding version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1441</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1441</id><created>2012-11-06</created><authors><author><keyname>Janakiraman</keyname><forenames>Vijay Manikandan</forenames></author><author><keyname>Assanis</keyname><forenames>Dennis</forenames></author></authors><title>Lyapunov Method Based Online Identification of Nonlinear Systems Using
  Extreme Learning Machines</title><categories>cs.SY</categories><comments>6 pages, 13 figures and in review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme Learning Machine (ELM) is an emerging learning paradigm for nonlinear
regression problems and has shown its effectiveness in the machine learning
community. An important feature of ELM is that the learning speed is extremely
fast thanks to its random projection preprocessing step. This feature is taken
advantage of in designing an online parameter estimation algorithm for
nonlinear dynamic systems in this paper. The ELM type random projection and a
nonlinear transformation in the hidden layer and a linear output layer is
considered as a generalized model structure for a given nonlinear system and a
parameter update law is constructed based on Lyapunov principles. Simulation
results on a DC motor and Lorentz oscillator show that the proposed algorithm
is stable and has improved performance over the online-learning ELM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1442</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1442</id><created>2012-11-06</created><updated>2014-08-27</updated><authors><author><keyname>Ardila</keyname><forenames>Federico</forenames></author><author><keyname>Baker</keyname><forenames>Tia</forenames></author><author><keyname>Yatchak</keyname><forenames>Rika</forenames></author></authors><title>Moving robots efficiently using the combinatorics of CAT(0) cubical
  complexes</title><categories>math.CO cs.CG cs.DM math.MG</categories><comments>25 pages, 19 figures. (Version 2 incorporates minor changes.)</comments><journal-ref>SIAM J. Discrete Math. 28(2) (2014) 986-1007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a reconfigurable system X, such as a robot moving on a grid or a set of
particles traversing a graph without colliding, the possible positions of X
naturally form a cubical complex S(X). When S(X) is a CAT(0) space, we can
explicitly construct the shortest path between any two points, for any of the
four most natural metrics: distance, time, number of moves, and number of steps
of simultaneous moves.
  CAT(0) cubical complexes are in correspondence with posets with inconsistent
pairs (PIPs), so we can prove that a state complex S(X) is CAT(0) by
identifying the corresponding PIP. We illustrate this very general strategy
with one known and one new example: Abrams and Ghrist's positive robotic arm on
a square grid, and the robotic arm in a strip. We then use the PIP as a
combinatorial &quot;remote control&quot; to move these robots efficiently from one
position to another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1447</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1447</id><created>2012-11-06</created><authors><author><keyname>Prajapati</keyname><forenames>Harshad B.</forenames></author><author><keyname>Shah</keyname><forenames>Vipul A.</forenames></author></authors><title>Advance Reservation based DAG Application Scheduling Simulator for Grid
  Environment</title><categories>cs.DC</categories><comments>7 pages, 9 figures, 1 table</comments><journal-ref>International Journal of Computer Applications, 2013</journal-ref><doi>10.5120/9944-4585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, scheduling of Directed Acyclic Graph (DAG) application in
the context of Grid environment has attracted attention of many researchers.
However, deployment of Grid environment requires skills, efforts, budget, and
time. Although various simulation toolkits or frameworks are available for
simulating Grid environment, either they support different possible studies in
Grid computing area or takes lot of efforts in molding them to make them
suitable for scheduling of DAG application. In this paper, we describe design
and implementation of GridSim based ready to use application scheduler for
scheduling of DAG application in Grid environment. The proposed application
scheduler supports supplying DAG application and configuration of Grid
resources through GUI. We also describe implementation of Min-Min static
scheduling algorithm for scheduling of DAG application to validate the proposed
scheduler. Our proposed DAG application scheduling simulator is useful, easy,
and time-saver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1451</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1451</id><created>2012-11-07</created><updated>2013-10-10</updated><authors><author><keyname>Reddy</keyname><forenames>S. Pavan Kumar</forenames></author><author><keyname>Reddy</keyname><forenames>Y. Ganesh Kumar</forenames></author><author><keyname>Mouli</keyname><forenames>K. Chandra</forenames></author><author><keyname>Seshadri</keyname><forenames>U.</forenames></author></authors><title>Underwater Efficient Energy Communication in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>This paper is withdrawn because of its originality</comments><journal-ref>CSC Vol 1 Issue 2, 2278-9200 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper will focus on the energy efficiency issue in Underwater Wireless
Sensor Networks. In underwater environment, the two main issues are namely:
reliability and energy efficiency. These two issues are twisted pair.
Reliability requires error correction, and error-correction requires energy.
More reliability tends to imply higher energy consumption, causing difficulty
in applications that require nodes to be operated underwater for long periods
of time without batteries recharging, and in aquatic environments that render
hard the task of recharging or replacing batteries. Appropriate strategy must
therefore be in-place to ensure reliable data transmission, while conserving
energy. We propose a mathematical function of the efficiency of acoustic data
communication in real underwater environment. We did the analysis of existing
error-correction techniques, and then propose a new technique that is the
hybrid error correction technique that improves the efficiency among the
existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1457</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1457</id><created>2012-11-07</created><authors><author><keyname>Vidhisha</keyname><forenames>G.</forenames></author><author><keyname>Surekha</keyname><forenames>C.</forenames></author><author><keyname>Rayudu</keyname><forenames>S. Sanjeeva</forenames></author><author><keyname>Seshadri</keyname><forenames>U.</forenames></author></authors><title>Preserving privacy for secure and outsourcing for Linear Programming in
  cloud computing</title><categories>cs.CR cs.DC</categories><comments>8, 2012 Vol 1 Issue 2 November 2278-9200, http://www.cschronicle.org</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud computing is the long dreamed vision of computing as a utility, where
users can remotely store their data into the cloud so as to enjoy the on-demand
high quality applications and services from a shared pool of configurable
computing resources. By data outsourcing, users can be relieved from the burden
of local data storage and maintenance. we utilize the public key based
homomorphism authenticator and uniquely integrate it with random mask technique
to achieve a privacy-preserving public auditing system for cloud data storage
security while keeping all above requirements in mind. To support efficient
handling of multiple auditing tasks, we further explore the technique of
bilinear aggregate signature to extend our main result into a multi-user
setting, where TPA can perform multiple auditing tasks simultaneously along
with investigates secure outsourcing of widely applicable linear programming
(LP) computations. In order to achieve practical efficiency, our mechanism
design explicitly decomposes the LP computation outsourcing into public LP
solvers running on the cloud and private LP parameters owned by the customer
Extensive security and performance analysis shows the proposed schemes are
provably secure and highly efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1467</identifier>
 <datestamp>2013-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1467</id><created>2012-11-07</created><updated>2013-09-04</updated><authors><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Vilenchik</keyname><forenames>Dan</forenames></author></authors><title>Edge distribution in generalized graph products</title><categories>cs.DM cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$, an integer $k$, and a function $f_G:V^k \times V^k
\to {0,1}$, the $k^{th}$ graph product of $G$ w.r.t $f_G$ is the graph with
vertex set $V^k$, and an edge between two vertices $x=(x_1,...,x_k)$ and
$y=(y_1,...,y_k)$ iff $f_G(x,y)=1$. Graph products are a basic combinatorial
object, widely studied and used in different areas such as hardness of
approximation, information theory, etc. We study graph products for functions
$f_G$ of the form $f_G(x,y)=1$ iff there are at least $t$ indices $i \in [k]$
s.t. $(x_i,y_i)\in E$, where $t \in [k]$ is a fixed parameter in $f_G$. This
framework generalizes the well-known graph tensor-product (obtained for $t=k$)
and the graph or-product (obtained for $t=1$). The property that interests us
is the edge distribution in such graphs. We show that if $G$ has a spectral
gap, then the number of edges connecting &quot;large-enough&quot; sets in $G^k$ is
&quot;well-behaved&quot;, namely, it is close to the expected value, had the sets been
random. We extend our results to bi-partite graph products as well. For a
bi-partite graph $G=(X,Y,E)$, the $k^{th}$ bi-partite graph product of $G$
w.r.t $f_G$ is the bi-partite graph with vertex sets $X^k$ and $Y^k$ and edges
between $x \in X^k$ and $y \in Y^k$ iff $f_G(x,y)=1$. Finally, for both types
of graph products, optimality is asserted using the &quot;Converse to the Expander
Mixing Lemma&quot; obtained by Bilu and Linial in 2006. A byproduct of our proof
technique is a new explicit construction of a family of co-spectral graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1482</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1482</id><created>2012-11-07</created><updated>2013-01-15</updated><authors><author><keyname>Ali</keyname><forenames>Sajid</forenames></author></authors><title>Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier
  Curve and Statistical Techniques</title><categories>cs.CV</categories><comments>wrongly uploaded</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion capture is the process of recording the movement of objects or people.
It is used in military, entertainment, sports, and medical applications, and
for validation of computer vision[2] and robotics. In filmmaking and video game
development, it refers to recording actions of human actors, and using that
information to animate digital character models in 2D or 3D computer animation.
When it includes face and fingers or captures subtle
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1490</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1490</id><created>2012-11-07</created><updated>2014-06-20</updated><authors><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Seara</keyname><forenames>Carlos</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo I.</forenames></author></authors><title>New results on stabbing segments with a polygon</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a natural variation of the concept of stabbing a segment by a
simple polygon: a segment is stabbed by a simple polygon $\mathcal{P}$ if at
least one of its two endpoints is contained in $\mathcal{P}$. A segment set $S$
is stabbed by $\mathcal{P}$ if every segment of $S$ is stabbed by
$\mathcal{P}$. We show that if $S$ is a set of pairwise disjoint segments, the
problem of computing the minimum perimeter polygon stabbing $S$ can be solved
in polynomial time. We also prove that for general segments the problem is
NP-hard. Further, an adaptation of our polynomial-time algorithm solves an open
problem posed by L\&quot;offler and van Kreveld [Algorithmica 56(2), 236--269
(2010)] about finding a maximum perimeter convex hull for a set of imprecise
points modeled as line segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1505</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1505</id><created>2012-11-07</created><authors><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>Solving weighted and counting variants of connectivity problems
  parameterized by treewidth deterministically in single exponential time</title><categories>cs.DS cs.CC cs.DM</categories><comments>36 pages</comments><acm-class>F.2.2; G.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that many local graph problems, like Vertex Cover and
Dominating Set, can be solved in 2^{O(tw)}|V|^{O(1)} time for graphs G=(V,E)
with a given tree decomposition of width tw. However, for nonlocal problems,
like the fundamental class of connectivity problems, for a long time we did not
know how to do this faster than tw^{O(tw)}|V|^{O(1)}. Recently, Cygan et al.
(FOCS 2011) presented Monte Carlo algorithms for a wide range of connectivity
problems running in time $c^{tw}|V|^{O(1)} for a small constant c, e.g., for
Hamiltonian Cycle and Steiner tree. Naturally, this raises the question whether
randomization is necessary to achieve this runtime; furthermore, it is
desirable to also solve counting and weighted versions (the latter without
incurring a pseudo-polynomial cost in terms of the weights).
  We present two new approaches rooted in linear algebra, based on matrix rank
and determinants, which provide deterministic c^{tw}|V|^{O(1)} time algorithms,
also for weighted and counting versions. For example, in this time we can solve
the traveling salesman problem or count the number of Hamiltonian cycles. The
rank-based ideas provide a rather general approach for speeding up even
straightforward dynamic programming formulations by identifying &quot;small&quot; sets of
representative partial solutions; we focus on the case of expressing
connectivity via sets of partitions, but the essential ideas should have
further applications. The determinant-based approach uses the matrix tree
theorem for deriving closed formulas for counting versions of connectivity
problems; we show how to evaluate those formulas via dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1506</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1506</id><created>2012-11-07</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>Fast Hamiltonicity checking via bases of perfect matchings</title><categories>cs.DS cs.CC cs.DM</categories><comments>39 pages</comments><acm-class>F.2.2; G.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an even integer t \geq 2, the Matchings Connecivity matrix H_t is a
matrix that has rows and columns both labeled by all perfect matchings of the
complete graph K_t on t vertices; an entry H_t[M_1,M_2] is 1 if M_1\cup M_2 is
a Hamiltonian cycle and 0 otherwise. Motivated by the computational study of
the Hamiltonicity problem, we present three results on the structure of H_t: We
first show that H_t has rank at most 2^{t/2-1} over GF(2) via an appropriate
factorization that explicitly provides families of matchings X_t forming bases
for H_t. Second, we show how to quickly change representation between such
bases. Third, we notice that the sets of matchings X_t induce permutation
matrices within H_t.
  Subsequently, we use the factorization to obtain an 1.888^n n^{O(1)} time
Monte Carlo algorithm that solves the Hamiltonicity problem in directed
bipartite graphs. Our algorithm as well counts the number of Hamiltonian cycles
modulo two in directed bipartite or undirected graphs in the same time bound.
Moreover, we use the fast basis change algorithm from the second result to
present a Monte Carlo algorithm that given an undirected graph on n vertices
along with a path decomposition of width at most pw decides Hamiltonicity in
(2+\sqrt{2})^{pw}n^{O(1)} time. Finally, we use the third result to show that
for every \epsilon &gt;0 this cannot be improved to
(2+\sqrt{2}-\epsilon)^{pw}n^{O(1)} time unless the Strong Exponential Time
Hypothesis fails, i.e., a faster algorithm for this problem would imply the
breakthrough result of a (2-\epsilon)^n time algorithm for CNF-Sat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1511</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1511</id><created>2012-11-07</created><updated>2012-11-26</updated><authors><author><keyname>Mio</keyname><forenames>Matteo</forenames><affiliation>LIX, Ecole Polytechnique</affiliation></author></authors><title>Probabilistic modal {\mu}-calculus with independent product</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4; F.3.0; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (November
  27, 2012) lmcs:789</journal-ref><doi>10.2168/LMCS-8(4:18)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic modal {\mu}-calculus is a fixed-point logic designed for
expressing properties of probabilistic labeled transition systems (PLTS's). Two
equivalent semantics have been studied for this logic, both assigning to each
state a value in the interval [0,1] representing the probability that the
property expressed by the formula holds at the state. One semantics is
denotational and the other is a game semantics, specified in terms of
two-player stochastic parity games. A shortcoming of the probabilistic modal
{\mu}-calculus is the lack of expressiveness required to encode other important
temporal logics for PLTS's such as Probabilistic Computation Tree Logic (PCTL).
To address this limitation we extend the logic with a new pair of operators:
independent product and coproduct. The resulting logic, called probabilistic
modal {\mu}-calculus with independent product, can encode many properties of
interest and subsumes the qualitative fragment of PCTL. The main contribution
of this paper is the definition of an appropriate game semantics for this
extended probabilistic {\mu}-calculus. This relies on the definition of a new
class of games which generalize standard two-player stochastic (parity) games
by allowing a play to be split into concurrent subplays, each continuing their
evolution independently. Our main technical result is the equivalence of the
two semantics. The proof is carried out in ZFC set theory extended with
Martin's Axiom at an uncountable cardinal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1513</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1513</id><created>2012-11-07</created><updated>2013-03-27</updated><authors><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>K-Plane Regression</title><categories>cs.LG</categories><doi>10.1016/j.ins.2014.08.058</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel algorithm for piecewise linear regression
which can learn continuous as well as discontinuous piecewise linear functions.
The main idea is to repeatedly partition the data and learn a liner model in in
each partition. While a simple algorithm incorporating this idea does not work
well, an interesting modification results in a good algorithm. The proposed
algorithm is similar in spirit to $k$-means clustering algorithm. We show that
our algorithm can also be viewed as an EM algorithm for maximum likelihood
estimation of parameters under a reasonable probability model. We empirically
demonstrate the effectiveness of our approach by comparing its performance with
the state of art regression learning algorithms on some real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1526</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1526</id><created>2012-11-07</created><updated>2012-11-08</updated><authors><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Zhang</keyname><forenames>Mingming</forenames></author><author><keyname>Shen</keyname><forenames>Liyong</forenames></author><author><keyname>Gao</keyname><forenames>Suixiang</forenames></author></authors><title>Explosion prediction of oil gas using SVM and Logistic Regression</title><categories>cs.CE cs.LG</categories><comments>14pages,7 figures, 7 tables</comments><msc-class>62P30, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prevention of dangerous chemical accidents is a primary problem of
industrial manufacturing. In the accidents of dangerous chemicals, the oil gas
explosion plays an important role. The essential task of the explosion
prevention is to estimate the better explosion limit of a given oil gas. In
this paper, Support Vector Machines (SVM) and Logistic Regression (LR) are used
to predict the explosion of oil gas. LR can get the explicit probability
formula of explosion, and the explosive range of the concentrations of oil gas
according to the concentration of oxygen. Meanwhile, SVM gives higher accuracy
of prediction. Furthermore, considering the practical requirements, the effects
of penalty parameter on the distribution of two types of errors are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1544</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1544</id><created>2012-11-07</created><updated>2012-11-09</updated><authors><author><keyname>Burger</keyname><forenames>Harold Christopher</forenames></author><author><keyname>Schuler</keyname><forenames>Christian J.</forenames></author><author><keyname>Harmeling</keyname><forenames>Stefan</forenames></author></authors><title>Image denoising with multi-layer perceptrons, part 1: comparison with
  existing algorithms and with bounds</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image denoising can be described as the problem of mapping from a noisy image
to a noise-free image. The best currently available denoising methods
approximate this mapping with cleverly engineered algorithms. In this work we
attempt to learn this mapping directly with plain multi layer perceptrons (MLP)
applied to image patches. We will show that by training on large image
databases we are able to outperform the current state-of-the-art image
denoising methods. In addition, our method achieves results that are superior
to one type of theoretical bound and goes a large way toward closing the gap
with a second type of theoretical bound. Our approach is easily adapted to less
extensively studied types of noise, such as mixed Poisson-Gaussian noise, JPEG
artifacts, salt-and-pepper noise and noise resembling stripes, for which we
achieve excellent results as well. We will show that combining a block-matching
procedure with MLPs can further improve the results on certain images. In a
second paper, we detail the training trade-offs and the inner mechanisms of our
MLPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1550</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1550</id><created>2012-11-07</created><updated>2012-11-12</updated><authors><author><keyname>Mishra</keyname><forenames>B.</forenames></author><author><keyname>Apuroop</keyname><forenames>K. Adithya</forenames></author><author><keyname>Sepulchre</keyname><forenames>R.</forenames></author></authors><title>A Riemannian geometry for low-rank matrix completion</title><categories>cs.LG cs.NA math.OC</categories><comments>Title modified, Typos removed. arXiv admin note: text overlap with
  arXiv:1209.0430</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new Riemannian geometry for fixed-rank matrices that is
specifically tailored to the low-rank matrix completion problem. Exploiting the
degree of freedom of a quotient space, we tune the metric on our search space
to the particular least square cost function. At one level, it illustrates in a
novel way how to exploit the versatile framework of optimization on quotient
manifold. At another level, our algorithm can be considered as an improved
version of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We develop
necessary tools needed to perform both first-order and second-order
optimization. In particular, we propose gradient descent schemes (steepest
descent and conjugate gradient) and trust-region algorithms. We also show that,
thanks to the simplicity of the cost function, it is numerically cheap to
perform an exact linesearch given a search direction, which makes our
algorithms competitive with the state-of-the-art on standard low-rank matrix
completion instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1552</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1552</id><created>2012-11-07</created><authors><author><keyname>Burger</keyname><forenames>Harold Christopher</forenames></author><author><keyname>Schuler</keyname><forenames>Christian J.</forenames></author><author><keyname>Harmeling</keyname><forenames>Stefan</forenames></author></authors><title>Image denoising with multi-layer perceptrons, part 2: training
  trade-offs and analysis of their mechanisms</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image denoising can be described as the problem of mapping from a noisy image
to a noise-free image. In another paper, we show that multi-layer perceptrons
can achieve outstanding image denoising performance for various types of noise
(additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts,
salt-and-pepper noise and noise resembling stripes). In this work we discuss in
detail which trade-offs have to be considered during the training procedure. We
will show how to achieve good results and which pitfalls to avoid. By analysing
the activation patterns of the hidden units we are able to make observations
regarding the functioning principle of multi-layer perceptrons trained for
image denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1565</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1565</id><created>2012-11-07</created><authors><author><keyname>Hausenblas</keyname><forenames>Michael</forenames></author><author><keyname>Villazon-Terrazas</keyname><forenames>Boris</forenames></author><author><keyname>Cyganiak</keyname><forenames>Richard</forenames></author></authors><title>Data Shapes and Data Transformations</title><categories>cs.DB</categories><comments>9 pages</comments><acm-class>C.2.0; C.2.1; C.2.4; D.2.11; D.2.12; D.3.3; E.2; H.2.1; H.2.3; H.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, information management systems deal with data originating from
different sources including relational databases, NoSQL data stores, and Web
data formats, varying not only in terms of data formats, but also in the
underlying data model. Integrating data from heterogeneous data sources is a
time-consuming and error-prone engineering task; part of this process requires
that the data has to be transformed from its original form to other forms,
repeating all along the life cycle. With this report we provide a principled
overview on the fundamental data shapes tabular, tree, and graph as well as
transformations between them, in order to gain a better understanding for
performing said transformations more efficiently and effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1572</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1572</id><created>2012-11-07</created><updated>2012-12-02</updated><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Embedding grayscale halftone pictures in QR Codes using Correction Trees</title><categories>cs.IT cs.CR cs.MM math.IT</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barcodes like QR Codes have made that encoded messages have entered our
everyday life, what suggests to attach them a second layer of information:
directly available to human receiver for informational or marketing purposes.
We will discuss a general problem of using codes with chosen statistical
constrains, for example reproducing given grayscale picture using halftone
technique. If both sender and receiver know these constrains, the optimal
capacity can be easily approached by entropy coder. The problem is that this
time only the sender knows them - we will refer to these scenarios as
constrained coding. Kuznetsov and Tsybakov problem in which only the sender
knows which bits are fixed can be seen as a special case, surprisingly
approaching the same capacity as if both sides would know the constrains. We
will analyze Correction Trees to approach analogous capacity in the general
case - use weaker: statistical constrains, what allows to apply them to all
bits. Finding satisfying coding is similar to finding the proper correction in
error correction problem, but instead of single ensured possibility, there are
now statistically expected some. While in standard steganography we hide
information in the least important bits, this time we create codes resembling
given picture - hide information in the freedom of realizing grayness by black
and white pixels using halftone technique. We will also discuss combining with
error correction and application to rate distortion problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1581</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1581</id><created>2012-11-07</created><authors><author><keyname>Weinberg</keyname><forenames>Volker</forenames></author></authors><title>Data-parallel programming with Intel Array Building Blocks (ArBB)</title><categories>cs.PF cs.PL</categories><comments>13 pages, 7 figures, PRACE Whitepaper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intel Array Building Blocks is a high-level data-parallel programming
environment designed to produce scalable and portable results on existing and
upcoming multi- and many-core platforms.
  We have chosen several mathematical kernels - a dense matrix-matrix
multiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a
conjugate gradients solver - as synthetic benchmarks and representatives of
scientific codes and ported them to ArBB. This whitepaper describes the ArBB
ports and presents performance and scaling measurements on the Westmere-EX
based system SuperMIG at LRZ in comparison with OpenMP and MKL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1599</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1599</id><created>2012-11-07</created><authors><author><keyname>Visscher</keyname><forenames>Pieter B.</forenames></author><author><keyname>Zhu</keyname><forenames>Ru</forenames></author></authors><title>Low-dimensionality energy landscapes: Magnetic switching mechanisms and
  rates</title><categories>cond-mat.mtrl-sci cs.CE</categories><comments>5 figures</comments><journal-ref>Physica B: Condensed Matter 407.9 (2012): 1340-1344</journal-ref><doi>10.1016/j.physb.2011.07.053</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new method for the study and visualization of
dynamic processes in magnetic nanostructures, and for the accurate calculation
of rates for such processes. The method is illustrated for the case of
switching of a grain of an exchange-coupled recording medium, which switches
through domain wall nucleation and motion, but is generalizable to other rate
processes such as vortex formation and annihilation. The method involves
calculating the most probable (lowest energy) switching path and projecting the
motion onto that path. The motion is conveniently visualized in a
two-dimensional (2D) projection parameterized by the dipole and quadrupole
moment of the grain. The motion along that path can then be described by a
Langevin equation, and its rate can be computed by the classic method of
Kramers. The rate can be evaluated numerically, or in an analytic approximation
- interestingly, the analytic result for domain-wall switching is very similar
to that obtained by Brown in 1963 for coherent switching, except for a factor
proportional to the domain-wall volume. Thus in addition to its lower
coercivity, an exchange-coupled medium has the additional advantage (over a
uniform medium) of greater thermal stability, for a fixed energy barrier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1604</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1604</id><created>2012-11-07</created><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author></authors><title>Graphic lambda calculus and knot diagrams</title><categories>math.GT cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In arXiv:1207.0332 [cs.LO] was proposed a graphic lambda calculus formalism,
which has sectors corresponding to untyped lambda calculus and emergent
algebras.
  Here we explore the sector covering knot diagrams, which are constructed as
macros over the graphic lambda calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1621</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1621</id><created>2012-11-07</created><updated>2013-07-04</updated><authors><author><keyname>Boumal</keyname><forenames>Nicolas</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author><author><keyname>Absil</keyname><forenames>P. -A.</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author></authors><title>Cram\'er-Rao bounds for synchronization of rotations</title><categories>cs.IT math.IT</categories><msc-class>62F99, 94C15, 22C05, 05C12</msc-class><journal-ref>Information and Inference, 3(1), 1-39 (2014)</journal-ref><doi>10.1093/imaiai/iat006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronization of rotations is the problem of estimating a set of rotations
R_i in SO(n), i = 1, ..., N, based on noisy measurements of relative rotations
R_i R_j^T. This fundamental problem has found many recent applications, most
importantly in structural biology. We provide a framework to study
synchronization as estimation on Riemannian manifolds for arbitrary n under a
large family of noise models. The noise models we address encompass zero-mean
isotropic noise, and we develop tools for Gaussian-like as well as heavy-tail
types of noise in particular. As a main contribution, we derive the
Cram\'er-Rao bounds of synchronization, that is, lower-bounds on the variance
of unbiased estimators. We find that these bounds are structured by the
pseudoinverse of the measurement graph Laplacian, where edge weights are
proportional to measurement quality. We leverage this to provide interpretation
in terms of random walks and visualization tools for these bounds in both the
anchored and anchor-free scenarios. Similar bounds previously established were
limited to rotations in the plane and Gaussian-like noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1622</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1622</id><created>2012-11-07</created><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author></authors><title>MISO Broadcast Channel with Delayed and Evolving CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Information Theory - November 2012 18
  double column pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work considers the two-user MISO broadcast channel with gradual and
delayed accumulation of channel state information at the transmitter (CSIT),
and addresses the question of how much feedback is necessary, and when, in
order to achieve a certain degrees-of-freedom (DoF) performance. Motivated by
limited-capacity feedback links that may not immediately convey perfect CSIT,
and focusing on the block fading scenario, we consider a progressively
increasing CSIT quality as time progresses across the coherence period (T
channel uses - evolving current CSIT), or at any time after (delayed CSIT).
  Specifically, for any set of feedback quality exponents a_t, t=1,...,T,
describing the high-SNR rates-of-decay of the mean square error of the current
CSIT estimates at time t&lt;=T (during the coherence period), the work describes
the optimal DOF region in several different evolving CSIT settings, including
the setting with perfect delayed CSIT, the asymmetric setting where the quality
of feedback differs from user to user, as well as considers the DoF region in
the presence of a imperfect delayed CSIT corresponding to having a limited
number of overall feedback bits. These results are supported by novel
multi-phase precoding schemes that utilize gradually improving CSIT.
  The approach here naturally incorporates different settings such as the
perfect-delayed CSIT setting of Maddah-Ali and Tse, the imperfect current CSIT
setting of Yang et al. and of Gou and Jafar, the asymmetric setting of Maleki
et al., as well as the not-so-delayed CSIT setting of Lee and Heath.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1628</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1628</id><created>2012-11-07</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir Yankov</forenames></author></authors><title>On the Number of Disjoint Pairs of S-permutation Matrices</title><categories>math.CO cs.DM</categories><comments>14 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1202.0401</comments><msc-class>15B34, 05B20, 05C50</msc-class><journal-ref>Discrete Applied Mathematics 161 (2013) 3072-3079</journal-ref><doi>10.1016/j.dam.2013.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Journal of Statistical Planning and Inference (141) (2011) 3697-3704],
Roberto Fontana offers an algorithm for obtaining Sudoku matrices. Introduced
by Geir Dahl concept disjoint pairs of S-permutation matrices [Linear Algebra
and its Applications (430) (2009) 2457-2463] is used in this algorithm.
Analyzing the works of G. Dahl and R. Fontana, the question of finding a
general formula for counting disjoint pairs of $n^2 \times n^2$ S-permutation
matrices as a function of the integer $n$ naturally arises. This is an
interesting combinatorial problem that deserves its consideration. The present
work solves this problem. To do that, the graph theory techniques have been
used. It has been shown that to count the number of disjoint pairs of $n^2
\times n^2$ S-permutation matrices, it is sufficient to obtain some numerical
characteristics of the set of all bipartite graphs of the type $g=&lt;R_g \cup
C_g, E_g&gt;$, where $V=R_g \cup C_g$ is the set of vertices, and $E_g$ is the set
of edges of the graph $g$, $R_g \cap C_g =\emptyset$, $|R_g|=|C_g|=n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1634</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1634</id><created>2012-11-07</created><updated>2012-11-12</updated><authors><author><keyname>Nobarany</keyname><forenames>Syavash</forenames></author></authors><title>Annotations for Supporting Collaboration through Artifacts</title><categories>cs.HC cs.SI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Shared artifacts and environments play a prominent role in shaping the
collaboration between their users. This article describes this role and
explains how annotations can provide a bridge between direct communication and
collaboration through artifacts. The various functions of annotations are
discussed through examples that represent some of the important trends in
annotation research. Ultimately, some of the research issues are briefly
discussed, followed by my perspective on the future of asynchronous distributed
collaborative systems with respect to annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1636</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1636</id><created>2012-11-07</created><authors><author><keyname>Hartung</keyname><forenames>Sepp</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>On the Parameterized and Approximation Hardness of Metric Dimension</title><categories>cs.CC</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-hard Metric Dimension problem is to decide for a given graph G and a
positive integer k whether there is a vertex subset of size at most k that
separates all vertex pairs in G. Herein, a vertex v separates a pair {u,w} if
the distance (length of a shortest path) between v and u is different from the
distance of v and w. We give a polynomial-time computable reduction from the
Bipartite Dominating Set problem to Metric Dimension on maximum degree three
graphs such that there is a one-to-one correspondence between the solution sets
of both problems. There are two main consequences of this: First, it proves
that Metric Dimension on maximum degree three graphs is W[2]-complete with
respect to the parameter k. This answers an open question concerning the
parameterized complexity of Metric Dimension posed by D\'iaz et al. [ESA'12]
and already mentioned by Lokshtanov [Dagstuhl seminar, 2009]. Additionally, it
implies that Metric Dimension cannot be solved in n^{o(k)} time, unless the
assumption FPT \neq W[1] fails. This proves that a trivial n^{O(k)} algorithm
is probably asymptotically optimal.
  Second, as Bipartite Dominating Set is inapproximable within o(log n), it
follows that Metric Dimension on maximum degree three graphs is also
inapproximable by a factor of o(log n), unless NP=P. This strengthens the
result of Hauptmann et al. [JDA 2012] who proved APX-hardness on bounded-degree
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1643</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1643</id><created>2012-11-07</created><updated>2013-01-11</updated><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author></authors><title>Hybrid Behaviour of Markov Population Models</title><categories>cs.SY cs.MA cs.PF q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the behaviour of population models written in Stochastic
Concurrent Constraint Programming (sCCP), a stochastic extension of Concurrent
Constraint Programming. In particular, we focus on models from which we can
define a semantics of sCCP both in terms of Continuous Time Markov Chains
(CTMC) and in terms of Stochastic Hybrid Systems, in which some populations are
approximated continuously, while others are kept discrete. We will prove the
correctness of the hybrid semantics from the point of view of the limiting
behaviour of a sequence of models for increasing population size. More
specifically, we prove that, under suitable regularity conditions, the sequence
of CTMC constructed from sCCP programs for increasing population size converges
to the hybrid system constructed by means of the hybrid semantics. We
investigate in particular what happens for sCCP models in which some
transitions are guarded by boolean predicates or in the presence of
instantaneous transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1650</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1650</id><created>2012-11-07</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>Different Operating Systems Compatible for Image Prepress Process in
  Color Management: Analysis and Performance Testing</title><categories>cs.CV</categories><comments>6 Pages</comments><journal-ref>International Journal of Electrical, Electronics and Computer
  Systems, Volume 11, Issue1, November2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image computing has become a real catchphrase over the past few years and the
interpretations of the meaning of the term vary greatly. The Imagecomputing
market is currently rapidly evolving with high growth prospects and almost
daily announcements of new devices and application platforms, which results in
an increasing diversification of devices, operating system and development
platforms. Compared to more traditional information technology markets like the
one of desktop computing, mobile computing is much less consolidated and
neither standards nor even industry standards have yet been established. There
are various platforms and interfaces which may be used to perform the desired
tasks through the device. We have tried to compare the various mobile operating
systems and their trade-offs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1654</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1654</id><created>2012-11-07</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Agaian</keyname><forenames>Sos</forenames></author><author><keyname>Noonan</keyname><forenames>Joseph P.</forenames></author></authors><title>A New Randomness Evaluation Method with Applications to Image Shuffling
  and Encryption</title><categories>cs.CR cs.CV stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter discusses the problem of testing the degree of randomness within
an image, particularly for a shuffled or encrypted image. Its key contributions
are: 1) a mathematical model of perfectly shuffled images; 2) the derivation of
the theoretical distribution of pixel differences; 3) a new $Z$-test based
approach to differentiate whether or not a test image is perfectly shuffled;
and 4) a randomized algorithm to unbiasedly evaluate the degree of randomness
within a given image. Simulation results show that the proposed method is
robust and effective in evaluating the degree of randomness within an image,
and may often be more suitable for image applications than commonly used
testing schemes designed for binary data like NIST 800-22. The developed method
may be also useful as a first step in determining whether or not a shuffling or
encryption scheme is suitable for a particular cryptographic application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1656</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1656</id><created>2012-11-07</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Tracey</keyname><forenames>Brian</forenames></author><author><keyname>Noonan</keyname><forenames>Joseph P.</forenames></author></authors><title>James-Stein Type Center Pixel Weights for Non-Local Means Image
  Denoising</title><categories>cs.CV</categories><doi>10.1109/LSP.2013.2247755</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Local Means (NLM) and variants have been proven to be effective and
robust in many image denoising tasks. In this letter, we study the parameter
selection problem of center pixel weights (CPW) in NLM. Our key contributions
are: 1) we give a novel formulation of the CPW problem from the statistical
shrinkage perspective; 2) we introduce the James-Stein type CPWs for NLM; and
3) we propose a new adaptive CPW that is locally tuned for each image pixel.
Our experimental results showed that compared to existing CPW solutions, the
new proposed CPWs are more robust and effective under various noise levels. In
particular, the NLM with the James-Stein type CPWs attain higher means with
smaller variances in terms of the peak signal and noise ratio, implying they
improve the NLM robustness and make it less sensitive to parameter selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1658</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1658</id><created>2012-11-07</created><authors><author><keyname>Kambadur</keyname><forenames>Prabhanjan</forenames></author><author><keyname>Ghoting</keyname><forenames>Amol</forenames></author><author><keyname>Gupta</keyname><forenames>Anshul</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Andrew</forenames></author></authors><title>Extending Task Parallelism for Frequent Pattern Mining</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for frequent pattern mining, a popular informatics application,
have unique requirements that are not met by any of the existing parallel
tools. In particular, such applications operate on extremely large data sets
and have irregular memory access patterns. For efficient parallelization of
such applications, it is necessary to support dynamic load balancing along with
scheduling mechanisms that allow users to exploit data locality. Given these
requirements, task parallelism is the most promising of the available parallel
programming models. However, existing solutions for task parallelism schedule
tasks implicitly and hence, custom scheduling policies that can exploit data
locality cannot be easily employed. In this paper we demonstrate and
characterize the speedup obtained in a frequent pattern mining application
using a custom clustered scheduling policy in place of the popular Cilk-style
policy. We present PFunc, a novel task parallel library whose customizable task
scheduling and task priorities facilitated the implementation of our clustered
scheduling policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1660</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1660</id><created>2012-11-07</created><authors><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>Secret-Key Agreement Capacity over Reciprocal Fading Channels: A
  Separation Approach</title><categories>cs.IT math.IT</categories><comments>Shorter Version appeared in Allerton 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental limits of secret-key agreement over reciprocal wireless channels
are investigated. We consider a two-way block-fading channel where the channel
gains in the forward and reverse links between the legitimate terminals are
correlated. The channel gains between the legitimate terminals are not revealed
to any terminal, whereas the channel gains of the eavesdropper are revealed to
the eavesdropper. We propose a two-phase transmission scheme, that reserves a
certain portion of each coherence block for channel estimation, and the
remainder of the coherence block for correlated source generation. The
resulting secret-key involves contributions of both channel sequences and
source sequences, with the contribution of the latter becoming dominant as the
coherence period increases. We also establish an upper bound on the secret-key
capacity, which has a form structurally similar to the lower bound. Our upper
and lower bounds coincide in the limit of high signal-to-noise-ratio (SNR) and
large coherence period, thus establishing the secret-key agreement capacity in
this asymptotic regime. Numerical results indicate that the proposed scheme
achieves significant gains over training-only schemes, even for moderate SNR
and small coherence periods, thus implying the necessity of randomness-sharing
in practical secret-key generation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1661</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1661</id><created>2012-11-07</created><authors><author><keyname>Korenblit</keyname><forenames>Mark</forenames></author><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author></authors><title>A One-Vertex Decomposition Algorithm for Generating Algebraic
  Expressions of Square Rhomboids</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates relationship between algebraic expressions and graphs.
We consider a digraph called a square rhomboid that is an example of
non-series-parallel graphs. Our intention is to simplify the expressions of
square rhomboids and eventually find their shortest representations. With that
end in view, we describe the new algorithm for generating square rhomboid
expressions, which improves on our previous algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1666</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1666</id><created>2012-11-06</created><authors><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author><author><keyname>Sow</keyname><forenames>Djiby</forenames></author></authors><title>Pairings on Generalized Huff Curves</title><categories>cs.CR math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Tate pairing computation on generalized Huff curves
proposed by Wu and Feng. In fact, we extend the results of the Tate pairing
computation on the standard Huff elliptic curves done previously by Joye,
Tibouchi and Vergnaud. We show that the addition step of the Miller loop can be
performed in $1\mathbf{M}+(k+15)\mathbf{m}+2\mathbf{c}$ and the doubling one in
$1\mathbf{M} + 1\mathbf{S} + (k + 12) \mathbf{m} + 5\mathbf{s} + 2\mathbf{c}$
on the generalized Huff curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1680</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1680</id><created>2012-11-07</created><updated>2013-09-23</updated><authors><author><keyname>Leistedt</keyname><forenames>B.</forenames></author><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>P.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>S2LET: A code to perform fast wavelet analysis on the sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>8 pages, 6 figures, version accepted for publication in A&amp;A. Code is
  publicly available from http://www.s2let.org</comments><journal-ref>A&amp;A 558, A128 (2013)</journal-ref><doi>10.1051/0004-6361/201220729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe S2LET, a fast and robust implementation of the scale-discretised
wavelet transform on the sphere. Wavelets are constructed through a tiling of
the harmonic line and can be used to probe spatially localised, scale-depended
features of signals on the sphere. The scale-discretised wavelet transform was
developed previously and reduces to the needlet transform in the axisymmetric
case. The reconstruction of a signal from its wavelets coefficients is made
exact here through the use of a sampling theorem on the sphere. Moreover, a
multiresolution algorithm is presented to capture all information of each
wavelet scale in the minimal number of samples on the sphere. In addition S2LET
supports the HEALPix pixelisation scheme, in which case the transform is not
exact but nevertheless achieves good numerical accuracy. The core routines of
S2LET are written in C and have interfaces in Matlab, IDL and Java. Real
signals can be written to and read from FITS files and plotted as Mollweide
projections. The S2LET code is made publicly available, is extensively
documented, and ships with several examples in the four languages supported. At
present the code is restricted to axisymmetric wavelets but will be extended to
directional, steerable wavelets in a future release.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1690</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1690</id><created>2012-11-07</created><authors><author><keyname>Ross</keyname><forenames>Stephane</forenames></author><author><keyname>Melik-Barkhudarov</keyname><forenames>Narek</forenames></author><author><keyname>Shankar</keyname><forenames>Kumar Shaurya</forenames></author><author><keyname>Wendel</keyname><forenames>Andreas</forenames></author><author><keyname>Dey</keyname><forenames>Debadeepta</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>Learning Monocular Reactive UAV Control in Cluttered Natural
  Environments</title><categories>cs.RO cs.CV cs.LG cs.SY</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly
straight-forward, as expensive sensors and monitoring devices can be employed.
In contrast, obstacle avoidance remains a challenging task for Micro Aerial
Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike
large vehicles, MAVs can only carry very light sensors, such as cameras, making
autonomous navigation through obstacles much more challenging. In this paper,
we describe a system that navigates a small quadrotor helicopter autonomously
at low altitude through natural forest environments. Using only a single cheap
camera to perceive the environment, we are able to maintain a constant velocity
of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent
state-of-the-art imitation learning techniques to train a controller that can
avoid trees by adapting the MAVs heading. We demonstrate the performance of our
system in a more controlled environment indoors, and in real natural forest
environments outdoors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1694</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1694</id><created>2012-11-07</created><authors><author><keyname>Farahat</keyname><forenames>Ayman</forenames></author><author><keyname>Ahmed</keyname><forenames>Nesreen</forenames></author><author><keyname>Dholakia</keyname><forenames>Utpal</forenames></author></authors><title>Does a Daily Deal Promotion Signal a Distressed Business? An Empirical
  Investigation of Small Business Survival</title><categories>cs.CE stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last four years, daily deals have emerged from nowhere to become a
multi-billion dollar industry world-wide. Daily deal sites such as Groupon and
Livingsocial offer products and services at deep discounts to consumers via
email and social networks. As the industry matures, there are many questions
regarding the impact of daily deals on the marketplace. Important questions in
this regard concern the reasons why businesses decide to offer daily deals and
their longer-term impact on businesses. In the present paper, we investigate
whether the unobserved factors that make marketers run daily deals are
correlated with the unobserved factors that influence the business, In
particular, we employ the framework of seemingly unrelated regression to model
the correlation between the errors in predicting whether a business uses a
daily deal and the errors in predicting the business' survival. Our analysis
consists of the survival of 985 small businesses that offered daily deals
between January and July 2011 in the city of Chicago. Our results indicate that
there is a statistically significant correlation between the unobserved factors
that influence the business' decision to offer a daily deal and the unobserved
factors that impact its survival. Furthermore, our results indicate that the
correlation coefficient is significant in certain business categories (e.g.
restaurants).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1699</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1699</id><created>2012-11-07</created><updated>2013-04-10</updated><authors><author><keyname>Bhalgat</keyname><forenames>Anand</forenames></author><author><keyname>Gollapudi</keyname><forenames>Sreenivas</forenames></author><author><keyname>Munagala</keyname><forenames>Kamesh</forenames></author></authors><title>Optimal Auctions via the Multiplicative Weight Method</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the multiplicative weight update method provides a simple recipe
for designing and analyzing optimal Bayesian Incentive Compatible (BIC)
auctions, and reduces the time complexity of the problem to pseudo-polynomial
in parameters that depend on single agent instead of depending on the size of
the joint type space. We use this framework to design computationally efficient
optimal auctions that satisfy ex-post Individual Rationality in the presence of
constraints such as (hard, private) budgets and envy-freeness. We also design
optimal auctions when buyers and a seller's utility functions are non-linear.
Scenarios with such functions include (a) auctions with &quot;quitting rights&quot;, (b)
cost to borrow money beyond budget, (c) a seller's and buyers' risk aversion.
Finally, we show how our framework also yields optimal auctions for variety of
auction settings considered in Cai et al, Alaei et al, albeit with
pseudo-polynomial running times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1703</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1703</id><created>2012-11-07</created><updated>2013-03-30</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Deckelbaum</keyname><forenames>Alan</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>The Complexity of Optimal Mechanism Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Myerson's seminal work provides a computationally efficient revenue-optimal
auction for selling one item to multiple bidders. Generalizing this work to
selling multiple items at once has been a central question in economics and
algorithmic game theory, but its complexity has remained poorly understood. We
answer this question by showing that a revenue-optimal auction in multi-item
settings cannot be found and implemented computationally efficiently, unless
ZPP contains P^#P. This is true even for a single additive bidder whose values
for the items are independently distributed on two rational numbers with
rational probabilities. Our result is very general: we show that it is hard to
compute any encoding of an optimal auction of any format (direct or indirect,
truthful or non-truthful) that can be implemented in expected polynomial time.
In particular, under well-believed complexity-theoretic assumptions,
revenue-optimization in very simple multi-item settings can only be tractably
approximated.
  We note that our hardness result applies to randomized mechanisms in a very
simple setting, and is not an artifact of introducing combinatorial structure
to the problem by allowing correlation among item values, introducing
combinatorial valuations, or requiring the mechanism to be deterministic (whose
structure is readily combinatorial). Our proof is enabled by a
flow-interpretation of the solutions of an exponential-size linear program for
revenue maximization with an additional supermodularity constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1716</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1716</id><created>2012-11-07</created><updated>2013-06-09</updated><authors><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author><author><keyname>Voss</keyname><forenames>James</forenames></author></authors><title>Blind Signal Separation in the Presence of Gaussian Noise</title><categories>cs.LG cs.DS stat.ML</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prototypical blind signal separation problem is the so-called cocktail
party problem, with n people talking simultaneously and n different microphones
within a room. The goal is to recover each speech signal from the microphone
inputs. Mathematically this can be modeled by assuming that we are given
samples from an n-dimensional random variable X=AS, where S is a vector whose
coordinates are independent random variables corresponding to each speaker. The
objective is to recover the matrix A^{-1} given random samples from X. A range
of techniques collectively known as Independent Component Analysis (ICA) have
been proposed to address this problem in the signal processing and machine
learning literature. Many of these techniques are based on using the kurtosis
or other cumulants to recover the components.
  In this paper we propose a new algorithm for solving the blind signal
separation problem in the presence of additive Gaussian noise, when we are
given samples from X=AS+\eta, where \eta is drawn from an unknown, not
necessarily spherical n-dimensional Gaussian distribution. Our approach is
based on a method for decorrelating a sample with additive Gaussian noise under
the assumption that the underlying distribution is a linear transformation of a
distribution with independent components. Our decorrelation routine is based on
the properties of cumulant tensors and can be combined with any standard
cumulant-based method for ICA to get an algorithm that is provably robust in
the presence of Gaussian noise. We derive polynomial bounds for the sample
complexity and error propagation of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1722</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1722</id><created>2012-11-07</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Inverse problems in approximate uniform generation</title><categories>cs.CC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of \emph{inverse} problems in approximate uniform
generation, focusing on uniform generation of satisfying assignments of various
types of Boolean functions. In such an inverse problem, the algorithm is given
uniform random satisfying assignments of an unknown function $f$ belonging to a
class $\C$ of Boolean functions, and the goal is to output a probability
distribution $D$ which is $\epsilon$-close, in total variation distance, to the
uniform distribution over $f^{-1}(1)$.
  Positive results: We prove a general positive result establishing sufficient
conditions for efficient inverse approximate uniform generation for a class
$\C$. We define a new type of algorithm called a \emph{densifier} for $\C$, and
show (roughly speaking) how to combine (i) a densifier, (ii) an approximate
counting / uniform generation algorithm, and (iii) a Statistical Query learning
algorithm, to obtain an inverse approximate uniform generation algorithm. We
apply this general result to obtain a poly$(n,1/\eps)$-time algorithm for the
class of halfspaces; and a quasipoly$(n,1/\eps)$-time algorithm for the class
of $\poly(n)$-size DNF formulas.
  Negative results: We prove a general negative result establishing that the
existence of certain types of signature schemes in cryptography implies the
hardness of certain inverse approximate uniform generation problems. This
implies that there are no {subexponential}-time inverse approximate uniform
generation algorithms for 3-CNF formulas; for intersections of two halfspaces;
for degree-2 polynomial threshold functions; and for monotone 2-CNF formulas.
  Finally, we show that there is no general relationship between the complexity
of the &quot;forward&quot; approximate uniform generation problem and the complexity of
the inverse problem for a class $\C$ -- it is possible for either one to be
easy while the other is hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1728</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1728</id><created>2012-11-07</created><updated>2013-07-31</updated><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Ji</keyname><forenames>Lijun</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Wang</keyname><forenames>Chengmin</forenames></author><author><keyname>Yin</keyname><forenames>Jianxing</forenames></author></authors><title>Maximum Distance Separable Codes for Symbol-Pair Read Channels</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study (symbol-pair) codes for symbol-pair read channels introduced
recently by Cassuto and Blaum (2010). A Singleton-type bound on symbol-pair
codes is established and infinite families of optimal symbol-pair codes are
constructed. These codes are maximum distance separable (MDS) in the sense that
they meet the Singleton-type bound. In contrast to classical codes, where all
known q-ary MDS codes have length O(q), we show that q-ary MDS symbol-pair
codes can have length \Omega(q^2). In addition, we completely determine the
existence of MDS symbol-pair codes for certain parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1733</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1733</id><created>2012-11-07</created><authors><author><keyname>Nath</keyname><forenames>Swaprava</forenames></author><author><keyname>Mitra</keyname><forenames>Subrata</forenames></author></authors><title>Linear Antenna Array with Suppressed Sidelobe and Sideband Levels using
  Time Modulation</title><categories>cs.NE</categories><journal-ref>International Conference on Computers and Devices for
  Communication (CODEC) 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the goal is to achieve an ultra low sidelobe level (SLL) and
sideband levels (SBL) of a time modulated linear antenna array. The approach
followed here is not to give fixed level of excitation to the elements of an
array, but to change it dynamically with time. The excitation levels of the
different array elements over time are varied to get the low sidelobe and
sideband levels. The mathematics of getting the SLL and SBL furnished in detail
and simulation is done using the mathematical results. The excitation pattern
over time is optimized using Genetic Algorithm (GA). Since, the amplitudes of
the excitations of the elements are varied within a finite limit, results show
it gives better sidelobe and sideband suppression compared to previous time
modulated arrays with uniform amplitude excitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1736</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1736</id><created>2012-11-07</created><authors><author><keyname>Basu</keyname><forenames>Kanad</forenames></author><author><keyname>Mitra</keyname><forenames>Subrata</forenames></author><author><keyname>Mukherjee</keyname><forenames>Srishti</forenames></author><author><keyname>Wang</keyname><forenames>Weixun</forenames></author></authors><title>A Novel Approach for Handling Misbehaving Nodes in Behavior-Aware Mobile
  Networking</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profile-cast is a service paradigm within the communication framework of
delay tolerant networks (DTN). Instead of using destination addresses to
determine the final destination it uses similarity-based forwarding protocol.
With the rise in popularity of various wireless networks, the need to make
wireless technologies robust, resilient to attacks and failure becomes
mandatory. One issue that remains to be addressed in behavioral networks is
node co-operation in forwarding packets. Nodes might behave selfishly (due to
bandwidth preservation, energy /power constraints) or maliciously by dropping
packets or not forwarding them to other nodes based on profile similarity. In
both cases the net result is degradation in the performance of the network. It
is our goal to show that the performance of the behavioral network can be
improved by employing self-policing scheme that would detect node misbehavior
and then decide how to tackle them in order to ensure node cooperation or so
that the overall performance does not fall below a certain threshold. For this
various existing self-policing techniques which are in use in ad-hoc networks
will be first tried on this behavioral scenario.At various stages simulation
would be used to measure performances of the network under different
constraints, and after subjected to different techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1752</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1752</id><created>2012-11-07</created><authors><author><keyname>Anand</keyname><forenames>Abhishek</forenames></author><author><keyname>Li</keyname><forenames>Sherwin</forenames></author></authors><title>3D Scene Grammar for Parsing RGB-D Pointclouds</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We pose 3D scene-understanding as a problem of parsing in a grammar. A
grammar helps us capture the compositional structure of real-word objects,
e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple
rules for an object helps us capture structural variations in objects, e.g., a
chair can optionally also have arm-rests. Finally, having rules to capture
composition at different levels helps us formulate the entire scene-processing
pipeline as a single problem of finding most likely parse-tree---small segments
combine to form parts of objects, parts to objects and objects to a scene. We
attach a generative probability model to our grammar by having a
feature-dependent probability function for every rule. We evaluated it by
extracting labels for every segment and comparing the results with the
state-of-the-art segment-labeling algorithm. Our algorithm was outperformed by
the state-or-the-art method. But, Our model can be trained very efficiently
(within seconds), and it scales only linearly in with the number of rules in
the grammar. Also, we think that this is an important problem for the 3D vision
community. So, we are releasing our dataset and related code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1759</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1759</id><created>2012-11-07</created><authors><author><keyname>West</keyname><forenames>Jevin D.</forenames></author><author><keyname>Jacquet</keyname><forenames>Jennifer</forenames></author><author><keyname>King</keyname><forenames>Molly M.</forenames></author><author><keyname>Correll</keyname><forenames>Shelley J.</forenames></author><author><keyname>Bergstrom</keyname><forenames>Carl T.</forenames></author></authors><title>The role of gender in scholarly authorship</title><categories>physics.soc-ph cs.DL</categories><doi>10.1371/journal.pone.0066212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gender disparities appear to be decreasing in academia according to a number
of metrics, such as grant funding, hiring, acceptance at scholarly journals,
and productivity, and it might be tempting to think that gender inequity will
soon be a problem of the past. However, a large-scale analysis based on over
eight million papers across the natural sciences, social sciences, and
humanities re- reveals a number of understated and persistent ways in which
gender inequities remain. For instance, even where raw publication counts seem
to be equal between genders, close inspection reveals that, in certain fields,
men predominate in the prestigious first and last author positions. Moreover,
women are significantly underrepresented as authors of single-authored papers.
Academics should be aware of the subtle ways that gender disparities can appear
in scholarly authorship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1768</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1768</id><created>2012-11-08</created><authors><author><keyname>Olivier</keyname><forenames>Rukundo</forenames></author><author><keyname>Hanqiang</keyname><forenames>Cao</forenames></author></authors><title>Nearest Neighbor Value Interpolation</title><categories>cs.GR</categories><comments>6 pages, 11 figures, 2 tables</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA),Vol. 3, No. 4, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the nearest neighbor value (NNV) algorithm for high
resolution (H.R.) image interpolation. The difference between the proposed
algorithm and conventional nearest neighbor algorithm is that the concept
applied, to estimate the missing pixel value, is guided by the nearest value
rather than the distance. In other words, the proposed concept selects one
pixel, among four directly surrounding the empty location, whose value is
almost equal to the value generated by the conventional bilinear interpolation
algorithm. The proposed method demonstrated higher performances in terms of
H.R. when compared to the conventional interpolation algorithms mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1780</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1780</id><created>2012-11-08</created><authors><author><keyname>Doush</keyname><forenames>Iyad Abu</forenames></author><author><keyname>Alkhateeb</keyname><forenames>Faisal</forenames></author><author><keyname>Maghayreh</keyname><forenames>Eslam Al</forenames></author><author><keyname>Alsmadi</keyname><forenames>Izzat</forenames></author><author><keyname>Samarah</keyname><forenames>Samer</forenames></author></authors><title>Annotations, Collaborative Tagging, and Searching Mathematics in
  E-Learning</title><categories>cs.IR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new framework for adding semantics into e-learning
system. The proposed approach relies on two principles. The first principle is
the automatic addition of semantic information when creating the mathematical
contents. The second principle is the collaborative tagging and annotation of
the e-learning contents and the use of an ontology to categorize the e-learning
contents. The proposed system encodes the mathematical contents using
presentation MathML with RDFa annotations. The system allows students to
highlight and annotate specific parts of the e-learning contents. The objective
is to add meaning into the e-learning contents, to add relationships between
contents, and to create a framework to facilitate searching the contents. This
semantic information can be used to answer semantic queries (e.g., SPARQL) to
retrieve information request of a user. This work is implemented as an embedded
code into Moodle e-learning system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1782</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1782</id><created>2012-11-08</created><authors><author><keyname>Jha</keyname><forenames>Rakesh Kumar</forenames></author><author><keyname>Dalal</keyname><forenames>Upena D</forenames></author><author><keyname>Wankhade</keyname><forenames>A. Vishal</forenames></author></authors><title>Resource Allocation in Mobile WiMAX Network: An Optimal Approach</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the last few years there has been significant growth in the area of
wireless communication. IEEE 802.16/WiMAX is the network which is designed for
providing high speed wide area broadband wireless access; WiMAX is an emerging
wireless technology for creating multi-hop Mesh network. Future generation
networks will be characterized by variable and high data rates, Quality of
Services (QoS), seamless mobility both within a network and between networks of
different technologies and service providers. A technology is developed to
accomplish these necessities is regular by IEEE, is 802.16, also called as
WiMAX (Worldwide Interoperability for Microwave Access). This architecture aims
to apply Long range connectivity, High data rates, High security, Low power
utilization and Excellent Quality of Services and squat deployment costs to a
wireless access technology on a metropolitan level. In this paper we have
observed the performance analysis of location based resource allocation for
WiMAX and WLAN-WiMAX client and in second phase we observed the rate-adaptive
algorithms. We know that base station (BS) is observed the ranging first for
all subscribers then established the link between them and in final phase they
will allocate the resource with Subcarriers allocation according to the demand
(UL) i.e. video, voice and data application. We propose linear approach,
Active-Set optimization and Genetic Algorithm for Resource Allocation in
downlink Mobile WiMAX networks. Purpose of proposed algorithms is to optimize
total throughput. Simulation results show that Genetic Algorithm and Active-Set
algorithm performs better than previous methods in terms of higher capacities
but GA have high complexity then active set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1788</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1788</id><created>2012-11-08</created><authors><author><keyname>Patil</keyname><forenames>Dipti</forenames></author><author><keyname>Wadhai</keyname><forenames>Dr. Vijay M.</forenames></author><author><keyname>Gund</keyname><forenames>Mayuri</forenames></author><author><keyname>Biyani</keyname><forenames>Richa</forenames></author><author><keyname>Andhalkar</keyname><forenames>Snehal</forenames></author><author><keyname>Agrawal</keyname><forenames>Bhagyashree</forenames></author></authors><title>An Adaptive parameter free data mining approach for healthcare
  application</title><categories>cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1105.1950 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world, healthcare is the most important factor affecting human
life. Due to heavy work load it is not possible for personal healthcare. The
proposed system acts as a preventive measure for determining whether a person
is fit or unfit based on person's historical and real time data by applying
clustering algorithms like K-means and D-stream. The Density-based clustering
algorithm i.e. the D-stream algorithm overcomes drawbacks of K-Means algorithm.
By calculating their performance measures we finally find out effectiveness and
efficiency of both the algorithms. Both clustering algorithms are applied on
patient's bio-medical historical database. To check the correctness of both the
algorithms, we apply them on patient's current bio-medical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1790</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1790</id><created>2012-11-08</created><authors><author><keyname>Hu</keyname><forenames>Ke</forenames></author><author><keyname>Xiang</keyname><forenames>Ju</forenames></author><author><keyname>Yang</keyname><forenames>Wanchun</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoke</forenames></author><author><keyname>Tang</keyname><forenames>Yi</forenames></author></authors><title>Link Prediction in Complex Networks by Multi Degree
  Preferential-Attachment Indices</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:0905.3558, arXiv:0901.0553, arXiv:1010.0725 by other authors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In principle, the rules of links formation of a network model can be
considered as a kind of link prediction algorithm. By revisiting the
preferential attachment mechanism for generating a scale-free network, here we
propose a class of preferential attachment indices which are different from the
previous one. Traditionally, the preferential attachment index is defined by
the product of the related nodes degrees, while the new indices will define the
similarity score of a pair of nodes by either the maximum in the two nodes
degrees or the summarization of their degrees. Extensive experiments are
carried out on fourteen real-world networks. Compared with the traditional
preferential attachment index, the new ones, especially the
degree-summarization similarity index, can provide more accurate prediction on
most of the networks. Due to the improved prediction accuracy and low
computational complexity, these proposed preferential attachment indices may be
of help to provide an instruction for mining unknown links in incomplete
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1799</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1799</id><created>2012-11-08</created><authors><author><keyname>Kaiser</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Algorithm for Missing Values Imputation in Categorical Data with Use of
  Association Rules</title><categories>cs.LG</categories><comments>4 pages, 3 tables, 2011 Third International Joint Journal Conference
  in Computer, Electronics and Electrical, ACEEE International Journal on
  Recent Trends in Engineering &amp; Technology, Vol. 06, Is. 01, Nov 2011</comments><journal-ref>ACEEE International Journal on Recent Trends in Engineering &amp;
  Technology 6 (2011) 111-114</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents algorithm for missing values imputation in categorical
data. The algorithm is based on using association rules and is presented in
three variants. Experimental shows better accuracy of missing values imputation
using the algorithm then using most common attribute value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1800</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1800</id><created>2012-11-08</created><authors><author><keyname>Hassen</keyname><forenames>Hamdi</forenames></author><author><keyname>khemakhem</keyname><forenames>Maher</forenames></author></authors><title>A Comparative study of Arabic handwritten characters invariant feature</title><categories>cs.CV</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No. 12, 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper is practically interested in the unchangeable feature of Arabic
handwritten character. It presents results of comparative study achieved on
certain features extraction techniques of handwritten character, based on Hough
transform, Fourier transform, Wavelet transform and Gabor Filter. Obtained
results show that Hough Transform and Gabor filter are insensible to the
rotation and translation, Fourier Transform is sensible to the rotation but
insensible to the translation, in contrast to Hough Transform and Gabor filter,
Wavelets Transform is sensitive to the rotation as well as to the translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1819</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1819</id><created>2012-11-08</created><updated>2013-01-11</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames><affiliation>David</affiliation></author><author><keyname>Chen</keyname><forenames>Yun</forenames><affiliation>David</affiliation></author><author><keyname>Ding</keyname><forenames>Na</forenames><affiliation>David</affiliation></author><author><keyname>Lin</keyname><forenames>Jia-Chin</forenames><affiliation>David</affiliation></author><author><keyname>Zeng</keyname><forenames>Xiaoyang</forenames><affiliation>David</affiliation></author><author><keyname>Defeng</keyname><affiliation>David</affiliation></author><author><keyname>Huang</keyname></author></authors><title>Accurate Sampling Timing Acquisition for Baseband OFDM Power-line
  Communication in Non-Gaussian Noise</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel technique is proposed to address the joint sampling
timing acquisition for baseband and broadband power-line communication (BB-PLC)
systems using Orthogonal-Frequency-Division-Multiplexing (OFDM), including the
sampling phase offset (SPO) and the sampling clock offset (SCO). Under pairwise
correlation and joint Gaussian assumption of received signals in frequency
domain, an approximated form of the log-likelihood function is derived. Instead
of a high complexity two-dimension grid-search on the likelihood function, a
five-step method is employed for accurate estimations. Several variants are
presented in the same framework with different complexities. Unlike
conventional pilot-assisted schemes using the extra phase rotations within one
OFDM block, the proposed technique turns to the phase rotations between
adjacent OFDM blocks. Analytical expressions of the variances and biases are
derived. Extensive simulation results indicate significant performance
improvements over conventional schemes. Additionally, effects of several noise
models including non-Gaussianity, cyclo-stationarity, and temporal correlation
are analyzed and simulated. Robustness of the proposed technique against
violation of the joint Gaussian assumption is also verified by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1830</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1830</id><created>2012-11-08</created><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Chen</keyname><forenames>Yun</forenames></author><author><keyname>Zeng</keyname><forenames>Xiaoyang</forenames></author></authors><title>Fine Residual Carrier Frequency and Sampling Frequency Estimation in
  Wireless OFDM Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2013, Budapest</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel algorithm for residual phase estimation in
wireless OFDM systems, including the carrier frequency offset (CFO) and the
sampling frequency offset (SFO). The subcarriers are partitioned into several
regions which exhibit pairwise correlations. The phase increment between
successive OFDM blocks is exploited which can be estimated by two estimators
with different computational loads. Numerical results of estimation variance
are presented. Simulations indicate performance improvement of the proposed
technique over several conventional schemes in a multipath channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1857</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1857</id><created>2012-11-08</created><authors><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author><author><keyname>Janssen</keyname><forenames>Jeffrey</forenames></author></authors><title>Simple I/O-efficient flow accumulation on grid terrains</title><categories>cs.DS</categories><comments>This paper is an exact copy of the paper that appeared in the
  abstract collection of the Workshop on Massive Data Algorithms, Aarhus, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The flow accumulation problem for grid terrains takes as input a matrix of
flow directions, that specifies for each cell of the grid to which of its eight
neighbours any incoming water would flow. The problem is to compute, for each
cell c, from how many cells of the terrain water would reach c. We show that
this problem can be solved in O(scan(N)) I/Os for a terrain of N cells. Taking
constant factors in the I/O-efficiency into account, our algorithm may be an
order of magnitude faster than the previously known algorithm that is based on
time-forward processing and needs O(sort(N)) I/Os.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1858</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1858</id><created>2012-11-08</created><updated>2013-07-12</updated><authors><author><keyname>Vuillemin</keyname><forenames>Pierre</forenames></author><author><keyname>Poussot-Vassal</keyname><forenames>Charles</forenames></author><author><keyname>Alazard</keyname><forenames>Daniel</forenames></author></authors><title>A Spectral Expression for the Frequency-Limited H2-norm</title><categories>cs.SY math.DS</categories><comments>Submitted</comments><msc-class>37N35 (Primary), 93A15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new simple but yet efficient spectral expression of the
frequency-limited H2-norm, denoted H2w-norm, is introduced. The proposed new
formulation requires the computation of the system eigenvalues and eigenvectors
only, and provides thus an alternative to the well established Gramian-based
approach. The interest of this new formulation is in three-folds: (i) it
provides a new theoretical framework for the H2w-norm-based optimization
approach, such as controller synthesis, filter design and model approximation,
(ii) it improves the H2w-norm computation velocity and it applicability to
models of higher dimension, and (iii) under some conditions, it allows to
handle systems with poles on the imaginary axis. Both mathematical proofs and
numerical illustrations are provided to assess this new H2w-norm expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1860</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1860</id><created>2012-11-08</created><updated>2013-06-19</updated><authors><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Telelis</keyname><forenames>Orestis</forenames></author></authors><title>On the Inefficiency of the Uniform Price Auction</title><categories>cs.GT</categories><comments>Additions and Improvements upon SAGT 2012 results (and minor
  corrections on the previous version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our results on Uniform Price Auctions, one of the standard
sealed-bid multi-unit auction formats, for selling multiple identical units of
a single good to multi-demand bidders. Contrary to the truthful and
economically efficient multi-unit Vickrey auction, the Uniform Price Auction
encourages strategic bidding and is socially inefficient in general. The
uniform pricing rule is, however, widely popular by its appeal to the natural
anticipation, that identical items should be identically priced. In this work
we study equilibria of the Uniform Price Auction for bidders with (symmetric)
submodular valuation functions, over the number of units that they win. We
investigate pure Nash equilibria of the auction in undominated strategies; we
produce a characterization of these equilibria that allows us to prove that a
fraction 1-1/e of the optimum social welfare is always recovered in undominated
pure Nash equilibrium -- and this bound is essentially tight. Subsequently, we
study the auction under the incomplete information setting and prove a bound of
4-2/k on the economic inefficiency of (mixed) Bayes Nash equilibria that are
supported by undominated strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1861</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1861</id><created>2012-11-08</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author></authors><title>Automating Legal Research through Data Mining</title><categories>cs.IR</categories><comments>8 pages, 11 figures, published in (IJACSA) International Journal of
  Advanced Computer Science and Applications. arXiv admin note: text overlap
  with wikipedia entry on text mining</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA) Vol. 1, No 6, December 2010, pp. 9-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term legal research generally refers to the process of identifying and
retrieving appropriate information necessary to support legal decision making
from past case records. At present, the process is mostly manual, but some
traditional technologies such as keyword searching are commonly used to speed
the process up. But a keyword search is not a comprehensive search to cater to
the requirements of legal research as the search result includes too many false
hits in terms of irrelevant case records. Hence the present generic tools
cannot be used to automate legal research.
  This paper presents a framework which was developed by combining several Text
Mining techniques to automate the process overcoming the difficulties in the
existing methods. Further, the research also identifies the possible
enhancements that could be done to enhance the effectiveness of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1878</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1878</id><created>2012-11-06</created><authors><author><keyname>Joosten</keyname><forenames>Joost J.</forenames></author></authors><title>On the necessity of complexity</title><categories>cs.LO cs.CC</categories><comments>17 pages, 3 figures</comments><msc-class>68Q80, 68Q01, 68Q05, 8Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wolfram's Principle of Computational Equivalence (PCE) implies that universal
complexity abounds in nature. This paper comprises three sections. In the first
section we consider the question why there are so many universal phenomena
around. So, in a sense, we week a driving force behind the PCE if any. We
postulate a principle GNS that we call the Generalized Natural Selection
Principle that together with the Church-Turing Thesis is seen to be equivalent
to a weak version of PCE. In the second section we ask the question why we do
not observe any phenomena that are complex but not-universal. We choose a
cognitive setting to embark on this question and make some analogies with
formal logic. In the third and final section we report on a case study where we
see rich structures arise everywhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1893</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1893</id><created>2012-11-06</created><authors><author><keyname>Karygianni</keyname><forenames>Sofia</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Tangent-based manifold approximation with locally linear models</title><categories>cs.LG cs.CV</categories><doi>10.1016/j.sigpro.2014.03.047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of manifold approximation with affine
subspaces. Our objective is to discover a set of low dimensional affine
subspaces that represents manifold data accurately while preserving the
manifold's structure. For this purpose, we employ a greedy technique that
partitions manifold samples into groups that can be each approximated by a low
dimensional subspace. We start by considering each manifold sample as a
different group and we use the difference of tangents to determine appropriate
group mergings. We repeat this procedure until we reach the desired number of
sample groups. The best low dimensional affine subspaces corresponding to the
final groups constitute our approximate manifold representation. Our
experiments verify the effectiveness of the proposed scheme and show its
superior performance compared to state-of-the-art methods for manifold
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1904</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1904</id><created>2012-11-08</created><authors><author><keyname>Benaloh</keyname><forenames>Josh</forenames></author><author><keyname>Byrne</keyname><forenames>Mike</forenames></author><author><keyname>Kortum</keyname><forenames>Philip</forenames></author><author><keyname>McBurnett</keyname><forenames>Neal</forenames></author><author><keyname>Pereira</keyname><forenames>Olivier</forenames></author><author><keyname>Stark</keyname><forenames>Philip B.</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana
DeBeauvoir described the qualities she wanted in her ideal election system to
replace their existing DREs. In response, in April of 2012, the authors,
working with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting
system with a DRE-style human interface and a &quot;belt and suspenders&quot; approach to
verifiability. It provides both a paper trail and end-to-end cryptography using
COTS hardware. It is designed to support both ballot-level risk-limiting
audits, and auditing by individual voters and observers. The human interface
and process flow is based on modern usability research. This paper describes
the STAR-Vote architecture, which could well be the next-generation voting
system for Travis County and perhaps elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1909</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1909</id><created>2012-11-08</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Chazelle</keyname><forenames>Bernard</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>On the Convergence of the Hegselmann-Krause System</title><categories>cs.DS cs.SI nlin.AO</categories><comments>9 pages, 1 figure, to appear in ITCS '13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study convergence of the following discrete-time non-linear dynamical
system: n agents are located in R^d and at every time step, each moves
synchronously to the average location of all agents within a unit distance of
it. This popularly studied system was introduced by Krause to model the
dynamics of opinion formation and is often referred to as the Hegselmann-Krause
model. We prove the first polynomial time bound for the convergence of this
system in arbitrary dimensions. This improves on the bound of n^{O(n)}
resulting from a more general theorem of Chazelle. Also, we show a quadratic
lower bound and improve the upper bound for one-dimensional systems to O(n^3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1932</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1932</id><created>2012-11-08</created><updated>2013-02-04</updated><authors><author><keyname>Kamath</keyname><forenames>Govinda M.</forenames></author><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Codes with Local Regeneration</title><categories>cs.IT math.IT</categories><comments>44 pages, 7 figures. A class of codes termed as Uniform Rank
  Accumulation (URA) codes is introduced and a minimum distance bound is
  derived when the local codes are URA codes. Also, the results of our earlier
  arXiv submssion(arXiv:1202:2414[cs.IT]) are included in Section 3 of this
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes and codes with locality are two schemes that have recently
been proposed to ensure data collection and reliability in a distributed
storage network. In a situation where one is attempting to repair a failed
node, regenerating codes seek to minimize the amount of data downloaded for
node repair, while codes with locality attempt to minimize the number of helper
nodes accessed. In this paper, we provide several constructions for a class of
vector codes with locality in which the local codes are regenerating codes,
that enjoy both advantages. We derive an upper bound on the minimum distance of
this class of codes and show that the proposed constructions achieve this
bound. The constructions include both the cases where the local regenerating
codes correspond to the MSR as well as the MBR point on the
storage-repair-bandwidth tradeoff curve of regenerating codes. Also included is
a performance comparison of various code constructions for fixed block length
and minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1958</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1958</id><created>2012-11-08</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Approximability and proof complexity</title><categories>cs.CC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with the proof-complexity of certifying that
optimization problems do \emph{not} have good solutions. Specifically we
consider bounded-degree &quot;Sum of Squares&quot; (SOS) proofs, a powerful algebraic
proof system introduced in 1999 by Grigoriev and Vorobjov. Work of Shor,
Lasserre, and Parrilo shows that this proof system is automatizable using
semidefinite programming (SDP), meaning that any $n$-variable degree-$d$ proof
can be found in time $n^{O(d)}$. Furthermore, the SDP is dual to the well-known
Lasserre SDP hierarchy, meaning that the &quot;$d/2$-round Lasserre value&quot; of an
optimization problem is equal to the best bound provable using a degree-$d$ SOS
proof. These ideas were exploited in a recent paper by Barak et al.\ (STOC
2012) which shows that the known &quot;hard instances&quot; for the Unique-Games problem
are in fact solved close to optimally by a constant level of the Lasserre SDP
hierarchy.
  We continue the study of the power of SOS proofs in the context of difficult
optimization problems. In particular, we show that the Balanced-Separator
integrality gap instances proposed by Devanur et al.\ can have their optimal
value certified by a degree-4 SOS proof. The key ingredient is an SOS proof of
the KKL Theorem. We also investigate the extent to which the Khot--Vishnoi
Max-Cut integrality gap instances can have their optimum value certified by an
SOS proof. We show they can be certified to within a factor .952 ($&gt; .878$)
using a constant-degree proof. These investigations also raise an interesting
mathematical question: is there a constant-degree SOS proof of the Central
Limit Theorem?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1968</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1968</id><created>2012-11-08</created><updated>2013-02-16</updated><authors><author><keyname>Zhao</keyname><forenames>Zhizhen</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Fourier-Bessel rotational invariant eigenimages</title><categories>cs.CV</categories><comments>7 pages</comments><journal-ref>JOSA A, Vol. 30, Issue 5, pp. 871-877 (2013)</journal-ref><doi>10.1364/JOSAA.30.000871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient and accurate algorithm for principal component
analysis (PCA) of a large set of two dimensional images, and, for each image,
the set of its uniform rotations in the plane and its reflection. The algorithm
starts by expanding each image, originally given on a Cartesian grid, in the
Fourier-Bessel basis for the disk. Because the images are bandlimited in the
Fourier domain, we use a sampling criterion to truncate the Fourier-Bessel
expansion such that the maximum amount of information is preserved without the
effect of aliasing. The constructed covariance matrix is invariant to rotation
and reflection and has a special block diagonal structure. PCA is efficiently
done for each block separately. This Fourier-Bessel based PCA detects more
meaningful eigenimages and has improved denoising capability compared to
traditional PCA for a finite number of noisy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1969</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1969</id><created>2012-11-08</created><authors><author><keyname>Tran</keyname><forenames>Le-Nam</forenames></author><author><keyname>Hanif</keyname><forenames>Muhammad Fainan</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>Fast Converging Algorithm for Weighted Sum Rate Maximization in
  Multicell MISO Downlink</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures. The MATLAB code of the proposed algorithm can be
  downloaded from: https://sites.google.com/site/namletran/publication</comments><journal-ref>IEEE Signal Processing Letters, vol.19, no.12, pp.872-875, Dec.
  2012. URL:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6327333&amp;isnumber=6323087</journal-ref><doi>10.1109/LSP.2012.2223211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing weighted sum rates in the downlink of a multicell
environment is of considerable interest. Unfortunately, this problem is known
to be NP-hard. For the case of multi-antenna base stations and single antenna
mobile terminals, we devise a low complexity, fast and provably convergent
algorithm that locally optimizes the weighted sum rate in the downlink of the
system. In particular, we derive an iterative second-order cone program
formulation of the weighted sum rate maximization problem. The algorithm
converges to a local optimum within a few iterations. Superior performance of
the proposed approach is established by numerically comparing it to other known
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.1999</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.1999</id><created>2012-11-08</created><updated>2014-02-04</updated><authors><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Reed</keyname><forenames>Bruce A.</forenames></author><author><keyname>Wu</keyname><forenames>Hehui</forenames></author></authors><title>A Proof of a Conjecture of Ohba</title><categories>math.CO cs.DM</categories><comments>21 pages</comments><msc-class>05C15</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a conjecture of Ohba which says that every graph $G$ on at most
$2\chi(G)+1$ vertices satisfies $\chi_\ell(G)=\chi(G)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2007</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2007</id><created>2012-11-08</created><authors><author><keyname>Ejbali</keyname><forenames>Ridha</forenames></author><author><keyname>Zaied</keyname><forenames>Mourad</forenames></author><author><keyname>Amar</keyname><forenames>Chokri Ben</forenames></author></authors><title>Multi-input Multi-output Beta Wavelet Network: Modeling of Acoustic
  Units for Speech Recognition</title><categories>cs.CV</categories><comments>7 pages, 10 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 3, No.4, 2012, 38-44</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel architecture of wavelet network called
Multi-input Multi-output Wavelet Network MIMOWN as a generalization of the old
architecture of wavelet network. This newel prototype was applied to speech
recognition application especially to model acoustic unit of speech. The
originality of our work is the proposal of MIMOWN to model acoustic unit of
speech. This approach was proposed to overcome limitation of old wavelet
network model. The use of the multi-input multi-output architecture will allows
training wavelet network on various examples of acoustic units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2008</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2008</id><created>2012-11-08</created><updated>2013-02-24</updated><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>On multidimensional generalized Cram\'er-Rao inequalities, uncertainty
  relations and characterizations of generalized $q$-Gaussian distributions</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><msc-class>28D20, 94A17, 62B10, 39B62</msc-class><journal-ref>J. Phys. A: Math. Theor. vol. 46, no 9, 095303, 2013</journal-ref><doi>10.1088/1751-8113/46/9/095303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, we show how the generalized Cram\'er-Rao inequality for
the estimation of a parameter, presented in a recent paper, can be extended to
the mutidimensional case with general norms on $\mathbb{R}^{n}$, and to a wider
context. As a particular case, we obtain a new multidimensional Cram\'er-Rao
inequality which is saturated by generalized $q$-Gaussian distributions. We
also give another related Cram\'er-Rao inequality, for a general norm, which is
saturated as well by these distributions. Finally, we derive uncertainty
relations from these Cram\'er-Rao inequalities. These uncertainty relations
involve moments computed with respect to escort distributions, and we show that
some of these relations are saturated by generalized $q$-Gaussian
distributions. These results introduce extended versions of Fisher information,
new Cram\'er-Rao inequalities, and new characterizations of generalized
$q$-Gaussian distributions which are important in several areas of physics and
mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2020</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2020</id><created>2012-11-08</created><updated>2013-11-29</updated><authors><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>J. M.</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>R.</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>P.</forenames></author><author><keyname>Ventura</keyname><forenames>I.</forenames></author></authors><title>New results on the coarseness of bicolored point sets</title><categories>math.CO cs.DM math.MG</categories><comments>Presented at the Mexican Conference on Discrete Mathematics and
  Computational Geometry 2013, Oaxaca, Mexico</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a 2-colored (red and blue) set of $n$ points in the plane. A
subset $I$ of $S$ is an island if there exits a convex set $C$ such that
$I=C\cap S$. The discrepancy of an island is the absolute value of the number
of red minus the number of blue points it contains. A convex partition of $S$
is a partition of $S$ into islands with pairwise disjoint convex hulls. The
discrepancy of a convex partition is the discrepancy of its island of minimum
discrepancy. The coarseness of $S$ is the discrepancy of the convex partition
of $S$ with maximum discrepancy. This concept was recently defined by Bereg et
al. [CGTA 2013]. In this paper we study the following problem: Given a set $S$
of $n$ points in general position in the plane, how to color each of them (red
or blue) such that the resulting 2-colored point set has small coarseness? We
prove that every $n$-point set $S$ can be colored such that its coarseness is
$O(n^{1/4}\sqrt{\log n})$. This bound is almost tight since there exist
$n$-point sets such that every 2-coloring gives coarseness at least
$\Omega(n^{1/4})$. Additionally, we show that there exists an approximation
algorithm for computing the coarseness of a 2-colored point set, whose ratio is
between $1/128$ and $1/64$, solving an open problem posted by Bereg et al.
[CGTA 2013]. All our results consider $k$-separable islands of $S$, for some
$k$, which are those resulting from intersecting $S$ with at most $k$
halfplanes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2026</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2026</id><created>2012-11-08</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author></authors><title>Multicasting over Overlay Networks A Critical Review</title><categories>cs.NI</categories><comments>8 pages, 1 figure, 1 table</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA) Vol. 2, No 3, March 2011, pp. 54-61</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicasting technology uses the minimum network resources to serve multiple
clients by duplicating the data packets at the closest possible point to the
clients. This way at most only one data packets travels down a network link at
any one time irrespective of how many clients receive this packet.
Traditionally multicasting has been implemented over a specialized network
built using multicast routers. This kind of network has the drawback of
requiring the deployment of special routers that are more expensive than
ordinary routers. Recently there is new interest in delivering multicast
traffic over application layer overlay networks. Application layer overlay
networks though built on top of the physical network, behave like an
independent virtual network made up of only logical links between the nodes.
Several authors have proposed systems, mechanisms and protocols for the
implementation of multicast media streaming over overlay networks. In this
paper, the author takes a critical look at these systems and mechanism with
special reference to their strengths and weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2028</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2028</id><created>2012-11-08</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author><author><keyname>Jayasundara</keyname><forenames>Ravindi</forenames></author></authors><title>A Decision Support Tool for Inferring Further Education Desires of Youth
  in Sri Lanka</title><categories>cs.CY</categories><comments>8 pages, 4 figures, 16 tables</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), vol. 2, no 6, June 2011, pp. 28-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results of a study carried out to identify the
factors that influence the further education desires of Sri Lankan youth.
Statistical modeling has been initially used to infer the desires of the youth
and then a decision support tool has been developed based on the statistical
model developed. In order to carry out the analysis and the development of the
model, data collected as part of the National Youth Survey has been used. The
accuracy of the model and the decision support tool has been tested by using a
random data sets and the accuracy was found to be well above 80 percent, which
is sufficient for any policy related decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2030</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2030</id><created>2012-11-08</created><authors><author><keyname>Accisano</keyname><forenames>Paul</forenames></author><author><keyname>&#xdc;ng&#xf6;r</keyname><forenames>Alper</forenames></author></authors><title>Hardness Results on Curve/Point Set Matching with Fr\'echet Distance</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let P be a polygonal curve in R^d of length n, and S be a point-set of size
k. We consider the problem of finding a polygonal curve Q on S such that all
points in S are visited and the Fr\'echet distance from $P$ is less than a
given epsilon. We show that this problem is NP-complete, regardless of whether
or not points from S are allowed be visited more than once. However, we also
show that if the problem instance satisfies certain restrictions, the problem
is polynomial-time solvable, and we briefly outline an algorithm that computes
Q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2032</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2032</id><created>2012-11-08</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author></authors><title>Implementation of Security in Distributed Systems - A Comparative Study</title><categories>cs.DC</categories><comments>6 pages, 4 figures</comments><journal-ref>International Journal of Computer Information Systems (IJCIS),
  Vol. 2, No. 2, February 2011, pp-1-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparative study of distributed systems and the
security issues associated with those systems. Four commonly used distributed
systems were considered for detailed analysis in terms of technologies
involved, security issues faced by them and solution proposed to circumvent
those issues. Finally the security issues and the solutions were summarized and
compared with each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2033</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2033</id><created>2012-11-08</created><authors><author><keyname>Firdhous</keyname><forenames>Mohamed</forenames></author><author><keyname>Karunaratne</keyname><forenames>P. M.</forenames></author></authors><title>An ICT Enhanced Life Quality for the Elderly in Developing Countries:
  Analysis Study Applied to Sri Lanka</title><categories>cs.CY</categories><journal-ref>Journal of Health Informatics in Developing Countries, Vol. 5, No
  1, 2011, pp. 47-59</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, the entire world has seen a tremendous increase in the
elderly population. Even though countries differ in the numerical criterion for
defining the old age, the UN has agreed that the cutoff of 60+ years refers to
the older population. When the percentage of older population increases in a
country, the country faces new challenges in the form of economical as well as
social impacts. The economic impacts are felt in the form of a shortfall in
labor supply, income, household savings while an increase in the payment of
retirement benefits and healthcare expenditures. On the social side, the
elderly population feels continuously isolated due to the changes in the value
system. The United Nations has identified five quality of life characteristics
for the elderly in its 1991 resolution named the United Nations Principles for
Older Persons. In this paper the authors initially look at the trends in the
increase of aged population in Asia in general and Sri Lanka in particular.
Then they discuss how the quality of life characteristics can be achieved in a
cost effective way through the use of Information and Communication Technology
(ICT). The authors take an in depth look at the recent developments in field of
ICT and its penetration in developing countries especially Sri Lanka with
special emphasis to constraints and challenges in adopting ICT for the
improving the quality of life of elderly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2037</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2037</id><created>2012-11-08</created><authors><author><keyname>J.</keyname><forenames>Rehna V.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>M. K.</forenames></author></authors><title>Time Complexity Analysis of Binary Space Partitioning Scheme for Image
  Compression</title><categories>cs.CV</categories><comments>5 pages, 5 figures, 2 tables, International Journal of Engineering
  and Innovative Technology; ISSN: 2277-3754 ISO 9001:2008</comments><journal-ref>Certified International Journal of Engineering and Innovative
  Technology (IJEIT) Volume 2, Issue 3, 2012, 109-113</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation-based image coding methods provide high compression ratios when
compared with traditional image coding approaches like the transform and sub
band coding for low bit-rate compression applications. In this paper, a
segmentation-based image coding method, namely the Binary Space Partition
scheme, that divides the desired image using a recursive procedure for coding
is presented. The BSP approach partitions the desired image recursively by
using bisecting lines, selected from a collection of discrete optional lines,
in a hierarchical manner. This partitioning procedure generates a binary tree,
which is referred to as the BSP-tree representation of the desired image. The
algorithm is extremely complex in computation and has high execution time. The
time complexity of the BSP scheme is explored in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2038</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2038</id><created>2012-11-08</created><authors><author><keyname>Thouti</keyname><forenames>Krishnahari</forenames></author><author><keyname>Sathe</keyname><forenames>S. R.</forenames></author></authors><title>Comparison of OpenMP &amp; OpenCL Parallel Processing Technologies</title><categories>cs.DC</categories><comments>6 pages, 7 figures;
  http://thesai.org/Downloads/Volume3No4/Paper_10-Comparison_of_OpenMP_OpenCL_Parallel_Processing_Technologies.pdf</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Volume 3, issue 4, 2012, 56-61</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparison of OpenMP and OpenCL based on the parallel
implementation of algorithms from various fields of computer applications. The
focus of our study is on the performance of benchmark comparing OpenMP and
OpenCL. We observed that OpenCL programming model is a good option for mapping
threads on different processing cores. Balancing all available cores and
allocating sufficient amount of work among all computing units, can lead to
improved performance. In our simulation, we used Fedora operating system; a
system with Intel Xeon Dual core processor having thread count 24 coupled with
NVIDIA Quadro FX 3800 as graphical processing unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2041</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2041</id><created>2012-11-08</created><authors><author><keyname>Yao</keyname><forenames>Yuan</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Yan</keyname><forenames>Xifeng</forenames></author><author><keyname>Xu</keyname><forenames>Feng</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>MaTrust: An Effective Multi-Aspect Trust Inference Model</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trust is a fundamental concept in many real-world applications such as
e-commerce and peer-to-peer networks. In these applications, users can generate
local opinions about the counterparts based on direct experiences, and these
opinions can then be aggregated to build trust among unknown users. The
mechanism to build new trust relationships based on existing ones is referred
to as trust inference. State-of-the-art trust inference approaches employ the
transitivity property of trust by propagating trust along connected users. In
this paper, we propose a novel trust inference model (MaTrust) by exploring an
equally important property of trust, i.e., the multi-aspect property. MaTrust
directly characterizes multiple latent factors for each trustor and trustee
from the locally-generated trust relationships. Furthermore, it can naturally
incorporate prior knowledge as specified factors. These factors in turn serve
as the basis to infer the unseen trustworthiness scores. Experimental
evaluations on real data sets show that the proposed MaTrust significantly
outperforms several benchmark trust inference models in both effectiveness and
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2063</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2063</id><created>2012-11-09</created><authors><author><keyname>Seshadri</keyname><forenames>Padmanabha Venkatagiri</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Chan</keyname><forenames>Mun Choon</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>Mobile-to-Mobile Video Recommendation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile device users can now easily capture and socially share video clips in
a timely manner by uploading them wirelessly to a server. When attending
crowded events, such as an exhibition or the Olympic Games, however, timely
sharing of videos becomes difficult due to choking bandwidth in the network
infrastructure, preventing like-minded attendees from easily sharing videos
with each other through a server. One solution to alleviate this problem is to
use direct device-to-device communication to share videos among nearby
attendees. Contact capacity between two devices, however, is limited, and thus
a recommendation algorithm, such as collaborative filtering, is needed to
select and transmit only videos of potential interest to an attendee. In this
paper, we address the question: which video clip should be transmitted to which
user. We proposed an video transmission scheduling algorithm, called CoFiGel,
that runs in a distributed manner and aims to improve both the prediction
coverage and precision of the collaborative filtering algorithm. At each
device, CoFiGel transmits the video that would increase the estimated number of
positive user-video ratings the most if this video is transferred to the
destination device. We evaluated CoFiGel using real-world traces and show that
substantial improvement can be achieved compared to baseline schemes that do
not consider rating or contact history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2064</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2064</id><created>2012-11-09</created><updated>2013-02-25</updated><authors><author><keyname>Chen</keyname><forenames>Shiyao</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author></authors><title>Distributed Learning and Multiaccess of On-Off Channels</title><categories>math.OC cs.IT math.IT</categories><comments>8 pages, 5 figures</comments><doi>10.1109/JSTSP.2013.2261278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed access of a set of N on-off channels by K&lt;N users
is considered. The channels are slotted and modeled as independent but not
necessarily identical alternating renewal processes. Each user decides to
either observe or transmit at the beginning of every slot. A transmission is
successful only if the channel is at the on state and there is only one user
transmitting. When a user observes, it identifies whether a transmission would
have been successful had it decided to transmit. A distributed learning and
access policy referred to as alternating sensing and access (ASA) is proposed.
It is shown that ASA has finite expected regret when compared with the optimal
centralized scheme with fixed channel allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2065</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2065</id><created>2012-11-09</created><authors><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Zhao</keyname><forenames>Qun</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoli</forenames></author><author><keyname>Cheng</keyname><forenames>Xiang</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Efficiency Resource Allocation for Device-to-Device Underlay
  Communication Systems: A Reverse Iterative Combinatorial Auction Based
  Approach</title><categories>cs.GT cs.NI</categories><comments>26 pages, 6 fgures; IEEE Journals on Selected Areas in
  Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer communication has been recently considered as a popular issue
for local area services. An innovative resource allocation scheme is proposed
to improve the performance of mobile peer-to-peer, i.e., device-to-device
(D2D), communications as an underlay in the downlink (DL) cellular networks. To
optimize the system sum rate over the resource sharing of both D2D and cellular
modes, we introduce a reverse iterative combinatorial auction as the allocation
mechanism. In the auction, all the spectrum resources are considered as a set
of resource units, which as bidders compete to obtain business while the
packages of the D2D pairs are auctioned off as goods in each auction round. We
first formulate the valuation of each resource unit, as a basis of the proposed
auction. And then a detailed non-monotonic descending price auction algorithm
is explained depending on the utility function that accounts for the channel
gain from D2D and the costs for the system. Further, we prove that the proposed
auction-based scheme is cheat-proof, and converges in a finite number of
iteration rounds. We explain non-monotonicity in the price update process and
show lower complexity compared to a traditional combinatorial allocation. The
simulation results demonstrate that the algorithm efficiently leads to a good
performance on the system sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2066</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2066</id><created>2012-11-09</created><authors><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>I/O-optimal algorithms on grid graphs</title><categories>cs.DS</categories><comments>12 pages' extended abstract plus 12 pages' appendix with details,
  proofs and calculations. Has not been published in and is currently not under
  review of any conference or journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph of which the n vertices form a regular two-dimensional grid,
and in which each (possibly weighted and/or directed) edge connects a vertex to
one of its eight neighbours, the following can be done in O(scan(n)) I/Os,
provided M = Omega(B^2): computation of shortest paths with non-negative edge
weights from a single source, breadth-first traversal, computation of a minimum
spanning tree, topological sorting, time-forward processing (if the input is a
plane graph), and an Euler tour (if the input graph is a tree). The
minimum-spanning tree algorithm is cache-oblivious. The best previously
published algorithms for these problems need Theta(sort(n)) I/Os. Estimates of
the actual I/O volume show that the new algorithms may often be very efficient
in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2073</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2073</id><created>2012-11-09</created><authors><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Mengying</forenames></author><author><keyname>Zhu</keyname><forenames>Kenny Q.</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author></authors><title>LAGE: A Java Framework to reconstruct Gene Regulatory Networks from
  Large-Scale Continues Expression Data</title><categories>cs.LG cs.CE q-bio.QM stat.ML</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LAGE is a systematic framework developed in Java. The motivation of LAGE is
to provide a scalable and parallel solution to reconstruct Gene Regulatory
Networks (GRNs) from continuous gene expression data for very large amount of
genes. The basic idea of our framework is motivated by the philosophy of
divideand-conquer. Specifically, LAGE recursively partitions genes into
multiple overlapping communities with much smaller sizes, learns
intra-community GRNs respectively before merge them altogether. Besides, the
complete information of overlapping communities serves as the byproduct, which
could be used to mine meaningful functional modules in biological networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2075</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2075</id><created>2012-11-09</created><authors><author><keyname>Leukkunen</keyname><forenames>L.</forenames></author><author><keyname>Verho</keyname><forenames>T.</forenames></author><author><keyname>Lopez-Acevedo</keyname><forenames>O.</forenames></author></authors><title>A multi-scale code for flexible hybrid simulations</title><categories>physics.comp-ph cs.MS</categories><doi>10.1109/MCSE.2013.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-scale computer simulations combine the computationally efficient
classical algorithms with more expensive but also more accurate ab-initio
quantum mechanical algorithms. This work describes one implementation of
multi-scale computations using the Atomistic Simulation Environment (ASE). This
implementation can mix classical codes like LAMMPS and the Density Functional
Theory-based GPAW. Any combination of codes linked via the ASE interface
however can be mixed. We also introduce a framework to easily add classical
force fields calculators for ASE using LAMMPS, which also allows harnessing the
full performance of classical-only molecular dynamics. Our work makes it
possible to combine different simulation codes, quantum mechanical or
classical, with great ease and minimal coding effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2081</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2081</id><created>2012-11-09</created><authors><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Dynamic Popular Content Distribution in Vehicular Networks using
  Coalition Formation Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by both safety concerns and commercial interests, vehicular ad hoc
networks (VANETs) have recently received considerable attentions. In this
paper, we address popular content distribution (PCD) in VANETs, in which one
large popular file is downloaded from a stationary roadside unit (RSU), by a
group of on-board units (OBUs) driving through an area of interest (AoI) along
a highway. Due to high speeds of vehicles and deep fadings of
vehicle-to-roadside (V2R) channels, some of the vehicles may not finish
downloading the entire file but only possess several pieces of it. To
successfully send a full copy to each OBU, we propose a cooperative approach
based on the coalition formation games, in which OBUs exchange their possessed
pieces by broadcasting to and receiving from their neighbors. Simulation
results show that our proposed approach presents a considerable performance
improvement relative to the non-cooperative approach, in which the OBUs
broadcast randomly selected pieces to their neighbors as along as the spectrum
is detected to be unoccupied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2082</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2082</id><created>2012-11-09</created><authors><author><keyname>Prabhakar</keyname><forenames>C. J.</forenames></author><author><keyname>Kumar</keyname><forenames>P. U. Praveen</forenames></author></authors><title>3D Surface Reconstruction of Underwater Objects</title><categories>cs.CV</categories><comments>International Journal of Computer Applications (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel technique to reconstruct 3D surface of an
underwater object using stereo images. Reconstructing the 3D surface of an
underwater object is really a challenging task due to degraded quality of
underwater images. There are various reason of quality degradation of
underwater images i.e., non-uniform illumination of light on the surface of
objects, scattering and absorption effects. Floating particles present in
underwater produces Gaussian noise on the captured underwater images which
degrades the quality of images. The degraded underwater images are preprocessed
by applying homomorphic, wavelet denoising and anisotropic filtering
sequentially. The uncalibrated rectification technique is applied to
preprocessed images to rectify the left and right images. The rectified left
and right image lies on a common plane. To find the correspondence points in a
left and right images, we have applied dense stereo matching technique i.e.,
graph cut method. Finally, we estimate the depth of images using triangulation
technique. The experimental result shows that the proposed method reconstruct
3D surface of underwater objects accurately using captured underwater stereo
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2087</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2087</id><created>2012-11-09</created><authors><author><keyname>Sarkar</keyname><forenames>Arindam</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Secured Wireless Communication using Fuzzy Logic based High Speed
  Public-Key Cryptography (FLHSPKC)</title><categories>cs.CR cs.AI</categories><comments>9 pages</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 10, 2012, 137-145</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper secured wireless communication using fuzzy logic based high
speed public key cryptography (FLHSPKC) has been proposed by satisfying the
major issues likes computational safety, power management and restricted usage
of memory in wireless communication. Wireless Sensor Network (WSN) has several
major constraints likes inadequate source of energy, restricted computational
potentiality and limited memory. Though conventional Elliptic Curve
Cryptography (ECC) which is a sort of public key cryptography used in wireless
communication provides equivalent level of security like other existing public
key algorithm using smaller parameters than other but this traditional ECC does
not take care of all these major limitations in WSN. In conventional ECC
consider Elliptic curve point p, an arbitrary integer k and modulus m, ECC
carry out scalar multiplication kP mod m, which takes about 80% of key
computation time on WSN. In this paper proposed FLHSPKC scheme provides some
novel strategy including novel soft computing based strategy to speed up scalar
multiplication in conventional ECC and which in turn takes shorter
computational time and also satisfies power consumption restraint, limited
usage of memory without hampering the security level. Performance analysis of
the different strategies under FLHSPKC scheme and comparison study with
existing conventional ECC methods has been done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2090</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2090</id><created>2012-11-09</created><updated>2013-03-22</updated><authors><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Feldmann</keyname><forenames>Andreas Emil</forenames></author><author><keyname>Klimm</keyname><forenames>Max</forenames></author><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author></authors><title>Improving the H_k-Bound on the Price of Stability in Undirected Shapley
  Network Design Games</title><categories>cs.GT</categories><comments>14 pages</comments><msc-class>91A10, 91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the price of stability of Shapley network design
games on undirected graphs with k players is at most (k^3(k+1)/2-k^2) /
(1+k^3(k+1)/2-k^2) H_k = (1 - \Theta(1/k^4)) H_k, where H_k denotes the k-th
harmonic number. This improves on the known upper bound of H_k, which is also
valid for directed graphs but for these, in contrast, is tight. Hence, we give
the first non-trivial upper bound on the price of stability for undirected
Shapley network design games that is valid for an arbitrary number of players.
Our bound is proved by analyzing the price of stability restricted to Nash
equilibria that minimize the potential function of the game. We also present a
game with k=3 players in which such a restricted price of stability is 1.634.
This shows that the analysis of Bil\`o and Bove (Journal of Interconnection
Networks, Volume 12, 2011) is tight. In addition, we give an example for three
players that improves the lower bound on the (unrestricted) price of stability
to 1.571.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2116</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2116</id><created>2012-11-09</created><authors><author><keyname>Arunkumar</keyname><forenames>S</forenames></author><author><keyname>Sahu</keyname><forenames>Pallab Kumar</forenames></author><author><keyname>Gorai</keyname><forenames>Sudeep</forenames></author><author><keyname>Ghosh</keyname><forenames>Kalyan</forenames></author></authors><title>Localisation of Numerical Date Field in an Indian Handwritten Document</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method to localise all those areas which may
constitute the date field in an Indian handwritten document. Spatial patterns
of the date field are studied from various handwritten documents and an
algorithm is developed through statistical analysis to identify those sets of
connected components which may constitute the date. Common date patterns
followed in India are considered to classify the date formats in different
classes. Reported results demonstrate promising performance of the proposed
approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2118</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2118</id><created>2012-11-09</created><authors><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Trees in simple Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every simple polygon contains a degree 3 tree encompassing a
prescribed set of vertices. We give tight bounds on the minimal number of
degree 3 vertices. We apply this result to reprove a result from Bose et al.
that every set of disjoint line segments in the plane admits a binary tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2126</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2126</id><created>2012-11-09</created><authors><author><keyname>Ltifi</keyname><forenames>Hela</forenames></author><author><keyname>Trabelsi</keyname><forenames>Ghada</forenames></author><author><keyname>Ayed</keyname><forenames>Mounir Ben</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>Dynamic Decision Support System Based on Bayesian Networks Application
  to fight against the Nosocomial Infections</title><categories>cs.AI cs.DB</categories><comments>8 pages, 6 figures, 43 references</comments><acm-class>H.2.8</acm-class><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence (IJARAI), vol 1(1), pp. 22-29, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The improvement of medical care quality is a significant interest for the
future years. The fight against nosocomial infections (NI) in the intensive
care units (ICU) is a good example. We will focus on a set of observations
which reflect the dynamic aspect of the decision, result of the application of
a Medical Decision Support System (MDSS). This system has to make dynamic
decision on temporal data. We use dynamic Bayesian network (DBN) to model this
dynamic process. It is a temporal reasoning within a real-time environment; we
are interested in the Dynamic Decision Support Systems in healthcare domain
(MDDSS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2132</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2132</id><created>2012-11-09</created><authors><author><keyname>Ghadimi</keyname><forenames>Euhanna</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Accelerated Gradient Methods for Networked Optimization</title><categories>math.OC cs.DC cs.SY</categories><doi>10.1109/TSP.2013.2278149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop multi-step gradient methods for network-constrained optimization
of strongly convex functions with Lipschitz-continuous gradients. Given the
topology of the underlying network and bounds on the Hessian of the objective
function, we determine the algorithm parameters that guarantee the fastest
convergence and characterize situations when significant speed-ups can be
obtained over the standard gradient method. Furthermore, we quantify how the
performance of the gradient method and its accelerated counterpart are affected
by uncertainty in the problem data, and conclude that in most cases our
proposed method outperforms gradient descent. Finally, we apply the proposed
technique to three engineering problems: resource allocation under network-wide
budget constraints, distributed averaging, and Internet congestion control. In
all cases, we demonstrate that our algorithm converges more rapidly than
alternative algorithms reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2150</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2150</id><created>2012-11-09</created><authors><author><keyname>Halima</keyname><forenames>Mohamed Ben</forenames></author><author><keyname>karray</keyname><forenames>Hichem</forenames></author><author><keyname>Alimi</keyname><forenames>Adel. M.</forenames></author><author><keyname>Vila</keyname><forenames>Ana Fern&#xe1;ndez</forenames></author></authors><title>NF-SAVO: Neuro-Fuzzy system for Arabic Video OCR</title><categories>cs.CV</categories><comments>09 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 10, 2012, 128-136</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a robust approach for text extraction and
recognition from video clips which is called Neuro-Fuzzy system for Arabic
Video OCR. In Arabic video text recognition, a number of noise components
provide the text relatively more complicated to separate from the background.
Further, the characters can be moving or presented in a diversity of colors,
sizes and fonts that are not uniform. Added to this, is the fact that the
background is usually moving making text extraction a more intricate process.
Video include two kinds of text, scene text and artificial text. Scene text is
usually text that becomes part of the scene itself as it is recorded at the
time of filming the scene. But artificial text is produced separately and away
from the scene and is laid over it at a later stage or during the post
processing time. The emergence of artificial text is consequently vigilantly
directed. This type of text carries with it important information that helps in
video referencing, indexing and retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2151</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2151</id><created>2012-11-09</created><authors><author><keyname>Dutle</keyname><forenames>Aaron</forenames></author><author><keyname>Kay</keyname><forenames>Bill</forenames></author></authors><title>Graph Odometry</title><categories>math.CO cs.DM</categories><comments>14 pages, 5 figures</comments><msc-class>05C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address problem of determining edge weights on a graph using
non-backtracking closed walks from a vertex. We show that the weights of all of
the edges can be determined from any starting vertex exactly when the graph has
minimum degree at least three. We also determine the minimum number of walks
required to reveal all edge weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2155</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2155</id><created>2012-11-09</created><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Improved Modeling of the Correlation Between Continuous-Valued Sources
  in LDPC-Based DSC</title><categories>cs.IT math.IT</categories><comments>5 Pages, 4 figures; presented at the Asilomar Conference on Signals,
  Systems, and Computers, Pacific Grove, CA, November 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate modeling of the correlation between the sources plays a crucial role
in the efficiency of distributed source coding (DSC) systems. This correlation
is commonly modeled in the binary domain by using a single binary symmetric
channel (BSC), both for binary and continuous-valued sources. We show that
&quot;one&quot; BSC cannot accurately capture the correlation between continuous-valued
sources; a more accurate model requires &quot;multiple&quot; BSCs, as many as the number
of bits used to represent each sample. We incorporate this new model into the
DSC system that uses low-density parity-check (LDPC) codes for compression. The
standard Slepian-Wolf LDPC decoder requires a slight modification so that the
parameters of all BSCs are integrated in the log-likelihood ratios (LLRs).
Further, using an interleaver the data belonging to different bit-planes are
shuffled to introduce randomness in the binary domain. The new system has the
same complexity and delay as the standard one. Simulation results prove the
effectiveness of the proposed model and system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2162</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2162</id><created>2012-11-09</created><updated>2013-12-14</updated><authors><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>A Distributed Differential Space-Time Coding Scheme With Analog Network
  Coding in Two-Way Relay Networks</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures, IEEE Transactions on Signal Processing</comments><journal-ref>IEEE Transactions on Signal Processing, Vol. 60, No. 9, pp. 4998 -
  5004, 2012</journal-ref><doi>10.1109/TSP.2012.2202654</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider general two-way relay networks (TWRNs) with two
source and N relay nodes. A distributed differential space time coding with
analog network coding (DDSTC-ANC) scheme is proposed. A simple blind estimation
and a differential signal detector are developed to recover the desired signal
at each source. The pairwise error probability (PEP) and block error rate
(BLER) of the DDSTC-ANC scheme are analyzed. Exact and simplified PEP
expressions are derived. To improve the system performance, the optimum power
allocation (OPA) between the source and relay nodes is determined based on the
simplified PEP expression. The analytical results are verified through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2169</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2169</id><created>2012-11-09</created><updated>2014-07-11</updated><authors><author><keyname>Cseh</keyname><forenames>Agnes</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>Paths to stable allocations</title><categories>cs.DM</categories><comments>28 pages, it is the full version of the paper with the same title
  appeared at SAGT14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable allocation problem is one of the broadest extensions of the
well-known stable marriage problem. In an allocation problem, edges of a
bipartite graph have capacities and vertices have quotas to fill. Here we
investigate the case of uncoordinated processes in stable allocation instances.
In this setting, a feasible allocation is given and the aim is to reach a
stable allocation by raising the value of the allocation along blocking edges
and reducing it on worse edges if needed. Do such myopic changes lead to a
stable solution?
  In our present work, we analyze both better and best response dynamics from
an algorithmic point of view. With the help of two deterministic algorithms we
show that random procedures reach a stable solution with probability one for
all rational input data in both cases. Surprisingly, while there is a
polynomial path to stability when better response strategies are played (even
for irrational input data), the more intuitive best response steps may require
exponential time. We also study the special case of correlated markets. There,
random best response strategies lead to a stable allocation in expected
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2177</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2177</id><created>2012-11-09</created><updated>2012-11-12</updated><authors><author><keyname>Kappmeier</keyname><forenames>Jan-Philipp W.</forenames></author><author><keyname>Matuschke</keyname><forenames>Jannik</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author></authors><title>Abstract flows over time: A first step towards solving dynamic packing
  problems</title><categories>cs.DM math.CO</categories><comments>ISAAC 2012 001-2012</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flows over time generalize classical network flows by introducing a notion of
time. Each arc is equipped with a transit time that specifies how long flow
takes to traverse it, while flow rates may vary over time within the given edge
capacities. In this paper, we extend this concept of a dynamic optimization
problem to the more general setting of abstract flows. In this model, the
underlying network is replaced by an abstract system of linearly ordered sets,
called &quot;paths&quot; satisfying a simple switching property: Whenever two paths P and
Q intersect, there must be another path that is contained in the beginning of P
and the end of Q.
  We show that a maximum abstract flow over time can be obtained by solving a
weighted abstract flow problem and constructing a temporally repeated flow from
its solution. In the course of the proof, we also show that the relatively
modest switching property of abstract networks already captures many essential
properties of classical networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2187</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2187</id><created>2012-11-09</created><authors><author><keyname>Eslami</keyname><forenames>Ali</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>H.</forenames></author></authors><title>On Finite-Length Performance of Polar Codes: Stopping Sets, Error Floor,
  and Concatenated Design</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates properties of polar codes that can be potentially
useful in real-world applications. We start with analyzing the performance of
finite-length polar codes over the binary erasure channel (BEC), while assuming
belief propagation as the decoding method. We provide a stopping set analysis
for the factor graph of polar codes, where we find the size of the minimum
stopping set. We also find the girth of the graph for polar codes. Our analysis
along with bit error rate (BER) simulations demonstrate that finite-length
polar codes show superior error floor performance compared to the conventional
capacity-approaching coding techniques. In order to take advantage from this
property while avoiding the shortcomings of polar codes, we consider the idea
of combining polar codes with other coding schemes. We propose a polar
code-based concatenated scheme to be used in Optical Transport Networks (OTNs)
as a potential real-world application. Comparing against conventional
concatenation techniques for OTNs, we show that the proposed scheme outperforms
the existing methods by closing the gap to the capacity while avoiding error
floor, and maintaining a low complexity at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2189</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2189</id><created>2012-11-09</created><authors><author><keyname>Matuschke</keyname><forenames>Jannik</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author></authors><title>Lattices and maximum flow algorithms in planar graphs</title><categories>cs.DM math.CO</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the left/right relation on the set of s-t-paths of a plane graph
induces a so-called submodular lattice. If the embedding of the graph is
s-t-planar, this lattice is even consecutive. This implies that Ford and
Fulkerson's uppermost path algorithm for maximum flow in such graphs is indeed
a special case of a two-phase greedy algorithm on lattice polyhedra. We also
show that the properties submodularity and consecutivity cannot be achieved
simultaneously by any partial order on the paths if the graph is planar but not
s-t-planar, thus providing a characterization of this class of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2190</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2190</id><created>2012-11-09</created><updated>2013-09-07</updated><authors><author><keyname>Read</keyname><forenames>Jesse</forenames></author><author><keyname>Martino</keyname><forenames>Luca</forenames></author><author><keyname>Luengo</keyname><forenames>David</forenames></author></authors><title>Efficient Monte Carlo Methods for Multi-Dimensional Learning with
  Classifier Chains</title><categories>cs.LG stat.CO stat.ML</categories><comments>Submitted to Pattern Recognition</comments><journal-ref>Pattern Recognition, Volume 47, Issue 3, Pages: 1535-1546, 2014</journal-ref><doi>10.1016/j.patcog.2013.10.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-dimensional classification (MDC) is the supervised learning problem
where an instance is associated with multiple classes, rather than with a
single class, as in traditional classification problems. Since these classes
are often strongly correlated, modeling the dependencies between them allows
MDC methods to improve their performance - at the expense of an increased
computational cost. In this paper we focus on the classifier chains (CC)
approach for modeling dependencies, one of the most popular and highest-
performing methods for multi-label classification (MLC), a particular case of
MDC which involves only binary classes (i.e., labels). The original CC
algorithm makes a greedy approximation, and is fast but tends to propagate
errors along the chain. Here we present novel Monte Carlo schemes, both for
finding a good chain sequence and performing efficient inference. Our
algorithms remain tractable for high-dimensional data sets and obtain the best
predictive performance across several real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2194</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2194</id><created>2012-11-09</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Mishra</keyname><forenames>Akhilesh</forenames></author></authors><title>A Novel Anticlustering Filtering Algorithm for the Prediction of Genes
  as a Drug Target</title><categories>cs.CE q-bio.GN</categories><comments>6 pages, 2 figures and 1 table</comments><journal-ref>American Journal of Biomedical Engineering 2012, 2(5):206-211</journal-ref><doi>10.5923/j.ajbe.20120205.03</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high-throughput data generated by microarray experiments provides
complete set of genes being expressed in a given cell or in an organism under
particular conditions. The analysis of these enormous data has opened a new
dimension for the researchers. In this paper we describe a novel algorithm to
microarray data analysis focusing on the identification of genes that are
differentially expressed in particular internal or external conditions and
which could be potential drug targets. The algorithm uses the time-series gene
expression data as an input and recognizes genes which are expressed
differentially. This algorithm implements standard statistics-based gene
functional investigations, such as the log transformation, mean, log-sigmoid
function, coefficient of variations, etc. It does not use clustering analysis.
The proposed algorithm has been implemented in Perl. The time-series gene
expression data on yeast Saccharomyces cerevisiae from the Stanford Microarray
Database (SMD)consisting of 6154 genes have been taken for the validation of
the algorithm. The developed method extracted 48 genes out of total 6154 genes.
These genes are mostly responsible for the yeast's resistants at a high
temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2197</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2197</id><created>2012-11-09</created><updated>2012-12-12</updated><authors><author><keyname>Li</keyname><forenames>Daifeng</forenames></author><author><keyname>Zhang</keyname><forenames>Jingwei</forenames></author><author><keyname>Sun</keyname><forenames>Gordon Guo-zheng</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Luo</keyname><forenames>Zhipeng</forenames></author></authors><title>What is the Nature of Chinese MicroBlogging: Unveiling the Unique
  Features of Tencent Weibo</title><categories>cs.SI physics.soc-ph</categories><comments>WWW2013(submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  China has the largest number of online users in the world and about 20%
internet users are from China. This is a huge, as well as a mysterious, market
for IT industry due to various reasons such as culture difference. Twitter is
the largest microblogging service in the world and Tencent Weibo is one of the
largest microblogging services in China. Employ the two data sets as a source
in our study, we try to unveil the unique behaviors of Chinese users. We have
collected the entire Tencent Weibo from 10th, Oct, 2011 to 5th, Jan, 2012 and
obtained 320 million user profiles, 5.15 billion user actions. We study Tencent
Weibo from both macro and micro levels. From the macro level, Tencent users are
more active on forwarding messages, but with less reciprocal relationships than
Twitter users, their topic preferences are very different from Twitter users
from both content and time consuming; besides, information can be diffused more
efficient in Tencent Weibo. From the micro level, we mainly evaluate users'
social influence from two indexes: &quot;Forward&quot; and \Follower&quot;, we study how
users' actions will contribute to their social influences, and further identify
unique features of Tencent users. According to our studies, Tencent users'
actions are more personalized and diversity, and the influential users play a
more important part in the whole networks. Based on the above analysis, we
design a graphical model for predicting users' forwarding behaviors. Our
experimental results on the large Tencent Weibo data validate the correctness
of the discoveries and the effectiveness of the proposed model. To the best of
our knowledge, this work is the first quantitative study on the entire
Tencentsphere and information diffusion on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2198</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2198</id><created>2012-11-09</created><authors><author><keyname>Eslami</keyname><forenames>Ali</forenames></author><author><keyname>Nekoui</keyname><forenames>Mohammad</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author><author><keyname>Fekri</keyname><forenames>F.</forenames></author></authors><title>Results on Finite Wireless Sensor Networks: Connectivity and Coverage</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many analytic results for the connectivity, coverage, and capacity of
wireless networks have been reported for the case where the number of nodes,
$n$, tends to infinity (large-scale networks). The majority of these results
have not been extended for small or moderate values of $n$; whereas in many
practical networks, $n$ is not very large. In this paper, we consider finite
(small-scale) wireless sensor networks. We first show that previous asymptotic
results provide poor approximations for such networks. We provide a set of
differences between small-scale and large-scale analysis and propose a
methodology for analysis of finite sensor networks. Furthermore, we consider
two models for such networks: unreliable sensor grids, and sensor networks with
random node deployment. We provide easily computable expressions for bounds on
the coverage and connectivity of these networks. With validation from
simulations, we show that the derived analytic expressions give very good
estimates of such quantities for finite sensor networks. Our investigation
confirms the fact that small-scale networks possesses unique characteristics
different from the large-scale counterparts, necessitating the development of a
new framework for their analysis and design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2227</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2227</id><created>2012-11-09</created><updated>2013-06-05</updated><authors><author><keyname>Anderson</keyname><forenames>Joseph</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author></authors><title>Efficient learning of simplices</title><categories>cs.LG cs.DS stat.ML</categories><comments>New author added to this version, Joseph Anderson. New results:
  reductions from learning a simplex and a linearly transformed l_p ball to ICA
  (sections 7 and 8)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an efficient algorithm for the following problem: Given uniformly
random points from an arbitrary n-dimensional simplex, estimate the simplex.
The size of the sample and the number of arithmetic operations of our algorithm
are polynomial in n. This answers a question of Frieze, Jerrum and Kannan
[FJK]. Our result can also be interpreted as efficiently learning the
intersection of n+1 half-spaces in R^n in the model where the intersection is
bounded and we are given polynomially many uniform samples from it. Our proof
uses the local search technique from Independent Component Analysis (ICA), also
used by [FJK]. Unlike these previous algorithms, which were based on analyzing
the fourth moment, ours is based on the third moment.
  We also show a direct connection between the problem of learning a simplex
and ICA: a simple randomized reduction to ICA from the problem of learning a
simplex. The connection is based on a known representation of the uniform
measure on a simplex. Similar representations lead to a reduction from the
problem of learning an affine transformation of an n-dimensional l_p ball to
ICA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2245</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2245</id><created>2012-11-09</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Composite Strategy for Multicriteria Ranking/Sorting (methodological
  issues, examples)</title><categories>math.OC cs.AI cs.SE</categories><comments>24 pages, 28 figures, 5 tables</comments><msc-class>68T20, 90C27, 90C59</msc-class><acm-class>D.2.2; D.2.10; D.2.11; G.1.6; G.2.1; F.2.2; H.1.1; H.4.2; I.2.8; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the modular design of composite solving strategies for
multicriteria ranking (sorting). Here a 'scale of creativity' that is close to
creative levels proposed by Altshuller is used as the reference viewpoint: (i)
a basic object, (ii) a selected object, (iii) a modified object, and (iv) a
designed object (e.g., composition of object components). These levels maybe
used in various parts of decision support systems (DSS) (e.g., information,
operations, user). The paper focuses on the more creative above-mentioned level
(i.e., composition or combinatorial synthesis) for the operational part (i.e.,
composite solving strategy). This is important for a search/exploration mode of
decision making process with usage of various procedures and techniques and
analysis/integration of obtained results. The paper describes methodological
issues of decision technology and synthesis of composite strategy for
multicriteria ranking. The synthesis of composite strategies is based on
'hierarchical morphological multicriteria design' (HMMD) which is based on
selection and combination of design alternatives (DAs) (here: local procedures
or techniques) while taking into account their quality and quality of their
interconnections (IC). A new version of HMMD with interval multiset estimates
for DAs is used. The operational environment of DSS COMBI for multicriteria
ranking, consisting of a morphology of local procedures or techniques (as
design alternatives DAs), is examined as a basic one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2251</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2251</id><created>2012-11-09</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author></authors><title>Independent subsets of powers of paths, and Fibonacci cubes</title><categories>cs.DM math.CO</categories><comments>Preprint submitted to Electronic Notes in Discrete Mathematics</comments><msc-class>68R05, 11B39</msc-class><journal-ref>Electronic Notes in Discrete Mathematics 40 (2013) 65-69</journal-ref><doi>10.1016/j.endm.2013.05.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a formula for the number of edges of the Hasse diagram of the
independent subsets of the h-th power of a path ordered by inclusion. For h=1
such a value is the number of edges of a Fibonacci cube. We show that, in
general, the number of edges of the diagram is obtained by convolution of a
Fibonacci-like sequence with itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2259</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2259</id><created>2012-11-09</created><authors><author><keyname>Ishikawa</keyname><forenames>Fuyuki</forenames></author><author><keyname>Romanovsky</keyname><forenames>Alexander</forenames></author></authors><title>Proceedings: Workshop on the experience of and advances in developing
  dependable systems in Event-B (DS-Event-B 2012)</title><categories>cs.SE</categories><comments>Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These proceedings include papers presented at the Workshop on &quot;The experience
of and advances in developing dependable systems in Event-B&quot; held on November
13, 2012 as part of the ICFEM 2012 (Kyoto, Japan).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2260</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2260</id><created>2012-11-09</created><authors><author><keyname>Streeter</keyname><forenames>Matthew</forenames></author><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author></authors><title>No-Regret Algorithms for Unconstrained Online Convex Optimization</title><categories>cs.LG</categories><comments>To appear</comments><journal-ref>NIPS 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some of the most compelling applications of online convex optimization,
including online prediction and classification, are unconstrained: the natural
feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in
this setting unless constraints on the comparator point x^* are known in
advance. We present algorithms that, without such prior knowledge, offer
near-optimal regret bounds with respect to any choice of x^*. In particular,
regret with respect to x^* = 0 is constant. We then prove lower bounds showing
that our guarantees are near-optimal in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2265</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2265</id><created>2012-11-09</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Optimal Detection For Sparse Mixtures</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of sparse signals arises in a wide range of modern scientific
studies. The focus so far has been mainly on Gaussian mixture models. In this
paper, we consider the detection problem under a general sparse mixture model
and obtain an explicit expression for the detection boundary. It is shown that
the fundamental limits of detection is governed by the behavior of the
log-likelihood ratio evaluated at an appropriate quantile of the null
distribution. We also establish the adaptive optimality of the higher criticism
procedure across all sparse mixtures satisfying certain mild regularity
conditions. In particular, the general results obtained in this paper recover
and extend in a unified manner the previously known results on sparse detection
far beyond the conventional Gaussian model and other exponential families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2268</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2268</id><created>2012-11-09</created><authors><author><keyname>Cheung</keyname><forenames>Yun Kuen</forenames></author><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Rastogi</keyname><forenames>Ashish</forenames></author></authors><title>Tatonnement in Ongoing Markets of Complementary Goods</title><categories>cs.GT</categories><comments>44 pages, ACM EC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues the study, initiated by Cole and Fleischer, of the
behavior of a tatonnement price update rule in Ongoing Fisher Markets. The
prior work showed fast convergence toward an equilibrium when the goods
satisfied the weak gross substitutes property and had bounded demand and income
elasticities.
  The current work shows that fast convergence also occurs for the following
types of markets:
  - All pairs of goods are complements to each other, and - the demand and
income elasticities are suitably bounded.
  In particular, these conditions hold when all buyers in the market are
equipped with CES utilities, where all the parameters $\rho$, one per buyer,
satisfy $-1 &lt; \rho \le 0$.
  In addition, we extend the above result to markets in which a mixture of
complements and substitutes occur. This includes characterizing a class of
nested CES utilities for which fast convergence holds.
  An interesting technical contribution, which may be of independent interest,
is an amortized analysis for handling asynchronous events in settings in which
there are a mix of continuous changes and discrete events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2280</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2280</id><created>2012-11-09</created><authors><author><keyname>Venkatalakshmi</keyname><forenames>B.</forenames></author><author><keyname>Shanmugavel</keyname><forenames>S.</forenames></author></authors><title>A Novel Architecture For Network Coded Electronic Health Record Storage
  System</title><categories>cs.IT math.IT</categories><comments>22-pages Journal</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of network coding for large scale content distribution improves
download time. This is demonstrated in this work by the use of network coded
Electronic Health Record Storage System (EHR-SS). An architecture of 4-layer to
build the EHR-SS is designed. The application integrates the data captured for
the patient from three modules namely administrative data, medical records of
consultation and reports of medical tests. The lower layer is the data
capturing layer using RFID reader. The data is captured in the lower level from
different nodes. The data is combined with some linear coefficients using
linear network coding. At the lower level the data from different tags are
combined and stored and at the level 2 coding combines the data from multiple
readers and a corresponding encoding vector is generated. This network coding
is done at the server node through small mat lab net-cod interface software.
While accessing the stored data, the user data has the data type represented in
the form of decoding vector. For storing and retrieval the primary key is the
patient id. The results obtained were observed with a reduction of download
time of about 12% for our case study set up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2287</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2287</id><created>2012-11-09</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Designing Rating Systems to Promote Mutual Security for Interconnected
  Networks</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnected autonomous systems often share security risks. However, an
autonomous system lacks the incentive to make (sufficient) security investments
if the cost exceeds its own benefit even though doing that would be socially
beneficial. In this paper, we develop a systematic and rigorous framework for
analyzing and significantly improving the mutual security of a collection of
ASs that interact frequently over a long period of time. Using this framework,
we show that simple incentive schemes based on rating systems can be designed
to encourage the autonomous systems' security investments, thereby
significantly improving their mutual security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2290</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2290</id><created>2012-11-10</created><authors><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Baldridge</keyname><forenames>Jason</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>Dating Texts without Explicit Temporal Cues</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles temporal resolution of documents, such as determining when
a document is about or when it was written, based only on its text. We apply
techniques from information retrieval that predict dates via language models
over a discretized timeline. Unlike most previous works, we rely {\it solely}
on temporal cues implicit in the text. We consider both document-likelihood and
divergence based techniques and several smoothing methods for both of them. Our
best model predicts the mid-point of individuals' lives with a median of 22 and
mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present
day. We also show that this approach works well when training on such
biographies and predicting dates both for non-biographical Wikipedia pages
about specific years (500 B.C. to 2010 A.D.) and for publication dates of short
stories (1798 to 2008). Together, our work shows that, even in absence of
temporal extraction resources, it is possible to achieve remarkable temporal
locality across a diverse set of texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2291</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2291</id><created>2012-11-10</created><authors><author><keyname>Naghshvar</keyname><forenames>Mohammad</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Sequentiality and Adaptivity Gains in Active Hypothesis Testing</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>12 double-column pages, 1 figure</comments><msc-class>62F03, 62B10, 62B15</msc-class><acm-class>G.3</acm-class><doi>10.1109/JSTSP.2013.2261279</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a decision maker who is responsible to collect observations so as to
enhance his information in a speedy manner about an underlying phenomena of
interest. The policies under which the decision maker selects sensing actions
can be categorized based on the following two factors: i) sequential vs.
non-sequential; ii) adaptive vs. non-adaptive. Non-sequential policies collect
a fixed number of observation samples and make the final decision afterwards;
while under sequential policies, the sample size is not known initially and is
determined by the observation outcomes. Under adaptive policies, the decision
maker relies on the previous collected samples to select the next sensing
action; while under non-adaptive policies, the actions are selected independent
of the past observation outcomes.
  In this paper, performance bounds are provided for the policies in each
category. Using these bounds, sequentiality gain and adaptivity gain, i.e., the
gains of sequential and adaptive selection of actions are characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2292</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2292</id><created>2012-11-10</created><authors><author><keyname>Duy</keyname><forenames>Truong Vinh Truong</forenames></author><author><keyname>Yamazaki</keyname><forenames>Katsuhiro</forenames></author><author><keyname>Ikegami</keyname><forenames>Kosai</forenames></author><author><keyname>Oyanagi</keyname><forenames>Shigeru</forenames></author></authors><title>Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body
  Simulation</title><categories>cs.DC cs.CE cs.PF</categories><comments>8 pages, 9 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clusters of SMP nodes provide support for a wide diversity of parallel
programming paradigms. Combining both shared memory and message passing
parallelizations within the same application, the hybrid MPI-OpenMP paradigm is
an emerging trend for parallel programming to fully exploit distributed
shared-memory architecture. In this paper, we improve the performance of MPEG-2
encoder and n-body simulation by employing the hybrid MPI-OpenMP programming
paradigm on SMP clusters. The hierarchical image data structure of the MPEG
bit-stream is eminently suitable for the hybrid model to achieve multiple
levels of parallelism: MPI for parallelism at the group of pictures level
across SMP nodes and OpenMP for parallelism within pictures at the slice level
within each SMP node. Similarly, the work load of the force calculation which
accounts for upwards of 90% of the cycles in typical computations in the n-body
simulation is shared among OpenMP threads after ORB domain decomposition among
MPI processes. Besides, loop scheduling of OpenMP threads is adopted with
appropriate chunk size to provide better load balance of work, leading to
enhanced performance. With the n-body simulation, experimental results
demonstrate that the hybrid MPI-OpenMP program outperforms the corresponding
pure MPI program by average factors of 1.52 on a 4-way cluster and 1.21 on a
2-way cluster. Likewise, the hybrid model offers a performance improvement of
18% compared to the MPI model for the MPEG-2 encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1211.2293</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1211.2293</id><created>2012-11-10</created><authors><author><keyname>Duy</keyname><forenames>Truong Vinh Truong</forenames></author><author><keyname>Yamazaki</keyname><forenames>Katsuhiro</forenames></author><author><keyname>Oyanagi</keyname><forenames>Shigeru</forenames></author></authors><title>Performance Evaluation of Treecode Algorithm for N-Body Simulation Using
  GridRPC System</title><categories>cs.DC cs.CE cs.PF</categories><comments>4 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed at improving the performance of the treecode algorithm
for N-Body simulation by employing the NetSolve GridRPC programming model to
exploit the use of multiple clusters. N-Body is a classical problem, and
appears in many areas of science and engineering, including astrophysics,
molecular dynamics, and graphics. In the simulation of N-Body, the specific
routine for calculating the forces on the bodies which accounts for upwards of
90% of the cycles in typical computations is eminently suitable for obtaining
parallelism with GridRPC calls. It is divided among the compute nodes by
simultaneously calling multiple GridRPC requests to them. The performance of
the GridRPC implementation is then compared to that of the MPI version and
hybrid MPI-OpenMP version for the treecode algorithm on individual clusters.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="37000" completeListSize="102538">1122234|38001</resumptionToken>
</ListRecords>
</OAI-PMH>
