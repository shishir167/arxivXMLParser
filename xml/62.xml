<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:31:47Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|61001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4890</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4890</id><created>2014-05-19</created><authors><author><keyname>Yue</keyname><forenames>Meng</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoyu</forenames></author></authors><title>A Revised Incremental Conductance MPPT Algorithm for Solar PV Generation
  Systems</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A revised Incremental Conductance (IncCond) maximum power point tracking
(MPPT) algorithm for PV generation systems is proposed in this paper. The
commonly adopted traditional IncCond method uses a constant step size for
voltage adjustment and is difficult to achieve both a good tracking performance
and quick elimination of the oscillations, especially under the dramatic
changes of the environment conditions. For the revised algorithm, the
incremental voltage change step size is adaptively adjusted based on the slope
of the power-voltage (P-V) curve. An accelerating factor and a decelerating
factor are further applied to adjust the voltage step change considering
whether the sign of the P-V curve slope remains the same or not in a subsequent
tracking step. In addition, the upper bound of the maximum voltage step change
is also updated considering the information of sign changes. The revised MPPT
algorithm can quickly track the maximum power points (MPPs) and remove the
oscillation of the actual operation points around the real MPPs. The
effectiveness of the revised algorithm is demonstrated using a simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4892</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4892</id><created>2014-05-19</created><updated>2014-07-11</updated><authors><author><keyname>Ghuman</keyname><forenames>Sukhpal Singh</forenames></author><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author><author><keyname>Tarhio</keyname><forenames>Jorma</forenames></author></authors><title>Alternative Algorithms for Lyndon Factorization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two variations of Duval's algorithm for computing the Lyndon
factorization of a word. The first algorithm is designed for the case of small
alphabets and is able to skip a significant portion of the characters of the
string, for strings containing runs of the smallest character in the alphabet.
Experimental results show that it is faster than Duval's original algorithm,
more than ten times in the case of long DNA strings. The second algorithm
computes, given a run-length encoded string $R$ of length $\rho$, the Lyndon
factorization of $R$ in $O(\rho)$ time and constant space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4894</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4894</id><created>2014-04-25</created><authors><author><keyname>Lellouch</keyname><forenames>Gabriel</forenames></author><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author></authors><title>Optimization of OFDM radar waveforms using genetic algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our investigations on the use of single objective
and multiobjective genetic algorithms based optimisation algorithms to improve
the design of OFDM pulses for radar. We discuss these optimization procedures
in the scope of a waveform design intended for two different radar processing
solutions. Lastly, we show how the encoding solution is suited to permit the
optimizations of waveform for OFDM radar related challenges such as enhanced
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4897</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4897</id><created>2014-05-19</created><authors><author><keyname>Xiang</keyname><forenames>Zhen James</forenames></author><author><keyname>Wang</keyname><forenames>Yun</forenames></author><author><keyname>Ramadge</keyname><forenames>Peter J.</forenames></author></authors><title>Screening Tests for Lasso Problems</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey of dictionary screening for the lasso problem. The
lasso problem seeks a sparse linear combination of the columns of a dictionary
to best match a given target vector. This sparse representation has proven
useful in a variety of subsequent processing and decision tasks. For a given
target vector, dictionary screening quickly identifies a subset of dictionary
columns that will receive zero weight in a solution of the corresponding lasso
problem. These columns can be removed from the dictionary, prior to solving the
lasso problem, without impacting the optimality of the solution obtained. This
has two potential advantages: it reduces the size of the dictionary, allowing
the lasso problem to be solved with less resources, and it may speed up
obtaining a solution. Using a geometrically intuitive framework, we provide
basic insights for understanding useful lasso screening tests and their
limitations. We also provide illustrative numerical studies on several
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4906</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4906</id><created>2014-05-19</created><authors><author><keyname>Matsumoto</keyname><forenames>Nobuyuki</forenames></author><author><keyname>Michimura</keyname><forenames>Yuta</forenames></author><author><keyname>Aso</keyname><forenames>Yoichi</forenames></author><author><keyname>Tsubono</keyname><forenames>Kimio</forenames></author></authors><title>An optically trapped mirror for reaching the standard quantum limit</title><categories>physics.optics cs.SY quant-ph</categories><comments>9 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1312.5031</comments><doi>10.1364/OE.22.012915</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preparation of a mechanical oscillator driven by quantum back-action is a
fundamental requirement to reach the standard quantum limit (SQL) for force
measurement, in optomechanical systems. However, thermal fluctuating force
generally dominates a disturbance on the oscillator. In the macroscopic scale,
an optical linear cavity including a suspended mirror has been used for the
weak force measurement, such as gravitational-wave detectors. This
configuration has the advantages of reducing the dissipation of the pendulum
(i.e., suspension thermal noise) due to a gravitational dilution by using a
thin wire, and of increasing the circulating laser power. However, the use of
the thin wire is weak for an optical torsional anti-spring effect in the
cavity, due to the low mechanical restoring force of the wire. Thus, there is
the trade-off between the stability of the system and the sensitivity. Here, we
describe using a triangular optical cavity to overcome this limitation for
reaching the SQL. The triangular cavity can provide a sensitive and stable
system, because it can optically trap the mirror's motion of the yaw, through
an optical positive torsional spring effect. To show this, we demonstrate a
measurement of the torsional spring effect caused by radiation pressure forces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4917</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4917</id><created>2014-05-19</created><updated>2014-06-02</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author></authors><title>An Algebraic Hardness Criterion for Surjective Constraint Satisfaction</title><categories>cs.LO cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constraint satisfaction problem (CSP) on a relational structure B is to
decide, given a set of constraints on variables where the relations come from
B, whether or not there is a assignment to the variables satisfying all of the
constraints; the surjective CSP is the variant where one decides the existence
of a surjective satisfying assignment onto the universe of B. We present an
algebraic condition on the polymorphism clone of B and prove that it is
sufficient for the hardness of the surjective CSP on a finite structure B, in
the sense that this problem admits a reduction from a certain fixed-structure
CSP. To our knowledge, this is the first result that allows one to use
algebraic information from a relational structure B to infer information on the
complexity hardness of surjective constraint satisfaction on B. A corollary of
our result is that, on any finite non-trivial structure having only essentially
unary polymorphisms, surjective constraint satisfaction is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4918</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4918</id><created>2014-05-19</created><authors><author><keyname>Almishari</keyname><forenames>Mishari</forenames></author><author><keyname>Oguz</keyname><forenames>Ekin</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Fighting Authorship Linkability with Crowdsourcing</title><categories>cs.DL cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive amounts of contributed content -- including traditional literature,
blogs, music, videos, reviews and tweets -- are available on the Internet
today, with authors numbering in many millions. Textual information, such as
product or service reviews, is an important and increasingly popular type of
content that is being used as a foundation of many trendy community-based
reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown
that, due partly to their specialized/topical nature, sets of reviews authored
by the same person are readily linkable based on simple stylometric features.
In practice, this means that individuals who author more than a few reviews
under different accounts (whether within one site or across multiple sites) can
be linked, which represents a significant loss of privacy.
  In this paper, we start by showing that the problem is actually worse than
previously believed. We then explore ways to mitigate authorship linkability in
community-based reviewing. We first attempt to harness the global power of
crowdsourcing by engaging random strangers into the process of re-writing
reviews. As our empirical results (obtained from Amazon Mechanical Turk)
clearly demonstrate, crowdsourcing yields impressively sensible reviews that
reflect sufficiently different stylometric characteristics such that prior
stylometric linkability techniques become largely ineffective. We also consider
using machine translation to automatically re-write reviews. Contrary to what
was previously believed, our results show that translation decreases authorship
linkability as the number of intermediate languages grows. Finally, we explore
the combination of crowdsourcing and machine translation and report on the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4921</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4921</id><created>2014-05-19</created><authors><author><keyname>Pressman</keyname><forenames>Annette</forenames></author><author><keyname>Hock</keyname><forenames>Kai</forenames></author></authors><title>Zgoubi: A startup guide for the complete beginner</title><categories>physics.acc-ph cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zgoubi is a code which can be used to model accelerators and beam lines,
comprised of magnetic and electrostatic elements. It has been extensively
developed since the mid-1980s to include circular accelerators and related beam
physics. It has been made freely available by its author on a code development
site, including a Users' Guide, a data treatment/graphic interfacing tool, and
many examples. This startup guide give directions to install the required
elements onto a Windows or Unix system to enable running of the Zgoubi code
with examples of code written to model the EMMA accelerator based at the
Cockcroft Institute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4925</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4925</id><created>2014-05-19</created><authors><author><keyname>Strzebonski</keyname><forenames>Adam</forenames></author></authors><title>Cylindrical Algebraic Decomposition Using Local Projections</title><categories>cs.SC</categories><acm-class>I.1.2; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which computes a cylindrical algebraic decomposition
of a semialgebraic set using projection sets computed for each cell separately.
Such local projection sets can be significantly smaller than the global
projection set used by the Cylindrical Algebraic Decomposition (CAD) algorithm.
This leads to reduction in the number of cells the algorithm needs to
construct. We give an empirical comparison of our algorithm and the classical
CAD algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4927</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4927</id><created>2014-05-19</created><authors><author><keyname>Freitas</keyname><forenames>Carlos A.</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Ghosh</keyname><forenames>Saptarshi</forenames></author><author><keyname>Veloso</keyname><forenames>Adriano</forenames></author></authors><title>Reverse Engineering Socialbot Infiltration Strategies in Twitter</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data extracted from social networks like Twitter are increasingly being used
to build applications and services that mine and summarize public reactions to
events, such as traffic monitoring platforms, identification of epidemic
outbreaks, and public perception about people and brands. However, such
services are vulnerable to attacks from socialbots $-$ automated accounts that
mimic real users $-$ seeking to tamper statistics by posting messages generated
automatically and interacting with legitimate users. Potentially, if created in
large scale, socialbots could be used to bias or even invalidate many existing
services, by infiltrating the social networks and acquiring trust of other
users with time. This study aims at understanding infiltration strategies of
socialbots in the Twitter microblogging platform. To this end, we create 120
socialbot accounts with different characteristics and strategies (e.g., gender
specified in the profile, how active they are, the method used to generate
their tweets, and the group of users they interact with), and investigate the
extent to which these bots are able to infiltrate the Twitter social network.
Our results show that even socialbots employing simple automated mechanisms are
able to successfully infiltrate the network. Additionally, using a $2^k$
factorial design, we quantify infiltration effectiveness of different bot
strategies. Our analysis unveils findings that are key for the design of
detection and counter measurements approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4930</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4930</id><created>2014-05-19</created><updated>2014-08-03</updated><authors><author><keyname>Dubey</keyname><forenames>Shiv Ram</forenames></author><author><keyname>Jalal</keyname><forenames>Anand Singh</forenames></author></authors><title>Adapted Approach for Fruit Disease Identification using Images</title><categories>cs.CV</categories><comments>15 pages, 8 figures, 1 table</comments><journal-ref>International Journal of Computer Vision and Image Processing
  (IJCVIP) 2, no. 3 (2012): 44-58</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diseases in fruit cause devastating problem in economic losses and production
in agricultural industry worldwide. In this paper, an adaptive approach for the
identification of fruit diseases is proposed and experimentally validated. The
image processing based proposed approach is composed of the following main
steps; in the first step K-Means clustering technique is used for the defect
segmentation, in the second step some state of the art features are extracted
from the segmented image, and finally images are classified into one of the
classes by using a Multi-class Support Vector Machine. We have considered
diseases of apple as a test case and evaluated our approach for three types of
apple diseases namely apple scab, apple blotch and apple rot. Our experimental
results express that the proposed solution can significantly support accurate
detection and automatic identification of fruit diseases. The classification
accuracy for the proposed solution is achieved up to 93%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4945</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4945</id><created>2014-05-19</created><updated>2014-12-15</updated><authors><author><keyname>Ye</keyname><forenames>Qiaoyang</forenames></author><author><keyname>Al-Shalash</keyname><forenames>Mazin</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Distributed Resource Allocation in Device-to-Device Enhanced Cellular
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular network performance can significantly benefit from direct
device-to-device (D2D) communication, but interference from cochannel D2D
communication limits the performance gain. In hybrid networks consisting of D2D
and cellular links, finding the optimal interference management is challenging.
In particular, we show that the problem of maximizing network throughput while
guaranteeing predefined service levels to cellular users is non- convex and
hence intractable. Instead, we adopt a distributed approach that is
computationally extremely efficient, and requires minimal coordination,
communication and cooperation among the nodes. The key algorithmic idea is a
signaling mechanism that can be seen as a fictional pricing mechanism, that the
base stations optimize and transmit to the D2D users, who then play a best
response (i.e., selfishly) to this signal. Numerical results show that our
algorithms converge quickly, have low overhead, and achieve a significant
throughput gain, while maintaining the quality of cellular links at a
predefined service level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4951</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4951</id><created>2014-05-20</created><authors><author><keyname>Hu</keyname><forenames>Pili</forenames></author><author><keyname>Chow</keyname><forenames>Sherman S. M.</forenames></author><author><keyname>Lau</keyname><forenames>Wing Cheong</forenames></author></authors><title>Secure Friend Discovery via Privacy-Preserving and Decentralized
  Community Detection</title><categories>cs.CR cs.SI stat.ML</categories><comments>ICML 2014 Workshop on Learning, Security and Privacy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of secure friend discovery on a social network has long been
proposed and studied. The requirement is that a pair of nodes can make
befriending decisions with minimum information exposed to the other party. In
this paper, we propose to use community detection to tackle the problem of
secure friend discovery. We formulate the first privacy-preserving and
decentralized community detection problem as a multi-objective optimization. We
design the first protocol to solve this problem, which transforms community
detection to a series of Private Set Intersection (PSI) instances using
Truncated Random Walk (TRW). Preliminary theoretical results show that our
protocol can uncover communities with overwhelming probability and preserve
privacy. We also discuss future works, potential extensions and variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4957</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4957</id><created>2014-05-20</created><updated>2014-10-13</updated><authors><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Symbol-Based Successive Cancellation List Decoder for Polar Codes</title><categories>cs.IT math.IT</categories><comments>Accepted by 2014 IEEE Workshop on Signal Processing Systems (SiPS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes is promising because they can provably achieve the channel
capacity while having an explicit construction method. Lots of work have been
done for the bit-based decoding algorithm for polar codes. In this paper,
generalized symbol-based successive cancellation (SC) and SC list decoding
algorithms are discussed. A symbol-based recursive channel combination
relationship is proposed to calculate the symbol-based channel transition
probability. This proposed method needs less additions than the
maximum-likelihood decoder used by the existing symbol-based polar decoding
algorithm. In addition, a two-stage list pruning network is proposed to
simplify the list pruning network for the symbol-based SC list decoding
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4969</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4969</id><created>2014-05-20</created><updated>2015-08-14</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author></authors><title>Sparsity Based Methods for Overparameterized Variational Problems</title><categories>cs.CV stat.ML</categories><comments>16 pages, 11 figures</comments><msc-class>47N10, 35A15, 49N45, 65M20, 65J22, 68U10, 94A12, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two complementary approaches have been extensively used in signal and image
processing leading to novel results, the sparse representation methodology and
the variational strategy. Recently, a new sparsity based model has been
proposed, the cosparse analysis framework, which may potentially help in
bridging sparse approximation based methods to the traditional total-variation
minimization. Based on this, we introduce a sparsity based framework for
solving overparameterized variational problems. The latter has been used to
improve the estimation of optical flow and also for general denoising of
signals and images. However, the recovery of the space varying parameters
involved was not adequately addressed by traditional variational methods. We
first demonstrate the efficiency of the new framework for one dimensional
signals in recovering a piecewise linear and polynomial function. Then, we
illustrate how the new technique can be used for denoising and segmentation of
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4979</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4979</id><created>2014-05-20</created><authors><author><keyname>Al-Harbi</keyname><forenames>Razen</forenames></author><author><keyname>Ebrahim</keyname><forenames>Yasser</forenames></author><author><keyname>Kalnis</keyname><forenames>Panos</forenames></author></authors><title>PHD-Store: An Adaptive SPARQL Engine with Dynamic Partitioning for
  Distributed RDF Repositories</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many repositories utilize the versatile RDF model to publish data.
Repositories are typically distributed and geographically remote, but data are
interconnected (e.g., the Semantic Web) and queried globally by a language such
as SPARQL. Due to the network cost and the nature of the queries, the execution
time can be prohibitively high. Current solutions attempt to minimize the
network cost by redistributing all data in a preprocessing phase, but here are
two drawbacks: (i) redistribution is based on heuristics that may not benefit
many of the future queries; and (ii) the preprocessing phase is very expensive
even for moderate size datasets. In this paper we propose PHD-Store, a SPARQL
engine for distributed RDF repositories. Our system does not assume any
particular initial data placement and does not require prepartitioning; hence,
it minimizes the startup cost. Initially, PHD-Store answers queries using a
potentially slow distributed semi-join algorithm, but adapts dynamically to the
query load by incrementally redistributing frequently accessed data.
Redistribution is done in a way that future queries can benefit from fast
hash-based parallel execution. Our experiments with synthetic and real data
verify that PHD-Store scales to very large datasets; many repositories;
converges to comparable or better quality of partitioning than existing
methods; and executes large query loads 1 to 2 orders of magnitude faster than
our competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4980</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4980</id><created>2014-05-20</created><updated>2015-11-16</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Convex Optimization: Algorithms and Complexity</title><categories>math.OC cs.CC cs.LG cs.NA stat.ML</categories><comments>A previous version of the manuscript was titled &quot;Theory of Convex
  Optimization for Machine Learning&quot;</comments><journal-ref>In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp
  231-357, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This monograph presents the main complexity theorems in convex optimization
and their corresponding algorithms. Starting from the fundamental theory of
black-box optimization, the material progresses towards recent advances in
structural optimization and stochastic optimization. Our presentation of
black-box optimization, strongly influenced by Nesterov's seminal book and
Nemirovski's lecture notes, includes the analysis of cutting plane methods, as
well as (accelerated) gradient descent schemes. We also pay special attention
to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror
descent, and dual averaging) and discuss their relevance in machine learning.
We provide a gentle introduction to structural optimization with FISTA (to
optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror
prox (Nemirovski's alternative to Nesterov's smoothing), and a concise
description of interior point methods. In stochastic optimization we discuss
stochastic gradient descent, mini-batches, random coordinate descent, and
sublinear algorithms. We also briefly touch upon convex relaxation of
combinatorial problems and the use of randomness to round solutions, as well as
random walks based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4981</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4981</id><created>2014-05-20</created><authors><author><keyname>Bracher</keyname><forenames>Annina</forenames></author><author><keyname>Hof</keyname><forenames>Eran</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>Distributed Storage for Data Security</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the secrecy of a distributed storage system for passwords. The
encoder, Alice, observes a length-n password and describes it using two hints,
which she then stores in different locations. The legitimate receiver, Bob,
observes both hints. The eavesdropper, Eve, sees only one of the hints; Alice
cannot control which. We characterize the largest normalized (by n) exponent
that we can guarantee for the number of guesses it takes Eve to guess the
password subject to the constraint that either the number of guesses it takes
Bob to guess the password or the size of the list that Bob must form to
guarantee that it contain the password approach 1 as n tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4989</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4989</id><created>2014-05-20</created><authors><author><keyname>Xu</keyname><forenames>Jiawei</forenames></author><author><keyname>Yue</keyname><forenames>Shigang</forenames></author><author><keyname>Wang</keyname><forenames>Ruisheng</forenames></author><author><keyname>Kiong</keyname><forenames>Loo Chu</forenames></author></authors><title>Perceiving Motion Cues Inspired by Microsoft Kinect Sensor on Game
  Experiencing</title><categories>cs.HC</categories><comments>4 pages,4 figures Confernece: 1st International Workshop on
  Bio-neuromorphic Systems and Human-Robot Interaction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a novel method to replace the traditional mouse
controller by using Microsoft Kinect Sensor to realize the functional
implementation on human-machine interaction. With human hand gestures and
movements, Kinect Sensor could accurately recognize the participants intention
and transmit our order to desktop or laptop. In addition, the trend in current
HCI market is giving the customer more freedom and experiencing feeling by
involving human cognitive factors more deeply. Kinect sensor receives the
motion cues continuously from the humans intention and feedback the reaction
during the experiments. The comparison accuracy between the hand movement and
mouse cursor demonstrates the efficiency for the proposed method. In addition,
the experimental results on hit rate in the game of Fruit Ninja and Shape
Touching proves the real-time ability of the proposed framework. The
performance evaluation built up a promise foundation for the further
applications in the field of human-machine interaction. The contribution of
this work is the expansion on hand gesture perception and early formulation on
Mac iPad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5003</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5003</id><created>2014-05-20</created><updated>2014-10-07</updated><authors><author><keyname>Yan</keyname><forenames>Rongjie</forenames></author><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Zhang</keyname><forenames>Guangquan</forenames></author><author><keyname>Chai</keyname><forenames>Yesheng</forenames></author></authors><title>Formal Consistency Checking over Specifications in Natural Languages</title><categories>cs.SE</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early stages of system development involve outlining desired features such as
functionality, availability, or usability. Specifications are derived from
these features that concretize vague ideas presented in natural languages. The
challenge for the validation of specifications arises from the syntax and
semantic gap between different representations and the need of automatic tools.
In this paper, we present a requirement-consistency maintenance framework to
produce consistent representations. The first part is the automatic translation
from natural languages describing functionalities to formal logic with an
abstraction of time. It extends pure syntactic parsing by adding semantic
reasoning and the support of partitioning input and output variables. The
second part is the use of synthesis techniques to examine if the requirements
are consistent in terms of realizability. When the process fails, the formulas
that cause the inconsistency are reported to locate the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5005</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5005</id><created>2014-05-20</created><authors><author><keyname>Romano</keyname><forenames>Francesco</forenames></author><author><keyname>Pucci</keyname><forenames>Daniele</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>Collocated Adaptive Control of Underactuated Mechanical Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collocated adaptive control of underactuated systems is still a main concern
for the control community, all the more so because the collocated dynamics is
no longer linear with respect to the constant base parameters. This work
extends and encompasses the well known adaptive control result for fully
actuated mechanical systems to the underactuated case. The key point is the
introduction of a fictitious control input that allows us to consider the
complete system dynamics, which is assumed to be linear with respect to the
base parameters. Local stability and convergence of time varying reference
trajectories for the collocated dynamics are demonstrated by using Lyapunov and
Barbalat arguments. Simulation and experimental results on a two-link
manipulator verify the soundness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5009</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5009</id><created>2014-05-20</created><authors><author><keyname>Arora</keyname><forenames>Megha</forenames></author><author><keyname>Gupta</keyname><forenames>Raghav</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Indian Premier League (IPL), Cricket, Online Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent past online social media has played a pivotal role in sharing of
information and opinions on real time events. Events in physical space are
reflected in digital world through online social networks. Event based studies
for such content have been widely done on Twitter in the computer science
community. In this report, we performed a detailed analysis of a sports event
called the Indian Premier League (IPL'13) for both Facebook and Twitter. IPL is
the most popular cricket league in India with players from across the world. We
analysed more than 2.6 million tweets and 700 thousand Facebook posts for
temporal activity, text quality, geography of users and the spot-fixing scandal
which came up during the league. We were able to draw strong correlations
between the brand value of teams and how much they were talked about on social
media across Facebook and Twitter. Analysis of geo-tagged data showed major
activity from metropolitan suburbs however activity was not restricted to the
regions geographically associated with each team. We present a decay
calculation methodology, using which we derive that activity died down on both
Twitter and Facebook in a very similar manner. Such analysis can be used to
model events and study their penetration in social networks. We analysed text
for spot-fixing and found that user response to allegations about matches being
fixed was cold. The complete analysis presented in this report, can be
particularly useful for studying events involving crisis or events of political
importance having similar structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5023</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5023</id><created>2014-05-20</created><authors><author><keyname>Kermarrec</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Thraves</keyname><forenames>Christopher</forenames></author></authors><title>Signed graph embedding: when everybody can sit closer to friends than
  enemies</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signed graphs are graphs with signed edges. They are commonly used to
represent positive and negative relationships in social networks. While balance
theory and clusterizable graphs deal with signed graphs to represent social
interactions, recent empirical studies have proved that they fail to reflect
some current practices in real social networks. In this paper we address the
issue of drawing signed graphs and capturing such social interactions. We relax
the previous assumptions to define a drawing as a model in which every vertex
has to be placed closer to its neighbors connected via a positive edge than its
neighbors connected via a negative edge in the resulting space. Based on this
definition, we address the problem of deciding whether a given signed graph has
a drawing in a given $\ell$-dimensional Euclidean space. We present forbidden
patterns for signed graphs that admit the introduced definition of drawing in
the Euclidean plane and line. We then focus on the $1$-dimensional case, where
we provide a polynomial time algorithm that decides if a given complete signed
graph has a drawing, and constructs it when applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5024</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5024</id><created>2014-05-20</created><updated>2015-10-24</updated><authors><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Multi-user guesswork and brute force security</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Guesswork problem was originally motivated by a desire to quantify
computational security for single user systems. Leveraging recent results from
its analysis, we extend the remit and utility of the framework to the
quantification of the computational security for multi-user systems. In
particular, assume that $V$ users independently select strings stochastically
from a finite, but potentially large, list. An inquisitor who does not know
which strings have been selected wishes to identify $U$ of them. The inquisitor
knows the selection probabilities of each user and is equipped with a method
that enables the testing of each (user, string) pair, one at a time, for
whether that string had been selected by that user.
  Here we establish that, unless $U=V$, there is no general strategy that
minimizes the distribution of the number of guesses, but in the asymptote as
the strings become long we prove the following: by construction, there is an
asymptotically optimal class of strategies; the number of guesses required in
an asymptotically optimal strategy satisfies a large deviation principle with a
rate function, which is not necessarily convex, that can be determined from the
rate functions of optimally guessing individual users' strings; if all user's
selection statistics are identical, the exponential growth rate of the average
guesswork as the string-length increases is determined by the specific R\'enyi
entropy of the string-source with parameter $(V-U+1)/(V-U+2)$, generalizing the
known $V=U=1$ case; and that the Shannon entropy of the source is a lower bound
on the average guesswork growth rate for all $U$ and $V$, thus providing a
bound on computational security for multi-user systems. Examples are presented
to illustrate these results and their ramifications for systems design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5047</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5047</id><created>2014-05-20</created><updated>2014-06-17</updated><authors><author><keyname>Burke</keyname><forenames>Michael</forenames></author><author><keyname>Lasenby</keyname><forenames>Joan</forenames></author></authors><title>Single camera pose estimation using Bayesian filtering and Kinect motion
  priors</title><categories>cs.CV cs.HC</categories><comments>25 pages, Technical report, related to Burke and Lasenby, AMDO 2014
  conference paper. Code sample: https://github.com/mgb45/SignerBodyPose Video:
  https://www.youtube.com/watch?v=dJMTSo7-uFE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional approaches to upper body pose estimation using monocular vision
rely on complex body models and a large variety of geometric constraints. We
argue that this is not ideal and somewhat inelegant as it results in large
processing burdens, and instead attempt to incorporate these constraints
through priors obtained directly from training data. A prior distribution
covering the probability of a human pose occurring is used to incorporate
likely human poses. This distribution is obtained offline, by fitting a
Gaussian mixture model to a large dataset of recorded human body poses, tracked
using a Kinect sensor. We combine this prior information with a random walk
transition model to obtain an upper body model, suitable for use within a
recursive Bayesian filtering framework. Our model can be viewed as a mixture of
discrete Ornstein-Uhlenbeck processes, in that states behave as random walks,
but drift towards a set of typically observed poses. This model is combined
with measurements of the human head and hand positions, using recursive
Bayesian estimation to incorporate temporal information. Measurements are
obtained using face detection and a simple skin colour hand detector, trained
using the detected face. The suggested model is designed with analytical
tractability in mind and we show that the pose tracking can be
Rao-Blackwellised using the mixture Kalman filter, allowing for computational
efficiency while still incorporating bio-mechanical properties of the upper
body. In addition, the use of the proposed upper body model allows reliable
three-dimensional pose estimates to be obtained indirectly for a number of
joints that are often difficult to detect using traditional object recognition
strategies. Comparisons with Kinect sensor results and the state of the art in
2D pose estimation highlight the efficacy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5048</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5048</id><created>2014-05-20</created><authors><author><keyname>Polceanu</keyname><forenames>Mihai</forenames></author><author><keyname>Buche</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Towards A Theory-Of-Mind-Inspired Generic Decision-Making Framework</title><categories>cs.AI</categories><comments>7 pages, 5 figures, IJCAI 2013 Symposium on AI in Angry Birds</comments><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation is widely used to make model-based predictions, but few approaches
have attempted this technique in dynamic physical environments of medium to
high complexity or in general contexts. After an introduction to the cognitive
science concepts from which this work is inspired and the current development
in the use of simulation as a decision-making technique, we propose a generic
framework based on theory of mind, which allows an agent to reason and perform
actions using multiple simulations of automatically created or externally
inputted models of the perceived environment. A description of a partial
implementation is given, which aims to solve a popular game within the
IJCAI2013 AIBirds contest. Results of our approach are presented, in comparison
with the competition benchmark. Finally, future developments regarding the
framework are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5050</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5050</id><created>2014-05-20</created><authors><author><keyname>Azarbonyad</keyname><forenames>Hosein</forenames></author><author><keyname>Babazadeh</keyname><forenames>Reza</forenames></author></authors><title>A Genetic Algorithm for solving Quadratic Assignment Problem(QAP)</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1305.2684 by other authors</comments><journal-ref>In Proceeding of 5th International Conference of Iranian
  Operations Research Society (ICIORS), Tabriz, Iran, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quadratic Assignment Problem (QAP) is one of the models used for the
multi-row layout problem with facilities of equal area. There are a set of n
facilities and a set of n locations. For each pair of locations, a distance is
specified and for each pair of facilities a weight or flow is specified (e.g.,
the amount of supplies transported between the two facilities). The problem is
to assign all facilities to different locations with the aim of minimizing the
sum of the distances multiplied by the corresponding flows. The QAP is among
the most difficult NP-hard combinatorial optimization problems. Because of
this, this paper presents an efficient Genetic algorithm (GA) to solve this
problem in reasonable time. For validation the proposed GA some examples are
selected from QAP library. The obtained results in reasonable time show the
efficiency of proposed GA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5057</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5057</id><created>2014-05-20</created><authors><author><keyname>Percino</keyname><forenames>Gamaliel</forenames></author><author><keyname>Klimek</keyname><forenames>Peter</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Instrumentational complexity of music genres and why simplicity sells</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 5 figures, Supporting Information</comments><doi>10.1371/journal.pone.0115255</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Listening habits are strongly influenced by two opposing aspects, the desire
for variety and the demand for uniformity in music. In this work we quantify
these two notions in terms of musical instrumentation and production
technologies that are typically involved in crafting popular music. We assign a
&quot;complexity value&quot; to each music style. A style is complex if it shows the
property of having both high variety and low uniformity in instrumentation. We
find a strong inverse relation between variety and uniformity of music styles
that is remarkably stable over the last half century. Individual styles,
however, show dramatic changes in their &quot;complexity&quot; during that period. Styles
like &quot;new wave&quot; or &quot;disco&quot; quickly climbed towards higher complexity in the 70s
and fell back to low complexity levels shortly afterwards, whereas styles like
&quot;folk rock&quot; remained at constant high complexity levels. We show that changes
in the complexity of a style are related to its number of sales and to the
number of artists contributing to that style. As a style attracts a growing
number of artists, its instrumentational variety usually increases. At the same
time the instrumentational uniformity of a style decreases, i.e. a unique
stylistic and increasingly complex expression pattern emerges. In contrast,
album sales of a given style typically increase with decreasing complexity.
This can be interpreted as music becoming increasingly formulaic once
commercial or mainstream success sets in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5066</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5066</id><created>2014-05-20</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Echavarria</keyname><forenames>Alonso</forenames></author><author><keyname>Ramirez-Ortegon</keyname><forenames>Marte A.</forenames></author></authors><title>An optimization algorithm inspired by the States of Matter that improves
  the balance between exploration and exploitation</title><categories>cs.AI</categories><comments>22 pages</comments><journal-ref>Applied Intelligence, 40(2) , (2014), 256-272</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability of an Evolutionary Algorithm (EA) to find a global optimal
solution depends on its capacity to find a good rate between exploitation of
found so far elements and exploration of the search space. Inspired by natural
phenomena, researchers have developed many successful evolutionary algorithms
which, at original versions, define operators that mimic the way nature solves
complex problems, with no actual consideration of the exploration/exploitation
balance. In this paper, a novel nature-inspired algorithm called the States of
Matter Search (SMS) is introduced. The SMS algorithm is based on the simulation
of the states of matter phenomenon. In SMS, individuals emulate molecules which
interact to each other by using evolutionary operations which are based on the
physical principles of the thermal-energy motion mechanism. The algorithm is
devised by considering each state of matter at one different
exploration/exploitation ratio. The evolutionary process is divided into three
phases which emulate the three states of matter: gas, liquid and solid. In each
state, molecules (individuals) exhibit different movement capacities. Beginning
from the gas state (pure exploration), the algorithm modifies the intensities
of exploration and exploitation until the solid state (pure exploitation) is
reached. As a result, the approach can substantially improve the balance
between exploration/exploitation, yet preserving the good search capabilities
of an evolutionary approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5070</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5070</id><created>2014-05-20</created><authors><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Tugores</keyname><forenames>Ant&#xf2;nia</forenames></author><author><keyname>Colet</keyname><forenames>Pere</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author></authors><title>Tweets on the road</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>15 pages, 17 figures</comments><journal-ref>PLoS ONE 9, e105407 (2014)</journal-ref><doi>10.1371/journal.pone.0105407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasiveness of mobile devices, which is increasing daily, is generating
a vast amount of geo-located data allowing us to gain further insights into
human behaviors. In particular, this new technology enables users to
communicate through mobile social media applications, such as Twitter, anytime
and anywhere. Thus, geo-located tweets offer the possibility to carry out
in-depth studies on human mobility. In this paper, we study the use of Twitter
in transportation by identifying tweets posted from roads and rails in Europe
between September 2012 and November 2013. We compute the percentage of highway
and railway segments covered by tweets in 39 countries. The coverages are very
different from country to country and their variability can be partially
explained by differences in Twitter penetration rates. Still, some of these
differences might be related to cultural factors regarding mobility habits and
interacting socially online. Analyzing particular road sectors, our results
show a positive correlation between the number of tweets on the road and the
Average Annual Daily Traffic on highways in France and in the UK. Transport
modality can be studied with these data as well, for which we discover very
heterogeneous usage patterns across the continent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5083</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5083</id><created>2014-05-20</created><authors><author><keyname>Dikstein</keyname><forenames>Lior</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Steinberg</keyname><forenames>Yossef</forenames></author></authors><title>On State Dependent Broadcast Channels with Cooperation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate problems of communication over physically
degraded, state-dependent broadcast channels (BCs) with cooperating decoders.
Two different setups are considered and their capacity regions are
characterized. First, we study a setting in which one decoder can use a finite
capacity link to send the other decoder information regarding the messages or
the channel states. In this scenario we analyze two cases: one where noncausal
state information is available to the encoder and the strong decoder and the
other where state information is available only to the encoder in a causal
manner. Second, we examine a setting in which the cooperation between the
decoders is limited to taking place before the outputs of the channel are
given. In this case, one decoder, which is informed of the state sequence
noncausally, can cooperate only to send the other decoder rate-limited
information about the state sequence. The proofs of the capacity regions
introduce a new method of coding for channels with cooperation between
different users, where we exploit the link between the decoders for
multiple-binning. Finally, we discuss the optimality of using rate splitting
techniques when coding for cooperative BCs. In particular, we show that rate
splitting is not necessarily optimal when coding for cooperative BCs by solving
an example in which our method of coding outperforms rate splitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5096</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5096</id><created>2014-05-20</created><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms</title><categories>cs.LG stat.ML</categories><comments>ICML 2014 (technical report). arXiv admin note: text overlap with
  arXiv:1307.7309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic multi-armed bandits where the expected reward is a
unimodal function over partially ordered arms. This important class of problems
has been recently investigated in (Cope 2009, Yu 2011). The set of arms is
either discrete, in which case arms correspond to the vertices of a finite
graph whose structure represents similarity in rewards, or continuous, in which
case arms belong to a bounded interval. For discrete unimodal bandits, we
derive asymptotic lower bounds for the regret achieved under any algorithm, and
propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm
optimally exploits the unimodal structure of the problem, and surprisingly, its
asymptotic regret does not depend on the number of arms. We also provide a
regret upper bound for OSUB in non-stationary environments where the expected
rewards smoothly evolve over time. The analytical results are supported by
numerical experiments showing that OSUB performs significantly better than the
state-of-the-art algorithms. For continuous sets of arms, we provide a brief
discussion. We show that combining an appropriate discretization of the set of
arms with the UCB algorithm yields an order-optimal regret, and in practice,
outperforms recently proposed algorithms designed to exploit the unimodal
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5097</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5097</id><created>2014-05-20</created><authors><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Design of Efficient Sampling Methods on Hybrid Social-Affiliation
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 13 figures, technique report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph sampling via crawling has become increasingly popular and important in
the study of measuring various characteristics of large scale complex networks.
While powerful, it is known to be challenging when the graph is loosely
connected or disconnected which slows down the convergence of random walks and
can cause poor estimation accuracy.
  In this work, we observe that the graph under study, or called target graph,
usually does not exist in isolation. In many situations, the target graph is
related to an auxiliary graph and an affiliation graph, and the target graph
becomes well connected when we view it from the perspective of these three
graphs together, or called a hybrid social-affiliation graph in this paper.
When directly sampling the target graph is difficult or inefficient, we can
indirectly sample it efficiently with the assistances of the other two graphs.
We design three sampling methods on such a hybrid social-affiliation network.
Experiments conducted on both synthetic and real datasets demonstrate the
effectiveness of our proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5101</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5101</id><created>2014-05-20</created><authors><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Perret</keyname><forenames>Ludovic</forenames></author><author><keyname>de Portzamparc</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Folding Alternant and Goppa Codes with Non-Trivial Automorphism Groups</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main practical limitation of the McEliece public-key encryption scheme is
probably the size of its key. A famous trend to overcome this issue is to focus
on subclasses of alternant/Goppa codes with a non trivial automorphism group.
Such codes display then symmetries allowing compact parity-check or generator
matrices. For instance, a key-reduction is obtained by taking quasi-cyclic (QC)
or quasi-dyadic (QD) alternant/Goppa codes. We show that the use of such
symmetric alternant/Goppa codes in cryptography introduces a fundamental
weakness. It is indeed possible to reduce the key-recovery on the original
symmetric public-code to the key-recovery on a (much) smaller code that has not
anymore symmetries. This result is obtained thanks to a new operation on codes
called folding that exploits the knowledge of the automorphism group. This
operation consists in adding the coordinates of codewords which belong to the
same orbit under the action of the automorphism group. The advantage is
twofold: the reduction factor can be as large as the size of the orbits, and it
preserves a fundamental property: folding the dual of an alternant (resp.
Goppa) code provides the dual of an alternant (resp. Goppa) code. A key point
is to show that all the existing constructions of alternant/Goppa codes with
symmetries follow a common principal of taking codes whose support is globally
invariant under the action of affine transformations (by building upon prior
works of T. Berger and A. D{\&quot;{u}}r). This enables not only to present a
unified view but also to generalize the construction of QC, QD and even
quasi-monoidic (QM) Goppa codes. All in all, our results can be harnessed to
boost up any key-recovery attack on McEliece systems based on symmetric
alternant or Goppa codes, and in particular algebraic attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5109</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5109</id><created>2014-05-16</created><authors><author><keyname>Morak</keyname><forenames>Michael</forenames></author></authors><title>The Impact of Disjunction on Reasoning under Existential Rules: Research
  Summary</title><categories>cs.DB cs.LO</categories><comments>A full version of a paper accepted to be presented at the Doctoral
  Consortium of the 30th International Conference on Logic Programming (ICLP
  2014), July 19-22, Vienna, Austria</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Datalog+/- is a Datalog-based language family enhanced with existential
quantification in rule heads, equalities and negative constraints. Query
answering over databases with respect to a Datalog+/- theory is generally
undecidable, however several syntactic restrictions have been proposed to
remedy this fact. However, a useful and natural feature however is as of yet
missing from Datalog+/-: The ability to express uncertain knowledge, or
choices, using disjunction. It is the precise objective of the doctoral thesis
herein discussed, to investigate the impact on the complexity of query
answering, of adding disjunction to well-known decidable Datalog+/- fragments,
namely guarded, sticky and weakly-acyclic Datalog+/- theories. For guarded
theories with disjunction, we obtain a strong 2EXP lower bound in the combined
complexity, even for very restricted formalisms like fixed sets of
(disjunctive) inclusion dependencies. For sticky theories, the query answering
problem becomes undecidable, even in the data complexity, and for
weakly-acyclic query answering we see a reasonable and expected increase in
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5119</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5119</id><created>2014-05-20</created><authors><author><keyname>Sarkar</keyname><forenames>Tanmoy</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Steganalysis: Detecting LSB Steganographic Techniques</title><categories>cs.MM cs.CR</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganalysis means analysis of stego images. Like cryptanalysis, steganalysis
is used to detect messages often encrypted using secret key from stego images
produced by steganography techniques. Recently lots of new and improved
steganography techniques are developed and proposed by researchers which
require robust steganalysis techniques to detect the stego images having
minimum false alarm rate. This paper discusses about the different Steganalysis
techniques and help to understand how, where and when this techniques can be
used based on different situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5133</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5133</id><created>2014-05-20</created><updated>2014-05-21</updated><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author><author><keyname>Plandowski</keyname><forenames>Wojciech</forenames></author></authors><title>Finding All Solutions of Equations in Free Groups and Monoids with
  Involution</title><categories>cs.LO cs.DM math.GR</categories><comments>A preliminary version of this paper was presented as an invited talk
  at CSR 2014 in Moscow, June 7 - 11, 2014</comments><acm-class>F.4; F.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a PSPACE algorithm which yields a finite
graph of exponential size and which describes the set of all solutions of
equations in free groups as well as the set of all solutions of equations in
free monoids with involution in the presence of rational constraints. This
became possible due to the recently invented emph{recompression} technique of
the second author.
  He successfully applied the recompression technique for pure word equations
without involution or rational constraints. In particular, his method could not
be used as a black box for free groups (even without rational constraints).
Actually, the presence of an involution (inverse elements) and rational
constraints complicates the situation and some additional analysis is
necessary. Still, the recompression technique is general enough to accommodate
both extensions. In the end, it simplifies proofs that solving word equations
is in PSPACE (Plandowski 1999) and the corresponding result for equations in
free groups with rational constraints (Diekert, Hagenah and Gutierrez 2001). As
a byproduct we obtain a direct proof that it is decidable in PSPACE whether or
not the solution set is finite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5145</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5145</id><created>2014-05-20</created><authors><author><keyname>Gafni</keyname><forenames>Eli</forenames></author></authors><title>Set Consensus: Captured by a Set of Runs with Ramifications</title><categories>cs.DC</categories><comments>Submited to DISC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are (set)-consensus objects necessary? This paper answer is negative.
  We show that the availability of consensus objects can be replaced by
restricting the set of runs we consider. In particular we concentrate of the
set of runs of the Immediate-Snapshot-Model (IIS), and given the object we
identify this restricted subset of IIS runs.
  We further show that given an $(m,k)$-set consensus, an object that provides
$k$-set consensus among $m$ processors, in a system of $n$, $n&gt;m$ processors,
we do not need to use the precise power of the objects but rather their
effective cumulative set consensus power. E.g. when $n=3, m=2,$ and $k=1$ and
all the 3 processors are active then we only use 2-set consensus among the 3
processors, as if 2-processors consensus is not available. We do this until at
least one of the 3 processors obtains an output. We show that this suggests a
new direction in the design of algorithms when consensus objects are involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5147</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5147</id><created>2014-05-20</created><authors><author><keyname>Aguiar</keyname><forenames>Everaldo</forenames></author><author><keyname>Nagrecha</keyname><forenames>Saurabh</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Predicting Online Video Engagement Using Clickstreams</title><categories>cs.LG cs.IR</categories><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the nascent days of e-content delivery, having a superior product was
enough to give companies an edge against the competition. With today's fiercely
competitive market, one needs to be multiple steps ahead, especially when it
comes to understanding consumers. Focusing on a large set of web portals owned
and managed by a private communications company, we propose methods by which
these sites' clickstream data can be used to provide a deep understanding of
their visitors, as well as their interests and preferences. We further expand
the use of this data to show that it can be effectively used to predict user
engagement to video streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5148</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5148</id><created>2014-05-20</created><authors><author><keyname>Zhu</keyname><forenames>Ting</forenames></author><author><keyname>Zhu</keyname><forenames>Yuxuan</forenames></author><author><keyname>Yang</keyname><forenames>Hong</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author></authors><title>Determination of Boiling Range of Xylene Mixed in PX Device Using
  Artificial Neural Networks</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Determination of boiling range of xylene mixed in PX device is currently a
crucial topic in the practical applications because of the recent disputes of
PX project in China. In our study, instead of determining the boiling range of
xylene mixed by traditional approach in laboratory or industry, we successfully
established two Artificial Neural Networks (ANNs) models to determine the
initial boiling point and final boiling point respectively. Results show that
the Multilayer Feedforward Neural Networks (MLFN) model with 7 nodes (MLFN-7)
is the best model to determine the initial boiling point of xylene mixed, with
the RMS error 0.18; while the MLFN model with 4 nodes (MLFN-4) is the best
model to determine the final boiling point of xylene mixed, with the RMS error
0.75. The training and testing processes both indicate that the models we
developed are robust and precise. Our research can effectively avoid the damage
of the PX device to human body and environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5156</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5156</id><created>2014-05-20</created><authors><author><keyname>Liu</keyname><forenames>Li-Ping</forenames></author><author><keyname>Sheldon</keyname><forenames>Daniel</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas G.</forenames></author></authors><title>Gaussian Approximation of Collective Graphical Models</title><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted by ICML 2014. 10 page version with appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Collective Graphical Model (CGM) models a population of independent and
identically distributed individuals when only collective statistics (i.e.,
counts of individuals) are observed. Exact inference in CGMs is intractable,
and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP
approximations for learning and inference. This paper studies Gaussian
approximations to the CGM. As the population grows large, we show that the CGM
distribution converges to a multivariate Gaussian distribution (GCGM) that
maintains the conditional independence properties of the original CGM. If the
observations are exact marginals of the CGM or marginals that are corrupted by
Gaussian noise, inference in the GCGM approximation can be computed efficiently
in closed form. If the observations follow a different noise model (e.g.,
Poisson), then expectation propagation provides efficient and accurate
approximate inference. The accuracy and speed of GCGM inference is compared to
the MCMC and MAP methods on a simulated bird migration problem. The GCGM
matches or exceeds the accuracy of the MAP method while being significantly
faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5164</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5164</id><created>2014-05-20</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Gonzalez</keyname><forenames>Maurici</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Multi-ellipses detection on images inspired by collective animal
  behavior</title><categories>cs.CV</categories><comments>21 pages</comments><journal-ref>Neural Computing and Applications, 24(5), (2014), 1019-1033</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel and effective technique for extracting multiple
ellipses from an image. The approach employs an evolutionary algorithm to mimic
the way animals behave collectively assuming the overall detection process as a
multi-modal optimization problem. In the algorithm, searcher agents emulate a
group of animals that interact to each other using simple biological rules
which are modeled as evolutionary operators. In turn, such operators are
applied to each agent considering that the complete group has a memory to store
optimal solutions (ellipses) seen so-far by applying a competition principle.
The detector uses a combination of five edge points as parameters to determine
ellipse candidates (possible solutions) while a matching function determines if
such ellipse candidates are actually present in the image. Guided by the values
of such matching functions, the set of encoded candidate ellipses are evolved
through the evolutionary algorithm so that the best candidates can be fitted
into the actual ellipses within the image. Just after the optimization process
ends, an analysis over the embedded memory is executed in order to find the
best obtained solution (the best ellipse) and significant local minima
(remaining ellipses). Experimental results over several complex synthetic and
natural images have validated the efficiency of the proposed technique
regarding accuracy, speed and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5170</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5170</id><created>2014-05-20</created><updated>2014-12-10</updated><authors><author><keyname>Drohmann</keyname><forenames>Martin</forenames></author><author><keyname>Carlberg</keyname><forenames>Kevin</forenames></author></authors><title>The ROMES method for statistical modeling of reduced-order-model error</title><categories>cs.NA math.NA stat.ML</categories><msc-class>65G99, 65Y20, 62M86</msc-class><journal-ref>SIAM/ASA Journal on Uncertainty Quantification, Vol. 3, No. 1, p.
  116-145 (2015)</journal-ref><doi>10.1137/140969841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a technique for statistically modeling errors introduced
by reduced-order models. The method employs Gaussian-process regression to
construct a mapping from a small number of computationally inexpensive `error
indicators' to a distribution over the true error. The variance of this
distribution can be interpreted as the (epistemic) uncertainty introduced by
the reduced-order model. To model normed errors, the method employs existing
rigorous error bounds and residual norms as indicators; numerical experiments
show that the method leads to a near-optimal expected effectivity in contrast
to typical error bounds. To model errors in general outputs, the method uses
dual-weighted residuals---which are amenable to uncertainty control---as
indicators. Experiments illustrate that correcting the reduced-order-model
output with this surrogate can improve prediction accuracy by an order of
magnitude; this contrasts with existing `multifidelity correction' approaches,
which often fail for reduced-order models and suffer from the curse of
dimensionality. The proposed error surrogates also lead to a notion of
`probabilistic rigor', i.e., the surrogate bounds the error with specified
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5172</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5172</id><created>2014-05-20</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Oliva</keyname><forenames>Diego</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Pajares</keyname><forenames>Gonzalo</forenames></author></authors><title>Opposition Based ElectromagnetismLike for Global Optimization</title><categories>cs.AI</categories><comments>27 Pages</comments><journal-ref>International Journal of Innovative Computing, Information and
  Control, 8 (12) , (2012), pp. 8181-8198</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electromagnetismlike Optimization (EMO) is a global optimization algorithm,
particularly well suited to solve problems featuring nonlinear and multimodal
cost functions. EMO employs searcher agents that emulate a population of
charged particles which interact to each other according to electromagnetisms
laws of attraction and repulsion. However, EMO usually requires a large number
of iterations for a local search procedure; any reduction or cancelling over
such number, critically perturb other issues such as convergence, exploration,
population diversity and accuracy. This paper presents an enhanced EMO
algorithm called OBEMO, which employs the Opposition-Based Learning (OBL)
approach to accelerate the global convergence speed. OBL is a machine
intelligence strategy which considers the current candidate solution and its
opposite value at the same time, achieving a faster exploration of the search
space. The proposed OBEMO method significantly reduces the required
computational effort yet avoiding any detriment to the good search capabilities
of the original EMO algorithm. Experiments are conducted over a comprehensive
set of benchmark functions, showing that OBEMO obtains promising performance
for most of the discussed test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5189</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5189</id><created>2014-05-20</created><updated>2015-12-09</updated><authors><author><keyname>Chen</keyname><forenames>Bowei</forenames></author><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A dynamic pricing model for unifying programmatic guarantee and
  real-time bidding in display advertising</title><categories>cs.GT</categories><comments>Chen, Bowei and Yuan, Shuai and Wang, Jun (2014) A dynamic pricing
  model for unifying programmatic guarantee and real-time bidding in display
  advertising. In: The Eighth International Workshop on Data Mining for Online
  Advertising, 24 - 27 August 2014, New York City</comments><doi>10.1145/2648584.2648585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5193</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5193</id><created>2014-04-19</created><authors><author><keyname>Yavuz</keyname><forenames>Faruk</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Towards $k$-connectivity of the random graph induced by a pairwise key
  predistribution scheme with unreliable links</title><categories>cs.DM cs.CR cs.IT math.CO math.IT math.PR</categories><comments>The full version (with proofs) of a paper to be presented at IEEE
  International Symposium on Information Theory (ISIT 2014), Honolulu (HI),
  July 2014. 37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the secure and reliable connectivity of wireless sensor networks.
Security is assumed to be ensured by the random pairwise key predistribution
scheme of Chan, Perrig, and Song, and unreliable wireless links are represented
by independent on/off channels. Modeling the network by an intersection of a
random $K$-out graph and an Erd\H{o}s-R\'enyi graph, we present scaling
conditions (on the number of nodes, the scheme parameter $K$, and the
probability of a wireless channel being on) such that the resulting graph
contains no nodes with degree less than $k$ with high probability, when the
number of nodes gets large. Results are given in the form of zero-one laws and
are shown to improve the previous results by Ya\u{g}an and Makowski on the
absence of isolated nodes (i.e., absence of nodes with degree zero). Via
simulations, the established zero-one laws are shown to hold also for the
property of $k$-connectivity; i.e., the property that graph remains connected
despite the deletion of any $k-1$ nodes or edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5195</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5195</id><created>2014-05-20</created><updated>2015-12-17</updated><authors><author><keyname>Aguerri</keyname><forenames>I&#xf1;aki Estella</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Capacity of a Class of State-Dependent Orthogonal Relay Channels</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of orthogonal relay channels in which the orthogonal channels
connecting the source terminal to the relay and the destination, and the relay
to the destination, depend on a state sequence, is considered. It is assumed
that the state sequence is fully known at the destination while it is not known
at the source or the relay. The capacity of this class of relay channels is
characterized, and shown to be achieved by the partial
decode-compress-and-forward (pDCF) scheme. Then the capacity of certain binary
and Gaussian state-dependent orthogonal relay channels are studied in detail,
and it is shown that the compress-and-forward (CF) and
partial-decode-and-forward (pDF) schemes are suboptimal in general. To the best
of our knowledge, this is the first single relay channel model for which the
capacity is achieved by pDCF, while pDF and CF schemes are both suboptimal.
Furthermore, it is shown that the capacity of the considered class of
state-dependent orthogonal relay channels is in general below the cut-set
bound. The conditions under which pDF or CF suffices to meet the cut-set bound,
and hence, achieve the capacity, are also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5197</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5197</id><created>2014-05-20</created><updated>2014-11-03</updated><authors><author><keyname>Zhu</keyname><forenames>Yitao</forenames></author><author><keyname>Sandu</keyname><forenames>Corina</forenames></author><author><keyname>Dopico</keyname><forenames>Daniel</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Optimization of Vehicle Dynamics based on Multibody Models using Adjoint
  Sensitivity Analysis</title><categories>cs.CE</categories><comments>I tried to replace this paper with a new one which has corrected
  several errors in this paper. However, I didn't know how to replace it at
  that time, I submitted a new one &quot;Dynamic Response Optimization of Complex
  Multibody Systems in a Penalty Formulation using Adjoint Sensitivity&quot;, the
  identifier is arXiv:1410.8422. Since I have already submitted that one, I
  want to withdraw this one</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multibody dynamics simulations have become widely used tools for vehicle
systems analysis and design. As this approach evolves, it becomes able to
provide additional information for various types of analyses. One very
important direction is the optimization of multibody systems. Sensitivity
analysis of multibody system dynamics is essential for design optimization.
Dynamic sensitivities, when needed, are often calculated by means of finite
differences. However, depending of the number of parameters involved, this
procedure can be computationally expensive. Moreover, in many cases the results
suffer from low accuracy when real perturbations are used. This paper develops
the adjoint sensitivity analysis of multibody systems in the context of penalty
formulations. The resulting sensitivities are applied to perform dynamical
optimization of a full vehicle system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5200</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5200</id><created>2014-01-09</created><authors><author><keyname>Tarannum</keyname><forenames>Narzu</forenames></author><author><keyname>Ahmed</keyname><forenames>Nova</forenames></author></authors><title>Efficient and Reliable Hybrid Cloud Architechture for Big Data</title><categories>cs.DC</categories><comments>13 pages, 9 figures, International Journal on Cloud Computing:
  Services and Architecture (IJCCSA), Vol.3, No.6, December 2013</comments><doi>10.5121/ijccsa.2013.3602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of our paper is to propose a Cloud computing framework which is
feasible and necessary for handling huge data. In our prototype system we
considered national ID database structure of Bangladesh which is prepared by
election commission of Bangladesh. Using this database we propose an
interactive graphical user interface for Bangladeshi People Search (BDPS) that
use a hybrid structure of cloud computing handled by apache Hadoop where
database is implemented by HiveQL. The infrastructure divides into two parts:
locally hosted cloud which is based on Eucalyptus and the remote cloud which is
implemented on well-known Amazon Web Service (AWS). Some common problems of
Bangladesh aspect which includes data traffic congestion, server time out and
server down issue is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5201</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5201</id><created>2014-01-15</created><authors><author><keyname>Zhang</keyname><forenames>Dongmo</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>An Ordinal Bargaining Solution with Fixed-Point Property</title><categories>cs.GT</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 33, pages
  433-464, 2008</journal-ref><doi>10.1613/jair.2656</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shapleys impossibility result indicates that the two-person bargaining
problem has no non-trivial ordinal solution with the traditional game-theoretic
bargaining model. Although the result is no longer true for bargaining problems
with more than two agents, none of the well known bargaining solutions are
ordinal. Searching for meaningful ordinal solutions, especially for the
bilateral bargaining problem, has been a challenging issue in bargaining theory
for more than three decades. This paper proposes a logic-based ordinal solution
to the bilateral bargaining problem. We argue that if a bargaining problem is
modeled in terms of the logical relation of players physical negotiation items,
a meaningful bargaining solution can be constructed based on the ordinal
structure of bargainers preferences. We represent bargainers demands in
propositional logic and bargainers preferences over their demands in total
preorder. We show that the solution satisfies most desirable logical
properties, such as individual rationality (logical version), consistency,
collective rationality as well as a few typical game-theoretic properties, such
as weak Pareto optimality and contraction invariance. In addition, if all
players demand sets are logically closed, the solution satisfies a fixed-point
condition, which says that the outcome of a negotiation is the result of mutual
belief revision. Finally, we define various decision problems in relation to
our bargaining model and study their computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5202</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5202</id><created>2014-01-16</created><authors><author><keyname>Rahman</keyname><forenames>Altaf</forenames></author><author><keyname>Ng</keyname><forenames>Vincent</forenames></author></authors><title>Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference
  Resolution</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 40, pages
  469-521, 2011</journal-ref><doi>10.1613/jair.3120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional learning-based coreference resolvers operate by training the
mention-pair model for determining whether two mentions are coreferent or not.
Though conceptually simple and easy to understand, the mention-pair model is
linguistically rather unappealing and lags far behind the heuristic-based
coreference models proposed in the pre-statistical NLP era in terms of
sophistication. Two independent lines of recent research have attempted to
improve the mention-pair model, one by acquiring the mention-ranking model to
rank preceding mentions for a given anaphor, and the other by training the
entity-mention model to determine whether a preceding cluster is coreferent
with a given mention. We propose a cluster-ranking approach to coreference
resolution, which combines the strengths of the mention-ranking model and the
entity-mention model, and is therefore theoretically more appealing than both
of these models. In addition, we seek to improve cluster rankers via two
extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity
by jointly modeling anaphoricity determination and coreference resolution.
Experimental results on the ACE data sets demonstrate the superior performance
of cluster rankers to competing approaches as well as the effectiveness of our
two extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5203</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5203</id><created>2014-01-20</created><authors><author><keyname>Thanh</keyname><forenames>Ta Minh</forenames></author><author><keyname>Iwakiri</keyname><forenames>Munetoshi</forenames></author></authors><title>An Incomplete Cryptography based Digital Rights Management with DCFF</title><categories>cs.CR cs.MM</categories><journal-ref>Journal of Soft Computing and Software Engineering [JSCSE], Vol.
  3, No. 3, pp. 507-513, 2013</journal-ref><doi>10.7321/jscse.v3.n3.77</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, DRM (Digital Rights Management) system is responsible for the
safe distribution of digital content, however, DRM system is achieved with
individual function modules of cryptography, watermarking and so on. In this
typical system flow, it has a problem that all original digital contents are
temporarily disclosed with perfect condition via decryption process. In this
paper, we propose the combination of the differential codes and fragile
fingerprinting (DCFF) method based on incomplete cryptography that holds
promise for a better compromise between practicality and security for emerging
digital rights management applications. Experimental results with simulation
confirmed that DCFF keeps compatibility with standard JPEG codec, and revealed
that the proposed method is suitable for DRM in the network distribution
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5206</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5206</id><created>2014-05-20</created><authors><author><keyname>Huang</keyname><forenames>Xiaohui</forenames></author><author><keyname>Hu</keyname><forenames>Xing</forenames></author><author><keyname>Jiang</keyname><forenames>Weichang</forenames></author><author><keyname>Yang</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author></authors><title>Application of Multilayer Feedforward Neural Networks in Predicting Tree
  Height and Forest Stock Volume of Chinese Fir</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wood increment is critical information in forestry management. Previous
studies used mathematics models to describe complex growing pattern of forest
stand, in order to determine the dynamic status of growing forest stand in
multiple conditions. In our research, we aimed at studying non-linear
relationships to establish precise and robust Artificial Neural Networks (ANN)
models to predict the precise values of tree height and forest stock volume
based on data of Chinese fir. Results show that Multilayer Feedforward Neural
Networks with 4 nodes (MLFN-4) can predict the tree height with the lowest RMS
error (1.77); Multilayer Feedforward Neural Networks with 7 nodes (MLFN-7) can
predict the forest stock volume with the lowest RMS error (4.95). The training
and testing process have proved that our models are precise and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5208</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5208</id><created>2014-01-22</created><authors><author><keyname>Rush</keyname><forenames>Alexander M.</forenames></author><author><keyname>Collins</keyname><forenames>Michael</forenames></author></authors><title>A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference
  in Natural Language Processing</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 45, pages
  305-362, 2012</journal-ref><doi>10.1613/jair.3680</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual decomposition, and more generally Lagrangian relaxation, is a classical
method for combinatorial optimization; it has recently been applied to several
inference problems in natural language processing (NLP). This tutorial gives an
overview of the technique. We describe example algorithms, describe formal
guarantees for the method, and describe practical issues in implementing the
algorithms. While our examples are predominantly drawn from the NLP literature,
the material should be of general relevance to inference problems in machine
learning. A central theme of this tutorial is that Lagrangian relaxation is
naturally applied in conjunction with a broad class of combinatorial
algorithms, allowing inference in models that go significantly beyond previous
work on Lagrangian relaxation for inference in graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5209</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5209</id><created>2014-01-24</created><authors><author><keyname>Haslegrave</keyname><forenames>John</forenames></author></authors><title>Bounds on Herman's algorithm</title><categories>cs.DS</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herman's self-stabilisation algorithm allows a ring of $N$ processors having
any odd number of tokens to reach a stable state where exactly one token
remains. McIver and Morgan conjecture that the expected time taken for
stabilisation is maximised when there are three equally-spaced tokens. We prove
exact results on a related cost function, and obtain a bound on expected time
which is very close to the conjectured bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5210</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5210</id><created>2014-05-20</created><authors><author><keyname>&#x106;usti&#x107;</keyname><forenames>Ante</forenames></author><author><keyname>Klinz</keyname><forenames>Bettina</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Planar 3-dimensional assignment problems with Monge-like cost arrays</title><categories>math.OC cs.DS</categories><comments>16 pages, appendix will follow in v2</comments><msc-class>90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an $n\times n\times p$ cost array $C$ we consider the problem $p$-P3AP
which consists in finding $p$ pairwise disjoint permutations
$\varphi_1,\varphi_2,\ldots,\varphi_p$ of $\{1,\ldots,n\}$ such that
$\sum_{k=1}^{p}\sum_{i=1}^nc_{i\varphi_k(i)k}$ is minimized. For the case $p=n$
the planar 3-dimensional assignment problem P3AP results.
  Our main result concerns the $p$-P3AP on cost arrays $C$ that are layered
Monge arrays. In a layered Monge array all $n\times n$ matrices that result
from fixing the third index $k$ are Monge matrices. We prove that the $p$-P3AP
and the P3AP remain NP-hard for layered Monge arrays. Furthermore, we show that
in the layered Monge case there always exists an optimal solution of the
$p$-3PAP which can be represented as matrix with bandwidth $\le 4p-3$. This
structural result allows us to provide a dynamic programming algorithm that
solves the $p$-P3AP in polynomial time on layered Monge arrays when $p$ is
fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5245</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5245</id><created>2014-05-20</created><authors><author><keyname>Tumolo</keyname><forenames>G.</forenames></author><author><keyname>Bonaventura</keyname><forenames>L.</forenames></author></authors><title>An accurate and efficient numerical framework for adaptive numerical
  weather prediction</title><categories>math.NA cs.CE cs.NA physics.ao-ph physics.comp-ph physics.flu-dyn</categories><msc-class>35L02, 65M60, 65M25, 76U05, 86A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an accurate and efficient discretization approach for the adaptive
discretization of typical model equations employed in numerical weather
prediction. A semi-Lagrangian approach is combined with the TR-BDF2
semi-implicit time discretization method and with a spatial discretization
based on adaptive discontinuous finite elements. The resulting method has full
second order accuracy in time and can employ polynomial bases of arbitrarily
high degree in space, is unconditionally stable and can effectively adapt the
number of degrees of freedom employed in each element, in order to balance
accuracy and computational cost. The p-adaptivity approach employed does not
require remeshing, therefore it is especially suitable for applications, such
as numerical weather prediction, in which a large number of physical quantities
are associated with a given mesh. Furthermore, although the proposed method can
be implemented on arbitrary unstructured and nonconforming meshes, even its
application on simple Cartesian meshes in spherical coordinates can cure
effectively the pole problem by reducing the polynomial degree used in the
polar elements. Numerical simulations of classical benchmarks for the shallow
water and for the fully compressible Euler equations validate the method and
demonstrate its capability to achieve accurate results also at large Courant
numbers, with time steps up to 100 times larger than those of typical explicit
discretizations of the same problems, while reducing the computational cost
thanks to the adaptivity algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5248</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5248</id><created>2014-05-20</created><authors><author><keyname>jayech</keyname><forenames>Khaoula</forenames></author><author><keyname>Trimech</keyname><forenames>Nesrine</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Amara</keyname><forenames>Najoua Essoukri Ben</forenames></author></authors><title>Dynamic Hierarchical Bayesian Network for Arabic Handwritten Word
  Recognition</title><categories>cs.CV</categories><comments>Fourth International Conference on Information and Communication
  Technology and Accessibility (ICTA), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new probabilistic graphical model used to model and
recognize words representing the names of Tunisian cities. In fact, this work
is based on a dynamic hierarchical Bayesian network. The aim is to find the
best model of Arabic handwriting to reduce the complexity of the recognition
process by permitting the partial recognition. Actually, we propose a
segmentation of the word based on smoothing the vertical histogram projection
using different width values to reduce the error of segmentation. Then, we
extract the characteristics of each cell using the Zernike and HU moments,
which are invariant to rotation, translation and scaling. Our approach is
tested using the IFN / ENIT database, and the experiment results are very
promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5249</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5249</id><created>2014-05-20</created><authors><author><keyname>Anis</keyname><forenames>Elbahi</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>Hidden Markov Model for Inferring Learner Task Using Mouse Movement</title><categories>cs.HC</categories><comments>Fourth International Conference on Information and Communication
  Technology and Accessibility (ICTA), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the issues of e-learning web based application is to understand how
the learner interacts with an e-learning application to perform a given task.
This study proposes a methodology to analyze learner mouse movement in order to
infer the task performed. To do this, a Hidden Markov Model is used for
modeling the interaction of the learner with an e-learning application. The
obtained results show the ability of our model to analyze the interaction in
order to recognize the task performed by the learner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5254</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5254</id><created>2014-05-20</created><updated>2015-10-18</updated><authors><author><keyname>Stahlke</keyname><forenames>Dan</forenames></author></authors><title>Quantum source-channel coding and non-commutative graph theory</title><categories>quant-ph cs.IT math.IT</categories><comments>24 pages</comments><doi>10.1109/TIT.2015.2496377</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alice and Bob receive a bipartite state (possibly entangled) from some finite
collection or from some subspace. Alice sends a message to Bob through a noisy
quantum channel such that Bob may determine the initial state, with zero chance
of error. This framework encompasses, for example, teleportation, dense coding,
entanglement assisted quantum channel capacity, and one-way communication
complexity of function evaluation.
  With classical sources and channels, this problem can be analyzed using graph
homomorphisms. We show this quantum version can be analyzed using homomorphisms
on non-commutative graphs (an operator space generalization of graphs).
Previously the Lov\'{a}sz $\vartheta$ number has been generalized to
non-commutative graphs; we show this to be a homomorphism monotone, thus
providing bounds on quantum source-channel coding. We generalize the Schrijver
and Szegedy numbers, and show these to be monotones as well. As an application
we construct a quantum channel whose entanglement assisted zero-error one-shot
capacity can only be unlocked by using a non-maximally entangled state.
  These homomorphisms allow definition of a chromatic number for
non-commutative graphs. Many open questions are presented regarding the
possibility of a more fully developed theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5263</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5263</id><created>2014-05-20</created><authors><author><keyname>Makonin</keyname><forenames>Stephen</forenames></author><author><keyname>Kashani</keyname><forenames>Maryam H.</forenames></author><author><keyname>Bartram</keyname><forenames>Lyn</forenames></author></authors><title>The Affect of Lifestyle Factors on Eco-Visualization Design</title><categories>cs.HC</categories><comments>In the proceedings of Computer Graphics International (CGI) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As people become more concerned with the need to conserve their power
consumption we need to find ways to inform them of how electricity is being
consumed within the home. There are a number of devices that have been designed
using different forms, sizes, and technologies. We are interested in large
ambient displays that can be read at a glance and from a distance as
informative art. However, from these objectives come a number of questions that
need to be explored and answered. To what degree might lifestyle factors
influence the design of eco-visualizations? To answer this we need to ask how
people with varying lifestyle factors perceive the utility of such devices and
their placement within a home. We explore these questions by creating four
ambient display prototypes. We take our prototypes and subject them to a user
study to gain insight as to the questions posed above. This paper discusses our
prototypes in detail and the results and findings of our user study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5268</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5268</id><created>2014-05-20</created><updated>2014-07-09</updated><authors><author><keyname>Dachman-Soled</keyname><forenames>Dana</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author><author><keyname>Wimmer</keyname><forenames>Karl</forenames></author></authors><title>Approximate resilience, monotonicity, and the complexity of agnostic
  learning</title><categories>cs.LG cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function $f$ is $d$-resilient if all its Fourier coefficients of degree at
most $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. We
study the notion of $\mathit{approximate}$ $\mathit{resilience}$ of Boolean
functions, where we say that $f$ is $\alpha$-approximately $d$-resilient if $f$
is $\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\ell_1$
distance. We show that approximate resilience essentially characterizes the
complexity of agnostic learning of a concept class $C$ over the uniform
distribution. Roughly speaking, if all functions in a class $C$ are far from
being $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ and
conversely, if $C$ contains a function close to being $d$-resilient then
agnostic learning of $C$ in the statistical query (SQ) framework of Kearns has
complexity of at least $n^{\Omega(d)}$. This characterization is based on the
duality between $\ell_1$ approximation by degree-$d$ polynomials and
approximate $d$-resilience that we establish. In particular, it implies that
$\ell_1$ approximation by low-degree polynomials, known to be sufficient for
agnostic learning over product distributions, is in fact necessary.
  Focusing on monotone Boolean functions, we exhibit the existence of
near-optimal $\alpha$-approximately
$\widetilde{\Omega}(\alpha\sqrt{n})$-resilient monotone functions for all
$\alpha&gt;0$. Prior to our work, it was conceivable even that every monotone
function is $\Omega(1)$-far from any $1$-resilient function. Furthermore, we
construct simple, explicit monotone functions based on ${\sf Tribes}$ and ${\sf
CycleRun}$ that are close to highly resilient functions. Our constructions are
based on a fairly general resilience analysis and amplification. These
structural results, together with the characterization, imply nearly optimal
lower bounds for agnostic learning of monotone juntas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5272</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5272</id><created>2014-05-20</created><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>University of Szeged</affiliation></author><author><keyname>F&#xfc;l&#xf6;p</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>University of Szeged</affiliation></author></authors><title>Proceedings 14th International Conference on Automata and Formal
  Languages</title><categories>cs.FL</categories><proxy>EPTCS</proxy><acm-class>F.1.1;F.1.2;F.4.2;F.4.3</acm-class><journal-ref>EPTCS 151, 2014</journal-ref><doi>10.4204/EPTCS.151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 14th International Conference Automata and Formal Languages (AFL 2014)
was held in Szeged, Hungary, from the 27th to the 29th of May, 2014. The
conference was organized by the Department of Foundations of Computer Science
of the University of Szeged. Topics of interest covered the theory and
applications of automata and formal languages and related areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5278</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5278</id><created>2014-05-20</created><authors><author><keyname>Yu</keyname><forenames>Long</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>The weight distribution of a family of p-ary cyclic codes</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let m, k be positive integers, p be an odd prime and $\pi $ be a primitive
element of $\mathbb{F}_{p^m}$. In this paper, we determine the weight
distribution of a family of cyclic codes $\mathcal{C}_t$ over $\mathbb{F}_p$,
whose duals have two zeros $\pi^{-t}$ and $-\pi^{-t}$, where $t$ satisfies
$t\equiv \frac{p^k+1}{2}p^\tau \ ({\rm mod}\ \frac{p^m-1}{2}) $ for some $\tau
\in \{0,1,\cdots, m-1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5279</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5279</id><created>2014-05-20</created><authors><author><keyname>Fernandes</keyname><forenames>Ricardo Q. A.</forenames></author><author><keyname>Haeusler</keyname><forenames>Edward H.</forenames></author><author><keyname>Pereira</keyname><forenames>Luiz Carlos</forenames></author></authors><title>Intuitionistic PUC-Logic for Constructive Counterfactuals</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1402.1535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the intuitionistic version of PUC-Logic. After that, we present a
constructive approach to Lewis' counterfactual abstraction to show that it does
not require the classical absurd rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5287</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5287</id><created>2014-05-20</created><authors><author><keyname>Ahmad</keyname><forenames>Rabiah</forenames></author><author><keyname>Sahib</keyname><forenames>Shahrin</forenames></author><author><keyname>Nor'Azuwa</keyname><forenames>Muhamad Pahri</forenames></author></authors><title>Effective Measurement Requirements for Network Security Management</title><categories>cs.CR</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS) , Vol. 12, No. 4, April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technical security metrics provide measurements in ensuring the effectiveness
of technical security controls or technology devices/objects that are used in
protecting the information systems. However, lack of understanding and method
to develop the technical security metrics may lead to unachievable security
control objectives and incompetence of the implementation. This paper proposes
a model of technical security metric to measure the effectiveness of network
security management. The measurement is based on the effectiveness of security
performance for (1) network security controls such as firewall, Intrusion
Detection Prevention System (IDPS), switch, wireless access point, wireless
controllers and network architecture; and (2) network services such as
Hypertext Transfer Protocol Secure (HTTPS) and virtual private network (VPN).
We use the Goal-Question-Metric (GQM) paradigm [1] which links the measurement
goals to measurement questions and produce the metrics that can easily be
interpreted in compliance with the requirements. The outcome of this research
method is the introduction of network security management metric as an
attribute to the Technical Security Metric (TSM) model. Apparently, the
proposed TSM model may provide guidance for organizations in complying with
effective measurement requirements of ISO/IEC 27001 Information Security
Management System (ISMS) standard. The proposed model will provide a
comprehensive measurement and guidance to support the use of ISO/IEC 27004 ISMS
Measurement template.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5293</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5293</id><created>2014-05-21</created><authors><author><keyname>Krone</keyname><forenames>Robert</forenames></author></authors><title>Numerical Hilbert functions for Macaulay2</title><categories>math.AC cs.SC math.AG</categories><comments>5 pages</comments><msc-class>14Q99, 68N01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NumericalHilbert package for Macaulay2 includes algorithms for computing
local dual spaces of polynomial ideals, and related local combinatorial data
about its scheme structure. These techniques are numerically stable, and can be
used with floating point arithmetic over the complex numbers. They provide a
viable alternative in this setting to purely symbolic methods such as standard
bases. In particular, these methods can be used to compute initial ideals,
local Hilbert functions and Hilbert regularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5300</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5300</id><created>2014-05-21</created><updated>2014-07-27</updated><authors><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Fast Distributed Coordinate Descent for Non-Strongly Convex Losses</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient distributed randomized coordinate descent method for
minimizing regularized non-strongly convex loss functions. The method attains
the optimal $O(1/k^2)$ convergence rate, where $k$ is the iteration counter.
The core of the work is the theoretical study of stepsize parameters. We have
implemented the method on Archer - the largest supercomputer in the UK - and
show that the method is capable of solving a (synthetic) LASSO optimization
problem with 50 billion variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5302</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5302</id><created>2014-05-21</created><authors><author><keyname>Wang</keyname><forenames>Hai</forenames></author><author><keyname>Chen</keyname><forenames>Zhe</forenames></author><author><keyname>Gong</keyname><forenames>Qingyuan</forenames></author><author><keyname>Xu</keyname><forenames>Weidong</forenames></author><author><keyname>Zhang</keyname><forenames>Xu</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Prometheus: LT Codes Meet Cooperative Transmission in Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following fast growth of cellular networks, more users have drawn attention
to the contradiction between dynamic user data traffic and static data plans.
To address this important but largely unexplored issue, in this paper, we
design a new data plan sharing system named Prometheus, which is based on the
scenario that some smartphone users have surplus data traffic and are willing
to help others download data. To realize this system, we first propose a
mechanism that incorporates LT codes into UDP. It is robust to transmission
errors and encourages more concurrent transmissions and forwardings. It also
can be implemented easily with low implementation complexity. Then we design an
incentive mechanism using a Stackelberg game to choose assistant users ($AUs$),
all participants will gain credits in return, which can be used to ask for
future help when they need to download something. Finally real environment
experiments are conducted and the results show that users in our Prometheus not
only can manage their surplus data plan more efficiently, but also achieve a
higher speed download rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5311</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5311</id><created>2014-05-21</created><authors><author><keyname>Ghosh</keyname><forenames>Atanu Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Arnab</forenames></author></authors><title>Compressive Sampling Using EM Algorithm</title><categories>stat.ME cs.LG stat.ML</categories><comments>9 pages, 4 figures. This paper has been published as a technical
  report in Applied Statistics Unit in Indian Statistical Institute, Kolkata</comments><report-no>Technical Report No: ASU/2014/4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional approaches of sampling signals follow the celebrated theorem of
Nyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and
Tao, is a new paradigm that goes against the conventional methods in data
acquisition and provides a way of recovering signals using fewer samples than
the traditional methods use. Here we suggest an alternative way of
reconstructing the original signals in compressive sampling using EM algorithm.
We first propose a naive approach which has certain computational difficulties
and subsequently modify it to a new approach which performs better than the
conventional methods of compressive sampling. The comparison of the different
approaches and the performance of the new approach has been studied using
simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5320</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5320</id><created>2014-05-21</created><authors><author><keyname>Jain</keyname><forenames>Shikha</forenames></author></authors><title>Security Threats in MANETS : A Review</title><categories>cs.NI cs.CR</categories><comments>14 pages, 7 figures, 1 table</comments><journal-ref>International Journal on Information Theory (IJIT), Vol.3, No.2,
  April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad hoc networks are the special networks formed for specific applications.
Operating in ad-hoc mode allows all wireless devices within range of each other
to discover and communicate in a peer-to-peer fashion without involving central
access points. Many routing protocols like AODV, DSR etc have been proposed for
these networks to find an end to end path between the nodes. These routing
protocols are prone to attacks by the malicious nodes. There is a need to
detect and prevent these attacks in a timely manner before destruction of
network services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5326</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5326</id><created>2014-05-21</created><authors><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author><author><keyname>Zamani</keyname><forenames>Mahdi</forenames></author></authors><title>Secure Anonymous Broadcast</title><categories>cs.DC cs.CR</categories><comments>18 Pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In anonymous broadcast, one or more parties want to anonymously send messages
to all parties. This problem is increasingly important as a black-box in many
privacy-preserving applications such as anonymous communication, distributed
auctions, and multi-party computation. In this paper, we design decentralized
protocols for anonymous broadcast that require each party to send (and compute)
a polylogarithmic number of bits (and operations) per anonymous bit delivered
with $O(\log n)$ rounds of communication. Our protocol is provably secure
against traffic analysis, does not require any trusted party, and is completely
load-balanced. The protocol tolerates up to $n/6$ statically-scheduled
Byzantine parties that are controlled by a computationally unbounded adversary.
Our main strategy for achieving scalability is to perform local communications
(and computations) among a logarithmic number of parties. We provide simulation
results to show that our protocol improves significantly over previous work. We
finally show that using a common cryptographic tool in our protocol one can
achieve practical results for anonymous broadcast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5329</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5329</id><created>2014-05-21</created><updated>2015-11-06</updated><authors><author><keyname>Kipnis</keyname><forenames>Alon</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Distortion-Rate Function of Sub-Nyquist Sampled Gaussian Sources</title><categories>cs.IT math.IT</categories><comments>Accepted for publication at the IEEE transactions on information
  theory</comments><journal-ref>Information Theory, IEEE Transactions on , vol.62, no.1,
  pp.401-429, Jan. 2016</journal-ref><doi>10.1109/TIT.2015.2485271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of information lost in sub-Nyquist sampling of a continuous-time
Gaussian stationary process is quantified. We consider a combined source coding
and sub-Nyquist reconstruction problem in which the input to the encoder is a
noisy sub-Nyquist sampled version of the analog source. We first derive an
expression for the mean squared error in the reconstruction of the process from
a noisy and information rate-limited version of its samples. This expression is
a function of the sampling frequency and the average number of bits describing
each sample. It is given as the sum of two terms: Minimum mean square error in
estimating the source from its noisy but otherwise fully observed sub-Nyquist
samples, and a second term obtained by reverse waterfilling over an average of
spectral densities associated with the polyphase components of the source. We
extend this result to multi-branch uniform sampling, where the samples are
available through a set of parallel channels with a uniform sampler and a
pre-sampling filter in each branch. Further optimization to reduce distortion
is then performed over the pre-sampling filters, and an optimal set of
pre-sampling filters associated with the statistics of the input signal and the
sampling frequency is found. This results in an expression for the minimal
possible distortion achievable under any analog to digital conversion scheme
involving uniform sampling and linear filtering. These results thus unify the
Shannon-Whittaker-Kotelnikov sampling theorem and Shannon rate-distortion
theory for Gaussian sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5336</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5336</id><created>2014-05-21</created><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Fundamental Limits of Caching in Wireless D2D Networks</title><categories>cs.IT math.IT</categories><comments>45 pages, 5 figures, Submitted to IEEE Transactions on Information
  Theory, This is the extended version of the conference (ITW) paper
  arXiv:1304.5856</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless Device-to-Device (D2D) network where communication is
restricted to be single-hop. Users make arbitrary requests from a finite
library of files and have pre-cached information on their devices, subject to a
per-node storage capacity constraint. A similar problem has already been
considered in an ``infrastructure'' setting, where all users receive a common
multicast (coded) message from a single omniscient server (e.g., a base station
having all the files in the library) through a shared bottleneck link. In this
work, we consider a D2D ``infrastructure-less'' version of the problem. We
propose a caching strategy based on deterministic assignment of subpackets of
the library files, and a coded delivery strategy where the users send linearly
coded messages to each other in order to collectively satisfy their demands. We
also consider a random caching strategy, which is more suitable to a fully
decentralized implementation. Under certain conditions, both approaches can
achieve the information theoretic outer bound within a constant multiplicative
factor. In our previous work, we showed that a caching D2D wireless network
with one-hop communication, random caching, and uncoded delivery, achieves the
same throughput scaling law of the infrastructure-based coded multicasting
scheme, in the regime of large number of users and files in the library. This
shows that the spatial reuse gain of the D2D network is order-equivalent to the
coded multicasting gain of single base station transmission. It is therefore
natural to ask whether these two gains are cumulative, i.e.,if a D2D network
with both local communication (spatial reuse) and coded multicasting can
provide an improved scaling law. Somewhat counterintuitively, we show that
these gains do not cumulate (in terms of throughput scaling law).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5340</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5340</id><created>2014-05-21</created><authors><author><keyname>Thakur</keyname><forenames>Manish K</forenames></author><author><keyname>Saxena</keyname><forenames>Vikas</forenames></author><author><keyname>Gupta</keyname><forenames>J P</forenames></author></authors><title>A hybrid video quality metric for analyzing quality degradation due to
  frame drop</title><categories>cs.MM</categories><comments>7 pages, 9 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 6, No 1, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In last decade, ever growing internet technologies provided platform to share
the multimedia data among different communities. As the ultimate users are
human subjects who are concerned about quality of visual information, it is
often desired to have good resumed perceptual quality of videos, thus arises
the need of quality assessment. This paper presents a full reference hybrid
video quality metric which is capable to analyse the video quality for
spatially or temporally (frame drop) or spatio-temporally distorted video
sequences. Simulated results show that the metric efficiently analyses the
quality degradation and more closer to the developed human visual system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5341</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5341</id><created>2014-05-21</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author></authors><title>A fast algorithm for computing the characteristic polynomial of the
  p-curvature</title><categories>cs.SC</categories><comments>ISSAC - 39th International Symposium on Symbolic and Algebraic
  Computation (2014)</comments><proxy>ccsd</proxy><doi>10.1145/2608628.2608650</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss theoretical and algorithmic questions related to the $p$-curvature
of differential operators in characteristic $p$. Given such an operator $L$,
and denoting by $\Chi(L)$ the characteristic polynomial of its $p$-curvature,
we first prove a new, alternative, description of $\Chi(L)$. This description
turns out to be particularly well suited to the fast computation of $\Chi(L)$
when $p$ is large: based on it, we design a new algorithm for computing
$\Chi(L)$, whose cost with respect to $p$ is $\softO(p^{0.5})$ operations in
the ground field. This is remarkable since, prior to this work, the fastest
algorithms for this task, and even for the subtask of deciding nilpotency of
the $p$-curvature, had merely slightly subquadratic complexity
$\softO(p^{1.79})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5342</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5342</id><created>2014-05-21</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Combot</keyname><forenames>Thierry</forenames><affiliation>IMB</affiliation></author><author><keyname>Mohab</keyname><forenames>Safey El Din</forenames><affiliation>UPMC</affiliation></author></authors><title>Computing necessary integrability conditions for planar parametrized
  homogeneous potentials</title><categories>cs.SC</categories><comments>ISSAC'14 - International Symposium on Symbolic and Algebraic
  Computation (2014)</comments><proxy>ccsd</proxy><doi>10.1145/2608628.2608662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $V\in\mathbb{Q}(i)(\a_1,\dots,\a_n)(\q_1,\q_2)$ be a rationally
parametrized planar homogeneous potential of homogeneity degree $k\neq -2, 0,
2$. We design an algorithm that computes polynomial \emph{necessary} conditions
on the parameters $(\a_1,\dots,\a_n)$ such that the dynamical system associated
to the potential $V$ is integrable. These conditions originate from those of
the Morales-Ramis-Sim\'o integrability criterion near all Darboux points. The
implementation of the algorithm allows to treat applications that were out of
reach before, for instance concerning the non-integrability of polynomial
potentials up to degree $9$. Another striking application is the first complete
proof of the non-integrability of the \emph{collinear three body problem}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5345</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5345</id><created>2014-05-21</created><updated>2014-06-12</updated><authors><author><keyname>Lallement</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>de Silva</keyname><forenames>Lavindra</forenames></author><author><keyname>Alami</keyname><forenames>Rachid</forenames></author></authors><title>HATP: An HTN Planner for Robotics</title><categories>cs.RO cs.AI</categories><comments>2nd ICAPS Workshop on Planning and Robotics, PlanRob 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Hierarchical Task Network (HTN) planning is a popular approach that cuts down
on the classical planning search space by relying on a given hierarchical
library of domain control knowledge. This provides an intuitive methodology for
specifying high-level instructions on how robots and agents should perform
tasks, while also giving the planner enough flexibility to choose the
lower-level steps and their ordering. In this paper we present the HATP
(Hierarchical Agent-based Task Planner) planning framework which extends the
traditional HTN planning domain representation and semantics by making them
more suitable for roboticists, and treating agents as &quot;first class&quot; entities in
the language. The former is achieved by allowing &quot;social rules&quot; to be defined
which specify what behaviour is acceptable/unacceptable by the agents/robots in
the domain, and interleaving planning with geometric reasoning in order to
validate online -with respect to a detailed geometric 3D world- the human/robot
actions currently being pursued by HATP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5351</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5351</id><created>2014-05-21</created><updated>2015-07-27</updated><authors><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Improving Energy Efficiency in Upstream EPON Channels by Packet
  Coalescing</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Communications 01/2012; 60(4):929-932</journal-ref><doi>10.1109/TCOMM.2012.022712.110142A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we research the feasibility of adapting the packet coalescing
algorithm, used successfully in IEEE 802.3az Ethernet cards, to upstream EPON
channels. Our simulation experiments show that, using this algorithm, great
power savings are feasible without requiring any changes to the deployed access
network infrastructure nor to protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5358</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5358</id><created>2014-05-21</created><authors><author><keyname>Harutyunyan</keyname><forenames>Anna</forenames></author><author><keyname>Brys</keyname><forenames>Tim</forenames></author><author><keyname>Vrancx</keyname><forenames>Peter</forenames></author><author><keyname>Nowe</keyname><forenames>Ann</forenames></author></authors><title>Off-Policy Shaping Ensembles in Reinforcement Learning</title><categories>cs.AI cs.LG</categories><comments>Full version of the paper to appear in Proc. ECAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances of gradient temporal-difference methods allow to learn
off-policy multiple value functions in parallel with- out sacrificing
convergence guarantees or computational efficiency. This opens up new
possibilities for sound ensemble techniques in reinforcement learning. In this
work we propose learning an ensemble of policies related through
potential-based shaping rewards. The ensemble induces a combination policy by
using a voting mechanism on its components. Learning happens in real time, and
we empirically show the combination policy to outperform the individual
policies of the ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5364</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5364</id><created>2014-05-21</created><updated>2015-09-01</updated><authors><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>The persistent congestion problem of FAST-TCP: analysis and solutions</title><categories>cs.NI</categories><journal-ref>European Transactions on Telecommunications, vol. 21, p. 504-518,
  2010</journal-ref><doi>10.1002/ett.1407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FAST-TCP achieves better performance than traditional TCP-Reno schemes, but
unfortunately it is inherently unfair to older connections due to wrong
estimations of the round-trip propagation delay.
  This paper presents a model for this anomalous behavior of FAST flows, known
as the persistent congestion problem. We first develop an elementary analysis
for a scenario with just two flows, and then build up the general case with an
arbitrary number of flows. The model correctly quantifies how much unfairness
shows up among the different connections, confirming experimental observations
made by several previous studies.
  We built on this model to develop an algorithm to obtain a good estimate of
the propagation delay for FAST-TCP that enables to achieve fairness between
aged and new connections while preserving the high throughput and low buffer
occupancy of the original protocol. Furthermore, our proposal only requires a
modification of the sender host, avoiding the need to upgrade the intermediate
routers in any way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5365</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5365</id><created>2014-05-21</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>Miguel Rodr&#xed;guez</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Common Problems in Delay-Based Congestion Control Algorithms: A Gallery
  of Solutions</title><categories>cs.NI</categories><journal-ref>Eur. Trans. on Telecommunications, vol. 22, p. 168-179, 2011</journal-ref><doi>10.1002/ett.1485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although delay-based congestion control protocols such as FAST promise to
deliver better performance than traditional TCP Reno, they have not yet been
widely incorporated to the Internet. Several factors have contributed to their
lack of deployment. Probably, the main contributing factor is that they are not
able to compete fairly against loss-based congestion control protocols. In
fact, the transmission rate in equilibrium of delay-based approaches is always
less than their fair share when they share the network with traditional
TCP-Reno derivatives, that employ packet losses as their congestion signal.
There are also other performance impairments caused by the sensitivity to
errors in the measurement of the congestion signal (queuing delay) that reduce
the efficiency and the intra-protocol fairness of the algorithms. In this paper
we report, analyze and discuss some recent proposals in the literature to
improve the dynamic behavior of delay-based congestion control algorithms, and
FAST in particular. Coexistence of sources reacting differently to congestion,
identifying congestion appearance in the reverse path and the persistent
congestion problem are the issues specifically addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5371</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5371</id><created>2014-05-21</created><updated>2014-11-27</updated><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Single machine scheduling problems with uncertain parameters and the OWA
  criterion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a class of single machine scheduling problems is discussed. It
is assumed that job parameters, such as processing times, due dates, or weights
are uncertain and their values are specified in the form of a discrete scenario
set. The Ordered Weighted Averaging (OWA) aggregation operator is used to
choose an optimal schedule. The OWA operator generalizes traditional criteria
in decision making under uncertainty, such as the maximum, average, median or
Hurwicz criterion. It also allows us to extend the robust approach to
scheduling by taking into account various attitudes of decision makers towards
the risk. In this paper a general framework for solving single machine
scheduling problems with the OWA criterion is proposed and some positive and
negative computational results for two basic single machine scheduling problems
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5376</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5376</id><created>2014-05-21</created><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Complexity of the robust weighted independent set problems on interval
  graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the max-min and min-max regret versions of the maximum
weighted independent set problem on interval graphswith uncertain vertex
weights. Both problems have been recently investigated by Nobibon and Leus
(2014), who showed that they are NP-hard for two scenarios and strongly NP-hard
if the number of scenarios is a part of the input. In this paper, new
complexity and approximation results on the problems under consideration are
provided, which extend the ones previously obtained. Namely, for the discrete
scenario uncertainty representation it is proven that if the number of
scenarios $K$ is a part of the input, then the max-min version of the problem
is not at all approximable. On the other hand, its min-max regret version is
approximable within $K$ and not approximable within $O(\log^{1-\epsilon}K)$ for
any $\epsilon&gt;0$ unless the problems in NP have quasi polynomial algorithms.
Furthermore, for the interval uncertainty representation it is shown that the
min-max regret version is NP-hard and approximable within 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5381</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5381</id><created>2014-05-21</created><updated>2014-11-13</updated><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Kurpisz</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Approximability of the robust representatives selection problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper new complexity and approximation results on the robust versions
of the representatives selection problem, under the scenario uncertainty
representation, are provided, which extend the results obtained in the recent
papers by Dolgui and Kovalev (2012), and Deineko and Woeginger (2013). Namely,
it is shown that if the number of scenarios is a part of input, then the
min-max (regret) representatives selection problem is not approximable within a
ratio of $O(\log^{1-\epsilon}K)$ for any $\epsilon&gt;0$, where $K$ is the number
of scenarios, unless the problems in NP have quasi-polynomial time algorithms.
An approximation algorithm with an approximation ratio of $O(\log K/ \log \log
K)$ for the min-max version of the problem is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5390</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5390</id><created>2014-05-21</created><authors><author><keyname>Hamidouche</keyname><forenames>Kenza</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Many-to-Many Matching Games for Proactive Social-Caching in Wireless
  Small Cell Networks</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the caching problem in small cell networks from a
game theoretic point of view. In particular, we formulate the caching problem
as a many-to-many matching game between small base stations and service
providers' servers. The servers store a set of videos and aim to cache these
videos at the small base stations in order to reduce the experienced delay by
the end-users. On the other hand, small base stations cache the videos
according to their local popularity, so as to reduce the load on the backhaul
links. We propose a new matching algorithm for the many-to-many problem and
prove that it reaches a pairwise stable outcome. Simulation results show that
the number of satisfied requests by the small base stations in the proposed
caching algorithm can reach up to three times the satisfaction of a random
caching policy. Moreover, the expected download time of all the videos can be
reduced significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5393</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5393</id><created>2014-05-21</created><updated>2016-03-07</updated><authors><author><keyname>Kerjean</keyname><forenames>Marie</forenames><affiliation>Laboratoire PPS</affiliation></author></authors><title>Weak topologies for Linear Logic</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 12 (1:3) 2016</journal-ref><doi>10.2168/LMCS-12(1:3)2016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a denotational model of linear logic, whose objects are all the
locally convex and separated topological vector spaces endowed with their weak
topology. The negation is interpreted as the dual, linear proofs are
interpreted as continuous linear functions, and non-linear proofs as sequences
of monomials. We do not complete our constructions by a double-orthogonality
operation. This yields an interpretation of the polarity of the connectives in
terms of topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5406</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5406</id><created>2014-05-21</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Wario</keyname><forenames>Fernando</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Circle detection on images using Learning Automata</title><categories>cs.CV</categories><comments>26 Pages</comments><journal-ref>ET Computer Vision 6 (2), (2012), pp. 121-132</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circle detection over digital images has received considerable attention from
the computer vision community over the last few years devoting a tremendous
amount of research seeking for an optimal detector. This article presents an
algorithm for the automatic detection of circular shapes from complicated and
noisy images with no consideration of conventional Hough transform principles.
The proposed algorithm is based on Learning Automata (LA) which is a
probabilistic optimization method that explores an unknown random environment
by progressively improving the performance via a reinforcement signal
(objective function). The approach uses the encoding of three non-collinear
points as a candidate circle over the edge image. A reinforcement signal
(matching function) indicates if such candidate circles are actually present in
the edge map. Guided by the values of such reinforcement signal, the
probability set of the encoded candidate circles is modified through the LA
algorithm so that they can fit to the actual circles on the edge map.
Experimental results over several complex synthetic and natural images have
validated the efficiency of the proposed technique regarding accuracy, speed
and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5410</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5410</id><created>2014-05-21</created><updated>2014-10-12</updated><authors><author><keyname>Mivule</keyname><forenames>Kato</forenames></author></authors><title>A Codon Frequency Obfuscation Heuristic for Raw Genomic Data Privacy</title><categories>cs.CR</categories><comments>8 Pages, 15 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genomic data provides clinical researchers with vast opportunities to study
various patient ailments. Yet the same data contains revealing information,
some of which a patient might want to remain concealed. The question then
arises: how can an entity transact in full DNA data while concealing certain
sensitive pieces of information in the genome sequence, and maintain DNA data
utility? As a response to this question, we propose a codon frequency
obfuscation heuristic, in which a redistribution of codon frequency values with
highly expressed genes is done in the same amino acid group, generating an
obfuscated DNA sequence. Our preliminary results show that it might be possible
to publish an obfuscated DNA sequence with a desired level of similarity
(utility) to the original DNA sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5421</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5421</id><created>2014-05-21</created><authors><author><keyname>Wang</keyname><forenames>Liqi</forenames></author><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author></authors><title>New quantum MDS codes derived from constacyclic codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum maximal-distance-separable (MDS) codes form an important class of
quantum codes. It is very hard to construct quantum MDS codes with relatively
large minimum distance. In this paper, based on classical constacyclic codes,
we construct two classes of quantum MDS codes with parameters
$$[[\lambda(q-1),\lambda(q-1)-2d+2,d]]_q$$ where $2\leq d\leq
(q+1)/2+\lambda-1$, and $q+1=\lambda r$ with $r$ even, and
$$[[\lambda(q-1),\lambda(q-1)-2d+2,d]]_q$$ where $2\leq d\leq
(q+1)/2+\lambda/2-1$, and $q+1=\lambda r$ with $r$ odd. The quantum MDS codes
exhibited here have parameters better than the ones available in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5422</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5422</id><created>2014-05-21</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Sanchez</keyname><forenames>Edgar</forenames></author><author><keyname>Ramirez</keyname><forenames>Marte</forenames></author></authors><title>Robust Fuzzy corner detector</title><categories>cs.CV</categories><comments>15 Pages</comments><journal-ref>Intelligent Automation and Soft Computing, 17 (4), (2011), pp.
  415-429</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable corner detection is an important task in determining the shape of
different regions within an image. Real-life image data are always imprecise
due to inherent uncertainties that may arise from the imaging process such as
defocusing, illumination changes, noise, etc. Therefore, the localization and
detection of corners has become a difficult task to accomplish under such
imperfect situations. On the other hand, Fuzzy systems are well known for their
efficient handling of impreciseness and incompleteness, which make them
inherently suitable for modelling corner properties by means of a rule-based
fuzzy system. The paper presents a corner detection algorithm which employs
such fuzzy reasoning. The robustness of the proposed algorithm is compared to
well-known conventional corner detectors and its performance is also tested
over a number of benchmark images to illustrate the efficiency of the algorithm
under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5443</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5443</id><created>2014-05-21</created><authors><author><keyname>Balduccini</keyname><forenames>Marcello</forenames></author><author><keyname>Regli</keyname><forenames>William C.</forenames></author><author><keyname>Nguyen</keyname><forenames>Duc N.</forenames></author></authors><title>Towards an ASP-Based Architecture for Autonomous UAVs in Dynamic
  Environments (Extended Abstract)</title><categories>cs.AI cs.MA</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP). arXiv
  admin note: substantial text overlap with arXiv:1405.1124</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional AI reasoning techniques have been used successfully in many
domains, including logistics, scheduling and game playing. This paper is part
of a project aimed at investigating how such techniques can be extended to
coordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments.
Specifically challenging are real-world environments where UAVs and other
network-enabled devices must communicate to coordinate -- and communication
actions are neither reliable nor free. Such network-centric environments are
common in military, public safety and commercial applications, yet most
research (even multi-agent planning) usually takes communications among
distributed agents as a given. We address this challenge by developing an agent
architecture and reasoning algorithms based on Answer Set Programming (ASP).
Although ASP has been used successfully in a number of applications, to the
best of our knowledge this is the first practical application of a complete
ASP-based agent architecture. It is also the first practical application of ASP
involving a combination of centralized reasoning, decentralized reasoning,
execution monitoring, and reasoning about network communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5447</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5447</id><created>2014-05-20</created><authors><author><keyname>Azarbonyad</keyname><forenames>Hosein</forenames></author><author><keyname>Shakery</keyname><forenames>Azadeh</forenames></author><author><keyname>Faili</keyname><forenames>Heshaam</forenames></author></authors><title>Learning to Exploit Different Translation Resources for Cross Language
  Information Retrieval</title><categories>cs.IR cs.CL</categories><journal-ref>International Journal of Information and Communication Technology
  Research, Volume 6, Issue 1, pp. 55-68, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important factors that affects the performance of Cross Language
Information Retrieval(CLIR)is the quality of translations being employed in
CLIR. In order to improve the quality of translations, it is important to
exploit available resources efficiently. Employing different translation
resources with different characteristics has many challenges. In this paper, we
propose a method for exploiting available translation resources simultaneously.
This method employs Learning to Rank(LTR) for exploiting different translation
resources. To apply LTR methods for query translation, we define different
translation relation based features in addition to context based features. We
use the contextual information contained in translation resources for
extracting context based features.The proposed method uses LTR to construct a
translation ranking model based on defined features. The constructed model is
used for ranking translation candidates of query words. To evaluate the
proposed method we do English-Persian CLIR, in which we employ the translation
ranking model to find translations of English queries and employ the
translations to retrieve Persian documents. Experimental results show that our
approach significantly outperforms single resource based CLIR methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5459</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5459</id><created>2014-05-21</created><authors><author><keyname>Melnikov</keyname><forenames>Alexey A.</forenames></author><author><keyname>Makmal</keyname><forenames>Adi</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Projective simulation applied to the grid-world and the mountain-car
  problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the model of projective simulation (PS) which is a novel approach to
artificial intelligence (AI). Recently it was shown that the PS agent performs
well in a number of simple task environments, also when compared to standard
models of reinforcement learning (RL). In this paper we study the performance
of the PS agent further in more complicated scenarios. To that end we chose two
well-studied benchmarking problems, namely the &quot;grid-world&quot; and the
&quot;mountain-car&quot; problem, which challenge the model with large and continuous
input space. We compare the performance of the PS agent model with those of
existing models and show that the PS agent exhibits competitive performance
also in such scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5461</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5461</id><created>2014-05-21</created><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Kopinsky</keyname><forenames>Justin</forenames></author><author><keyname>Matveev</keyname><forenames>Alexander</forenames></author><author><keyname>Shavit</keyname><forenames>Nir</forenames></author></authors><title>The LevelArray: A Fast, Practical Long-Lived Renaming Algorithm</title><categories>cs.DC cs.DS</categories><comments>ICDCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long-lived renaming problem appears in shared-memory systems where a set
of threads need to register and deregister frequently from the computation,
while concurrent operations scan the set of currently registered threads.
Instances of this problem show up in concurrent implementations of
transactional memory, flat combining, thread barriers, and memory reclamation
schemes for lock-free data structures. In this paper, we analyze a randomized
solution for long-lived renaming. The algorithmic technique we consider, called
the LevelArray, has previously been used for hashing and one-shot (single-use)
renaming. Our main contribu- tion is to prove that, in long-lived executions,
where processes may register and deregister polynomially many times, the
technique guarantees constant steps on average and O(log log n) steps with high
probability for registering, unit cost for deregistering, and O(n) steps for
collect queries, where n is an upper bound on the number of processes that may
be active at any point in time. We also show that the algorithm has the
surprising property that it is self-healing: under reasonable assumptions on
the schedule, operations running while the data structure is in a degraded
state implicitly help the data structure re-balance itself. This subtle
mechanism obviates the need for expensive periodic rebuilding procedures. Our
benchmarks validate this approach, showing that, for typical use parameters,
the average number of steps a process takes to register is less than two and
the worst-case number of steps is bounded by six, even in executions with
billions of operations. We contrast this with other randomized implementations,
whose worst-case behavior we show to be unreliable, and with deterministic
implementations, whose cost is linear in n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5474</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5474</id><created>2014-05-21</created><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author></authors><title>New Perspectives in Sinographic Language Processing Through the Use of
  Character Structure</title><categories>cs.CL</categories><comments>17 pages, 5 figures, presented at CICLing 2013</comments><journal-ref>Lecture Notes in Computer Science 7816 (2013), pp. 201--217</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chinese characters have a complex and hierarchical graphical structure
carrying both semantic and phonetic information. We use this structure to
enhance the text model and obtain better results in standard NLP operations.
First of all, to tackle the problem of graphical variation we define
allographic classes of characters. Next, the relation of inclusion of a
subcharacter in a characters, provides us with a directed graph of allographic
classes. We provide this graph with two weights: semanticity (semantic relation
between subcharacter and character) and phoneticity (phonetic relation) and
calculate &quot;most semantic subcharacter paths&quot; for each character. Finally,
adding the information contained in these paths to unigrams we claim to
increase the efficiency of text mining methods. We evaluate our method on a
text classification task on two corpora (Chinese and Japanese) of a total of 18
million characters and get an improvement of 3% on an already high baseline of
89.6% precision, obtained by a linear SVM classifier. Other possible
applications and perspectives of the system are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5483</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5483</id><created>2014-05-21</created><updated>2014-05-22</updated><authors><author><keyname>Susik</keyname><forenames>Robert</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Fredriksson</keyname><forenames>Kimmo</forenames></author></authors><title>Multiple pattern matching revisited</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>F.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical exact multiple string matching problem. Our
solution is based on $q$-grams combined with pattern superimposition,
bit-parallelism and alphabet size reduction. We discuss the pros and cons of
the various alternatives of how to achieve best combination. Our method is
closely related to previous work by (Salmela et al., 2006). The experimental
results show that our method performs well on different alphabet sizes and that
they scale to large pattern sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5488</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5488</id><created>2014-04-23</created><authors><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author></authors><title>On Learning Where To Look</title><categories>cs.CV cs.LG</categories><comments>deep learning, vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current automatic vision systems face two major challenges: scalability and
extreme variability of appearance. First, the computational time required to
process an image typically scales linearly with the number of pixels in the
image, therefore limiting the resolution of input images to thumbnail size.
Second, variability in appearance and pose of the objects constitute a major
hurdle for robust recognition and detection. In this work, we propose a model
that makes baby steps towards addressing these challenges. We describe a
learning based method that recognizes objects through a series of glimpses.
This system performs an amount of computation that scales with the complexity
of the input rather than its number of pixels. Moreover, the proposed method is
potentially more robust to changes in appearance since its parameters are
learned in a data driven manner. Preliminary experiments on a handwritten
dataset of digits demonstrate the computational advantages of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5490</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5490</id><created>2014-05-21</created><updated>2015-01-30</updated><authors><author><keyname>Gupta</keyname><forenames>Aditi</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>Meier</keyname><forenames>Patrick</forenames></author></authors><title>TweetCred: Real-Time Credibility Assessment of Content on Twitter</title><categories>cs.CR cs.SI physics.soc-ph</categories><comments>16 Pages in Proceedings of Social Informatics (SocInfo 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During sudden onset crisis events, the presence of spam, rumors and fake
content on Twitter reduces the value of information contained on its messages
(or &quot;tweets&quot;). A possible solution to this problem is to use machine learning
to automatically evaluate the credibility of a tweet, i.e. whether a person
would deem the tweet believable or trustworthy. This has been often framed and
studied as a supervised classification problem in an off-line (post-hoc)
setting. In this paper, we present a semi-supervised ranking model for scoring
tweets according to their credibility. This model is used in TweetCred, a
real-time system that assigns a credibility score to tweets in a user's
timeline. TweetCred, available as a browser plug-in, was installed and used by
1,127 Twitter users within a span of three months. During this period, the
credibility score for about 5.4 million tweets was computed, allowing us to
evaluate TweetCred in terms of response time, effectiveness and usability. To
the best of our knowledge, this is the first research work to develop a
real-time system for credibility on Twitter, and to evaluate it on a user base
of this size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5494</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5494</id><created>2014-05-15</created><authors><author><keyname>Moshin</keyname><forenames>Yasir Q.</forenames></author><author><keyname>Ongie</keyname><forenames>Greg</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Iterative Non-Local Shrinkage Algorithm for MR Image Reconstruction</title><categories>cs.CV</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a fast iterative non-local shrinkage algorithm to recover MRI
data from undersampled Fourier measurements. This approach is enabled by the
reformulation of current non-local schemes as an alternating algorithm to
minimize a global criterion. The proposed algorithm alternates between a
non-local shrinkage step and a quadratic subproblem. We derive analytical
shrinkage rules for several penalties that are relevant in non-local
regularization. The redundancy in the searches used to evaluate the shrinkage
steps are exploited using filtering operations. The resulting algorithm is
observed to be considerably faster than current alternating non-local
algorithms. The comparisons of the proposed scheme with state-of-the-art
regularization schemes show a considerable reduction in alias artifacts and
preservation of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5498</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5498</id><created>2014-05-21</created><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Griffith</keyname><forenames>J. Daniel</forenames></author><author><keyname>Gupta</keyname><forenames>Vishal</forenames></author><author><keyname>Kochenderfer</keyname><forenames>Mykel J.</forenames></author><author><keyname>Mi&#x161;i&#x107;</keyname><forenames>Velibor V.</forenames></author><author><keyname>Moss</keyname><forenames>Robert</forenames></author></authors><title>A Comparison of Monte Carlo Tree Search and Mathematical Optimization
  for Large Scale Dynamic Resource Allocation</title><categories>math.OC cs.AI</categories><comments>37 pages, 13 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic resource allocation (DRA) problems are an important class of dynamic
stochastic optimization problems that arise in a variety of important
real-world applications. DRA problems are notoriously difficult to solve to
optimality since they frequently combine stochastic elements with intractably
large state and action spaces. Although the artificial intelligence and
operations research communities have independently proposed two successful
frameworks for solving dynamic stochastic optimization problems---Monte Carlo
tree search (MCTS) and mathematical optimization (MO), respectively---the
relative merits of these two approaches are not well understood. In this paper,
we adapt both MCTS and MO to a problem inspired by tactical wildfire and
management and undertake an extensive computational study comparing the two
methods on large scale instances in terms of both the state and the action
spaces. We show that both methods are able to greatly improve on a baseline,
problem-specific heuristic. On smaller instances, the MCTS and MO approaches
perform comparably, but the MO approach outperforms MCTS as the size of the
problem increases for a fixed computational budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5501</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5501</id><created>2014-05-21</created><authors><author><keyname>Kopczynski</keyname><forenames>Dominik</forenames></author><author><keyname>Rahmann</keyname><forenames>Sven</forenames></author></authors><title>Using the Expectation Maximization Algorithm with Heterogeneous Mixture
  Components for the Analysis of Spectrometry Data</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coupling a multi-capillary column (MCC) with an ion mobility (IM)
spectrometer (IMS) opened a multitude of new application areas for gas
analysis, especially in a medical context, as volatile organic compounds (VOCs)
in exhaled breath can hint at a person's state of health. To obtain a potential
diagnosis from a raw MCC/IMS measurement, several computational steps are
necessary, which so far have required manual interaction, e.g., human
evaluation of discovered peaks. We have recently proposed an automated pipeline
for this task that does not require human intervention during the analysis.
Nevertheless, there is a need for improved methods for each computational step.
In comparison to gas chromatography / mass spectrometry (GC/MS) data, MCC/IMS
data is easier and less expensive to obtain, but peaks are more diffuse and
there is a higher noise level. MCC/IMS measurements can be described as samples
of mixture models (i.e., of convex combinations) of two-dimensional probability
distributions. So we use the expectation-maximization (EM) algorithm to
deconvolute mixtures in order to develop methods that improve data processing
in three computational steps: denoising, baseline correction and peak
clustering. A common theme of these methods is that mixture components within
one model are not homogeneous (e.g., all Gaussian), but of different types.
Evaluation shows that the novel methods outperform the existing ones. We
provide Python software implementing all three methods and make our evaluation
data available at http://www.rahmannlab.de/research/ims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5505</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5505</id><created>2014-05-21</created><updated>2016-02-25</updated><authors><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author><author><keyname>Sriperumbudur</keyname><forenames>Bharath</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Kernel Mean Shrinkage Estimators</title><categories>stat.ML cs.LG</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel
mean, is central to kernel methods in that it is used by many classical
algorithms such as kernel principal component analysis, and it also forms the
core inference step of modern kernel methods that rely on embedding probability
distributions in RKHSs. Given a finite sample, an empirical average has been
used commonly as a standard estimator of the true kernel mean. Despite a
widespread use of this estimator, we show that it can be improved thanks to the
well-known Stein phenomenon. We propose a new family of estimators called
kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical
justifications and good empirical performance. The results demonstrate that the
proposed estimators outperform the standard one, especially in a &quot;large d,
small n&quot; paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5507</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5507</id><created>2014-05-21</created><authors><author><keyname>Wu</keyname><forenames>Tianqing</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author></authors><title>Improved Performance of RF Energy Powered Wireless Sensor Node with
  Cooperative Beam Selection</title><categories>cs.IT math.IT</categories><comments>17pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RF energy harvesting is a promising potential solution to provide convenient
and perpetual energy supplies to low-power wireless sensor networks. In this
paper, we investigate the energy harvesting performance of a wireless sensor
node powered by harvesting RF energy from existing multiuser MIMO system.
Specifically, we propose a random unitary beamforming (RUB) based cooperative
beam selection scheme to enhance the energy harvesting performance at the
sensor. Under a constant total transmission power constraint, the multiuser
MIMO system tries to select a maximal number of active beams for data
transmission, while satisfying the energy harvesting requirement at the sensor.
We derive the exact closed-form expression for the distribution function of
harvested energy in a coherence time over Rayleigh fading channels. We further
investigate the performance tradeoff of the average harvested energy at the
sensor versus the sum-rate of the multiuser MIMO system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5509</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5509</id><created>2014-05-21</created><authors><author><keyname>Chitraa</keyname><forenames>V.</forenames></author><author><keyname>Thanamani</keyname><forenames>Antony Selvadoss</forenames></author></authors><title>Web Log Data Analysis by Enhanced Fuzzy C Means Clustering</title><categories>cs.IR</categories><comments>15 pages, International Journal on Computational Sciences &amp;
  Applications April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  World Wide Web is a huge repository of information and there is a tremendous
increase in the volume of information daily. The number of users are also
increasing day by day. To reduce users browsing time lot of research is taken
place. Web Usage Mining is a type of web mining in which mining techniques are
applied in log data to extract the behaviour of users. Clustering plays an
important role in a broad range of applications like Web analysis, CRM,
marketing, medical diagnostics, computational biology, and many others.
Clustering is the grouping of similar instances or objects. The key factor for
clustering is some sort of measure that can determine whether two objects are
similar or dissimilar . In this paper a novel clustering method to partition
user sessions into accurate clusters is discussed. The accuracy and various
performance measures of the proposed algorithm shows that the proposed method
is a better method for web log mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5512</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5512</id><created>2014-05-21</created><updated>2014-06-11</updated><authors><author><keyname>Das</keyname><forenames>Sima</forenames></author></authors><title>On Local and Global Centrality in Large Scale Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating influential nodes in large scale networks including but not
limited to social networks, biological networks, communication networks,
emerging smart grids etc. is a topic of fundamental interest. To understand
influences of nodes in a network, a classical metric is centrality within which
there are multiple specific instances including degree centrality, closeness
centrality, betweenness centrality and more. As of today, existing algorithms
to identify nodes with high centrality measures operate upon the entire (or
rather global) network, resulting in high computational complexity. In this
paper, we design efficient algorithms for determining the betweenness
centrality in large scale networks by taking advantage of the modular topology
exhibited by most of these large scale networks. Very briefly, modular
topologies are those wherein the entire network appears partitioned into
distinct modules (or clusters or communities), wherein nodes within the module
(that likely share highly similar profiles) have dense connections between
them, while connections across modules are relatively sparse. Using a novel
adaptation of Dijkstra's shortest path algorithm, and executing it over local
modules and over sparse edges between modules, we design algorithms that can
correctly compute the betweenness centrality much faster than existing
algorithms. To the best of our knowledge, ours is the first work that leverage
modular topologies of large scale networks to address the centrality problem,
though here we mostly limit our discussions to social networks. We also provide
more insights on centrality in general, and also how our algorithms can be used
to determine other centrality measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5523</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5523</id><created>2014-03-03</created><authors><author><keyname>Blakley</keyname><forenames>Bob</forenames></author><author><keyname>Blakley</keyname><forenames>G R</forenames></author><author><keyname>Blakley</keyname><forenames>Sean M</forenames></author></authors><title>How to Draw Graphs: Seeing and Redrafting Large Networks in Security and
  Biology</title><categories>cs.HC</categories><comments>14 pages, 10 figures</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is a mathematical object consisting of a set of vertices and a set of
edges connecting vertices. Graphs can be drawn on paper in various ways, but
until recently all published methods of drawing graphs have had undesirable
properties: (i) for graphs which are not plane embeddable, intersections
between the lines representing edges appear at points which are not vertices,
creating the appearance of vertices where none exist, (ii) vertex labels can be
placed inside vertex symbols, but there is no consistent, logical, and visually
clean place to put edge labels, and (iii) representations of large graphs are
visually dense and difficult to interpret. This paper describes a new
cartographic method of drawing graphs which solves all of these problems, and
has other advantages as well. Complements, comparisons and contrasts of graphs
are usually better shown cartographically than in node-link form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5531</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5531</id><created>2014-05-21</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Wario</keyname><forenames>Fernando</forenames></author><author><keyname>Osuna</keyname><forenames>Valentin</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Fast algorithm for Multiple-Circle detection on images using Learning
  Automata</title><categories>cs.CV</categories><comments>30 Pages. arXiv admin note: text overlap with arXiv:1405.5406</comments><journal-ref>IET Image Processing 6 (8) , (2012), pp. 1124-1135</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hough transform (HT) has been the most common method for circle detection
exhibiting robustness but adversely demanding a considerable computational load
and large storage. Alternative approaches include heuristic methods that employ
iterative optimization procedures for detecting multiple circles under the
inconvenience that only one circle can be marked at each optimization cycle
demanding a longer execution time. On the other hand, Learning Automata (LA) is
a heuristic method to solve complex multi-modal optimization problems. Although
LA converges to just one global minimum, the final probability distribution
holds valuable information regarding other local minima which have emerged
during the optimization process. The detection process is considered as a
multi-modal optimization problem, allowing the detection of multiple circular
shapes through only one optimization procedure. The algorithm uses a
combination of three edge points as parameters to determine circles candidates.
A reinforcement signal determines if such circle candidates are actually
present at the image. Guided by the values of such reinforcement signal, the
set of encoded candidate circles are evolved using the LA so that they can fit
into actual circular shapes over the edge-only map of the image. The overall
approach is a fast multiple-circle detector despite facing complicated
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5543</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5543</id><created>2014-05-21</created><authors><author><keyname>Cao</keyname><forenames>Nianxia</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Target Tracking via Crowdsourcing: A Mechanism Design Approach</title><categories>cs.SY cs.GT</categories><comments>13 pages, 11 figures, IEEE Signal Processing Transaction</comments><doi>10.1109/TSP.2015.2398838</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a crowdsourcing based framework for myopic target
tracking by designing an incentive-compatible mechanism based optimal auction
in a wireless sensor network (WSN) containing sensors that are selfish and
profit-motivated. For typical WSNs which have limited bandwidth, the fusion
center (FC) has to distribute the total number of bits that can be transmitted
from the sensors to the FC among the sensors. To accomplish the task, the FC
conducts an auction by soliciting bids from the selfish sensors, which reflect
how much they value their energy cost. Furthermore, the rationality and
truthfulness of the sensors are guaranteed in our model. The final problem is
formulated as a multiple-choice knapsack problem (MCKP), which is solved by the
dynamic programming method in pseudo-polynomial time. Simulation results show
the effectiveness of our proposed approach in terms of both the tracking
performance and lifetime of the sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5550</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5550</id><created>2014-05-21</created><authors><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Yang</keyname><forenames>Dazuo</forenames></author><author><keyname>Chen</keyname><forenames>Fudi</forenames></author><author><keyname>Zhou</keyname><forenames>Yibing</forenames></author><author><keyname>Xiu</keyname><forenames>Zhilong</forenames></author></authors><title>Application of Artificial Neural Networks in Predicting Abrasion
  Resistance of Solution Polymerized Styrene-Butadiene Rubber Based Composites</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Abrasion resistance of solution polymerized styrene-butadiene rubber (SSBR)
based composites is a typical and crucial property in practical applications.
Previous studies show that the abrasion resistance can be calculated by the
multiple linear regression model. In our study, considering this relationship
can also be described into the non-linear conditions, a Multilayer Feed-forward
Neural Networks model with 3 nodes (MLFN-3) was successfully established to
describe the relationship between the abrasion resistance and other properties,
using 23 groups of data, with the RMS error 0.07. Our studies have proved that
Artificial Neural Networks (ANN) model can be used to predict the SSBR-based
composites, which is an accurate and robust process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5572</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5572</id><created>2014-05-21</created><authors><author><keyname>Gao</keyname><forenames>Jianhang</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author><author><keyname>Swami</keyname><forenames>Anathram</forenames></author></authors><title>Minimum Information Dominating Set for Opinion Sampling</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of inferring the opinions of a social network through
strategically sampling a minimum subset of nodes by exploiting correlations in
node opinions. We first introduce the concept of information dominating set
(IDS). A subset of nodes in a given network is an IDS if knowing the opinions
of nodes in this subset is sufficient to infer the opinion of the entire
network. We focus on two fundamental algorithmic problems: (i) given a subset
of the network, how to determine whether it is an IDS; (ii) how to construct a
minimum IDS. Assuming binary opinions and the local majority rule for opinion
correlation, we show that the first problem is co-NP-complete and the second
problem is NP-hard in general networks. We then focus on networks with special
structures, in particular, acyclic networks. We show that in acyclic networks,
both problems admit linear-complexity solutions by establishing a connection
between the IDS problems and the vertex cover problem. Our technique for
establishing the hardness of the IDS problems is based on a novel graph
transformation that transforms the IDS problems in a general network to that in
an odd-degree network. This graph transformation technique not only gives an
approximation algorithm to the IDS problems, but also provides a useful tool
for general studies related to the local majority rule. Besides opinion
sampling for applications such as political polling and market survey, the
concept of IDS and the results obtained in this paper also find applications in
data compression and identifying critical nodes in information networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5574</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5574</id><created>2014-05-21</created><authors><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Zhou</keyname><forenames>Michelle X.</forenames></author><author><keyname>Megiddo</keyname><forenames>Nimrod</forenames></author><author><keyname>Nichols</keyname><forenames>Jeffrey</forenames></author><author><keyname>Drews</keyname><forenames>Clemens</forenames></author></authors><title>Recommending Targeted Strangers from Whom to Solicit Information on
  Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an intelligent, crowd-powered information collection system that
automatically identifies and asks target-ed strangers on Twitter for desired
information (e.g., cur-rent wait time at a nightclub). Our work includes three
parts. First, we identify a set of features that characterize ones willingness
and readiness to respond based on their exhibited social behavior, including
the content of their tweets and social interaction patterns. Second, we use the
identified features to build a statistical model that predicts ones likelihood
to respond to information solicitations. Third, we develop a recommendation
algorithm that selects a set of targeted strangers using the probabilities
computed by our statistical model with the goal to maximize the over-all
response rate. Our experiments, including several in the real world,
demonstrate the effectiveness of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5581</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5581</id><created>2014-05-21</created><authors><author><keyname>Ferguson</keyname><forenames>Sarah</forenames></author><author><keyname>Luders</keyname><forenames>Brandon</forenames></author><author><keyname>Grande</keyname><forenames>Robert C.</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author></authors><title>Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with
  Uncertain, Changing Intentions</title><categories>cs.RO</categories><comments>Submitted to 2014 International Workshop on the Algorithmic
  Foundations of Robotics</comments><msc-class>68T40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To plan safe trajectories in urban environments, autonomous vehicles must be
able to quickly assess the future intentions of dynamic agents. Pedestrians are
particularly challenging to model, as their motion patterns are often uncertain
and/or unknown a priori. This paper presents a novel changepoint detection and
clustering algorithm that, when coupled with offline unsupervised learning of a
Gaussian process mixture model (DPGP), enables quick detection of changes in
intent and online learning of motion patterns not seen in prior training data.
The resulting long-term movement predictions demonstrate improved accuracy
relative to offline learning alone, in terms of both intent and trajectory
prediction. By embedding these predictions within a chance-constrained motion
planner, trajectories which are probabilistically safe to pedestrian motions
can be identified in real-time. Hardware experiments demonstrate that this
approach can accurately predict pedestrian motion patterns from onboard
sensor/perception data and facilitate robust navigation within a dynamic
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5590</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5590</id><created>2014-05-21</created><authors><author><keyname>Raghothaman</keyname><forenames>Mukund</forenames></author><author><keyname>Udupa</keyname><forenames>Abhishek</forenames></author></authors><title>Language to Specify Syntax-Guided Synthesis Problems</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a language to specify syntax guided synthesis (SyGuS) problems.
Syntax guidance is a prominent theme in contemporary program synthesis
approaches, and SyGuS was first described in [1]. This paper describes
concretely the input format of a SyGuS solver.
  [1] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund
Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina
Torlak, and Abhishek Udupa. Syntax-guided synthesis. In FMCAD, pages 1--17,
2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5593</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5593</id><created>2014-05-21</created><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames><affiliation>LIGM, Universit&#xe9; Paris-Est, CNRS</affiliation></author><author><keyname>Hague</keyname><forenames>Matthew</forenames><affiliation>Department of Computer Science, Royal Holloway University of London</affiliation></author></authors><title>Saturation algorithms for model-checking pushdown systems</title><categories>cs.FL cs.LO</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 1-24</journal-ref><doi>10.4204/EPTCS.151.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a survey of the saturation method for model-checking pushdown
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5594</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5594</id><created>2014-05-21</created><authors><author><keyname>Gruber</keyname><forenames>Hermann</forenames><affiliation>knowledgepark AG</affiliation></author><author><keyname>Holzer</keyname><forenames>Markus</forenames><affiliation>Universit&#xe4;t Giessen</affiliation></author></authors><title>From Finite Automata to Regular Expressions and Back--A Summary on
  Descriptional Complexity</title><categories>cs.FL cs.DM</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.2.3; F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 25-48</journal-ref><doi>10.4204/EPTCS.151.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The equivalence of finite automata and regular expressions dates back to the
seminal paper of Kleene on events in nerve nets and finite automata from 1956.
In the present paper we tour a fragment of the literature and summarize results
on upper and lower bounds on the conversion of finite automata to regular
expressions and vice versa. We also briefly recall the known bounds for the
removal of spontaneous transitions (epsilon-transitions) on non-epsilon-free
nondeterministic devices. Moreover, we report on recent results on the average
case descriptional complexity bounds for the conversion of regular expressions
to finite automata and brand new developments on the state elimination
algorithm that converts finite automata to regular expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5595</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5595</id><created>2014-05-21</created><authors><author><keyname>Kl&#xed;ma</keyname><forenames>Ond&#x159;ej</forenames><affiliation>Department of Mathematics and Statistics, Masaryk University, Brno, Czech Republic</affiliation></author></authors><title>On Varieties of Automata Enriched with an Algebraic Structure (Extended
  Abstract)</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 49-54</journal-ref><doi>10.4204/EPTCS.151.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eilenberg correspondence, based on the concept of syntactic monoids, relates
varieties of regular languages with pseudovarieties of finite monoids. Various
modifications of this correspondence related more general classes of regular
languages with classes of more complex algebraic objects. Such generalized
varieties also have natural counterparts formed by classes of finite automata
equipped with a certain additional algebraic structure. In this survey, we
overview several variants of such varieties of enriched automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5596</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5596</id><created>2014-05-21</created><authors><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames></author></authors><title>Decision Problems for Deterministic Pushdown Automata on Infinite Words</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 55-73</journal-ref><doi>10.4204/EPTCS.151.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article surveys some decidability results for DPDAs on infinite words
(omega-DPDA). We summarize some recent results on the decidability of the
regularity and the equivalence problem for the class of weak omega-DPDAs.
Furthermore, we present some new results on the parity index problem for
omega-DPDAs. For the specification of a parity condition, the states of the
omega-DPDA are assigned priorities (natural numbers), and a run is accepting if
the highest priority that appears infinitely often during a run is even. The
basic simplification question asks whether one can determine the minimal number
of priorities that are needed to accept the language of a given omega-DPDA. We
provide some decidability results on variations of this question for some
classes of omega-DPDAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5597</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5597</id><created>2014-05-21</created><authors><author><keyname>Maneth</keyname><forenames>Sebastian</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Equivalence Problems for Tree Transducers: A Brief Survey</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 74-93</journal-ref><doi>10.4204/EPTCS.151.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decidability of equivalence for three important classes of tree
transducers is discussed. Each class can be obtained as a natural restriction
of deterministic macro tree transducers (MTTs): (1) no context parameters,
i.e., top-down tree transducers, (2) linear size increase, i.e., MSO definable
tree transducers, and (3) monadic input and output ranked alphabets. For the
full class of MTTs, decidability of equivalence remains a long-standing open
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5598</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5598</id><created>2014-05-21</created><authors><author><keyname>Barash</keyname><forenames>Mikhail</forenames></author><author><keyname>Okhotin</keyname><forenames>Alexander</forenames></author></authors><title>Grammars with two-sided contexts</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 94-108</journal-ref><doi>10.4204/EPTCS.151.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper (M. Barash, A. Okhotin, &quot;Defining contexts in context-free
grammars&quot;, LATA 2012), the authors introduced an extension of the context-free
grammars equipped with an operator for referring to the left context of the
substring being defined. This paper proposes a more general model, in which
context specifications may be two-sided, that is, both the left and the right
contexts can be specified by the corresponding operators. The paper gives the
definitions and establishes the basic theory of such grammars, leading to a
normal form and a parsing algorithm working in time O(n^4), where n is the
length of the input string.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5599</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5599</id><created>2014-05-21</created><authors><author><keyname>Berglund</keyname><forenames>Martin</forenames></author><author><keyname>Drewes</keyname><forenames>Frank</forenames></author><author><keyname>van der Merwe</keyname><forenames>Brink</forenames></author></authors><title>Analyzing Catastrophic Backtracking Behavior in Practical Regular
  Expression Matching</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 109-123</journal-ref><doi>10.4204/EPTCS.151.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a formal perspective on how regular expression matching works in
Java, a popular representative of the category of regex-directed matching
engines. In particular, we define an automata model which captures all the
aspects needed to study such matching engines in a formal way. Based on this,
we propose two types of static analysis, which take a regular expression and
tell whether there exists a family of strings which makes Java-style matching
run in exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5600</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5600</id><created>2014-05-21</created><authors><author><keyname>Bordihn</keyname><forenames>Henning</forenames></author><author><keyname>Kutrib</keyname><forenames>Martin</forenames></author><author><keyname>Malcher</keyname><forenames>Andreas</forenames></author></authors><title>Measuring Communication in Parallel Communicating Finite Automata</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 124-138</journal-ref><doi>10.4204/EPTCS.151.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems of deterministic finite automata communicating by sending their
states upon request are investigated, when the amount of communication is
restricted. The computational power and decidability properties are studied for
the case of returning centralized systems, when the number of necessary
communications during the computations of the system is bounded by a function
depending on the length of the input. It is proved that an infinite hierarchy
of language families exists, depending on the number of messages sent during
their most economical recognitions. Moreover, several properties are shown to
be not semi-decidable for the systems under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5601</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5601</id><created>2014-05-21</created><authors><author><keyname>C&#xe2;mpeanu</keyname><forenames>Cezar</forenames><affiliation>The University of Prince Edward Island</affiliation></author></authors><title>Simplifying Nondeterministic Finite Cover Automata</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 162-173</journal-ref><doi>10.4204/EPTCS.151.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of Deterministic Finite Cover Automata (DFCA) was introduced at
WIA '98, as a more compact representation than Deterministic Finite Automata
(DFA) for finite languages. In some cases representing a finite language,
Nondeterministic Finite Automata (NFA) may significantly reduce the number of
states used. The combined power of the succinctness of the representation of
finite languages using both cover languages and non-determinism has been
suggested, but never systematically studied. In the present paper, for
nondeterministic finite cover automata (NFCA) and l-nondeterministic finite
cover automaton (l-NFCA), we show that minimization can be as hard as
minimizing NFAs for regular languages, even in the case of NFCAs using unary
alphabets. Moreover, we show how we can adapt the methods used to reduce, or
minimize the size of NFAs/DFCAs/l-DFCAs, for simplifying NFCAs/l-NFCAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5602</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5602</id><created>2014-05-21</created><authors><author><keyname>Carnino</keyname><forenames>Vincent</forenames><affiliation>LIGM, Universit&#xe9; Paris-Est</affiliation></author><author><keyname>Lombardy</keyname><forenames>Sylvain</forenames><affiliation>LaBRI, Insitut Polytechnique de Bordeaux</affiliation></author></authors><title>On Determinism and Unambiguity of Weighted Two-way Automata</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 188-200</journal-ref><doi>10.4204/EPTCS.151.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first study the conversion of weighted two-way automata to
one-way automata. We show that this conversion preserves the unambiguity but
does not preserve the determinism. Yet, we prove that the conversion of an
unambiguous weighted one-way automaton into a two-way automaton leads to a
deterministic two-way automaton. As a consequence, we prove that unambiguous
weighted two-way automata are equivalent to deterministic weighted two-way
automata in commutative semirings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5603</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5603</id><created>2014-05-21</created><authors><author><keyname>&#x10c;evorov&#xe1;</keyname><forenames>Krist&#xed;na</forenames></author><author><keyname>Jir&#xe1;skov&#xe1;</keyname><forenames>Galina</forenames></author><author><keyname>Mlyn&#xe1;r&#x10d;ik</keyname><forenames>Peter</forenames></author><author><keyname>Palmovsk&#xfd;</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>&#x160;ebej</keyname><forenames>Juraj</forenames></author></authors><title>Operations on Automata with All States Final</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 201-215</journal-ref><doi>10.4204/EPTCS.151.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of basic regular operations on languages represented
by incomplete deterministic or nondeterministic automata, in which all states
are final. Such languages are known to be prefix-closed. We get tight bounds on
both incomplete and nondeterministic state complexity of complement,
intersection, union, concatenation, star, and reversal on prefix-closed
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5604</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5604</id><created>2014-05-21</created><authors><author><keyname>Reghizzi</keyname><forenames>Stefano Crespi</forenames><affiliation>DEIB, Politecnico di Milano and CNR-IEIIT</affiliation></author><author><keyname>Pietro</keyname><forenames>Pierluigi San</forenames><affiliation>DEIB, Politecnico di Milano and CNR-IEIIT</affiliation></author></authors><title>Commutative Languages and their Composition by Consensual Methods</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 216-230</journal-ref><doi>10.4204/EPTCS.151.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commutative languages with the semilinear property (SLIP) can be naturally
recognized by real-time NLOG-SPACE multi-counter machines. We show that unions
and concatenations of such languages can be similarly recognized, relying on --
and further developing, our recent results on the family of consensually
regular (CREG) languages. A CREG language is defined by a regular language on
the alphabet that includes the terminal alphabet and its marked copy. New
conditions, for ensuring that the union or concatenation of CREG languages is
closed, are presented and applied to the commutative SLIP languages. The paper
contributes to the knowledge of the CREG family, and introduces novel
techniques for language composition, based on arithmetic congruences that act
as language signatures. Open problems are listed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5605</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5605</id><created>2014-05-21</created><authors><author><keyname>Du</keyname><forenames>Chen Fei</forenames><affiliation>University of Waterloo</affiliation></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames><affiliation>University of Waterloo</affiliation></author></authors><title>Similarity density of the Thue-Morse word with overlap-free infinite
  binary words</title><categories>cs.FL cs.DM</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 231-245</journal-ref><doi>10.4204/EPTCS.151.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a measure of similarity for infinite words that generalizes the
notion of asymptotic or natural density of subsets of natural numbers from
number theory. We show that every overlap-free infinite binary word, other than
the Thue-Morse word t and its complement t bar, has this measure of similarity
with t between 1/4 and 3/4. This is a partial generalization of a classical
1927 result of Mahler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5606</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5606</id><created>2014-05-21</created><authors><author><keyname>Fernau</keyname><forenames>Henning</forenames><affiliation>Universit&#xe4;t Trier, Germany</affiliation></author><author><keyname>Freund</keyname><forenames>Rudolf</forenames><affiliation>TU Wien, Austria</affiliation></author><author><keyname>Holzer</keyname><forenames>Markus</forenames><affiliation>Universit&#xe4;t Gie&#xdf;en, Germany</affiliation></author></authors><title>Cooperating Distributed Grammar Systems of Finite Index Working in
  Hybrid Modes</title><categories>cs.FL cs.CC</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 246-260</journal-ref><doi>10.4204/EPTCS.151.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cooperating distributed grammar systems working in hybrid modes in
connection with the finite index restriction in two different ways: firstly, we
investigate cooperating distributed grammar systems working in hybrid modes
which characterize programmed grammars with the finite index restriction;
looking at the number of components of such systems, we obtain surprisingly
rich lattice structures for the inclusion relations between the corresponding
language families. Secondly, we impose the finite index restriction on
cooperating distributed grammar systems working in hybrid modes themselves,
which leads us to new characterizations of programmed grammars of finite index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5607</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5607</id><created>2014-05-21</created><authors><author><keyname>Heged&#xfc;s</keyname><forenames>L&#xe1;szl&#xf3;</forenames><affiliation>University of Debrecen</affiliation></author><author><keyname>Nagy</keyname><forenames>Benedek</forenames><affiliation>University of Debrecen</affiliation></author></authors><title>Representations of Circular Words</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.4.3</acm-class><journal-ref>EPTCS 151, 2014, pp. 261-270</journal-ref><doi>10.4204/EPTCS.151.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we give two different ways of representations of circular
words. Representations with tuples are intended as a compact notation, while
representations with trees give a way to easily process all conjugates of a
word. The latter form can also be used as a graphical representation of
periodic properties of finite (in some cases, infinite) words. We also define
iterative representations which can be seen as an encoding utilizing the
flexible properties of circular words. Every word over the two letter alphabet
can be constructed starting from ab by applying the fractional power and the
cyclic shift operators one after the other, iteratively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5608</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5608</id><created>2014-05-21</created><authors><author><keyname>Holzer</keyname><forenames>Markus</forenames></author><author><keyname>Jakobi</keyname><forenames>Sebastian</forenames></author></authors><title>More Structural Characterizations of Some Subregular Language Families
  by Biautomata</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 271-285</journal-ref><doi>10.4204/EPTCS.151.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study structural restrictions on biautomata such as, e.g., acyclicity,
permutation-freeness, strongly permutation-freeness, and orderability, to
mention a few. We compare the obtained language families with those induced by
deterministic finite automata with the same property. In some cases, it is
shown that there is no difference in characterization between deterministic
finite automata and biautomata as for the permutation-freeness, but there are
also other cases, where it makes a big difference whether one considers
deterministic finite automata or biautomata. This is, for instance, the case
when comparing strongly permutation-freeness, which results in the family of
definite language for deterministic finite automata, while biautomata induce
the family of finite and co-finite languages. The obtained results nicely fall
into the known landscape on classical language families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5609</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5609</id><created>2014-05-21</created><authors><author><keyname>Hutagalung</keyname><forenames>Milka</forenames><affiliation>University of Kassel</affiliation></author><author><keyname>Lange</keyname><forenames>Martin</forenames><affiliation>University of Kassel</affiliation></author><author><keyname>Lozes</keyname><forenames>Etienne</forenames><affiliation>University of Kassel</affiliation></author></authors><title>Buffered Simulation Games for B\&quot;uchi Automata</title><categories>cs.FL cs.CC</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 286-300</journal-ref><doi>10.4204/EPTCS.151.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation relations are an important tool in automata theory because they
provide efficiently computable approximations to language inclusion. In recent
years, extensions of ordinary simulations have been studied, for instance
multi-pebble and multi-letter simulations which yield better approximations and
are still polynomial-time computable.
  In this paper we study the limitations of approximating language inclusion in
this way: we introduce a natural extension of multi-letter simulations called
buffered simulations. They are based on a simulation game in which the two
players share a FIFO buffer of unbounded size. We consider two variants of
these buffered games called continuous and look-ahead simulation which differ
in how elements can be removed from the FIFO buffer. We show that look-ahead
simulation, the simpler one, is already PSPACE-hard, i.e. computationally as
hard as language inclusion itself. Continuous simulation is even EXPTIME-hard.
We also provide matching upper bounds for solving these games with infinite
state spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5610</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5610</id><created>2014-05-21</created><authors><author><keyname>Maletti</keyname><forenames>Andreas</forenames><affiliation>Universit&#xe4;t Leipzig</affiliation></author><author><keyname>Quernheim</keyname><forenames>Daniel</forenames><affiliation>Universit&#xe4;t Stuttgart</affiliation></author></authors><title>Hyper-Minimization for Deterministic Weighted Tree Automata</title><categories>cs.FL cs.CC cs.DS</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 314-326</journal-ref><doi>10.4204/EPTCS.151.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyper-minimization is a state reduction technique that allows a finite change
in the semantics. The theory for hyper-minimization of deterministic weighted
tree automata is provided. The presence of weights slightly complicates the
situation in comparison to the unweighted case. In addition, the first
hyper-minimization algorithm for deterministic weighted tree automata, weighted
over commutative semifields, is provided together with some implementation
remarks that enable an efficient implementation. In fact, the same run-time O(m
log n) as in the unweighted case is obtained, where m is the size of the
deterministic weighted tree automaton and n is its number of states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5611</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5611</id><created>2014-05-21</created><authors><author><keyname>Valdats</keyname><forenames>Maris</forenames></author></authors><title>Boolean Circuit Complexity of Regular Languages</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 342-354</journal-ref><doi>10.4204/EPTCS.151.24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define a new descriptional complexity measure for
Deterministic Finite Automata, BC-complexity, as an alternative to the state
complexity. We prove that for two DFAs with the same number of states
BC-complexity can differ exponentially. In some cases minimization of DFA can
lead to an exponential increase in BC-complexity, on the other hand
BC-complexity of DFAs with a large state space which are obtained by some
standard constructions (determinization of NFA, language operations), is
reasonably small. But our main result is the analogue of the &quot;Shannon effect&quot;
for finite automata: almost all DFAs with a fixed number of states have
BC-complexity that is close to the maximum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5613</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5613</id><created>2014-05-21</created><authors><author><keyname>Higashikawa</keyname><forenames>Yuya</forenames></author><author><keyname>Golin</keyname><forenames>Mordecai J.</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author></authors><title>Improved Algorithms for Multiple Sink Location Problems in Dynamic Path
  Networks</title><categories>cs.DS</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the k-sink location problem in dynamic path networks. In
our model, a dynamic path network consists of an undirected path with positive
edge lengths, uniform edge capacity, and positive vertex supplies. Here, each
vertex supply corresponds to a set of evacuees. Then, the problem requires to
find the optimal location of $k$ sinks in a given path so that each evacuee is
sent to one of k sinks. Let x denote a k-sink location. Under the optimal
evacuation for a given x, there exists a (k-1)-dimensional vector d, called
(k-1)-divider, such that each component represents the boundary dividing all
evacuees between adjacent two sinks into two groups, i.e., all supplies in one
group evacuate to the left sink and all supplies in the other group evacuate to
the right sink. Therefore, the goal is to find x and d which minimize the
maximum cost or the total cost, which are denoted by the minimax problem and
the minisum problem, respectively. We study the k-sink location problem in
dynamic path networks with continuous model, and prove that the minimax problem
can be solved in O(kn) time and the minisum problem can be solved in O(n^2
min{k, 2^{sqrt{log k log log n}}}) time, where n is the number of vertices in
the given network. Note that these improve the previous results by [6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5618</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5618</id><created>2014-05-21</created><updated>2014-10-17</updated><authors><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Compressive Phase Retrieval via Generalized Approximate Message Passing</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2014.2386294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In phase retrieval, the goal is to recover a signal
$\mathbf{x}\in\mathbb{C}^N$ from the magnitudes of linear measurements
$\mathbf{Ax}\in\mathbb{C}^M$. While recent theory has established that
$M\approx 4N$ intensity measurements are necessary and sufficient to recover
generic $\mathbf{x}$, there is great interest in reducing the number of
measurements through the exploitation of sparse $\mathbf{x}$, which is known as
compressive phase retrieval. In this work, we detail a novel, probabilistic
approach to compressive phase retrieval based on the generalized approximate
message passing (GAMP) algorithm. We then present a numerical study of the
proposed PR-GAMP algorithm, demonstrating its excellent phase-transition
behavior, robustness to noise, and runtime. Our experiments suggest that
approximately $M\geq 2K\log_2(N/K)$ intensity measurements suffice to recover
$K$-sparse Bernoulli-Gaussian signals for $\mathbf{A}$ with i.i.d Gaussian
entries and $K\ll N$. Meanwhile, when recovering a 6k-sparse 65k-pixel
grayscale image from 32k randomly masked and blurred Fourier intensity
measurements at 30~dB measurement SNR, PR-GAMP achieved an output SNR of no
less than 28~dB in all of 100 random trials, with a median runtime of only 7.3
seconds. Compared to the recently proposed CPRL, sparse-Fienup, and GESPAR
algorithms, our experiments suggest that PR-GAMP has a superior phase
transition and orders-of-magnitude faster runtimes as the sparsity and problem
dimensions increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5626</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5626</id><created>2014-05-21</created><updated>2014-06-26</updated><authors><author><keyname>Buss</keyname><forenames>Samuel R.</forenames><affiliation>Univ. of California, San Diego</affiliation></author><author><keyname>Kolodziejczyk</keyname><forenames>Leszek Aleksander</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Small Stone in Pool</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 27,
  2014) lmcs:852</journal-ref><doi>10.2168/LMCS-10(2:16)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Stone tautologies are known to have polynomial size resolution
refutations and require exponential size regular refutations. We prove that the
Stone tautologies also have polynomial size proofs in both pool resolution and
the proof system of regular tree-like resolution with input lemmas (regRTI).
Therefore, the Stone tautologies do not separate resolution from DPLL with
clause learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5628</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5628</id><created>2014-05-22</created><authors><author><keyname>Cuppens</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LUSSI, Lab-STICC</affiliation></author><author><keyname>Gabillon</keyname><forenames>Alban</forenames><affiliation>GePaSUD</affiliation></author></authors><title>Cover Story Management</title><categories>cs.CR cs.DB cs.LO</categories><proxy>ccsd</proxy><journal-ref>Data and Knowledge Engineering 37, 2 (2001) 177-201</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multilevel database, cover stories are usually managed using the
ambiguous technique of polyinstantiation. In this paper, we define a new
technique to manage cover stories and propose a formal representation of a
multilevel database containing cover stories. Our model aims to be a generic
model, that is, it can be interpreted for any kind of database (e.g.
relational, object- oriented etc). We then consider the problem of updating a
multilevel database containing cover stories managed with our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5630</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5630</id><created>2014-05-22</created><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Resource Allocation in Wireless Networks with RF Energy Harvesting and
  Transfer</title><categories>cs.NI</categories><comments>To appear in IEEE Network</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio frequency (RF) energy harvesting and transfer techniques have recently
become alternative methods to power the next generation of wireless networks.
As this emerging technology enables proactive replenishment of wireless
devices, it is advantageous in supporting applications with quality-of-service
(QoS) requirement. This article focuses on the resource allocation issues in
wireless networks with RF energy harvesting capability, referred to as RF
energy harvesting networks (RF-EHNs). First, we present an overview of the
RF-EHNs, followed by a review of a variety of issues regarding resource
allocation. Then, we present a case study of designing in the receiver
operation policy, which is of paramount importance in the RF-EHNs. We focus on
QoS support and service differentiation, which have not been addressed by
previous literatures. Furthermore, we outline some open research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5634</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5634</id><created>2014-05-22</created><authors><author><keyname>Natusch</keyname><forenames>Tim</forenames></author></authors><title>Application of Lossless Data Compression Techniques to Radio Astronomy
  Data flows</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>In preparation for submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern practice of Radio Astronomy is characterized by extremes of data
volume and rates, principally because of the direct relationship between the
signal to noise ratio that can be achieved and the need to Nyquist sample the
RF bandwidth necessary by way of support. The transport of these data flows is
costly. By examining the statistical nature of typical data flows and applying
well known techniques from the field of Information Theory the following work
shows that lossless compression of typical radio astronomy data flows is in
theory possible. The key parameter in determining the degree of compression
possible is the standard deviation of the data. The practical application of
compression could prove beneficial in reducing the costs of data transport and
(arguably) storage for new generation instruments such as the Square Kilometer
Array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5641</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5641</id><created>2014-05-22</created><updated>2014-05-26</updated><authors><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Iosifidis</keyname><forenames>George</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author><author><keyname>Li</keyname><forenames>Duozhe</forenames></author></authors><title>Bargaining-based Mobile Data Offloading</title><categories>cs.GT cs.NI</categories><comments>This manuscript is the complete technical report for the journal
  version published in IEEE Journal on Selected Areas in Communications (JSAC)
  Special Issue in 5G Communication Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unprecedented growth of mobile data traffic challenges the performance
and economic viability of today's cellular networks, and calls for novel
network architectures and communication solutions. Data offloading through
third-party WiFi or femtocell access points (APs) can effectively alleviate the
cellular network congestion in a low operational and capital expenditure. This
solution requires the cooperation and agreement of mobile cellular network
operators (MNOs) and AP owners (APOs). In this paper, we model and analyze the
interaction among one MNO and multiple APOs (for the amount of MNO's offloading
data and the respective APOs' compensations) by using the Nash bargaining
theory. Specifically, we introduce a one-to-many bargaining game among the MNO
and APOs, and analyze the bargaining solution (game equilibrium) systematically
under two different bargaining protocols: (i) sequential bargaining, where the
MNO bargains with APOs sequentially, with one APO at a time, in a given order,
and (ii) concurrent bargaining, where the MNO bargains with all APOs
concurrently. We quantify the benefits for APOs when bargaining sequentially
and earlier with the MNO, and the losses for APOs when bargaining concurrently
with the MNO. We further study the group bargaining scenario where multiple
APOs form a group bargaining with the MNO jointly, and quantify the benefits
for APOs when forming such a group. Interesting, our analysis indicates that
grouping of APOs not only benefits the APOs in the group, but may also benefit
some APOs not in the group. Our results shed light on the economic aspects and
the possible outcomes of the MNO/APOs interactions, and can be used as a
roadmap for designing policies for this promising data offloading solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5643</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5643</id><created>2014-05-22</created><authors><author><keyname>Huber</keyname><forenames>Sandra</forenames></author><author><keyname>Geiger</keyname><forenames>Martin Josef</forenames></author><author><keyname>Sevaux</keyname><forenames>Marc</forenames></author></authors><title>Interactive Reference Point-Based Guided Local Search for the
  Bi-objective Inventory Routing Problem</title><categories>cs.AI</categories><journal-ref>Proceedings of the 10th Metaheuristics International Conference
  MIC 2013, August 5-8, 2013, Singapore, Pages 152-161</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eliciting preferences of a decision maker is a key factor to successfully
combine search and decision making in an interactive method. Therefore, the
progressively integration and simulation of the decision maker is a main
concern in an application. We contribute in this direction by proposing an
interactive method based on a reference point-based guided local search to the
bi-objective Inventory Routing Problem. A local search metaheuristic, working
on the delivery intervals, and the Clarke &amp; Wright savings heuristic is
employed for the subsequently obtained Vehicle Routing Problem. To elicit
preferences, the decision maker selects a reference point to guide the search
in interesting subregions. Additionally, the reference point is used as a
reservation point to discard solutions outside the cone, introduced as a
convergence criterion. Computational results of the reference point-based
guided local search are reported and analyzed on benchmark data in order to
show the applicability of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5645</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5645</id><created>2014-05-22</created><authors><author><keyname>Stephan</keyname><forenames>Heike</forenames></author><author><keyname>Brass</keyname><forenames>Stefan</forenames></author></authors><title>A Variant of Earley Deduction With Partial Evaluation</title><categories>cs.LO cs.DB</categories><comments>WLP 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for query evaluation given a logic program consisting
of function-free Datalog rules. It is based on Earley Deduction [4, 6] and uses
a partial evaluation similar to the one we devel oped for our SLDMagic method
[1]. With this, finite automata modeling the evaluation of given queries are
generated. In certain cases, the new method is more efficient than SLDMagic and
the standard Magic Set method since it can process several deduction steps as
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5646</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5646</id><created>2014-05-22</created><authors><author><keyname>Blum</keyname><forenames>Christian</forenames></author><author><keyname>Lozano</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Davidson</keyname><forenames>Pedro Pinacho</forenames></author></authors><title>Mathematical Programming Strategies for Solving the Minimum Common
  String Partition Problem</title><categories>cs.AI cs.DS</categories><msc-class>90-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum common string partition problem is an NP-hard combinatorial
optimization problem with applications in computational biology. In this work
we propose the first integer linear programming model for solving this problem.
Moreover, on the basis of the integer linear programming model we develop a
deterministic 2-phase heuristic which is applicable to larger problem
instances. The results show that provenly optimal solutions can be obtained for
problem instances of small and medium size from the literature by solving the
proposed integer linear programming model with CPLEX. Furthermore, new
best-known solutions are obtained for all considered problem instances from the
literature. Concerning the heuristic, we were able to show that it outperforms
heuristic competitors from the related literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5648</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5648</id><created>2014-05-22</created><authors><author><keyname>Gadaleta</keyname><forenames>Francesco</forenames></author><author><keyname>Nikiforakis</keyname><forenames>Nick</forenames></author><author><keyname>Muhlberg</keyname><forenames>Jan Tobias</forenames></author><author><keyname>Joosen</keyname><forenames>Wouter</forenames></author></authors><title>HyperForce: Hypervisor-enForced Execution of Security-Critical Code</title><categories>cs.SE cs.CR</categories><comments>12 pages, SEC, Heraklion, 04-06 June 2012, IFIP Advances in
  Information and Communication Technology 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The sustained popularity of the cloud and cloud-related services accelerate
the evolution of virtualization-enabling technologies. Modern off-the-shelf
computers are already equipped with specialized hardware that enables a
hypervisor to manage the simultaneous execution of multiple operating systems.
Researchers have proposed security mechanisms that operate within such a
hypervisor to protect the \textit{virtualized} operating systems from attacks.
These mechanisms improve in security over previous techniques since the defense
system is no longer part of an operating system's attack surface. However, due
to constant transitions between the hypervisor and the operating systems, these
countermeasures typically incur a significant performance overhead.
  In this paper we present HyperForce, a framework which allows the deployment
of security-critical code in a way that significantly outperforms previous
\textit{in-hypervisor} systems while maintaining similar guarantees with
respect to security and integrity. HyperForce is a hybrid system which combines
the performance of an \textit{in-guest} security mechanism with the security of
in-hypervisor one. We evaluate our framework by using it to re-implement an
invariance-based rootkit detection system and show the performance benefits of
a HyperForce-utilizing countermeasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5651</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5651</id><created>2014-05-22</created><authors><author><keyname>Gadaleta</keyname><forenames>Francesco</forenames></author><author><keyname>Nikiforakis</keyname><forenames>Nick</forenames></author><author><keyname>Younan</keyname><forenames>Yves</forenames></author><author><keyname>Joosen</keyname><forenames>Wouter</forenames></author></authors><title>Hello rootKitty: A lightweight invariance-enforcing framework</title><categories>cs.OS cs.CR</categories><comments>16 pages, ISC Information Security Conference, Xi'an China, 2011,
  Springer</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In monolithic operating systems, the kernel is the piece of code that
executes with the highest privileges and has control over all the software
running on a host. A successful attack against an operating system's kernel
means a total and complete compromise of the running system. These attacks
usually end with the installation of a rootkit, a stealthy piece of software
running with kernel privileges. When a rootkit is present, no guarantees can be
made about the correctness, privacy or isolation of the operating system.
  In this paper we present \emph{Hello rootKitty}, an invariance-enforcing
framework which takes advantage of current virtualization technology to protect
a guest operating system against rootkits. \emph{Hello rootKitty} uses the idea
of invariance to detect maliciously modified kernel data structures and restore
them to their original legitimate values. Our prototype has negligible
performance and memory overhead while effectively protecting commodity
operating systems from modern rootkits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5654</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5654</id><created>2014-05-22</created><authors><author><keyname>Chen</keyname><forenames>Lijiang</forenames></author></authors><title>Machine Translation Model based on Non-parallel Corpus and
  Semi-supervised Transductive Learning</title><categories>cs.CL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the parallel corpus has an irreplaceable role in machine
translation, its scale and coverage is still beyond the actual needs.
Non-parallel corpus resources on the web have an inestimable potential value in
machine translation and other natural language processing tasks. This article
proposes a semi-supervised transductive learning method for expanding the
training corpus in statistical machine translation system by extracting
parallel sentences from the non-parallel corpus. This method only requires a
small amount of labeled corpus and a large unlabeled corpus to build a
high-performance classifier, especially for when there is short of labeled
corpus. The experimental results show that by combining the non-parallel corpus
alignment and the semi-supervised transductive learning method, we can more
effectively use their respective strengths to improve the performance of
machine translation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5661</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5661</id><created>2014-05-22</created><authors><author><keyname>Li</keyname><forenames>Yan Kit</forenames></author><author><keyname>Xu</keyname><forenames>Min</forenames></author><author><keyname>Ng</keyname><forenames>Chun Ho</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author></authors><title>Efficient Hybrid Inline and Out-of-line Deduplication for Backup Storage</title><categories>cs.DC cs.DB</categories><acm-class>D.4.2; D.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backup storage systems often remove redundancy across backups via inline
deduplication, which works by referring duplicate chunks of the latest backup
to those of existing backups. However, inline deduplication degrades restore
performance of the latest backup due to fragmentation, and complicates deletion
of ex- pired backups due to the sharing of data chunks. While out-of-line
deduplication addresses the problems by forward-pointing existing duplicate
chunks to those of the latest backup, it introduces additional I/Os of writing
and removing duplicate chunks. We design and implement RevDedup, an efficient
hybrid inline and out-of-line deduplication system for backup storage. It
applies coarse-grained inline deduplication to remove duplicates of the latest
backup, and then fine-grained out-of-line reverse deduplication to remove
duplicates from older backups. Our reverse deduplication design limits the I/O
overhead and prepares for efficient deletion of expired backups. Through
extensive testbed experiments using synthetic and real-world datasets, we show
that RevDedup can bring high performance to the backup, restore, and deletion
operations, while maintaining high storage efficiency comparable to
conventional inline deduplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5662</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5662</id><created>2014-05-22</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Zantema</keyname><forenames>Hans</forenames></author></authors><title>Non-termination using Regular Languages</title><categories>cs.LO</categories><comments>Published at International Workshop on Termination 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for proving non-looping non-termination, that is, of
term rewriting systems that do not admit looping reductions. As certificates of
non-termination, we employ regular (tree) automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5668</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5668</id><created>2014-05-22</created><authors><author><keyname>Magron</keyname><forenames>Victor</forenames></author></authors><title>NLCertify: A Tool for Formal Nonlinear Optimization</title><categories>cs.MS cs.LO math.OC</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NLCertify is a software package for handling formal certification of
nonlinear inequalities involving transcendental multivariate functions. The
tool exploits sparse semialgebraic optimization techniques with approximation
methods for transcendental functions, as well as formal features. Given a box
and a transcendental multivariate function as input, NLCertify provides OCaml
libraries that produce nonnegativity certificates for the function over the
box, which can be ultimately proved correct inside the Coq proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5669</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5669</id><created>2014-05-22</created><authors><author><keyname>Ramani</keyname><forenames>Sagar V.</forenames></author><author><keyname>Tank</keyname><forenames>Yagnik N.</forenames></author></authors><title>Indoor Navigation on Google Maps and Indoor Localization Using RSS
  Fingerprinting</title><categories>cs.NI cs.CY</categories><comments>3 pages, 2 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>Sagar V. Ramani , Yagnik N. Tank. &quot;Indoor Navigation on Google
  Maps and Indoor Localization Using RSS Fingerprinting&quot;, International Journal
  of Engineering Trends and Technology (IJETT), V11(4),171-173 May 2014.
  ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V11P234</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Contrasting to advances in street/outdoor navigation, wall mounted maps and
signs continue to be the primary reference indoor navigation in hospitals,
malls, museums, etc. The proliferation of mobile devices and the growing demand
for location aware systems that filter information based on currently device
location have led to an increase in research and product development in this
field. An attempt has been made to provide solution for indoor navigation on
Google maps and to provide location of a user in a building using Wi-Fi signal
strength on android Smartphone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5671</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5671</id><created>2014-05-22</created><authors><author><keyname>Gabillon</keyname><forenames>Alban</forenames><affiliation>GePaSUD</affiliation></author></authors><title>A Logical Formalization of a Secure XML Database</title><categories>cs.DB cs.CR cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first define a logical theory representing an XML database
supporting XPath as query language and XUpdate as modification language. We
then extend our theory with predicates allowing us to specify the security
policy protecting the database. The security policy includes rules addressing
the read and write privileges. We propose axioms to derive the database view
each user is permitted to see. We also propose axioms to derive the new
database content after an update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5674</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5674</id><created>2014-05-22</created><authors><author><keyname>Mangeot</keyname><forenames>Mathieu</forenames><affiliation>LIG</affiliation></author></authors><title>Mot\`aMot project: conversion of a French-Khmer published dictionary for
  building a multilingual lexical system</title><categories>cs.CL</categories><comments>8 pages, Languages Resources and Evaluation Conference, Reykjavik :
  Iceland (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Economic issues related to the information processing techniques are very
important. The development of such technologies is a major asset for developing
countries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and
Thailand. The MotAMot project aims to computerize an under-resourced language:
Khmer, spoken mainly in Cambodia. The main goal of the project is the
development of a multilingual lexical system targeted for Khmer. The
macrostructure is a pivot one with each word sense of each language linked to a
pivot axi. The microstructure comes from a simplification of the explanatory
and combinatory dictionary. The lexical system has been initialized with data
coming mainly from the conversion of the French-Khmer bilingual dictionary of
Denis Richer from Word to XML format. The French part was completed with
pronunciation and parts-of-speech coming from the FeM French-english-Malay
dictionary. The Khmer headwords noted in IPA in the Richer dictionary were
converted to Khmer writing with OpenFST, a finite state transducer tool. The
resulting resource is available online for lookup, editing, download and remote
programming via a REST API on a Jibiki platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5689</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5689</id><created>2014-05-22</created><updated>2015-02-17</updated><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Kopinsky</keyname><forenames>Justin</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author><author><keyname>Shavit</keyname><forenames>Nir</forenames></author></authors><title>Inherent Limitations of Hybrid Transactional Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several Hybrid Transactional Memory (HyTM) schemes have recently been
proposed to complement the fast, but best-effort, nature of Hardware
Transactional Memory (HTM) with a slow, reliable software backup. However, the
fundamental limitations of building a HyTM with nontrivial concurrency between
hardware and software transactions are still not well understood.
  In this paper, we propose a general model for HyTM implementations, which
captures the ability of hardware transactions to buffer memory accesses, and
allows us to formally quantify and analyze the amount of overhead
(instrumentation) of a HyTM scheme. We prove the following: (1) it is
impossible to build a strictly serializable HyTM implementation that has both
uninstrumented reads and writes, even for weak progress guarantees, and (2)
under reasonable assumptions, in any opaque progressive HyTM, a hardware
transaction must incur instrumentation costs linear in the size of its data
set. We further provide two upper bound implementations whose instrumentation
costs are optimal with respect to their progress guarantees. In sum, this paper
captures for the first time an inherent trade-off between the degree of
concurrency a HyTM provides between hardware and software transactions, and the
amount of instrumentation overhead the implementation must incur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5704</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5704</id><created>2014-05-22</created><authors><author><keyname>Trujillo-Rasua</keyname><forenames>Rolando</forenames></author><author><keyname>Martin</keyname><forenames>Benjamin</forenames></author><author><keyname>Avoine</keyname><forenames>Gildas</forenames></author></authors><title>Distance-bounding facing both mafia and distance frauds: Technical
  report*</title><categories>cs.CR</categories><comments>This 23-page long document contains content accepted for publication
  at the IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contactless technologies such as RFID, NFC, and sensor networks are
vulnerable to mafia and distance frauds. Both frauds aim at passing an
authentication protocol by cheating on the actual distance between the prover
and the verifier. To cope these security issues, distance-bounding protocols
have been designed. However, none of the current proposals simultaneously
resists to these two frauds without requiring additional memory and
computation. The situation is even worse considering that just a few
distance-bounding protocols are able to deal with the inherent background noise
on the communication channels. This article introduces a noise-resilient
distance-bounding protocol that resists to both mafia and distance frauds. The
security of the protocol is analyzed with respect to these two frauds in both
scenarios, namely noisy and noiseless channels. Analytical expressions for the
adversary's success probabilities are provided, and are illustrated by
experimental results. The analysis, performed in an already existing framework
for fairness reasons, demonstrates the undeniable advantage of the introduced
lightweight design over the previous proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5726</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5726</id><created>2014-05-22</created><updated>2015-09-27</updated><authors><author><keyname>Formentin</keyname><forenames>Marco</forenames></author><author><keyname>Lovison</keyname><forenames>Alberto</forenames></author><author><keyname>Maritan</keyname><forenames>Amos</forenames></author><author><keyname>Zanzotto</keyname><forenames>Giovanni</forenames></author></authors><title>New activity pattern in human interactive dynamics</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>15 pages, 7 figures</comments><journal-ref>J. Stat. Mech. 2015 P09006</journal-ref><doi>10.1088/1742-5468/2015/09/P09006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the response function of human agents as demonstrated by
written correspondence, uncovering a new universal pattern for how the reactive
dynamics of individuals is distributed across the set of each agent's contacts.
In long-term empirical data on email, we find that the set of response times
considered separately for the messages to each different correspondent of a
given writer, generate a family of heavy-tailed distributions, which have
largely the same features for all agents, and whose characteristic times grow
exponentially with the rank of each correspondent. We furthermore show that
this universal behavioral pattern emerges robustly by considering weighted
moving averages of the priority-conditioned response-time probabilities
generated by a basic prioritization model. Our findings clarify how the range
of priorities in the inputs from one's environment underpin and shape the
dynamics of agents embedded in a net of reactive relations. These newly
revealed activity patterns might be present in other general interactive
environments, and constrain future models of communication and interaction
networks, affecting their architecture and evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5730</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5730</id><created>2014-05-22</created><authors><author><keyname>Huang</keyname><forenames>Xueqing</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Joint Spectrum and Power Allocation for Multi-node Cooperative Wireless
  Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is a growing concern for wireless networks, not only due to
the emerging traffic demand from smart devices, but also because of the
dependence on the traditional unsustainable energy and the overall
environmental concerns. The urgent call for reducing power consumption while
meeting system requirements has motivated increasing research efforts on green
radio. In this paper, we investigate a new joint spectrum and power allocation
scheme for a cooperative downlink multi-user system using the frequency
division multiple access scheme, in which arbitrary M base stations (BSs)
coordinately allocate their resources to each user equipment (UE). With the
assumption that multi-BS UE (user being served by multi-BS) would require the
same amount of spectrum from these BSs, we conclude that when the number of
multi-BS UEs is limited by M-1, the resource allocation scheme can always
guarantee the minimum overall transmit power consumption while meeting the
throughput requirement of each UE and also each BS's power constraint. Then, to
decide the clusters of multi-BS UEs and the clusters of individual-BS UEs
(users being served by individual BSs), we propose a UE-BS association scheme
and a complexity reduction scheme. Finally, a novel joint spectrum and power
allocation algorithm is proposed to minimize the total power consumption.
Simulation results are presented to verify the optimality of the derived
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5732</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5732</id><created>2014-05-22</created><updated>2014-05-26</updated><authors><author><keyname>Azizpour</keyname><forenames>Hossein</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>Self-tuned Visual Subclass Learning with Shared Samples An Incremental
  Approach</title><categories>cs.CV</categories><comments>Updated ICCV 2013 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision tasks are traditionally defined and evaluated using semantic
categories. However, it is known to the field that semantic classes do not
necessarily correspond to a unique visual class (e.g. inside and outside of a
car). Furthermore, many of the feasible learning techniques at hand cannot
model a visual class which appears consistent to the human eye. These problems
have motivated the use of 1) Unsupervised or supervised clustering as a
preprocessing step to identify the visual subclasses to be used in a
mixture-of-experts learning regime. 2) Felzenszwalb et al. part model and other
works model mixture assignment with latent variables which is optimized during
learning 3) Highly non-linear classifiers which are inherently capable of
modelling multi-modal input space but are inefficient at the test time. In this
work, we promote an incremental view over the recognition of semantic classes
with varied appearances. We propose an optimization technique which
incrementally finds maximal visual subclasses in a regularized risk
minimization framework. Our proposed approach unifies the clustering and
classification steps in a single algorithm. The importance of this approach is
its compliance with the classification via the fact that it does not need to
know about the number of clusters, the representation and similarity measures
used in pre-processing clustering methods a priori. Following this approach we
show both qualitatively and quantitatively significant results. We show that
the visual subclasses demonstrate a long tail distribution. Finally, we show
that state of the art object detection methods (e.g. DPM) are unable to use the
tails of this distribution comprising 50\% of the training samples. In fact we
show that DPM performance slightly increases on average by the removal of this
half of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5737</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5737</id><created>2014-05-22</created><updated>2014-09-26</updated><authors><author><keyname>Mahmood</keyname><forenames>Arif</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal S.</forenames></author></authors><title>Semi-supervised Spectral Clustering for Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Classification Via Clustering (CVC) algorithm which enables
existing clustering methods to be efficiently employed in classification
problems. In CVC, training and test data are co-clustered and class-cluster
distributions are used to find the label of the test data. To determine an
efficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC)
algorithm is proposed. Clusters are obtained by hierarchically applying two-way
NCut by using signs of the Fiedler vector of the normalized graph Laplacian. To
this end, a Direct Fiedler Vector Computation algorithm is proposed. The graph
cut is based on the data structure and does not consider labels. Labels are
used only to define the stopping criterion for graph cut. We propose clustering
to be performed on the Grassmannian manifolds facilitating the formation of
spectral ensembles. The proposed algorithm outperformed state-of-the-art
image-set classification algorithms on five standard datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5741</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5741</id><created>2014-05-22</created><authors><author><keyname>Reed</keyname><forenames>Stephen L.</forenames></author></authors><title>Bitcoin Cooperative Proof-of-Stake</title><categories>cs.CY</categories><comments>16 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A hard-fork reconfiguration of the peer to peer Bitcoin network is described
that substitutes tamper-evident logs and proof-of-stake consensus for
proof-of-work consensus. The block creation rewards and transaction fees are
reallocated to establish and staff a secure financial data network capable of
handling the world's transactions with subsecond response time. The new system
pays dividends to stake-offering bitcoin holders. In contrast to Satoshi
Nakamoto's mesh network consisting of competing peers, this system uses an
enterprise class network that is efficient, robust, and scalable, consisting of
cooperating peers. The network backbone nodes host trustless nomadic agents.
Thousands of distributed full nodes are paid to replicate a singleton
blockchain built upon every 10 minutes by a nomadic mint agent whose actions
are verified by its peers. This arrangement enables immediate acknowledgment to
an issuing node that its transaction has been accepted. Less effort means that
subsidized transaction costs will be lower. Network reconfiguration enables the
processing of numerous microtransactions. Stake-weighted distributed consensus
is achieved when necessary with less than one-half arbitrarily faulty nodes.
Important invariants of the Satoshi Social Contract between core developers and
users are maintained: The reward schedule, the blockchain format, the fixed
number of bitcoins, and the decentralized, trustless protocol are untouched.
The system remains a global distributed database, with additions to the
database by consent of the majority, based on a set of transparent rules they
follow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5747</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5747</id><created>2014-05-22</created><updated>2014-08-06</updated><authors><author><keyname>Huang</keyname><forenames>Xueqing</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>On Green Energy Powered Cognitive Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Green energy powered cognitive radio (CR) network is capable of liberating
the wireless access networks from spectral and energy constraints. The
limitation of the spectrum is alleviated by exploiting cognitive networking in
which wireless nodes sense and utilize the spare spectrum for data
communications, while dependence on the traditional unsustainable energy is
assuaged by adopting energy harvesting (EH) through which green energy can be
harnessed to power wireless networks. Green energy powered CR increases the
network availability and thus extends emerging network applications. Designing
green CR networks is challenging. It requires not only the optimization of
dynamic spectrum access but also the optimal utilization of green energy. This
paper surveys the energy efficient cognitive radio techniques and the
optimization of green energy powered wireless networks. Existing works on
energy aware spectrum sensing, management, and sharing are investigated in
detail. The state of the art of the energy efficient CR based wireless access
network is discussed in various aspects such as relay and cooperative radio and
small cells. Envisioning green energy as an important energy resource in the
future, network performance highly depends on the dynamics of the available
spectrum and green energy. As compared with the traditional energy source, the
arrival rate of green energy, which highly depends on the environment of the
energy harvesters, is rather random and intermittent. To optimize and adapt the
usage of green energy according to the opportunistic spectrum availability, we
discuss research challenges in designing cognitive radio networks which are
powered by energy harvesters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5753</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5753</id><created>2014-05-22</created><authors><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Malone</keyname><forenames>David</forenames></author></authors><title>Modeling, Analysis and Impact of a Long Transitory Phase in Random
  Access Protocols</title><categories>cs.NI</categories><comments>13 pages, 10 figures, Submitted to IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In random access protocols, the service rate depends on the number of
stations with a packet buffered for transmission. We demonstrate via numerical
analysis that this state-dependent rate along with the consideration of Poisson
traffic and infinite (or large enough to be considered infinite) buffer size
may cause a high-throughput and extremely long (in the order of hours)
transitory phase when traffic arrivals are right above the stability limit. We
also perform an experimental evaluation to provide further insight into the
characterisation of this transitory phase of the network by analysing
statistical properties of its duration. The identification of the presence as
well as the characterisation of this behaviour is crucial to avoid
misprediction, which has a significant potential impact on network performance
and optimisation. Furthermore, we discuss practical implications of this
finding and propose a distributed and low-complexity mechanism to keep the
network operating in the high-throughput phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5754</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5754</id><created>2014-05-22</created><updated>2014-06-24</updated><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Frank</keyname><forenames>Michael</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and
  Twenty-Nine for Ten)</title><categories>cs.DM cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a computer-assisted non-existence proof of nine-input
sorting networks consisting of 24 comparators, hence showing that the
25-comparator sorting network found by Floyd in 1964 is optimal. As a
corollary, we obtain that the 29-comparator network found by Waksman in 1969 is
optimal when sorting ten inputs.
  This closes the two smallest open instances of the optimal size sorting
network problem, which have been open since the results of Floyd and Knuth from
1966 proving optimality for sorting networks of up to eight inputs.
  The proof involves a combination of two methodologies: one based on
exploiting the abundance of symmetries in sorting networks, and the other,
based on an encoding of the problem to that of satisfiability of propositional
logic. We illustrate that, while each of these can single handed solve smaller
instances of the problem, it is their combination which leads to an efficient
solution for nine inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5755</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5755</id><created>2014-05-22</created><authors><author><keyname>Duarte</keyname><forenames>Eduardo Ruiz</forenames></author><author><keyname>Osuna</keyname><forenames>Octavio P&#xe1;ez</forenames></author></authors><title>Explicit endomorphism of the Jacobian of a hyperelliptic function field
  of genus 2 using base field operations</title><categories>math.AG cs.CR</categories><comments>3 figures, presented at CECC2014, Budapest Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient endomorphism for the Jacobian of a curve $C$ of genus
2 (hyperelliptic) for divisors having a Non disjoint support. This extends the
work of Costello and Lauter in [12] who calculated explicit formulae for
divisor doubling and addition of divisors with disjoint support in
$\mathbb{J}(C)$ using only base field operations. Explicit formulae is
presented for this third case and a slightly different approach for divisor
doubling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5756</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5756</id><created>2014-05-22</created><authors><author><keyname>Peng</keyname><forenames>Luo</forenames></author><author><keyname>Yongli</keyname><forenames>Li</forenames></author><author><keyname>Chong</keyname><forenames>Wu</forenames></author></authors><title>Towards Cost-efficient Sampling Methods</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sampling method has been paid much attention in the field of complex
network in general and statistical physics in particular. This paper presents
two new sampling methods based on the perspective that a small part of vertices
with high node degree can possess the most structure information of a network.
The two proposed sampling methods are efficient in sampling the nodes with high
degree. The first new sampling method is improved on the basis of the
stratified random sampling method and selects the high degree nodes with higher
probability by classifying the nodes according to their degree distribution.
The second sampling method improves the existing snowball sampling method so
that it enables to sample the targeted nodes selectively in every sampling
step. Besides, the two proposed sampling methods not only sample the nodes but
also pick the edges directly connected to these nodes. In order to demonstrate
the two methods' availability and accuracy, we compare them with the existing
sampling methods in three commonly used simulation networks that are scale-free
network, random network, small-world network, and two real networks. The
experimental results show that the two proposed sampling methods perform much
better than the compared existing sampling methods in terms of sampling cost
and obtaining the true network structural characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5764</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5764</id><created>2014-05-22</created><updated>2014-07-07</updated><authors><author><keyname>Huang</keyname><forenames>Xueqing</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Optimal Cooperative Power Allocation for Energy Harvesting Enabled Relay
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new power allocation scheme for a
decode-and-forward (DF) relaying-enhanced cooperative wireless system. While
both source and relay nodes may have limited traditional brown power supply or
fixed green energy storage, the hybrid source node can also draw power from the
surrounding radio frequency (RF) signals. In particular, we assume a
deterministic RF energy harvesting (EH) model under which the signals
transmitted by the relay serve as the renewable energy source for the source
node. The amount of harvested energy is known for a given transmission power of
the forwarding signal and channel condition between the source and relay nodes.
To maximize the overall throughput while meeting the constraints imposed by the
non-sustainable energy sources and the renewable energy source, an optimization
problem is formulated and solved. Based on different harvesting efficiency and
channel condition, closed form solutions are derived to obtain the optimal
source and relay power allocation jointly. It is shown that instead of
demanding high on-grid power supply or high green energy availability, the
system can achieve compatible or higher throughput by utilizing the harvested
energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5769</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5769</id><created>2014-05-22</created><updated>2015-06-24</updated><authors><author><keyname>Fischer</keyname><forenames>Philipp</forenames></author><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Descriptor Matching with Convolutional Neural Networks: a Comparison to
  SIFT</title><categories>cs.CV cs.LG</categories><comments>This paper has been merged with arXiv:1406.6909</comments><acm-class>I.2.6; I.4.7; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latest results indicate that features learned via convolutional neural
networks outperform previous descriptors on classification tasks by a large
margin. It has been shown that these networks still work well when they are
applied to datasets or recognition tasks different from those they were trained
on. However, descriptors like SIFT are not only used in recognition but also
for many correspondence problems that rely on descriptor matching. In this
paper we compare features from various layers of convolutional neural nets to
standard SIFT descriptors. We consider a network that was trained on ImageNet
and another one that was trained without supervision. Surprisingly,
convolutional neural networks clearly outperform SIFT on descriptor matching.
This paper has been merged with arXiv:1406.6909
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5777</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5777</id><created>2014-05-22</created><authors><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Perera</keyname><forenames>Roly</forenames></author></authors><title>An Analytical Survey of Provenance Sanitization</title><categories>cs.DB cs.CR</categories><comments>To appear, IPAW 2014</comments><acm-class>H.3.5; D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is likely becoming a critical factor in the future adoption of
provenance technology, because of the risk of inadvertent disclosure of
sensitive information. In this survey paper we review the state of the art in
secure provenance, considering mechanisms for controlling access, and the
extent to which these mechanisms preserve provenance integrity. We examine
seven systems or approaches, comparing features and identifying areas for
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5793</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5793</id><created>2014-05-22</created><updated>2014-05-26</updated><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author></authors><title>Extended AIGER Format for Synthesis</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the AIGER format, as used in HWMCC, to a format that is suitable to
define synthesis problems with safety specifications. We recap the original
format and define one format for posing synthesis problems and one for
solutions of synthesis problems in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5827</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5827</id><created>2014-05-22</created><updated>2015-01-06</updated><authors><author><keyname>Leung</keyname><forenames>Samantha</forenames></author><author><keyname>Lui</keyname><forenames>Edward</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Voting with Coarse Beliefs</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic Gibbard-Satterthwaite theorem says that every strategy-proof
voting rule with at least three possible candidates must be dictatorial.
Similar impossibility results hold even if we consider a weaker notion of
strategy-proofness where voters believe that the other voters' preferences are
i.i.d.~(independent and identically distributed). In this paper, we take a
bounded-rationality approach to this problem and consider a setting where
voters have &quot;coarse&quot; beliefs (a notion that has gained popularity in the
behavioral economics literature). In particular, we construct good voting rules
that satisfy a notion of strategy-proofness with respect to coarse
i.i.d.~beliefs, thus circumventing the above impossibility results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5829</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5829</id><created>2014-05-22</created><authors><author><keyname>Dallachiesa</keyname><forenames>Michele</forenames></author><author><keyname>Aggarwal</keyname><forenames>Charu</forenames></author><author><keyname>Palpanas</keyname><forenames>Themis</forenames></author></authors><title>Node Classification in Uncertain Graphs</title><categories>cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real applications that use and analyze networked data, the links in
the network graph may be erroneous, or derived from probabilistic techniques.
In such cases, the node classification problem can be challenging, since the
unreliability of the links may affect the final results of the classification
process. If the information about link reliability is not used explicitly, the
classification accuracy in the underlying network may be affected adversely. In
this paper, we focus on situations that require the analysis of the uncertainty
that is present in the graph structure. We study the novel problem of node
classification in uncertain graphs, by treating uncertainty as a first-class
citizen. We propose two techniques based on a Bayes model and automatic
parameter selection, and show that the incorporation of uncertainty in the
classification process as a first-class citizen is beneficial. We
experimentally evaluate the proposed approach using different real data sets,
and study the behavior of the algorithms under different conditions. The
results demonstrate the effectiveness and efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5845</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5845</id><created>2014-05-22</created><authors><author><keyname>Pringle</keyname><forenames>Ben</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Mukkai</forenames></author><author><keyname>Simons</keyname><forenames>Kenneth</forenames></author></authors><title>Case study to approaches to finding patterns in citation networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>16 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Analysis of a dataset including a network of LED patents and their metadata
is carried out using several methods in order to answer questions about the
domain. We are interested in finding the relationship between the metadata and
the network structure; for example, are central patents in the network produced
by larger or smaller companies? We begin by exploring the structure of the
network without any metadata, applying known techniques in citation analysis
and a simple clustering scheme. These techinques are then combined with
metadata analysis to draw preliminary conclusions about the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5848</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5848</id><created>2014-05-22</created><updated>2015-08-13</updated><authors><author><keyname>Gammell</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha S.</forenames></author><author><keyname>Barfoot</keyname><forenames>Timothy D.</forenames></author></authors><title>Batch Informed Trees (BIT*): Sampling-based Optimal Planning via the
  Heuristically Guided Search of Implicit Random Geometric Graphs</title><categories>cs.RO</categories><comments>8 Pages. 6 Figures. Video available at
  http://www.youtube.com/watch?v=TQIoCC48gp4</comments><journal-ref>2015 IEEE International Conference on Robotics and Automation
  (ICRA 2015), pp. 3067-3074, 26-30 May 2015</journal-ref><doi>10.1109/ICRA.2015.7139620</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present Batch Informed Trees (BIT*), a planning algorithm
based on unifying graph- and sampling-based planning techniques. By recognizing
that a set of samples describes an implicit random geometric graph (RGG), we
are able to combine the efficient ordered nature of graph-based techniques,
such as A*, with the anytime scalability of sampling-based algorithms, such as
Rapidly-exploring Random Trees (RRT).
  BIT* uses a heuristic to efficiently search a series of increasingly dense
implicit RGGs while reusing previous information. It can be viewed as an
extension of incremental graph-search techniques, such as Lifelong Planning A*
(LPA*), to continuous problem domains as well as a generalization of existing
sampling-based optimal planners. It is shown that it is probabilistically
complete and asymptotically optimal.
  We demonstrate the utility of BIT* on simulated random worlds in
$\mathbb{R}^2$ and $\mathbb{R}^8$ and manipulation problems on CMU's HERB, a
14-DOF two-armed robot. On these problems, BIT* finds better solutions faster
than RRT, RRT*, Informed RRT*, and Fast Marching Trees (FMT*) with faster
anytime convergence towards the optimum, especially in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5854</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5854</id><created>2014-05-22</created><authors><author><keyname>Valerdi</keyname><forenames>Juan Luis</forenames></author></authors><title>Diferenciaci\'on Autom\'atica Anidada. Un enfoque algebraico</title><categories>cs.SC</categories><comments>51 pages, in Spanish, bachelor thesis</comments><msc-class>08A99</msc-class><acm-class>G.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
diferenciaci\'on autom\'atica y se calculen el valor y la derivada que se
requiera. Tambi\'en se presenta un enfoque algebraico de la Diferenciaci\'on
Autom\'atica y en particular del espacio de los SuperAdoubles.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required. Also, an Automatic Differentiation algebraic point
of view is presented with particular emphasis in Nested Automatic
Differentiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5860</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5860</id><created>2014-05-19</created><authors><author><keyname>Belavkin</keyname><forenames>Roman V.</forenames></author></authors><title>Asymmetry of Risk and Value of Information</title><categories>math.OC cs.GT cs.IT math.IT math.PR</categories><comments>16 pages, 2 figures</comments><doi>10.1007/978-3-319-10046-3_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The von Neumann and Morgenstern theory postulates that rational choice under
uncertainty is equivalent to maximization of expected utility (EU). This view
is mathematically appealing and natural because of the affine structure of the
space of probability measures. Behavioural economists and psychologists, on the
other hand, have demonstrated that humans consistently violate the EU postulate
by switching from risk-averse to risk-taking behaviour. This paradox has led to
the development of descriptive theories of decisions, such as the celebrated
prospect theory, which uses an $S$-shaped value function with concave and
convex branches explaining the observed asymmetry. Although successful in
modelling human behaviour, these theories appear to contradict the natural set
of axioms behind the EU postulate. Here we show that the observed asymmetry in
behaviour can be explained if, apart from utilities of the outcomes, rational
agents also value information communicated by random events. We review the main
ideas of the classical value of information theory and its generalizations.
Then we prove that the value of information is an $S$-shaped function, and that
its asymmetry does not depend on how the concept of information is defined, but
follows only from linearity of the expected utility. Thus, unlike many
descriptive and `non-expected' utility theories that abandon the linearity
(i.e. the `independence' axiom), we formulate a rigorous argument that the von
Neumann and Morgenstern rational agents should be both risk-averse and
risk-taking if they are not indifferent to information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5864</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5864</id><created>2014-05-22</created><authors><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ott</keyname><forenames>David</forenames></author><author><keyname>Foerster</keyname><forenames>Jeffrey R.</forenames></author><author><keyname>Bethanabhotla</keyname><forenames>Dilip</forenames></author><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author></authors><title>Caching Eliminates the Wireless Bottleneck in Video-Aware Wireless
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>12 pages, 9 figures, An overview paper submitted to Hindawi's journal
  of Advances in Electrical Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular data traffic almost doubles every year, greatly straining network
capacity. The main driver for this development is wireless video. Traditional
methods for capacity increase (like using more spectrum and increasing base
station density) are very costly, and do not exploit the unique features of
video, in particular a high degree of {\em asynchronous content reuse}. In this
paper we give an overview of our work that proposed and detailed a new
transmission paradigm exploiting content reuse, and the fact that storage is
the fastest-increasing quantity in modern hardware. Our network structure uses
caching in helper stations (femto-caching) and/or devices, combined with highly
spectrally efficient short-range communications to deliver video files. For
femto-caching, we develop optimum storage schemes and dynamic streaming
policies that optimize video quality. For caching on devices, combined with
device-to-device communications, we show that communications within {\em
clusters} of mobile stations should be used; the cluster size can be adjusted
to optimize the tradeoff between frequency reuse and the probability that a
device finds a desired file cached by another device in the same cluster. We
show that in many situations the network throughput increases linearly with the
number of users, and that D2D communications also is superior in providing a
better tradeoff between throughput and outage than traditional base-station
centric systems. Simulation results with realistic numbers of users and channel
conditions show that network throughput (possibly with outage constraints) can
be increased by two orders of magnitude compared to conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5867</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5867</id><created>2014-05-22</created><authors><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author></authors><title>MOSDEN: A Scalable Mobile Collaborative Platform for Opportunistic
  Sensing Applications</title><categories>cs.NI</categories><comments>Accepted to be published in Transactions on Collaborative Computing,
  2014. arXiv admin note: substantial text overlap with arXiv:1310.4052</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile smartphones along with embedded sensors have become an efficient
enabler for various mobile applications including opportunistic sensing. The
hi-tech advances in smartphones are opening up a world of possibilities. This
paper proposes a mobile collaborative platform called MOSDEN that enables and
supports opportunistic sensing at run time. MOSDEN captures and shares sensor
data across multiple apps, smartphones and users. MOSDEN supports the emerging
trend of separating sensors from application-specific processing, storing and
sharing. MOSDEN promotes reuse and re-purposing of sensor data hence reducing
the efforts in developing novel opportunistic sensing applications. MOSDEN has
been implemented on Android-based smartphones and tablets. Experimental
evaluations validate the scalability and energy efficiency of MOSDEN and its
suitability towards real world applications. The results of evaluation and
lessons learned are presented and discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5868</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5868</id><created>2014-05-22</created><updated>2014-11-10</updated><authors><author><keyname>Atwood</keyname><forenames>James</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Gile</keyname><forenames>Krista</forenames></author><author><keyname>Jensen</keyname><forenames>David</forenames></author></authors><title>Learning to Generate Networks</title><categories>cs.LG cs.SI physics.soc-ph</categories><comments>Neural Information Processing Systems 2014 Workshop on Networks: From
  Graphs to Rich Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of learning to generate complex networks from
data. Specifically, we consider whether deep belief networks, dependency
networks, and members of the exponential random graph family can learn to
generate networks whose complex behavior is consistent with a set of input
examples. We find that the deep model is able to capture the complex behavior
of small networks, but that no model is able capture this behavior for networks
with more than a handful of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5869</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5869</id><created>2014-05-22</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search
  (MIPS)</title><categories>stat.ML cs.DS cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first provably sublinear time algorithm for approximate
\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first
hashing algorithm for searching with (un-normalized) inner product as the
underlying similarity measure. Finding hashing schemes for MIPS was considered
hard. We formally show that the existing Locality Sensitive Hashing (LSH)
framework is insufficient for solving MIPS, and then we extend the existing LSH
framework to allow asymmetric hashing schemes. Our proposal is based on an
interesting mathematical phenomenon in which inner products, after independent
asymmetric transformations, can be converted into the problem of approximate
near neighbor search. This key observation makes efficient sublinear hashing
scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we
provide an explicit construction of provably fast hashing scheme for MIPS. The
proposed construction and the extended LSH framework could be of independent
theoretical interest. Our proposed algorithm is simple and easy to implement.
We evaluate the method, for retrieving inner products, in the collaborative
filtering task of item recommendations on Netflix and Movielens datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5873</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5873</id><created>2014-05-22</created><authors><author><keyname>Vlachos</keyname><forenames>Michail</forenames></author><author><keyname>Freris</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author></authors><title>Compressive Mining: Fast and Optimal Data Mining in the Compressed
  Domain</title><categories>stat.ML cs.DS cs.IT math.IT</categories><comments>25 pages, 20 figures, accepted in VLDB</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world data typically contain repeated and periodic patterns. This
suggests that they can be effectively represented and compressed using only a
few coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).
However, distance estimation when the data are represented using different sets
of coefficients is still a largely unexplored area. This work studies the
optimization problems related to obtaining the \emph{tightest} lower/upper
bound on Euclidean distances when each data object is potentially compressed
using a different set of orthonormal coefficients. Our technique leads to
tighter distance estimates, which translates into more accurate search,
learning and mining operations \textit{directly} in the compressed domain.
  We formulate the problem of estimating lower/upper distance bounds as an
optimization problem. We establish the properties of optimal solutions, and
leverage the theoretical analysis to develop a fast algorithm to obtain an
\emph{exact} solution to the problem. The suggested solution provides the
tightest estimation of the $L_2$-norm or the correlation. We show that typical
data-analysis operations, such as k-NN search or k-Means clustering, can
operate more accurately using the proposed compression and distance
reconstruction technique. We compare it with many other prevalent compression
and reconstruction techniques, including random projections and PCA-based
techniques. We highlight a surprising result, namely that when the data are
highly sparse in some basis, our technique may even outperform PCA-based
compression.
  The contributions of this work are generic as our methodology is applicable
to any sequential or high-dimensional data as well as to any orthogonal data
transformation used for the underlying data compression scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5887</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5887</id><created>2014-05-22</created><authors><author><keyname>Zhan</keyname><forenames>Jinchun</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author></authors><title>Performance Guarantees for ReProCS -- Correlated Low-Rank Matrix Entries
  Case</title><categories>cs.IT math.IT</categories><comments>long version of conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online or recursive robust PCA can be posed as a problem of recovering a
sparse vector, $S_t$, and a dense vector, $L_t$, which lies in a slowly
changing low-dimensional subspace, from $M_t:= S_t + L_t$ on-the-fly as new
data comes in. For initialization, it is assumed that an accurate knowledge of
the subspace in which $L_0$ lies is available. In recent works, Qiu et al
proposed and analyzed a novel solution to this problem called recursive
projected compressed sensing or ReProCS. In this work, we relax one limiting
assumption of Qiu et al's result. Their work required that the $L_t$'s be
mutually independent over time. However this is not a practical assumption,
e.g., in the video application, $L_t$ is the background image sequence and one
would expect it to be correlated over time. In this work we relax this and
allow the $L_t$'s to follow an autoregressive model. We are able to show that
under mild assumptions and under a denseness assumption on the unestimated part
of the changed subspace, with high probability (w.h.p.), ReProCS can exactly
recover the support set of $S_t$ at all times; the reconstruction errors of
both $S_t$ and $L_t$ are upper bounded by a time invariant and small value; and
the subspace recovery error decays to a small value within a finite delay of a
subspace change time. Because the last assumption depends on an algorithm
estimate, this result cannot be interpreted as a correctness result but only a
useful step towards it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5892</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5892</id><created>2014-05-22</created><authors><author><keyname>Zois</keyname><forenames>Daphney-Stavroula</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Nonlinear POMDPs for Active State Tracking with Sensing Costs</title><categories>cs.SY math.OC</categories><comments>32 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active state tracking is needed in object classification, target tracking,
medical diagnosis and estimation of sparse signals among other various
applications. Herein, active state tracking of a discrete-time, finite-state
Markov chain is considered. Noisy Gaussian observations are dynamically
collected by exerting appropriate control over their information content, while
incurring a related sensing cost. The objective is to devise sensing strategies
to optimize the trade-off between tracking performance and sensing cost. A
recently proposed Kalman-like estimator \cite{ZoisTSP14} is employed for state
tracking. The associated mean-squared error and a generic sensing cost metric
are then used in a partially observable Markov decision process formulation,
and the optimal sensing strategy is derived via a dynamic programming
recursion. The resulting recursion proves to be non-linear, challenging control
policy design. Properties of the related cost functions are derived and
sufficient conditions are provided regarding the structure of the optimal
control policy enabling characterization of when passive state tracking is
optimal. To overcome the associated computational burden of the optimal sensing
strategy, two lower complexity strategies are proposed, which exploit the
aforementioned properties. The performance of the proposed strategies is
illustrated in a wireless body sensing application, where cost savings as high
as $60\%$ are demonstrated for a $4\%$ detection error with respect to a static
equal allocation sensing strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5893</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5893</id><created>2014-05-22</created><authors><author><keyname>Enguehard</keyname><forenames>Chantal</forenames><affiliation>LINA</affiliation></author><author><keyname>Mangeot</keyname><forenames>Mathieu</forenames><affiliation>LIG</affiliation></author></authors><title>Computerization of African languages-French dictionaries</title><categories>cs.CL</categories><comments>8 pages</comments><proxy>ccsd</proxy><journal-ref>Iceland (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper relates work done during the DiLAF project. It consists in
converting 5 bilingual African language-French dictionaries originally in Word
format into XML following the LMF model. The languages processed are Bambara,
Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced
languages concerning Natural Language Processing tools. Once converted, the
dictionaries are available online on the Jibiki platform for lookup and
modification. The DiLAF project is first presented. A description of each
dictionary follows. Then, the conversion methodology from .doc format to XML
files is presented. A specific point on the usage of Unicode follows. Then,
each step of the conversion into XML and LMF is detailed. The last part
presents the Jibiki lexical resources management platform used for the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5902</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5902</id><created>2014-05-22</created><authors><author><keyname>Courtieu</keyname><forenames>Pierre</forenames><affiliation>CEDRIC</affiliation></author><author><keyname>Rieg</keyname><forenames>Lionel</forenames><affiliation>CEDRIC, ENSIIE</affiliation></author><author><keyname>Urbain</keyname><forenames>Xavier</forenames><affiliation>CEDRIC, ENSIIE, LRI</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>Impossibility of Gathering, a Certification</title><categories>cs.LO cs.DC cs.RO</categories><comments>10p</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in Distributed Computing highlight models and algorithms for
autonomous swarms of mobile robots that self-organise and cooperate to solve
global objectives. The overwhelming majority of works so far considers handmade
algorithms and proofs of correctness. This paper builds upon a previously
proposed formal framework to certify the correctness of impossibility results
regarding distributed algorithms that are dedicated to autonomous mobile robots
evolving in a continuous space. As a case study, we consider the problem of
gathering all robots at a particular location, not known beforehand. A
fundamental (but not yet formally certified) result, due to Suzuki and
Yamashita, states that this simple task is impossible for two robots executing
deterministic code and initially located at distinct positions. Not only do we
obtain a certified proof of the original impossibility result, we also get the
more general impossibility of gathering with an even number of robots, when any
two robots are possibly initially at the same exact location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5905</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5905</id><created>2014-05-22</created><updated>2015-05-17</updated><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author><author><keyname>Porto</keyname><forenames>Fabio</forenames></author></authors><title>Managing large-scale scientific hypotheses as uncertain and
  probabilistic data with support for predictive analytics</title><categories>cs.DB</categories><comments>16 pages, 9 figures, 1 table</comments><acm-class>H.2.1</acm-class><journal-ref>IEEE Computing in Science and Eng. 17(5):35-43, 2015</journal-ref><doi>10.1109/MCSE.2015.102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sheer scale of high-resolution raw data generated by simulation has
motivated non-conventional approaches for data exploration referred as
`immersive' and `in situ' query processing of the raw simulation data. Another
step towards supporting scientific progress is to enable data-driven hypothesis
management and predictive analytics out of simulation results. We present a
synthesis method and tool for encoding and managing competing hypotheses as
uncertain data in a probabilistic database that can be conditioned in the
presence of observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5919</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5919</id><created>2014-05-22</created><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Raniszewski</keyname><forenames>Marcin</forenames></author></authors><title>Two simple full-text indexes based on the suffix array</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>E.5; F.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two suffix array inspired full-text indexes. One, called SA-hash,
augments the suffix array with a hash table to speed up pattern searches due to
significantly narrowed search interval before the binary search phase. The
other, called FBCSA, is a compact data structure, similar to M{\&quot;a}kinen's
compact suffix array, but working on fixed sized blocks, which allows to
arrange the data in multiples of 32 bits, beneficial for CPU access.
Experimental results on the Pizza~\&amp;~Chili 200\,MB datasets show that SA-hash
is about 2.5--3 times faster in pattern searches (counts) than the standard
suffix array, for the price of requiring $0.3n-2.0n$ extra space, where $n$ is
the text length, and setting a minimum pattern length. The latter limitation
can be removed for the price of even more extra space. FBCSA is relatively fast
in single cell accesses (a few times faster than related indexes at about the
same or better compression), but not competitive if many consecutive cells are
to be extracted. Still, for the task of extracting e.g. 10 successive cells its
time-space relation remains attractive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5924</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5924</id><created>2014-05-22</created><authors><author><keyname>de Silva</keyname><forenames>Brian</forenames></author><author><keyname>Compton</keyname><forenames>Ryan</forenames></author></authors><title>Prediction of Foreign Box Office Revenues Based on Wikipedia Page
  Activity</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 10 figures, Computational Approaches to Social Modeling
  (ChASM) Workshop, WebSci 2014, Bloomington, Indiana-June 24-26 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of attempts have been made at estimating the amount of box office
revenue a film will generate during its opening weekend. One such attempt makes
extensive use of the number of views a film's Wikipedia page has attracted as a
predictor of box office success in the United States. In this paper we develop
a similar method of approximating box office success. We test our method using
325 films from the United States and then apply it to films from four foreign
markets: Japan (95 films), Australia (118 films), Germany (105 films), and the
United Kingdom (141 films). We find the technique to have inconsistent
performance in these nations. While it makes relatively accurate predictions
for the United States and Australia, its predictions in the remaining markets
are not accurate enough to be useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5926</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5926</id><created>2014-05-22</created><authors><author><keyname>Omidi</keyname><forenames>Ehsan</forenames></author></authors><title>Modeling and Nonlinear Control of Gantry Crane Using Feedback
  Linearization Method</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the requirements of high positioning accuracy, small swing angle,
short transportation time, and high safety, both motion and stabilization
control for an gantry crane system becomes an interesting issue in the field of
control technology development. In this paper, dynamic model of gantry crane is
extracted using Lagrange method. This model has been linearized and weaknesses
of state feedback control of this model is reviewed. To solve these problems,
state feedback gain matrix is calculated using LQR method and results are fully
investigated. Finally, a full nonlinear solution of problem using feedback
linearization method is implemented and results are compared with previous
works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5927</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5927</id><created>2014-05-22</created><updated>2014-06-16</updated><authors><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author><author><keyname>Plump</keyname><forenames>Detlef</forenames></author></authors><title>Verifying Monadic Second-Order Properties of Graph Programs</title><categories>cs.LO</categories><comments>Extended version of a paper to appear at ICGT 2014</comments><journal-ref>Proc. International Conference on Graph Transformation (ICGT
  2014), volume 8571 of LNCS, pages 33-48. Springer, 2014</journal-ref><doi>10.1007/978-3-319-09108-2_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The core challenge in a Hoare- or Dijkstra-style proof system for graph
programs is in defining a weakest liberal precondition construction with
respect to a rule and a postcondition. Previous work addressing this has
focused on assertion languages for first-order properties, which are unable to
express important global properties of graphs such as acyclicity,
connectedness, or existence of paths. In this paper, we extend the nested graph
conditions of Habel, Pennemann, and Rensink to make them equivalently
expressive to monadic second-order logic on graphs. We present a weakest
liberal precondition construction for these assertions, and demonstrate its use
in verifying non-local correctness specifications of graph programs in the
sense of Habel et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5932</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5932</id><created>2014-05-22</created><authors><author><keyname>Okano</keyname><forenames>Kunihisa</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author></authors><title>Minimum data rate for stabilization of linear systems with parametric
  uncertainties</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a stabilization problem of linear uncertain systems with parametric
uncertainties via feedback control over data-rate-constrained channels. The
objective is to find the limitation on the amount of information that must be
conveyed through the channels for achieving stabilization and in particular how
the plant uncertainties affect it. We derive a necessary condition and a
sufficient condition for stabilizing the closed-loop system. These conditions
provide limitations in the form of bounds on data rate and magnitude of
uncertainty on plant parameters. The bounds are characterized by the product of
the poles of the nominal plant and are less conservative than those known in
the literature. In the course of deriving these results, a new class of
nonuniform quantizers is found to be effective in reducing the required data
rate. For scalar plants, these quantizers are shown to minimize the required
data rate, and the obtained conditions become tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5937</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5937</id><created>2014-05-22</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Ramirez</keyname><forenames>Marte</forenames></author></authors><title>Polynomial trajectory algorithm for a biped robot</title><categories>cs.RO</categories><comments>25 Pages</comments><journal-ref>International Journal of Robotics and Automation 25 (4), (2010),
  pp. 294-303</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building trajectories for biped robot walking is a complex task considering
all degrees of freedom (DOFs) commonly bound within the mechanical structure. A
typical problem for such robots is the instability produced by violent
transitions between walking phases in particular when a swinging leg impacts
the surface. Although extensive research on novel efficient walking algorithms
has been conducted, falls commonly appear as the walking speed increases or as
the terrain condition changes. This paper presents a polynomial trajectory
generation algorithm (PTA) to implement the walking on biped robots following
the cubic Hermitian polynomial interpolation between initial and final
conditions. The proposed algorithm allows smooth transitions between walking
phases, significantly reducing the possibility of falling. The algorithm has
been successfully tested by generating walking trajectories under different
terrain conditions on a biped robot of 10 DOFs. PTA has shown to be simple and
suitable to generate real time walking trajectories, despite reduced computing
resources of a commercial embedded microcontroller. Experimental evidence and
comparisons to other state-of-the-art methods demonstrates a better performance
of the proposed method in generating walking trajectories under different
ground conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5940</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5940</id><created>2014-05-22</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Weinberg</keyname><forenames>S. Matthew</forenames></author></authors><title>Bayesian Truthful Mechanisms for Job Scheduling from Bi-criterion
  Approximation Algorithms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide polynomial-time approximately optimal Bayesian mechanisms for
makespan minimization on unrelated machines as well as for max-min fair
allocations of indivisible goods, with approximation factors of $2$ and
$\min\{m-k+1, \tilde{O}(\sqrt{k})\}$ respectively, matching the approximation
ratios of best known polynomial-time \emph{algorithms} (for max-min fairness,
the latter claim is true for certain ratios of the number of goods $m$ to
people $k$). Our mechanisms are obtained by establishing a polynomial-time
approximation-sensitive reduction from the problem of designing approximately
optimal {\em mechanisms} for some arbitrary objective ${\cal O}$ to that of
designing bi-criterion approximation {\em algorithms} for the same objective
${\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled
by extending the celebrated &quot;equivalence of separation and
optimization&quot;[GLSS81,KP80] to also accommodate bi-criterion approximations.
Moreover, to apply the reduction to the specific problems of makespan and
max-min fairness we develop polynomial-time bi-criterion approximation
algorithms for makespan minimization with costs and max-min fairness with
costs, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of
bi-criterion approximation that is required by the reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5948</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5948</id><created>2014-05-22</created><authors><author><keyname>Liu</keyname><forenames>Jing</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Ou</keyname><forenames>Zhijian</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>Low-complexity video encoder for smart eyes based on underdetermined
  blind signal separation</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a low complexity video coding method based on
Underdetermined Blind Signal Separation (UBSS). The detailed coding framework
is designed. Three key techniques are proposed to enhance the compression ratio
and the quality of the decoded frames. The experiments validate that the
proposed method costs 30ms encoding time less than DISCOVER. The simulation
shows that this new method can save 50% energy compared with H.264.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5956</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5956</id><created>2014-05-22</created><authors><author><keyname>Carette</keyname><forenames>Jacques</forenames></author><author><keyname>Farmer</keyname><forenames>William M.</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author></authors><title>Realms: A Structure for Consolidating Knowledge about Mathematical
  Theories</title><categories>cs.MS cs.LO</categories><comments>As accepted for CICM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since there are different ways of axiomatizing and developing a mathematical
theory, knowledge about a such a theory may reside in many places and in many
forms within a library of formalized mathematics. We introduce the notion of a
realm as a structure for consolidating knowledge about a mathematical theory. A
realm contains several axiomatizations of a theory that are separately
developed. Views interconnect these developments and establish that the
axiomatizations are equivalent in the sense of being mutually interpretable. A
realm also contains an external interface that is convenient for users of the
library who want to apply the concepts and facts of the theory without delving
into the details of how the concepts and facts were developed. We illustrate
the utility of realms through a series of examples. We also give an outline of
the mechanisms that are needed to create and maintain realms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5960</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5960</id><created>2014-05-23</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author></authors><title>LASS: a simple assignment model with Laplacian smoothing</title><categories>cs.LG math.OC stat.ML</categories><comments>20 pages, 4 figures. A shorter version appears in AAAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning soft assignments of $N$ items to $K$
categories given two sources of information: an item-category similarity
matrix, which encourages items to be assigned to categories they are similar to
(and to not be assigned to categories they are dissimilar to), and an item-item
similarity matrix, which encourages similar items to have similar assignments.
We propose a simple quadratic programming model that captures this intuition.
We give necessary conditions for its solution to be unique, define an
out-of-sample mapping, and derive a simple, effective training algorithm based
on the alternating direction method of multipliers. The model predicts
reasonable assignments from even a few similarity values, and can be seen as a
generalization of semisupervised learning. It is particularly useful when items
naturally belong to multiple categories, as for example when annotating
documents with keywords or pictures with tags, with partially tagged items, or
when the categories have complex interrelations (e.g. hierarchical) that are
unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5966</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5966</id><created>2014-05-23</created><updated>2014-11-08</updated><authors><author><keyname>Berhuy</keyname><forenames>Gr&#xe9;gory</forenames></author><author><keyname>Markin</keyname><forenames>Nadya</forenames></author><author><keyname>Sethuraman</keyname><forenames>B. A.</forenames></author></authors><title>Bounds of fast decodability of space time block codes, skew-Hermitian
  matrices, and Azumaya algebras</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study fast lattice decodability of space-time block codes for $n$ transmit
and receive antennas, written very generally as a linear combination
$\sum_{i=1}^{2l} s_i A_i$, where the $s_i$ are real information symbols and the
$A_i$ are $n\times n$ $\mathbb R$-linearly independent complex valued matrices.
We show that the mutual orthogonality condition $A_iA_j^* + A_jA_i^*=0$ for
distinct basis matrices is not only sufficient but also necessary for fast
decodability. We build on this to show that for full-rate ($l = n^2$)
transmission, the decoding complexity can be no better than $|S|^{n^2+1}$,
where $|S|$ is the size of the effective real signal constellation. We also
show that for full-rate transmission, $g$-group decodability, as defined in
[1], is impossible for any $g \ge 2$. We then use the theory of Azumaya
algebras to derive bounds on the maximum number of groups into which the basis
matrices can be partitioned so that the matrices in different groups are
mutually orthogonal---a key measure of fast decodability. We show that in
general, this maximum number is of the order of only the $2$-adic value of $n$.
In the case where the matrices $A_i$ arise from a division algebra, which is
most desirable for diversity, we show that the maximum number of groups is only
$4$. As a result, the decoding complexity for this case is no better than
$|S|^{\lceil l/2 \rceil}$ for any rate $l$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5974</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5974</id><created>2014-05-23</created><authors><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Living on the Edge: The Role of Proactive Caching in 5G Wireless
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted for publication in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article explores one of the key enablers of beyond $4$G wireless
networks leveraging small cell network deployments, namely proactive caching.
Endowed with predictive capabilities and harnessing recent developments in
storage, context-awareness and social networks, peak traffic demands can be
substantially reduced by proactively serving predictable user demands, via
caching at base stations and users' devices. In order to show the effectiveness
of proactive caching, we examine two case studies which exploit the spatial and
social structure of the network, where proactive caching plays a crucial role.
Firstly, in order to alleviate backhaul congestion, we propose a mechanism
whereby files are proactively cached during off-peak demands based on file
popularity and correlations among users and files patterns. Secondly,
leveraging social networks and device-to-device (D2D) communications, we
propose a procedure that exploits the social structure of the network by
predicting the set of influential users to (proactively) cache strategic
contents and disseminate them to their social ties via D2D communications.
Exploiting this proactive caching paradigm, numerical results show that
important gains can be obtained for each case study, with backhaul savings and
a higher ratio of satisfied users of up to $22\%$ and $26\%$, respectively.
Higher gains can be further obtained by increasing the storage capability at
the network edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5975</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5975</id><created>2014-05-23</created><updated>2014-09-17</updated><authors><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>Solving Multi-choice Secretary Problem in Parallel: An Optimal
  Observation-Selection Protocol</title><categories>cs.DS cs.CC cs.GT</categories><comments>This work is accepted by ISAAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical secretary problem investigates the question of how to hire the
best secretary from $n$ candidates who come in a uniformly random order. In
this work we investigate a parallel generalizations of this problem introduced
by Feldman and Tennenholtz [14]. We call it shared $Q$-queue $J$-choice
$K$-best secretary problem. In this problem, $n$ candidates are evenly
distributed into $Q$ queues, and instead of hiring the best one, the employer
wants to hire $J$ candidates among the best $K$ persons. The $J$ quotas are
shared by all queues. This problem is a generalized version of $J$-choice
$K$-best problem which has been extensively studied and it has more practical
value as it characterizes the parallel situation.
  Although a few of works have been done about this generalization, to the best
of our knowledge, no optimal deterministic protocol was known with general $Q$
queues. In this paper, we provide an optimal deterministic protocol for this
problem. The protocol is in the same style of the $1\over e$-solution for the
classical secretary problem, but with multiple phases and adaptive criteria.
Our protocol is very simple and efficient, and we show that several
generalizations, such as the fractional $J$-choice $K$-best secretary problem
and exclusive $Q$-queue $J$-choice $K$-best secretary problem, can be solved
optimally by this protocol with slight modification and the latter one solves
an open problem of Feldman and Tennenholtz [14].
  In addition, we provide theoretical analysis for two typical cases, including
the 1-queue 1-choice $K$-best problem and the shared 2-queue 2-choice 2-best
problem. For the former, we prove a lower bound $1-O(\frac{\ln^2K}{K^2})$ of
the competitive ratio. For the latter, we show the optimal competitive ratio is
$\approx0.372$ while previously the best known result is 0.356 [14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5978</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5978</id><created>2014-05-23</created><authors><author><keyname>&#x17d;iberna</keyname><forenames>Ale&#x161;</forenames></author></authors><title>Blockmodeling of multilevel networks</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>33 pages, 10 figures, 6 tables</comments><journal-ref>Social networks, vol. 39, no. 1, pages 46-61, October 2014</journal-ref><doi>10.1016/j.socnet.2014.04.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents several approaches to the blockmodeling of multilevel
network data. Multilevel network data consist of networks that are measured on
at least two levels (e.g. between organizations and people) and information on
ties between those levels (e.g. information on which people are members of
which organizations). Several approaches will be considered: a separate
analysis of the levels; transforming all networks to one level and
blockmodeling on this level using information from all levels; and a truly
multilevel approach where all levels and ties among them are modeled at the
same time. Advantages and disadvantages of these approaches will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.5981</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.5981</id><created>2014-05-23</created><updated>2014-06-04</updated><authors><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author><author><keyname>Ghosh</keyname><forenames>Bappaditya</forenames></author><author><keyname>Pattanayak</keyname><forenames>Sukhamoy</forenames></author></authors><title>Cyclic codes over the ring $ \Z_p[u, v]/\langle u^2, v^2, uv-vu\rangle$</title><categories>cs.IT math.IT</categories><comments>Following things included: ternary optimal code of length 12,
  characterization p-ary image and some minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be a prime number. In this paper, we study cyclic codes over the ring
$ \Z_p[u, v]/\langle u^2, v^2, uv-vu\rangle$. We find a unique set of
generators for these codes. We also study the rank and the Hamming distance of
these codes. We obtain all except one ternary optimal code of length 12 as the
Gray image of the cyclic codes over the ring $ \Z_p[u, v]/\langle u^2, v^2,
uv-vu\rangle$. We also characterize the $p$-ary image of these cyclic codes
under the Gray map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6003</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6003</id><created>2014-05-23</created><authors><author><keyname>Rashmanlou</keyname><forenames>Hossein</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Isometry on Interval-valued Fuzzy Graphs</title><categories>cs.DM</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:1205.6123 by other
  authors</comments><msc-class>05C78</msc-class><journal-ref>International Journal of Fuzzy Mathematical Archive, vol. 3,
  (2013) 28-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Especially in research areas of computer science such as data mining, image
segmentation, clustering image capturing and networking. The interval-valued
fuzzy graphs are more flexible and compatible than fuzzy graphs due to the fact
that they allowed the degree of membership of a vertex to an edge to be
represented by interval valued in [0,1] rather than the crisp real values
between 0 and 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6008</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6008</id><created>2014-05-23</created><updated>2015-05-25</updated><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Beelen</keyname><forenames>Peter</forenames></author></authors><title>Sub-quadratic Decoding of One-point Hermitian Codes</title><categories>cs.IT math.IT</categories><comments>New version includes simulation results, improves some complexity
  results, as well as a number of reviewer corrections. 20 pages</comments><journal-ref>IEEE Transactions of Information Theory, vol 61 (6), p.
  3225--3240. 2015</journal-ref><doi>10.1109/TIT.2015.2424415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first two sub-quadratic complexity decoding algorithms for
one-point Hermitian codes. The first is based on a fast realisation of the
Guruswami-Sudan algorithm by using state-of-the-art algorithms from computer
algebra for polynomial-ring matrix minimisation. The second is a Power decoding
algorithm: an extension of classical key equation decoding which gives a
probabilistic decoding algorithm up to the Sudan radius. We show how the
resulting key equations can be solved by the same methods from computer
algebra, yielding similar asymptotic complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6009</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6009</id><created>2014-05-23</created><updated>2014-11-19</updated><authors><author><keyname>Palchykov</keyname><forenames>Vasyl</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>Janos</forenames></author></authors><title>Transmission of cultural traits in layered ego-centric networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 2 figures</comments><proxy>Bohdan Markiv</proxy><journal-ref>Condens. Matter Phys., 2014, Vol. 17, No. 3, 33802</journal-ref><doi>10.5488/CMP.17.33802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although a number of models have been developed to investigate the emergence
of culture and evolutionary phases in social systems, one important aspect has
not yet been sufficiently emphasized. This is the structure of the underlaying
network of social relations serving as channels in transmitting cultural
traits, which is expected to play a crucial role in the evolutionary processes
in social systems. In this paper we contribute to the understanding of the role
of the network structure by developing a layered ego-centric network structure
based model, inspired by the social brain hypothesis, to study transmission of
cultural traits and their evolution in social network. For this model we first
find analytical results in the spirit of mean-field approximation and then to
validate the results we compare them with the results of extensive numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6012</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6012</id><created>2014-05-23</created><authors><author><keyname>Xie</keyname><forenames>Qi</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Gu</keyname><forenames>Shuhang</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Feng</keyname><forenames>Xiangchu</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>On the Optimal Solution of Weighted Nuclear Norm Minimization</title><categories>cs.CV cs.LG stat.ML</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the nuclear norm minimization (NNM) problem has been
attracting much attention in computer vision and machine learning. The NNM
problem is capitalized on its convexity and it can be solved efficiently. The
standard nuclear norm regularizes all singular values equally, which is however
not flexible enough to fit real scenarios. Weighted nuclear norm minimization
(WNNM) is a natural extension and generalization of NNM. By assigning properly
different weights to different singular values, WNNM can lead to
state-of-the-art results in applications such as image denoising. Nevertheless,
so far the global optimal solution of WNNM problem is not completely solved yet
due to its non-convexity in general cases. In this article, we study the
theoretical properties of WNNM and prove that WNNM can be equivalently
transformed into a quadratic programming problem with linear constraints. This
implies that WNNM is equivalent to a convex problem and its global optimum can
be readily achieved by off-the-shelf convex optimization solvers. We further
show that when the weights are non-descending, the globally optimal solution of
WNNM can be obtained in closed-form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6015</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6015</id><created>2014-05-23</created><updated>2014-07-17</updated><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author><author><keyname>Yao</keyname><forenames>Penghui</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Multipartite Quantum Correlation and Communication Complexities</title><categories>quant-ph cs.CC</categories><comments>19 pages; some typos are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of quantum correlation complexity and quantum communication
complexity were recently proposed to quantify the minimum amount of resources
needed in generating bipartite classical or quantum states in the single-shot
setting. The former is the minimum size of the initially shared state $\sigma$
on which local operations by the two parties (without communication) can
generate the target state $\rho$, and the latter is the minimum amount of
communication needed when initially sharing nothing. In this paper, we
generalize these two concepts to multipartite cases, for both exact and
approximate state generation. Our results are summarized as follows. (1) For
multipartite pure states, the correlation complexity can be completely
characterized by local ranks of sybsystems. (2) We extend the notion of
PSD-rank of matrices to that of tensors, and use it to bound the quantum
correlation complexity for generating multipartite classical distributions. (3)
For generating multipartite mixed quantum states, communication complexity is
not always equal to correlation complexity (as opposed to bipartite case). But
they differ by at most a factor of 2. Generating a multipartite mixed quantum
state has the same communication complexity as generating its optimal
purification. But for correlation complexity of these two tasks can be
different (though still related by less than a factor of 2). (4) To generate a
bipartite classical distribution $P(x,y)$ approximately, the quantum
communication complexity is completely characterized by the approximate
PSD-rank of $P$. The quantum correlation complexity of approximately generating
multipartite pure states is bounded by approximate local ranks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6033</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6033</id><created>2014-05-23</created><authors><author><keyname>Suzuki</keyname><forenames>Joe</forenames></author></authors><title>Universal Bayesian Measures and Universal Histogram Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider universal data compression: the length $l(x^n)$ of sequence $x^n\in
A^n$ with finite alphabet $A$ and length $n$ satisfies Kraft's inequality over
$A^n$, and $-\frac{1}{n}\log \frac{P^n(x^n)}{Q^n(x^n)}$ almost surely converges
to zero as $n$ grows for the $Q^n(x^n)=2^{-l(x^n)}$ and any stationary ergodic
source $P$. In this paper, we say such a $Q$ is a universal Bayesian measure.
We generalize the notion to the sources in which the random variables may be
either discrete, continuous, or none of them. The basic idea is due to Boris
Ryabko who utilized model weighting over histograms that approximate $P$,
assuming that a density function of $P$ exists. However, the range of $P$
depends on the choice of the histogram sequence. The universal Bayesian measure
constructed in this paper overcomes the drawbacks and has many applications to
infer relation among random variables, and extends the application area of the
minimum description length principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6043</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6043</id><created>2014-05-23</created><authors><author><keyname>Brault-Baron</keyname><forenames>Johann</forenames></author><author><keyname>Capelli</keyname><forenames>Florent</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Understanding model counting for $\beta$-acyclic CNF-formulas</title><categories>cs.CC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the knowledge about so-called structural restrictions of
$\mathrm{\#SAT}$ by giving a polynomial time algorithm for $\beta$-acyclic
$\mathrm{\#SAT}$. In contrast to previous algorithms in the area, our algorithm
does not proceed by dynamic programming but works along an elimination order,
solving a weighted version of constraint satisfaction. Moreover, we give
evidence that this deviation from more standard algorithm is not a coincidence,
but that there is likely no dynamic programming algorithm of the usual style
for $\beta$-acyclic $\mathrm{\#SAT}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6052</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6052</id><created>2014-05-23</created><updated>2015-12-01</updated><authors><author><keyname>Mirza</keyname><forenames>Jawad</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>Limited Feedback Massive MISO Systems with Trellis Coded Quantization
  for Correlated Channels</title><categories>cs.IT math.IT</categories><comments>13 pages, 18 figures, IEEE Transactions on Vehicular Technology,
  accepted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose trellis coded quantization (TCQ) based limited
feedback techniques for massive multiple-input single-output (MISO) frequency
division duplexing (FDD) systems in temporally and spatially correlated
channels. We exploit the correlation present in the channel to effectively
quantize channel direction information (CDI). For multiuser (MU) systems with
matched-filter (MF) precoding, we show that the number of feedback bits
required by the random vector quantization (RVQ) codebook to match even a small
fraction of the perfect CDI signal-to-interference-plus-noise ratio (SINR)
performance is large. With such large numbers of bits, the exhaustive search
required by conventional codebook approaches make them infeasible for massive
MISO systems. Motivated by this, we propose a differential TCQ scheme for
temporally correlated channels that transforms the source constellation at each
stage in a trellis using 2D translation and scaling techniques. We derive a
scaling parameter for the source constellation as a function of the temporal
correlation and the number of BS antennas. We also propose a TCQ based limited
feedback scheme for spatially correlated channels where the channel is
quantized directly without performing decorrelation at the receiver. Simulation
results show that the proposed TCQ schemes outperform the existing noncoherent
TCQ (NTCQ) schemes, by improving the spectral efficiency and beamforming gain
of the system. The proposed differential TCQ also reduces the feedback overhead
of the system compared to the differential NTCQ method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6058</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6058</id><created>2014-05-22</created><authors><author><keyname>Gadaleta</keyname><forenames>Francesco</forenames></author><author><keyname>Strackx</keyname><forenames>Raoul</forenames></author><author><keyname>Nikiforakis</keyname><forenames>Nick</forenames></author><author><keyname>Piessens</keyname><forenames>Frank</forenames></author><author><keyname>Joosen</keyname><forenames>Wouter</forenames></author></authors><title>On the effectiveness of virtualization-based security</title><categories>cs.CR</categories><comments>12 pages, 07-10 May 2012, Max Planck Institute IT Security, Freiburg
  (Germany)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Protecting commodity operating systems and applications against malware and
targeted attacks has proven to be difficult. In recent years, virtualization
has received attention from security researchers who utilize it to harden
existing systems and provide strong security guarantees. This has lead to
interesting use cases such as cloud computing where possibly sensitive data is
processed on remote, third party systems. The migration and processing of data
in remote servers, poses new technical and legal questions, such as which
security measures should be taken to protect this data or how can it be proven
that execution of code wasn't tampered with. In this paper we focus on
technological aspects. We discuss the various possibilities of security within
the virtualization layer and we use as a case study \HelloRootkitty{}, a
lightweight invariance-enforcing framework which allows an operating system to
recover from kernel-level attacks. In addition to \HelloRootkitty{}, we also
explore the use of special hardware chips as a way of further protecting and
guaranteeing the integrity of a virtualized system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6068</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6068</id><created>2014-05-23</created><authors><author><keyname>Lande</keyname><forenames>Dmitry</forenames></author></authors><title>Building of Networks of Natural Hierarchies of Terms Based on Analysis
  of Texts Corpora</title><categories>cs.CL</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technique of building of networks of hierarchies of terms based on the
analysis of chosen text corpora is offered. The technique is based on the
methodology of horizontal visibility graphs. Constructed and investigated
language network, formed on the basis of electronic preprints arXiv on topics
of information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6073</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6073</id><created>2014-05-23</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author><author><keyname>Pal</keyname><forenames>Nilanjan</forenames></author><author><keyname>Majumder</keyname><forenames>Soumajit</forenames></author></authors><title>Ancilla-Quantum Cost Trade-off during Reversible Logic Synthesis using
  Exclusive Sum-of-Products</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging technologies with asymptotic zero power dissipation, such as quantum
computing, require the logical operations to be done in a reversible manner. In
recent years, the problem of synthesizing Boolean functions in the reversible
logic domain has gained significant research attention. The efficiency of the
synthesis methods is measured in terms of quantum cost, gate cost, garbage
lines, logic depth and speed of synthesis. In this paper, we present an
approach based on Exclusive sum-of-Products (ESOP), which allows the user to
explore the trade-off between quantum cost and garbage lines. The proposed
technique adds a new dimension to the reversible logic synthesis solutions. We
demonstrate by detailed experiments that controlled improvement in quantum cost
and gate count by increasing garbage count can be achieved. In some cases,
improved quantum cost and gate count compared to state-of-the-art synthesis
methods are reported. Furthermore, we propose a novel rule-based approach to
achieve ancilla-free reversible logic synthesis starting from an ESOP
formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6076</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6076</id><created>2014-05-23</created><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Lee</keyname><forenames>Chansoo</forenames></author><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Online Linear Optimization via Smoothing</title><categories>cs.LG</categories><comments>COLT 2014</comments><journal-ref>JMLR 2014 W&amp;CP</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new optimization-theoretic approach to analyzing
Follow-the-Leader style algorithms, particularly in the setting where
perturbations are used as a tool for regularization. We show that adding a
strongly convex penalty function to the decision rule and adding stochastic
perturbations to data correspond to deterministic and stochastic smoothing
operations, respectively. We establish an equivalence between &quot;Follow the
Regularized Leader&quot; and &quot;Follow the Perturbed Leader&quot; up to the smoothness
properties. This intuition leads to a new generic analysis framework that
recovers and improves the previous known regret bounds of the class of
algorithms commonly known as Follow the Perturbed Leader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6077</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6077</id><created>2014-05-23</created><authors><author><keyname>Kandiah</keyname><forenames>V.</forenames></author><author><keyname>Georgeot</keyname><forenames>B.</forenames></author><author><keyname>Giraud</keyname><forenames>O.</forenames></author></authors><title>Move ordering and communities in complex networks describing the game of
  go</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 21 figures</comments><journal-ref>Eur. Phys. J. B 87, 246 (2014)</journal-ref><doi>10.1140/epjb/e2014-50497-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the game of go from the point of view of complex networks. We
construct three different directed networks of increasing complexity, defining
nodes as local patterns on plaquettes of increasing sizes, and links as actual
successions of these patterns in databases of real games. We discuss the
peculiarities of these networks compared to other types of networks. We explore
the ranking vectors and community structure of the networks and show that this
approach enables to extract groups of moves with common strategic properties.
We also investigate different networks built from games with players of
different levels or from different phases of the game. We discuss how the study
of the community structure of these networks may help to improve the computer
simulations of the game. More generally, we believe such studies may help to
improve the understanding of human decision process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6081</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6081</id><created>2014-05-23</created><authors><author><keyname>Ferdous</keyname><forenames>S. M.</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>An Integer Programming Formulation of the Minimum Common String
  Partition problem</title><categories>cs.DM math.CO math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1401.4539</comments><doi>10.1371/journal.pone.0130266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a minimum common partition of two strings
(MCSP). The problem has its application in genome comparison. MCSP problem is
proved to be NP-hard. In this paper, we develop an Integer Programming (IP)
formulation for the problem and implement it. The experimental results are
compared with the previous state-of-the-art algorithms and are found to be
promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6082</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6082</id><created>2014-05-23</created><authors><author><keyname>Huang</keyname><forenames>Zongyan</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>Paulson</keyname><forenames>Lawrence C.</forenames></author></authors><title>A comparison of three heuristics to choose the variable ordering for CAD</title><categories>cs.SC</categories><msc-class>68W30, O3C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>ACM Communications in Computer Algebra 48:3 (issue 189), pp.
  121-123, ACM, 2014</journal-ref><doi>10.1145/2733693.2733706</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition (CAD) is a key tool for problems in real
algebraic geometry and beyond. When using CAD there is often a choice over the
variable ordering to use, with some problems infeasible in one ordering but
simple in another. Here we discuss a recent experiment comparing three
heuristics for making this choice on thousands of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6090</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6090</id><created>2014-05-23</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author></authors><title>Using the Regular Chains Library to build cylindrical algebraic
  decompositions by projecting and lifting</title><categories>cs.SC</categories><msc-class>68W30, 03C10</msc-class><acm-class>G.4; I.1.2</acm-class><journal-ref>H. Hong and C. Yap, eds. Mathematical Software - ICMS 2014, pp.
  458-465. (Lecture Notes in Computer Science, 8592). Springer, 2014</journal-ref><doi>10.1007/978-3-662-44199-2_69</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition (CAD) is an important tool, both for
quantifier elimination over the reals and a range of other applications.
Traditionally, a CAD is built through a process of projection and lifting to
move the problem within Euclidean spaces of changing dimension. Recently, an
alternative approach which first decomposes complex space using triangular
decomposition before refining to real space has been introduced and implemented
within the RegularChains Library of Maple. We here describe a freely available
package ProjectionCAD which utilises the routines within the RegularChains
Library to build CADs by projection and lifting. We detail how the projection
and lifting algorithms were modified to allow this, discuss the motivation and
survey the functionality of the package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6094</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6094</id><created>2014-05-23</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author></authors><title>Choosing a variable ordering for truth-table invariant cylindrical
  algebraic decomposition by incremental triangular decomposition</title><categories>cs.SC</categories><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>H. Hong and C. Yap, eds. Mathematical Software - ICMS 2014, pp.
  450-457. (Lecture Notes in Computer Science, 8592). Springer, 2014</journal-ref><doi>10.1007/978-3-662-44199-2_68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition (CAD) is a key tool for solving problems
in real algebraic geometry and beyond. In recent years a new approach has been
developed, where regular chains technology is used to first build a
decomposition in complex space. We consider the latest variant of this which
builds the complex decomposition incrementally by polynomial and produces CADs
on whose cells a sequence of formulae are truth-invariant. Like all CAD
algorithms the user must provide a variable ordering which can have a profound
impact on the tractability of a problem. We evaluate existing heuristics to
help with the choice for this algorithm, suggest improvements and then derive a
new heuristic more closely aligned with the mechanics of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6095</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6095</id><created>2014-05-20</created><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author></authors><title>Zipper logic</title><categories>math.CO cs.LO math.GT</categories><comments>16 pages, 24 colour figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zipper logic is a graph rewrite system, consisting in only local rewrites on
a class of zipper graphs. Connections with the chemlambda artificial chemistry
and with knot diagrammatics based computation are explored in the article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6100</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6100</id><created>2014-05-23</created><updated>2014-06-24</updated><authors><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>DeVries</keyname><forenames>Edsko</forenames><affiliation>Well-Typed LLP</affiliation></author><author><keyname>Hennessy</keyname><forenames>Matthew</forenames><affiliation>Trinity College Dublin, Ireland</affiliation></author></authors><title>Compositional Reasoning for Explicit Resource Management in
  Channel-Based Concurrency</title><categories>cs.LO</categories><comments>51 pages, 7 figures</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 26,
  2014) lmcs:691</journal-ref><doi>10.2168/LMCS-10(2:15)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a pi-calculus variant with a costed semantics where channels are
treated as resources that must explicitly be allocated before they are used and
can be deallocated when no longer required. We use a substructural type system
tracking permission transfer to construct coinductive proof techniques for
comparing behaviour and resource usage efficiency of concurrent processes. We
establish full abstraction results between our coinductive definitions and a
contextual behavioural preorder describing a notion of process efficiency
w.r.t. its management of resources. We also justify these definitions and
respective proof techniques through numerous examples and a case study
comparing two concurrent implementations of an extensible buffer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6103</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6103</id><created>2014-05-23</created><authors><author><keyname>Winkler</keyname><forenames>Kurt</forenames></author><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Volk</keyname><forenames>Martin</forenames></author></authors><title>Evaluating the fully automatic multi-language translation of the Swiss
  avalanche bulletin</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Swiss avalanche bulletin is produced twice a day in four languages. Due
to the lack of time available for manual translation, a fully automated
translation system is employed, based on a catalogue of predefined phrases and
predetermined rules of how these phrases can be combined to produce sentences.
The system is able to automatically translate such sentences from German into
the target languages French, Italian and English without subsequent
proofreading or correction. Our catalogue of phrases is limited to a small
sublanguage. The reduction of daily translation costs is expected to offset the
initial development costs within a few years. After being operational for two
winter seasons, we assess here the quality of the produced texts based on an
evaluation where participants rate real danger descriptions from both origins,
the catalogue of phrases versus the manually written and translated texts. With
a mean recognition rate of 55%, users can hardly distinguish between the two
types of texts, and give similar ratings with respect to their language
quality. Overall, the output from the catalogue system can be considered
virtually equivalent to a text written by avalanche forecasters and then
manually translated by professional translators. Furthermore, forecasters
declared that all relevant situations were captured by the system with
sufficient accuracy and within the limited time available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6130</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6130</id><created>2014-02-04</created><authors><author><keyname>Bhatt</keyname><forenames>Ms. Drashti H.</forenames></author><author><keyname>Rathod</keyname><forenames>Mr. Kirit R.</forenames></author><author><keyname>Agravat</keyname><forenames>Mr. Shardul J.</forenames></author></authors><title>A Study of Local Binary Pattern Method for Facial Expression Detection</title><categories>cs.CV</categories><comments>3 pages, 2 images, International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>Ms.Drashti H. Bhatt , Mr.Kirit R. Rathod , Mr.Shardul J. Agravat.
  Article: A Study of Local Binary Pattern Method for Facial Expression
  Detection. IJCTT 7(3):151-153, January 2014. Published by Seventh Sense
  Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V7P143</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Face detection is a basic task for expression recognition. The reliability of
face detection &amp; face recognition approach has a major role on the performance
and usability of the entire system. There are several ways to undergo face
detection &amp; recognition. We can use Image Processing Operations, various
classifiers, filters or virtual machines for the former. Various strategies are
being available for Facial Expression Detection. The field of facial expression
detection can have various applications along with its importance &amp; can be
interacted between human being &amp; computer. Many few options are available to
identify a face in an image in accurate &amp; efficient manner. Local Binary
Pattern (LBP) based texture algorithms have gained popularity in these years.
LBP is an effective approach to have facial expression recognition &amp; is a
feature-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6131</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6131</id><created>2014-02-04</created><authors><author><keyname>Ait-Aoudia</keyname><forenames>Samy</forenames></author><author><keyname>Jegou</keyname><forenames>Roland</forenames></author><author><keyname>Michelucci</keyname><forenames>Dominique</forenames></author></authors><title>Reduction of constraint systems</title><categories>cs.DM math.CO</categories><comments>10 pages, 17 figures, Conference: COMPUGRAPHICS, Alvor, Portugal,
  1993</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric modeling by constraints leads to large systems of algebraic
equations. This paper studies bipartite graphs underlaid by systems of
equations. It shows how these graphs make possible to polynomially decompose
these systems into well constrained, over-, and underconstrained subsystems.
This paper also gives an efficient method to decompose well constrained systems
into irreducible ones. These decompositions greatly speed up the resolution in
case of reducible systems. They also allow debugging systems of constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6132</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6132</id><created>2014-02-04</created><authors><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author><author><keyname>Arun</keyname><forenames>P. V.</forenames></author></authors><title>Comparative analysis of common edge detection techniques in context of
  object extraction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edges characterize boundaries and are therefore a problem of practical
importance in remote sensing.In this paper a comparative study of various edge
detection techniques and band wise analysis of these algorithms in the context
of object extraction with regard to remote sensing satellite images from the
Indian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 as
well as Google Earth is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6133</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6133</id><created>2014-02-05</created><authors><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author><author><keyname>Arun</keyname><forenames>P. V.</forenames></author></authors><title>A review over the applicability of image entropy in analyses of remote
  sensing datasets</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.6926</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy is the measure of uncertainty in any data and is adopted for
maximisation of mutual information in many remote sensing operations. The
availability of wide entropy variations motivated us for an investigation over
the suitability preference of these versions to specific operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6135</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6135</id><created>2014-02-05</created><authors><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author><author><keyname>Arun</keyname><forenames>P. V.</forenames></author></authors><title>Cellular Automata based adaptive resampling technique for the processing
  of remotely sensed imagery</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resampling techniques are being widely used at different stages of satellite
image processing. The existing methodologies cannot perfectly recover features
from a completely under sampled image and hence an intelligent adaptive
resampling methodology is required. We address these issues and adopt an error
metric from the available literature to define interpolation quality. We also
propose a new resampling scheme that adapts itself with regard to the pixel and
texture variation in the image. The proposed CNN based hybrid method has been
found to perform better than the existing methods as it adapts itself with
reference to the image features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6136</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6136</id><created>2014-02-05</created><authors><author><keyname>Arun</keyname><forenames>P. V.</forenames></author><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author></authors><title>An evolutionary computational based approach towards automatic image
  registration</title><categories>cs.CV cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.6711</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration is a key component of various image processing operations
which involve the analysis of different image data sets. Automatic image
registration domains have witnessed the application of many intelligent
methodologies over the past decade; however inability to properly model object
shape as well as contextual information had limited the attainable accuracy. In
this paper, we propose a framework for accurate feature shape modeling and
adaptive resampling using advanced techniques such as Vector Machines, Cellular
Neural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to be
effective in improving feature matching as well as resampling stages of
registration and complexity of the approach has been considerably reduced using
corset optimization The salient features of this work are cellular neural
network approach based SIFT feature point optimisation, adaptive resampling and
intelligent object modelling. Developed methodology has been compared with
contemporary methods using different statistical measures. Investigations over
various satellite images revealed that considerable success was achieved with
the approach. System has dynamically used spectral and spatial information for
representing contextual knowledge using CNN-prolog approach. Methodology also
illustrated to be effective in providing intelligent interpretation and
adaptive resampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6137</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6137</id><created>2014-02-05</created><authors><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author><author><keyname>Arun</keyname><forenames>P. V.</forenames></author></authors><title>An enhanced neural network based approach towards object extraction</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The improvements in spectral and spatial resolution of the satellite images
have facilitated the automatic extraction and identification of the features
from satellite images and aerial photographs. An automatic object extraction
method is presented for extracting and identifying the various objects from
satellite images and the accuracy of the system is verified with regard to IRS
satellite images. The system is based on neural network and simulates the
process of visual interpretation from remote sensing images and hence increases
the efficiency of image analysis. This approach obtains the basic
characteristics of the various features and the performance is enhanced by the
automatic learning approach, intelligent interpretation, and intelligent
interpolation. The major advantage of the method is its simplicity and that the
system identifies the features not only based on pixel value but also based on
the shape, haralick features etc of the objects. Further the system allows
flexibility for identifying the features within the same category based on size
and shape. The successful application of the system verified its effectiveness
and the accuracy of the system were assessed by ground truth verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6139</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6139</id><created>2014-05-11</created><authors><author><keyname>Aristov</keyname><forenames>Vladimir</forenames></author><author><keyname>Stroganov</keyname><forenames>Andrey</forenames></author></authors><title>Development of the method of computer analogy for studying and solving
  complex nonlinear systems</title><categories>cs.NA math.NA</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method of representation of a solution as segments of the series in powers
of the step of the independent variable is expanded for solving complex systems
of ordinary differential equations (ODE): the Lorenz system and other systems.
A new procedure of reduction of the representation of the solution to a sum of
two parts (regular and random) is performed. A shifting procedure is applied in
each level of the independent variable to the random part and it acts as the
filter that extracts the values to the regular part. In certain cases it is
possible to omit the random part and construct the approximation which does not
converge but still provides the qualitative information about the full solution
(a linear approximation provides a simple exact solution). Evaluation of the
error for this case is performed. Constructing the analytical representation of
the solutions for these systems by the developed method is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6142</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6142</id><created>2014-05-08</created><authors><author><keyname>Maguire</keyname><forenames>Phil</forenames></author><author><keyname>Moser</keyname><forenames>Philippe</forenames></author><author><keyname>Maguire</keyname><forenames>Rebecca</forenames></author><author><keyname>Keane</keyname><forenames>Mark</forenames></author></authors><title>A Computational Theory of Subjective Probability</title><categories>cs.AI</categories><comments>Maguire, P., Moser, P. Maguire, R. &amp; Keane, M.T. (2013) &quot;A
  computational theory of subjective probability.&quot; In M. Knauff, M. Pauen, N.
  Sebanz, &amp; I. Wachsmuth (Eds.), Proceedings of the 35th Annual Conference of
  the Cognitive Science Society (pp. 960-965). Austin, TX: Cognitive Science
  Society</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we demonstrate how algorithmic probability theory is applied
to situations that involve uncertainty. When people are unsure of their model
of reality, then the outcome they observe will cause them to update their
beliefs. We argue that classical probability cannot be applied in such cases,
and that subjective probability must instead be used. In Experiment 1 we show
that, when judging the probability of lottery number sequences, people apply
subjective rather than classical probability. In Experiment 2 we examine the
conjunction fallacy and demonstrate that the materials used by Tversky and
Kahneman (1983) involve model uncertainty. We then provide a formal
mathematical proof that, for every uncertain model, there exists a conjunction
of outcomes which is more subjectively probable than either of its constituents
in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6146</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6146</id><created>2014-05-23</created><updated>2014-08-21</updated><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Weinberg</keyname><forenames>S. Matthew</forenames></author></authors><title>A Simple and Approximately Optimal Mechanism for an Additive Buyer</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a monopolist seller with $n$ heterogeneous items, facing a single
buyer. The buyer has a value for each item drawn independently according to
(non-identical) distributions, and his value for a set of items is additive.
The seller aims to maximize his revenue. It is known that an optimal mechanism
in this setting may be quite complex, requiring randomization [HR12] and menus
of infinite size [DDT13]. Hart and Nisan [HN12] have initiated a study of two
very simple pricing schemes for this setting: item pricing, in which each item
is priced at its monopoly reserve; and bundle pricing, in which the entire set
of items is priced and sold as one bundle. Hart and Nisan [HN12] have shown
that neither scheme can guarantee more than a vanishingly small fraction of the
optimal revenue. In sharp contrast, we show that for any distributions, the
better of item and bundle pricing is a constant-factor approximation to the
optimal revenue. We further discuss extensions to multiple buyers and to
valuations that are correlated across items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6147</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6147</id><created>2014-05-08</created><authors><author><keyname>Raid</keyname><forenames>A. M.</forenames></author><author><keyname>Khedr</keyname><forenames>W. M.</forenames></author><author><keyname>El-dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Wesam</forenames></author></authors><title>Jpeg Image Compression Using Discrete Cosine Transform - A Survey</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing requirements for transmission of images in computer,
mobile environments, the research in the field of image compression has
increased significantly. Image compression plays a crucial role in digital
image processing, it is also very important for efficient transmission and
storage of images. When we compute the number of bits per image resulting from
typical sampling rates and quantization methods, we find that Image compression
is needed. Therefore development of efficient techniques for image compression
has become necessary .This paper is a survey for lossy image compression using
Discrete Cosine Transform, it covers JPEG compression algorithm which is used
for full-colour still image applications and describes all the components of
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6154</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6154</id><created>2014-05-23</created><authors><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Energy Efficient Multiuser Scheduling: Statistical Guarantees on Bursty
  Packet Loss</title><categories>cs.IT cs.NI math.IT</categories><comments>Proc. Physcomnet in conjunction with WIOPT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider energy efficient multiuser scheduling. Packet loss
tolerance of the applications is exploited to minimize average system energy.
There is a constraint on average packet drop rate and maximum number of packets
dropped successively (bursty loss). A finite buffer size is assumed. We propose
a scheme which schedules the users opportunistically according to the channel
conditions, packet loss constraints and buffer size parameters. We assume
imperfect channel state information at the transmitter side and analyze the
scheme in large user limit using stochastic optimization techniques. First, we
optimize system energy for a fixed buffer size which results in a corresponding
statistical guarantee on successive packet drop. Then, we determine the minimum
buffer size to achieve a target (improved) energy efficiency for the same (or
better) statistical guarantee. We show that buffer size can be traded
effectively to achieve system energy efficiency for target statistical
guarantees on packet loss parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6157</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6157</id><created>2014-05-04</created><updated>2014-10-20</updated><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author></authors><title>Fractional Repetition and Erasure Batch Codes</title><categories>cs.IT math.IT</categories><comments>Was presented at the fourth International Castle Meeting on Coding
  Theory and Applications (4ICMCTA), Palmela, Portugal, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batch codes are a family of codes that represent a distributed storage system
(DSS) of $n$ nodes so that any batch of $t$ data symbols can be retrieved by
reading at most one symbol from each node. Fractional repetition codes are a
family of codes for DSS that enable efficient uncoded repairs of failed nodes.
In this work these two families of codes are combined to obtain fractional
repetition batch (FRB) codes which provide both uncoded repairs and parallel
reads of subsets of stored symbols. In addition, new batch codes which can
tolerate node failures are considered. This new family of batch codes is called
erasure combinatorial batch codes (ECBCs). Some properties of FRB codes and
ECBCs and examples of their constructions based on transversal designs and
affine planes are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6159</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6159</id><created>2014-04-30</created><updated>2014-08-20</updated><authors><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>A Bi-clustering Framework for Consensus Problems</title><categories>cs.CV cs.LG stat.ML</categories><doi>10.1137/140967325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider grouping as a general characterization for problems such as
clustering, community detection in networks, and multiple parametric model
estimation. We are interested in merging solutions from different grouping
algorithms, distilling all their good qualities into a consensus solution. In
this paper, we propose a bi-clustering framework and perspective for reaching
consensus in such grouping problems. In particular, this is the first time that
the task of finding/fitting multiple parametric models to a dataset is formally
posed as a consensus problem. We highlight the equivalence of these tasks and
establish the connection with the computational Gestalt program, that seeks to
provide a psychologically-inspired detection theory for visual events. We also
present a simple but powerful bi-clustering algorithm, specially tuned to the
nature of the problem we address, though general enough to handle many
different instances inscribed within our characterization. The presentation is
accompanied with diverse and extensive experimental results in clustering,
community detection, and multiple parametric model estimation in image
processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6161</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6161</id><created>2014-04-29</created><authors><author><keyname>Rakhshan</keyname><forenames>Ali</forenames></author><author><keyname>Ray</keyname><forenames>Evan</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author></authors><title>Real-Time Estimation of the Distribution of Brake Response Times for an
  Individual Driver Using Vehicular Ad Hoc Network</title><categories>cs.OH cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adapting the functioning of the collision warning systems to the specific
drivers' characteristics is of great benefit to drivers. For example, by
customizing collision warning algorithms we can minimize false alarms, thereby
reducing injuries and deaths in highway traffic accidents. In order to take the
behaviors of individual drivers into account, the system needs to have a
Real-Time estimation of the distribution of brake response times for an
individual driver. In this paper, we propose a method for doing this estimation
which is not computationally intensive and can take advantage of the
information contained in all data points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6162</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6162</id><created>2014-04-29</created><updated>2014-07-31</updated><authors><author><keyname>Gray</keyname><forenames>Alan</forenames></author><author><keyname>Stratford</keyname><forenames>Kevin</forenames></author></authors><title>targetDP: an Abstraction of Lattice Based Parallelism with Portable
  Performance</title><categories>cs.DC hep-lat physics.comp-ph</categories><comments>4 pages, 1 figure, to appear in proceedings of HPCC 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve high performance on modern computers, it is vital to map
algorithmic parallelism to that inherent in the hardware. From an application
developer's perspective, it is also important that code can be maintained in a
portable manner across a range of hardware. Here we present targetDP (target
Data Parallel), a lightweight programming layer that allows the abstraction of
data parallelism for applications that employ structured grids. A single source
code may be used to target both thread level parallelism (TLP) and instruction
level parallelism (ILP) on either SIMD multi-core CPUs or GPU-accelerated
platforms. targetDP is implemented via standard C preprocessor macros and
library functions, can be added to existing applications incrementally, and can
be combined with higher-level paradigms such as MPI. We present CPU and GPU
performance results for a benchmark taken from the lattice Boltzmann
application that motivated this work. These demonstrate not only performance
portability, but also the optimisation resulting from the intelligent exposure
of ILP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6163</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6163</id><created>2014-04-28</created><updated>2014-06-04</updated><authors><author><keyname>Li</keyname><forenames>Borui</forenames></author><author><keyname>Mu</keyname><forenames>Chundi</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Peng</keyname><forenames>Qian</forenames></author></authors><title>Revised Version of a JCIT Paper-Comparison of Feature Point Extraction
  Algorithms for Vision Based Autonomous Aerial Refueling</title><categories>cs.OH</categories><journal-ref>Journal of Convergence Information Technology. 2012, 7(20):
  108-118</journal-ref><doi>10.4156/jcit.vol7.issue20.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a revised version of our paper published in Journal of Convergence
Information Technology(JCIT): &quot;Comparison of Feature Point Extraction
Algorithms for Vision Based Autonomous Aerial Refueling&quot;. We corrected some
errors including measurement unit errors, spelling errors and so on. Since the
published papers in JCIT are not allowed to be modified, we submit the revised
version to arXiv.org to make the paper more rigorous and not to confuse other
researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6164</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6164</id><created>2014-04-23</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Lampouras</keyname><forenames>Gerasimos</forenames></author><author><keyname>Galanis</keyname><forenames>Dimitrios</forenames></author></authors><title>Generating Natural Language Descriptions from OWL Ontologies: the
  NaturalOWL System</title><categories>cs.CL cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 48, pages
  671-715, 2013</journal-ref><doi>10.1613/jair.4017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present NaturalOWL, a natural language generation system that produces
texts describing individuals or classes of OWL ontologies. Unlike simpler OWL
verbalizers, which typically express a single axiom at a time in controlled,
often not entirely fluent natural language primarily for the benefit of domain
experts, we aim to generate fluent and coherent multi-sentence texts for
end-users. With a system like NaturalOWL, one can publish information in OWL on
the Web, along with automatically produced corresponding texts in multiple
languages, making the information accessible not only to computer programs and
domain experts, but also end-users. We discuss the processing stages of
NaturalOWL, the optional domain-dependent linguistic resources that the system
can use at each stage, and why they are useful. We also present trials showing
that when the domain-dependent llinguistic resources are available, NaturalOWL
produces significantly better texts compared to a simpler verbalizer, and that
the resources can be created with relatively light effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6166</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6166</id><created>2014-04-10</created><authors><author><keyname>Kumar</keyname><forenames>D. Sachin</forenames></author><author><keyname>Seshadri</keyname><forenames>P. R.</forenames></author><author><keyname>Vaishnav</keyname><forenames>N.</forenames></author><author><keyname>Janaki</keyname><forenames>Dr. Saraswathi</forenames></author></authors><title>Real Time Speckle Image De-Noising</title><categories>cs.CV</categories><comments>9 pages</comments><journal-ref>Advances in Vision Computing: An International Journal (AVC)
  Vol.1, No.1, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents real time speckle de-noising based on activity computation
algorithm and wavelet transform. Speckles arise in an image when laser light is
reflected from an illuminated surface. The process involves detection of
speckles in an image by obtaining a number of frames of the same object under
different illumination or angle and comparing the frames for the granular
computation and de-noising the same on presence of greater activity index. The
project can be implemented in FPGA (Field Programmable Gate Array) technology.
The results can be shown that the used activity computation algorithm and
wavelet transform has better accuracy in the process of speckle detection and
de-noising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6168</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6168</id><created>2014-04-07</created><authors><author><keyname>Warnars</keyname><forenames>Spits</forenames></author></authors><title>Human Face as human single identity</title><categories>cs.CV</categories><comments>6 pages, 5 figures, the 4th Indonesia Japan Joint Scientific
  Symposium (IJJSS), Bali, Indonesia, Sept 29- Oct 1 2010. (ISSN : 2087-577)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human face as a physical human recognition can be used as a unique identity
for computer to recognize human by transforming human face with face algorithm
as simple text number which can be primary key for human. Human face as single
identity for human will be done by making a huge and large world centre human
face database, where the human face around the world will be recorded from time
to time and from generation to generation. Architecture database will be
divided become human face image database which will save human face images and
human face output code which will save human face output code as a
transformation human face image with face algorithm. As an improvement the
slightly and simple human face output code database will make human face
searching process become more fast. Transaction with human face as a
transaction without card can make human no need their card for the transaction
and office automation and banking system as an example for implementation
architecture. As an addition suspect human face database can be extended for
fighting crime and terrorism by doing surveillance and searching suspect human
face around the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6169</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6169</id><created>2014-04-01</created><authors><author><keyname>Takahashi</keyname><forenames>Takeshi</forenames></author><author><keyname>Kadobayashi</keyname><forenames>Youki</forenames></author><author><keyname>Fujiwara</keyname><forenames>Hiroyuki</forenames></author></authors><title>Ontological Approach toward Cybersecurity in Cloud Computing</title><categories>cs.CR cs.DC</categories><comments>This is a preprint version of our paper presented in SIN'10, Sept.
  7-11, 2010, Taganrog, Rostov Oblast, Russia, International Conference on
  Security of Information and Networks, 2010</comments><doi>10.1145/1854099.1854121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Widespread deployment of the Internet enabled building of an emerging IT
delivery model, i.e., cloud computing. Albeit cloud computing-based services
have rapidly developed, their security aspects are still at the initial stage
of development. In order to preserve cybersecurity in cloud computing,
cybersecurity information that will be exchanged within it needs to be
identified and discussed. For this purpose, we propose an ontological approach
to cybersecurity in cloud computing. We build an ontology for cybersecurity
operational information based on actual cybersecurity operations mainly focused
on non-cloud computing. In order to discuss necessary cybersecurity information
in cloud computing, we apply the ontology to cloud computing. Through the
discussion, we identify essential changes in cloud computing such as data-asset
decoupling and clarify the cybersecurity information required by the changes
such as data provenance and resource dependency information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6170</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6170</id><created>2014-03-12</created><authors><author><keyname>Kushwah</keyname><forenames>Mr. Atul Singh</forenames></author><author><keyname>Manglasheril</keyname><forenames>Mr. Sachin</forenames></author></authors><title>Performance Estimation of 2*3 MIMO-MC-CDMA in Rayleigh Fading Channel</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures and 2 tables</comments><journal-ref>Mr. Atul Singh Kushwah , Mr. Sachin Manglasheril.&quot;Performance
  Estimation of 2*3 MIMO-MC-CDMA in Rayleigh Fading Channel&quot;. International
  Journal of Computer Trends and Technology (IJCTT) V9(1):32-35, March 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9P107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the performance of 2by3 MIMOMCCDMA system in MATLAB
which greatly reduces BER by increasing the efficiency of system. MIMO and
MCCDMA system arrangement is used to decrease bit error rate and also figure a
new system called MCCDMA which is multiple user and multiple access system used
to enhance the performance of the system. MCCDMA is a narrowband flat fading in
character which changes frequency selective to several narrowband flat fading
multiple parallel subcarriers to enhance the effectiveness of the system. Now
this MCCDMA further improved by grouping by 2by3 MIMO system which make use of
ZF decoder at the receiver to reduce BER in which half rate convolutional
encoded Alamouti STBC block code is intended for channel encoding as transmit
diversity for MIMO with multiple transmit and receive antennas. Main
improvement of using MIMOMCCDMA is for reducing complication of system and also
for dropping BER and finally increasing gain of the system. Then we estimate
the system in different modulation techniques like, 64QAM, 8PSK, 16QAM, QPSK,
32QAM and 8QAM using MATLAB in Rayleigh fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6171</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6171</id><created>2014-03-12</created><authors><author><keyname>Kushwah</keyname><forenames>Atul Singh</forenames></author><author><keyname>Manglasheril</keyname><forenames>Sachin</forenames></author></authors><title>Performance Estimation of 2*3 MIMO-MC-CDMA using Convolution Code</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures and 2 tables</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V9(1):21-25, March 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9P105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we estimate the performance of 2by3 MIMOMCCDMA system using
convolution code in MATLAB which highly reduces BER by increasing the
efficiency of system. MIMO and MCCDMA system combination is used to reduce bit
error rate and also for forming a new system called MCCDMA which is multi user
and multiple access schemes used to increase the performance of the system.
MCCDMA system is a narrowband flat fading in nature which converts frequency
selective to numerous narrowband flat fading multiple parallel sub-carriers to
increase the efficiency of the system. Now this MCCDMA system can also be
enhanced by grouping with 2by3 MIMO system which utilizes ZF decoder at the
receiver to decrease BER in which half rate convolutionally encoded Alamouti
STBC block code is used for channel encoding scheme as transmit diversity of
MIMO with multiple transmit antenna. And convolution encoder is also used as
source encoder or FEC encoder in MIMOMCCDMA. Advantage of using MIMO-MC-CDMA
using convolution code is due to reduce complexity of system and also for
reducing BER and finally to increase gain of system. Now after this we examine
system in various modulation techniques like, 8PSK, 16QAM, QPSK, 32QAM, 8QAM
and 64QAM using MATLAB in Rayleigh fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6173</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6173</id><created>2014-02-27</created><authors><author><keyname>Marghny</keyname><forenames>M. H.</forenames></author><author><keyname>El-Aziz</keyname><forenames>Rasha M. Abd</forenames></author><author><keyname>Taloba</keyname><forenames>Ahmed I.</forenames></author></authors><title>An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study</title><categories>cs.NE cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Clustering analysis plays an important role in scientific research and
commercial application. K-means algorithm is a widely used partition method in
clustering. However, it is known that the K-means algorithm may get stuck at
suboptimal solutions, depending on the choice of the initial cluster centers.
In this article, we propose a technique to handle large scale data, which can
select initial clustering center purposefully using Genetic algorithms (GAs),
reduce the sensitivity to isolated point, avoid dissevering big cluster, and
overcome deflexion of data in some degree that caused by the disproportion in
data partitioning owing to adoption of multi-sampling. We applied our method to
some public datasets these show the advantages of the proposed approach for
example Hepatitis C dataset that has been taken from the machine learning
warehouse of University of California. Our aim is to evaluate hepatitis
dataset. In order to evaluate this dataset we did some preprocessing operation,
the reason to preprocessing is to summarize the data in the best and suitable
way for our algorithm. Missing values of the instances are adjusted using local
mean method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6174</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6174</id><created>2014-02-26</created><updated>2014-11-01</updated><authors><author><keyname>Wang</keyname><forenames>Shuliang</forenames></author><author><keyname>Zhou</keyname><forenames>Zhe</forenames></author><author><keyname>Shi</keyname><forenames>Wenzhong</forenames></author></authors><title>Adaptive Minimum-Maximum Exclusive Mean Filter for Impulse Noise Removal</title><categories>cs.OH</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  experiment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many filters are proposed for impulse noise removal. However, they are hard
to keep excellent denoising performance with high computational efficiency. In
response to this difficulty, this paper presents a novel fast filter, adaptive
minimum-maximum exclusive mean (AMMEM) filter to remove impulse noise. Although
the AMMEM filter is a variety of the maximum-minimum exclusive mean (MMEM)
filter, however, the AMMEM filter inherits the advantages, and overcomes the
drawbacks, compared with the MMEM filter. To increase the various performances
of noise removal, the AMMEM filter uses an adaptive size window, introduces two
flexible factors, projection factor P and detection factor T, and limits the
calculation scope of the AVG. The experimental results show the AMMEM filter
makes a significant improvement in terms of noise detection, image restoration,
and computational efficiency. Even at noise level as high as 95%, the AMMEM
filter still can restore the images with good visual effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6177</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6177</id><created>2014-02-14</created><authors><author><keyname>Habib</keyname><forenames>Md. Tarek</forenames></author><author><keyname>Faisal</keyname><forenames>Rahat Hossain</forenames></author><author><keyname>Rokonuzzaman</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>Farruk</forenames></author></authors><title>Automated Fabric Defect Inspection: A Survey of Classifiers</title><categories>cs.CV cs.LG</categories><comments>9 pages, 4 figures, 2 tables</comments><doi>10.5121/ijfcst.2014.4102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality control at each stage of production in textile industry has become a
key factor to retaining the existence in the highly competitive global market.
Problems of manual fabric defect inspection are lack of accuracy and high time
consumption, where early and accurate fabric defect detection is a significant
phase of quality control. Computer vision based, i.e. automated fabric defect
inspection systems are thought by many researchers of different countries to be
very useful to resolve these problems. There are two major challenges to be
resolved to attain a successful automated fabric defect inspection system. They
are defect detection and defect classification. In this work, we discuss
different techniques used for automated fabric defect classification, then show
a survey of classifiers used in automated fabric defect inspection systems, and
finally, compare these classifiers by using performance metrics. This work is
expected to be very useful for the researchers in the area of automated fabric
defect inspection to understand and evaluate the many potential options in this
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6178</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6178</id><created>2014-02-09</created><authors><author><keyname>Bahrami</keyname><forenames>Mehdi</forenames></author><author><keyname>arebi</keyname><forenames>Peyman</forenames></author><author><keyname>Bakhshizadeh</keyname><forenames>Hosseyn</forenames></author><author><keyname>Barangi</keyname><forenames>Hamed</forenames></author></authors><title>A Novel Self-Recognition Method for Autonomic Grid Networks Case Study:
  Advisor Labor Law Software Application</title><categories>cs.DC</categories><journal-ref>Advanced in Information Sciences and Service Sciences. Volume 3,
  Number 5, pp.262-269, June 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, Grid Computing Systems have provided wide integrated use of
resources. Grid computing systems provide the ability to share, select and
aggregate distributed resources as computers, storage systems or other devices
in an integrated way. Grid computing systems have solved many problems in
science, engineering and commerce fields. In this paper we introduce a
self-recognition algorithm for grid network and introduced this algorithm to
have exclusive management control on the autonomic grid networks. This
algorithm is base on binomial heap to allocate and recognition any node in the
grid. We try to using this algorithm in advisor labor law software application
as case study and shown in this application how to use this method for any
advisor application on the network. By this implementation model shown this
method can get better answer to any question as a best labor law advisor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6179</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6179</id><created>2014-02-06</created><authors><author><keyname>Khoshkbarforoushha</keyname><forenames>A.</forenames><affiliation>School of Computer Science, Australian National University</affiliation><affiliation>CSIRO Computational Informatics, Building 108, Australian National University</affiliation></author><author><keyname>Jamshidi</keyname><forenames>P.</forenames><affiliation>Lero - The Irish Software Engineering Research Centre, School of Computing, Dublin City University</affiliation></author><author><keyname>Fahmideh</keyname><forenames>M.</forenames><affiliation>Automated Software Engineering Research Group, Electrical and Computer Engineering Faculty, Shahid Beheshti University</affiliation></author><author><keyname>Wang</keyname><forenames>L.</forenames><affiliation>School of Computer Science, China University of Geosciences</affiliation></author><author><keyname>Ranjan</keyname><forenames>R.</forenames><affiliation>CSIRO Computational Informatics, Building 108, Australian National University</affiliation></author></authors><title>Metrics for BPEL Process Reusability Analysis in a Workflow System</title><categories>cs.SE</categories><comments>This the extended version of the conference paper that Published in:
  WETSoM '10 Proceedings of the 2010 ICSE Workshop on Emerging Trends in
  Software Metrics Pages 67-74, ACM New York, NY, USA \c{opyright}2010</comments><doi>10.1145/1809223.1809233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a quantitative metric to analyze potential reusability of
a BPEL (Business Process Execution Language) Process. The approach is based on
Description and Logic Mismatch Probability of a BPEL Process that will be
reused within potential contexts. The mismatch probabilities have been
consolidated to a metric formula for quantifying the probability of potential
reuse of BPEL processes. An initial empirical evaluation suggests that the
proposed metric properly predict potential reusability of BPEL processes.
According to the experiment, there exists a significant statistical correlation
between results of the metric and the experts judgements. This indicates a
predictive dependency between the proposed metric and potential reusability of
BPEL processes as a measuring stick for this phenomena. If future studies
ascertain these findings by replicating this experiment, the practical
implications of such a metric are early detection of the design flaws and
aiding architects to judge various design alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6181</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6181</id><created>2014-05-06</created><authors><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author></authors><title>Py-oopsi: the python implementation of the fast-oopsi algorithm</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Fast-oopsi was developed by Joshua Vogelstein in 2009, which is now widely
used to extract neuron spike activities from calcium fluorescence signals.
Here, we propose detailed implementation of the fast-oopsi algorithm in python
programming language. Some corrections are also made to the original fast-oopsi
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6196</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6196</id><created>2014-05-23</created><updated>2015-09-13</updated><authors><author><keyname>Tallapragada</keyname><forenames>Pavankumar</forenames></author><author><keyname>Cortes</keyname><forenames>Jorge</forenames></author></authors><title>Event-Triggered Stabilization of Linear Systems Under Bounded Bit Rates</title><categories>cs.SY math.OC</categories><comments>Accepted for publication in IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2015.2480215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of exponential practical stabilization of
linear time-invariant systems with disturbances using event-triggered control
and bounded communication bit rate. We consider both the case of instantaneous
communication with finite precision data at each transmission and the case of
non-instantaneous communication with bounded communication rate. Given a
prescribed rate of convergence, the proposed event-triggered control
implementations opportunistically determine the transmission instants and the
finite precision data to be transmitted on each transmission. We show that our
design exponentially practically stabilizes the origin while guaranteeing a
uniform positive lower bound on the inter-transmission and inter-reception
times, ensuring that the number of bits transmitted on each transmission is
upper bounded uniformly in time, and allowing for the possibility of
transmitting fewer bits at any given time if more bits than prescribed were
transmitted earlier. We also characterize the necessary and sufficient average
data rate for exponential practical stabilization. Several simulations
illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6200</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6200</id><created>2014-05-23</created><authors><author><keyname>Dong</keyname><forenames>Mianxiong</forenames></author><author><keyname>Li</keyname><forenames>He</forenames></author><author><keyname>Ota</keyname><forenames>Kaoru</forenames></author><author><keyname>Zhu</keyname><forenames>Haojin</forenames></author></authors><title>HVSTO: Efficient Privacy Preserving Hybrid Storage in Cloud Data Center</title><categories>cs.DC cs.CR</categories><comments>7 pages, 8 figures, in proceeding of The Second International
  Workshop on Security and Privacy in Big Data (BigSecurity 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud data center, shared storage with good management is a main structure
used for the storage of virtual machines (VM). In this paper, we proposed
Hybrid VM storage (HVSTO), a privacy preserving shared storage system designed
for the virtual machine storage in large-scale cloud data center. Unlike
traditional shared storage, HVSTO adopts a distributed structure to preserve
privacy of virtual machines, which are a threat in traditional centralized
structure. To improve the performance of I/O latency in this distributed
structure, we use a hybrid system to combine solid state disk and distributed
storage. From the evaluation of our demonstration system, HVSTO provides a
scalable and sufficient throughput for the platform as a service
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6215</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6215</id><created>2014-03-30</created><authors><author><keyname>Bashir</keyname><forenames>Mohammed Bakri</forenames></author><author><keyname>Latiff</keyname><forenames>Muhammad Shafie Abd</forenames></author><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Loon</keyname><forenames>Cheah Tek</forenames></author></authors><title>Grid-based Search Technique for Massive Academic Publications</title><categories>cs.DC cs.DL</categories><comments>4 pages, 5 figures, conference. The 2014 Third ICT International
  Student Project Conference (ICT-ISPC2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numerical size of academic publications that are being published in
recent years had grown rapidly. Accessing and searching massive academic
publications that are distributed over several locations need large amount of
computing resources to increase the system performance. Therefore, many
grid-based search techniques were proposed to provide flexible methods for
searching the distributed extensive data. This paper proposes search technique
that is capable of searching the extensive publications by utilizing grid
computing technology. The search technique is implemented as interconnected
grid services to offer a mechanism to access different data locations. The
experimental result shows that the grid-based search technique has enhanced the
performance of the search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6216</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6216</id><created>2014-02-10</created><authors><author><keyname>Aggarwal</keyname><forenames>Akshai</forenames></author><author><keyname>Gandhi</keyname><forenames>Savita</forenames></author><author><keyname>Chaubey</keyname><forenames>Nirbhay</forenames></author><author><keyname>Tada</keyname><forenames>Naren</forenames></author><author><keyname>Trivedi</keyname><forenames>Srushti</forenames></author></authors><title>NDTAODV: Neighbor Defense Technique for Ad Hoc On-Demand Distance
  Vector(AODV) to mitigate flood attack in MANETS</title><categories>cs.NI cs.CR</categories><comments>14 Pages, 13 Figure. arXiv admin note: text overlap with
  arXiv:1202.4628 by other authors</comments><doi>10.5121/ijcnc.2014.6102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad Hoc Networks (MANETs) are collections of mobile nodes that can
communicate with one another using multihop wireless links. MANETs are often
deployed in the environments, where there is no fixed infrastructure and
centralized management. The nodes of mobile ad hoc networks are susceptible to
compromise. In such a scenario, designing an efficient, reliable and secure
routing protocol has been a major challengesue over the last many years. The
routing protocol Ad hoc On-demand Distance Vector (AODV) has no security
measures in-built in it. It is vulnerable to many types of routing attacks. The
flood attack is one of them. In this paper, we propose a simple and effective
technique to secure Ad hoc Ondemand Distance Vector (AODV) routing protocol
against flood attacks. To deal with a flood attack, we have proposed Neighbor
Defense Technique for Ad hoc On-demand Distance Vector (NDTAODV). This makes
AODV more robust. The proposed technique has been designed to isolate the flood
attacker with the use of timers, peak value and hello alarm technique. We have
simulated our work in Network Simulator NS-2.33 (NS-2) with different pause
times by way of different number of malicious nodes. We have compared the
performance of NDTAODV with the AODV in normal situation as well as in the
presence of malicious attacks. We have considered Packet Delivery Fraction
(PDF), Average Throughput (AT) and Normalized Routing Load (NRL) for comparing
the performance of NDTAODV and AODV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6217</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6217</id><created>2014-05-11</created><authors><author><keyname>Ahmad</keyname><forenames>Tanveer</forenames></author><author><keyname>feng</keyname><forenames>Yan tian</forenames></author></authors><title>An improved accelerated frame slotted aloha (afsa) algorithm for tag
  collision in rfid</title><categories>cs.NI</categories><comments>8 pages</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT) Vol.2, No.4, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of tag identification in an RFID system can be low down due to
the tag collision problems. the tag collision problem occurs when a reader try
to read multiple tags in an interrogation zone. as a result the reader does not
identify the tag correctly. that causes a loss of information or data
interference. to solve such kind of issues a series of aloha based algorithm
and binary search algorithm have beed proposed. the most simple popular and
good giving performance algorithm are aloha based anti-collision algorithms. in
this paper we present a new variation in accelerated slotted aloha afsa. our
proposed algorithm by using the bitmaps and avoids wastages in bit times due to
idleness and collided slots reduce the tag reading time. the simulation result
shows that afsa can significantly reduce the average tag reading time with
respect to the base protocols and achieve high tag reading rates under both
static and mobile setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6218</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6218</id><created>2014-05-09</created><authors><author><keyname>Jungum</keyname><forenames>Nevin Vunka</forenames></author><author><keyname>Doomun</keyname><forenames>Razvi M.</forenames></author><author><keyname>Ghurbhurrun</keyname><forenames>Soulakshmee D.</forenames></author><author><keyname>Pudaruth</keyname><forenames>Sameerchand</forenames></author></authors><title>Collaborative Driving Support System in Mobile Pervasive Environments</title><categories>cs.NI</categories><comments>pp 358-363</comments><journal-ref>Nevin Vunka Jungum, R. Doomun, S. D. Ghurbhurrun, S. Pudaruth,
  &quot;Collaborative Driving Support System in Mobile Pervasive Environments&quot;, The
  4th Intl' Conference on Wireless and Mobile Communications, pp 358-363, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bluetooth protocol can be used for intervehicle communication equipped
with Bluetooth devices. This work investigates the challenges and feasibility
of developing intelligent driving system providing timesensitive information
about traffic conditions and roadside facilities. The architecture for
collaborative vehicle communication system is presented using the concepts of
wireless networks and Bluetooth protocol. We discuss how vehicles can form
mobile ad-hoc networks and exchange data by the on-board Bluetooth sensors. The
key design concepts of the intelligent driving service infrastructure are
analyzed showing collaborative fusion of multiple positional data could give a
better understanding of the surrounding traffic conditions for collaborative
driving. The technical feasibility of using Bluetooth for data exchange among
moving vehicles is evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6222</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6222</id><created>2014-04-07</created><updated>2015-06-08</updated><authors><author><keyname>Trefois</keyname><forenames>Maguy</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author></authors><title>Zero forcing number, constrained matchings and strong structural
  controllability</title><categories>cs.DM math.CO</categories><comments>Submitted as a journal paper in May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The zero forcing number is a graph invariant introduced to study the minimum
rank of the graph. In 2008, Aazami proved the NP-hardness of computing the zero
forcing number of a simple undirected graph. We complete this NP-hardness
result by showing that the non-equivalent problem of computing the zero forcing
number of a directed graph allowing loops is also NP-hard. The rest of the
paper is devoted to the strong controllability of a networked system. This kind
of controllability takes into account only the structure of the interconnection
graph, but not the interconnection strengths along the edges. We provide a
necessary and sufficient condition in terms of zero forcing sets for the strong
controllability of a system whose underlying graph is a directed graph allowing
loops. Moreover, we explain how our result differs from a recent related result
discovered by Monshizadeh et al. Finally, we show how to solve the problem of
finding efficiently a minimum-size input set for the strong controllability of
a self-damped system with a tree-structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6223</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6223</id><created>2014-04-07</created><authors><author><keyname>Li</keyname><forenames>Fangfang</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author><author><keyname>Cao</keyname><forenames>Longbing</forenames></author></authors><title>Coupled Item-based Matrix Factorization</title><categories>cs.LG cs.IR</categories><comments>7 pages submitted to AAAI2014. arXiv admin note: substantial text
  overlap with arXiv:1404.7467</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The essence of the challenges cold start and sparsity in Recommender Systems
(RS) is that the extant techniques, such as Collaborative Filtering (CF) and
Matrix Factorization (MF), mainly rely on the user-item rating matrix, which
sometimes is not informative enough for predicting recommendations. To solve
these challenges, the objective item attributes are incorporated as
complementary information. However, most of the existing methods for inferring
the relationships between items assume that the attributes are &quot;independently
and identically distributed (iid)&quot;, which does not always hold in reality. In
fact, the attributes are more or less coupled with each other by some implicit
relationships. Therefore, in this pa-per we propose an attribute-based coupled
similarity measure to capture the implicit relationships between items. We then
integrate the implicit item coupling into MF to form the Coupled Item-based
Matrix Factorization (CIMF) model. Experimental results on two open data sets
demonstrate that CIMF outperforms the benchmark methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6224</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6224</id><created>2014-05-23</created><updated>2014-10-15</updated><authors><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames><affiliation>Sandy</affiliation></author><author><keyname>Waber</keyname><forenames>Benjamin N.</forenames><affiliation>Sandy</affiliation></author><author><keyname>Alex</keyname><affiliation>Sandy</affiliation></author><author><keyname>Pentland</keyname></author><author><keyname>Schnorf</keyname><forenames>Sebastian</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>Using sociometers to quantify social interaction patterns</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on human social interactions has traditionally relied on
self-reports. Despite their widespread use, self-reported accounts of behaviour
are prone to biases and necessarily reduce the range of behaviours, and the
number of subjects, that may be studied simultaneously. The development of ever
smaller sensors makes it possible to study group-level human behaviour in
naturalistic settings outside research laboratories. We used such sensors,
sociometers, to examine gender, talkativeness and interaction style in two
different contexts. Here, we find that in the collaborative context, women were
much more likely to be physically proximate to other women and were also
significantly more talkative than men, especially in small groups. In contrast,
there were no gender-based differences in the non-collaborative setting. Our
results highlight the importance of objective measurement in the study of human
behaviour, here enabling us to discern context specific, gender-based
differences in interaction style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6228</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6228</id><created>2014-05-23</created><authors><author><keyname>Silva</keyname><forenames>Edmundo de Souza e</forenames></author><author><keyname>Leao</keyname><forenames>Rosa M.</forenames></author><author><keyname>Menasche</keyname><forenames>Daniel S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Scalability Issues in P2P Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most fundamental problems in the realm of peer-to-peer systems
consists of determining their service capacity. In this paper, we first propose
a new Markov model to compute the throughput of peer-to-peer systems. Then, we
present a simple approximate model for obtaining the system throughput for
large peer populations. From these models, we obtain novel insights on the
behavior of p2p swarming systems that motivate new mechanisms for publishers
and peers to improve the overall performance. In particular, we show that
system capacity can significantly increase if publishers adopt the most
deprived peer selection and peers reduce their service rate when they have all
the file blocks but one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6230</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6230</id><created>2014-05-23</created><updated>2014-08-26</updated><authors><author><keyname>Wolf</keyname><forenames>Ingo</forenames></author><author><keyname>Schroeder</keyname><forenames>Tobias</forenames></author><author><keyname>Neumann</keyname><forenames>Jochen</forenames></author><author><keyname>de Haan</keyname><forenames>Gerhard</forenames></author></authors><title>Changing minds about electric cars: An empirically grounded agent-based
  modeling approach</title><categories>cs.MA</categories><doi>10.1016/j.techfore.2014.10.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diffusion of electric vehicles (EVs) is considered an effective policy
strategy to meet greenhouse gas reduction targets. For large-scale adoption,
however, demand-side oriented policy measures are required, based on consumers
transport needs, values and social norms. We introduce an empirically grounded,
spatially explicit, agent-based model, InnoMind Innovation diffusion driven by
changing Minds), to simulate the effects of policy interventions and social
influence on consumers transport mode preferences. The agents in this model
represent individual consumers. They are calibrated based on empirically
derived attributes and characteristics of survey respondents. We model agent
decision-making with artificial neural networks that account for the role of
emotions in information processing. We present simulations of 4 scenarios for
the diffusion of EVs in the city of Berlin, Germany (3 policy scenarios and 1
base case). The results illustrate the varying effectiveness of measures in
different market segments and the need for appropriate policies tailored to the
heterogeneous needs of different travelers. Moreover, the simulations suggest
that introducing an exclusive zone for EVs in the city would accelerate the
early-phase diffusion of EVs more effectively than financial incentives only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6249</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6249</id><created>2014-05-23</created><authors><author><keyname>Sharifi</keyname><forenames>Shahrouz</forenames></author><author><keyname>Tanc</keyname><forenames>A. Korhan</forenames></author><author><keyname>Duman</keyname><forenames>Tolga M.</forenames></author></authors><title>On LDPC Codes for Gaussian Interference Channels</title><categories>cs.IT math.IT</categories><comments>ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the two-user Gaussian interference channel (GIC),
and study the Han-Kobayashi (HK) coding/decoding strategy with the objective of
designing low-density parity-check (LDPC) codes. A code optimization algorithm
is proposed which adopts a random perturbation technique via tracking the
average mutual information. The degree distribution optimization and
convergence threshold computation are carried out for strong and weak
interference channels, employing binary phase-shift keying (BPSK). Under strong
interference, it is observed that optimized codes operate close to the capacity
boundary. For the case of weak interference, it is shown that via the newly
designed codes, a nontrivial rate pair is achievable, which is not attainable
by single user codes with time-sharing. Performance of the designed LDPC codes
are also studied for finite block lengths through simulations of specific codes
picked from the optimized degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6256</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6256</id><created>2014-05-23</created><authors><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Xia</keyname><forenames>Lingli</forenames></author><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author></authors><title>Weight Distributions of a Class of Cyclic Codes with Arbitrary Number of
  Zeros II</title><categories>math.NT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are an important class of linear codes, whose weight
distribution have been extensively studied. So far, most of previous results
obtained were for cyclic codes with no more than three zeros. Recently,
\cite{Y-X-D12} constructed a class of cyclic codes with arbitrary number of
zeros, and computed the weight distributions for several cases. In this paper,
we determine the weight distribution for a new family of such codes. This is
achieved by certain new methods, such as the theory of Jacobi sums over finite
fields and subtle treatment of some complicated combinatorial identities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6260</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6260</id><created>2014-05-23</created><updated>2014-08-06</updated><authors><author><keyname>Bowers</keyname><forenames>John C.</forenames></author></authors><title>Faster Reductions for Straight Skeletons to Motorcycle Graphs</title><categories>cs.CG</categories><comments>Fixed typos from V3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm that reduces the straight skeleton to the motorcycle
graph in $O(n\log n)$ time for simple polygons and $O(n(\log n)\log m)$ time
for a planar straight line graph (PSLG) with $m$ connected components. This
improves on the previous best of $O(n(\log n)\log r)$ for polygons with $r$
reflex vertices (possibly with holes) and $O(n^2\log n)$ for general planar
straight line graphs. This allows us to speed up the straight skeleton
algorithm for polygons and PSLGs. For a polygon with $h$ holes and $r$ reflex
vertices we achieve a speedup from $O(n(\log n)\log r + r^{4/3+\epsilon})$ time
to $O(n(\log n)\log h + r^{4/3 + \epsilon})$ time in the non-degenerate case
and from $O(n(\log n)\log r + r^{17/11 + \epsilon})$ to $O(n(\log n)\log h +
r^{17/11 + \epsilon})$ in degenerate cases. For a PSLG with $m$ connected
components and $r$ reflex vertices, we gain a speed up from $O(n^{1 + \epsilon}
+ n^{8/11 + \epsilon}r^{9/11+\epsilon})$ to $O(n(\log n)\log m + r^{4/3 +
\epsilon})$ in the non-degenerate case and from $O(n^{1 + \epsilon} + n^{8/11 +
\epsilon}r^{9/11+\epsilon})$ to $O(n(\log n)\log m + r^{17/11 + \epsilon})$ in
the degenerate case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6261</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6261</id><created>2014-05-23</created><authors><author><keyname>Bansal</keyname><forenames>Mayank</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Geometric Polynomial Constraints in Higher-Order Graph Matching</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Correspondence is a ubiquitous problem in computer vision and graph matching
has been a natural way to formalize correspondence as an optimization problem.
Recently, graph matching solvers have included higher-order terms representing
affinities beyond the unary and pairwise level. Such higher-order terms have a
particular appeal for geometric constraints that include three or more
correspondences like the PnP 2D-3D pose problems. In this paper, we address the
problem of finding correspondences in the absence of unary or pairwise
constraints as it emerges in problems where unary appearance similarity like
SIFT matches is not available. Current higher order matching approaches have
targeted problems where higher order affinity can simply be formulated as a
difference of invariances such as lengths, angles, or cross-ratios. In this
paper, we present a method of how to apply geometric constraints modeled as
polynomial equation systems. As opposed to RANSAC where such systems have to be
solved and then tested for inlier hypotheses, our constraints are derived as a
single affinity weight based on $n&gt;2$ hypothesized correspondences without
solving the polynomial system. Since the result is directly a correspondence
without a transformation model, our approach supports correspondence matching
in the presence of multiple geometric transforms like articulated motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6262</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6262</id><created>2014-05-23</created><updated>2014-10-27</updated><authors><author><keyname>Ma</keyname><forenames>Xudong</forenames></author></authors><title>Write-Once-Memory Codes by Source Polarization</title><categories>cs.IT math.IT</categories><comments>5 pages, Proceedings of the International Conference on Computing,
  Networking and Communications (ICNC 2015), Anaheim, California, USA, February
  16-19, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new Write-Once-Memory (WOM) coding scheme based on source
polarization. By applying a source polarization transformation on the
to-be-determined codeword, the proposed WOM coding scheme encodes information
into the bits in the high-entropy set. We prove in this paper that the proposed
WOM codes are capacity-achieving. WOM codes have found many applications in
modern data storage systems, such as flash memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6263</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6263</id><created>2014-05-23</created><authors><author><keyname>Gosavi</keyname><forenames>Hemalata A.</forenames></author><author><keyname>Umale</keyname><forenames>Manish R.</forenames></author></authors><title>Public Auditing and Data Dynamics for Storage Security in Cloud
  Computing</title><categories>cs.CR cs.DC</categories><comments>04 pages, 8 figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;. arXiv admin note: text overlap
  with arXiv:1211.1457 by other authors</comments><journal-ref>IJETT, V11(4), 174-177 May 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V11P235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has been envisioned as the next generation architecture of IT
Enterprise. Using Cloud Storage,users can remotely store their data and enjoy
the on demand high quality applications and services from a shared pool of
configurable computing resources, without the burden local copy data storage
and maintenance. It moves the application of software data stored to the
centralized large data centers, where the management of the data stored
services may not be completely trusted. There are many new security challenges
and the problems taken into account for ensuring the integrity of data storage
in Cloud Computing. In particular, we consider the task of allowing a third
party auditor (TPA) to perform verifies the integrity of the dynamic data
stored in the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6267</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6267</id><created>2014-05-24</created><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author></authors><title>Instantaneous Quantum Channel Estimation during Quantum Information
  Processing</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a nonintrusive method for reliably estimating the noise level
during quantum computation and quantum communication protected by quantum
error-correcting codes. As preprocessing of quantum error correction, our
scheme estimates the current noise level through a negligible amount of
classical computation using error syndromes and updates the decoder's knowledge
on the spot before inferring the locations of errors. This preprocessing
requires no additional quantum interaction or modification in the system. The
estimate can be of higher quality than the maximum likelihood estimate based on
perfect knowledge of channel parameters, thereby eliminating the need of the
unrealistic assumption that the decoder accurately knows channel parameters a
priori. Simulations demonstrate that not only can the decoder pick up on a
change of channel parameters, but even if the channel stays the same, a quantum
low-density parity-check code can perform better when the decoder exploits the
on-the-spot estimates instead of the true error probabilities of the quantum
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6275</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6275</id><created>2014-05-24</created><authors><author><keyname>Liang</keyname><forenames>Dong</forenames></author><author><keyname>Kaneko</keyname><forenames>Shun'ichi</forenames></author></authors><title>Improvements and Experiments of a Compact Statistical Background Model</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change detection plays an important role in most video-based applications.
The first stage is to build appropriate background model, which is now becoming
increasingly complex as more sophisticated statistical approaches are
introduced to cover challenging situations and provide reliable detection. This
paper reports a simple and intuitive statistical model based on deeper learning
spatial correlation among pixels: For each observed pixel, we select a group of
supporting pixels with high correlation, and then use a single Gaussian to
model the intensity deviations between the observed pixel and the supporting
ones. In addition, a multi-channel model updating is integrated on-line and a
temporal intensity constraint for each pixel is defined. Although this method
is mainly designed for coping with sudden illumination changes, experimental
results using all the video sequences provided on changedetection.net validate
it is comparable with other recent methods under various situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6282</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6282</id><created>2014-05-24</created><authors><author><keyname>Wu</keyname><forenames>Daoyuan</forenames></author><author><keyname>Luo</keyname><forenames>Xiapu</forenames></author><author><keyname>Chang</keyname><forenames>Rocky K. C.</forenames></author></authors><title>A Sink-driven Approach to Detecting Exposed Component Vulnerabilities in
  Android Apps</title><categories>cs.CR</categories><comments>This is a technical report from HKPolyU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android apps could expose their components for cooperating with other apps.
This convenience, however, makes apps susceptible to the exposed component
vulnerability (ECV), in which a dangerous API (commonly known as sink) inside
its component can be triggered by other (malicious) apps. In the prior works,
detecting these ECVs use a set of sinks pertaining to the ECVs under detection.
In this paper, we argue that a more comprehensive and effective approach should
start by a systematic selection and classification of vulnerability-specific
sinks (VSinks). The set of VSinks is much larger than those used in the
previous works. Based on these VSinks, our sink-driven approach can detect
different kinds of ECVs in an app in two steps. First, VSinks and their
categories are identified through a typical forward reachability analysis.
Second, based on each VSink's category, a corresponding detection method is
used to identify the ECV via a customized backward dataflow analysis. We also
design a semi-auto guided analysis and validation capability for system-only
broadcast checking to remove some false positives. We implement our sink-driven
approach in a tool called ECVDetector and evaluate it with the top 1K Android
apps. Using ECVDetector we successfully identify a total of 49 vulnerable apps
across all four ECV categories we have defined. To our knowledge, most of them
are previously undisclosed, such as the very popular Go SMS Pro and Clean
Master. Moreover, the performance of ECVDetector is high, requiring only 9.257
seconds on average to process each component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6285</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6285</id><created>2014-05-24</created><authors><author><keyname>Rodrigues</keyname><forenames>David M. S.</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>Traversing News with Ant Colony Optimisation and Negative Pheromones</title><categories>cs.IR cs.NE</categories><comments>accepted as preprint for oral presentation at ECCS'14 in Lucca, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past decade has seen the rapid development of the online newsroom. News
published online are the main outlet of news surpassing traditional printed
newspapers. This poses challenges to the production and to the consumption of
those news. With those many sources of information available it is important to
find ways to cluster and organise the documents if one wants to understand this
new system. A novel bio inspired approach to the problem of traversing the news
is presented. It finds Hamiltonian cycles over documents published by the
newspaper The Guardian. A Second Order Swarm Intelligence algorithm based on
Ant Colony Optimisation was developed that uses a negative pheromone to mark
unrewarding paths with a &quot;no-entry&quot; signal. This approach follows recent
findings of negative pheromone usage in real ants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6286</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6286</id><created>2014-05-24</created><authors><author><keyname>Poularakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Exploiting User Mobility for Wireless Content Delivery</title><categories>cs.IT math.IT</categories><comments>This work has been published in IEEE International Symposium on
  Information Theory (ISIT), 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of storing segments of encoded versions of content
files in a set of base stations located in a communication cell. These base
stations work in conjunction with the main base station of the cell. Users move
randomly across the space based on a discrete-time Markov chain model. At each
time slot each user accesses a single base station based on it's current
position and it can download only a part of the content stored in it, depending
on the time slot duration. We assume that file requests must be satisfied
within a given time deadline in order to be successful. If the amount of the
downloaded (encoded) data by the accessed base stations when the time deadline
expires does not suffice to recover the requested file, the main base station
of the cell serves the request. Our aim is to find the storage allocation that
minimizes the probability of using the main base station for file delivery.
This problem is intractable in general. However, we show that the optimal
solution of the problem can be efficiently attained in case that the time
deadline is small. To tackle the general case, we propose a distributed
approximation algorithm based on large deviation inequalities. Systematic
experiments on a real world data set demonstrate the effectiveness of our
proposed algorithms. Index Terms: Mobility-aware Caching, Markov Chain, MDS
Coding, Small-cell Networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6287</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6287</id><created>2014-05-24</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>\'Etude des dimensions sp\'ecifiques du contexte dans un syst\`eme de
  filtrage d'informations</title><categories>cs.IR</categories><comments>in French</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of business information systems, e-commerce and access to
knowledge, the relevance of the information provided to use is a key fact to
the success of information systems. Therefore the quality of access is
determined by access to the right information at the right time, at the right
place. In this context, it is important to consider the users needs when access
to information and his contextual situation in order to provide relevant
information, tailored to their needs and context use. In what follows we
describe the prelude to a project that tries to combine all of these needs to
improve information systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6293</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6293</id><created>2014-05-24</created><authors><author><keyname>Yousef</keyname><forenames>Ahmed H.</forenames></author></authors><title>Cross-Language Personal Name Mapping</title><categories>cs.CL</categories><journal-ref>International Journal of Computational Linguistics Research, vol
  4, issue 4, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Name matching between multiple natural languages is an important step in
cross-enterprise integration applications and data mining. It is difficult to
decide whether or not two syntactic values (names) from two heterogeneous data
sources are alternative designation of the same semantic entity (person), this
process becomes more difficult with Arabic language due to several factors
including spelling and pronunciation variation, dialects and special vowel and
consonant distinction and other linguistic characteristics. This paper proposes
a new framework for name matching between the Arabic language and other
languages. The framework uses a dictionary based on a new proposed version of
the Soundex algorithm to encapsulate the recognition of special features of
Arabic names. The framework proposes a new proximity matching algorithm to suit
the high importance of order sensitivity in Arabic name matching. New
performance evaluation metrics are proposed as well. The framework is
implemented and verified empirically in several case studies demonstrating
substantial improvements compared to other well-known techniques found in
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6296</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6296</id><created>2014-05-24</created><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Four Classes of Morphogenetic Collective Systems</title><categories>nlin.AO cs.NE</categories><comments>8 pages, 2 figures, 1 table; accepted for publication in ALIFE 14
  proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We studied the roles of morphogenetic principles---heterogeneity of
components, dynamic differentiation/re-differentiation of components, and local
information sharing among components---in the self-organization of
morphogenetic collective systems. By incrementally introducing these principles
to collectives, we defined four distinct classes of morphogenetic collective
systems. Monte Carlo simulations were conducted using an extended version of
the Swarm Chemistry model that was equipped with dynamic
differentiation/re-differentiation and local information sharing capabilities.
Self-organization of swarms was characterized by several kinetic and
topological measurements, the latter of which were facilitated by a newly
developed network-based method. Results of simulations revealed that, while
heterogeneity of components had a strong impact on the structure and behavior
of the swarms, dynamic differentiation/re-differentiation of components and
local information sharing helped the swarms maintain spatially adjacent,
coherent organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6298</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6298</id><created>2014-05-24</created><updated>2014-11-11</updated><authors><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>Differentially positive systems</title><categories>cs.SY math.DS</categories><comments>32 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces and studies differentially positive systems, that is,
systems whose linearization along an arbitrary trajectory is positive. A
generalization of Perron Frobenius theory is developed in this differential
framework to show that the property induces a (conal) order that strongly
constrains the asymptotic behavior of solutions. The results illustrate that
behaviors constrained by local order properties extend beyond the well-studied
class of linear positive systems and monotone systems, which both require a
constant cone field and a linear state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6307</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6307</id><created>2014-05-24</created><updated>2015-03-31</updated><authors><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Wireless Scheduling with Partial Channel State Information: Large
  Deviations and Optimality</title><categories>cs.IT cs.NI math.IT</categories><comments>A shorter version appeared in the proceedings of IEEE INFOCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a server serving a time-slotted queued system of multiple
packet-based flows, with exogenous packet arrivals and time-varying service
rates. At each time, the server can observe instantaneous service rates for
only a subset of flows (from within a fixed collection of observable subsets)
before scheduling a flow in the subset for service. We are interested in
queue-length aware scheduling to keep the queues short, and develop scheduling
algorithms that use only partial service rate information from subsets of
channels to minimize the likelihood of queue overflow in the system.
Specifically, we present a new joint subset-sampling and scheduling algorithm
called Max-Exp that uses only the current queue lengths to pick a subset of
flows, and subsequently schedules a flow using the Exponential rule. When the
collection of observable subsets is disjoint, we show that Max-Exp achieves the
best exponential decay rate, among all scheduling algorithms using partial
information, of the tail of the longest queue in the system. Towards this, we
employ novel analytical techniques for studying the performance of scheduling
algorithms using partial state, which may be of independent interest. These
include new sample-path large deviations results for processes obtained by
non-random, predictable sampling of sequences of independent and identically
distributed random variables. A consequence of these results is that scheduling
with partial state information yields a rate function significantly different
from scheduling with full channel information. In the special case when the
observable subsets are singleton flows, i.e., when there is effectively no a
priori channel-state information, Max-Exp reduces to simply serving the flow
with the longest queue; thus, our results show that to always serve the longest
queue in the absence of any channel-state information is large-deviations
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6317</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6317</id><created>2014-05-24</created><updated>2014-08-14</updated><authors><author><keyname>Wirth</keyname><forenames>Claus-Peter</forenames></author></authors><title>Herbrand's Fundamental Theorem: The Historical Facts and their
  Streamlining</title><categories>math.LO cs.LO</categories><comments>ii + 47 pages</comments><report-no>SEKI Report SR-2014-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Heijenoort's unpublished generalized rules of quantification, we
discuss the proof of Herbrand's Fundamental Theorem in the form of Heijenoort's
correction of Herbrand's &quot;False Lemma&quot; and present a didactic example. Although
we are mainly concerned with the inner structure of Herbrand's Fundamental
Theorem and the questions of its quality and its depth, we also discuss the
outer questions of its historical context and why Bernays called it &quot;the
central theorem of predicate logic&quot; and considered the form of its expression
to be &quot;concise and felicitous&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6322</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6322</id><created>2014-05-24</created><authors><author><keyname>Krishnan</keyname><forenames>Nikhil</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>M\ih&#xe7;ak</keyname><forenames>Mehmet K\ivan&#xe7;</forenames></author></authors><title>A Parallel Two-Pass MDL Context Tree Algorithm for Universal Source
  Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel lossless universal source coding algorithm that uses
parallel computational units to increase the throughput. The length-$N$ input
sequence is partitioned into $B$ blocks. Processing each block independently of
the other blocks can accelerate the computation by a factor of $B$, but
degrades the compression quality. Instead, our approach is to first estimate
the minimum description length (MDL) source underlying the entire input, and
then encode each of the $B$ blocks in parallel based on the MDL source. With
this two-pass approach, the compression loss incurred by using more parallel
units is insignificant. Our algorithm is work-efficient, i.e., its
computational complexity is $O(N/B)$. Its redundancy is approximately
$B\log(N/B)$ bits above Rissanen's lower bound on universal coding performance,
with respect to any tree source whose maximal depth is at most $\log(N/B)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6326</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6326</id><created>2014-05-24</created><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author></authors><title>Second Life: game, simulator, or serious game?</title><categories>cs.CY</categories><comments>20 pages</comments><acm-class>H.5.1; I.6.8</acm-class><journal-ref>Acta Scientiae, 16(1):72-92 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reports on an exploratory case study conducted to examine the
viability of Second Life (SL) as an environment for physical simulations and
microworlds. It begins by discussing specific features of the SL environment
relevant to its use as a support for microworlds and simulations as well as a
few differences found between SL and traditional simulators such as Modellus,
along with their implications to simulations, as a support for subsequent
analysis. Afterwards, we will use Narayanasamy et al. and Johnston and
Whitehead criteria to analyze the SL environment and determine into which of
training simulators, games, simulation games, or serious games categories SL
fits best. We conclude that SL shows itself as a huge and sophisticated
simulator of an entire Earthlike world used by thousands of users to simulate
real life in some sense and a viable and flexible platform for microworlds and
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6327</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6327</id><created>2014-05-24</created><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author></authors><title>Second Life Physics: Virtual, real, or surreal?</title><categories>physics.ed-ph cs.CY</categories><comments>21 pages</comments><acm-class>K.3.1; I.6.3; H.5.1; J.2</acm-class><journal-ref>Journal of Virtual Worlds Research, 2(1) (2009) 1-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Science teaching detached itself from reality and became restricted to the
classrooms and textbooks with their overreliance on standardized and repetitive
exercises, while students keep their own alternative conceptions. Papert,
displeased with this inefficient learning process, championed physics
microworlds, where students could experience a variety of laws of motion, from
Aristotle to Newton and Einstein or even new laws invented by the students
themselves. While often mistakenly seen as a game, Second Life (SL), the online
3-D virtual world hosted by Linden Lab, imposes essentially no rules on the
residents beyond reasonable restrictions on improper behavior and the physical
rules that guarantee its similitude to the real world. As a consequence, SL
qualifies itself as an environment for personal discovery and exploration as
proposed by constructivist theories. The physical laws are implemented through
the well-known physics engine Havok, whose design aims to provide game-players
a consistent, realistic environment. The Havok User Guide (2008) explicitly
encourages developers to use several tricks to cheat the simulator in order to
make games funnier or easier to play. As it is shown in this study, SL physics
is unexpectedly neither the Newtonian idealized physics nor a real world
physics virtualization, intentionally diverging from reality in such a way that
it could be called hyper-real. As a matter of fact, if some of its features
make objects behave more realistically than real ones, certain quantities like
energy have a totally different meaning in SL as compared to physics. Far from
considering it as a problem, however, the author argues that its hyper-reality
may be a golden teaching opportunity, allowing surreal physics simulations and
epistemologically rich classroom discussions around the what is a physical law?
issue, in accordance with Papert's never-implemented proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6328</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6328</id><created>2014-05-24</created><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author></authors><title>Big Data as a Mediator in Science Teaching: A Proposal</title><categories>physics.ed-ph cs.CY cs.DB</categories><comments>13 pages, 2 figures</comments><acm-class>K.3.1; J.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in a digital world that, in 2010, crossed the mark of one zettabyte
data. This huge amount of data processed on computers extremely fast with
optimized techniques allows one to find insights in new and emerging types of
data and content and to answer questions that were previously considered beyond
reach. This is the idea of Big Data. Google now offers the Google Correlate
analysis public tool that, from a search term or a series of temporal or
regional data, provides a list of queries on Google whose frequencies follow
patterns that best correlate with the data, according to the Pearson
determination coefficient R2. Of course, correlation does not imply causation.
We believe, however, that there is potential for these big data tools to find
unexpected correlations that may serve as clues to interesting phenomena, from
the pedagogical and even scientific point of view. As far as we know, this is
the first proposal for the use of Big Data in Science Teaching, of
constructionist character, taking as mediators the computer and the public and
free tools such as Google Correlate. It also has an epistemological bias, not
being merely a training in computational infrastructure or predictive
analytics, but aiming at providing students a better understanding of physical
concepts, such as phenomena, observation, measurement, physical laws, theory,
and causality. With it, they would be able to become good Big Data specialists,
the so needed 'data scientists' to solve the challenges of Big Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6331</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6331</id><created>2014-05-24</created><updated>2015-08-31</updated><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Interaction Graphs: Graphings</title><categories>cs.LO math.LO</categories><msc-class>03B70, 03F52, 28E15</msc-class><acm-class>F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two previous papers, we exposed a combinatorial approach to the program of
Geometry of Interaction, a program initiated by Jean-Yves Girard. The strength
of our approach lies in the fact that we interpret proofs by simpler structures
- graphs - than Girard's constructions, while generalizing the latter since
they can be recovered as special cases of our setting. This third paper extends
this approach by considering a generalization of graphs named graphings, which
is in some way a geometric realization of a graph. This very general framework
leads to a number of new models of multiplicative-additive linear logic which
generalize Girard's geometry of interaction models and opens several new lines
of research. As an example, we exhibit a family of such models which account
for second-order quantification without suffering the same limitations as
Girard's models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6334</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6334</id><created>2014-05-24</created><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author></authors><title>TATI -- A Logo-like interface for microworlds and simulations for
  physics teaching in Second Life</title><categories>physics.ed-ph cs.CY cs.HC cs.PL</categories><comments>12 pages, 4 figures, Proceedings of ESERA 2013 - 10th biannual
  Conference of the European Science Education Research Association, September
  2nd - 7th 2013, Nicosia, Cyprus. Nicosia: University of Cyprus, 2013</comments><acm-class>K.3.1; J.2; I.6.8; H.5.2; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Student difficulties in learning Physics have been thoroughly discussed in
the scientific literature. Already in 1980, Papert complained that schools
teach Newtonian motion by manipulating equations rather than by manipulating
the Newtonian objects themselves, what would be possible in a 'physics
microworld'. On the other hand, Second Life and its scripting language have a
remarkable learning curve that discourages most teachers at using it as an
environment for educational computer simulations and microworlds. The objective
of this work is to describe TATI, a textual interface which, through TATILogo,
an accessible Logo language extension, allows the generation of various physics
microworlds in Second Life, containing different types of objects that follow
different physical laws, providing a learning path into Newtonian Physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6335</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6335</id><created>2014-05-24</created><authors><author><keyname>Khan</keyname><forenames>Parvez Mahmood</forenames></author><author><keyname>Beg</keyname><forenames>M. M. Sufyan</forenames></author></authors><title>Application of Sizing Estimation Techniques for Business Critical
  Software Project Management</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation is one of the most critical areas in software project management
life cycle, which is still evolving and less matured as compared to many other
industries like construction, manufacturing etc. Originally the word
estimation, in the context of software projects use to refer to cost and
duration estimates only with software-size almost always assumed to be a fixed
input. Continued legacy of bad estimates has compelled researchers,
practitioners and business organizations to draw their attention towards
another dimension of the problem and seriously validate an additional
component, viz. size estimation. Recent studies have shown that size is the
principal determinant of cost, and therefore an accurate size estimate is
crucial to good cost estimation. Improving the accuracy of size estimates is,
therefore, instrumental in improving the accuracy of cost and schedule
estimates. Moreover, software size and cost estimates have the highest utility
at the time of project inception, when most important decisions (e.g. budget
allocation, personnel allocation, etc). are taken. The dilemma, however, is
that only high-level requirements for a project are available at this stage.
Leveraging this high-level information to produce an accurate estimate of
software size is an extremely challenging and high risk task. This study
acknowledges the presence and effect of risk in any software estimate and
offers pragmatic strategies for risk mitigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6341</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6341</id><created>2014-05-24</created><authors><author><keyname>Nikolaidis</keyname><forenames>Stefanos</forenames></author><author><keyname>Gu</keyname><forenames>Keren</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Ramya</forenames></author><author><keyname>Shah</keyname><forenames>Julie</forenames></author></authors><title>Efficient Model Learning for Human-Robot Collaborative Tasks</title><categories>cs.RO cs.AI cs.LG cs.SY</categories><acm-class>I.2.6; I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for learning human user models from joint-action
demonstrations that enables the robot to compute a robust policy for a
collaborative task with a human. The learning takes place completely
automatically, without any human intervention. First, we describe the
clustering of demonstrated action sequences into different human types using an
unsupervised learning algorithm. These demonstrated sequences are also used by
the robot to learn a reward function that is representative for each type,
through the employment of an inverse reinforcement learning algorithm. The
learned model is then used as part of a Mixed Observability Markov Decision
Process formulation, wherein the human type is a partially observable variable.
With this framework, we can infer, either offline or online, the human type of
a new user that was not included in the training set, and can compute a policy
for the robot that will be aligned to the preference of this new user and will
be robust to deviations of the human actions from prior demonstrations. Finally
we validate the approach using data collected in human subject experiments, and
conduct proof-of-concept demonstrations in which a person performs a
collaborative task with a small industrial robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6347</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6347</id><created>2014-05-24</created><updated>2015-06-22</updated><authors><author><keyname>Kaftan</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>A more efficient way of finding Hamiltonian cycle</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to find Hamiltonian cycle, algorithm should find edges that creates
a Hamiltonian cycle. Higher number of edges creates more possibilities to check
to solve the problem. Algorithm rests on analysis of original graph and
opposite graph to it. Algorithm can remove unnecessary edges from graph and
test when Hamiltonian cycle can't exist in graph. Algorithm prefers &quot;to think
over&quot; which paths should be checked than check many wrong paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6353</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6353</id><created>2014-05-24</created><authors><author><keyname>Noorshams</keyname><forenames>Nima</forenames></author><author><keyname>Iyengar</keyname><forenames>Aravind</forenames></author></authors><title>A Novel Stochastic Decoding of LDPC Codes with Quantitative Guarantees</title><categories>cs.IT math.IT stat.ML</categories><comments>This paper has been submitted to IEEE Transactions on Information
  Theory on May 24th 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check codes, a class of capacity-approaching linear codes,
are particularly recognized for their efficient decoding scheme. The decoding
scheme, known as the sum-product, is an iterative algorithm consisting of
passing messages between variable and check nodes of the factor graph. The
sum-product algorithm is fully parallelizable, owing to the fact that all
messages can be update concurrently. However, since it requires extensive
number of highly interconnected wires, the fully-parallel implementation of the
sum-product on chips is exceedingly challenging. Stochastic decoding
algorithms, which exchange binary messages, are of great interest for
mitigating this challenge and have been the focus of extensive research over
the past decade. They significantly reduce the required wiring and
computational complexity of the message-passing algorithm. Even though
stochastic decoders have been shown extremely effective in practice, the
theoretical aspect and understanding of such algorithms remains limited at
large. Our main objective in this paper is to address this issue. We first
propose a novel algorithm referred to as the Markov based stochastic decoding.
Then, we provide concrete quantitative guarantees on its performance for
tree-structured as well as general factor graphs. More specifically, we provide
upper-bounds on the first and second moments of the error, illustrating that
the proposed algorithm is an asymptotically consistent estimate of the
sum-product algorithm. We also validate our theoretical predictions with
experimental results, showing we achieve comparable performance to other
practical stochastic decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6360</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6360</id><created>2014-05-25</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Hassan</keyname><forenames>Naveed Ul</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author></authors><title>Design of A Scalable Hybrid MAC Protocol for Heterogeneous M2M Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust and resilient Medium Access Control (MAC) protocol is crucial for
numerous machine-type devices to concurrently access the channel in a
Machine-to-Machine (M2M) network. Simplex (reservation or contention based) MAC
protocols are studied in most literatures which may not be able to provide a
scalable solution for M2M networks with large number of heterogeneous devices.
In this paper, a scalable hybrid MAC protocol, which consists of a contention
period and a transmission period, is designed for heterogeneous M2M networks.
In this protocol, different devices with pre-set priorities (hierarchical
contending probabilities) firstly contend the transmission opportunities
following the convention based $p$-persistent CSMA mechanism. Only the
successful devices will be assigned a time slot for transmission following the
reservation based TDMA mechanism. If the devices failed in contention at
previous frame, to ensure the fairness among all devices, their contending
priorities will be raised by increasing their contending probabilities at the
next frame. To balance the tradeoff between the contention and transmission
period in each frame, an optimization problem is formulated to maximize the
channel utility by finding the key design parameters: the contention duration,
initial contending probability and the incremental indicator. Analytical and
simulation results demonstrate the effectiveness of the proposed Hybrid MAC
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6362</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6362</id><created>2014-05-25</created><authors><author><keyname>Ibeid</keyname><forenames>Huda</forenames></author><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>A Performance Model for the Communication in Fast Multipole Methods on
  HPC Platforms</title><categories>cs.DC</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exascale systems are predicted to have approximately one billion cores,
assuming Gigahertz cores. Limitations on affordable network topologies for
distributed memory systems of such massive scale bring new challenges to the
current parallel programing model. Currently, there are many efforts to
evaluate the hardware and software bottlenecks of exascale designs. There is
therefore an urgent need to model application performance and to understand
what changes need to be made to ensure extrapolated scalability. The fast
multipole method (FMM) was originally developed for accelerating N-body
problems in astrophysics and molecular dynamics, but has recently been extended
to a wider range of problems, including preconditioners for sparse linear
solvers. It's high arithmetic intensity combined with its linear complexity and
asynchronous communication patterns makes it a promising algorithm for exascale
systems. In this paper, we discuss the challenges for FMM on current parallel
computers and future exascale architectures, with a focus on inter-node
communication. We develop a performance model that considers the communication
patterns of the FMM, and observe a good match between our model and the actual
communication time, when latency, bandwidth, network topology, and multi-core
penalties are all taken into account. To our knowledge, this is the first
formal characterization of inter-node communication in FMM, which validates the
model against actual measurements of communication time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6369</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6369</id><created>2014-05-25</created><authors><author><keyname>Ruijl</keyname><forenames>Ben</forenames></author><author><keyname>Vermaseren</keyname><forenames>Jos</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author></authors><title>HEPGAME and the Simplification of Expressions</title><categories>cs.AI</categories><comments>Keynote at the 11th International Workshop on Boolean Problems,
  Freiberg Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in high energy physics have created the need to increase
computational capacity. Project HEPGAME was composed to address this challenge.
One of the issues is that numerical integration of expressions of current
interest have millions of terms and takes weeks to compute. We have
investigated ways to simplify these expressions, using Horner schemes and
common subexpression elimination. Our approach applies MCTS, a search procedure
that has been successful in AI. We use it to find near-optimal Horner schemes.
Although MCTS finds better solutions, this approach gives rise to two further
challenges. (1) MCTS (with UCT) introduces a constant, $C_p$ that governs the
balance between exploration and exploitation. This constant has to be tuned
manually. (2) There should be more guided exploration at the bottom of the
tree, since the current approach reduces the quality of the solution towards
the end of the expression. We investigate NMCS (Nested Monte Carlo Search) to
address both issues, but find that NMCS is computationally unfeasible for our
problem. Then, we modify the MCTS formula by introducing a dynamic
exploration-exploitation parameter $T$ that decreases linearly with the
iteration number. Consequently, we provide a performance analysis. We observe
that a variable $C_p$ solves our domain: it yields more exploration at the
bottom and as a result the tuning problem has been simplified. The region in
$C_p$ for which good values are found is increased by more than a tenfold. This
result encourages us to continue our research to solve other prominent problems
in High Energy Physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6380</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6380</id><created>2014-05-25</created><updated>2015-05-27</updated><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>van Oostrom</keyname><forenames>Vincent</forenames><affiliation>Utrecht University</affiliation></author></authors><title>Nested Term Graphs (Work In Progress)</title><categories>cs.LO</categories><comments>In Proceedings TERMGRAPH 2014, arXiv:1505.06818</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 183, 2015, pp. 48-65</journal-ref><doi>10.4204/EPTCS.183.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on work in progress on 'nested term graphs' for formalizing
higher-order terms (e.g. finite or infinite lambda-terms), including those
expressing recursion (e.g. terms in the lambda-calculus with letrec). The idea
is to represent the nested scope structure of a higher-order term by a nested
structure of term graphs.
  Based on a signature that is partitioned into atomic and nested function
symbols, we define nested term graphs both in a functional representation, as
tree-like recursive graph specifications that associate nested symbols with
usual term graphs, and in a structural representation, as enriched term graph
structures. These definitions induce corresponding notions of bisimulation
between nested term graphs. Our main result states that nested term graphs can
be implemented faithfully by first-order term graphs.
  keywords: higher-order term graphs, context-free grammars, cyclic
lambda-terms, higher-order rewrite systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6397</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6397</id><created>2014-05-25</created><authors><author><keyname>Kurz</keyname><forenames>Gerhard</forenames></author><author><keyname>Gilitschenski</keyname><forenames>Igor</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Efficient Evaluation of the Probability Density Function of a Wrapped
  Normal Distribution</title><categories>stat.CO cs.SY math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wrapped normal distribution arises when a the density of a
one-dimensional normal distribution is wrapped around the circle infinitely
many times. At first look, evaluation of its probability density function
appears tedious as an infinite series is involved. In this paper, we
investigate the evaluation of two truncated series representations. As one
representation performs well for small uncertainties whereas the other performs
well for large uncertainties, we show that in all cases a small number of
summands is sufficient to achieve high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6399</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6399</id><created>2014-05-25</created><updated>2014-06-17</updated><authors><author><keyname>Stegmann</keyname><forenames>Johannes</forenames></author></authors><title>Research at UNIS - The University Centre in Svalbard. A bibliometric
  study</title><categories>cs.DL</categories><comments>17 pages, 3 figures, 10 tables corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific output 1994-2014 of the University Centre in Svalbard (UNIS)
was bibliometrically analysed. It was found that the majority of the papers
have been published as international cooperations and rank above world average.
Analysis of the content of the papers reveals that UNIS works and publishes in
a wide variety of scientific topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6400</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6400</id><created>2014-05-25</created><updated>2015-06-18</updated><authors><author><keyname>Jackson</keyname><forenames>Matthew O.</forenames></author><author><keyname>Nei</keyname><forenames>Stephen M.</forenames></author></authors><title>Networks of Military Alliances, Wars, and International Trade</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>39 pages, 14 figures</comments><msc-class>99C99</msc-class><doi>10.1073/pnas.1520970112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the role of networks of alliances in preventing (multilateral)
interstate wars. We first show that, in the absence of international trade, no
network of alliances is peaceful and stable. We then show that international
trade induces peaceful and stable networks: trade increases the density of
alliances so that countries are less vulnerable to attack and also reduces
countries' incentives to attack an ally. We present historical data on wars and
trade, noting that the dramatic drop in interstate wars since 1950, and
accompanying densification and stabilization of alliances, are consistent with
the model but not other prominent theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6408</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6408</id><created>2014-05-25</created><updated>2015-04-23</updated><authors><author><keyname>Morales-Jimenez</keyname><forenames>David</forenames></author><author><keyname>Louie</keyname><forenames>Raymond H. Y.</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author></authors><title>Analysis and Design of Multiple-Antenna Cognitive Radios with Multiple
  Primary User Signals</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>Revised version (14 pages). Change in title</comments><doi>10.1109/TSP.2015.2448528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multiple-antenna signal detection of primary user transmission
signals by a secondary user receiver in cognitive radio networks. The optimal
detector is analyzed for the scenario where the number of primary user signals
is no less than the number of receive antennas at the secondary user. We first
derive exact expressions for the moments of the generalized likelihood ratio
test (GLRT) statistic, yielding approximations for the false alarm and
detection probabilities. We then show that the normalized GLRT statistic
converges in distribution to a Gaussian random variable when the number of
antennas and observations grow large at the same rate. Further, using results
from large random matrix theory, we derive expressions to compute the detection
probability without explicit knowledge of the channel, and then particularize
these expressions for two scenarios of practical interest: 1) a single primary
user sending spatially multiplexed signals, and 2) multiple spatially
distributed primary users. Our analytical results are finally used to obtain
simple design rules for the signal detection threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6415</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6415</id><created>2014-05-25</created><authors><author><keyname>J</keyname><forenames>Jeya Pradha</forenames></author><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Energy Harvesting Cognitive Radio with Channel-Aware Sensing Strategy</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in IEEE Communications Letters</comments><doi>10.1109/LCOMM.2014.2323240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An energy harvesting cognitive radio scenario is considered where a secondary
user (SU) with finite battery capacity opportunistically accesses the primary
user (PU) channels. The objective is to maximize the throughput of SU under
energy neutrality constraint and fading channel conditions in a single-user
multi-channel setting. Channel selection criterion based on the probabilistic
availability of energy with SU, channel conditions, and primary network's
belief state is proposed, which chooses the best subset of channels for
sensing, yielding higher throughput. We construct channel-aware optimal and
myopic sensing strategies in a Partially Observable Markov Decision Process
framework based on the proposed channel selection criterion. The effects of
sensing errors and collisions between PU and SU on the throughput of latter are
studied. It is shown that there exists a trade-off between the transmission
duration and the energy lost in collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6426</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6426</id><created>2014-05-25</created><updated>2014-09-11</updated><authors><author><keyname>Qi</keyname><forenames>Junjian</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author></authors><title>Power System Dynamic State Estimation by Unscented Kalman Filter with
  Guaranteed Positive Semidefinite State Covariance</title><categories>cs.IT math.IT math.OC</categories><comments>Submitted to IEEE Power Engineering Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an unscented Kalman filter with guaranteed positive
semidefinite state covariance is proposed by calculating the nearest symmetric
positive definite matrix in Frobenius norm and is applied to power system
dynamic state estimation. The proposed method is tested on NPCC 48-machine
140-bus system and the results validate its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6432</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6432</id><created>2014-05-25</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>Exact values for the Grundy number of some graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Grundy number of a graph G is the maximum number k of colors used to
color the vertices of G such that the coloring is proper and every vertex x
colored with color i, is adjacent to (i - 1) vertices colored with each color
j, In this paper we give bounds for the Grundy number of some graphs and
Cartesian products of graphs. In particular, we determine an exact value of
this parameter for n-dimensional meshes and some n-dimensional toroidal meshes.
Finally, we present an algorithm to generate all graphs for a given Grundy
number
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6433</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6433</id><created>2014-05-25</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>Results for grundy number of the complement of bipartite graphs</title><categories>cs.DM</categories><journal-ref>Ubiquitous Computing and Communication Journal Special Issue for
  The International Conference on Computing, Communications and Information
  Technology Applications (CCITA-2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Grundy k-coloring of a graph G, is a vertex k-coloring of G such that for
each two colors i and j with i &lt; j, every vertex of G colored by j has a
neighbor with color i. The Grundy chromatic number (G), is the largest integer
k for which there exists a Grundy k-coloring for G. In this note we first give
an interpretation of (G) in terms of the total graph of G, when G is the
complement of a bipartite graph. Then we prove that determining the Grundy
number of the complement of bipartite graphs is an NP-Complete problem
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6434</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6434</id><created>2014-05-25</created><updated>2015-11-25</updated><authors><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Wang</keyname><forenames>Lingbo</forenames></author><author><keyname>Guo</keyname><forenames>Yanwen</forenames></author></authors><title>Multi-view Metric Learning for Multi-view Video Summarization</title><categories>cs.CV cs.LG cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional methods on video summarization are designed to generate summaries
for single-view video records; and thus they cannot fully exploit the
redundancy in multi-view video records. In this paper, we present a multi-view
metric learning framework for multi-view video summarization that combines the
advantages of maximum margin clustering with the disagreement minimization
criterion. The learning framework thus has the ability to find a metric that
best separates the data, and meanwhile to force the learned metric to maintain
original intrinsic information between data points, for example geometric
information. Facilitated by such a framework, a systematic solution to the
multi-view video summarization problem is developed. To the best of our
knowledge, it is the first time to address multi-view video summarization from
the viewpoint of metric learning. The effectiveness of the proposed method is
demonstrated by experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6440</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6440</id><created>2014-05-25</created><updated>2015-10-05</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Optimal Downlink Power Allocation in Cellular Networks</title><categories>cs.NI</categories><comments>ELSEVIER Physical Communication July 2015</comments><doi>10.1016/j.phycom.2015.07.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel approach for power allocation in cellular
networks. In our model, we use sigmoidal-like utility functions to represent
different users' modulation schemes. Each utility function is a representation
of the probability of successfully transmitted packets per unit of power
consumed by a user, when using a certain modulation scheme. We consider power
allocation with utility proportional fairness policy, where the fairness among
users is in utility percentage i.e. percentage of successfully transmitted
packets of the corresponding modulation scheme. We formulate our network
utility maximization problem as a product of utilities of all users and prove
that our power allocation optimization problem is convex and therefore the
optimal solution is tractable. We present a distributed algorithm to allocate
base station (BS) powers optimally with priority given to users running lower
modulation schemes while ensuring non-zero power allocation to users running
higher modulation schemes. Our algorithm prevents fluctuation in the power
allocation process and is capable of traffic and modulation dependent pricing
i.e. charges different price per unit power from different users depending in
part on their modulation scheme and total power available at the BS. This is
used to flatten traffic and decrease the service price for users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6444</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6444</id><created>2014-05-25</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>The role of dimensionality reduction in linear classification</title><categories>cs.LG math.OC stat.ML</categories><comments>15 pages, 6 figures. A shorter version appears in AAAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction (DR) is often used as a preprocessing step in
classification, but usually one first fixes the DR mapping, possibly using
label information, and then learns a classifier (a filter approach). Best
performance would be obtained by optimizing the classification error jointly
over DR mapping and classifier (a wrapper approach), but this is a difficult
nonconvex problem, particularly with nonlinear DR. Using the method of
auxiliary coordinates, we give a simple, efficient algorithm to train a
combination of nonlinear DR and a classifier, and apply it to a RBF mapping
with a linear SVM. This alternates steps where we train the RBF mapping and a
linear SVM as usual regression and classification, respectively, with a
closed-form step that coordinates both. The resulting nonlinear low-dimensional
classifier achieves classification errors competitive with the state-of-the-art
but is fast at training and testing, and allows the user to trade off runtime
for classification accuracy easily. We then study the role of nonlinear DR in
linear classification, and the interplay between the DR mapping, the number of
latent dimensions and the number of classes. When trained jointly, the DR
mapping takes an extreme role in eliminating variation: it tends to collapse
classes in latent space, erasing all manifold structure, and lay out class
centroids so they are linearly separable with maximum margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6448</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6448</id><created>2014-05-25</created><updated>2015-10-05</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>An Optimal Resource Allocation with Joint Carrier Aggregation in 4G-LTE</title><categories>cs.NI</categories><comments>Computing, Networking and Communications (ICNC), 2015 International
  Conference on</comments><doi>10.1109/ICCNC.2015.7069330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel approach for optimal resource allocation
from multiple carriers for users with elastic and inelastic traffic in fourth
generation long term evolution (4G-LTE) system. In our model, we use
logarithmic and sigmoidal-like utility functions to represent the user
applications running on different user equipments (UE)s. We use utility
proportional fairness policy, where the fairness among users is in utility
percentage of the application running on the mobile station. Our objective is
to allocate the resources to the users optimally from multiple carriers. In
addition, every user subscribing for the mobile service is guaranteed to have a
minimum quality-of-service (QoS) with a priority criterion. Our rate allocation
algorithm selects the carrier or multiple carriers that provide the minimum
price for the needed resources. We prove that the novel resource allocation
optimization problem with joint carrier aggregation is convex and therefore the
optimal solution is tractable. We present a distributed algorithm to allocate
the resources optimally from multiple evolved NodeBs (eNodeB)s. Finally, we
present simulation results for the performance of our rate allocation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6450</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6450</id><created>2014-05-25</created><authors><author><keyname>Yeo</keyname><forenames>Jeongho</forenames></author><author><keyname>Cho</keyname><forenames>Joon Ho</forenames></author><author><keyname>Lehnert</keyname><forenames>James S.</forenames></author></authors><title>Joint Transmitter and Receiver Optimization for Improper-Complex
  Second-Order Stationary Data Sequence</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the transmission of an improper-complex second-order
stationary data sequence is considered over a strictly band-limited
frequency-selective channel. It is assumed that the transmitter employs linear
modulation and that the channel output is corrupted by additive proper-complex
cyclostationary noise. Under the average transmit power constraint, the problem
of minimizing the mean-squared error at the output of a widely linear receiver
is formulated in the time domain to find the optimal transmit and receive
waveforms. The optimization problem is converted into a frequency-domain
problem by using the vectorized Fourier transform technique and put into the
form of a double minimization. First, the widely linear receiver is optimized
that requires, unlike the linear receiver design with only one waveform, the
design of two receive waveforms. Then, the optimal transmit waveform for the
linear modulator is derived by introducing the notion of the impropriety
frequency function of a discrete-time random process and by performing a line
search combined with an iterative algorithm. The optimal solution shows that
both the periodic spectral correlation due to the cyclostationarity and the
symmetric spectral correlation about the origin due to the impropriety are well
exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6467</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6467</id><created>2014-05-26</created><authors><author><keyname>Mallada</keyname><forenames>Enrique</forenames></author><author><keyname>Freeman</keyname><forenames>Randy A.</forenames></author><author><keyname>Tang</keyname><forenames>Ao</forenames></author></authors><title>Distributed Synchronization of Heterogeneous Oscillators on Networks
  with Arbitrary Topology</title><categories>math.OC cs.SY math-ph math.DS math.MP</categories><comments>under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many network applications rely on the synchronization of coupled oscillators.
For example, such synchronization can provide networked devices with a common
temporal reference necessary for coordinating actions or decoding transmitted
messages. In this paper, we study the problem of using distributed control to
achieve both phase and frequency synchronization of a network of coupled
heterogeneous nonlinear oscillators. Not only do our controllers guarantee zero
phase error in steady state under arbitrary frequency heterogeneity, but they
also require little knowledge of the oscillator nonlinearities and network
topology. Furthermore, we provide a global convergence analysis, in the absence
of noise and propagation delay, for the resulting nonlinear system whose phase
vector evolves on the n-torus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6472</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6472</id><created>2014-05-26</created><authors><author><keyname>Chen</keyname><forenames>Yuansi</forenames><affiliation>EECS, INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author></authors><title>Fast and Robust Archetypal Analysis for Representation Learning</title><categories>cs.CV cs.LG stat.ML</categories><proxy>ccsd</proxy><journal-ref>CVPR 2014 - IEEE Conference on Computer Vision \&amp; Pattern
  Recognition (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a pioneer unsupervised learning technique called archetypal
analysis, which is related to successful data analysis methods such as sparse
coding and non-negative matrix factorization. Since it was proposed, archetypal
analysis did not gain a lot of popularity even though it produces more
interpretable models than other alternatives. Because no efficient
implementation has ever been made publicly available, its application to
important scientific problems may have been severely limited. Our goal is to
bring back into favour archetypal analysis. We propose a fast optimization
scheme using an active-set strategy, and provide an efficient open-source
implementation interfaced with Matlab, R, and Python. Then, we demonstrate the
usefulness of archetypal analysis for computer vision tasks, such as codebook
learning, signal classification, and large image collection visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6477</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6477</id><created>2014-05-26</created><updated>2014-07-28</updated><authors><author><keyname>Mallada</keyname><forenames>Enrique</forenames></author><author><keyname>Meng</keyname><forenames>Xiaoqiao</forenames></author><author><keyname>Hack</keyname><forenames>Michel</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Ao</forenames></author></authors><title>Skewless Network Clock Synchronization Without Discontinuity:
  Convergence and Performance</title><categories>math.OC cs.DC cs.NI cs.SY</categories><comments>to appear in ToN. arXiv admin note: substantial text overlap with
  arXiv:1208.5703</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines synchronization of computer clocks connected via a data
network and proposes a skewless algorithm to synchronize them. Unlike existing
solutions, which either estimate and compensate the frequency difference (skew)
among clocks or introduce offset corrections that can generate jitter and
possibly even backward jumps, our solution achieves synchronization without
these problems. We first analyze the convergence property of the algorithm and
provide explicit necessary and sufficient conditions on the parameters to
guarantee synchronization. We then study the effect of noisy measurements
(jitter) and frequency drift (wander) on the offsets and synchronization
frequency, and further optimize the parameter values to minimize their
variance. Our study reveals a few insights, for example, we show that our
algorithm can converge even in the presence of timing loops and noise, provided
that there is a well defined leader. This marks a clear contrast with current
standards such as NTP and PTP, where timing loops are specifically avoided.
Furthermore, timing loops can even be beneficial in our scheme as it is
demonstrated that highly connected subnetworks can collectively outperform
individual clients when the time source has large jitter. The results are
supported by experiments running on a cluster of IBM BladeCenter servers with
Linux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6483</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6483</id><created>2014-05-26</created><updated>2014-06-16</updated><authors><author><keyname>Yi</keyname><forenames>Wentan</forenames></author><author><keyname>Chen</keyname><forenames>Shaozhen</forenames></author></authors><title>Integral Cryptanalysis of the Block Cipher E2</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block cipher E2, designed and submitted by Nippon Telegraph and Telephone
Corporation, is a first-round Advanced Encryption Standard candidate. It
employs a Feistel structure as global structure and two-layer
substitution-permutation network structure in round function with initial
transformation IT function before the first round and final transformation FT
function after the last round. The design principles influences several more
recent block ciphers including Camellia, an ISO/IEC standard cipher. In this
paper, we focus on the key-recovery attacks on reduced-round E2-128/192 taking
both IT and FT functions in consideration with integral cryptanalysis. We first
improve the relations between zero-correlation linear approximations and
integral distinguishers, and then deduce some integral distinguishers from
zero-correlation linear approximations over 6 rounds of E2. Furthermore, we
apply these integral distinguishers to break 6-round E2-128 with 2^{120} known
plaintexts (KPs), 2^{115.4} encryptions and 2^{28} bytes memory. In addition,
the attack on 7-round E2-192 requires 2^{120} KPs, 2^{167.2} encryptions and
2^{60} bytes memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6490</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6490</id><created>2014-05-26</created><authors><author><keyname>Singh</keyname><forenames>Bohar</forenames></author><author><keyname>Luthra</keyname><forenames>Pawan</forenames></author></authors><title>Review of Linpack and Cloudsim on VMM</title><categories>cs.DC</categories><comments>5 Pages</comments><doi>10.14445/22315381/IJETT-V11P251</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization is a framework of dividing the resources of a computer into
multiple execution environments which offers a lot of benefits including
flexibility, security, ease to configuration and reduction of cost but at the
same time it also brings a certain degree of performance overhead. Furthermore,
Virtual Machine Monitor (VMM) is the core component of virtual machine (VM)
system and its effectiveness greatly impacts the performance of the whole
system. This review paper will try to describe the basic knowledge about
various virtual machine monitors such as VMware and VirtualBox. It also
discussed and explores the benchmark LINPACK and CloudSim available for cloud
computing. This benchmark and CloudSim can be used to measure the performance
of two different virtual machine monitors in terms of processing speed, time,
bandwidth, quality and response of the cloud computing network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6500</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6500</id><created>2014-05-26</created><updated>2014-06-09</updated><authors><author><keyname>Gai</keyname><forenames>Lei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Zhichao</forenames></author><author><keyname>Qiu</keyname><forenames>Changhe</forenames></author><author><keyname>Wang</keyname><forenames>Tengjiao</forenames></author></authors><title>Towards Efficient Path Query on Social Network with Hybrid RDF
  Management</title><categories>cs.DB cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scalability and exibility of Resource Description Framework(RDF) model
make it ideally suited for representing online social networks(OSN). One basic
operation in OSN is to ?nd chains of relations,such as k-Hop friends. Property
path query in SPARQL can express this type of operation, but its implementation
su?ers from performance problem considering the ever growing data size and
complexity of OSN.In this paper, we present a main memory/disk based hybrid RDF
data management framework for efficient property path query. In this hybrid
framework, we realize an efficient in-memory algebra operator for property path
query using graph traversal, and estimate the cost of this operator to
cooperate with existing cost-based optimization. Experiments on benchmark and
real dataset demonstrated that our approach can achieve a good tradeoff between
data load expense and online query performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6503</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6503</id><created>2014-05-26</created><updated>2015-01-09</updated><authors><author><keyname>Arndt</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Subset-lex: did we miss an order?</title><categories>math.CO cs.DS</categories><comments>Two obvious errors corrected (indicated by &quot;Correction:&quot; in the LaTeX
  source)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize a well-known algorithm for the generation of all subsets of a
set in lexicographic order with respect to the sets as lists of elements
(subset-lex order). We obtain algorithms for various combinatorial objects such
as the subsets of a multiset, compositions and partitions represented as lists
of parts, and for certain restricted growth strings. The algorithms are often
loopless and require at most one extra variable for the computation of the next
object. The performance of the algorithms is very competitive even when not
loopless. A Gray code corresponding to the subset-lex order and a Gray code for
compositions that was found during this work are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6509</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6509</id><created>2014-05-26</created><updated>2015-07-19</updated><authors><author><keyname>Awad</keyname><forenames>Edmond</forenames></author><author><keyname>Booth</keyname><forenames>Richard</forenames></author><author><keyname>Tohme</keyname><forenames>Fernando</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author></authors><title>Judgment Aggregation in Multi-Agent Argumentation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of conflicting arguments, there can exist multiple plausible
opinions about which arguments should be accepted, rejected, or deemed
undecided. We study the problem of how multiple such judgments can be
aggregated. We define the problem by adapting various classical
social-choice-theoretic properties for the argumentation domain. We show that
while argument-wise plurality voting satisfies many properties, it fails to
guarantee the collective rationality of the outcome, and struggles with ties.
We then present more general results, proving multiple impossibility results on
the existence of any good aggregation operator. After characterising the
sufficient and necessary conditions for satisfying collective rationality, we
study whether restricting the domain of argument-wise plurality voting to
classical semantics allows us to escape the impossibility result. We close by
listing graph-theoretic restrictions under which argument-wise plurality rule
does produce collectively rational outcomes. In addition to identifying
fundamental barriers to collective argument evaluation, our results open up the
door for a new research agenda for the argumentation and computational social
choice communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6524</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6524</id><created>2014-05-26</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Automatic large-scale classification of bird sounds is strongly improved
  by unsupervised feature learning</title><categories>cs.SD cs.LG</categories><journal-ref>PeerJ 2:e488, 2014</journal-ref><doi>10.7717/peerj.488</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Automatic species classification of birds from their sound is a computational
tool of increasing importance in ecology, conservation monitoring and vocal
communication studies. To make classification useful in practice, it is crucial
to improve its accuracy while ensuring that it can run at big data scales. Many
approaches use acoustic measures based on spectrogram-type data, such as the
Mel-frequency cepstral coefficient (MFCC) features which represent a
manually-designed summary of spectral information. However, recent work in
machine learning has demonstrated that features learnt automatically from data
can often outperform manually-designed feature transforms. Feature learning can
be performed at large scale and &quot;unsupervised&quot;, meaning it requires no manual
data labelling, yet it can improve performance on &quot;supervised&quot; tasks such as
classification. In this work we introduce a technique for feature learning from
large volumes of bird sound recordings, inspired by techniques that have proven
useful in other domains. We experimentally compare twelve different feature
representations derived from the Mel spectrum (of which six use this
technique), using four large and diverse databases of bird vocalisations, with
a random forest classifier. We demonstrate that MFCCs are of limited power in
this context, leading to worse performance than the raw Mel spectral data.
Conversely, we demonstrate that unsupervised feature learning provides a
substantial boost over MFCCs and Mel spectra without adding computational
complexity after the model has been trained. The boost is particularly notable
for single-label classification tasks at large scale. The spectro-temporal
activations learned through our procedure resemble spectro-temporal receptive
fields calculated from avian primary auditory forebrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6539</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6539</id><created>2014-05-26</created><authors><author><keyname>Jain</keyname><forenames>Paridhi</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>@I to @Me: An Anatomy of Username Changing Behavior on Twitter</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An identity of a user on an online social network (OSN) is defined by her
profile, content and network attributes. OSNs allow users to change their
online attributes with time, to reflect changes in their real-life. Temporal
changes in users' content and network attributes have been well studied in
literature, however little research has explored temporal changes in profile
attributes of online users. This work makes the first attempt to study changes
to a unique profile attribute of a user - username and on a popular OSN which
allows users to change usernames multiple times - Twitter. We collect, monitor
and analyze 8.7 million Twitter users at macroscopic level and 10,000 users at
microscopic level to understand username changing behavior. We find that around
10% of monitored Twitter users opt to change usernames for possible reasons
such as space gain, followers gain, and username promotion. Few users switch
back to any of their past usernames, however prefer recently dropped usernames
to switch back to. Users who change usernames are more active and popular than
users who don't. In-degree, activity and account creation year of users are
weakly correlated with their frequency of username change. We believe that past
usernames of a user and their associated benefits inferred from the past, can
help Twitter to suggest its users a set of suitable usernames to change to.
Past usernames may also help in other applications such as searching and
linking multiple OSN accounts of a user and correlating multiple Twitter
profiles to a single user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6544</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6544</id><created>2014-05-26</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Continuous Compressed Sensing With a Single or Multiple Measurement
  Vectors</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, in IEEE Workshop on Statistical Signal Processing
  (SSP), pp. 308--311, June 2014</comments><doi>10.1109/SSP.2014.6884632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a single or multiple frequency-sparse
signals, which share the same frequency components, from a subset of regularly
spaced samples. The problem is referred to as continuous compressed sensing
(CCS) in which the frequencies can take any values in the normalized domain
[0,1). In this paper, a link between CCS and low rank matrix completion (LRMC)
is established based on an $\ell_0$-pseudo-norm-like formulation, and
theoretical guarantees for exact recovery are analyzed. Practically efficient
algorithms are proposed based on the link and convex and nonconvex relaxations,
and validated via numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6562</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6562</id><created>2014-05-26</created><authors><author><keyname>Yang</keyname><forenames>Yongjie</forenames></author></authors><title>Election Attacks with Few Candidates</title><categories>cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the parameterized complexity of strategic behaviors in
generalized scoring rules. In particular, we prove that the manipulation,
control (all the 22 standard types), and bribery problems are fixed-parameter
tractable for most of the generalized scoring rules, with respect to the number
of candidates. Our results imply that all these strategic voting problems are
fixed-parameter tractable for most of the common voting rules, such as
Plurality, r-Approval, Borda, Copeland, Maximin, Bucklin, etc., with respect to
the number of candidates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6563</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6563</id><created>2014-05-26</created><authors><author><keyname>Cuzzolin</keyname><forenames>Fabio</forenames></author><author><keyname>Mateus</keyname><forenames>Diana</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D
  Articulated Bodies</title><categories>cs.CV cs.GR cs.LG</categories><comments>31 pages, 26 figures</comments><journal-ref>International Journal of Computer Vision 112(1), 43-70, 2015</journal-ref><doi>10.1007/s11263-014-0754-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In motion analysis and understanding it is important to be able to fit a
suitable model or structure to the temporal series of observed data, in order
to describe motion patterns in a compact way, and to discriminate between them.
In an unsupervised context, i.e., no prior model of the moving object(s) is
available, such a structure has to be learned from the data in a bottom-up
fashion. In recent times, volumetric approaches in which the motion is captured
from a number of cameras and a voxel-set representation of the body is built
from the camera views, have gained ground due to attractive features such as
inherent view-invariance and robustness to occlusions. Automatic, unsupervised
segmentation of moving bodies along entire sequences, in a temporally-coherent
and robust way, has the potential to provide a means of constructing a
bottom-up model of the moving body, and track motion cues that may be later
exploited for motion classification. Spectral methods such as locally linear
embedding (LLE) can be useful in this context, as they preserve &quot;protrusions&quot;,
i.e., high-curvature regions of the 3D volume, of articulated shapes, while
improving their separation in a lower dimensional space, making them in this
way easier to cluster. In this paper we therefore propose a spectral approach
to unsupervised and temporally-coherent body-protrusion segmentation along time
sequences. Volumetric shapes are clustered in an embedding space, clusters are
propagated in time to ensure coherence, and merged or split to accommodate
changes in the body's topology. Experiments on both synthetic and real
sequences of dense voxel-set data are shown. This supports the ability of the
proposed method to cluster body-parts consistently over time in a totally
unsupervised fashion, its robustness to sampling density and shape quality, and
its potential for bottom-up model construction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6564</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6564</id><created>2014-05-26</created><updated>2014-07-28</updated><authors><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Schmidt</keyname><forenames>Christiane</forenames></author></authors><title>A PTAS for the continuous 1.5D Terrain Guarding Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the continuous 1.5-dimensional terrain guarding problem we are given an
$x$-monotone chain (the \emph{terrain} $T$) and ask for the minimum number of
point guards (located anywhere on $T$), such that all points of $T$ are covered
by at least one guard. It has been shown that the 1.5-dimensional terrain
guarding problem is \NP-hard. The currently best known approximation algorithm
achieves a factor of $4$. For the discrete problem version with a finite set of
guard candidates and a finite set of points on the terrain that need to be
monitored, a polynomial time approximation scheme (PTAS) has been presented
[10]. We show that for the general problem we can construct finite guard and
witness sets, $G$ and $W$, such that there exists an optimal guard cover $G^*
\subseteq G$ that covers $T$, and when these guards monitor all points in $W$
the entire terrain is guarded. This leads to a PTAS as well as an (exact) IP
formulation for the continuous terrain guarding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6573</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6573</id><created>2014-05-26</created><authors><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Lou</keyname><forenames>Jian</forenames></author><author><keyname>Wen</keyname><forenames>Zhonghua</forenames></author></authors><title>Allocating Indivisible Resources under Price Rigidities in Polynomial
  Time</title><categories>cs.GT cs.MA</categories><comments>IJCAI-2013 Multidisciplinary Workshop on Advances in Preference
  Handling</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many realistic problems of allocating resources, economy efficiency must
be taken into consideration together with social equality, and price rigidities
are often made according to some economic and social needs. We study the
computational issues of dynamic mechanisms for selling multiple indivisible
items under price rigidities. We propose a polynomial algorithm that can be
used to find over-demanded sets of items, and then introduce a dynamic
mechanism with rationing to discover constrained Walrasian equilibria under
price rigidities in polynomial time. We also address the computation of
sellers' expected profits and items' expected prices, and discuss strategical
issues in the sense of expected profits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6578</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6578</id><created>2014-05-26</created><authors><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Lou</keyname><forenames>Jian</forenames></author><author><keyname>Wen</keyname><forenames>Zhonghua</forenames></author></authors><title>A Parallel Elicitation-Free Protocol for Allocating Indivisible Goods</title><categories>cs.MA cs.GT</categories><comments>IJCAI-2013 Multidisciplinary Workshop on Advances in Preference
  Handling</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of allocating a set of indivisible goods to multiple
agents. Recent work [Bouveret and Lang, 2011] focused on allocating goods in a
sequential way, and studied what is the &quot;best&quot; sequence of agents to pick
objects based on utilitarian or egalitarian criterion. In this paper, we
propose a parallel elicitation-free protocol for allocating indivisible goods.
In every round of the allocation process, some agents will be selected
(according to some policy) to report their preferred objects among those that
remain, and every reported object will be allocated randomly to an agent
reporting it. Empirical comparison between the parallel protocol (applying a
simple selection policy) and the sequential protocol (applying the optimal
sequence) reveals that our proposed protocol is promising. We also address
strategical issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6585</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6585</id><created>2014-05-26</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Exact Joint Sparse Frequency Recovery via Optimization Methods</title><categories>cs.IT math.IT</categories><comments>41 pages, single column, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency recovery/estimation from samples of superimposed sinusoidal signals
is a classical problem in statistical signal processing. Its research has been
recently advanced by atomic norm techniques which deal with continuous-valued
frequencies and completely eliminate basis mismatches of existing compressed
sensing methods. This work investigates the frequency recovery problem in the
presence of multiple measurement vectors (MMVs) which share the same frequency
components, termed as joint sparse frequency recovery and arising naturally
from array processing applications. $\ell_0$- and $\ell_1$-norm-like
formulations, referred to as atomic $\ell_0$ norm and the atomic norm, are
proposed to recover the frequencies and cast as (nonconvex) rank minimization
and (convex) semidefinite programming, respectively. Their guarantees for exact
recovery are theoretically analyzed which extend existing results with a single
measurement vector (SMV) to the MMV case and meanwhile generalize the existing
joint sparse compressed sensing framework to the continuous dictionary setting.
In particular, given a set of $N$ regularly spaced samples per measurement
vector it is shown that the frequencies can be exactly recovered via solving a
convex optimization problem once they are separate by at least (approximately)
$\frac{4}{N}$. Under the same frequency separation condition, a random subset
of $N$ regularly spaced samples of size $O(K\log K\log N)$ per measurement
vector is sufficient to guarantee exact recovery of the $K$ frequencies and
missing samples with high probability via similar convex optimization.
Extensive numerical simulations are provided to validate our analysis and
demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6594</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6594</id><created>2014-05-26</created><authors><author><keyname>Ngassa</keyname><forenames>Christiane L. Kameni</forenames></author><author><keyname>Savin</keyname><forenames>Valentin</forenames></author><author><keyname>Dupraz</keyname><forenames>Elsa</forenames></author><author><keyname>Declercq</keyname><forenames>David</forenames></author></authors><title>Density Evolution and Functional Threshold for the Noisy Min-Sum Decoder</title><categories>cs.IT math.IT</categories><comments>46 pages (draft version); extended version of the paper with same
  title, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the behavior of the Min-Sum decoder running on noisy
devices. The aim is to evaluate the robustness of the decoder in the presence
of computation noise, e.g. due to faulty logic in the processing units, which
represents a new source of errors that may occur during the decoding process.
To this end, we first introduce probabilistic models for the arithmetic and
logic units of the the finite-precision Min-Sum decoder, and then carry out the
density evolution analysis of the noisy Min-Sum decoder. We show that in some
particular cases, the noise introduced by the device can help the Min-Sum
decoder to escape from fixed points attractors, and may actually result in an
increased correction capacity with respect to the noiseless decoder. We also
reveal the existence of a specific threshold phenomenon, referred to as
functional threshold. The behavior of the noisy decoder is demonstrated in the
asymptotic limit of the code-length -- by using &quot;noisy&quot; density evolution
equations -- and it is also verified in the finite-length case by Monte-Carlo
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6623</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6623</id><created>2014-05-22</created><authors><author><keyname>Magee</keyname><forenames>Andrew F.</forenames></author><author><keyname>May</keyname><forenames>Michael R.</forenames></author><author><keyname>Moore</keyname><forenames>Brian R.</forenames></author></authors><title>The Dawn of Open Access to Phylogenetic Data</title><categories>q-bio.PE cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific enterprise depends critically on the preservation of and open
access to published data. This basic tenet applies acutely to phylogenies
(estimates of evolutionary relationships among species). Increasingly,
phylogenies are estimated from increasingly large, genome-scale datasets using
increasingly complex statistical methods that require increasing levels of
expertise and computational investment. Moreover, the resulting phylogenetic
data provide an explicit historical perspective that critically informs
research in a vast and growing number of scientific disciplines. One such use
is the study of changes in rates of lineage diversification (speciation -
extinction) through time. As part of a meta-analysis in this area, we sought to
collect phylogenetic data (comprising nucleotide sequence alignment and tree
files) from 217 studies published in 46 journals over a 13-year period. We
document our attempts to procure those data (from online archives and by direct
request to corresponding authors), and report results of analyses (using
Bayesian logistic regression) to assess the impact of various factors on the
success of our efforts. Overall, complete phylogenetic data for ~60% of these
studies are effectively lost to science. Our study indicates that phylogenetic
data are more likely to be deposited in online archives and/or shared upon
request when: (1) the publishing journal has a strong data-sharing policy; (2)
the publishing journal has a higher impact factor, and; (3) the data are
requested from faculty rather than students. Although the situation appears
dire, our analyses suggest that it is far from hopeless: recent initiatives by
the scientific community -- including policy changes by journals and funding
agencies -- are improving the state of affairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6627</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6627</id><created>2014-05-07</created><authors><author><keyname>N</keyname><forenames>Rajkumar</forenames></author><author><keyname>G</keyname><forenames>Anand M.</forenames></author><author><keyname>N</keyname><forenames>Barathiraja</forenames></author></authors><title>Portable Camera-Based Product Label Reading For Blind People</title><categories>cs.HC cs.CY</categories><comments>4 Pages, 5 Figures and &quot;Published with International Journal of
  Engineering Trends and Technology(IJETT)&quot;</comments><journal-ref>IJETT,V10(11),521-524 April 2014.ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V10P303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a camera-based assistive text reading framework to help blind
persons read text labels and product packaging from hand-held objects in their
daily life. To isolate the object from untidy backgrounds or other surrounding
objects in the camera vision, we initially propose an efficient and effective
motion based method to define a region of interest (ROI) in the video by asking
the user to tremble the object. This scheme extracts moving object region by a
mixture-of-Gaussians-based background subtraction technique. In the extracted
ROI, text localization and recognition are conducted to acquire text details.
To automatically focus the text regions from the object ROI, we offer a novel
text localization algorithm by learning gradient features of stroke
orientations and distributions of edge pixels in an Adaboost model. Text
characters in the localized text regions are then binarized and recognized by
off-the-shelf optical character identification software. The renowned text
codes are converted into audio output to the blind users. Performance of the
suggested text localization algorithm is quantitatively evaluated on ICDAR-2003
and ICDAR-2011 Robust Reading Datasets. Experimental results demonstrate that
our algorithm achieves the highest level of developments at present time. The
proof-of-concept example is also evaluated on a dataset collected using ten
blind persons to evaluate the effectiveness of the scheme. We explore the user
interface issues and robustness of the algorithm in extracting and reading text
from different objects with complex backgrounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6630</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6630</id><created>2014-05-26</created><authors><author><keyname>Wojtas</keyname><forenames>Krzysztof</forenames></author><author><keyname>Magiera</keyname><forenames>Krzysztof</forenames></author><author><keyname>Mi\kasko</keyname><forenames>Tomasz</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author></authors><title>Possible Winners in Noisy Elections</title><categories>cs.GT cs.CC cs.MA</categories><comments>34 pages</comments><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of predicting winners in elections, for the case
where we are given complete knowledge about all possible candidates, all
possible voters (together with their preferences), but where it is uncertain
either which candidates exactly register for the election or which voters cast
their votes. Under reasonable assumptions, our problems reduce to counting
variants of election control problems. We either give polynomial-time
algorithms or prove #P-completeness results for counting variants of control by
adding/deleting candidates/voters for Plurality, k-Approval, Approval,
Condorcet, and Maximin voting rules. We consider both the general case, where
voters' preferences are unrestricted, and the case where voters' preferences
are single-peaked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6636</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6636</id><created>2014-05-26</created><updated>2014-09-18</updated><authors><author><keyname>Zhuang</keyname><forenames>Binnan</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Traffic Driven Resource Allocation in Heterogenous Wireless Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures IEEE GLOBECOM 2014 (accepted for publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work on wireless network resource allocation use physical layer
performance such as sum rate and outage probability as the figure of merit.
These metrics may not reflect the true user QoS in future heterogenous networks
(HetNets) with many small cells, due to large traffic variations in overlapping
cells with complicated interference conditions. This paper studies the spectrum
allocation problem in HetNets using the average packet sojourn time as the
performance metric. To be specific, in a HetNet with $K$ base terminal stations
(BTS's), we determine the optimal partition of the spectrum into $2^K$ possible
spectrum sharing combinations. We use an interactive queueing model to
characterize the flow level performance, where the service rates are decided by
the spectrum partition. The spectrum allocation problem is formulated using a
conservative approximation, which makes the optimization problem convex. We
prove that in the optimal solution the spectrum is divided into at most $K$
pieces. A numerical algorithm is provided to solve the spectrum allocation
problem on a slow timescale with aggregate traffic and service information.
Simulation results show that the proposed solution achieves significant gains
compared to both orthogonal and full spectrum reuse allocations with moderate
to heavy traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6642</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6642</id><created>2014-05-26</created><updated>2015-08-30</updated><authors><author><keyname>Sun</keyname><forenames>Wei</forenames><affiliation>Yahoo Labs</affiliation></author><author><keyname>Qiao</keyname><forenames>Xingye</forenames><affiliation>Binghamton</affiliation></author><author><keyname>Cheng</keyname><forenames>Guang</forenames><affiliation>Purdue</affiliation></author></authors><title>Stabilized Nearest Neighbor Classifier and Its Statistical Properties</title><categories>stat.ML cs.LG</categories><comments>48 Pages, 11 Figures. To Appear in JASA--T&amp;M</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stability of statistical analysis is an important indicator for
reproducibility, which is one main principle of scientific method. It entails
that similar statistical conclusions can be reached based on independent
samples from the same underlying population. In this paper, we introduce a
general measure of classification instability (CIS) to quantify the sampling
variability of the prediction made by a classification method. Interestingly,
the asymptotic CIS of any weighted nearest neighbor classifier turns out to be
proportional to the Euclidean norm of its weight vector. Based on this concise
form, we propose a stabilized nearest neighbor (SNN) classifier, which
distinguishes itself from other nearest neighbor classifiers, by taking the
stability into consideration. In theory, we prove that SNN attains the minimax
optimal convergence rate in risk, and a sharp convergence rate in CIS. The
latter rate result is established for general plug-in classifiers under a
low-noise condition. Extensive simulated and real examples demonstrate that SNN
achieves a considerable improvement in CIS over existing nearest neighbor
classifiers, with comparable classification accuracy. We implement the
algorithm in a publicly available R package snn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6646</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6646</id><created>2014-05-26</created><updated>2016-02-23</updated><authors><author><keyname>Maidl</keyname><forenames>Andr&#xe9; Murbach</forenames></author><author><keyname>Medeiros</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Mascarenhas</keyname><forenames>Fabio</forenames></author><author><keyname>Ierusalimschy</keyname><forenames>Roberto</forenames></author></authors><title>Error Reporting in Parsing Expression Grammars</title><categories>cs.PL cs.FL</categories><comments>Preprint submitted to Science of Computer Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsing Expression Grammars (PEGs) are a formalism to describe a top-down
parser for a language. Unfortunately, error handling techniques that are often
applied to top-down parsers are not directly applicable to PEGs. This problem
is usually solved by using an heuristic that helps to simulate the error
reporting technique from top-down parsers. We show how implementations that
have semantic actions can use this heuristic even if the implementation does
not implement it directly. We also add this heuristic to a formal semantics of
PEGs.
  We also propose a complementary error reporting strategy that may lead to
better error messages: labeled failures. This approach is inspired by standard
exception handling of programming languages, and lets a PEG define different
kinds of failure, with each ordered choice operator specifying which kinds it
catches. Labeled failures not only give a way to annotate grammars for better
error reporting, but also are expressive enough to express some of the error
reporting strategies used by deterministic parser combinators, and give a way
of encoding predictive top-down parsing in a PEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6660</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6660</id><created>2014-05-08</created><authors><author><keyname>Gonz&#xe1;lez-Valiente</keyname><forenames>Carlos Luis</forenames></author><author><keyname>Santos</keyname><forenames>Magda Le&#xf3;n</forenames></author><author><keyname>Rivera</keyname><forenames>Zoia</forenames></author></authors><title>El egresado de la carrera Ciencias de la Informaci\'on y su inserci\'on
  en la gesti\'on de mercadotecnia</title><categories>cs.CY</categories><comments>in Spanish</comments><journal-ref>ACIMED 2014; 25(2):234-248</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The study aims to identify the possibilities offered by academic studies in
Cuba for the training of information professionals as marketing managers. A
theoretical analysis is conducted of the basic functions of marketing and its
information dimension, the specific features of the work of the marketing
manager, and the competencies developed by Curriculum D in information science
graduates in the Cuban context. Based on the analysis of curricular contents
and interviews with ten information professionals working as marketing
managers, determination was made of some indispensable competencies which
should be developed during the training and later on during service practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6661</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6661</id><created>2014-05-08</created><authors><author><keyname>G</keyname><forenames>Divyajyothi M</forenames></author><author><keyname>Rachappa</keyname></author><author><keyname>Rao</keyname><forenames>D H</forenames></author></authors><title>A scenario based approach for dealing with challenges in a pervasive
  computing environment</title><categories>cs.MM</categories><comments>8 pages, IJCSA, Vol 4, No.2,April 2014</comments><doi>10.5121/ijcsa.2014.4204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the surge in modern research focus towards Pervasive Computing, lot of
techniques and challenges needs to be addressed so as to effectively create
smart spaces and achieve miniaturization. In the process of scaling down to
compact devices, the real things to ponder upon are the Information Retrieval
challenges. In this work, we discuss the aspects of multimedia which makes
information access challenging. An Example Pattern Recognition scenario is
presented and the mathematical techniques that can be used to model uncertainty
are also presented for developing a system that can sense, compute and
communicate in a way that can make human life easy with smart objects assisting
from around his surroundings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6662</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6662</id><created>2014-05-09</created><authors><author><keyname>Dawn</keyname><forenames>Suma</forenames></author><author><keyname>Saxena</keyname><forenames>Vikas</forenames></author><author><keyname>Sharma</keyname><forenames>Bhudev</forenames></author></authors><title>Cognitive-mapping and contextual pyramid based Digital Elevation Model
  Registration and its effective storage using fractal based compression</title><categories>cs.AI cs.CV</categories><comments>17 pages, 8 tables, and 3 figures; IJCSI International Journal of
  Computer Science Issues, Vol. 10, Issue 3, No 1, May 2013, ISSN (Print):
  1694-0814 | ISSN (Online): 1694-0784 (www.IJCSI.org)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Elevation models (DEM) are images having terrain information embedded
into them. Using cognitive mapping concepts for DEM registration, has evolved
from this basic idea of using the mapping between the space to objects and
defining their relationships to form the basic landmarks that need to be
marked, stored and manipulated in and about the environment or other candidate
environments, namely, in our case, the DEMs. The progressive two-level
encapsulation of methods of geo-spatial cognition includes landmark knowledge
and layout knowledge and can be useful for DEM registration. Space-based
approach, that emphasizes on explicit extent of the environment under
consideration, and object-based approach, that emphasizes on the relationships
between objects in the local environment being the two paradigms of cognitive
mapping can be methodically integrated in this three-architecture for DEM
registration. Initially, P-model based segmentation is performed followed by
landmark formation for contextual mapping that uses contextual pyramid
formation. Apart from landmarks being used for registration key-point finding,
Euclidean distance based deformation calculation has been used for
transformation and change detection. Landmarks have been categorized to belong
to either being flat-plain areas without much variation in the land heights;
peaks that can be found when there is gradual increase in height as compared to
the flat areas; valleys, marked with gradual decrease in the height seen in
DEM; and finally, ripple areas with very shallow crests and nadirs. Fractal
based compression was used for storage of co-registered DEMs. This method may
further be extended for DEM-topographic map and DEM-to-remote sensed image
registration. Experimental results further cement the fact that DEM
registration may be effectively done using the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6664</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6664</id><created>2014-05-26</created><updated>2014-08-03</updated><authors><author><keyname>Tillmann</keyname><forenames>Andreas M.</forenames></author></authors><title>On the Computational Intractability of Exact and Approximate Dictionary
  Learning</title><categories>cs.IT cs.LG math.IT</categories><comments>5 pages; accepted for publication</comments><doi>10.1109/LSP.2014.2345761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficient sparse coding and reconstruction of signal vectors via linear
observations has received a tremendous amount of attention over the last
decade. In this context, the automated learning of a suitable basis or
overcomplete dictionary from training data sets of certain signal classes for
use in sparse representations has turned out to be of particular importance
regarding practical signal processing applications. Most popular dictionary
learning algorithms involve NP-hard sparse recovery problems in each iteration,
which may give some indication about the complexity of dictionary learning but
does not constitute an actual proof of computational intractability. In this
technical note, we show that learning a dictionary with which a given set of
training signals can be represented as sparsely as possible is indeed NP-hard.
Moreover, we also establish hardness of approximating the solution to within
large factors of the optimal sparsity level. Furthermore, we give NP-hardness
and non-approximability results for a recent dictionary learning variation
called the sensor permutation problem. Along the way, we also obtain a new
non-approximability result for the classical sparse recovery problem from
compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6667</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6667</id><created>2014-05-26</created><authors><author><keyname>Ludu</keyname><forenames>Puneet Singh</forenames></author></authors><title>Inferring gender of a Twitter user using celebrities it follows</title><categories>cs.IR cs.CL</categories><comments>Submitted at CSE department, SUNY Buffalo, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the task of user gender classification in social media,
with an application to Twitter. The approach automatically predicts gender by
leveraging observable information such as the tweet behavior, linguistic
content of the user's Twitter feed and the celebrities followed by the user.
This paper first evaluates linguistic content based features using LIWC
dictionary and popular neighborhood features using Wikipedia and Freebase. Then
augments both features which yielded a significant increase in the accuracy for
gender prediction. Results show that rich linguistic features combined with
popular neighborhood prove valuables and promising for additional user
classification needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6671</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6671</id><created>2014-05-26</created><updated>2014-10-16</updated><authors><author><keyname>Geffert</keyname><forenames>Viliam</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Classical automata on promise problems</title><categories>cs.FL cs.CC</categories><comments>21 pages, significantly improved with a correction (the single
  sentence statement given just before Corollary 4 in the previous version and
  (in the conference version) is not correct). A preliminary version appeared
  in DCFS2014 [vol. 8614 of LNCS, pp. 126--137, Springer-Verlag, 2014]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Promise problems were mainly studied in quantum automata theory. Here we
focus on state complexity of classical automata for promise problems. First, it
was known that there is a family of unary promise problems solvable by quantum
automata by using a single qubit, but the number of states required by
corresponding one-way deterministic automata cannot be bounded by a constant.
For this family, we show that even two-way nondeterminism does not help to save
a single state. By comparing this with the corresponding state complexity of
alternating machines, we then get a tight exponential gap between two-way
nondeterministic and one-way alternating automata solving unary promise
problems. Second, despite of the existing quadratic gap between Las Vegas
realtime probabilistic automata and one-way deterministic automata for language
recognition, we show that, by turning to promise problems, the tight gap
becomes exponential. Last, we show that the situation is different for one-way
probabilistic automata with two-sided bounded-error. We present a family of
unary promise problems that is very easy for these machines; solvable with only
two states, but the number of states in two-way alternating or any simpler
automata is not limited by a constant. Moreover, we show that one-way
bounded-error probabilistic automata can solve promise problems not solvable at
all by any other classical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6676</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6676</id><created>2014-05-26</created><updated>2014-10-05</updated><authors><author><keyname>Besse</keyname><forenames>Philippe</forenames><affiliation>IMT</affiliation></author><author><keyname>Villa-Vialaneix</keyname><forenames>Nathalie</forenames><affiliation>MIAT INRA</affiliation></author></authors><title>Statistique et Big Data Analytics; Volum\'etrie, L'Attaque des Clones</title><categories>stat.OT cs.LG math.ST stat.TH</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article assumes acquired the skills and expertise of a statistician in
unsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,
random forest). What skills and knowledge do a statistician must acquire to
reach the &quot;Volume&quot; scale of big data? After a quick overview of the different
strategies available and especially of those imposed by Hadoop, the algorithms
of some available learning methods are outlined in order to understand how they
are adapted to the strong stresses of the Map-Reduce functionalities
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6678</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6678</id><created>2014-05-26</created><authors><author><keyname>Moot</keyname><forenames>Richard</forenames><affiliation>LaBRI</affiliation></author></authors><title>Hybrid Type-Logical Grammars, First-Order Linear Logic and the
  Descriptive Inadequacy of Lambda Grammars</title><categories>cs.LO cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we show that hybrid type-logical grammars are a fragment of
first-order linear logic. This embedding result has several important
consequences: it not only provides a simple new proof theory for the calculus,
thereby clarifying the proof-theoretic foundations of hybrid type-logical
grammars, but, since the translation is simple and direct, it also provides
several new parsing strategies for hybrid type-logical grammars. Second,
NP-completeness of hybrid type-logical grammars follows immediately. The main
embedding result also sheds new light on problems with lambda grammars/abstract
categorial grammars and shows lambda grammars/abstract categorial grammars
suffer from problems of over-generation and from problems at the
syntax-semantics interface unlike any other categorial grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6682</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6682</id><created>2014-05-26</created><authors><author><keyname>Poibeau</keyname><forenames>Thierry</forenames><affiliation>LaTTICe</affiliation></author></authors><title>Optimality Theory as a Framework for Lexical Acquisition</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>15th International Conference on Intelligent Text Processing and
  Computational Linguistics, Nepal (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper re-investigates a lexical acquisition system initially developed
for French.We show that, interestingly, the architecture of the system
reproduces and implements the main components of Optimality Theory. However, we
formulate the hypothesis that some of its limitations are mainly due to a poor
representation of the constraints used. Finally, we show how a better
representation of the constraints used would yield better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6684</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6684</id><created>2014-05-26</created><authors><author><keyname>P&#x142;o&#x144;ski</keyname><forenames>Piotr</forenames></author><author><keyname>Zaremba</keyname><forenames>Krzysztof</forenames></author></authors><title>Visualizing Random Forest with Self-Organising Map</title><categories>cs.LG</categories><journal-ref>Lecture Notes in Computer Science Volume 8468, 2014, pp 63-71</journal-ref><doi>10.1007/978-3-319-07176-3_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Forest (RF) is a powerful ensemble method for classification and
regression tasks. It consists of decision trees set. Although, a single tree is
well interpretable for human, the ensemble of trees is a black-box model. The
popular technique to look inside the RF model is to visualize a RF proximity
matrix obtained on data samples with Multidimensional Scaling (MDS) method.
Herein, we present a novel method based on Self-Organising Maps (SOM) for
revealing intrinsic relationships in data that lay inside the RF used for
classification tasks. We propose an algorithm to learn the SOM with the
proximity matrix obtained from the RF. The visualization of RF proximity matrix
with MDS and SOM is compared. What is more, the SOM learned with the RF
proximity matrix has better classification accuracy in comparison to SOM
learned with Euclidean distance. Presented approach enables better
understanding of the RF and additionally improves accuracy of the SOM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6689</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6689</id><created>2014-05-26</created><updated>2014-05-30</updated><authors><author><keyname>Asadi</keyname><forenames>Arash</forenames></author><author><keyname>Jacko</keyname><forenames>Peter</forenames></author><author><keyname>Mancuso</keyname><forenames>Vincenzo</forenames></author></authors><title>Modeling Multi-mode D2D Communications in LTE</title><categories>cs.NI</categories><comments>A shorter version of this manuscript is accepted for publication in
  MAMA workshop collocated with Sigmetrics'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a roadmap towards the analytical understanding of
Device-to-Device (D2D) communications in LTE-A networks. Various D2D solutions
have been proposed, which include inband and outband D2D transmission modes,
each of which exhibits different pros and cons in terms of complexity,
interference, and spectral efficiency achieved. We go beyond traditional mode
optimization and mode-selection schemes. Specifically, we formulate a general
problem for the joint per-user mode selection, connection activation and
resource scheduling of connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6703</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6703</id><created>2014-05-24</created><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author></authors><title>Second Life as a Platform for Physics Simulations and Microworlds: An
  Evaluation</title><categories>physics.ed-ph cs.CY</categories><comments>9 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1405.6326</comments><acm-class>K.3.1; J.2; I.6.8</acm-class><journal-ref>Proceedings of the CBLIS 2012. Barcelona: CRECIM - Centre for
  Research in Science and Mathematics, 2012. p.173-180</journal-ref><doi>10.13140/RG.2.1.3617.0721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Often mistakenly seen as a game, the online 3D immersive virtual world Second
Life (SL) is itself a huge and sophisticated simulator of an entire Earthlike
world. Differently from other metaverses where physical laws are not seriously
taken into account, objects created in SL are automatically controlled by a
powerful physics engine software. Despite that, it has been used mostly as a
mere place for exploration and inquiry, with emphasis on group interaction.
This article reports on a study conducted to evaluate the SL environment as a
platform for physical simulations and microworlds. It begins by discussing a
few relevant features of SL and a few differences found between it and
traditional simulators e.g. Modellus. Finally, the SL environment as a platform
for physical simulations and microworlds is evaluated. Some concrete examples
of simulations in SL, including two of our own authorship, will be presented
briefly in order to clarify and enrich both discussion and analysis. However,
implementation of simulations in SL is not without drawbacks like the lack of
experience many teachers have with programming and the differences found
between SL Physics and Newtonian Physics. Despite of that, findings suggest it
may possible for teachers to overcome these obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6707</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6707</id><created>2014-05-26</created><updated>2014-06-11</updated><authors><author><keyname>Lawyer</keyname><forenames>Glenn</forenames></author></authors><title>Understanding the spreading power of all nodes in a network: a
  continuous-time perspective</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality measures such as the degree, k-shell, or eigenvalue centrality can
identify a network's most influential nodes, but are rarely usefully accurate
in quantifying the spreading power of the vast majority of nodes which are not
highly influential. The spreading power of all network nodes is better
explained by considering, from a continuous-time epidemiological perspective,
the distribution of the force of infection each node generates. The resulting
metric, the \textit{expected force}, accurately quantifies node spreading power
under all primary epidemiological models across a wide range of archetypical
human contact networks. When node power is low, influence is a function of
neighbor degree. As power increases, a node's own degree becomes more
important. The strength of this relationship is modulated by network structure,
being more pronounced in narrow, dense networks typical of social networking
and weakening in broader, looser association networks such as the Internet. The
expected force can be computed independently for individual nodes, making it
applicable for networks whose adjacency matrix is dynamic, not well specified,
or overwhelmingly large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6738</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6738</id><created>2014-05-26</created><authors><author><keyname>Moussa</keyname><forenames>Karima Haddou ou</forenames></author><author><keyname>Sondergeld</keyname><forenames>Ute</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Rittberger</keyname><forenames>Marc</forenames></author></authors><title>Assessing Educational Research -- An Information Service for Monitoring
  a Heterogeneous Research Field</title><categories>cs.DL</categories><comments>8 pages, 10 figures, Libraries in the digital age (LIDA) 2014
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a web prototype that visualises different characteristics
of research projects in the heterogeneous domain of educational research. The
concept of the application derives from the project &quot;Monitoring Educational
Research&quot; (MoBi) that aims at identifying and implementing indicators that
adequately describe structural properties and dynamics of the research field.
The prototype enables users to visualise data regarding different indicators,
e.g. &quot;research activity&quot;, &quot;funding&quot;, &quot;qualification project&quot;, &quot;disciplinary
area&quot;. Since the application is based on Semantic MediaWikitechnology it
furthermore provides an easily accessible opportunity to collaboratively work
on a database of research projects. Users can jointly and in a semantically
controlled way enter metadata on research projects which are the basis for the
computation and visualisation of indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6741</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6741</id><created>2014-05-26</created><authors><author><keyname>Houshmand</keyname><forenames>Mahboobeh</forenames></author><author><keyname>Zamani</keyname><forenames>Morteza Saheb</forenames></author><author><keyname>Sedighi</keyname><forenames>Mehdi</forenames></author><author><keyname>Arabzadeh</keyname><forenames>Mona</forenames></author></authors><title>Decomposition of Diagonal Hermitian Quantum Gates Using
  Multiple-Controlled Pauli Z Gates</title><categories>cs.ET quant-ph</categories><comments>To Appear in ACM Journal on Emerging Technologies in Computing
  Systems</comments><journal-ref>ACM Journal of Emerging Technologies in Computing Systems (JETC),
  Vol. 11, No. 3, Article 28, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum logic decomposition refers to decomposing a given quantum gate to a
set of physically implementable gates. An approach has been presented to
decompose arbitrary diagonal quantum gates to a set of multiplexed-rotation
gates around z axis. In this paper, a special class of diagonal quantum gates,
namely diagonal Hermitian quantum gates, is considered and a new perspective to
the decomposition problem with respect to decomposing these gates is presented.
It is first shown that these gates can be decomposed to a set that solely
consists of multiple-controlled Z gates. Then a binary representation for the
diagonal Hermitian gates is introduced. It is shown that the binary
representations of multiple-controlled Z gates form a basis for the vector
space that is produced by the binary representations of all diagonal Hermitian
quantum gates. Moreover, the problem of decomposing a given diagonal Hermitian
gate is mapped to the problem of writing its binary representation in the
specific basis mentioned above. Moreover, CZ gate is suggested to be the
two-qubit gate in the decomposition library, instead of previously used CNOT
gate. Experimental results show that the proposed approach can lead to circuits
with lower costs in comparison with the previous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6744</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6744</id><created>2014-05-26</created><authors><author><keyname>Trabert</keyname><forenames>Christoph</forenames></author><author><keyname>Ulbig</keyname><forenames>Andreas</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Model Predictive Frequency Control Employing Stability Constraints</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article a model predictive control (MPC) based frequency control
scheme for energy storage units was derived, focusing on the incorporation of
stability constraints based on Lyapunov theory and the concept of passivity.
The proposed control schemes, guaranteeing closed-loop stability, are applied
on a one-area and two-area power system. For the two-area power system, a
coordinated (centralized) control and an uncoordinated (decentralized) control
approach is conducted. The stability properties of the different MPC setups
were derived, implemented and simulated. Furthermore the corresponding control
performance was analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6757</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6757</id><created>2014-05-26</created><authors><author><keyname>Mahadevan</keyname><forenames>Sridhar</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author><author><keyname>Thomas</keyname><forenames>Philip</forenames></author><author><keyname>Dabney</keyname><forenames>Will</forenames></author><author><keyname>Giguere</keyname><forenames>Steve</forenames></author><author><keyname>Jacek</keyname><forenames>Nicholas</forenames></author><author><keyname>Gemp</keyname><forenames>Ian</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author></authors><title>Proximal Reinforcement Learning: A New Theory of Sequential Decision
  Making in Primal-Dual Spaces</title><categories>cs.LG</categories><comments>121 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we set forth a new vision of reinforcement learning developed
by us over the past few years, one that yields mathematically rigorous
solutions to longstanding important questions that have remained unresolved:
(i) how to design reliable, convergent, and robust reinforcement learning
algorithms (ii) how to guarantee that reinforcement learning satisfies
pre-specified &quot;safety&quot; guarantees, and remains in a stable region of the
parameter space (iii) how to design &quot;off-policy&quot; temporal difference learning
algorithms in a reliable and stable manner, and finally (iv) how to integrate
the study of reinforcement learning into the rich theory of stochastic
optimization. In this paper, we provide detailed answers to all these questions
using the powerful framework of proximal operators.
  The key idea that emerges is the use of primal dual spaces connected through
the use of a Legendre transform. This allows temporal difference updates to
occur in dual spaces, allowing a variety of important technical advantages. The
Legendre transform elegantly generalizes past algorithms for solving
reinforcement learning problems, such as natural gradient methods, which we
show relate closely to the previously unconnected framework of mirror descent
methods. Equally importantly, proximal operator theory enables the systematic
development of operator splitting methods that show how to safely and reliably
decompose complex products of gradients that occur in recent variants of
gradient-based temporal difference learning. This key technical innovation
makes it possible to finally design &quot;true&quot; stochastic gradient methods for
reinforcement learning. Finally, Legendre transforms enable a variety of other
benefits, including modeling sparsity and domain geometry. Our work builds
extensively on recent work on the convergence of saddle-point algorithms, and
on the theory of monotone operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6773</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6773</id><created>2014-05-26</created><authors><author><keyname>Kim</keyname><forenames>Dongmyoung</forenames></author><author><keyname>Park</keyname><forenames>Taejun</forenames></author><author><keyname>Kim</keyname><forenames>Hyoil</forenames></author><author><keyname>Choi</keyname><forenames>Sunghyun</forenames></author></authors><title>Load Balancing in Two-Tier Cellular Networks with Open and Hybrid Access
  Femtocells</title><categories>cs.NI</categories><comments>17 pages, The shorter version is submitted to IEEE/ACM Transactions
  on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocell base station (BS) is a low-power, low-price BS based on cellular
communication technology. It is expected to become a cost-effective solution
for improving the communication performance of indoor users, whose traffic
demands are large in general. There are mainly three access strategies for
femtocell, i.e., closed access, open access and hybrid access strategies. While
it has been generally known that open/hybrid access femtocells contribute more
to enhancing the system-wide performance than closed access femtocells, the
operating parameters of both macro and femtocells should be carefully chosen
according to the mobile operator's policy, consumer's requirements, and so on.
We propose long-term parameter optimization schemes, which maximize the average
throughput of macrocell users while guaranteeing some degree of benefits to
femtocell owners. To achieve this goal, we jointly optimize the ratio of
dedicated resources for femtocells as well as the femtocell service area in
open access femtocell networks through the numerical analysis. It is proved
that the optimal parameter selection of open access femtocell is a convex
optimization problem in typical environments. Then, we extend our algorithm to
hybrid access femtocells where some intra-femtocell resources are dedicated
only for femtocell owners while remaining resources are shared with foreign
macrocell users. Our evaluation results show that the proposed parameter
optimization schemes significantly enhance the performance of macrocell users
thanks to the large offloading gain. The benefits provided to femtocell users
are also adaptively maintained according to the femtocell users' requirements.
The results in this paper provide insights about the situations where femtocell
deployment on dedicated channels is preferred to the co-channel deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6785</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6785</id><created>2014-05-27</created><authors><author><keyname>Markopoulos</keyname><forenames>Panos P.</forenames></author><author><keyname>Karystinos</keyname><forenames>George N.</forenames></author><author><keyname>Pados</keyname><forenames>Dimitris A.</forenames></author></authors><title>Optimal Algorithms for $L_1$-subspace Signal Processing</title><categories>cs.DS</categories><doi>10.1109/TSP.2014.2338077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe ways to define and calculate $L_1$-norm signal subspaces which
are less sensitive to outlying data than $L_2$-calculated subspaces. We start
with the computation of the $L_1$ maximum-projection principal component of a
data matrix containing $N$ signal samples of dimension $D$. We show that while
the general problem is formally NP-hard in asymptotically large $N$, $D$, the
case of engineering interest of fixed dimension $D$ and asymptotically large
sample size $N$ is not. In particular, for the case where the sample size is
less than the fixed dimension ($N&lt;D$), we present in explicit form an optimal
algorithm of computational cost $2^N$. For the case $N \geq D$, we present an
optimal algorithm of complexity $\mathcal O(N^D)$. We generalize to multiple
$L_1$-max-projection components and present an explicit optimal $L_1$ subspace
calculation algorithm of complexity $\mathcal O(N^{DK-K+1})$ where $K$ is the
desired number of $L_1$ principal components (subspace rank). We conclude with
illustrations of $L_1$-subspace signal processing in the fields of data
dimensionality reduction, direction-of-arrival estimation, and image
conditioning/restoration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6790</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6790</id><created>2014-05-27</created><updated>2014-12-31</updated><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author></authors><title>A PMU Scheduling Scheme for Transmission of Synchrophasor Data in
  Electric Power Systems</title><categories>cs.IT math.IT</categories><comments>9 pages, 6 figures; an extra figure included in the published
  version. appears in IEEE Transactions on Smart Grid, Special Issue on Cyber
  Physical Systems and Security for Smart Grid, 2015</comments><doi>10.1109/TSG.2014.2388238</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proposition to install a large number of phasor measurement units
(PMUs) in the future power grid, it is essential to provide robust
communications infrastructure for phasor data across the network. We make
progress in this direction by devising a simple time division multiplexing
scheme for transmitting phasor data from the PMUs to a central server: Time is
divided into frames and the PMUs take turns to transmit to the control center
within the time frame. The main contribution of this work is a scheduling
policy based on which PMU transmissions are ordered during a time frame.
  The scheduling scheme is independent of the approach taken to solve the PMU
placement problem, and unlike strategies devised for conventional
communications, it is intended for the power network since it is fully governed
by the measure of electrical connectedness between buses in the grid. To
quantify the performance of the scheduling scheme, we couple it with a fault
detection algorithm used to detect changes in the susceptance parameters in the
grid. Results demonstrate that scheduling the PMU transmissions leads to an
improved performance of the fault detection scheme compared to PMUs
transmitting at random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6791</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6791</id><created>2014-05-27</created><updated>2015-05-25</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author></authors><title>Agnostic Learning of Disjunctions on Symmetric Distributions</title><categories>cs.LG cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating and learning disjunctions (or
equivalently, conjunctions) on symmetric distributions over $\{0,1\}^n$.
Symmetric distributions are distributions whose PDF is invariant under any
permutation of the variables. We give a simple proof that for every symmetric
distribution $\mathcal{D}$, there exists a set of $n^{O(\log{(1/\epsilon)})}$
functions $\mathcal{S}$, such that for every disjunction $c$, there is function
$p$, expressible as a linear combination of functions in $\mathcal{S}$, such
that $p$ $\epsilon$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$ or
$\mathbf{E}_{x \sim \mathcal{D}}[ |c(x)-p(x)|] \leq \epsilon$. This directly
gives an agnostic learning algorithm for disjunctions on symmetric
distributions that runs in time $n^{O( \log{(1/\epsilon)})}$. The best known
previous bound is $n^{O(1/\epsilon^4)}$ and follows from approximation of the
more general class of halfspaces (Wimmer, 2010). We also show that there exists
a symmetric distribution $\mathcal{D}$, such that the minimum degree of a
polynomial that $1/3$-approximates the disjunction of all $n$ variables is
$\ell_1$ distance on $\mathcal{D}$ is $\Omega( \sqrt{n})$. Therefore the
learning result above cannot be achieved via $\ell_1$-regression with a
polynomial basis used in most other agnostic learning algorithms.
  Our technique also gives a simple proof that for any product distribution
$\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of
degree $O(\log{(1/\epsilon)})$ such that $p$ $\epsilon$-approximates $c$ in
$\ell_1$ distance on $\mathcal{D}$. This was first proved by Blais et al.
(2008) via a more involved argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6802</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6802</id><created>2014-05-27</created><authors><author><keyname>Conway</keyname><forenames>Andrew R</forenames></author><author><keyname>Guttmann</keyname><forenames>Anthony J</forenames></author></authors><title>On the growth rate of 1324-avoiding permutations</title><categories>math.CO cs.DS math-ph math.MP</categories><comments>20 pages, 10 figures</comments><msc-class>05A05, 05A15, 05A16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an improved algorithm for counting the number of $1324$-avoiding
permutations, resulting in 5 further terms of the generating function. We
analyse the known coefficients and find compelling evidence that unlike other
classical length-4 pattern-avoiding permutations, the generating function in
this case does not have an algebraic singularity. Rather, the number of
1324-avoiding permutations of length $n$ behaves as $$B\cdot \mu^n \cdot
\mu_1^{n^{\sigma}} \cdot n^g.$$ We estimate $\mu=11.60 \pm 0.01,$ $\sigma=1/2,$
$\mu_1 = 0.0398 \pm 0.0010,$ $g = -1.1 \pm 0.2$ and $B =9.5 \pm 1.0.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6804</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6804</id><created>2014-05-27</created><updated>2014-05-27</updated><authors><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author><author><keyname>Dollar</keyname><forenames>Piotr</forenames></author><author><keyname>Wu</keyname><forenames>Yingnian</forenames></author></authors><title>Layered Logic Classifiers: Exploring the `And' and `Or' Relations</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing effective and efficient classifier for pattern analysis is a key
problem in machine learning and computer vision. Many the solutions to the
problem require to perform logic operations such as `and', `or', and `not'.
Classification and regression tree (CART) include these operations explicitly.
Other methods such as neural networks, SVM, and boosting learn/compute a
weighted sum on features (weak classifiers), which weakly perform the 'and' and
'or' operations. However, it is hard for these classifiers to deal with the
'xor' pattern directly. In this paper, we propose layered logic classifiers for
patterns of complicated distributions by combining the `and', `or', and `not'
operations. The proposed algorithm is very general and easy to implement. We
test the classifiers on several typical datasets from the Irvine repository and
two challenging vision applications, object segmentation and pedestrian
detection. We observe significant improvements on all the datasets over the
widely used decision stump based AdaBoost algorithm. The resulting classifiers
have much less training complexity than decision tree based AdaBoost, and can
be applied in a wide range of domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6815</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6815</id><created>2014-05-27</created><authors><author><keyname>Gupta</keyname><forenames>Uma</forenames></author></authors><title>Research On Permanent Magnet BLDC for small electric vehicle</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, different electric motors are studied and compared to see the
benefits of each motor and the one that is more suitable to be used in the
electric vehicle applications. There are five main electric motor types, DC,
induction, permanent magnet synchronous, switched reluctance and brush-less DC
motors are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6817</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6817</id><created>2014-05-27</created><authors><author><keyname>Francois</keyname><forenames>Fouquet</forenames><affiliation>SnT</affiliation></author><author><keyname>Nain</keyname><forenames>Gr&#xe9;gory</forenames><affiliation>SnT</affiliation></author><author><keyname>Morin</keyname><forenames>Brice</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Daubert</keyname><forenames>Erwan</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Barais</keyname><forenames>Olivier</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Plouzeau</keyname><forenames>No&#xeb;l</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>J&#xe9;z&#xe9;quel</keyname><forenames>Jean-Marc</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Kevoree Modeling Framework (KMF): Efficient modeling techniques for
  runtime use</title><categories>cs.SE</categories><comments>ISBN 978-2-87971-131-7; N&amp;deg; TR-SnT-2014-11 (2014)</comments><proxy>ccsd</proxy><report-no>TR-SnT-2014-11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The creation of Domain Specific Languages(DSL) counts as one of the main
goals in the field of Model-Driven Software Engineering (MDSE). The main
purpose of these DSLs is to facilitate the manipulation of domain specific
concepts, by providing developers with specific tools for their domain of
expertise. A natural approach to create DSLs is to reuse existing modeling
standards and tools. In this area, the Eclipse Modeling Framework (EMF) has
rapidly become the defacto standard in the MDSE for building Domain Specific
Languages (DSL) and tools based on generative techniques. However, the use of
EMF generated tools in domains like Internet of Things (IoT), Cloud Computing
or Models@Runtime reaches several limitations. In this paper, we identify
several properties the generated tools must comply with to be usable in other
domains than desktop-based software systems. We then challenge EMF on these
properties and describe our approach to overcome the limitations. Our approach,
implemented in the Kevoree Modeling Framework (KMF), is finally evaluated
according to the identified properties and compared to EMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6820</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6820</id><created>2014-05-27</created><updated>2015-07-29</updated><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Hoffmann</keyname><forenames>Philipp</forenames></author></authors><title>Negotiation Games (with abstract)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Negotiations, a model of concurrency with multi party negotiation as
primitive, have been recently introduced in arXiv:1307.2145, arXiv:1403.4958.
We initiate the study of games for this model. We study coalition problems: can
a given coalition of agents force that a negotiation terminates (resp. block
the negotiation so that it goes on forever)?; can the coalition force a given
outcome of the negotiation? We show that for arbitrary negotiations the
problems are EXPTIME-complete. Then we show that for sound and deterministic or
even weakly deterministic negotiations the problems can be solved in PTIME.
Notice that the input of the problems is a negotiation, which can be
exponentially more compact than its state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6822</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6822</id><created>2014-05-27</created><authors><author><keyname>Zaidi</keyname><forenames>Hiba</forenames></author></authors><title>Mobile Application for GBAS Air Traffic Status Unit</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present, the Air Traffic Status Unit is a windows PC based application,
which receives the status of ground based augmentation system station over
Ethernet and displays on the screen. The objective of this project is to
convert the PC based Application into Mobile application using Android OS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6824</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6824</id><created>2014-05-27</created><authors><author><keyname>Lietz</keyname><forenames>Haiko</forenames></author><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Bleier</keyname><forenames>Arnim</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>When Politicians Talk: Assessing Online Conversational Practices of
  Political Parties on Twitter</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 pages, 2 figures, 3 tables, Proc. 8th International AAAI
  Conference on Weblogs and Social Media (ICWSM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing political conversations in social media requires a deeper
understanding of the underlying practices and styles that drive these
conversations. In this paper, we present a computational approach for assessing
online conversational practices of political parties. Following a deductive
approach, we devise a number of quantitative measures from a discussion of
theoretical constructs in sociological theory. The resulting measures make
different - mostly qualitative - aspects of online conversational practices
amenable to computation. We evaluate our computational approach by applying it
in a case study. In particular, we study online conversational practices of
German politicians on Twitter during the German federal election 2013. We find
that political parties share some interesting patterns of behavior, but also
exhibit some unique and interesting idiosyncrasies. Our work sheds light on (i)
how complex cultural phenomena such as online conversational practices are
amenable to quantification and (ii) the way social media such as Twitter are
utilized by political parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6843</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6843</id><created>2014-05-27</created><authors><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Xiao</keyname><forenames>Di</forenames></author></authors><title>Robust Coding of Encrypted Images via Structural Matrix</title><categories>cs.CR</categories><comments>10 pages, 11 figures</comments><doi>10.1016/j.image.2015.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust coding of natural images and the effective compression of
encrypted images have been studied individually in recent years. However,
little work has been done in the robust coding of encrypted images. The
existing results in these two individual research areas cannot be combined
directly for the robust coding of encrypted images. This is because the robust
coding of natural images relies on the elimination of spatial correlations
using sparse transforms such as discrete wavelet transform (DWT), which is
ineffective to encrypted images due to the weak correlation between encrypted
pixels. Moreover, the compression of encrypted images always generates code
streams with different significance. If one or more such streams are lost, the
quality of the reconstructed images may drop substantially or decoding error
may exist, which violates the goal of robust coding of encrypted images. In
this work, we intend to design a robust coder, based on compressive sensing
with structurally random matrix, for encrypted images over packet transmission
networks. The proposed coder can be applied in the scenario that Alice needs a
semi-trusted channel provider Charlie to encode and transmit the encrypted
image to Bob. In particular, Alice first encrypts an image using globally
random permutation and then sends the encrypted image to Charlie who samples
the encrypted image using a structural matrix. Through an imperfect channel
with packet loss, Bob receives the compressive measurements and reconstructs
the original image by joint decryption and decoding. Experimental results show
that the proposed coder can be considered as an efficient multiple description
coder with a number of descriptions against packet loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6845</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6845</id><created>2014-05-27</created><authors><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author></authors><title>A Measurement Framework for Directed Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 6 figures</comments><journal-ref>IEEE Journal on Selected Areas in Communications (JSAC): Special
  Issue on Network Science, 31(6):1007-1016, June 2013</journal-ref><doi>10.1109/JSAC.2013.130603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially-observed network data collected by link-tracing based sampling
methods is often being studied to obtain the characteristics of a large complex
network. However, little attention has been paid to sampling from directed
networks such as WWW and Peer-to-Peer networks. In this paper, we propose a
novel two-step (sampling/estimation) framework to measure nodal characteristics
which can be defined by an average target function in an arbitrary directed
network. To this end, we propose a personalized PageRank-based algorithm to
visit and sample nodes. This algorithm only uses already visited nodes as local
information without any prior knowledge about the latent structure of the
network. Moreover, we introduce a new estimator based on the approximate
importance sampling to estimate average target functions. The proposed
estimator utilizes calculated PageRank value of each sampled node as an
approximation for the exact visiting probability. To the best of our knowledge,
this is the first study on correcting the bias of a sampling method by
re-weighting of measured values that considers the effect of approximation of
visiting probabilities. Comprehensive theoretical and empirical analysis of the
estimator demonstrate that it is asymptotically unbiased even in situations
where stationary distribution of PageRank is poorly approximated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6851</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6851</id><created>2014-05-27</created><updated>2014-11-03</updated><authors><author><keyname>Ueno</keyname><forenames>Kenya</forenames></author></authors><title>Exact Algorithms for 0-1 Integer Programs with Linear Equality
  Constraints</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show $O(1.415^n)$-time and $O(1.190^n)$-space exact
algorithms for 0-1 integer programs where constraints are linear equalities and
coefficients are arbitrary real numbers. Our algorithms are quadratically
faster than exhaustive search and almost quadratically faster than an algorithm
for an inequality version of the problem by Impagliazzo, Lovett, Paturi and
Schneider (arXiv:1401.5512), which motivated our work. Rather than improving
the time and space complexity, we advance to a simple direction as inclusion of
many NP-hard problems in terms of exact exponential algorithms. Specifically,
we extend our algorithms to linear optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6866</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6866</id><created>2014-05-27</created><updated>2015-07-09</updated><authors><author><keyname>Kihara</keyname><forenames>Takayuki</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Point degree spectra of represented spaces</title><categories>math.GN cs.LO math.LO</categories><msc-class>03D78, 03D30, 54F45, 54H05, 46J10</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the point degree spectrum of a represented space as a
substructure of the Medvedev degrees, which integrates the notion of Turing
degrees, enumeration degrees, continuous degrees, and so on. The notion of
point degree spectrum creates a connection among various areas of mathematics
including computability theory, descriptive set theory, infinite dimensional
topology and Banach space theory. Through this new connection, for instance, we
construct a family of continuum many infinite dimensional Cantor manifolds with
property $C$ whose Borel structures at an arbitrary finite rank are mutually
non-isomorphic. This provides new examples of Banach algebras of real valued
Baire class two functions on metrizable compacta, and strengthen various
theorems in infinite dimensional topology such as Pol's solution to
Alexandrov's old problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6874</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6874</id><created>2014-05-27</created><updated>2014-09-18</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author><author><keyname>Roguski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Disk-based genome sequencing data compression</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: High-coverage sequencing data have significant, yet hard to
exploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA
stream of large datasets, since the redundancy between overlapping reads cannot
be easily captured in the (relatively small) main memory. More interesting
solutions for this problem are disk-based~(Yanovsky, 2011; Cox et al., 2012),
where the better of these two, from Cox~{\it et al.}~(2012), is based on the
Burrows--Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0
Gb human genome sequencing collection with almost 45-fold coverage.
  Results: We propose ORCOM (Overlapping Reads COmpression with Minimizers), a
compression algorithm dedicated to sequencing reads (DNA only). Our method
makes use of a conceptually simple and easily parallelizable idea of
minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to
fit the 134.0 Gb dataset into only 5.31 GB of space.
  Availability: http://sun.aei.polsl.pl/orcom under a free license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6879</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6879</id><created>2014-05-27</created><updated>2014-10-23</updated><authors><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Complex contagion process in spreading of online innovation</title><categories>physics.soc-ph cs.SI nlin.AO physics.comp-ph physics.data-an</categories><comments>27 pages, 11 figures, 2 tables</comments><journal-ref>J. R. Soc. Interface 11, 101 (2014)</journal-ref><doi>10.1098/rsif.2014.0694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of innovation can be interpreted as a social spreading phenomena
governed by the impact of media and social interactions. Although these
mechanisms have been identified by quantitative theories, their role and
relative importance are not entirely understood, since empirical verification
has so far been hindered by the lack of appropriate data. Here we analyse a
dataset recording the spreading dynamics of the world's largest Voice over
Internet Protocol service to empirically support the assumptions behind models
of social contagion. We show that the rate of spontaneous service adoption is
constant, the probability of adoption via social influence is linearly
proportional to the fraction of adopting neighbours, and the rate of service
termination is time-invariant and independent of the behaviour of peers. By
implementing the detected diffusion mechanisms into a dynamical agent-based
model, we are able to emulate the adoption dynamics of the service in several
countries worldwide. This approach enables us to make medium-term predictions
of service adoption and disclose dependencies between the dynamics of
innovation spreading and the socioeconomic development of a country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6880</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6880</id><created>2014-05-27</created><authors><author><keyname>Schuh</keyname><forenames>Fabian</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Low Complexity Decoding for Higher Order Punctured Trellis-Coded
  Modulation Over Intersymbol Interference Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures, 3 algorithms, accepted and published at 6th
  International Symposium on Communications, Control, and Signal Processing
  (ISCCSP 2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Trellis-coded modulation (TCM) is a power and bandwidth efficient digital
transmission scheme which offers very low structural delay of the data stream.
Classical TCM uses a signal constellation of twice the cardinality compared to
an uncoded transmission with one bit of redundancy per PAM symbol, i.e.,
application of codes with rates $\frac{n-1}{n}$ when $2^{n}$ denotes the
cardinality of the signal constellation.
  Recently published work allows rate adjustment for TCM by means of puncturing
the convolutional code (CC) on which a TCM scheme is based on.
  In this paper it is shown how punctured TCM-signals transmitted over
intersymbol interference (ISI) channels can favorably be decoded. Significant
complexity reductions at only minor performance loss can be achieved by means
of reduced state sequence estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6885</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6885</id><created>2014-05-27</created><updated>2014-05-28</updated><authors><author><keyname>Santos</keyname><forenames>Renato P. dos</forenames></author><author><keyname>Roque</keyname><forenames>Waldir L.</forenames></author></authors><title>On the design of an expert help system for computer algebra systems</title><categories>cs.SC gr-qc</categories><comments>4 pages; fixed author names</comments><acm-class>I.1; J.2</acm-class><journal-ref>ACM SIGSAM Bulletin (1990) 24(4):22-25</journal-ref><doi>10.1145/101108.101110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is our intention here only to discuss the nature, complexity and tools
concerning the design of Smart Help, an expert help facility for aiding users
of Computer Algebra Systems. Although the expert help system presented here has
been particularly oriented to REDUCE (as a consequence of our former experience
with this system), we point out that the concept of Smart Help can be extended
to other Computer Algebra Systems. Technically, Smart Help is a Production
System on the top of a particular implementation of MANTRA, a hybrid knowledge
representation system, which has REDUCE integrated as an additional knowledge
representation module. Since the heuristic level of MANTRA has not yet been
implemented, being presently represented by the Lisp language itself, Smart
Help is coded in Lisp and resides in the same Lisp session of MANTRA. A
prototype of Smart Help is now running on a SUN work-station on an experimental
basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6886</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6886</id><created>2014-05-27</created><authors><author><keyname>Troelsg&#xe5;rd</keyname><forenames>Rasmus</forenames></author><author><keyname>Jensen</keyname><forenames>Bj&#xf8;rn Sand</forenames></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author></authors><title>A Topic Model Approach to Multi-Modal Similarity</title><categories>cs.IR stat.ML</categories><comments>topic modelling workshop at NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating similarities between objects defined by many heterogeneous data
modalities is an important challenge in many multimedia applications. We use a
multi-modal topic model as a basis for defining such a similarity between
objects. We propose to compare the resulting similarities from different model
realizations using the non-parametric Mantel test. The approach is evaluated on
a music dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6893</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6893</id><created>2014-05-27</created><updated>2016-02-17</updated><authors><author><keyname>Keranen</keyname><forenames>Melissa</forenames></author><author><keyname>Lauri</keyname><forenames>Juho</forenames></author></authors><title>Computing Minimum Rainbow and Strong Rainbow Colorings of Block Graphs</title><categories>cs.DM</categories><comments>12 pages, 3 figures</comments><msc-class>68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A path in an edge-colored graph $G$ is \emph{rainbow} if no two edges of it
are colored the same. The graph $G$ is \emph{rainbow colored} if there is a
rainbow path between every pair of vertices. If there is a rainbow shortest
path between every pair of vertices, the graph $G$ is \emph{strong rainbow
colored}. The minimum number of colors needed to make $G$ rainbow colored is
known as the \emph{rainbow connection number}, and is denoted by $\rc(G)$.
Similarly, the minimum number of colors needed to make $G$ strong rainbow
colored is known as the \emph{strong rainbow connection number}, and is denoted
by $\src(G)$. We prove that for every $k \geq 3$, deciding whether $\src(G)
\leq k$ is $\NP$-complete for split graphs, which form a subclass of chordal
graphs. Furthermore, there exists no polynomial time algorithm for
approximating the strong rainbow connection number of an $n$-vertex split graph
with a factor of $n^{1/2-\epsilon}$ for any $\epsilon &gt; 0$ unless $\P = \NP$.
We then consider the rainbow and strong rainbow connection numbers of block
graphs, which also form a subclass of chordal graphs. We give an exact linear
time algorithm for strong rainbow coloring block graphs exploiting a clique
tree representation each chordal graph has. We also derive a tight upper bound
of $|S|+2$ on $\rc(G)$, where $G$ is a block graph, and $S$ its set of minimal
separators. Finally, we provide a polynomial-time characterization of
bridgeless block graphs with rainbow connection number at most 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6899</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6899</id><created>2014-05-27</created><authors><author><keyname>Pedersen</keyname><forenames>Truls</forenames></author><author><keyname>Dyrkolbotn</keyname><forenames>Sjur</forenames></author><author><keyname>Ka&#x17a;mierczak</keyname><forenames>Piotr</forenames></author></authors><title>Big, but not unruly: Tractable norms for anonymous game structures</title><categories>cs.MA cs.LO</categories><comments>Accepted at COIN@PRIMA 2013 workshop and presented on December 3rd,
  2013 in Dunedin, New Zealand. http://coin2013-prima.tudelft.nl/</comments><msc-class>68T27, 68T42</msc-class><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new strategic logic NCHATL that allows for reasoning about norm
compliance on concurrent game structures that satisfy anonymity. We represent
such game structures compactly, avoiding models that have exponential size in
the number of agents. Then we show that model checking can be done in
polynomial time with respect to this compact representation, even for normative
systems that are not anonymous. That is, as long as the underlying game
structures are anonymous, model checking normative formulas is tractable even
if norms can prescribe different sets of forbidden actions to different agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6908</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6908</id><created>2014-05-27</created><authors><author><keyname>Larrousse</keyname><forenames>Benjamin</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Coded Power Control: Performance Analysis</title><categories>cs.IT math.IT</categories><comments>Information Theory Proceedings (ISIT), 2013 IEEE International
  Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the general concept of coded power control (CPC)
in a particular setting of the interference channel. Roughly, the idea of CPC
consists in embedding information (about a random state) into the transmit
power levels themselves: in this new framework, provided the power levels of a
given transmitter can be observed (through a noisy channels) by other
transmitters, a sequence of power levels of the former can therefore be used to
coordinate the latter. To assess the limiting performance of CPC (and therefore
the potential performance brought by this new approach), we derive, as a first
step towards many extensions of the present work, a general result which not
only concerns power control (PC) but also any scenario involving two
decision-makers (DMs) which communicate through their actions and have the
following information and decision structures. We assume that the DMs want to
maximize the average of an arbitrarily chosen instantaneous payoff function
which depends on the DMs' actions and the state realization. DM 1 is assumed to
know non-causally the state (e.g., the channel state) which affects the common
payoff while the other, say DM 2, has only a strictly causal knowledge of it.
DM 1 can only use its own actions (e.g., power levels) to inform DM 2 about its
best action in terms of payoff. Importantly, DM 2 can only monitor the actions
of DM 1 imperfectly and DM 1 does not observe DM 2. The latter assumption leads
us to exploiting Shannon-theoretic tools in order to generalize an existing
theorem which provides the information constraint under which the payoff is
maximized. The derived result is then exploited to fully characterize the
performance of good CPC policies for a given instance of the interference
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6912</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6912</id><created>2014-05-27</created><authors><author><keyname>Peroli</keyname><forenames>Michele</forenames></author><author><keyname>Vigan&#xf2;</keyname><forenames>Luca</forenames></author><author><keyname>Zavatteri</keyname><forenames>Matteo</forenames></author></authors><title>Non-collaborative Attackers and How and Where to Defend Flawed Security
  Protocols (Extended Version)</title><categories>cs.CR</categories><comments>29 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Security protocols are often found to be flawed after their deployment. We
present an approach that aims at the neutralization or mitigation of the
attacks to flawed protocols: it avoids the complete dismissal of the interested
protocol and allows honest agents to continue to use it until a corrected
version is released. Our approach is based on the knowledge of the network
topology, which we model as a graph, and on the consequent possibility of
creating an interference to an ongoing attack of a Dolev-Yao attacker, by means
of non-collaboration actuated by ad-hoc benign attackers that play the role of
network guardians. Such guardians, positioned in strategical points of the
network, have the task of monitoring the messages in transit and discovering at
runtime, through particular types of inference, whether an attack is ongoing,
interrupting the run of the protocol in the positive case. We study not only
how but also where we can attempt to defend flawed security protocols: we
investigate the different network topologies that make security protocol
defense feasible and illustrate our approach by means of concrete examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6914</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6914</id><created>2014-05-27</created><authors><author><keyname>Ivek</keyname><forenames>Ivan</forenames></author></authors><title>Supervised Dictionary Learning by a Variational Bayesian Group Sparse
  Nonnegative Matrix Factorization</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) with group sparsity constraints is
formulated as a probabilistic graphical model and, assuming some observed data
have been generated by the model, a feasible variational Bayesian algorithm is
derived for learning model parameters. When used in a supervised learning
scenario, NMF is most often utilized as an unsupervised feature extractor
followed by classification in the obtained feature subspace. Having mapped the
class labels to a more general concept of groups which underlie sparsity of the
coefficients, what the proposed group sparse NMF model allows is incorporating
class label information to find low dimensional label-driven dictionaries which
not only aim to represent the data faithfully, but are also suitable for class
discrimination. Experiments performed in face recognition and facial expression
recognition domains point to advantages of classification in such label-driven
feature subspaces over classification in feature subspaces obtained in an
unsupervised manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6920</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6920</id><created>2014-05-27</created><updated>2014-08-30</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Bogdan</forenames></author></authors><title>On the Relation Between the Randomized Extended Kaczmarz Algorithm and
  Coordinate Descent</title><categories>math.NA cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we compare the randomized extended Kaczmarz (EK) algorithm and
randomized coordinate descent (CD) for solving the full-rank overdetermined
linear least-squares problem and prove that CD needs less operations for
satisfying the same residual-related termination criteria. For the general
least-squares problems, we show that running first CD to compute the residual
and then standard Kaczmarz on the resulting consistent system is more efficient
than EK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6922</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6922</id><created>2014-05-27</created><authors><author><keyname>Aghazadeh</keyname><forenames>Omid</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>Large Scale, Large Margin Classification using Indefinite Similarity
  Measures</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the success of the popular kernelized support vector machines, they
have two major limitations: they are restricted to Positive Semi-Definite (PSD)
kernels, and their training complexity scales at least quadratically with the
size of the data. Many natural measures of similarity between pairs of samples
are not PSD e.g. invariant kernels, and those that are implicitly or explicitly
defined by latent variable models. In this paper, we investigate scalable
approaches for using indefinite similarity measures in large margin frameworks.
In particular we show that a normalization of similarity to a subset of the
data points constitutes a representation suitable for linear classifiers. The
result is a classifier which is competitive to kernelized SVM in terms of
accuracy, despite having better training and test time complexities.
Experimental results demonstrate that on CIFAR-10 dataset, the model equipped
with similarity measures invariant to rigid and non-rigid deformations, can be
made more than 5 times sparser while being more accurate than kernelized SVM
using RBF kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6929</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6929</id><created>2014-05-27</created><updated>2014-11-30</updated><authors><author><keyname>Jim&#xe9;nez</keyname><forenames>Andrea</forenames></author><author><keyname>Loebl</keyname><forenames>Martin</forenames></author></authors><title>Directed cycle double covers and cut-obstacles</title><categories>math.CO cs.DM</categories><comments>24 pages, 20 figures. This version contains several improvements; in
  particular, the main result is strengthened</comments><msc-class>05C38, 05C40, 05C70</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A directed cycle double cover of a graph G is a family of cycles of G, each
provided with an orientation, such that every edge of G is covered by exactly
two oppositely directed cycles. Explicit obstacles to the existence of a
directed cycle double cover in a graph are bridges. Jaeger conjectured that
bridges are actually the only obstacles. One of the difficulties in proving the
Jaeger's conjecture lies in discovering and avoiding obstructions to partial
strategies that, if successful, create directed cycle double covers. In this
work, we suggest a way to circumvent this difficulty. We formulate a conjecture
on graph connections, whose validity follows by the successful avoidance of one
cut-type obstruction that we call cut-obstacles. The main result of this work
claims that our 'cut-obstacles avoidance conjecture' already implies Jaeger's
directed cycle double cover conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6939</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6939</id><created>2014-05-27</created><authors><author><keyname>Christ</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Hoenicke</keyname><forenames>Jochen</forenames></author></authors><title>Weakly Equivalent Arrays</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (extensional) theory of arrays is widely used to model systems. Hence,
efficient decision procedures are needed to model check such systems. Current
decision procedures for the theory of arrays saturate the read-over-write and
extensionality axioms originally proposed by McCarthy. Various filters are used
to limit the number of axiom instantiations while preserving completeness. We
present an algorithm that lazily instantiates lemmas based on weak equivalence
classes. These lemmas are easier to interpolate as they only contain existing
terms. We formally define weak equivalence and show correctness of the
resulting decision procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6945</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6945</id><created>2014-05-25</created><authors><author><keyname>Gully</keyname><forenames>A.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Sparsity-Aware Filtered-X Affine Projection Algorithms for Active Noise
  Control</title><categories>cs.SD</categories><comments>5 figures, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel technique for promoting sparsity in the modified
filtered-x algorithms required for active noise control. The proposed
algorithms are based on recent techniques incorporating approximations to the
\ell_0-norm in the cost functions that are used to derive adaptive filtering
algorithms. In particular, zero-attracting and reweighted zero-attracting
filtered-x adaptive algorithms are developed and considered for active noise
control problems. The results of simulations indicate that the proposed
techniques improve the convergence of the existing modified algorithm in the
case where the primary and secondary paths exhibit a degree of sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6948</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6948</id><created>2014-05-27</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Multidimensional Manifold Extraction for Multicarrier
  Continuous-Variable Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>39 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the multidimensional manifold extraction for multicarrier
continuous-variable (CV) quantum key distribution (QKD). The manifold
extraction utilizes the resources that are injected into the transmission by
the additional degrees of freedom of the multicarrier modulation. We
demonstrate the results through the AMQD (adaptive multicarrier quadrature
division) scheme, which granulates the information into Gaussian subcarrier CVs
and divides the physical link into several Gaussian sub-channels for the
transmission. We prove that the exploitable extra degree of freedom in a
multicarrier CVQKD scenario significantly extends the possibilities of
single-carrier CVQKD. The manifold extraction allows for the parties to reach
decreased error probabilities by utilizing those extra resources of a
multicarrier transmission that are not available in a single-carrier CVQKD
setting. We define the multidimensional manifold space of multicarrier CVQKD
and the optimal tradeoff between the available degrees of freedom of the
multicarrier transmission. We also extend the manifold extraction for the
multiple-access AMQD-MQA (multiuser quadrature allocation) multicarrier
protocol. The additional resources of multicarrier CVQKD allow the achievement
of significant performance improvements that are particularly crucial in an
experimental scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6952</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6952</id><created>2014-05-27</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Zhu</keyname><forenames>Hongbo</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author></authors><title>Power Scaling of Uplink Massive MIMO Systems with Arbitrary-Rank Channel
  Means</title><categories>cs.IT math.IT</categories><doi>10.1109/JSTSP.2014.2324534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the uplink achievable rates of massive multiple-input
multiple-output (MIMO) antenna systems in Ricean fading channels, using
maximal-ratio combining (MRC) and zero-forcing (ZF) receivers, assuming perfect
and imperfect channel state information (CSI). In contrast to previous relevant
works, the fast fading MIMO channel matrix is assumed to have an arbitrary-rank
deterministic component as well as a Rayleigh-distributed random component. We
derive tractable expressions for the achievable uplink rate in the
large-antenna limit, along with approximating results that hold for any finite
number of antennas. Based on these analytical results, we obtain the scaling
law that the users' transmit power should satisfy, while maintaining a
desirable quality of service. In particular, it is found that regardless of the
Ricean $K$-factor, in the case of perfect CSI, the approximations converge to
the same constant value as the exact results, as the number of base station
antennas, $M$, grows large, while the transmit power of each user can be scaled
down proportionally to $1/M$. If CSI is estimated with uncertainty, the same
result holds true but only when the Ricean $K$-factor is non-zero. Otherwise,
if the channel experiences Rayleigh fading, we can only cut the transmit power
of each user proportionally to $1/\sqrt M$. In addition, we show that with an
increasing Ricean $K$-factor, the uplink rates will converge to fixed values
for both MRC and ZF receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6953</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6953</id><created>2014-05-27</created><authors><author><keyname>Farkas</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Haddock</keyname><forenames>Stephen</forenames></author><author><keyname>Saltsidis</keyname><forenames>Panagiotis</forenames></author></authors><title>Software Defined Networking Supported by IEEE 802.1Q</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview on how Software Defined Networking (SDN)
principles can be applied to existing Ethernet merchant silicon considering the
requirements modern networks face. We show that existing Layer 2 features
specified by IEEE 802.1Q support SDN. The bridge architecture supports control
plane/data plane split by design and also allows for external control e.g. by
an SDN Controller. The data plane provided by existing chips is feature rich
for network virtualization and supports even more features like OAM. We outline
the principles of SDN over bridges and show a number of possibilities for
further research and development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6974</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6974</id><created>2014-05-27</created><authors><author><keyname>Kuhn</keyname><forenames>Max</forenames></author></authors><title>Futility Analysis in the Cross-Validation of Machine Learning Models</title><categories>stat.ML cs.LG</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning models have important structural tuning parameters that
cannot be directly estimated from the data. The common tactic for setting these
parameters is to use resampling methods, such as cross--validation or the
bootstrap, to evaluate a candidate set of values and choose the best based on
some pre--defined criterion. Unfortunately, this process can be time consuming.
However, the model tuning process can be streamlined by adaptively resampling
candidate values so that settings that are clearly sub-optimal can be
discarded. The notion of futility analysis is introduced in this context. An
example is shown that illustrates how adaptive resampling can be used to reduce
training time. Simulation studies are used to understand how the potential
speed--up is affected by parallel processing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6985</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6985</id><created>2014-05-26</created><authors><author><keyname>Ahmed</keyname><forenames>Waqar</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author><author><keyname>Tahar</keyname><forenames>Sofiene</forenames></author><author><keyname>Hamdi</keyname><forenames>Mohammad Salah</forenames></author></authors><title>Towards the Formal Reliability Analysis of Oil and Gas Pipelines</title><categories>cs.LO</categories><comments>15 pages</comments><journal-ref>Intelligent Computer Mathematics, pp. 30-44. (Lecture Notes in
  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014</journal-ref><doi>10.1007/978-3-319-08434-3_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is customary to assess the reliability of underground oil and gas
pipelines in the presence of excessive loading and corrosion effects to ensure
a leak-free transport of hazardous materials. The main idea behind this
reliability analysis is to model the given pipeline system as a Reliability
Block Diagram (RBD) of segments such that the reliability of an individual
pipeline segment can be represented by a random variable. Traditionally,
computer simulation is used to perform this reliability analysis but it
provides approximate results and requires an enormous amount of CPU time for
attaining reasonable estimates. Due to its approximate nature, simulation is
not very suitable for analyzing safety-critical systems like oil and gas
pipelines, where even minor analysis flaws may result in catastrophic
consequences. As an accurate alternative, we propose to use a
higher-order-logic theorem prover (HOL) for the reliability analysis of
pipelines. As a first step towards this idea, this paper provides a
higher-order-logic formalization of reliability and the series RBD using the
HOL theorem prover. For illustration, we present the formal analysis of a
simple pipeline that can be modeled as a series RBD of segments with
exponentially distributed failure times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6987</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6987</id><created>2014-05-27</created><updated>2015-10-23</updated><authors><author><keyname>Checco</keyname><forenames>Alessandro</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Fast, Responsive Decentralised Graph Colouring</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve, in a fully decentralised way (\ie with no message passing), the
classic problem of colouring a graph. We propose a novel algorithm that is
automatically responsive to topology changes, and we prove that it converges
quickly to a proper colouring in $O(N\log{N})$ time with high probability for
generic graphs (and in $O(\log{N})$ time if $\Delta=o(N)$) when the number of
available colours is greater than $\Delta$, the maximum degree of the graph.
  We believe the proof techniques used in this work are of independent interest
and provide new insight into the properties required to ensure fast convergence
of decentralised algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.6989</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.6989</id><created>2014-05-20</created><authors><author><keyname>Fellman</keyname><forenames>Philip Vos</forenames></author><author><keyname>Wright</keyname><forenames>Roxana</forenames></author></authors><title>Modeling Terrorist Networks, Complex Systems at the Mid-range</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 2 figures; The Intelligencer, Journal of U.S. Intelligence
  Studies, Vol. 14, No. 1 Winter/Spring 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop the themes presented at the 2003 Joint Complexity
Conference at the London School of Economics and subsequently published in The
Intelligencer (2004) and O Tempo Das Redes (2008). Following the data analysis
of the 9/11 high-jacker network developed by Valdis Krebs from open sources, we
apply social network theory to examine salient arguments regarding terrorism as
seen from the standpoint of complex adaptive systems theory. In particular, we
explore the concepts of group cohesion, adhesion and alternative network
mappings derived from node removal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7011</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7011</id><created>2014-05-27</created><authors><author><keyname>M&#xe9;ndez-D&#xed;az</keyname><forenames>Isabel</forenames></author><author><keyname>Nasini</keyname><forenames>Graciela</forenames></author><author><keyname>Sever&#xed;n</keyname><forenames>Daniel</forenames></author></authors><title>An exact DSatur-based algorithm for the Equitable Coloring Problem</title><categories>cs.DM</categories><journal-ref>Electronic Notes in Discrete Mathematics. Volume 44 (2013), p.
  281--286</journal-ref><doi>10.1016/j.endm.2013.10.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an exact algorithm for the Equitable Coloring Problem,
based on the well known DSatur algorithm for the classic Coloring Problem with
new pruning rules specifically derived from the equity constraint.
Computational experiences show that our algorithm is competitive with those
known in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7012</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7012</id><created>2014-05-27</created><authors><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>H&#xf6;lzl</keyname><forenames>Johannes</forenames></author><author><keyname>Serafin</keyname><forenames>Luke</forenames></author></authors><title>A formally verified proof of the Central Limit Theorem</title><categories>cs.MS cs.LO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a formally verified proof of the Central Limit Theorem in the
Isabelle proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7014</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7014</id><created>2014-05-27</created><authors><author><keyname>McKilliam</keyname><forenames>Robby G.</forenames></author><author><keyname>Grant</keyname><forenames>Alex</forenames></author><author><keyname>Clarkson</keyname><forenames>I. Vaughan L.</forenames></author></authors><title>Finding a closest point in a lattice of Voronoi's first kind</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for those lattices of Voronoi's first kind with known obtuse
superbasis, a closest lattice point can be computed in $O(n^4)$ operations
where $n$ is the dimension of the lattice. To achieve this a series of relevant
lattice vectors that converges to a closest lattice point is found. We show
that the series converges after at most $n$ terms. Each vector in the series
can be efficiently computed in $O(n^3)$ operations using an algorithm to
compute a minimum cut in an undirected flow network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7020</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7020</id><created>2014-05-27</created><authors><author><keyname>D&#xed;az</keyname><forenames>Isabel M&#xe9;ndez</forenames></author><author><keyname>Nasini</keyname><forenames>Graciela</forenames></author><author><keyname>Sever&#xed;n</keyname><forenames>Daniel</forenames></author></authors><title>A tabu search heuristic for the Equitable Coloring Problem</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Equitable Coloring Problem is a variant of the Graph Coloring Problem
where the sizes of two arbitrary color classes differ in at most one unit. This
additional condition, called equity constraints, arises naturally in several
applications. Due to the hardness of the problem, current exact algorithms can
not solve large-sized instances. Such instances must be addressed only via
heuristic methods. In this paper we present a tabu search heuristic for the
Equitable Coloring Problem. This algorithm is an adaptation of the dynamic
TabuCol version of Galinier and Hao. In order to satisfy equity constraints,
new local search criteria are given. Computational experiments are carried out
in order to find the best combination of parameters involved in the dynamic
tenure of the heuristic. Finally, we show the good performance of our heuristic
over known benchmark instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7028</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7028</id><created>2014-05-27</created><authors><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>Pseudorandomness and Fourier Growth Bounds for Width 3 Branching
  Programs</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1306.3004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an explicit pseudorandom generator for oblivious, read-once,
width-$3$ branching programs, which can read their input bits in any order. The
generator has seed length $\tilde{O}( \log^3 n ).$ The previously best known
seed length for this model is $n^{1/2+o(1)}$ due to Impagliazzo, Meka, and
Zuckerman (FOCS '12). Our work generalizes a recent result of Reingold,
Steinke, and Vadhan (RANDOM '13) for \textit{permutation} branching programs.
The main technical novelty underlying our generator is a new bound on the
Fourier growth of width-3, oblivious, read-once branching programs.
Specifically, we show that for any $f:\{0,1\}^n\rightarrow \{0,1\}$ computed by
such a branching program, and $k\in [n],$ $$\sum_{s\subseteq [n]: |s|=k} \left|
\hat{f}[s] \right| \leq n^2 \cdot (O(\log n))^k,$$ where $\widehat{f}[s] =
\mathbb{E}\left[f[U] \cdot (-1)^{s \cdot U}\right]$ is the standard Fourier
transform over $\mathbb{Z}_2^n$. The base $O(\log n)$ of the Fourier growth is
tight up to a factor of $\log \log n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7032</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7032</id><created>2014-05-27</created><authors><author><keyname>Tao</keyname><forenames>Luo</forenames></author><author><keyname>zaifeng</keyname><forenames>Shi</forenames></author></authors><title>An FPGA-based Parallel Architecture for Face Detection using Mixed Color
  Models</title><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a reliable method for detecting human faces in color images is
proposed. This system firstly detects skin color in YCgCr and YIQ color space,
then filters binary texture and the result is morphological processed, finally
converts skin tone to the preferred skin color configured by users in YIQ color
space. The real-time adjusting circuit is implemented and some of simulation
results are given out. Experimental results demonstrate that the method has
achieved high rates and low false positives, another advantage is its
simplicity and minor computational costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7058</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7058</id><created>2014-05-27</created><authors><author><keyname>Rathnayake</keyname><forenames>Asiri</forenames></author><author><keyname>Thielecke</keyname><forenames>Hayo</forenames></author></authors><title>Static Analysis for Regular Expression Exponential Runtime via
  Substructural Logics</title><categories>cs.PL cs.LO</categories><comments>(Under review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular expression matching using backtracking can have exponential runtime,
leading to an algorithmic complexity attack known as REDoS in the systems
security literature. In this paper, we build on a recently published static
analysis that detects whether a given regular expression can have exponential
runtime for some inputs. We systematically construct a more accurate analysis
by forming powers and products of transition relations and thereby reducing the
REDoS problem to reachability. The correctness of the analysis is proved using
a substructural calculus of search trees, where the branching of the tree
causing exponential blowup is characterized as a form of non-linearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7076</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7076</id><created>2014-05-27</created><updated>2014-08-20</updated><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>On minimal sets of graded attribute implications</title><categories>cs.AI</categories><msc-class>68P20, 68T30, 03B52</msc-class><acm-class>H.2.8; H.3.3</acm-class><journal-ref>Information Sciences 294 (2015), 478-488</journal-ref><doi>10.1016/j.ins.2014.09.059</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the structure of non-redundant and minimal sets consisting of
graded if-then rules. The rules serve as graded attribute implications in
object-attribute incidence data and as similarity-based functional dependencies
in a similarity-based generalization of the relational model of data. Based on
our observations, we derive a polynomial-time algorithm which transforms a
given finite set of rules into an equivalent one which has the least size in
terms of the number of rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7084</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7084</id><created>2014-05-27</created><authors><author><keyname>Ferreira</keyname><forenames>Ronedo</forenames></author><author><keyname>Moreira</keyname><forenames>Waldir</forenames></author><author><keyname>Mendes</keyname><forenames>Paulo</forenames></author><author><keyname>Gerla</keyname><forenames>Mario</forenames></author><author><keyname>Cerqueira</keyname><forenames>Eduardo</forenames></author></authors><title>Improving the Delivery Rate of Digital Inclusion Applications for Amazon
  Riverside Communities by Using an Integrated Bluetooth DTN Architecture</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the evolution in deployed infrastructure and in the way that people
access information, still there are those who are socially excluded and have no
access to information due to their geographic location (e.g.,
riverside/countryside communities). This paper proposes an extension to a DTN
architecture implementation to allow the dissemination of information in such
communities, including educational short-video clips and audio books. The
IBR-DTN architecture is complemented with a Bluetooth Convergence Layer, to
facilitate the exchange of information over this short-range wireless
technology, and with a Bundle Compression mechanism that aims at improving data
exchange in short-lived opportunistic contacts happening among nodes.
Experiments in a small-scale testbed and in a large-scale simulator environment
show that nodes are indeed able to efficiently use contact opportunities to
exchange an increased amount of data, allowing people in riverside communities
to receive more content related to digital inclusion services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7085</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7085</id><created>2014-05-27</created><updated>2014-10-17</updated><authors><author><keyname>Bassily</keyname><forenames>Raef</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author><author><keyname>Thakurta</keyname><forenames>Abhradeep</forenames></author></authors><title>Differentially Private Empirical Risk Minimization: Efficient Algorithms
  and Tight Error Bounds</title><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we initiate a systematic investigation of differentially
private algorithms for convex empirical risk minimization. Various
instantiations of this problem have been studied before. We provide new
algorithms and matching lower bounds for private ERM assuming only that each
data point's contribution to the loss function is Lipschitz bounded and that
the domain of optimization is bounded. We provide a separate set of algorithms
and matching lower bounds for the setting in which the loss functions are known
to also be strongly convex.
  Our algorithms run in polynomial time, and in some cases even match the
optimal non-private running time (as measured by oracle complexity). We give
separate algorithms (and lower bounds) for $(\epsilon,0)$- and
$(\epsilon,\delta)$-differential privacy; perhaps surprisingly, the techniques
used for designing optimal algorithms in the two cases are completely
different.
  Our lower bounds apply even to very simple, smooth function families, such as
linear and quadratic functions. This implies that algorithms from previous work
can be used to obtain optimal error rates, under the additional assumption that
the contributions of each data point to the loss function is smooth. We show
that simple approaches to smoothing arbitrary loss functions (in order to apply
previous techniques) do not yield optimal error rates. In particular, optimal
algorithms were not previously known for problems such as training support
vector machines and the high-dimensional median.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7092</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7092</id><created>2014-05-27</created><updated>2015-01-13</updated><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Clique-width of Graph Classes Defined by Two Forbidden Induced Subgraphs</title><categories>cs.DM math.CO</categories><comments>24 pages, An extended abstract of this paper will appear in the
  proceedings of CIAC 2015</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a graph has no induced subgraph isomorphic to any graph in a finite family
$\{H_1,\ldots,H_p\}$, it is said to be $(H_1,\ldots,H_p)$-free. The class of
$H$-free graphs has bounded clique-width if and only if $H$ is an induced
subgraph of the 4-vertex path $P_4$. We study the (un)boundedness of the
clique-width of graph classes defined by two forbidden induced subgraphs $H_1$
and $H_2$. Prior to our study it was not known whether the number of open cases
was finite. We provide a positive answer to this question. To reduce the number
of open cases we determine new graph classes of bounded clique-width and new
graph classes of unbounded clique-width. For obtaining the latter results we
first present a new, generic construction for graph classes of unbounded
clique-width. Our results settle the boundedness or unboundedness of the
clique-width of the class of $(H_1,H_2)$-free graphs (i) for all pairs
$(H_1,H_2)$, both of which are connected, except two non-equivalent cases, and
(ii) for all pairs $(H_1,H_2)$, at least one of which is not connected, except
11 non-equivalent cases.
  We also consider classes characterized by forbidding a finite family of
graphs $\{H_1,\ldots,H_p\}$ as subgraphs, minors and topological minors,
respectively, and completely determine which of these classes have bounded
clique-width. Finally, we show algorithmic consequences of our results for the
graph colour
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7094</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7094</id><created>2014-05-27</created><authors><author><keyname>Powell</keyname><forenames>Alexander M.</forenames></author><author><keyname>Whitehouse</keyname><forenames>J. Tyler</forenames></author></authors><title>Error bounds for consistent reconstruction: random polytopes and
  coverage processes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistent reconstruction is a method for producing an estimate
$\widetilde{x} \in \mathbb{R}^d$ of a signal $x\in \mathbb{R}^d$ if one is
given a collection of $N$ noisy linear measurements $q_n = \langle x, \varphi_n
\rangle + \epsilon_n$, $1 \leq n \leq N$, that have been corrupted by i.i.d.
uniform noise $\{\epsilon_n\}_{n=1}^N$. We prove mean squared error bounds for
consistent reconstruction when the measurement vectors
$\{\varphi_n\}_{n=1}^N\subset \mathbb{R}^d$ are drawn independently at random
from a suitable distribution on the unit-sphere $\mathbb{S}^{d-1}$. Our main
results prove that the mean squared error (MSE) for consistent reconstruction
is of the optimal order $\mathbb{E}\|x - \widetilde{x}\|^2 \leq K\delta^2/N^2$
under general conditions on the measurement vectors. We also prove refined MSE
bounds when the measurement vectors are i.i.d. uniformly distributed on the
unit-sphere $\mathbb{S}^{d-1}$ and, in particular, show that in this case the
constant $K$ is dominated by $d^3$, the cube of the ambient dimension. The
proofs involve an analysis of random polytopes using coverage processes on the
sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7096</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7096</id><created>2014-05-27</created><authors><author><keyname>Venkatramanan</keyname><forenames>Srinivasan</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Influence Spread in Social Networks: A Study via a Fluid Limit of the
  Linear Threshold Model</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold based models have been widely used in characterizing collective
behavior on social networks. An individual's threshold indicates the minimum
level of influence that must be exerted, by other members of the population
engaged in some activity, before the individual will join the activity. In this
work, we begin with a homogeneous version of the Linear Threshold model
proposed by Kempe et al. in the context of viral marketing, and generalize this
model to arbitrary threshold distributions. We show that the evolution can be
modeled as a discrete time Markov chain, and, by using a certain scaling, we
obtain a fluid limit that provides an ordinary differential equation model
(o.d.e.). We find that the threshold distribution appears in the o.d.e. via its
hazard rate function. We demonstrate the accuracy of the o.d.e. approximation
and derive explicit expressions for the trajectory of influence under the
uniform threshold distribution. Also, for an exponentially distributed
threshold, we show that the fluid dynamics are equivalent to the well-known SIR
model in epidemiology. We also numerically study how other hazard functions
(obtained from the Weibull and loglogistic distributions) provide qualitative
different characteristics of the influence evolution, compared to traditional
epidemic models, even in a homogeneous setting. We finally show how the model
can be extended to a setting with multiple communities and conclude with
possible future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7102</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7102</id><created>2014-05-27</created><updated>2014-06-14</updated><authors><author><keyname>Althoff</keyname><forenames>Tim</forenames></author><author><keyname>Song</keyname><forenames>Hyun Oh</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Detection Bank: An Object Detection Based Video Representation for
  Multimedia Event Recognition</title><categories>cs.MM cs.CV</categories><comments>ACM Multimedia 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While low-level image features have proven to be effective representations
for visual recognition tasks such as object recognition and scene
classification, they are inadequate to capture complex semantic meaning
required to solve high-level visual tasks such as multimedia event detection
and recognition. Recognition or retrieval of events and activities can be
improved if specific discriminative objects are detected in a video sequence.
In this paper, we propose an image representation, called Detection Bank, based
on the detection images from a large number of windowed object detectors where
an image is represented by different statistics derived from these detections.
This representation is extended to video by aggregating the key frame level
image representations through mean and max pooling. We empirically show that it
captures complementary information to state-of-the-art representations such as
Spatial Pyramid Matching and Object Bank. These descriptors combined with our
Detection Bank representation significantly outperforms any of the
representations alone on TRECVID MED 2011 data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7112</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7112</id><created>2014-05-27</created><authors><author><keyname>Wimmer</keyname><forenames>Karl</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Peng</forenames></author></authors><title>Optimal query complexity for estimating the trace of a matrix</title><categories>cs.CC cs.DS</categories><comments>full version of the paper in ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an implicit $n\times n$ matrix $A$ with oracle access $x^TA x$ for any
$x\in \mathbb{R}^n$, we study the query complexity of randomized algorithms for
estimating the trace of the matrix. This problem has many applications in
quantum physics, machine learning, and pattern matching. Two metrics are
commonly used for evaluating the estimators: i) variance; ii) a high
probability multiplicative-approximation guarantee. Almost all the known
estimators are of the form $\frac{1}{k}\sum_{i=1}^k x_i^T A x_i$ for $x_i\in
\mathbb{R}^n$ being i.i.d. for some special distribution.
  Our main results are summarized as follows. We give an exact characterization
of the minimum variance unbiased estimator in the broad class of linear
nonadaptive estimators (which subsumes all the existing known estimators). We
also consider the query complexity lower bounds for any (possibly nonlinear and
adaptive) estimators: (1) We show that any estimator requires
$\Omega(1/\epsilon)$ queries to have a guarantee of variance at most
$\epsilon$. (2) We show that any estimator requires
$\Omega(\frac{1}{\epsilon^2}\log \frac{1}{\delta})$ queries to achieve a
$(1\pm\epsilon)$-multiplicative approximation guarantee with probability at
least $1 - \delta$. Both above lower bounds are asymptotically tight.
  As a corollary, we also resolve a conjecture in the seminal work of Avron and
Toledo (Journal of the ACM 2011) regarding the sample complexity of the
Gaussian Estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7115</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7115</id><created>2014-05-28</created><authors><author><keyname>Reid</keyname><forenames>Greg</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Wu</keyname><forenames>Wenyuan</forenames></author></authors><title>Geometric involutive bases for positive dimensional polynomial ideals
  and SDP methods</title><categories>math.AG cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric involutive bases for polynomial systems of equations have their
origin in the prolongation and projection methods of the geometers Cartan and
Kuranishi for systems of PDE. They are useful for numerical ideal membership
testing and the solution of polynomial systems. In this paper we further
develop our symbolic-numeric methods for such bases. We give methods to
explicitly extract and decrease the degree of intermediate systems and the
output basis. Algorithms for the numerical computation of involutivity criteria
for positive dimensional ideals are also discussed.
  We were also motivated by some remarkable recent work by Lasserre and
collaborators who employed our prolongation projection involutive criteria as a
part of their semi-definite based programming (SDP) method for identifying the
real radical of zero dimensional polynomial ideals. Consequently in this paper
we begin an exploration of the interaction between geometric involutive bases
and these methods particularly in the positive dimensional case. Motivated by
the extension of these methods to the positive dimensional case we explore the
interplay between geometric involutive bases and the new SDP methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7134</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7134</id><created>2014-05-28</created><updated>2014-08-24</updated><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author></authors><title>Role Discovery in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roles represent node-level connectivity patterns such as star-center,
star-edge nodes, near-cliques or nodes that act as bridges to different regions
of the graph. Intuitively, two nodes belong to the same role if they are
structurally similar. Roles have been mainly of interest to sociologists, but
more recently, roles have become increasingly useful in other domains.
Traditionally, the notion of roles were defined based on graph equivalences
such as structural, regular, and stochastic equivalences. We briefly revisit
these early notions and instead propose a more general formulation of roles
based on the similarity of a feature representation (in contrast to the graph
representation). This leads us to propose a taxonomy of three general classes
of techniques for discovering roles that includes (i) graph-based roles, (ii)
feature-based roles, and (iii) hybrid roles. We also propose a flexible
framework for discovering roles using the notion of similarity on a
feature-based representation. The framework consists of two fundamental
components: (a) role feature construction and (b) role assignment using the
learned feature representation. We discuss the different possibilities for
discovering feature-based roles and the tradeoffs of the many techniques for
computing them. Finally, we discuss potential applications and future
directions and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7135</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7135</id><created>2014-05-28</created><authors><author><keyname>Leme&#x161;</keyname><forenames>Samir</forenames></author></authors><title>Odr\v{z}avanje ra\v{c}unarskih sistema</title><categories>cs.OH</categories><comments>3. konferencija Odr\v{z}avanje - Maintenance 2014 (S. Brdarevi\'c, S.
  Ja\v{s}arevi\'c, editors), ISSN 1986-583X, Zenica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer hardware and software are resources without which the modern
business of any organization, from manufacturing to services, is impossible.
Not enough attention is being payed to maintenance of computer systems as an
aspect of business. This paper gives some recommendations for the selection of
the computer systems maintenance approach, based on many years of experience
maintaining these systems at the University of Zenica.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7136</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7136</id><created>2014-05-28</created><authors><author><keyname>Becattini</keyname><forenames>Francesco</forenames></author><author><keyname>Chatterjee</keyname><forenames>Arnab</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>Mitrovi&#x107;</keyname><forenames>Marija</forenames></author><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Parolo</keyname><forenames>Pietro Della Briotta</forenames></author></authors><title>The Nobel Prize delay</title><categories>physics.soc-ph cs.DL</categories><comments>Extended version of Nature 508, 186 (2014)
  http://www.nature.com/nature/journal/v508/n7495/full/508186a.html ;
  http://scitation.aip.org/content/aip/magazine/physicstoday/news/10.1063/PT.5.2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time lag between the publication of a Nobel discovery and the conferment
of the prize has been rapidly increasing for all disciplines, especially for
Physics. Does this mean that fundamental science is running out of
groundbreaking discoveries?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7141</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7141</id><created>2014-05-28</created><updated>2015-07-05</updated><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author><author><keyname>Terraf</keyname><forenames>Pedro S&#xe1;nchez</forenames></author></authors><title>Stochastic Nondeterminism and Effectivity Functions</title><categories>cs.LO math.LO</categories><comments>Minor changes in the text, correction of typos; new and extended
  abstract; added an acknowledgement (paper accepted by J. Logic Comput.)</comments><report-no>SWT-Memo 200</report-no><msc-class>03B70, 03E15, 28A05</msc-class><acm-class>F.4.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates stochastic nondeterminism on continuous state spaces
by relating nondeterministic kernels and stochastic effectivity functions to
each other. Nondeterministic kernels are functions assigning each state a set o
subprobability measures, and effectivity functions assign to each state an
upper-closed set of subsets of measures. Both concepts are generalizations of
Markov kernels used for defining two different models: Nondeterministic
labelled Markov processes and stochastic game models, respectively. We show
that an effectivity function that maps into principal filters is given by an
image-countable nondeterministic kernel, and that image-finite kernels give
rise to effectivity functions. We define state bisimilarity for the latter,
considering its connection to morphisms. We provide a logical characterization
of bisimilarity in the finitary case. A generalization of congruences (event
bisimulations) to effectivity functions and its relation to the categorical
presentation of bisimulation are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7143</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7143</id><created>2014-05-28</created><updated>2014-06-06</updated><authors><author><keyname>Jeyakumar</keyname><forenames>Vimalkumar</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Geng</keyname><forenames>Yilong</forenames></author><author><keyname>Kim</keyname><forenames>Changhoon</forenames></author><author><keyname>Mazi&#xe8;res</keyname><forenames>David</forenames></author></authors><title>Millions of Little Minions: Using Packets for Low Latency Network
  Programming and Visibility (Extended Version)</title><categories>cs.NI</categories><acm-class>C.2.1; C.2.3; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a practical approach to rapidly introduce new dataplane
functionality into networks: End-hosts embed tiny programs into packets to
actively query and manipulate a network's internal state. We show how this
&quot;tiny packet program&quot; (TPP) interface gives end-hosts unprecedented visibility
into network behavior, enabling them to work with the network to achieve a
common goal. Our design leverages what each component does best: (a) switches
forward and execute tiny packet programs (at most 5 instructions) at line rate,
and (b) end-hosts perform arbitrary computation on network state, which are
easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is
feasible, at a reasonable cost. By implementing three different research
proposals, we show that TPPs are also useful. And finally, we present an
architecture in which they can be made secure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7147</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7147</id><created>2014-05-28</created><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Siap</keyname><forenames>Irfan</forenames></author></authors><title>New extremal binary self-dual codes from F_4 + uF_4-lifts of quadratic
  double circulant codes over F_4</title><categories>math.CO cs.IT math.IT</categories><comments>Under review since May 2014, 11 pages, 6 tables</comments><msc-class>94B05, 94B99</msc-class><journal-ref>Finite Fileds and Their Applications Volume 35 2015</journal-ref><doi>10.1016/j.ffa.2015.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, quadratic double and quadratic bordered double circulant
constructions are applied to F_4 + uF_4 as well as F_4, as a result of which
extremal binary self-dual codes of length 56 and 64 are obtained. The binary
extension theorems as well as the ring extension version are used to obtain 7
extremal self-dual binary codes of length 58, 24 extremal self-dual binary
codes of length 66 and 29 extremal self-dual binary codes of length 68, all
with new weight enumerators, updating the list of all the known extremal
self-dual codes in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7152</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7152</id><created>2014-05-28</created><authors><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Iterative List-Decoding of Gabidulin Codes via Gr\&quot;obner Based
  Interpolation</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Information Theory Workshop 2014 in Hobart,
  Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how Gabidulin codes can be list decoded by using an iterative
parametrization approach. For a given received word, our decoding algorithm
processes its entries one by one, constructing four polynomials at each step.
This then yields a parametrization of interpolating solutions for the data so
far. From the final result a list of all codewords that are closest to the
received word with respect to the rank metric is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7153</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7153</id><created>2014-05-28</created><updated>2015-07-27</updated><authors><author><keyname>West</keyname><forenames>Scott</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Efficient and Reasonable Object-Oriented Concurrency</title><categories>cs.DC cs.PL</categories><comments>Proceedings of the 10th Joint Meeting of the European Software
  Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of
  Software Engineering (ESEC/FSE '15). ACM, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making threaded programs safe and easy to reason about is one of the chief
difficulties in modern programming. This work provides an efficient execution
model for SCOOP, a concurrency approach that provides not only data race
freedom but also pre/postcondition reasoning guarantees between threads. The
extensions we propose influence both the underlying semantics to increase the
amount of concurrent execution that is possible, exclude certain classes of
deadlocks, and enable greater performance. These extensions are used as the
basis an efficient runtime and optimization pass that improve performance 15x
over a baseline implementation. This new implementation of SCOOP is also 2x
faster than other well-known safe concurrent languages. The measurements are
based on both coordination-intensive and data-manipulation-intensive benchmarks
designed to offer a mixture of workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7161</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7161</id><created>2014-05-28</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Secure Transmission in Multi-Cell Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider physical layer security provisioning in multi-cell
massive multiple-input multiple-output (MIMO) systems. Specifically, we
consider secure downlink transmission in a multi-cell massive MIMO system with
matched-filter precoding and artificial noise (AN) generation at the base
station (BS) in the presence of a passive multi-antenna eavesdropper. We
investigate the resulting achievable ergodic secrecy rate and the secrecy
outage probability for the cases of perfect training and pilot contamination.
Thereby, we consider two different AN shaping matrices, namely, the
conventional AN shaping matrix, where the AN is transmitted in the null space
of the matrix formed by all user channels, and a random AN shaping matrix,
which avoids the complexity associated with finding the null space of a large
matrix. Our analytical and numerical results reveal that in multi-cell massive
MIMO systems employing matched-filter precoding (1) AN generation is required
to achieve a positive ergodic secrecy rate if the user and the eavesdropper
experience the same path-loss, (2) even with AN generation secure transmission
may not be possible if the number of eavesdropper antennas is too large and not
enough power is allocated to channel estimation, (3) for a given fraction of
power allocated to AN and a given number of users, in case of pilot
contamination, the ergodic secrecy rate is not a monotonically increasing
function of the number of BS antennas, and (4) random AN shaping matrices
provide a favourable performance/complexity tradeoff and are an attractive
alternative to conventional AN shaping matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7175</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7175</id><created>2014-05-28</created><updated>2014-06-13</updated><authors><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Chen</keyname><forenames>Ying-Ju</forenames></author><author><keyname>Shou</keyname><forenames>Biying</forenames></author></authors><title>Combining Spot and Futures Markets: A Hybrid Market Approach to Dynamic
  Spectrum Access</title><categories>cs.NI cs.GT</categories><comments>This manuscript is the complete technical report for the journal
  version submitted to INFORMS Operations Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum access (DSA) is a new paradigm of secondary spectrum
utilization and sharing. It allows unlicensed secondary users (SUs) to exploit
the under-utilized radio spectrum licensed to primary users opportunistically.
Market mechanism is a widely-used promising means to regulate the consuming
behaviours of users and, hence, achieve the efficient allocation and
consumption of limited resources. In this paper, we propose and study a hybrid
secondary spectrum market consisting of both the futures market and the spot
market, in which SUs (buyers) purchase under-utilized licensed spectrum from a
primary spectrum regulator (PR), either through predefined contracts via the
futures market, or through spot transactions via the spot market. We focus on
the optimal spectrum allocation among SUs in an exogenous hybrid market that
maximizes the secondary spectrum utilization efficiency. The problem is
challenging due to the stochasticity and asymmetry of network information. To
solve this problem, we first derive an o?ff-line optimal allocation policy that
maximizes the ex-ante expected spectrum utilization efficiency based on the
stochastic distribution of network information. We then propose an on-line VCG
auction that determines the real-time allocation and pricing of every spectrum
based on the realized network information and the derived allocation policy. We
further show that with the spatial frequency reuse, the proposed VCG auction is
NP-hard; hence, it is not suitable for on-line implementation, especially in a
large-scale market. To this end, we propose a heuristics approach based on an
on-line VCG-like mechanism with polynomial-time complexity, and further
characterize the corresponding performance loss bound analytically. We finally
provide extensive numerical results to evaluate the performance of the proposed
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7178</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7178</id><created>2014-05-28</created><updated>2015-10-19</updated><authors><author><keyname>Yoshida</keyname><forenames>Katsutoshi</forenames></author><author><keyname>Matsumoto</keyname><forenames>Shigeki</forenames></author><author><keyname>Matsue</keyname><forenames>Yoichi</forenames></author></authors><title>Artificial Wrestling: A Dynamical Formulation of Autonomous Agents
  Fighting in a Coupled Inverted Pendula Framework</title><categories>cs.RO cs.SY</categories><comments>The 12th International Conference on Motion and Vibration Control
  (MOVIC 2014), August 3-7, 2014, Sapporo, Japan. This article was selected as
  an article of Mechanical Engineering Journal after minor revisions; the final
  version is available at http://dx.doi.org/10.1299/mej.14-00518</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop autonomous agents fighting with each other, inspired by human
wrestling. For this purpose, we propose a coupled inverted pendula (CIP)
framework in which: 1) tips of two inverted pendulums are linked by a
connection rod, 2) each pendulum is primarily stabilized by a PD-controller, 3)
and is additionally equipped with an intelligent controller. Based on this
framework, we dynamically formulate an intelligent controller designed to store
dynamical correspondence from initial states to final states of the CIP model,
to receive state vectors of the model, and to output impulsive control forces
to produce desired final states of the model. Developing a quantized and
reduced order design of this controller, we have a practical control procedure
based on an off-line learning method. We then conduct numerical simulations to
investigate individual performance of the intelligent controller, showing that
the performance can be improved by adding a delay element into the intelligent
controller. The result shows that the performance depends not only on
quantization resolutions of learning data but also on delay time of the delay
element. Finally, we install the intelligent controllers into both pendulums in
the proposed framework to demonstrate autonomous competitive behavior between
inverted pendulums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7183</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7183</id><created>2014-05-28</created><updated>2014-11-17</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Arag&#xf3;n</keyname><forenames>Pablo</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Interactions of cultures and top people of Wikipedia from ranking of 24
  language editions</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>32 pages. 10 figures. Submitted for publication. Supporting
  information is available on
  http://www.quantware.ups-tlse.fr/QWLIB/topwikipeople/</comments><journal-ref>PLoS ONE 10(3): e0114825 (2015)</journal-ref><doi>10.1371/journal.pone.0114825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is a huge global repository of human knowledge, that can be
leveraged to investigate interwinements between cultures. With this aim, we
apply methods of Markov chains and Google matrix, for the analysis of the
hyperlink networks of 24 Wikipedia language editions, and rank all their
articles by PageRank, 2DRank and CheiRank algorithms. Using automatic
extraction of people names, we obtain the top 100 historical figures, for each
edition and for each algorithm. We investigate their spatial, temporal, and
gender distributions in dependence of their cultural origins. Our study
demonstrates not only the existence of skewness with local figures, mainly
recognized only in their own cultures, but also the existence of global
historical figures appearing in a large number of editions. By determining the
birth time and place of these persons, we perform an analysis of the evolution
of such figures through 35 centuries of human history for each language, thus
recovering interactions and entanglement of cultures over time. We also obtain
the distributions of historical figures over world countries, highlighting
geographical aspects of cross-cultural links. Considering historical figures
who appear in multiple editions as interactions between cultures, we construct
a network of cultures and identify the most influential cultures according to
this network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7192</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7192</id><created>2014-05-28</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>The PeerRank Method for Peer Assessment</title><categories>cs.AI cs.DS</categories><comments>To appear in Proc. of ECAI 2014</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the PeerRank method for peer assessment. This constructs a grade
for an agent based on the grades proposed by the agents evaluating the agent.
Since the grade of an agent is a measure of their ability to grade correctly,
the PeerRank method weights grades by the grades of the grading agent. The
PeerRank method also provides an incentive for agents to grade correctly. As
the grades of an agent depend on the grades of the grading agents, and as these
grades themselves depend on the grades of other agents, we define the PeerRank
method by a fixed point equation similar to the PageRank method for ranking
web-pages. We identify some formal properties of the PeerRank method (for
example, it satisfies axioms of unanimity, no dummy, no discrimination and
symmetry), discuss some examples, compare with related work and evaluate the
performance on some synthetic data. Our results show considerable promise,
reducing the error in grade predictions by a factor of 2 or more in many cases
over the natural baseline of averaging peer grades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7197</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7197</id><created>2014-05-28</created><authors><author><keyname>Prandini</keyname><forenames>M.</forenames></author><author><keyname>Garatti</keyname><forenames>S.</forenames></author><author><keyname>Vignali</keyname><forenames>R.</forenames></author></authors><title>Performance assessment and design of abstracted models for stochastic
  hybrid systems through a randomized approach</title><categories>cs.SY</categories><comments>Preliminary version of a paper accepted for publication in Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a simulation-based method for the analysis and design of
abstracted models for a stochastic hybrid system is proposed. The accuracy of a
model is evaluated in terms of its capability to reproduce the system output
for all the realizations of the stochastic input except for a set of (small)
probability epsilon (epsilon-abstraction). This naturally leads to
chance-constrained optimization problems, which are here tackled by means of a
recently developed randomized approach. The main thrust of this paper is that,
by testing how close the model and system outputs are over a finite number N of
input realizations only, conclusions can be drawn about the model capability as
an epsilon-abstraction. The key feature of the proposed method is its high
versatility since it does not require specific assumptions on the system to be
approximated. The only requirement is that of being able to run multiple
simulations of the system behavior for different input realizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7221</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7221</id><created>2014-05-28</created><updated>2014-07-21</updated><authors><author><keyname>Nguyen</keyname><forenames>Linh Anh</forenames></author><author><keyname>Goli&#x144;ska-Pilarek</keyname><forenames>Joanna</forenames></author></authors><title>ExpTime Tableaux with Global Caching for the Description Logic SHOQ</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1205.5838</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first ExpTime (complexity-optimal) tableau decision procedure for
checking satisfiability of a knowledge base in the description logic SHOQ,
which extends the basic description logic ALC with transitive roles,
hierarchies of roles, nominals and quantified number restrictions. The
complexity is measured using unary representation for numbers. Our procedure is
based on global caching and integer linear feasibility checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7229</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7229</id><created>2014-05-28</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Sencion</keyname><forenames>Felipe</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Sossa</keyname><forenames>Humberto</forenames></author></authors><title>A Multi-threshold Segmentation Approach Based on Artificial Bee Colony
  Optimization</title><categories>cs.CV cs.NE</categories><comments>16 Pages</comments><journal-ref>Applied Intelligence 37 (3), (2012), pp. 321-336</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the use of the Artificial Bee Colony (ABC) algorithm to
compute threshold selection for image segmentation. ABC is a heuristic
algorithm motivated by the intelligent behavior of honey-bees which has been
successfully employed to solve complex optimization problems. In this approach,
an image 1D histogram is approximated through a Gaussian mixture model whose
parameters are calculated by the ABC algorithm. For the approximation scheme,
each Gaussian function represents a pixel class and therefore a threshold.
Unlike the Expectation Maximization (EM) algorithm, the ABC based method shows
fast convergence and low sensitivity to initial conditions. Remarkably, it also
improves complex time consuming computations commonly required by
gradient-based methods. Experimental results demonstrate the algorithms ability
to perform automatic multi threshold selection yet showing interesting
advantages by comparison to other well known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7237</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7237</id><created>2014-05-28</created><authors><author><keyname>Yi&#x11f;itler</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author></authors><title>Detecting Human-induced Reflections using RSS of Narrowband Wireless
  Transceivers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio frequency sensor networks are becoming increasingly popular as an
indoor localization and monitoring technology for gaining unobtrusive
situational awareness of the surrounding environment. The localization effort
in these networks is built upon the well-established fact that the received
signal strength measurements vary due to a person's presence on the
line-of-sight of a transmitter-receiver pair. To date, modeling this decrease
in received signal strength and utilizing it for localization purposes have
received a considerable amount of attention in the research field. However,
when the person is in the close vicinity of the line-of-sight but not
obstructing it, the signal reflected from the human body is also affecting the
received signal strength and can be used for occupancy assessment purposes. In
this paper, we first model the effect of human-induced reflections as a
function of communication frequency, and then use the model as a basis for
energy based occupancy detection. The derived methods are evaluated numerically
and the detection probability of the proposed detector is validated with
experimental data. The results suggest that when more than eight frequency
channels are utilized, presence of a person can be detected using RSS
measurements of a single transmit-receive pair with detection probability
higher than 0.95 and false alarm probability less than 0.01 in an area of 2 m x
2.5 m. Moreover, the important implications of the studied methods on the
available narrowband radio frequency sensor network applications are discussed
in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7242</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7242</id><created>2014-05-28</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Ortega</keyname><forenames>Noe</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Circle detection by Harmony Search Optimization</title><categories>cs.CV</categories><comments>18 Pages</comments><journal-ref>Intelligent and Robotic Systems: Theory and Applications 66 (3),
  (2013), pp. 359-376</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic circle detection in digital images has received considerable
attention over the last years in computer vision as several efforts have aimed
for an optimal circle detector. This paper presents an algorithm for automatic
detection of circular shapes that considers the overall process as an
optimization problem. The approach is based on the Harmony Search Algorithm
(HSA), a derivative free meta-heuristic optimization algorithm inspired by
musicians while improvising new harmonies. The algorithm uses the encoding of
three points as candidate circles (harmonies) over the edge-only image. An
objective function evaluates (harmony quality) if such candidate circles are
actually present in the edge image. Guided by the values of this objective
function, the set of encoded candidate circles are evolved using the HSA so
that they can fit to the actual circles on the edge map of the image (optimal
harmony). Experimental results from several tests on synthetic and natural
images with a varying complexity range have been included to validate the
efficiency of the proposed technique regarding accuracy, speed and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7253</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7253</id><created>2014-05-28</created><updated>2014-10-08</updated><authors><author><keyname>Egly</keyname><forenames>Uwe</forenames></author><author><keyname>Kronegger</keyname><forenames>Martin</forenames></author><author><keyname>Lonsing</keyname><forenames>Florian</forenames></author><author><keyname>Pfandler</keyname><forenames>Andreas</forenames></author></authors><title>Conformant Planning as a Case Study of Incremental QBF Solving</title><categories>cs.LO cs.AI</categories><comments>revision (camera-ready, to appear in the proceedings of AISC 2014,
  volume 8884 of LNAI, Springer)</comments><doi>10.1007/978-3-319-13770-4_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider planning with uncertainty in the initial state as a case study of
incremental quantified Boolean formula (QBF) solving. We report on experiments
with a workflow to incrementally encode a planning instance into a sequence of
QBFs. To solve this sequence of incrementally constructed QBFs, we use our
general-purpose incremental QBF solver DepQBF. Since the generated QBFs have
many clauses and variables in common, our approach avoids redundancy both in
the encoding phase and in the solving phase. Experimental results show that
incremental QBF solving outperforms non-incremental QBF solving. Our results
are the first empirical study of incremental QBF solving in the context of
planning and motivate its use in other application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7254</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7254</id><created>2014-05-28</created><authors><author><keyname>Ku</keyname><forenames>Meng-Lin</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author></authors><title>Data-Driven Stochastic Models and Policies for Energy Harvesting Sensor
  Communications</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting from the surroundings is a promising solution to
perpetually power-up wireless sensor communications. This paper presents a
data-driven approach of finding optimal transmission policies for a
solar-powered sensor node that attempts to maximize net bit rates by adapting
its transmission parameters, power levels and modulation types, to the changes
of channel fading and battery recharge. We formulate this problem as a
discounted Markov decision process (MDP) framework, whereby the energy
harvesting process is stochastically quantized into several representative
solar states with distinct energy arrivals and is totally driven by historical
data records at a sensor node. With the observed solar irradiance at each time
epoch, a mixed strategy is developed to compute the belief information of the
underlying solar states for the choice of transmission parameters. In addition,
a theoretical analysis is conducted for a simple on-off policy, in which a
predetermined transmission parameter is utilized whenever a sensor node is
active. We prove that such an optimal policy has a threshold structure with
respect to battery states and evaluate the performance of an energy harvesting
node by analyzing the expected net bit rate. The design framework is
exemplified with real solar data records, and the results are useful in
characterizing the interplay that occurs between energy harvesting and
expenditure under various system configurations. Computer simulations show that
the proposed policies significantly outperform other schemes with or without
the knowledge of short-term energy harvesting and channel fading patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7258</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7258</id><created>2014-05-28</created><authors><author><keyname>Mehdiabadi</keyname><forenames>Motahareh Eslami</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author></authors><title>Sampling from Diffusion Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Published in Proceedings of the 2012 International Conference on
  Social Informatics, Pages 106-112</comments><doi>10.1109/SocialInformatics.2012.79</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diffusion phenomenon has a remarkable impact on Online Social Networks
(OSNs). Gathering diffusion data over these large networks encounters many
challenges which can be alleviated by adopting a suitable sampling approach.
The contributions of this paper is twofold. First we study the sampling
approaches over diffusion networks, and for the first time, classify these
approaches into two categories; (1) Structure-based Sampling (SBS), and (2)
Diffusion-based Sampling (DBS). The dependency of the former approach to
topological features of the network, and unavailability of real diffusion paths
in the latter, converts the problem of choosing an appropriate sampling
approach to a trade-off. Second, we formally define the diffusion network
sampling problem and propose a number of new diffusion-based characteristics to
evaluate introduced sampling approaches. Our experiments on large scale
synthetic and real datasets show that although DBS performs much better than
SBS in higher sampling rates (16% ~ 29% on average), their performances differ
about 7% in lower sampling rates. Therefore, in real large scale systems with
low sampling rate requirements, SBS would be a better choice according to its
lower time complexity in gathering data compared to DBS. Moreover, we show that
the introduced sampling approaches (SBS and DBS) play a more important role
than the graph exploration techniques such as Breadth-First Search (BFS) and
Random Walk (RW) in the analysis of diffusion processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7264</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7264</id><created>2014-05-28</created><updated>2014-05-31</updated><authors><author><keyname>Interlandi</keyname><forenames>Matteo</forenames></author><author><keyname>Tanca</keyname><forenames>Letizia</forenames></author></authors><title>On the CALM Principle for Bulk Synchronous Parallel Computation</title><categories>cs.DB</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years a lot of emphasis has been placed on two apparently
disjoined fields: data-parallel and eventually consistent distributed systems.
In this paper we propose a theoretical study over an eventually consistent
data-parallel computational model. The keystone is provided by the recent
finding that a class of programs exists which can be computed in an eventually
consistent, coordination-free way: monotonic programs. This principle is called
CALM and has been proven for distributed asynchronous settings. We make the
case that, using the techniques developed by Ameloot et al., CALM does not hold
in general for data-parallel systems, wherein computation usually proceeds
synchronously in rounds and where communication is reliable. We then show that
using novel techniques subsuming the one of Ameloot et al., the satisfiability
of the CALM principle is directly related with the assumptions imposed on the
behavior of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7268</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7268</id><created>2014-05-27</created><authors><author><keyname>Waskita</keyname><forenames>A. A.</forenames></author><author><keyname>Suhartanto</keyname><forenames>H.</forenames></author><author><keyname>Persadha</keyname><forenames>P. D.</forenames></author><author><keyname>Handoko</keyname><forenames>L. T.</forenames></author></authors><title>A simple statistical analysis approach for Intrusion Detection System</title><categories>cs.CR</categories><comments>Proceeding of the IEEE Conference on Systems, Process and Control
  (2013) pp. 193-197</comments><report-no>INFORMATIKALIPI-13040</report-no><doi>10.1109/SPC.2013.6735130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach to analyze statistically the network traffic raw data is
proposed. The huge amount of raw data of actual network traffic from the
Intrusion Detection System is analyzed to determine if a traffic is a normal or
harmful one. Using the active ports in each host in a network as sensors, the
system continuously monitors the incoming packets, and generates its average
behaviors at different time scales including its variances. The average region
of behaviors at certain time scale is then being used as the baseline of normal
traffic. Deploying the exhaustive search based decission system, the system
detects the incoming threats to the whole network under supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7285</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7285</id><created>2014-05-20</created><authors><author><keyname>Fernandes</keyname><forenames>Ricardo Q. A.</forenames></author><author><keyname>Haeusler</keyname><forenames>Edward H.</forenames></author><author><keyname>Pereira</keyname><forenames>Luiz Carlos</forenames></author></authors><title>PUC-Logic embedding of Lewis' Deontic Logics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a embedding of Lewis Deontic logics in PUC-Logic. We achieve this
by representing the vary basic $\boldsymbol{CO}$ logic and showing its relative
completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7290</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7290</id><created>2014-05-28</created><updated>2014-08-21</updated><authors><author><keyname>Jacobs</keyname><forenames>Christian T.</forenames></author><author><keyname>Avdis</keyname><forenames>Alexandros</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author></authors><title>PyRDM: A Python-based library for automating the management and online
  publication of scientific software and data</title><categories>cs.CE cs.DL cs.MS</categories><comments>Revised version. The main changes are: Added pdfLaTeX to the
  dependencies list; Improved Figure 1 to show the 'publish' option selected in
  Diamond; Added two paragraphs to explain why users would want to use PyRDM;
  Added some content on the PyRDM roadmap, and also some content regarding
  engagement with libraries and research software engineers</comments><journal-ref>Journal of Open Research Software 2:e28 (2014) 1-6</journal-ref><doi>10.5334/jors.bj</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The recomputability and reproducibility of results from scientific software
requires access to both the source code and all associated input and output
data. However, the full collection of these resources often does not accompany
the key findings published in journal articles, thereby making it difficult or
impossible for the wider scientific community to verify the correctness of a
result or to build further research on it. This paper presents a new
Python-based library, PyRDM, whose functionality aims to automate the process
of sharing the software and data via online, citable repositories such as
Figshare. The library is integrated into the workflow of an open-source
computational fluid dynamics package, Fluidity, to demonstrate an example of
its usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7292</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7292</id><created>2014-05-28</created><updated>2014-06-05</updated><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>White</keyname><forenames>Andrew</forenames></author><author><keyname>Giraud-Carrier</keyname><forenames>Christophe</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>An Easy to Use Repository for Comparing and Improving Machine Learning
  Algorithm Usage</title><categories>stat.ML cs.LG</categories><comments>7 pages, 1 figure, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results from most machine learning experiments are used for a specific
purpose and then discarded. This results in a significant loss of information
and requires rerunning experiments to compare learning algorithms. This also
requires implementation of another algorithm for comparison, that may not
always be correctly implemented. By storing the results from previous
experiments, machine learning algorithms can be compared easily and the
knowledge gained from them can be used to improve their performance. The
purpose of this work is to provide easy access to previous experimental results
for learning and comparison. These stored results are comprehensive -- storing
the prediction for each test instance as well as the learning algorithm,
hyperparameters, and training set that were used. Previous results are
particularly important for meta-learning, which, in a broad sense, is the
process of learning from previous machine learning results such that the
learning process is improved. While other experiment databases do exist, one of
our focuses is on easy access to the data. We provide meta-learning data sets
that are ready to be downloaded for meta-learning experiments. In addition,
queries to the underlying database can be made if specific information is
desired. We also differ from previous experiment databases in that our
databases is designed at the instance level, where an instance is an example in
a data set. We store the predictions of a learning algorithm trained on a
specific training set for each instance in the test set. Data set level
information can then be obtained by aggregating the results from the instances.
The instance level information can be used for many tasks such as determining
the diversity of a classifier or algorithmically determining the optimal subset
of training instances for a learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7295</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7295</id><created>2014-05-28</created><authors><author><keyname>Nov&#xe1;k</keyname><forenames>Peter</forenames></author><author><keyname>Witteveen</keyname><forenames>Cees</forenames></author></authors><title>On the cost-complexity of multi-context systems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-context systems provide a powerful framework for modelling
information-aggregation systems featuring heterogeneous reasoning components.
Their execution can, however, incur non-negligible cost. Here, we focus on
cost-complexity of such systems. To that end, we introduce cost-aware
multi-context systems, an extension of non-monotonic multi-context systems
framework taking into account costs incurred by execution of semantic operators
of the individual contexts. We formulate the notion of cost-complexity for
consistency and reasoning problems in MCSs. Subsequently, we provide a series
of results related to gradually more and more constrained classes of MCSs and
finally introduce an incremental cost-reducing algorithm solving the reasoning
problem for definite MCSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7300</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7300</id><created>2014-05-28</created><authors><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>Radio Network Lower Bounds Made Easy</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoreticians have studied distributed algorithms in the radio network model
for close to three decades. A significant fraction of this work focuses on
lower bounds for basic communication problems such as wake-up (symmetry
breaking among an unknown set of nodes) and broadcast (message dissemination
through an unknown network topology). In this paper, we introduce a new
technique for proving this type of bound, based on reduction from a
probabilistic hitting game, that simplifies and strengthens much of this
existing work. In more detail, in this single paper we prove new expected time
and high probability lower bounds for wake-up and global broadcast in single
and multichannel versions of the radio network model both with and without
collision detection. In doing so, we are able to reproduce results that
previously spanned a half-dozen papers published over a period of twenty-five
years. In addition to simplifying these existing results, our technique, in
many places, also improves the state of the art: of the eight bounds we prove,
four strictly strengthen the best known previous result (in terms of time
complexity and/or generality of the algorithm class for which it holds), and
three provide the first known non-trivial bound for the case in question. The
fact that the same technique can easily generate this diverse collection of
lower bounds indicates a surprising unity underlying communication tasks in the
radio network model---revealing that deep down, below the specifics of the
problem definition and model assumptions, communication in this setting reduces
to finding efficient strategies for a simple game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7320</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7320</id><created>2014-05-28</created><authors><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>Speeding Up SMT-Based Quantitative Program Analysis</title><categories>cs.LO</categories><comments>Full version of an SMT 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative program analysis involves computing numerical quantities about
individual or collections of program executions. An example of such a
computation is quantitative information flow analysis, where one estimates the
amount of information leaked about secret data through a program's output
channels. Such information can be quantified in several ways, including channel
capacity and (Shannon) entropy. In this paper, we formalize a class of
quantitative analysis problems defined over a weighted control flow graph of a
loop-free program. These problems can be solved using a combination of path
enumeration, SMT solving, and model counting. However, existing methods can
only handle very small programs, primarily because the number of execution
paths can be exponential in the program size. We show how path explosion can be
mitigated in some practical cases by taking advantage of special branching
structure and by novel algorithm design. We demonstrate our techniques by
computing the channel capacities of the timing side-channels of two programs
with extremely large numbers of paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7322</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7322</id><created>2014-05-28</created><authors><author><keyname>Tong</keyname><forenames>Guangmo</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author></authors><title>Supporting Soft Real-Time Sporadic Task Systems on Heterogeneous
  Multiprocessors with No Utilization Loss</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous multicore architectures are becoming increasingly popular due
to their potential of achieving high performance and energy efficiency compared
to the homogeneous multicore architectures. In such systems, the real-time
scheduling problem becomes more challenging in that processors have different
speeds. A job executing on a processor with speed $x$ for $t$ time units
completes $(x \cdot t)$ units of execution. Prior research on heterogeneous
multiprocessor real-time scheduling has focused on hard real-time systems,
where, significant processing capacity may have to be sacrificed in the
worst-case to ensure that all deadlines are met. As meeting hard deadlines is
overkill for many soft real-time systems in practice, this paper shows that on
soft real-time heterogeneous multiprocessors, bounded response times can be
ensured for globally-scheduled sporadic task systems with no utilization loss.
A GEDF-based scheduling algorithm, namely GEDF-H, is presented and response
time bounds are established under both preemptive and non-preemptive GEDF-H
scheduling. Extensive experiments show that the magnitude of the derived
response time bound is reasonable, often smaller than three task periods. To
the best of our knowledge, this paper is the first to show that soft real-time
sporadic task systems can be supported on heterogeneous multiprocessors without
utilization loss, and with reasonable predicted response time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7348</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7348</id><created>2014-05-28</created><authors><author><keyname>Yaveroglu</keyname><forenames>Omer Nebil</forenames></author><author><keyname>Fitzhugh</keyname><forenames>Sean M.</forenames></author><author><keyname>Kurant</keyname><forenames>Maciej</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author><author><keyname>Butts</keyname><forenames>Carter T.</forenames></author><author><keyname>Przulj</keyname><forenames>Natasa</forenames></author></authors><title>ergm.graphlets: A Package for ERG Modeling Based on Graphlet Statistics</title><categories>cs.SI</categories><comments>32 pages, 9 figures, under review by Journal of Statistical Software</comments><journal-ref>Journal of Statistical Software, Vol. 65, Issue 12, Jun 2015</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Exponential-family random graph models (ERGMs) are probabilistic network
models that are parametrized by sufficient statistics based on structural
(i.e., graph-theoretic) properties. The ergm package for the R statistical
computing system is a collection of tools for the analysis of network data
within an ERGM framework. Many different network properties can be employed as
sufficient statistics for ERGMs by using the model terms defined in the ergm
package; this functionality can be expanded by the creation of packages that
code for additional network statistics. Here, our focus is on the addition of
statistics based on graphlets. Graphlets are small, connected, and
non-isomorphic induced subgraphs that describe the topological structure of a
network. We introduce an R package called ergm.graphlets that enables the use
of graphlet properties of a network within the ergm package of R. The
ergm.graphlets package provides a complete list of model terms that allows to
incorporate statistics of any 2-, 3-, 4- and 5-node graphlet into ERGMs. The
new model terms of ergm.graphlets package enable both ERG modelling of global
structural properties and investigation of relationships between nodal
attributes (i.e., covariates) and local topologies around nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7349</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7349</id><created>2014-01-09</created><authors><author><keyname>Wang</keyname><forenames>Bing</forenames></author><author><keyname>Meng</keyname><forenames>Yao-hua</forenames></author><author><keyname>Yu</keyname><forenames>Xiao-hong</forenames></author></authors><title>Radial basis function process neural network training based on
  generalized frechet distance and GA-SA hybrid strategy</title><categories>cs.NE</categories><comments>9 pages, 4 figures,14 references</comments><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 3, No. 6, December 2013:1-9</journal-ref><doi>10.5121/cseij.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For learning problem of Radial Basis Function Process Neural Network
(RBF-PNN), an optimization training method based on GA combined with SA is
proposed in this paper. Through building generalized Fr\'echet distance to
measure similarity between time-varying function samples, the learning problem
of radial basis centre functions and connection weights is converted into the
training on corresponding discrete sequence coefficients. Network training
objective function is constructed according to the least square error
criterion, and global optimization solving of network parameters is implemented
in feasible solution space by use of global optimization feature of GA and
probabilistic jumping property of SA . The experiment results illustrate that
the training algorithm improves the network training efficiency and stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7358</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7358</id><created>2014-05-14</created><authors><author><keyname>Laciana</keyname><forenames>C. E.</forenames></author><author><keyname>Gual</keyname><forenames>G.</forenames></author><author><keyname>Kalmus</keyname><forenames>D.</forenames></author><author><keyname>Oteiza-Aguirre</keyname><forenames>N.</forenames></author><author><keyname>Rovere</keyname><forenames>S. L.</forenames></author></authors><title>Diffusion of two brands in competition: cross-brand effect</title><categories>cs.SI physics.soc-ph</categories><comments>21 pages, 4 figures</comments><doi>10.1016/j.physa.2014.06.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the equilibrium points of a system of equations corresponding to a
Bass based model that describes the diffusion of two brands in competition. To
increase the understanding of the effects of the cross-brand parameters, we
perform a sensitivity analysis. Finally, we show a comparison with an
agent-based model inspired in the Potts model. Conclusions include that both
models give the same diffusion curves only when the cross coeficients are not
null.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7361</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7361</id><created>2014-05-28</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Seeking multi-thresholds for image segmentation with Learning Automata</title><categories>cs.CV</categories><comments>22 Pages. arXiv admin note: text overlap with arXiv:1405.7229</comments><journal-ref>Machine Vision and Applications 22 (5), (2011), pp. 805-818</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the use of the Learning Automata (LA) algorithm to
compute threshold selection for image segmentation as it is a critical
preprocessing step for image analysis, pattern recognition and computer vision.
LA is a heuristic method which is able to solve complex optimization problems
with interesting results in parameter estimation. Despite other techniques
commonly seek through the parameter map, LA explores in the probability space
providing appropriate convergence properties and robustness. The segmentation
task is therefore considered as an optimization problem and the LA is used to
generate the image multi-threshold separation. In this approach, one 1D
histogram of a given image is approximated through a Gaussian mixture model
whose parameters are calculated using the LA algorithm. Each Gaussian function
approximating the histogram represents a pixel class and therefore a threshold
point. The method shows fast convergence avoiding the typical sensitivity to
initial conditions such as the Expectation Maximization (EM) algorithm or the
complex time-consuming computations commonly found in gradient methods.
Experimental results demonstrate the algorithm ability to perform automatic
multi-threshold selection and show interesting advantages as it is compared to
other algorithms solving the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7362</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7362</id><created>2014-05-28</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Ramirez</keyname><forenames>Marte</forenames></author></authors><title>Circle detection using Discrete Differential Evolution Optimization</title><categories>cs.CV</categories><comments>20 Pages. arXiv admin note: text overlap with arXiv:1405.7242</comments><journal-ref>Pattern Analysis and Applications 14 (1), (2011), pp. 93-107</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a circle detection method based on Differential
Evolution (DE) optimization. Just as circle detection has been lately
considered as a fundamental component for many computer vision algorithms, DE
has evolved as a successful heuristic method for solving complex optimization
problems, still keeping a simple structure and an easy implementation. It has
also shown advantageous convergence properties and remarkable robustness. The
detection process is considered similar to a combinational optimization
problem. The algorithm uses the combination of three edge points as parameters
to determine circles candidates in the scene yielding a reduction of the search
space. The objective function determines if some circle candidates are actually
present in the image. This paper focuses particularly on one DE-based algorithm
known as the Discrete Differential Evolution (DDE), which eventually has shown
better results than the original DE in particular for solving combinatorial
problems. In the DDE, suitable conversion routines are incorporated into the
DE, aiming to operate from integer values to real values and then getting
integer values back, following the crossover operation. The final algorithm is
a fast circle detector that locates circles with sub-pixel accuracy even
considering complicated conditions and noisy images. Experimental results on
several synthetic and natural images with varying range of complexity validate
the efficiency of the proposed technique considering accuracy, speed, and
robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7375</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7375</id><created>2014-05-28</created><updated>2014-09-26</updated><authors><author><keyname>Biamonte</keyname><forenames>Jacob D.</forenames></author><author><keyname>Morton</keyname><forenames>Jason</forenames></author><author><keyname>Turner</keyname><forenames>Jacob W.</forenames></author></authors><title>Tensor Network Contractions for #SAT</title><categories>quant-ph cs.CC cs.LO math-ph math.MP</categories><comments>16 pages, 8 diagrams</comments><acm-class>F.2; F.2.1; F.4</acm-class><journal-ref>Journal of Statistical Physics 160:5 1389-1404 (2015)</journal-ref><doi>10.1007/s10955-015-1276-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational cost of counting the number of solutions satisfying a
Boolean formula, which is a problem instance of #SAT, has proven subtle to
quantify. Even when finding individual satisfying solutions is computationally
easy (e.g. 2-SAT, which is in P), determining the number of solutions is
#P-hard. Recently, computational methods simulating quantum systems experienced
advancements due to the development of tensor network algorithms and associated
quantum physics-inspired techniques. By these methods, we give an algorithm
using an axiomatic tensor contraction language for n-variable #SAT instances
with complexity $O((g+cd)^{O(1)} 2^c)$ where $c$ is the number of COPY-tensors,
$g$ is the number of gates, and $d$ is the maximal degree of any COPY-tensor.
Thus, counting problems can be solved efficiently when their tensor network
expression has at most $O(\log c)$ COPY-tensors and polynomial fan-out. This
framework also admits an intuitive proof of a variant of the Tovey conjecture
(the r,1-SAT instance of the Dubois-Tovey theorem). This study increases the
theory, expressiveness and application of tensor based algorithmic tools and
provides an alternative insight on these problems which have a long history in
statistical physics and computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7381</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7381</id><created>2014-05-28</created><updated>2014-12-23</updated><authors><author><keyname>de Beaudrap</keyname><forenames>Niel</forenames></author></authors><title>On computation with 'probabilities' modulo k</title><categories>cs.CC quant-ph</categories><comments>53 pages, 3 figures. Simplified to focus on cyclic rings. New content
  includes a treatment of &quot;affine&quot; state spaces and bounded-error computation,
  and an argument (Appendix C) to allow certain infinite gate-sets in the study
  of quantum algorithms. Many minor technical errors also corrected. Keywords:
  counting complexity, destructive interference, indeterminism, modal quantum
  theory</comments><msc-class>68Q05, 68Q10, 68Q12, 68Q15, 81P16</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework to study models of computation of indeterministic
data, represented by abstract &quot;distributions&quot;. In these distributions,
probabilities are replaced by &quot;amplitudes&quot; drawn from a fixed semi-ring $S$, of
which the non-negative reals, the complex numbers, finite fields $\mathbb
F_{p^r}$, and cyclic rings $\mathbb Z_k$ are examples. Varying $S$ yields
different models of computation, which we may investigate to better understand
the (likely) difference in power between randomised and quantum computation.
The &quot;modal quantum states&quot; of Schumacher and Westmoreland [arXiv:1010.2929] are
examples of such distributions, for $S$ a finite field. For $S = \mathbb F_2$,
Willcock and Sabry [arXiv:1102.3587] show that UNIQUE-SAT is solvable by
polynomial-time uniform circuit families consisting of invertible gates. We
characterize the decision problems solvable by polynomial uniform circuit
families, using either invertible or &quot;unitary&quot; transformations over cyclic
rings $S = \mathbb Z_k$, or (in the case that $k$ is a prime power) finite
fields $S = \mathbb F_k$. In particular, for $k$ a prime power, these are
precisely the problems in the class $\mathsf{Mod}_k\mathsf P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7383</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7383</id><created>2014-05-28</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>Coloration de nombre de Grundy pour les graphes triangul\'es</title><categories>cs.DM</categories><comments>In French</comments><journal-ref>SETIT 2009 5th International Conference: Sciences of Electronic,
  Technologies of Information and T'elecommunications March 22 26 2009 TUNISIA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of the data routing management, it provides a method or a
strategy that guarantees at any time the connection between any pair of nodes
in the network. This routing method must be able to cope with frequent changes
in the topology and also other characteristics of the ad hoc network as
bandwidth, the number of links, network resources etc.. We also illustrate the
utility of the proposed algorithms: the problem of assignment or frequency
allotment in a radio network or mobile phones as in the following way: how to
attribute a frequency to every transmitter(issuer) or an unity(unit) of the
network, so that two broadcasting stations (issuers) which can interfere have
frequencies distant enough from each other ? Thus to affect the wavelengths
means finding a coloring of the graph, but because the network is not stable
and the topology is dynamic, we need a method which maintains the initial
allocation of the frequencies or to look for a new allocation to maintain the
stability of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7392</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7392</id><created>2014-05-28</created><authors><author><keyname>Arslan</keyname><forenames>Oktay</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos</forenames></author><author><keyname>Tsiotras</keyname><forenames>Panagiotis</forenames></author></authors><title>Information-Theoretic Stochastic Optimal Control via Incremental
  Sampling-based Algorithms</title><categories>cs.RO cs.SY</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers optimal control of dynamical systems which are
represented by nonlinear stochastic differential equations. It is well-known
that the optimal control policy for this problem can be obtained as a function
of a value function that satisfies a nonlinear partial differential equation,
namely, the Hamilton-Jacobi-Bellman equation. This nonlinear PDE must be solved
backwards in time, and this computation is intractable for large scale systems.
Under certain assumptions, and after applying a logarithmic transformation, an
alternative characterization of the optimal policy can be given in terms of a
path integral. Path Integral (PI) based control methods have recently been
shown to provide elegant solutions to a broad class of stochastic optimal
control problems. One of the implementation challenges with this formalism is
the computation of the expectation of a cost functional over the trajectories
of the unforced dynamics. Computing such expectation over trajectories that are
sampled uniformly may induce numerical instabilities due to the exponentiation
of the cost. Therefore, sampling of low-cost trajectories is essential for the
practical implementation of PI-based methods. In this paper, we use incremental
sampling-based algorithms to sample useful trajectories from the unforced
system dynamics, and make a novel connection between Rapidly-exploring Random
Trees (RRTs) and information-theoretic stochastic optimal control. We show the
results from the numerical implementation of the proposed approach to several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7393</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7393</id><created>2014-01-07</created><authors><author><keyname>Desai</keyname><forenames>Kalpit V</forenames></author><author><keyname>Ranjan</keyname><forenames>Roopesh</forenames></author></authors><title>Insights from the Wikipedia Contest (IEEE Contest for Data Mining 2011)</title><categories>cs.CY physics.soc-ph stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Wikimedia Foundation has recently observed that newly joining editors on
Wikipedia are increasingly failing to integrate into the Wikipedia editors'
community, i.e. the community is becoming increasingly harder to penetrate. To
sustain healthy growth of the community, the Wikimedia Foundation aims to
quantitatively understand the factors that determine the editing behavior, and
explain why most new editors become inactive soon after joining. As a step
towards this broader goal, the Wikimedia foundation sponsored the ICDM (IEEE
International Conference for Data Mining) contest for the year 2011.
  The objective for the participants was to develop models to predict the
number of edits that an editor will make in future five months based on the
editing history of the editor. Here we describe the approach we followed for
developing predictive models towards this goal, the results that we obtained
and the modeling insights that we gained from this exercise. In addition,
towards the broader goal of Wikimedia Foundation, we also summarize the factors
that emerged during our model building exercise as powerful predictors of
future editing activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7397</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7397</id><created>2014-05-28</created><authors><author><keyname>Gayen</keyname><forenames>Vivekananda</forenames></author><author><keyname>Sarkar</keyname><forenames>Kamal</forenames></author></authors><title>An HMM Based Named Entity Recognition System for Indian Languages: The
  JU System at ICON 2013</title><categories>cs.CL</categories><comments>The ICON 2013 tools contest on Named Entity Recognition in Indian
  languages (IL) co-located with the 10th International Conference on Natural
  Language Processing(ICON), CDAC Noida, India,18-20 December, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named
Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,
Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model
has been used to implement our system. The system has been trained and tested
on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of
0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,
English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7406</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7406</id><created>2014-05-28</created><authors><author><keyname>Osuna-Enciso</keyname><forenames>Valent&#xed;n</forenames></author><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Sossa</keyname><forenames>Humberto</forenames></author></authors><title>A Comparison of Nature Inspired Algorithms for Multi-threshold Image
  Segmentation</title><categories>cs.CV cs.NE</categories><comments>16 pages, this is a draft of the final version of the article sent to
  the Journal</comments><journal-ref>Expert Systems with Applications, Volume 40, Issue 4, March 2013,
  Pages 1213-1219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of image analysis, segmentation is one of the most important
preprocessing steps. One way to achieve segmentation is by mean of threshold
selection, where each pixel that belongs to a determined class islabeled
according to the selected threshold, giving as a result pixel groups that share
visual characteristics in the image. Several methods have been proposed in
order to solve threshold selectionproblems; in this work, it is used the method
based on the mixture of Gaussian functions to approximate the 1D histogram of a
gray level image and whose parameters are calculated using three nature
inspired algorithms (Particle Swarm Optimization, Artificial Bee Colony
Optimization and Differential Evolution). Each Gaussian function approximates
thehistogram, representing a pixel class and therefore a threshold point.
Experimental results are shown, comparing in quantitative and qualitative
fashion as well as the main advantages and drawbacks of each algorithm, applied
to multi-threshold problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7412</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7412</id><created>2014-05-28</created><updated>2014-10-02</updated><authors><author><keyname>Zhu</keyname><forenames>Dengkui</forenames></author><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Liang</keyname><forenames>Ping</forenames></author></authors><title>Power Allocation for Precoding in Large-Scale MIMO Systems with
  Per-Antenna Constraint</title><categories>cs.IT math.IT</categories><comments>submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale MIMO systems have been considered as one of the possible
candidates for the next-generation wireless communication technique, due to
their potential to provide significant higher throughput than conventional
wireless systems. For such systems, Zero-Forcing (ZF) and Conjugate Beamforming
(CB) precoding have been considered as two possible practical spatial
multiplexing techniques, and their average achievable sum rates have been
derived on the sum power constraint. However, in practice, the transmitting
power at a base station is constrained under each antenna. In this case, the
optimal power allocation is a very difficult problem. In this paper, the
suboptimal power allocation methods for both ZF-based and CB-based precoding in
large-scale MIMO systems under per-antenna constraint are investigated, which
could provide useful references for practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7418</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7418</id><created>2014-05-28</created><updated>2014-07-05</updated><authors><author><keyname>Biryukov</keyname><forenames>Alex</forenames></author><author><keyname>Khovratovich</keyname><forenames>Dmitry</forenames></author><author><keyname>Pustogarov</keyname><forenames>Ivan</forenames></author></authors><title>Deanonymisation of clients in Bitcoin P2P network</title><categories>cs.CR</categories><comments>15 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a digital currency which relies on a distributed set of miners to
mint coins and on a peer-to-peer network to broadcast transactions. The
identities of Bitcoin users are hidden behind pseudonyms (public keys) which
are recommended to be changed frequently in order to increase transaction
unlinkability.
  We present an efficient method to deanonymize Bitcoin users, which allows to
link user pseudonyms to the IP addresses where the transactions are generated.
Our techniques work for the most common and the most challenging scenario when
users are behind NATs or firewalls of their ISPs. They allow to link
transactions of a user behind a NAT and to distinguish connections and
transactions of different users behind the same NAT. We also show that a
natural countermeasure of using Tor or other anonymity services can be cut-off
by abusing anti-DoS countermeasures of the bitcoin network. Our attacks require
only a few machines and have been experimentally verified. We propose several
countermeasures to mitigate these new attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7421</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7421</id><created>2014-05-28</created><updated>2015-10-26</updated><authors><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Optimal Sampling-Based Motion Planning under Differential Constraints:
  the Drift Case with Linear Affine Dynamics</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a thorough, rigorous theoretical framework to assess
optimality guarantees of sampling-based algorithms for drift control systems:
systems that, loosely speaking, can not stop instantaneously due to momentum.
We exploit this framework to design and analyze a sampling-based algorithm (the
Differential Fast Marching Tree algorithm) that is asymptotically optimal, that
is, it is guaranteed to converge, as the number of samples increases, to an
optimal solution. In addition, our approach allows us to provide concrete
bounds on the rate of this convergence. The focus of this paper is on mixed
time/control energy cost functions and on linear affine dynamical systems,
which encompass a range of models of interest to applications (e.g.,
double-integrators) and represent a necessary step to design, via successive
linearization, sampling-based and provably-correct algorithms for non-linear
drift control systems. Our analysis relies on an original perturbation analysis
for two-point boundary value problems, which could be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7424</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7424</id><created>2014-05-28</created><authors><author><keyname>Wetzels</keyname><forenames>Jos</forenames></author></authors><title>Broken keys to the kingdom: Security and privacy aspects of RFID-based
  car keys</title><categories>cs.CR</categories><comments>20 pages</comments><acm-class>K.6.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents an overview of the current state-of-the-art of security
and privacy concerns regarding RFID-based car key applications. We will first
present a general overview of the technology and its evolution before moving on
to an overview and discussion of the various known security weaknesses and
attacks against such systems and the associated privacy risks they introduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7430</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7430</id><created>2014-05-28</created><authors><author><keyname>Martinez-Cantin</keyname><forenames>Ruben</forenames></author></authors><title>BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization,
  Experimental Design and Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BayesOpt is a library with state-of-the-art Bayesian optimization methods to
solve nonlinear optimization, stochastic bandits or sequential experimental
design problems. Bayesian optimization is sample efficient by building a
posterior distribution to capture the evidence and prior knowledge for the
target function. Built in standard C++, the library is extremely efficient
while being portable and flexible. It includes a common interface for C, C++,
Python, Matlab and Octave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7440</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7440</id><created>2014-05-28</created><authors><author><keyname>Nguyen</keyname><forenames>Hieu Duy</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Improper Signaling for Symbol Error Rate Minimization in K-User
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>28 pages, 9 figures, 3 tables. Submitted to IEEE Transactions on
  Communications. H. D. Nguyen and S. Sun are with the Institute for Infocomm
  Research (I2R), ASTAR, Singapore (email(s): {nguyendh,
  sunsm}@i2r.a-star.edu.sg). R. Zhang is with the ECE Department, National
  University of Singapore (email: elezhang@nus.edu.sg). He is also with the
  I2R, ASTAR, Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate maximization for the $K$-user interference channels (ICs) has been
investigated extensively in the literature. However, the dual problem of
minimizing the error probability with given signal modulations and/or data
rates of the users is less exploited. In this paper, by utilizing the
additional degrees of freedom attained from the improper signaling (versus the
conventional proper signaling), we optimize the precoding matrices for the
$K$-user single-input single-output (SISO) ICs to achieve minimal pair-wise
error probability (PEP) and symbol error rate (SER) with two proposed
algorithms, respectively. Compared to conventional proper signaling as well as
other state-of-the-art improper signaling designs, our proposed improper
signaling schemes achieve notable SER improvement in SISO-ICs under both
additive white Gaussian noise (AWGN) channel and cellular system setups. Our
study provides another viewpoint for optimizing transmissions in ICs and
further justifies the practical benefit of improper signaling in
interference-limited communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7441</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7441</id><created>2014-05-28</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Key Capacity with Limited One-Way Communication for Product Sources</title><categories>cs.IT math.IT</categories><comments>5 pages, ISIT 2014</comments><journal-ref>2014 IEEE International Symposium on Information Theory (ISIT),
  pages: 1146 - 1150, June 29-July 4 2014, Honolulu, HI</journal-ref><doi>10.1109/ISIT.2014.6875012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for product sources, rate splitting is optimal for secret key
agreement using limited one-way communication at two terminals. This yields an
alternative proof of the tensorization property of a strong data processing
inequality originally studied by Erkip and Cover and amended recently by
Anantharam et al. We derive a `water-filling' solution of the
communication-rate--key-rate tradeoff for two arbitrarily correlated vector
Gaussian sources, for the case with an eavesdropper, and for stationary
Gaussian processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7442</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7442</id><created>2014-05-28</created><updated>2014-07-29</updated><authors><author><keyname>Favier</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>de Almeida</keyname><forenames>Andr&#xe9; L. F.</forenames></author></authors><title>Overview of Constrained PARAFAC Models</title><categories>cs.NA</categories><doi>10.1186/1687-6180-2014-142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an overview of constrained PARAFAC models where the
constraints model linear dependencies among columns of the factor matrices of
the tensor decomposition, or alternatively, the pattern of interactions between
different modes of the tensor which are captured by the equivalent core tensor.
Some tensor prerequisites with a particular emphasis on mode combination using
Kronecker products of canonical vectors that makes easier matricization
operations, are first introduced. This Kronecker product based approach is also
formulated in terms of the index notation, which provides an original and
concise formalism for both matricizing tensors and writing tensor models. Then,
after a brief reminder of PARAFAC and Tucker models, two families of
constrained tensor models, the co-called PARALIND/CONFAC and PARATUCK models,
are described in a unified framework, for $N^{th}$ order tensors. New tensor
models, called nested Tucker models and block PARALIND/CONFAC models, are also
introduced. A link between PARATUCK models and constrained PARAFAC models is
then established. Finally, new uniqueness properties of PARATUCK models are
deduced from sufficient conditions for essential uniqueness of their associated
constrained PARAFAC models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7446</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7446</id><created>2014-05-28</created><authors><author><keyname>Erpek</keyname><forenames>Tugba</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>An Optimal Application-Aware Resource Block Scheduling in LTE</title><categories>cs.NI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an approach for application-aware resource block
scheduling of elastic and inelastic adaptive real-time traffic in fourth
generation Long Term Evolution (LTE) systems. The users are assigned to
resource blocks. A transmission may use multiple resource blocks scheduled over
frequency and time. In our model, we use logarithmic and sigmoidal-like utility
functions to represent the users applications running on different user
equipments (UE)s. We present an optimal problem with utility proportional
fairness policy, where the fairness among users is in utility percentage (i.e
user satisfaction with the service) of the corresponding applications. Our
objective is to allocate the resources to the users with priority given to the
adaptive real-time application users. In addition, a minimum resource
allocation for users with elastic and inelastic traffic should be guaranteed.
Every user subscribing for the mobile service should have a minimum
quality-of-service (QoS) with a priority criterion. We prove that our
scheduling policy exists and achieves the maximum. Therefore the optimal
solution is tractable. We present a centralized scheduling algorithm to
allocate evolved NodeB (eNodeB) resources optimally with a priority criterion.
Finally, we present simulation results for the performance of our scheduling
algorithm and compare our results with conventional proportional fairness
approaches. The results show that the user satisfaction is higher with our
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7452</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7452</id><created>2014-05-28</created><updated>2014-06-14</updated><authors><author><keyname>Althoff</keyname><forenames>Tim</forenames></author><author><keyname>Borth</keyname><forenames>Damian</forenames></author><author><keyname>Hees</keyname><forenames>J&#xf6;rn</forenames></author><author><keyname>Dengel</keyname><forenames>Andreas</forenames></author></authors><title>Analysis and Forecasting of Trending Topics in Online Media Streams</title><categories>cs.SI cs.MM physics.soc-ph</categories><comments>ACM Multimedia 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the vast information available on the web, social media streams capture
what people currently pay attention to and how they feel about certain topics.
Awareness of such trending topics plays a crucial role in multimedia systems
such as trend aware recommendation and automatic vocabulary selection for video
concept detection systems.
  Correctly utilizing trending topics requires a better understanding of their
various characteristics in different social media streams. To this end, we
present the first comprehensive study across three major online and social
media streams, Twitter, Google, and Wikipedia, covering thousands of trending
topics during an observation period of an entire year. Our results indicate
that depending on one's requirements one does not necessarily have to turn to
Twitter for information about current events and that some media streams
strongly emphasize content of specific categories. As our second key
contribution, we further present a novel approach for the challenging task of
forecasting the life cycle of trending topics in the very moment they emerge.
Our fully automated approach is based on a nearest neighbor forecasting
technique exploiting our assumption that semantically similar topics exhibit
similar behavior.
  We demonstrate on a large-scale dataset of Wikipedia page view statistics
that forecasts by the proposed approach are about 9-48k views closer to the
actual viewing statistics compared to baseline methods and achieve a mean
average percentage error of 45-19% for time periods of up to 14 days.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7457</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7457</id><created>2014-05-29</created><authors><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Machchhar</keyname><forenames>Jinesh</forenames></author><author><keyname>Sohoni</keyname><forenames>Milind</forenames></author></authors><title>Incorporating Sharp Features in the General Solid Sweep Framework</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends a recently proposed robust computational framework for
constructing the boundary representation (brep) of the volume swept by a given
smooth solid moving along a one parameter family $h$ of rigid motions. Our
extension allows the input solid to have sharp features, i.e., to be of class
G0 wherein, the unit outward normal to the solid may be discontinuous. In the
earlier framework, the solid to be swept was restricted to be G1, and thus this
is a significant and useful extension of that work. This naturally requires a
precise description of the geometry of the surface generated by the sweep of a
sharp edge supported by two intersecting smooth faces. We uncover the geometry
along with the related issues like parametrization, self-intersection and
singularities via a novel mathematical analysis. Correct trimming of such a
surface is achieved by a delicate analysis of the interplay between the cone of
normals at a sharp point and its trajectory under $h$. The overall topology is
explicated by a key lifting theorem which allows us to compute the adjacency
relations amongst entities in the swept volume by relating them to
corresponding adjacencies in the input solid. Moreover, global issues related
to body-check such as orientation are efficiently resolved. Many examples from
a pilot implementation illustrate the efficiency and effectiveness of our
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7460</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7460</id><created>2014-05-29</created><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Jafarpour</keyname><forenames>Ashkan</forenames></author><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Universal Compression of Envelope Classes: Tight Characterization via
  Poisson Sampling</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Poisson-sampling technique eliminates dependencies among symbol
appearances in a random sequence. It has been used to simplify the analysis and
strengthen the performance guarantees of randomized algorithms. Applying this
method to universal compression, we relate the redundancies of fixed-length and
Poisson-sampled sequences, use the relation to derive a simple single-letter
formula that approximates the redundancy of any envelope class to within an
additive logarithmic term. As a first application, we consider i.i.d.
distributions over a small alphabet as a step-envelope class, and provide a
short proof that determines the redundancy of discrete distributions over a
small al- phabet up to the first order terms. We then show the strength of our
method by applying the formula to tighten the existing bounds on the redundancy
of exponential and power-law classes, in particular answering a question posed
by Boucheron, Garivier and Gassiat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7461</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7461</id><created>2014-05-29</created><updated>2014-09-12</updated><authors><author><keyname>Gowanlock</keyname><forenames>Michael G.</forenames></author><author><keyname>Casanova</keyname><forenames>Henri</forenames></author></authors><title>Technical Report: Parallel Distance Threshold Query Processing for
  Spatiotemporal Trajectory Databases on the GPU</title><categories>cs.DB cs.DC</categories><comments>35 pages, 16 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing moving object trajectories arises in many application domains and
has been addressed by practitioners in the spatiotemporal database and
Geographical Information System communities. In this work, we focus on a
trajectory similarity search, the distance threshold query, which finds all
trajectories within a given distance d of a search trajectory over a time
interval. We demonstrate the performance of a multithreaded implementation
which features the use of an R-tree index and which has high parallel
efficiency (78%-90%). We introduce a GPGPU implementation which avoids the use
of index-trees, and instead features a GPU-friendly indexing method. We compare
the performance of the multithreaded and GPU implementations, and show that a
speedup can be obtained using the latter. We propose two classes of algorithms,
SetSplit and GreedySetSplit, to create efficient query batches that reduce
memory pressure and computational cost on the GPU. However, we find that using
fixed-size batches is sufficiently efficient in practice. We develop an
empirical performance model for our GPGPU implementation that can be used to
predict the response time of the distance threshold query. This model can be
used to pick a good query batch size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7464</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7464</id><created>2014-05-29</created><updated>2014-10-05</updated><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Cross-Error Correcting Integer Codes over $\mathbb{Z}_{2^m}$</title><categories>cs.IT math.IT</categories><comments>To be published in the proceedings of ISITA 2014, IEICE copyright</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate codes in $\mathbb{Z}_{2^m}^n$ that can correct
errors that occur in just one coordinate of the codeword, with a magnitude of
up to a given parameter $t$. We will show upper bounds on these cross codes,
derive constructions for linear codes and respective decoding algorithm. The
constructions (and decoding algorithms) are given for length $n = 2$ and $n =
3$, but for general $m$ and $t$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7470</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7470</id><created>2014-05-29</created><authors><author><keyname>Kl&#xf6;ckner</keyname><forenames>Andreas</forenames></author></authors><title>Loo.py: transformation-based code generation for GPUs and CPUs</title><categories>cs.PL cs.MS math.NA</categories><acm-class>D.3.4; D.1.3; G.4</acm-class><journal-ref>Proceedings of ARRAY 2014: ACM SIGPLAN Workshop on Libraries,
  Languages, and Compilers for Array Programming</journal-ref><doi>10.1145/2627373.2627387</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's highly heterogeneous computing landscape places a burden on
programmers wanting to achieve high performance on a reasonably broad
cross-section of machines. To do so, computations need to be expressed in many
different but mathematically equivalent ways, with, in the worst case, one
variant per target machine.
  Loo.py, a programming system embedded in Python, meets this challenge by
defining a data model for array-style computations and a library of
transformations that operate on this model. Offering transformations such as
loop tiling, vectorization, storage management, unrolling, instruction-level
parallelism, change of data layout, and many more, it provides a convenient way
to capture, parametrize, and re-unify the growth among code variants. Optional,
deep integration with numpy and PyOpenCL provides a convenient computing
environment where the transition from prototype to high-performance
implementation can occur in a gradual, machine-assisted form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7471</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7471</id><created>2014-05-29</created><authors><author><keyname>Bora</keyname><forenames>Mr. Dibya Jyoti</forenames></author><author><keyname>Gupta</keyname><forenames>Dr. Anil Kumar</forenames></author></authors><title>Effect of Different Distance Measures on the Performance of K-Means
  Algorithm: An Experimental Study in Matlab</title><categories>cs.LG</categories><comments>6 pages, 11 figures, Clustering, K Means</comments><journal-ref>International Journal of Computer Science and Information
  Technologies,(IJCSIT), Vol. 5 (2) , 2014, 2501-2506,ISSN 0975-9646</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K-means algorithm is a very popular clustering algorithm which is famous for
its simplicity. Distance measure plays a very important rule on the performance
of this algorithm. We have different distance measure techniques available. But
choosing a proper technique for distance calculation is totally dependent on
the type of the data that we are going to cluster. In this paper an
experimental study is done in Matlab to cluster the iris and wine data sets
with different distance measures and thereby observing the variation of the
performances shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7475</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7475</id><created>2014-05-29</created><authors><author><keyname>Tippenhauer</keyname><forenames>Nils Ole</forenames></author><author><keyname>Temple</keyname><forenames>William G.</forenames></author><author><keyname>Vu</keyname><forenames>An Hoa</forenames></author><author><keyname>Chen</keyname><forenames>Binbin</forenames></author><author><keyname>Nicol</keyname><forenames>David M.</forenames></author><author><keyname>Kalbarczyk</keyname><forenames>Zbigniew</forenames></author><author><keyname>Sanders</keyname><forenames>William H.</forenames></author></authors><title>Automatic Generation of Security Argument Graphs</title><categories>cs.CR</categories><comments>10 pages, 8 figures, 1 table and 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based assessment formalisms have proven to be useful in the safety,
dependability, and security communities to help stakeholders manage risk and
maintain appropriate documentation throughout the system lifecycle. In this
paper, we propose a set of methods to automatically construct security argument
graphs, a graphical formalism that integrates various security-related
information to argue about the security level of a system. Our approach is to
generate the graph in a progressive manner by exploiting logical relationships
among pieces of diverse input information. Using those emergent argument
patterns as a starting point, we define a set of extension templates that can
be applied iteratively to grow a security argument graph. Using a scenario from
the electric power sector, we demonstrate the graph generation process and
highlight its application for system security evaluation in our prototype
software tool, CyberSAGE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7484</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7484</id><created>2014-05-29</created><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Zheng</keyname><forenames>Kai</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author><author><keyname>Ren</keyname><forenames>Shaolei</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>Energy-Efficient Flow Scheduling and Routing with Hard Deadlines in Data
  Center Networks</title><categories>cs.NI</categories><comments>11 pages, accepted by ICDCS'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power consumption of enormous network devices in data centers has emerged
as a big concern to data center operators. Despite many
traffic-engineering-based solutions, very little attention has been paid on
performance-guaranteed energy saving schemes. In this paper, we propose a novel
energy-saving model for data center networks by scheduling and routing
&quot;deadline-constrained flows&quot; where the transmission of every flow has to be
accomplished before a rigorous deadline, being the most critical requirement in
production data center networks. Based on speed scaling and power-down energy
saving strategies for network devices, we aim to explore the most energy
efficient way of scheduling and routing flows on the network, as well as
determining the transmission speed for every flow. We consider two general
versions of the problem. For the version of only flow scheduling where routes
of flows are pre-given, we show that it can be solved polynomially and we
develop an optimal combinatorial algorithm for it. For the version of joint
flow scheduling and routing, we prove that it is strongly NP-hard and cannot
have a Fully Polynomial-Time Approximation Scheme (FPTAS) unless P=NP. Based on
a relaxation and randomized rounding technique, we provide an efficient
approximation algorithm which can guarantee a provable performance ratio with
respect to a polynomial of the total number of flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7487</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7487</id><created>2014-05-29</created><authors><author><keyname>AbdulJabbar</keyname><forenames>Mustafa</forenames></author><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Asynchronous Execution of the Fast Multipole Method Using Charm++</title><categories>cs.DC</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast multipole methods (FMM) on distributed mem- ory have traditionally used
a bulk-synchronous model of com- municating the local essential tree (LET) and
overlapping it with computation of the local data. This could be perceived as
an extreme case of data aggregation, where the whole LET is communicated at
once. Charm++ allows a much finer control over the granularity of
communication, and has a asynchronous execution model that fits well with the
structure of our FMM code. Unlike previous work on asynchronous fast N-body
methods such as ChaNGa and PEPC, the present work performs a direct comparison
against the traditional bulk-synchronous approach and the asynchronous approach
using Charm++. Furthermore, the serial performance of our FMM code is over an
order of magnitude better than these previous codes, so it is much more
challenging to hide the overhead of Charm++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7497</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7497</id><created>2014-05-29</created><authors><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>Carrieri</keyname><forenames>Anna Paola</forenames></author><author><keyname>Della Vedova</keyname><forenames>Gianluca</forenames></author><author><keyname>Trucco</keyname><forenames>Gabriella</forenames></author></authors><title>Algorithms for the Constrained Perfect Phylogeny with Persistent
  Characters</title><categories>cs.DS q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The perfect phylogeny is one of the most used models in different areas of
computational biology. In this paper we consider the problem of the Persistent
Perfect Phylogeny (referred as P-PP) recently introduced to extend the perfect
phylogeny model allowing persistent characters, that is characters can be
gained and lost at most once. We define a natural generalization of the P-PP
problem obtained by requiring that for some pairs (character, species), neither
the species nor any of its ancestors can have the character. In other words,
some characters cannot be persistent for some species. This new problem is
called Constrained P-PP (CP-PP).
  Based on a graph formulation of the CP-PP problem, we are able to provide a
polynomial time solution for the CP-PP problem for matrices having an empty
conflict-graph. In particular we show that all such matrices admit a persistent
perfect phylogeny in the unconstrained case. Using this result, we develop a
parameterized algorithm for solving the CP-PP problem where the parameter is
the number of characters. A preliminary experimental analysis of the algorithm
shows that it performs efficiently and it may analyze real haplotype data not
conforming to the classical perfect phylogeny model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7500</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7500</id><created>2014-05-29</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames></author><author><keyname>Klop</keyname><forenames>Jan Willem</forenames></author><author><keyname>Polonsky</keyname><forenames>Andrew</forenames></author></authors><title>An Introduction to the Clocked Lambda Calculus</title><categories>cs.LO</categories><doi>10.1017/S0960129515000389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a brief introduction to the clocked lambda calculus, an extension of
the classical lambda calculus with a unary symbol tau used to witness the
beta-steps. In contrast to the classical lambda calculus, this extension is
infinitary strongly normalising and infinitary confluent. The infinitary normal
forms are enriched Boehm Trees, which we call clocked Boehm Trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7519</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7519</id><created>2014-05-29</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Malhotra</keyname><forenames>Vikrant</forenames></author><author><keyname>Tyagi</keyname><forenames>Ridhi</forenames></author></authors><title>Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value</title><categories>cs.IR cs.CL</categories><comments>IJCSIT, MAY 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion Mining and Sentiment Analysis is a process of identifying opinions in
large unstructured/structured data and then analysing polarity of those
opinions. Opinion mining and sentiment analysis have found vast application in
analysing online ratings, analysing product based reviews, e-governance, and
managing hostile content over the internet. This paper proposes an algorithm to
implement aspect level sentiment analysis. The algorithm takes input from the
remarks submitted by various teachers of a student. An aspect tree is formed
which has various levels and weights are assigned to each branch to identify
level of aspect. Aspect value is calculated by the algorithm by means of the
proposed aspect tree. Dictionary based method is implemented to evaluate the
polarity of the remark. The algorithm returns the aspect value clubbed with
opinion value and sentiment value which helps in concluding the summarized
value of remark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7520</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7520</id><created>2014-05-29</created><updated>2015-06-11</updated><authors><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>Della Vedova</keyname><forenames>Gianluca</forenames></author><author><keyname>Pirola</keyname><forenames>Yuri</forenames></author><author><keyname>Previtali</keyname><forenames>Marco</forenames></author><author><keyname>Rizzi</keyname><forenames>Raffaella</forenames></author></authors><title>An External-Memory Algorithm for String Graph Construction</title><categories>cs.DS q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some recent results have introduced external-memory algorithms to compute
self-indexes of a set of strings, mainly via computing the Burrows-Wheeler
Transform (BWT) of the input strings. The motivations for those results stem
from Bioinformatics, where a large number of short strings (called reads) are
routinely produced and analyzed. In that field, a fundamental problem is to
assemble a genome from a large set of much shorter samples extracted from the
unknown genome. The approaches that are currently used to tackle this problem
are memory-intensive. This fact does not bode well with the ongoing increase in
the availability of genomic data. A data structure that is used in genome
assembly is the string graph, where vertices correspond to samples and arcs
represent two overlapping samples. In this paper we address an open problem: to
design an external-memory algorithm to compute the string graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7538</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7538</id><created>2014-05-29</created><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Michel</keyname><forenames>Jerod</forenames></author><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>On the Existence of Certain Optimal Self-Dual Codes with Lengths Between
  $74$ and $116$</title><categories>cs.IT math.IT</categories><comments>15 pages, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of optimal binary self-dual codes is a long-standing research
problem. In this paper, we present some results concerning the decomposition of
binary self-dual codes with a dihedral automorphism group $D_{2p}$, where $p$
is a prime. These results are applied to construct new self-dual codes with
length $78$ or $116$. We obtain $16$ inequivalent self-dual $[78,39,14]$ codes,
four of which have new weight enumerators. We also show that there are at least
$141$ inequivalent self-dual $[116,58,18]$ codes, most of which are new up to
equivalence. Meanwhile, we give some restrictions on the weight enumerators of
singly even self-dual codes. We use these restrictions to exclude some possible
weight enumerators of self-dual codes with lengths $74$, $76$, $82$, $98$ and
$100$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7544</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7544</id><created>2014-05-29</created><authors><author><keyname>Nguyen</keyname><forenames>Hai Thanh</forenames></author><author><keyname>Mary</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Preux</keyname><forenames>Philippe</forenames></author></authors><title>Cold-start Problems in Recommendation Systems via Contextual-bandit
  Algorithms</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a cold-start problem in recommendation systems where
we have completely new users entered the systems. There is not any interaction
or feedback of the new users with the systems previoustly, thus no ratings are
available. Trivial approaches are to select ramdom items or the most popular
ones to recommend to the new users. However, these methods perform poorly in
many case. In this research, we provide a new look of this cold-start problem
in recommendation systems. In fact, we cast this cold-start problem as a
contextual-bandit problem. No additional information on new users and new items
is needed. We consider all the past ratings of previous users as contextual
information to be integrated into the recommendation framework. To solve this
type of the cold-start problems, we propose a new efficient method which is
based on the LinUCB algorithm for contextual-bandit problems. The experiments
were conducted on three different publicly-available data sets, namely
Movielens, Netflix and Yahoo!Music. The new proposed methods were also compared
with other state-of-the-art techniques. Experiments showed that our new method
significantly improves upon all these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7545</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7545</id><created>2014-05-29</created><authors><author><keyname>Sapienza</keyname><forenames>Michael</forenames></author><author><keyname>Cuzzolin</keyname><forenames>Fabio</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Feature sampling and partitioning for visual vocabulary generation on
  large action classification datasets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent trend in action recognition is towards larger datasets, an
increasing number of action classes and larger visual vocabularies.
State-of-the-art human action classification in challenging video data is
currently based on a bag-of-visual-words pipeline in which space-time features
are aggregated globally to form a histogram. The strategies chosen to sample
features and construct a visual vocabulary are critical to performance, in fact
often dominating performance. In this work we provide a critical evaluation of
various approaches to building a vocabulary and show that good practises do
have a significant impact. By subsampling and partitioning features
strategically, we are able to achieve state-of-the-art results on 5 major
action recognition datasets using relatively small visual vocabularies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7567</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7567</id><created>2014-05-29</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author><author><keyname>Salem</keyname><forenames>Abdel-Badeeh M.</forenames></author></authors><title>Analogy-Based and Case-Based Reasoning: Two sides of the same coin</title><categories>cs.AI</categories><comments>47 pages, 2 figures, 1 table, 124 references</comments><msc-class>68T05, 68T20</msc-class><journal-ref>International Journal of Applications of Fuzzy Sets and Artificial
  Intelligence (IJAFSAI), Vol. 4, 5-51, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analogy-Based (or Analogical) and Case-Based Reasoning (ABR and CBR) are two
similar problem solving processes based on the adaptation of the solution of
past problems for use with a new analogous problem. In this paper we review
these two processes and we give some real world examples with emphasis to the
field of Medicine, where one can find some of the most common and useful CBR
applications. We also underline the differences between CBR and the classical
rule-induction algorithms, we discuss the criticism for CBR methods and we
focus on the future trends of research in the area of CBR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7571</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7571</id><created>2014-05-29</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Ng</keyname><forenames>Tian-Tsong</forenames></author><author><keyname>Li</keyname><forenames>Xiaolong</forenames></author><author><keyname>Tan</keyname><forenames>Shunquan</forenames></author><author><keyname>Huang</keyname><forenames>Jiwu</forenames></author></authors><title>JPEG Noises beyond the First Compression Cycle</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the JPEG noises, which include the quantization noise
and the rounding noise, during a JPEG compression cycle. The JPEG noises in the
first compression cycle have been well studied; however, so far less attention
has been paid on the JPEG noises in higher compression cycles. In this work, we
present a statistical analysis on JPEG noises beyond the first compression
cycle. To our knowledge, this is the first work on this topic. We find that the
noise distributions in higher compression cycles are different from those in
the first compression cycle, and they are dependent on the quantization
parameters used between two successive cycles. To demonstrate the benefits from
the statistical analysis, we provide two applications that can employ the
derived noise distributions to uncover JPEG compression history with
state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7584</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7584</id><created>2014-05-29</created><authors><author><keyname>Frailis</keyname><forenames>Marco</forenames><affiliation>INAF-Osservatorio Astronomico di Trieste</affiliation></author><author><keyname>Sartor</keyname><forenames>Stefano</forenames><affiliation>INAF-Osservatorio Astronomico di Trieste</affiliation></author><author><keyname>Zacchei</keyname><forenames>Andrea</forenames><affiliation>INAF-Osservatorio Astronomico di Trieste</affiliation></author><author><keyname>Lodi</keyname><forenames>Marcello</forenames><affiliation>Telescopio Nazionale Galileo FGG - INAF</affiliation></author><author><keyname>Cirami</keyname><forenames>Roberto</forenames><affiliation>INAF-Osservatorio Astronomico di Trieste</affiliation></author><author><keyname>Pasian</keyname><forenames>Fabio</forenames><affiliation>INAF-Osservatorio Astronomico di Trieste</affiliation></author><author><keyname>Trifoglio</keyname><forenames>Massimo</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Bulgarelli</keyname><forenames>Andrea</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Gianotti</keyname><forenames>Fulvio</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Franceschi</keyname><forenames>Enrico</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Nicastro</keyname><forenames>Luciano</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Conforti</keyname><forenames>Vito</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Zoli</keyname><forenames>Andrea</forenames><affiliation>INAF-IASF Bologna</affiliation></author><author><keyname>Smart</keyname><forenames>Ricky</forenames><affiliation>INAF-Osservatorio Astrofisico di Torino</affiliation></author><author><keyname>Morbidelli</keyname><forenames>Roberto</forenames><affiliation>INAF-Osservatorio Astrofisico di Torino</affiliation></author><author><keyname>Dadina</keyname><forenames>Mauro</forenames><affiliation>INAF-IASF Bologna</affiliation></author></authors><title>DAS: a data management system for instrument tests and operations</title><categories>astro-ph.IM cs.DB</categories><comments>Accepted for pubblication on ADASS Conference Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Data Access System (DAS) is a metadata and data management software
system, providing a reusable solution for the storage of data acquired both
from telescopes and auxiliary data sources during the instrument development
phases and operations. It is part of the Customizable Instrument WorkStation
system (CIWS-FW), a framework for the storage, processing and quick-look at the
data acquired from scientific instruments. The DAS provides a data access layer
mainly targeted to software applications: quick-look displays, pre-processing
pipelines and scientific workflows. It is logically organized in three main
components: an intuitive and compact Data Definition Language (DAS DDL) in XML
format, aimed for user-defined data types; an Application Programming Interface
(DAS API), automatically adding classes and methods supporting the DDL data
types, and providing an object-oriented query language; a data management
component, which maps the metadata of the DDL data types in a relational Data
Base Management System (DBMS), and stores the data in a shared (network) file
system. With the DAS DDL, developers define the data model for a particular
project, specifying for each data type the metadata attributes, the data format
and layout (if applicable), and named references to related or aggregated data
types. Together with the DDL user-defined data types, the DAS API acts as the
only interface to store, query and retrieve the metadata and data in the DAS
system, providing both an abstract interface and a data model specific one in
C, C++ and Python. The mapping of metadata in the back-end database is
automatic and supports several relational DBMSs, including MySQL, Oracle and
PostgreSQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7586</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7586</id><created>2014-05-29</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Montali</keyname><forenames>Eugenio</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>AONT-LT: a Data Protection Scheme for Cloud and Cooperative Storage
  Systems</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 8 figures, to be presented at the 2014 High Performance
  Computing &amp; Simulation Conference (HPCS 2014) - Workshop on Security, Privacy
  and Performance in Cloud Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a variant of the well-known AONT-RS scheme for dispersed storage
systems. The novelty consists in replacing the Reed-Solomon code with rateless
Luby transform codes. The resulting system, named AONT-LT, is able to improve
the performance by dispersing the data over an arbitrarily large number of
storage nodes while ensuring limited complexity. The proposed solution is
particularly suitable in the case of cooperative storage systems. It is shown
that while the AONT-RS scheme requires the adoption of fragmentation for
achieving widespread distribution, thus penalizing the performance, the new
AONT-LT scheme can exploit variable length codes which allow to achieve very
good performance and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7596</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7596</id><created>2014-05-29</created><authors><author><keyname>Jastrz&#x119;bski</keyname><forenames>Micha&#x142;</forenames></author></authors><title>On total communication complexity of collapsing protocols for pointer
  jumping problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on bounding the total communication complexity of
collapsing protocols for multiparty pointer jumping problem ($MPJ_k^n$). Brody
and Chakrabati in \cite{bc08} proved that in such setting one of the players
must communicate at least $n - 0.5\log{n}$ bits. Liang in \cite{liang} has
shown protocol matching this lower bound on maximum complexity. His protocol,
however, was behaving worse than the trivial one in terms of total complexity
(number of bits sent by all players). He conjectured that achieving total
complexity better then the trivial one is impossible. In this paper we prove
this conjecture. Namely, we show that for a collapsing protocol for $MPJ_k^n$,
the total communication complexity is at least $n-2$ which closes the gap
between lower and upper bound for total complexity of $MPJ_k^n$ in collapsing
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7601</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7601</id><created>2014-05-29</created><updated>2014-08-07</updated><authors><author><keyname>Petroni</keyname><forenames>Nicola Cufaro</forenames></author></authors><title>Entropy and its discontents: A note on definitions</title><categories>cs.IT math.IT physics.comp-ph physics.data-an</categories><comments>18 pages, 7 figures; minor modifications required by referees; 1
  reference and Acknowledgements added</comments><msc-class>94A17, 54C70</msc-class><journal-ref>Entropy (2014), 16, 4044-4059</journal-ref><doi>10.3390/e16074044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The routine definitions of both entropy, and differential entropy show
inconsistencies that make them not reciprocally coherent. We propose a few
possible modifications of these quantities so that 1) they no longer show
incongruities, 2) they go one into the other in a suitable limit as the result
of a renormalization. The properties of the new quantities would slightly
differ from that of the usual entropies in a few other respects
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7615</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7615</id><created>2014-05-29</created><updated>2014-06-08</updated><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Richards</keyname><forenames>Arthur</forenames></author></authors><title>Formal Verification of Control Systems Properties with Theorem Proving</title><categories>cs.SY cs.LO</categories><comments>Accepted to be presented in UKACC, Loughborough, UK, 2014. Final
  reference to appear on publication</comments><journal-ref>Proc. IEEE CONTROL (International Conference on Control) 2014, pp.
  244 - 249, Loughborough, UK</journal-ref><doi>10.1109/CONTROL.2014.6915147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the deductive formal verification of high-level
properties of control systems with theorem proving, using the Why3 tool.
Properties that can be verified with this approach include stability, feedback
gain, and robustness, among others. For the systems, modelled in Simulink, we
propose three main steps to achieve the verification: specifying the properties
of interest over the signals within the model using Simulink blocks, an
automatic translation of the model into Why3, and the automatic verification of
the properties with theorem provers in Why3. We present a methodology to
specify the properties in the model and a library of relevant assertion blocks
(logic expressions), currently in development. The functionality of the blocks
in the Simulink models are automatically translated to Why3 as theories and
verification goals by our tool implemented in MATLAB. A library of theories in
Why3 corresponding to each supported block has been developed to facilitate the
process of translation. The goals are automatically verified in Why3 with
relevant theorem provers. A simple first-order discrete system is used to
exemplify the specification of the Simulink model, the translation process from
Simulink to the Why3 formal logic language, and the verification of Lyapunov
stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7619</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7619</id><created>2014-05-29</created><authors><author><keyname>Wilson</keyname><forenames>David B.</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>A forward-backward single-source shortest paths algorithm</title><categories>cs.DS math.PR</categories><msc-class>05C85, 68Q87</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new forward-backward variant of Dijkstra's and Spira's
Single-Source Shortest Paths (SSSP) algorithms. While essentially all SSSP
algorithm only scan edges forward, the new algorithm scans some edges backward.
The new algorithm assumes that edges in the outgoing and incoming adjacency
lists of the vertices appear in non-decreasing order of weight. (Spira's
algorithm makes the same assumption about the outgoing adjacency lists, but
does not use incoming adjacency lists.) The running time of the algorithm on a
complete directed graph on $n$ vertices with independent exponential edge
weights is $O(n)$, with very high probability. This improves on the previously
best result of $O(n\log n)$, which is best possible if only forward scans are
allowed, exhibiting an interesting separation between forward-only and
forward-backward SSSP algorithms. As a consequence, we also get a new all-pairs
shortest paths algorithm. The expected running time of the algorithm on
complete graphs with independent exponential edge weights is $O(n^2)$, matching
a recent algorithm of Demetrescu and Italiano as analyzed by Peres et al.
Furthermore, the probability that the new algorithm requires more than $O(n^2)$
time is exponentially small, improving on the $O(n^{-1/26})$ probability bound
obtained by Peres et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7624</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7624</id><created>2014-05-29</created><authors><author><keyname>Peralta</keyname><forenames>Billy</forenames></author></authors><title>Simultaneous Feature and Expert Selection within Mixture of Experts</title><categories>cs.LG</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A useful strategy to deal with complex classification scenarios is the
&quot;divide and conquer&quot; approach. The mixture of experts (MOE) technique makes use
of this strategy by joinly training a set of classifiers, or experts, that are
specialized in different regions of the input space. A global model, or gate
function, complements the experts by learning a function that weights their
relevance in different parts of the input space. Local feature selection
appears as an attractive alternative to improve the specialization of experts
and gate function, particularly, for the case of high dimensional data. Our
main intuition is that particular subsets of dimensions, or subspaces, are
usually more appropriate to classify instances located in different regions of
the input space. Accordingly, this work contributes with a regularized variant
of MoE that incorporates an embedded process for local feature selection using
$L1$ regularization, with a simultaneous expert selection. The experiments are
still pending.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7626</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7626</id><created>2014-05-29</created><authors><author><keyname>Kambo</keyname><forenames>Rubi</forenames></author><author><keyname>Yerpude</keyname><forenames>Amit</forenames></author></authors><title>Classification of Basmati Rice Grain Variety using Image Processing and
  Principal Component Analysis</title><categories>cs.CV</categories><comments>6 pages from page no:80-85, 8 Figures</comments><journal-ref>IJAREEIE, vol. 2, no. 7, pp. 2893-2900, july 2013</journal-ref><doi>10.14445/22312803/IJCTT-V11P117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All important decisions about the variety of rice grain end product are based
on the different features of rice grain.There are various methods available for
classification of basmati rice. This paper proposed a new principal component
analysis based approach for classification of different variety of basmati
rice. The experimental result shows the effectiveness of the proposed
methodology for various samples of different variety of basmati rice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7629</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7629</id><created>2014-05-28</created><authors><author><keyname>Bradai</keyname><forenames>Abbas</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Medjiah</keyname><forenames>Samir</forenames><affiliation>LaBRI</affiliation></author></authors><title>QoE assessment for SVC streaming in ENVISION</title><categories>cs.MM</categories><comments>IEEE 20th International Conference on Electronics, Circuits, and
  Systems (IEEE ICECS 2013), Abu Dhabi : United Arab Emirates (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable video coding has drawn great interest in content delivery in many
multimedia services thanks to its capability to handle terminal heterogeneity
and network conditions variation. In our previous work, and under the umbrella
of ENVISION, we have proposed a playout smoothing mechanism to ensure the
uniform delivery of the layered stream, by reducing the quality changes that
the stream undergoes when adapting to changing network conditions. In this
paper we study the resulting video quality, from the final user perception
under different network conditions of loss and delays. For that we have adopted
the Double Stimulus Impairment Scale (DSIS) method. The results show that the
Mean Opinion Score for the smoothed video clips was higher under different
network configuration. This confirms the effectiveness of the proposed
smoothing mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7631</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7631</id><created>2014-05-29</created><authors><author><keyname>Mehdiabadi</keyname><forenames>Motahareh Eslami</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author></authors><title>Diffusion-Aware Sampling and Estimation in Information Diffusion
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 4 figures, Published in: International Confernece on Social
  Computing 2012 (SocialCom12)</comments><doi>10.1109/SocialCom-PASSAT.2012.98</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially-observed data collected by sampling methods is often being studied
to obtain the characteristics of information diffusion networks. However, these
methods usually do not consider the behavior of diffusion process. In this
paper, we propose a novel two-step (sampling/estimation) measurement framework
by utilizing the diffusion process characteristics. To this end, we propose a
link-tracing based sampling design which uses the infection times as local
information without any knowledge about the latent structure of diffusion
network. To correct the bias of sampled data, we introduce three estimators for
different categories; link-based, node-based, and cascade-based. To the best of
our knowledge, this is the first attempt to introduce a complete measurement
framework for diffusion networks. We also show that the estimator plays an
important role in correcting the bias of sampling from diffusion networks. Our
comprehensive empirical analysis over large synthetic and real datasets
demonstrates that in average, the proposed framework outperforms the common BFS
and RW sampling methods in terms of link-based characteristics by about 37% and
35%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7705</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7705</id><created>2014-01-16</created><authors><author><keyname>Sturm</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Stachniss</keyname><forenames>Cyrill</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author></authors><title>A Probabilistic Framework for Learning Kinematic Models of Articulated
  Objects</title><categories>cs.RO</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 41, pages
  477-526, 2011</journal-ref><doi>10.1613/jair.3229</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots operating in domestic environments generally need to interact with
articulated objects, such as doors, cabinets, dishwashers or fridges. In this
work, we present a novel, probabilistic framework for modeling articulated
objects as kinematic graphs. Vertices in this graph correspond to object parts,
while edges between them model their kinematic relationship. In particular, we
present a set of parametric and non-parametric edge models and how they can
robustly be estimated from noisy pose observations. We furthermore describe how
to estimate the kinematic structure and how to use the learned kinematic models
for pose prediction and for robotic manipulation tasks. We finally present how
the learned models can be generalized to new and previously unseen objects. In
various experiments using real robots with different camera systems as well as
in simulation, we show that our approach is valid, accurate and efficient.
Further, we demonstrate that our approach has a broad set of applications, in
particular for the emerging fields of mobile manipulation and service robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7709</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7709</id><created>2014-05-29</created><updated>2014-10-08</updated><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author><author><keyname>Rosenbaum</keyname><forenames>Will</forenames></author></authors><title>A Stable Marriage Requires Communication</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gale-Shapley algorithm for the Stable Marriage Problem is known to take
$\Theta(n^2)$ steps to find a stable marriage in the worst case, but only
$\Theta(n \log n)$ steps in the average case (with $n$ women and $n$ men). In
1976, Knuth asked whether the worst-case running time can be improved in a
model of computation that does not require sequential access to the whole
input. A partial negative answer was given by Ng and Hirschberg, who showed
that $\Theta(n^2)$ queries are required in a model that allows certain natural
random-access queries to the participants' preferences. A significantly more
general - albeit slightly weaker - lower bound follows from Segal's elaborate
analysis of communication complexity, namely that $\Omega(n^2)$ Boolean queries
are required in order to find a stable marriage, regardless of the set of
allowed Boolean queries.
  Using a reduction to the communication complexity of the disjointness
problem, we give a far simpler, yet significantly more powerful argument
showing that $\Omega(n^2)$ Boolean queries of any type are indeed required.
Notably, unlike Segal's lower bound, our lower bound generalizes also to (A)
randomized algorithms, (B) finding approximately-stable marriages (C) verifying
the stability (or the approximate stability) of a proposed marriage, (D)
allowing arbitrary separate preprocessing of the women's preferences profile
and of the men's preferences profile, and (E) several variants of the basic
problem, such as whether a given pair is married in every/some stable marriage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7711</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7711</id><created>2014-01-15</created><authors><author><keyname>Chen</keyname><forenames>David L.</forenames></author><author><keyname>Kim</keyname><forenames>Joohyun</forenames></author><author><keyname>Mooney</keyname><forenames>Raymond J.</forenames></author></authors><title>Training a Multilingual Sportscaster: Using Perceptual Context to Learn
  Language</title><categories>cs.CL</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 37, pages
  397-435, 2010</journal-ref><doi>10.1613/jair.2962</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for learning to interpret and generate language
using only perceptual context as supervision. We demonstrate its capabilities
by developing a system that learns to sportscast simulated robot soccer games
in both English and Korean without any language-specific prior knowledge.
Training employs only ambiguous supervision consisting of a stream of
descriptive textual comments and a sequence of events extracted from the
simulation trace. The system simultaneously establishes correspondences between
individual comments and the events that they describe while building a
translation model that supports both parsing and generation. We also present a
novel algorithm for learning which events are worth describing. Human
evaluations of the generated commentaries indicate they are of reasonable
quality and in some cases even on par with those produced by humans for our
limited domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7713</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7713</id><created>2014-01-15</created><authors><author><keyname>Katrenko</keyname><forenames>Sophia</forenames></author><author><keyname>Adriaans</keyname><forenames>Pieter</forenames></author><author><keyname>van Someren</keyname><forenames>Maarten</forenames></author></authors><title>Using Local Alignments for Relation Recognition</title><categories>cs.CL cs.IR cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 38, pages
  1-48, 2010</journal-ref><doi>10.1613/jair.2964</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the problem of marrying structural similarity with
semantic relatedness for Information Extraction from text. Aiming at accurate
recognition of relations, we introduce local alignment kernels and explore
various possibilities of using them for this task. We give a definition of a
local alignment (LA) kernel based on the Smith-Waterman score as a sequence
similarity measure and proceed with a range of possibilities for computing
similarity between elements of sequences. We show how distributional similarity
measures obtained from unlabeled data can be incorporated into the learning
task as semantic knowledge. Our experiments suggest that the LA kernel yields
promising results on various biomedical corpora outperforming two baselines by
a large margin. Additional series of experiments have been conducted on the
data sets of seven general relation types, where the performance of the LA
kernel is comparable to the current state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7714</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7714</id><created>2014-05-28</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>The Computational Impact of Partial Votes on Strategic Voting</title><categories>cs.GT cs.AI</categories><comments>To appear in Proceedings of ECAI 2014</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real world elections, agents are not required to rank all candidates.
We study three of the most common methods used to modify voting rules to deal
with such partial votes. These methods modify scoring rules (like the Borda
count), elimination style rules (like single transferable vote) and rules based
on the tournament graph (like Copeland) respectively. We argue that with an
elimination style voting rule like single transferable vote, partial voting
does not change the situations where strategic voting is possible. However,
with scoring rules and rules based on the tournament graph, partial voting can
increase the situations where strategic voting is possible. As a consequence,
the computational complexity of computing a strategic vote can change. For
example, with Borda count, the complexity of computing a strategic vote can
decrease or stay the same depending on how we score partial votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7716</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7716</id><created>2014-05-29</created><updated>2014-06-03</updated><authors><author><keyname>Eryilmaz</keyname><forenames>S. Burc</forenames></author><author><keyname>Kuzum</keyname><forenames>Duygu</forenames></author><author><keyname>Jeyasingh</keyname><forenames>Rakesh G. D.</forenames></author><author><keyname>Kim</keyname><forenames>SangBum</forenames></author><author><keyname>BrightSky</keyname><forenames>Matthew</forenames></author><author><keyname>Lam</keyname><forenames>Chung</forenames></author><author><keyname>Wong</keyname><forenames>H. -S. Philip</forenames></author></authors><title>Experimental Demonstration of Array-level Learning with Phase Change
  Synaptic Devices</title><categories>cs.NE cs.AI</categories><comments>IEDM 2013</comments><journal-ref>Electron Devices Meeting (IEDM), 2013 IEEE International,
  pp.25.5.1,25.5.4, 9-11 Dec. 2013</journal-ref><doi>10.1109/IEDM.2013.6724691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational performance of the biological brain has long attracted
significant interest and has led to inspirations in operating principles,
algorithms, and architectures for computing and signal processing. In this
work, we focus on hardware implementation of brain-like learning in a
brain-inspired architecture. We demonstrate, in hardware, that 2-D crossbar
arrays of phase change synaptic devices can achieve associative learning and
perform pattern recognition. Device and array-level studies using an
experimental 10x10 array of phase change synaptic devices have shown that
pattern recognition is robust against synaptic resistance variations and large
variations can be tolerated by increasing the number of training iterations.
Our measurements show that increase in initial variation from 9 % to 60 %
causes required training iterations to increase from 1 to 11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7718</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7718</id><created>2014-05-29</created><updated>2014-09-02</updated><authors><author><keyname>Lingala</keyname><forenames>Sajan Goud</forenames></author><author><keyname>DiBella</keyname><forenames>Edward</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Deformation corrected compressed sensing (DC-CS): a novel framework for
  accelerated dynamic MRI</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deformation corrected compressed sensing (DC-CS) framework
to recover dynamic magnetic resonance images from undersampled measurements. We
introduce a generalized formulation that is capable of handling a wide class of
sparsity/compactness priors on the deformation corrected dynamic signal. In
this work, we consider example compactness priors such as sparsity in temporal
Fourier domain, sparsity in temporal finite difference domain, and nuclear norm
penalty to exploit low rank structure. Using variable splitting, we decouple
the complex optimization problem to simpler and well understood sub problems;
the resulting algorithm alternates between simple steps of shrinkage based
denoising, deformable registration, and a quadratic optimization step.
Additionally, we employ efficient continuation strategies to minimize the risk
of convergence to local minima. The proposed formulation contrasts with
existing DC-CS schemes that are customized for free breathing cardiac cine
applications, and other schemes that rely on fully sampled reference frames or
navigator signals to estimate the deformation parameters. The efficient
decoupling enabled by the proposed scheme allows its application to a wide
range of applications including contrast enhanced dynamic MRI. Through
experiments on numerical phantom and in vivo myocardial perfusion MRI datasets,
we demonstrate the utility of the proposed DC-CS scheme in providing robust
reconstructions with reduced motion artifacts over classical compressed sensing
schemes that utilize the compact priors on the original deformation
un-corrected signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7720</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7720</id><created>2014-05-29</created><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Li</keyname><forenames>Zhouyuan</forenames></author><author><keyname>Cetiner</keyname><forenames>Bedri A.</forenames></author></authors><title>Full-Duplex Systems Using Multi-Reconfigurable Antennas</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex systems are expected to achieve 100% rate improvement over
half-duplex systems if the self-interference signal can be significantly
mitigated. In this paper, we propose the first full-duplex system utilizing
Multi-Reconfigurable Antenna (MRA) with ?90% rate improvement compared to
half-duplex systems. MRA is a dynamically reconfigurable antenna structure,
that is capable of changing its properties according to certain input
configurations. A comprehensive experimental analysis is conducted to
characterize the system performance in typical indoor environments. The
experiments are performed using a fabricated MRA that has 4096 configurable
radiation patterns. The achieved MRA-based passive self-interference
suppression is investigated, with detailed analysis for the MRA training
overhead. In addition, a heuristic-based approach is proposed to reduce the MRA
training overhead. The results show that at 1% training overhead, a total of
95dB self-interference cancellation is achieved in typical indoor environments.
The 95dB self-interference cancellation is experimentally shown to be
sufficient for 90% full-duplex rate improvement compared to half-duplex
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7724</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7724</id><created>2014-05-29</created><updated>2015-03-04</updated><authors><author><keyname>Ronqui</keyname><forenames>Jos&#xe9; Ricardo Furlan</forenames></author><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author></authors><title>Analyzing complex networks through correlations in centrality
  measurements</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 1 table, 6 figures</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment 2015, 5
  (2015)</journal-ref><doi>10.1088/1742-5468/2015/05/P05030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world systems can be expressed as complex networks of
interconnected nodes. It is frequently important to be able to quantify the
relative importance of the various nodes in the network, a task accomplished by
defining some centrality measures, with different centrality definitions
stressing different aspects of the network. It is interesting to know to what
extent these different centrality definitions are related for different
networks. In this work, we study the correlation between pairs of a set of
centrality measures for different real world networks and two network models.
We show that the centralities are in general correlated, but with stronger
correlations for network models than for real networks. We also show that the
strength of the correlation of each pair of centralities varies from network to
network. Taking this fact into account, we propose the use of a centrality
correlation profile, consisting of the values of the correlation coefficients
between all pairs of centralities of interest, as a way to characterize
networks. Using the yeast protein interaction network as an example we show
also that the centrality correlation profile can be used to assess the adequacy
of a network model as a representation of a given real network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7739</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7739</id><created>2014-05-29</created><authors><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames></author></authors><title>(Quantified) Horn Constraint Solving for Program Verification and
  Synthesis</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how automatic tools for the verification of linear and branching time
properties of procedural, multi-threaded, and functional programs as well as
program synthesis can be naturally and uniformly seen as solvers of constraints
in form of (quantified) Horn clauses over background logical theories. Such a
perspective can offer various advantages, e. g., a logical separation of
concerns between constraint generation (also known as generation of proof
obligations) and constraint solving (also known as proof discovery), reuse of
solvers across different verifications tasks, and liberation of proof designers
from low level algorithmic concerns and vice versa.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7751</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7751</id><created>2014-05-29</created><authors><author><keyname>Lee</keyname><forenames>Hooyeon</forenames></author><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author></authors><title>Stable Invitations</title><categories>cs.GT</categories><comments>To appear in COMSOC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the situation in which an organizer is trying to convene an
event, and needs to choose a subset of agents to be invited. Agents have
preferences over how many attendees should be at the event and possibly also
who the attendees should be. This induces a stability requirement: All invited
agents should prefer attending to not attending, and all the other agents
should not regret being not invited. The organizer's objective is to find the
invitation of maximum size subject to the stability requirement. We investigate
the computational complexity of finding the maximum stable invitation when all
agents are truthful, as well as the mechanism design problem when agents may
strategically misreport their preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7752</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7752</id><created>2014-05-29</created><updated>2014-11-21</updated><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author><author><keyname>Valko</keyname><forenames>Michal</forenames></author></authors><title>Learning to Act Greedily: Polymatroid Semi-Bandits</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many important optimization problems, such as the minimum spanning tree and
minimum-cost flow, can be solved optimally by a greedy method. In this work, we
study a learning variant of these problems, where the model of the problem is
unknown and has to be learned by interacting repeatedly with the environment in
the bandit setting. We formalize our learning problem quite generally, as
learning how to maximize an unknown modular function on a known polymatroid. We
propose a computationally efficient algorithm for solving our problem and bound
its expected cumulative regret. Our gap-dependent upper bound is tight up to a
constant and our gap-free upper bound is tight up to polylogarithmic factors.
Finally, we evaluate our method on three problems and demonstrate that it is
practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7764</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7764</id><created>2014-05-29</created><updated>2014-10-07</updated><authors><author><keyname>Tulabandhula</keyname><forenames>Theja</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Generalization Bounds for Learning with Linear, Polygonal, Quadratic and
  Conic Side Knowledge</title><categories>stat.ML cs.LG</categories><comments>37 pages, 3 figures, a shorter version appeared in ISAIM 2014 (new
  additions include a reference change and a new figure)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a supervised learning setting where side knowledge
is provided about the labels of unlabeled examples. The side knowledge has the
effect of reducing the hypothesis space, leading to tighter generalization
bounds, and thus possibly better generalization. We consider several types of
side knowledge, the first leading to linear and polygonal constraints on the
hypothesis space, the second leading to quadratic constraints, and the last
leading to conic constraints. We show how different types of domain knowledge
can lead directly to these kinds of side knowledge. We prove bounds on
complexity measures of the hypothesis space for quadratic and conic side
knowledge, and show that these bounds are tight in a specific sense for the
quadratic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7769</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7769</id><created>2014-05-29</created><authors><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Yuan</keyname><forenames>Nicholas Jing</forenames></author><author><keyname>Xie</keyname><forenames>Xing</forenames></author><author><keyname>Lian</keyname><forenames>Defu</forenames></author><author><keyname>Rui</keyname><forenames>Yong</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Indigenization of Urban Mobility</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>17 pages, 4 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncovering urban mobility patterns is crucial for further predicting and
controlling spatially embedded events. In this article, we analyze millions of
geographical check-ins crawled from a Chinese leading location-based social
networking service, Jiepang.com, which contains demographical information and
thus allows the group-specific studies. We found distinguishable mobility
patterns of natives and non-natives in all five large cities under
consideration, and by assigning different algorithms onto natives and
non-natives, the accuracy of location prediction can be largely improved
compared with pure algorithms. We further propose the so-called indigenization
coefficients to quantify to which extent an individual behaves like a native,
which depend only on check-in behaviors, instead of any demographical
information. To our surprise, a hybrid algorithm weighted by the indigenization
coefficients outperforms the mixed algorithm accounting for additional
demographical information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7771</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7771</id><created>2014-05-30</created><authors><author><keyname>Dawn</keyname><forenames>Suma</forenames></author><author><keyname>Saxena</keyname><forenames>Vikas</forenames></author><author><keyname>Sharma</keyname><forenames>Bhu Dev</forenames></author></authors><title>DEM Registration and Error Analysis using ASCII values</title><categories>cs.CV</categories><comments>10 pages, 4 figures, 1 table, Proceeding of International Conference
  on Signal Processing and Imaging Engineering 2010, San Francisco, USA, 20-22
  October 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Elevation Model (DEM), while providing a bare earth look, is heavily
used in many applications including construction modeling, visualization, and
GIS. Their registration techniques have not been explored much. Methods like
Coarse-to-fine or pyramid making are common in DEM-to-image or DEM-to-map
registration. Self-consistency measure is used to detect any change in terrain
elevation and hence was used for DEM-to-DEM registration. But these methods
apart from being time and complexity intensive, lack in error matrix
evaluation. This paper gives a method of registration of DEMs using specified
height values as control points by initially converting these DEMs to ASCII
files. These control points may be found by two mannerisms - either by direct
detection of appropriate height data in ASCII files or by edge matching along
congruous quadrangle of the control point, followed by sub-graph matching.
Error analysis for the same has also been done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7777</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7777</id><created>2014-05-30</created><authors><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author></authors><title>Online and Adaptive Pseudoinverse Solutions for ELM Weights</title><categories>cs.NE</categories><comments>Accepted for publication in Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ELM method has become widely used for classification and regressions
problems as a result of its accuracy, simplicity and ease of use. The solution
of the hidden layer weights by means of a matrix pseudoinverse operation is a
significant contributor to the utility of the method; however, the conventional
calculation of the pseudoinverse by means of a singular value decomposition
(SVD) is not always practical for large data sets or for online updates to the
solution. In this paper we discuss incremental methods for solving the
pseudoinverse which are suitable for ELM. We show that careful choice of
methods allows us to optimize for accuracy, ease of computation, or
adaptability of the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7780</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7780</id><created>2014-05-30</created><authors><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>ELM Solutions for Event-Based Systems</title><categories>cs.NE</categories><comments>Accepted for publication in Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whilst most engineered systems use signals that are continuous in time, there
is a domain of systems in which signals consist of events. Events, like Dirac
delta functions, have no meaningful time duration. Many important real-world
systems are intrinsically event-based, including the mammalian brain, in which
the primary packets of data are spike events, or action potentials. In this
domain, signal processing requires responses to spatio-temporal patterns of
events. We show that some straightforward modifications to the standard ELM
topology produce networks that are able to perform spatio-temporal event
processing online with a high degree of accuracy. The modifications involve the
re-definition of hidden layer units as synaptic kernels, in which the input
delta functions are transformed into continuous-valued signals using a variety
of impulse-response functions. This permits the use of linear solution methods
in the output layer, which can produce events as output, if modeled as a
classifier; the output classes are 'event' or 'no event'. We illustrate the
method in application to a spike-processing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7786</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7786</id><created>2014-05-30</created><updated>2016-02-24</updated><authors><author><keyname>Lee</keyname><forenames>Namgil</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Fundamental Tensor Operations for Large-Scale Data Analysis in Tensor
  Train Formats</title><categories>math.NA cs.ET</categories><comments>36 pages; Several improvements and corrected references</comments><msc-class>15A63, 15A69, 65F25, 65F30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss extended definitions of linear and multilinear operations such as
Kronecker, Hadamard, and contracted products, and establish links between them
for tensor calculus. Then we introduce effective low-rank tensor approximation
techniques including Candecomp/Parafac (CP), Tucker, and tensor train (TT)
decompositions with a number of mathematical and graphical representations. We
also provide a brief review of mathematical properties of the TT decomposition
as a low-rank approximation technique. With the aim of breaking the
curse-of-dimensionality in large-scale numerical analysis, we describe basic
operations on large-scale vectors, matrices, and high-order tensors represented
by TT decomposition. The proposed representations can be used for describing
numerical methods based on TT decomposition for solving large-scale
optimization problems such as systems of linear equations and symmetric
eigenvalue problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7794</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7794</id><created>2014-05-30</created><updated>2014-06-01</updated><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>OPTICS Based Coverage in Wireless Sensor Network</title><categories>cs.NI</categories><comments>International Conference on Facets of Uncertainties and Applications
  (ICFUA '13), Kolkata, India, 05-07 Dec., 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the coverage problem of wireless sensor network. We use
the density based clustering technique - OPTICS to cover a target region with
less number of sensor nodes. OPTICS works well to identify the outliers, core
points and it obtains the denser regions. We define a level of acceptance to
find next appropriate sensor in the region. We eliminate overlapped area and
obtain a decision tree to minimally cover up the target region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7796</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7796</id><created>2014-05-30</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu</forenames></author></authors><title>Improving Computer Assisted Speech Therapy Through Speech Based Emotion
  Recognition</title><categories>cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech therapy consists in a wide range of services whose aim is to prevent,
diagnose and treat different types of speech impairments. One of the most
important conditions for obtaining favourable and steady results is the
&quot;immersing&quot; of the subject as long as possible into therapeutic context: at
home, at school/work, on the street. Since nowadays portable computers tend to
become habitual accessories, it seems a good idea to create virtual versions of
human SLTs and to integrate them into these devices. However one of the main
distinctions between a Speech and Language Therapist (SLT) and a Computer Based
Speech Therapy System (CBST) arise from the field of emotion intelligence. The
inability of current CBSTs to detect emotional state of human subjects leads to
inadequate behavioural responses. Furthermore, this &quot;unresponsive&quot; behaviour is
perceived as a lack of empathy and, especially when subjects are children,
leads to negative emotional state such as frustration. Thus in this article we
propose an original emotions recognition framework - named PhonEM - to be
integrated in our previous developed CBST - Logomon. The originality consists
in both emotions representation (a fuzzy model) and detection (using only
subjects' speech stream). These exceptional restrictions along with the fuzzy
representation of emotions lie at the origin of our approach and make our task
a difficult and, in the same time, an innovative one. As far as we know, this
is the first attempt to combine these techniques in order to improve assisted
speech therapy and the obtained results encourage as to further develop our
CBST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7806</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7806</id><created>2014-05-30</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu Andrei</forenames></author><author><keyname>Belciug</keyname><forenames>Felicia Giza</forenames></author><author><keyname>Pentiuc</keyname><forenames>Stefan-Gheorghe</forenames></author><author><keyname>Belciug</keyname><forenames>Cristian Eduard</forenames></author><author><keyname>Nestor</keyname><forenames>Marian</forenames></author></authors><title>Software Package with Exercises for Therapy of Children with Dyslalia</title><categories>cs.CY</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a consistent set of exercises for children with
dyslalia (dyslalia is a speech disorder that affect pronunciation of one ore
many sounds). The achievement has gone from &quot;Therapeutic Guide&quot; made available
by the team of researchers led by Professor Mrs. Iolanda TOBOLCEA from the
&quot;Alexandru Ioan Cuza&quot; University of Iasi. The specifications of the
&quot;Therapeutic Guide&quot; have been fully complied with, such exercises being adapted
both age and level of children. To achieve these exercises were recorded,
processed and used more than 10000 voice production. They also have been made
over the 2000 corresponding recorded nouns. These exercises are being tested by
Interschool Regional Logopaedic Center of Suceava - Romania.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7811</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7811</id><created>2014-05-30</created><authors><author><keyname>Burgos</keyname><forenames>Enrique</forenames></author><author><keyname>Hernandez</keyname><forenames>Laura</forenames></author><author><keyname>Ceva</keyname><forenames>Horacio</forenames></author><author><keyname>Perazzo</keyname><forenames>Roberto P. J.</forenames></author></authors><title>Entropic determination of the phase transition in a coevolving
  opinion-formation model</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 6 figures</comments><msc-class>46N55</msc-class><doi>10.1103/PhysRevE.91.032808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an opinion formation model by the means of a co-evolving complex
network where the vertices represent the individuals, characterised by their
evolving opinions, and the edges represent the interactions among them. The
network adapts to the spreading of opinions in two ways: not only connected
agents interact and eventually change their thinking but an agent may also
rewire one of its links to a neighborhood holding the same opinion as his. The
dynamics depends on an external parameter {\Phi}, which controls the plasticity
of the network. We show how the information entropy associated to the
distribution of group sizes, allows to locate the phase transition between full
consensus and a society where different opinions coexist. We also determine the
minimum size of the most informative sampling. At the transition the
distribution of the sizes of groups holding the same opinion is scale free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7812</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7812</id><created>2014-05-30</created><updated>2015-01-29</updated><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Duality of a Source Coding Problem and the Semi-Deterministic Broadcast
  Channel with Rate-Limited Cooperation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wyner-Ahlswede-K\&quot;orner (WAK) empirical-coordination problem where the
encoders cooperate via a finite-capacity one-sided link is considered. The
coordination-capacity region is derived by combining several source coding
techniques, such as Wyner-Ziv (WZ) coding, binning and superposition coding.
Furthermore, a semi-deterministic (SD) broadcast channel (BC) with one-sided
decoder cooperation is considered. Duality principles relating the two problems
are presented, and the capacity region for the SD-BC setting is derived. The
direct part follows from an achievable region for a general BC that is tight
for the SD scenario. A converse is established by using telescoping identities.
The SD-BC is shown to be operationally equivalent to a class of relay-BCs
(RBCs) and the correspondence between their capacity regions is established.
The capacity region of the SD-BC is transformed into an equivalent region that
is shown to be dual to the admissible region of the WAK problem in the sense
that the information measures defining the corner points of both regions
coincide. Achievability and converse proofs for the equivalent region are
provided. For the converse, we use a probabilistic construction of auxiliary
random variables that depends on the distribution induced by the codebook.
Several examples illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7828</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7828</id><created>2014-05-30</created><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Rolinek</keyname><forenames>Michal</forenames></author></authors><title>Superconcentrators of Density 25.3</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $N$-superconcentrator is a directed, acyclic graph with $N$ input nodes
and $N$ output nodes such that every subset of the inputs and every subset of
the outputs of same cardinality can be connected by node-disjoint paths. It is
known that linear-size and bounded-degree superconcentrators exist. We prove
the existence of such superconcentrators with asymptotic density $25.3$ (where
the density is the number of edges divided by $N$). The previously best known
densities were $28$ \cite{Scho2006} and $27.4136$ \cite{YuanK12}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7831</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7831</id><created>2014-05-30</created><authors><author><keyname>Tormo</keyname><forenames>Gin&#xe9;s D&#xf3;lera</forenames></author><author><keyname>M&#xe1;rmol</keyname><forenames>F&#xe9;lix G&#xf3;mez</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Gregorio Mart&#xed;nez</forenames></author></authors><title>ROMEO: ReputatiOn Model Enhancing OpenID Simulator</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenID is a standard decentralized initiative aimed at allowing Internet
users to use the same personal account to access different services. Since it
does not rely on any central authority, it is hard for such users or other
entities to validate the trust level of each entity deployed in the system.
Some research has been conducted to handle this issue, defining a reputation
framework to determine the trust level of a relying party based on past
experiences. However, this framework has been proposed in a theoretical way and
some deeper analysis and validation is still missing. Our main contribution in
this paper consist of a simulation environment able to validate the feasibility
of the reputation framework and analyze its behaviour within different
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7832</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7832</id><created>2014-05-30</created><updated>2015-04-09</updated><authors><author><keyname>Bohlin</keyname><forenames>Ludvig</forenames></author><author><keyname>Esquivel</keyname><forenames>Alcides Viamontes</forenames></author><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Robustness of journal rankings by network flows with different amounts
  of memory</title><categories>physics.soc-ph cs.DL</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of scientific journals has multiplied, journal rankings have
become increasingly important for scientific decisions. From submissions and
subscriptions to grants and hirings, researchers, policy makers, and funding
agencies make important decisions with influence from journal rankings such as
the ISI journal impact factor. Typically, the rankings are derived from the
citation network between a selection of journals and unavoidably depend on this
selection. However, little is known about how robust rankings are to the
selection of included journals. Here we compare the robustness of three journal
rankings based on network flows induced on citation networks. They model
pathways of researchers navigating scholarly literature, stepping between
journals and remembering their previous steps to different degree: zero-step
memory as impact factor, one-step memory as Eigenfactor, and two-step memory,
corresponding to zero-, first-, and second-order Markov models of citation flow
between journals. We conclude that higher-order Markov models perform better
and are more robust to the selection of journals. Whereas our analysis
indicates that higher-order models perform better, the performance gain for the
second-order Markov model comes at the cost of requiring more citation data
over a longer time period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7840</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7840</id><created>2014-05-30</created><authors><author><keyname>Funde</keyname><forenames>Nitesh</forenames></author><author><keyname>Pardhi</keyname><forenames>P R</forenames></author></authors><title>Analysis Of Possible Attack On AODV Protocol In MANET</title><categories>cs.NI cs.CR</categories><comments>4 pages,5 figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V11(6),306-309 May 2014</journal-ref><doi>10.14445/22315381/IJETT-V11P258</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mobile Ad Hoc Networks (MANETs) consist of wireless mobile nodes which
coordinate with each other to form temporary network without its pre-existing
infrastructure. AODV is popular Ad-hoc distance vector routing reactive
protocol which is used to find correct &amp; shortest route to destination. Due to
openness, dynamic, infrastructure-less nature, MANET are vulnerable to various
attacks. One of these possible attacks is a Black Hole Attack in which a mobile
node falsely replies to the source node that it is having a shortest path to
the destination without checking its routing table. Therefore source node send
all of its data to the black hole node and it deprives all the traffic of the
source node. In this paper, We are proposing a technique to detect and prevent
the multiple black hole nodes from MANET so that source to destination
communication can be made easily. We also analysed the performance of the
network in terms of number of packets sent, received, throughput, energy of
network before attack and after detection &amp; prevention of Attack. From these
analysis, we can conclude that performance decreased due to attack can be
improved after detection &amp; prevention black hole attack in MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7849</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7849</id><created>2014-05-30</created><authors><author><keyname>Ablayev</keyname><forenames>Farid</forenames></author><author><keyname>Gainutdinova</keyname><forenames>Aida</forenames></author><author><keyname>Khadiev</keyname><forenames>Kamil</forenames></author><author><keyname>Yakarylmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Very narrow quantum OBDDs and width hierarchies for classical OBDDs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several results on comparative complexity for di?erent variants of
OBDD models.
  - We present some results on comparative complexity of classical and quantum
OBDDs. We consider a partial function depending on pa- rameter k such that for
any k &gt; 0 this function is computed by an exact quantum OBDD of width 2 but any
classical OBDD (deter- ministic or stable bounded error probabilistic) needs
width 2k+1.
  - We consider quantum and classical nondeterminism. We show that quantum
nondeterminism can be more e?cient than classical one. In particular, an
explicit function is presented which is computed by a quantum nondeterministic
OBDD with constant width but any clas- sical nondeterministic OBDD for this
function needs non-constant width.
  - We also present new hierarchies on widths of deterministic and non-
deterministic OBDDs. We focus both on small and large widths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7851</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7851</id><created>2014-05-30</created><authors><author><keyname>Peppas</keyname><forenames>Kostas P.</forenames></author><author><keyname>Zamkotsian</keyname><forenames>Martin</forenames></author><author><keyname>Lazarakis</keyname><forenames>Fotis</forenames></author><author><keyname>Cottis</keyname><forenames>Panayotis G.</forenames></author></authors><title>Asymptotic Error Performance Analysis of Spatial Modulation under
  Generalized Fading</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a comprehensive framework analyzing the asymptotic error
performance of a multiple-input-multiple-output (MIMO) wireless system
employing spatial modulation (SM) with maximum likelihood detection and perfect
channel state information. Generic analytical expressions for the diversity and
coding gains are deduced that reveal fundamental properties of MIMO SM systems.
The presented analysis can be used to obtain closed-form upper bounds for the
average bit error probability (ABEP) of MIMO SM systems under generalized
fading which become asymptotically tight in the high signal-to-noise ratio
(SNR) region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7857</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7857</id><created>2014-05-30</created><authors><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Gu&#xe9;ret</keyname><forenames>Christophe</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Hansen</keyname><forenames>Preben</forenames></author><author><keyname>Slavic</keyname><forenames>Aida</forenames></author></authors><title>Knowledge Maps and Information Retrieval (KMIR)</title><categories>cs.IR cs.DL cs.HC</categories><comments>6 pages, accepted workshop proposal for Digital Libraries 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information systems usually show as a particular point of failure the
vagueness between user search terms and the knowledge orders of the information
space in question. Some kind of guided searching therefore becomes more and
more important in order to precisely discover information without knowing the
right search terms. Knowledge maps of digital library collections are promising
navigation tools through knowledge spaces but still far away from being
applicable for searching digital libraries. However, there is no continuous
knowledge exchange between the &quot;map makers&quot; on the one hand and the Information
Retrieval (IR) specialists on the other hand. Thus, there is also a lack of
models that properly combine insights of the two strands. The proposed workshop
aims at bringing together these two communities: experts in IR reflecting on
visual enhanced search interfaces and experts in knowledge mapping reflecting
on visualizations of the content of a collection that might also present a
context for a search term in a visual manner. The intention of the workshop is
to raise awareness of the potential of interactive knowledge maps for
information seeking purposes and to create a common ground for experiments
aiming at the incorporation of knowledge maps into IR models at the level of
the user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7859</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7859</id><created>2014-05-30</created><authors><author><keyname>Cao</keyname><forenames>Yixin</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Chordal Editing is Fixed-Parameter Tractable</title><categories>cs.DS</categories><msc-class>05C75, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph modification problems are typically asked as follows: is there a small
set of operations that transforms a given graph to have a certain property. The
most commonly considered operations include vertex deletion, edge deletion, and
edge addition; for the same property, one can define significantly different
versions by allowing different operations. We study a very general graph
modification problem which allows all three types of operations: given a graph
$G$ and integers $k_1$, $k_2$, and $k_3$, the \textsc{chordal editing} problem
asks whether $G$ can be transformed into a chordal graph by at most $k_1$
vertex deletions, $k_2$ edge deletions, and $k_3$ edge additions. Clearly, this
problem generalizes both \textsc{chordal vertex/edge deletion} and
\textsc{chordal completion} (also known as \textsc{minimum fill-in}). Our main
result is an algorithm for \textsc{chordal editing} in time $2^{O(k\log
k)}\cdot n^{O(1)}$, where $k:=k_1+k_2+k_3$ and $n$ is the number of vertices of
$G$. Therefore, the problem is fixed-parameter tractable parameterized by the
total number of allowed operations. Our algorithm is both more efficient and
conceptually simpler than the previously known algorithm for the special case
\textsc{chordal deletion}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7866</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7866</id><created>2014-05-30</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Giza</keyname><forenames>Felicia-Florentina</forenames></author></authors><title>Vocal signal digital processing. Instrument for analog to digital
  conversion study</title><categories>cs.SD</categories><comments>7 pages, 4 figures, in Romanian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this article is to present interactive didactic software for
analog to digital conversion using PCM method. After a short introduction
regarding vocal signal processing we present some method for analog to digital
conversion. The didactic software is an applet that can be direct accessed by
any interested person.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7868</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7868</id><created>2014-05-08</created><authors><author><keyname>Bajaj</keyname><forenames>Priya</forenames></author><author><keyname>Raheja</keyname><forenames>Supriya</forenames></author></authors><title>A Vague Improved Markov Model Approach for Web Page Prediction</title><categories>cs.IR cs.AI</categories><comments>8 pages, 4 figures, 1 table, International Journal of Computer
  Science &amp; Engineering Survey (IJCSES) Vol.5, No.2, April 2014</comments><doi>10.5121/ijcses.2014.5205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today most of the information in all areas is available over the web. It
increases the web utilization as well as attracts the interest of researchers
to improve the effectiveness of web access and web utilization. As the number
of web clients gets increased, the bandwidth sharing is performed that
decreases the web access efficiency. Web page prefetching improves the
effectiveness of web access by availing the next required web page before the
user demand. It is an intelligent predictive mining that analyze the user web
access history and predict the next page. In this work, vague improved markov
model is presented to perform the prediction. In this work, vague rules are
suggested to perform the pruning at different levels of markov model. Once the
prediction table is generated, the association mining will be implemented to
identify the most effective next page. In this paper, an integrated model is
suggested to improve the prediction accuracy and effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7869</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7869</id><created>2014-05-08</created><authors><author><keyname>Bajaj</keyname><forenames>Priya</forenames></author><author><keyname>Raheja</keyname><forenames>Supriya</forenames></author></authors><title>Integrating Vague Association Mining with Markov Model</title><categories>cs.IR cs.AI</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand of world wide web raises the need of predicting the
user's web page request.The most widely used approach to predict the web pages
is the pattern discovery process of Web usage mining. This process involves
inevitability of many techniques like Markov model, association rules and
clustering. Fuzzy theory with different techniques has been introduced for the
better results. Our focus is on Markov models. This paper is introducing the
vague Rules with Markov models for more accuracy using the vague set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7874</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7874</id><created>2014-05-30</created><updated>2014-09-10</updated><authors><author><keyname>Dobson</keyname><forenames>Edward</forenames></author><author><keyname>Hujdurovi&#x107;</keyname><forenames>Ademir</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Verret</keyname><forenames>Gabriel</forenames></author></authors><title>Vertex-transitive CIS graphs</title><categories>math.CO cs.DM</categories><msc-class>05C69, 05C25</msc-class><journal-ref>European Journal of Combinatorics 44 (2015) 87-98</journal-ref><doi>10.1016/j.ejc.2014.09.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A CIS graph is a graph in which every maximal stable set and every maximal
clique intersect. A graph is well-covered if all its maximal stable sets are of
the same size, co-well-covered if its complement is well-covered, and
vertex-transitive if, for every pair of vertices, there exists an automorphism
of the graph mapping one to the other. We show that a vertex-transitive graph
is CIS if and only if it is well-covered, co-well-covered, and the product of
its clique and stability numbers equals its order. A graph is irreducible if no
two distinct vertices have the same neighborhood. We classify irreducible
well-covered CIS graphs with clique number at most 3 and vertex-transitive CIS
graphs of valency at most 7, which include an infinite family. We also exhibit
an infinite family of vertex-transitive CIS graphs which are not Cayley.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7879</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7879</id><created>2014-05-30</created><updated>2014-06-04</updated><authors><author><keyname>Eftimiades</keyname><forenames>Alex</forenames></author></authors><title>Kahler: An Implementation of Discrete Exterior Calculus on Hermitian
  Manifolds</title><categories>cs.NA cs.CG cs.MS math.DG math.NA</categories><comments>Added a link to the code</comments><msc-class>65N30</msc-class><acm-class>G.4; G.1.8; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details the techniques and algorithms implemented in Kahler, a
Python library that implements discrete exterior calculus on arbitrary
Hermitian manifolds. Borrowing techniques and ideas first implemented in PyDEC,
Kahler provides a uniquely general framework for computation using discrete
exterior calculus. Manifolds can have arbitrary dimension, topology, bilinear
Hermitian metrics, and embedding dimension. Kahler comes equipped with tools
for generating triangular meshes in arbitrary dimensions with arbitrary
topology. Kahler can also generate discrete sharp operators and implement de
Rham maps. Computationally intensive tasks are automatically parallelized over
the number of cores detected. The program itself is written in Cython--a
superset of the Python language that is translated to C and compiled for extra
speed. Kahler is applied to several example problems: normal modes of a
vibrating membrane, electromagnetic resonance in a cavity, the quantum harmonic
oscillator, and the Dirac-Kahler equation. Convergence is demonstrated on
random meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7895</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7895</id><created>2014-05-08</created><authors><author><keyname>Kemiha</keyname><forenames>Mina</forenames></author></authors><title>Empirical mode decomposition and normalshrink tresholding for speech
  denoising</title><categories>cs.IT cs.SY math.IT</categories><comments>8 pages, 6 figures</comments><acm-class>C.3</acm-class><journal-ref>IJIT Journal 3( 2), 2014, pp 27-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a signal denoising scheme based on Empirical mode decomposition
(EMD) is presented. The denoising method is a fully data driven approach. Noisy
signal is decomposed adaptively into intrinsic oscillatory components called
Intrinsic mode functions (IMFs) using a decomposition algorithm called sifting
process. The basic principle of the method is to decompose a speech signal into
segments each frame is categorised as either signal-dominant or noise-dominant
then reconstruct the signal with IMFs signal dominant frame previously filtered
or thresholded. It is shown, on the basis of intensive simulations that EMD
improves the signal to noise ratio and address the problem of signal
degradation. The denoising method is applied to real signal with different
noise levels and the results compared to Winner and universal threshold of
DONOHO and JOHNSTONE [11] with soft and hard tresholding. The effect of level
noise value on the performances of the proposed denoising is analysed. The
study is limited to signals corrupted by additive white Gaussian random noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7897</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7897</id><created>2014-05-30</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh</forenames></author></authors><title>Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1</title><categories>cs.LG</categories><comments>Supplementary material for B. Jain. Flip-Flop Sublinear Models for
  Graphs, S+SSPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there is no class-dual for almost all sublinear models on
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7898</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7898</id><created>2014-05-14</created><updated>2014-06-02</updated><authors><author><keyname>Stulova</keyname><forenames>Nataliia</forenames></author><author><keyname>Morales</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Towards Assertion-based Debugging of Higher-Order (C)LP Programs</title><categories>cs.PL</categories><comments>2 pages, to be published as a technical communication in the on-line
  addendum of the special issue(s) of the TPLP journal for ICLP14. To appear in
  Theory and Practice of Logic Programming (TPLP). arXiv admin note:
  substantial text overlap with arXiv:1404.4246; corrected the header
  publication info and submission, revision and acceptance dates</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order constructs extend the expressiveness of first-order (Constraint)
Logic Programming ((C)LP) both syntactically and semantically. At the same time
assertions have been in use for some time in (C)LP systems helping programmers
detect errors and validate programs. However, these assertion-based extensions
to (C)LP have not been integrated well with higher order to date. Our work
contributes to filling this gap by extending the assertion-based approach to
error detection and program validation to the higher-order context, within
(C)LP. It is based on an extension of properties and assertions as used in
(C)LP in order to be able to fully describe arguments that are predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7903</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7903</id><created>2014-05-30</created><authors><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author><author><keyname>Schuhmacher</keyname><forenames>Dominic</forenames></author></authors><title>The Shortlist Method for Fast Computation of the Earth Mover's Distance
  and Finding Optimal Solutions to Transportation Problems</title><categories>cs.CV</categories><journal-ref>PLOS ONE 9(10): e110214, Oct. 2014</journal-ref><doi>10.1371/journal.pone.0110214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding solutions to the classical transportation problem is of great
importance, since this optimization problem arises in many engineering and
computer science applications. Especially the Earth Mover's Distance is used in
a plethora of applications ranging from content-based image retrieval, shape
matching, fingerprint recognition, object tracking and phishing web page
detection to computing color differences in linguistics and biology. Our
starting point is the well-known revised simplex algorithm, which iteratively
improves a feasible solution to optimality. The Shortlist Method that we
propose substantially reduces the number of candidates inspected for improving
the solution, while at the same time balancing the number of pivots required.
Tests on simulated benchmarks demonstrate a considerable reduction in
computation time for the new method as compared to the usual revised simplex
algorithm implemented with state-of-the-art initialization and pivot
strategies. As a consequence, the Shortlist Method facilitates the computation
of large scale transportation problems in viable time. In addition we describe
a novel method for finding an initial feasible solution which we coin Modified
Russell's Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7908</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7908</id><created>2014-05-30</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames></author></authors><title>Semantic Composition and Decomposition: From Recognition to Generation</title><categories>cs.CL cs.AI cs.LG</categories><comments>National Research Council Canada - Technical Report</comments><acm-class>H.3.1; I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic composition is the task of understanding the meaning of text by
composing the meanings of the individual words in the text. Semantic
decomposition is the task of understanding the meaning of an individual word by
decomposing it into various aspects (factors, constituents, components) that
are latent in the meaning of the word. We take a distributional approach to
semantics, in which a word is represented by a context vector. Much recent work
has considered the problem of recognizing compositions and decompositions, but
we tackle the more difficult generation problem. For simplicity, we focus on
noun-modifier bigrams and noun unigrams. A test for semantic composition is,
given context vectors for the noun and modifier in a noun-modifier bigram (&quot;red
salmon&quot;), generate a noun unigram that is synonymous with the given bigram
(&quot;sockeye&quot;). A test for semantic decomposition is, given a context vector for a
noun unigram (&quot;snifter&quot;), generate a noun-modifier bigram that is synonymous
with the given unigram (&quot;brandy glass&quot;). With a vocabulary of about 73,000
unigrams from WordNet, there are 73,000 candidate unigram compositions for a
bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a
unigram. We generate ranked lists of potential solutions in two passes. A fast
unsupervised learning algorithm generates an initial list of candidates and
then a slower supervised learning algorithm refines the list. We evaluate the
candidate solutions by comparing them to WordNet synonym sets. For
decomposition (unigram to bigram), the top 100 most highly ranked bigrams
include a WordNet synonym of the given unigram 50.7% of the time. For
composition (bigram to unigram), the top 100 most highly ranked unigrams
include a WordNet synonym of the given bigram 77.8% of the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7910</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7910</id><created>2014-05-30</created><updated>2014-07-16</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Optimal CUR Matrix Decompositions</title><categories>cs.DS cs.LG math.NA</categories><comments>small revision in lemma 4.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CUR decomposition of an $m \times n$ matrix $A$ finds an $m \times c$
matrix $C$ with a subset of $c &lt; n$ columns of $A,$ together with an $r \times
n$ matrix $R$ with a subset of $r &lt; m$ rows of $A,$ as well as a $c \times r$
low-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$
that is, $ || A - CUR ||_F^2 \le (1+\epsilon) || A - A_k||_F^2$, where
$||.||_F$ denotes the Frobenius norm and $A_k$ is the best $m \times n$ matrix
of rank $k$ constructed via the SVD. We present input-sparsity-time and
deterministic algorithms for constructing such a CUR decomposition where
$c=O(k/\epsilon)$ and $r=O(k/\epsilon)$ and rank$(U) = k$. Up to constant
factors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7921</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7921</id><created>2014-05-30</created><authors><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Panteley</keyname><forenames>Elena</forenames></author></authors><title>When is a Parameterized Controller Suitable for Adaptive Control?</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate when a parameterized controller, designed for a
plant depending on unknown parameters, admits a realization which is
independent of the parameters. It is argued that adaptation is unnecessary for
this class of parameterized controllers. We prove that standard model reference
controllers (state and output--feedback) for linear time invariant systems with
a filter at the plant input admit a parameter independent realization. Although
the addition of such a filter is of questionable interest, our result formally,
and unquestionably, establishes the deleterious effect of such a modification,
which has been widely publicized in the control literature under the name
L1-adaptive control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7923</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7923</id><created>2014-05-30</created><authors><author><keyname>Jancar</keyname><forenames>Petr</forenames></author></authors><title>Bisimulation Equivalence of First-Order Grammars</title><categories>cs.LO cs.FL</categories><comments>This paper extends the version accepted to ICALP'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decidability proof for bisimulation equivalence of first-order grammars
(finite sets of labelled rules for rewriting roots of first-order terms) is
presented. The equivalence generalizes the DPDA (deterministic pushdown
automata) equivalence, and the result corresponds to the result achieved by
Senizergues (1998, 2005) in the framework of equational graphs, or of PDA with
restricted epsilon-steps. The framework of classical first-order terms seems
particularly useful for providing a proof that should be understandable for a
wider audience. We also discuss an extension to branching bisimilarity,
announced by Fu and Yin (2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7944</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7944</id><created>2014-05-30</created><authors><author><keyname>Jadon</keyname><forenames>Shruti</forenames></author><author><keyname>Singhal</keyname><forenames>Anubhav</forenames></author><author><keyname>Dawn</keyname><forenames>Suma</forenames></author></authors><title>Military Simulator - A Case Study of Behaviour Tree and Unity based
  architecture</title><categories>cs.AI</categories><comments>4 pages, 4 figures</comments><msc-class>90-00</msc-class><acm-class>G.1.3; G.2.2; I.2.1</acm-class><doi>10.5120/15350-3691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how the combination of Behaviour Tree and Utility Based
AI architecture can be used to design more realistic bots for Military
Simulators. In this work, we have designed a mathematical model of a simulator
system which in turn helps in analyzing the results and finding out the various
spaces on which our favorable situation might exist, this is done
geometrically. In the mathematical model, we have explained the matrix
formation and its significance followed up in dynamic programming approach we
explained the possible graph formation which will led improvisation of AI,
latter we explained the possible geometrical structure of the matrix operations
and its impact on a particular decision, we also explained the conditions under
which it tend to fail along with a possible solution in future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7958</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7958</id><created>2014-05-30</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Pan</keyname><forenames>Tony</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin</forenames></author><author><keyname>Kong</keyname><forenames>Jun</forenames></author><author><keyname>Cooper</keyname><forenames>Lee</forenames></author><author><keyname>Klasky</keyname><forenames>Scott</forenames></author><author><keyname>Saltz</keyname><forenames>Joel</forenames></author></authors><title>Region Templates: Data Representation and Management for Large-Scale
  Image Analysis</title><categories>cs.DC</categories><comments>43 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed memory machines equipped with CPUs and GPUs (hybrid computing
nodes) are hard to program because of the multiple layers of memory and
heterogeneous computing configurations. In this paper, we introduce a region
template abstraction for the efficient management of common data types used in
analysis of large datasets of high resolution images on clusters of hybrid
computing nodes. The region template provides a generic container template for
common data structures, such as points, arrays, regions, and object sets,
within a spatial and temporal bounding box. The region template abstraction
enables different data management strategies and data I/O implementations,
while providing a homogeneous, unified interface to the application for data
storage and retrieval. The execution of region templates applications is
coordinated by a runtime system that supports efficient execution in hybrid
machines. Region templates applications are represented as hierarchical
dataflow in which each computing stage may be represented as another dataflow
of finer-grain tasks. A number of optimizations for hybrid machines are
available in our runtime system, including performance-aware scheduling for
maximizing utilization of computing devices and techniques to reduce impact of
data transfers between CPUs and GPUs. An experimental evaluation on a
state-of-the-art hybrid cluster using a microscopy imaging study shows that
this abstraction adds negligible overhead (about 3%) and achieves good
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7962</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7962</id><created>2014-05-30</created><authors><author><keyname>Henry</keyname><forenames>Julien</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Asavoae</keyname><forenames>Mihail</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Ma&#xef;za</keyname><forenames>Claire</forenames><affiliation>VERIMAG - IMAG</affiliation></author></authors><title>How to Compute Worst-Case Execution Time by Optimization Modulo Theory
  and a Clever Encoding of Program Semantics</title><categories>cs.PL cs.LO</categories><comments>ACM SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for
  Embedded Systems 2014, Edimbourg : United Kingdom (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In systems with hard real-time constraints, it is necessary to compute upper
bounds on the worst-case execution time (WCET) of programs; the closer the
bound to the real WCET, the better. This is especially the case of synchronous
reactive control loops with a fixed clock; the WCET of the loop body must not
exceed the clock period. We compute the WCET (or at least a close upper bound
thereof) as the solution of an optimization modulo theory problem that takes
into account the semantics of the program, in contrast to other methods that
compute the longest path whether or not it is feasible according to these
semantics. Optimization modulo theory extends satisfiability modulo theory
(SMT) to maximization problems. Immediate encodings of WCET problems into SMT
yield formulas intractable for all current production-grade solvers; this is
inherent to the DPLL(T) approach to SMT implemented in these solvers. By
conjoining some appropriate &quot;cuts&quot; to these formulas, we considerably reduce
the computation time of the SMT-solver. We experimented our approach on a
variety of control programs, using the OTAWA analyzer both as baseline and as
underlying microarchitectural analysis for our analysis, and show notable
improvement on the WCET bound on a variety of benchmarks and control programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7964</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7964</id><created>2014-05-30</created><updated>2014-06-02</updated><authors><author><keyname>Karaaslan</keyname><forenames>Faruk</forenames></author></authors><title>Neutrosophic soft sets with applications in decision making</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1305.2724 by other authors</comments><journal-ref>International Journal of Information Science and Intelligent
  System, 4(2), 1-20, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We firstly present definitions and properties in study of Maji
\cite{maji-2013} on neutrosophic soft sets. We then give a few notes on his
study. Next, based on \c{C}a\u{g}man \cite{cagman-2014}, we redefine the notion
of neutrosophic soft set and neutrosophic soft set operations to make more
functional. By using these new definitions we construct a decision making
method and a group decision making method which selects a set of optimum
elements from the alternatives. We finally present examples which shows that
the methods can be successfully applied to many problems that contain
uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.7975</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.7975</id><created>2014-05-17</created><authors><author><keyname>Canhasi</keyname><forenames>Ercan</forenames></author></authors><title>Multi-layered graph-based multi-document summarization model</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-document summarization is a process of automatic generation of a
compressed version of the given collection of documents. Recently, the
graph-based models and ranking algorithms have been actively investigated by
the extractive document summarization community. While most work to date
focuses on homogeneous connecteness of sentences and heterogeneous connecteness
of documents and sentences (e.g. sentence similarity weighted by document
importance), in this paper we present a novel 3-layered graph model that
emphasizes not only sentence and document level relations but also the
influence of under sentence level relations (e.g. a part of sentence
similarity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0013</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0013</id><created>2014-05-30</created><authors><author><keyname>Perrault-Joncas</keyname><forenames>Dominique</forenames></author><author><keyname>Meila</keyname><forenames>Marina</forenames></author></authors><title>Estimating Vector Fields on Manifolds and the Embedding of Directed
  Graphs</title><categories>stat.ML cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of embedding directed graphs in Euclidean
space while retaining directional information. We model a directed graph as a
finite set of observations from a diffusion on a manifold endowed with a vector
field. This is the first generative model of its kind for directed graphs. We
introduce a graph embedding algorithm that estimates all three features of this
model: the low-dimensional embedding of the manifold, the data density and the
vector field. In the process, we also obtain new theoretical results on the
limits of &quot;Laplacian type&quot; matrices derived from directed graphs. The
application of our method to both artificially constructed and real data
highlights its strengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0017</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0017</id><created>2014-05-30</created><authors><author><keyname>Iv&#xe1;n</keyname><forenames>Szabolcs</forenames></author><author><keyname>Lelkes</keyname><forenames>&#xc1;d&#xe1;m D&#xe1;niel</forenames></author><author><keyname>Nagy-Gy&#xf6;rgy</keyname><forenames>Judit</forenames></author><author><keyname>Sz&#xf6;r&#xe9;nyi</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Tur&#xe1;n</keyname><forenames>Gy&#xf6;rgy</forenames></author></authors><title>Biclique coverings, rectifier networks and the cost of
  $\varepsilon$-removal</title><categories>cs.FL cs.DM</categories><comments>12 pages, to appear in proceedings of DCFS 2014: 16th International
  Conference on Descriptional Complexity of Finite-State Systems</comments><msc-class>68R10</msc-class><acm-class>G.2.2; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We relate two complexity notions of bipartite graphs: the minimal weight
biclique covering number $\mathrm{Cov}(G)$ and the minimal rectifier network
size $\mathrm{Rect}(G)$ of a bipartite graph $G$. We show that there exist
graphs with $\mathrm{Cov}(G)\geq \mathrm{Rect}(G)^{3/2-\epsilon}$. As a
corollary, we establish that there exist nondeterministic finite automata
(NFAs) with $\varepsilon$-transitions, having $n$ transitions total such that
the smallest equivalent $\varepsilon$-free NFA has $\Omega(n^{3/2-\epsilon})$
transitions. We also formulate a version of previous bounds for the weighted
set cover problem and discuss its connections to giving upper bounds for the
possible blow-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0022</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0022</id><created>2014-05-30</created><updated>2015-06-23</updated><authors><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>Error Decay of (almost) Consistent Signal Estimations from Quantized
  Gaussian Random Projections</title><categories>cs.IT math.IT</categories><comments>23 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new error bounds on &quot;consistent&quot; reconstruction methods
for signals observed from quantized random projections. Those signal estimation
techniques guarantee a perfect matching between the available quantized data
and a new observation of the estimated signal under the same sensing model.
Focusing on dithered uniform scalar quantization of resolution $\delta&gt;0$, we
prove first that, given a Gaussian random frame of $\mathbb R^N$ with $M$
vectors, the worst-case $\ell_2$-error of consistent signal reconstruction
decays with high probability as $O(\frac{N}{M}\log\frac{M}{\sqrt N})$ uniformly
for all signals of the unit ball $\mathbb B^N \subset \mathbb R^N$. Up to a log
factor, this matches a known lower bound in $\Omega(N/M)$ and former empirical
validations in $O(N/M)$. Equivalently, with a minimal number of frame
coefficients growing like $M = O(\frac{N}{\epsilon_0}\log\frac{\sqrt
N}{\epsilon_0})$, any vectors in $\mathbb B^N$ with $M$ identical quantized
projections are at most $\epsilon_0$ apart with high probability. Second, in
the context of Quantized Compressed Sensing with $M$ Gaussian random
measurements and under the same scalar quantization scheme, consistent
reconstructions of $K$-sparse signals of $\mathbb R^N$ have a worst-case error
that decreases with high probability as $O(\frac{K}{M}\log\frac{MN}{\sqrt
K^3})$ uniformly for all such signals. Finally, we show that the proximity of
vectors whose quantized random projections are only approximately consistent
can still be bounded with high probability. A certain level of corruption is
thus allowed in the quantization process, up to the appearance of a systematic
bias in the reconstruction error of (almost) consistent signal estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0023</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0023</id><created>2014-05-30</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Oliva</keyname><forenames>Diego</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez-Cisneros</keyname><forenames>Marco</forenames></author><author><keyname>Sossa</keyname><forenames>Humberto</forenames></author></authors><title>Circle detection using electro-magnetism optimization</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.7362</comments><journal-ref>Information Sciences, Volume 182, Issue 1, 1 January 2012, Pages
  40-55, ISSN 0020-0255,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a circle detection method based on Electromagnetism-Like
Optimization (EMO). Circle detection has received considerable attention over
the last years thanks to its relevance for many computer vision tasks. EMO is a
heuristic method for solving complex optimization problems inspired in
electromagnetism principles. This algorithm searches a solution based in the
attraction and repulsion among prototype candidates. In this paper the
detection process is considered to be similar to an optimization problem, the
algorithm uses the combination of three edge points (x, y, r) as parameters to
determine circles candidates in the scene. An objective function determines if
such circle candidates are actually present in the image. The EMO algorithm is
used to find the circle candidate that is better related with the real circle
present in the image according to the objective function. The final algorithm
is a fast circle detector that locates circles with sub-pixel accuracy even
considering complicated conditions and noisy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0032</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0032</id><created>2014-05-30</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Pollyanna</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Matheus</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author></authors><title>Comparing and Combining Sentiment Analysis Methods</title><categories>cs.CL</categories><comments>Proceedings of the first ACM conference on Online social networks
  (2013) 27-38</comments><doi>10.1145/2512938.2512951</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Several messages express opinions about events, products, and services,
political views or even their author's emotional state and mood. Sentiment
analysis has been used in several applications including analysis of the
repercussions of events in social networks, analysis of opinions about products
and services, and simply to better understand aspects of social communication
in Online Social Networks (OSNs). There are multiple methods for measuring
sentiments, including lexical-based approaches and supervised machine learning
methods. Despite the wide use and popularity of some methods, it is unclear
which method is better for identifying the polarity (i.e., positive or
negative) of a message as the current literature does not provide a method of
comparison among existing methods. Such a comparison is crucial for
understanding the potential limitations, advantages, and disadvantages of
popular methods in analyzing the content of OSNs messages. Our study aims at
filling this gap by presenting comparisons of eight popular sentiment analysis
methods in terms of coverage (i.e., the fraction of messages whose sentiment is
identified) and agreement (i.e., the fraction of identified sentiments that are
in tune with ground truth). We develop a new method that combines existing
approaches, providing the best coverage results and competitive agreement. We
also present a free Web service called iFeel, which provides an open API for
accessing and comparing results across different sentiment methods for a given
text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0043</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0043</id><created>2014-05-30</created><authors><author><keyname>Bayless</keyname><forenames>Sam</forenames></author><author><keyname>Bayless</keyname><forenames>Noah</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Hu</keyname><forenames>Alan J.</forenames></author></authors><title>SAT Modulo Monotonic Theories</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the concept of a monotonic theory and show how to build efficient
SMT (SAT Modulo Theory) solvers, including effective theory propagation and
clause learning, for such theories. We present examples showing that monotonic
theories arise from many common problems, e.g., graph properties such as
reachability, shortest paths, connected components, minimum spanning tree, and
max-flow/min-cut, and then demonstrate our framework by building SMT solvers
for each of these theories. We apply these solvers to procedural content
generation problems, demonstrating major speed-ups over state-of-the-art
approaches based on SAT or Answer Set Programming, and easily solving several
instances that were previously impractical to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0045</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0045</id><created>2014-05-30</created><updated>2014-06-06</updated><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>A belief-based evolutionarily stable strategy</title><categories>cs.GT q-bio.PE</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an equilibrium refinement of the Nash equilibrium, evolutionarily stable
strategy (ESS) is a key concept in evolutionary game theory and has attracted
growing interest. An ESS can be either a pure strategy or a mixed strategy.
Even though the randomness is allowed in mixed strategy, the selection
probability of pure strategy in a mixed strategy may fluctuate due to the
impact of many factors. The fluctuation can lead to more uncertainty. In this
paper, such uncertainty involved in mixed strategy has been further taken into
consideration: a belief strategy is proposed in terms of Dempster-Shafer
evidence theory. Furthermore, based on the proposed belief strategy, a
belief-based ESS has been developed. The belief strategy and belief-based ESS
can reduce to the mixed strategy and mixed ESS, which provide more realistic
and powerful tools to describe interactions among agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0049</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0049</id><created>2014-05-31</created><authors><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Yin</keyname><forenames>Rui</forenames></author></authors><title>Ergodic Capacity Comparison of Different Relay Precoding Schemes in
  Dual-Hop AF Systems with Co-Channel Interference</title><categories>cs.IT math.IT</categories><comments>Accepted, to appear in IEEE Transactions on Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the ergodic capacity of a dual-hop
amplify-and-forward relaying system where the relay is equipped with multiple
antennas and subject to co-channel interference (CCI) and the additive white
Gaussian noise. Specifically, we consider three heuristic precoding schemes,
where the relay first applies the 1) maximal-ratio combining (MRC) 2)
zero-forcing (ZF) 3) minimum mean-squared error (MMSE) principle to combine the
signal from the source, and then steers the transformed signal towards the
destination with the maximum ratio transmission (MRT) technique. For the
MRC/MRT and MMSE/MRT schemes, we present new tight analytical upper and lower
bounds for the ergodic capacity, while for the ZF/MRT scheme, we derive a new
exact analytical ergodic capacity expression. Moreover, we make a comparison
among all the three schemes, and our results reveal that, in terms of the
ergodic capacity performance, the MMSE/MRT scheme always has the best
performance and the ZF/MRT scheme is slightly inferior, while the MRC/MRT
scheme is always the worst one. Finally, the asymptotic behavior of ergodic
capacity for the three proposed schemes are characterized in large $N$
scenario, where $N$ is the number of relay antennas. Our results reveal that,
in the large $N$ regime, both the ZF/MRT and MMSE/MRT schemes have perfect
interference cancelation capability, which is not possible with the MRC/MRT
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0053</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0053</id><created>2014-05-31</created><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author></authors><title>Fast K\&quot;otter-Nielsen-H{\o}holdt Interpolation in the Guruswami-Sudan
  Algorithm</title><categories>cs.IT cs.SC math.IT</categories><comments>Submitted to ACCT-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K\&quot;otter-Nielsen-H{\o}holdt algorithm is a popular way to construct the
bivariate interpolation polynomial in the Guruswami-Sudan decoding algorithm
for Reed-Solomon codes. In this paper, we show how one can use Divide &amp; Conquer
techniques to provide an asymptotic speed-up of the algorithm, rendering its
complexity quasi-linear in n. Several of our observations can also provide a
practical speed-up to the classical version of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0062</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0062</id><created>2014-05-31</created><authors><author><keyname>Kebair</keyname><forenames>Fahem</forenames></author><author><keyname>Serin</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Towards a Multiagent Decision Support System for crisis Management</title><categories>cs.AI</categories><comments>14 pages. arXiv admin note: text overlap with arXiv:0907.0499</comments><journal-ref>J. Intelligent Systems 20(1): 47-60 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crisis management is a complex problem raised by the scientific community
currently. Decision support systems are a suitable solution for such issues,
they are indeed able to help emergency managers to prevent and to manage crisis
in emergency situations. However, they should be enough flexible and adaptive
in order to be reliable to solve complex problems that are plunged in dynamic
and unpredictable environments. The approach we propose in this paper addresses
this challenge. We expose here a modelling of information for an emergency
environment and an architecture of a multiagent decision support system that
deals with these information in order to prevent and to manage the occur of a
crisis in emergency situations. We focus on the first level of the system
mechanism which intends to perceive and to reflect the evolution of the current
situation. The general approach and experimentations are provided here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0067</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0067</id><created>2014-05-31</created><updated>2015-05-10</updated><authors><author><keyname>Le</keyname><forenames>Can M.</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>Optimization via Low-rank Approximation for Community Detection in
  Networks</title><categories>stat.ML cs.SI math.ST physics.soc-ph stat.TH</categories><comments>45 pages, 7 figures; added discussions about computational complexity
  and extension to more than two communities</comments><msc-class>62E10, 62G05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is one of the fundamental problems of network analysis,
for which a number of methods have been proposed. Most model-based or
criteria-based methods have to solve an optimization problem over a discrete
set of labels to find communities, which is computationally infeasible. Some
fast spectral algorithms have been proposed for specific methods or models, but
only on a case-by-case basis. Here we propose a general approach for maximizing
a function of a network adjacency matrix over discrete labels by projecting the
set of labels onto a subspace approximating the leading eigenvectors of the
expected adjacency matrix. This projection onto a low-dimensional space makes
the feasible set of labels much smaller and the optimization problem much
easier. We prove a general result about this method and show how to apply it to
several previously proposed community detection criteria, establishing its
consistency for label estimation in each case and demonstrating the fundamental
connection between spectral properties of the network and various model-based
approaches to community detection. Simulations and applications to real-world
data are included to demonstrate our method performs well for multiple problems
over a wide range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0073</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0073</id><created>2014-05-31</created><updated>2014-06-10</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Vihrovs</keyname><forenames>Jevg&#x113;nijs</forenames></author></authors><title>Size of Sets with Small Sensitivity: a Generalization of Simon's Lemma</title><categories>cs.CC</categories><doi>10.1007/978-3-319-17142-5_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of sets $S\subseteq\{0, 1\}^n$ with small sensitivity.
The well-known Simon's lemma says that any $S\subseteq\{0, 1\}^n$ of
sensitivity $s$ must be of size at least $2^{n-s}$. This result has been useful
for proving lower bounds on sensitivity of Boolean functions, with applications
to the theory of parallel computing and the &quot;sensitivity vs. block sensitivity&quot;
conjecture.
  In this paper, we take a deeper look at the size of such sets and their
structure. We show an unexpected &quot;gap theorem&quot;: if $S\subseteq\{0, 1\}^n$ has
sensitivity $s$, then we either have $|S|=2^{n-s}$ or $|S|\geq \frac{3}{2}
2^{n-s}$. This is shown via classifying such sets into sets that can be
constructed from low-sensitivity subsets of $\{0, 1\}^{n'}$ for $n'&lt;n$ and
irreducible sets which cannot be constructed in such a way and then proving a
lower bound on the size of irreducible sets.
  This provides new insights into the structure of low sensitivity subsets of
the Boolean hypercube $\{0, 1\}^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0074</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0074</id><created>2014-05-31</created><authors><author><keyname>Dakhare</keyname><forenames>Shradha</forenames></author><author><keyname>Chowhan</keyname><forenames>Harshal</forenames></author><author><keyname>Chandak</keyname><forenames>Manoj B.</forenames></author></authors><title>Combined Approach for Image Segmentation</title><categories>cs.CV</categories><comments>4 pages, 5 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>IJCTT, Volume 11 Number 3 May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many image segmentation techniques have been developed over the past two
decades for segmenting the images, which help for object recognition, occlusion
boundary estimation within motion or stereo systems, image compression, image
editing.
  In this, there is a combined approach for segmenting the image. By using
histogram equalization to the input image, from which it gives contrast
enhancement output image .After that by applying median filtering,which will
remove noise from contrast output image . At last I applied fuzzy c-mean
clustering algorithm to denoising output image, which give segmented output
image. In this way it produce better segmented image with less computation
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0079</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0079</id><created>2014-05-31</created><authors><author><keyname>Ramakrishna</keyname><forenames>Shashishekar</forenames></author><author><keyname>Paschke</keyname><forenames>Adrian</forenames></author></authors><title>Bridging the gap between Legal Practitioners and Knowledge Engineers
  using semi-formal KR</title><categories>cs.CL cs.AI</categories><comments>published in proceedings of the 8th International Workshop on Value
  Modeling and Business Ontology, VMBO, Berlin, 2014</comments><acm-class>K.6.3; D.2.5; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Structured English as a computation independent knowledge
representation format for non-technical users in business rules representation
has been proposed in OMGs Semantics and Business Vocabulary Representation
(SBVR). In the legal domain we face a similar problem. Formal representation
languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2
ontologies etc.) support the technical knowledge engineer and the automated
reasoning. But, they can be hardly used directly by the legal domain experts
who do not have a computer science background. In this paper we adapt the SBVR
Structured English approach for the legal domain and implement a
proof-of-concept, called KR4IPLaw, which enables legal domain experts to
represent their knowledge in Structured English in a computational independent
and hence, for them, more usable way. The benefit of this approach is that the
underlying pre-defined semantics of the Structured English approach makes
transformations into formal languages such as OASIS LegalRuleML and OWL2
ontologies possible. We exemplify our approach in the domain of patent law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0080</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0080</id><created>2014-05-31</created><updated>2015-01-09</updated><authors><author><keyname>Derksen</keyname><forenames>Harm</forenames></author></authors><title>On the equivalence between low rank matrix completion and tensor rank</title><categories>cs.NA</categories><msc-class>15A69, 15A83</msc-class><acm-class>F.2.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rank Minimization Problem asks to find a matrix of lowest rank inside a
linear variety of the space of n x n matrices. The Low Rank Matrix Completion
problem asks to complete a partially filled matrix such that the resulting
matrix has smallest possible rank.
  The Tensor Rank Problem asks to determine the rank of a tensor. We show that
these three problems are equivalent: each one of the problems can be reduced to
the other two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0085</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0085</id><created>2014-05-31</created><authors><author><keyname>Seyboth</keyname><forenames>Georg</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Cooperative Control of Linear Multi-Agent Systems via Distributed Output
  Regulation and Transient Synchronization</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of multi-agent coordination problems including reference
tracking and disturbance rejection requirements can be formulated as a
cooperative output regulation problem. The general framework captures typical
problems such as output synchronization, leader-follower synchronization, and
many more. In the present paper, we propose a novel distributed regulator for
groups of identical and non-identical linear agents. We consider global
external signals affecting all agents and local external signals affecting only
individual agents in the group. Both signal types may contain references and
disturbances. Our main contribution is a novel coupling among the agents based
on their transient state components or estimates thereof in the output feedback
case. This coupling achieves transient synchronization in order to improve the
cooperative behavior of the group in transient phases and guarantee a desired
decay rate of the synchronization error. This leads to a cooperative reaction
of the group on local disturbances acting on individual agents. The
effectiveness of the proposed distributed regulator is illustrated by a vehicle
platooning example and a coordination example for a group of four non-identical
3-DoF helicopter models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0086</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0086</id><created>2014-05-31</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Joint Source-Channel Vector Quantization for Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>15 Pages, Accepted for publication in IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study joint source-channel coding (JSCC) of compressed sensing (CS)
measurements using vector quantizer (VQ). We develop a framework for realizing
optimum JSCC schemes that enable encoding and transmitting CS measurements of a
sparse source over discrete memoryless channels, and decoding the sparse source
signal. For this purpose, the optimal design of encoder-decoder pair of a VQ is
considered, where the optimality is addressed by minimizing end-to-end mean
square error (MSE). We derive a theoretical lower-bound on the MSE performance,
and propose a practical encoder-decoder design through an iterative algorithm.
The resulting coding scheme is referred to as channel- optimized VQ for CS,
coined COVQ-CS. In order to address the encoding complexity issue of the
COVQ-CS, we propose to use a structured quantizer, namely low complexity
multi-stage VQ (MSVQ). We derive new encoding and decoding conditions for the
MSVQ, and then propose a practical encoder-decoder design algorithm referred to
as channel-optimized MSVQ for CS, coined COMSVQ-CS. Through simulation studies,
we compare the proposed schemes vis-a-vis relevant quantizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0089</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0089</id><created>2014-05-31</created><updated>2015-08-19</updated><authors><author><keyname>Isaac</keyname><forenames>Tobin</forenames></author><author><keyname>Burstedde</keyname><forenames>Carsten</forenames></author><author><keyname>Wilcox</keyname><forenames>Lucas C.</forenames></author><author><keyname>Ghattas</keyname><forenames>Omar</forenames></author></authors><title>Recursive Algorithms for Distributed Forests of Octrees</title><categories>cs.DC cs.CE cs.MS</categories><comments>35 pages, 15 figures, 3 tables</comments><doi>10.1137/140970963</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The forest-of-octrees approach to parallel adaptive mesh refinement and
coarsening (AMR) has recently been demonstrated in the context of a number of
large-scale PDE-based applications. Although linear octrees, which store only
leaf octants, have an underlying tree structure by definition, it is not often
exploited in previously published mesh-related algorithms. This is because the
branches are not explicitly stored, and because the topological relationships
in meshes, such as the adjacency between cells, introduce dependencies that do
not respect the octree hierarchy. In this work we combine hierarchical and
topological relationships between octree branches to design efficient recursive
algorithms.
  We present three important algorithms with recursive implementations. The
first is a parallel search for leaves matching any of a set of multiple search
criteria. The second is a ghost layer construction algorithm that handles
arbitrarily refined octrees that are not covered by previous algorithms, which
require a 2:1 condition between neighboring leaves. The third is a universal
mesh topology iterator. This iterator visits every cell in a domain partition,
as well as every interface (face, edge and corner) between these cells. The
iterator calculates the local topological information for every interface that
it visits, taking into account the nonconforming interfaces that increase the
complexity of describing the local topology. To demonstrate the utility of the
topology iterator, we use it to compute the numbering and encoding of
higher-order $C^0$ nodal basis functions.
  We analyze the complexity of the new recursive algorithms theoretically, and
assess their performance, both in terms of single-processor efficiency and in
terms of parallel scalability, demonstrating good weak and strong scaling up to
458k cores of the JUQUEEN supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0090</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0090</id><created>2014-05-31</created><authors><author><keyname>Puri</keyname><forenames>Arjun</forenames></author><author><keyname>Kumar</keyname><forenames>Sudesh</forenames></author></authors><title>Error Control Codes: A Novel Solution for Secret Key Generation and Key
  Refreshment Problem</title><categories>cs.CR</categories><comments>6 pages, 2 figures</comments><journal-ref>International Journal of Computer Applications 92(1):1-6, April
  2014</journal-ref><doi>10.5120/15970-4720</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography is the science of encrypting the information so that it is
rendered unreadable for an intruder. Cryptographic techniques are of utmost
importance in today's world as the information to be sent might be of
invaluable importance to both the sender and the receiver. Various
cryptographic techniques ensure that even if an intruder intercepts the sent
information, he is not able to decipher it thus render ending it useless for
the intruder. Cryptography can be grouped into two types, that is Symmetric key
cryptography and Asymmetric key cryptography. Symmetric key cryptography uses
the same key for encryption as well as decryption thus making it faster
compared to Asymmetric Key cryptography which uses different keys for
encryption and decryption. Generation of dynamic keys for Symmetric key
cryptography is an interesting field and in this we have tapped this field so
as to generate dynamic keys for symmetric key cryptography. In this work, we
have devised an algorithm for generating dynamic keys for sending messages over
a communication channel and also solving key refreshment problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0117</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0117</id><created>2014-05-31</created><authors><author><keyname>Field</keyname><forenames>Hayden</forenames></author><author><keyname>Anderson</keyname><forenames>Glen</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>EACOF: A Framework for Providing Energy Transparency to enable
  Energy-Aware Software Development</title><categories>cs.SE</categories><acm-class>D.2.8; D.2.2; D.2.13</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making energy consumption data accessible to software developers is an
essential step towards energy efficient software engineering. The presence of
various different, bespoke and incompatible, methods of instrumentation to
obtain energy readings is currently limiting the widespread use of energy data
in software development. This paper presents EACOF, a modular Energy-Aware
Computing Framework that provides a layer of abstraction between sources of
energy data and the applications that exploit them. EACOF replaces platform
specific instrumentation through two APIs - one accepts input to the framework
while the other provides access to application software. This allows developers
to profile their code for energy consumption in an easy and portable manner
using simple API calls. We outline the design of our framework and provide
details of the API functionality. In a use case, where we investigate the
impact of data bit width on the energy consumption of various sorting
algorithms, we demonstrate that the data obtained using EACOF provides
interesting, sometimes counter-intuitive, insights. All the code is available
online under an open source license. http://github.com/eacof
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0118</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0118</id><created>2014-05-31</created><authors><author><keyname>Perrault-Joncas</keyname><forenames>Dominique</forenames></author><author><keyname>Meila</keyname><forenames>Marina</forenames></author></authors><title>Improved graph Laplacian via geometric self-consistency</title><categories>stat.ML cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of setting the kernel bandwidth used by Manifold
Learning algorithms to construct the graph Laplacian. Exploiting the connection
between manifold geometry, represented by the Riemannian metric, and the
Laplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian's
ability to preserve the geometry of the data. Experiments show that this
principled approach is effective and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0124</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0124</id><created>2014-05-31</created><authors><author><keyname>Jammal</keyname><forenames>Manar</forenames></author><author><keyname>Singh</keyname><forenames>Taranpreet</forenames></author><author><keyname>Shami</keyname><forenames>Abdallah</forenames></author><author><keyname>Asal</keyname><forenames>Rasool</forenames></author><author><keyname>Li</keyname><forenames>Yiming</forenames></author></authors><title>Software-Defined Networking: State of the Art and Research Challenges</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-and-play information technology (IT) infrastructure has been expanding
very rapidly in recent years. With the advent of cloud computing, many
ecosystem and business paradigms are encountering potential changes and may be
able to eliminate their IT infrastructure maintenance processes. Real-time
performance and high availability requirements have induced telecom networks to
adopt the new concepts of the cloud model: software-defined networking (SDN)
and network function virtualization (NFV). NFV introduces and deploys new
network functions in an open and standardized IT environment, while SDN aims to
transform the way networks function. SDN and NFV are complementary
technologies; they do not depend on each other. However, both concepts can be
merged and have the potential to mitigate the challenges of legacy networks. In
this paper, our aim is to describe the benefits of using SDN in a multitude of
environments such as in data centers, data center networks, and Network as
Service offerings. We also present the various challenges facing SDN, from
scalability to reliability and security concerns, and discuss existing
solutions to these challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0132</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0132</id><created>2014-06-01</created><authors><author><keyname>Zheng</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author><author><keyname>He</keyname><forenames>Fei</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Seeing the Big Picture: Deep Embedding with Contextual Evidences</title><categories>cs.CV</categories><comments>10 pages, 13 figures, 7 tables, submitted to ACM Multimedia 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the Bag-of-Words (BoW) model based image retrieval task, the precision of
visual matching plays a critical role in improving retrieval performance.
Conventionally, local cues of a keypoint are employed. However, such strategy
does not consider the contextual evidences of a keypoint, a problem which would
lead to the prevalence of false matches. To address this problem, this paper
defines &quot;true match&quot; as a pair of keypoints which are similar on three levels,
i.e., local, regional, and global. Then, a principled probabilistic framework
is established, which is capable of implicitly integrating discriminative cues
from all these feature levels.
  Specifically, the Convolutional Neural Network (CNN) is employed to extract
features from regional and global patches, leading to the so-called &quot;Deep
Embedding&quot; framework. CNN has been shown to produce excellent performance on a
dozen computer vision tasks such as image classification and detection, but few
works have been done on BoW based image retrieval. In this paper, firstly we
show that proper pre-processing techniques are necessary for effective usage of
CNN feature. Then, in the attempt to fit it into our model, a novel indexing
structure called &quot;Deep Indexing&quot; is introduced, which dramatically reduces
memory usage.
  Extensive experiments on three benchmark datasets demonstrate that, the
proposed Deep Embedding method greatly promotes the retrieval accuracy when CNN
feature is integrated. We show that our method is efficient in terms of both
memory and time cost, and compares favorably with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0134</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0134</id><created>2014-06-01</created><authors><author><keyname>Karimi</keyname><forenames>Ahmad</forenames></author><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Theoremizing Yablo's Paradox</title><categories>math.LO cs.LO</categories><comments>Preprint; all comments/suggestions are most welcome!</comments><msc-class>03B44, 03A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To counter a general belief that all the paradoxes stem from a kind of
circularity (or involve some self--reference, or use a diagonal argument)
Stephen Yablo designed a paradox in 1993 that seemingly avoided
self--reference. We turn Yablo's paradox, the most challenging paradox in the
recent years, into a genuine mathematical theorem in Linear Temporal Logic
(LTL). Indeed, Yablo's paradox comes in several varieties; and he showed in
2004 that there are other versions that are equally paradoxical. Formalizing
these versions of Yablo's paradox, we prove some theorems in LTL. This is the
first time that Yablo's paradox(es) become new(ly discovered) theorems in
mathematics and logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0140</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0140</id><created>2014-06-01</created><updated>2015-04-27</updated><authors><author><keyname>Fazli</keyname><forenames>MohammadAmin</forenames></author><author><keyname>Ghazimatin</keyname><forenames>Azin</forenames></author><author><keyname>Habibi</keyname><forenames>Jafar</forenames></author><author><keyname>Haghshenas</keyname><forenames>Hamid</forenames></author></authors><title>Team Selection For Prediction Tasks</title><categories>cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a random variable $O \in \mathbb{R}$ and a set of experts $E$, we
describe a method for finding a subset of experts $S \subseteq E$ whose
aggregated opinion best predicts the outcome of $O$. Therefore, the problem can
be regarded as a team formation for performing a prediction task. We show that
in case of aggregating experts' opinions by simple averaging, finding the best
team (the team with the lowest total error during past $k$ turns) can be
modeled with an integer quadratic programming and we prove its NP-hardness
whereas its relaxation is solvable in polynomial time. Finally, we do an
experimental comparison between different rounding and greedy heuristics and
show that our suggested tabu search works effectively.
  Keywords: Team Selection, Information Aggregation, Opinion Pooling, Quadratic
Programming, NP-Hard
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0143</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0143</id><created>2014-06-01</created><authors><author><keyname>Zhou</keyname><forenames>Fangfang</forenames></author><author><keyname>Chen</keyname><forenames>Hongbin</forenames></author><author><keyname>Yu</keyname><forenames>Rong</forenames></author><author><keyname>Fan</keyname><forenames>Lisheng</forenames></author></authors><title>Power Allocation and Transmitter Switching for Broadcasting with
  Multiple Energy Harvesting Transmitters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of battery technology, energy harvesting communication
systems attracted great research attention in recent years. However, energy
harvesting communication systems with multiple transmitters and multiple
receivers have not been considered yet. In this paper, the problem of
broadcasting in a communication system with multiple energy harvesting
transmitters and multiple receivers is studied. First, regarding the
transmitters as a 'hole transmitter' [1], the optimal total transmission power
is obtained and the optimal power allocation policy in [2] is extended to our
system setup, with the aim of minimizing the transmission completion time.
Then, a simpler power allocation policy is developed to allocate the optimal
total transmission power to the data transmissions. As transmitter switching
can provide flexibility and robustness to an energy harvesting communication
system, especially when a transmitter is broken or the energy harvested by a
transmitter is insufficient, a transmitter switching policy is further
developed to choose a suitable transmitter to work whenever necessary. The
results show that the proposed power allocation policy performs close to the
optimal one and outperforms some heuristic ones in terms of transmission
completion time. Besides, the proposed transmitter switching policy outperforms
some heuristic ones in terms of number of switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0146</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0146</id><created>2014-06-01</created><updated>2014-12-11</updated><authors><author><keyname>Hric</keyname><forenames>Darko</forenames></author><author><keyname>Darst</keyname><forenames>Richard K.</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Community detection in networks: Structural communities versus ground
  truth</title><categories>physics.soc-ph cs.IR cs.SI q-bio.QM</categories><comments>21 pages, 19 figures</comments><journal-ref>Phys. Rev. E 90, 062805 (2014)</journal-ref><doi>10.1103/PhysRevE.90.062805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms to find communities in networks rely just on structural
information and search for cohesive subsets of nodes. On the other hand, most
scholars implicitly or explicitly assume that structural communities represent
groups of nodes with similar (non-topological) properties or functions. This
hypothesis could not be verified, so far, because of the lack of network
datasets with information on the classification of the nodes. We show that
traditional community detection methods fail to find the metadata groups in
many large networks. Our results show that there is a marked separation between
structural communities and metadata groups, in line with recent findings. That
means that either our current modeling of community structure has to be
substantially modified, or that metadata groups may not be recoverable from
topology alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0154</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0154</id><created>2014-06-01</created><authors><author><keyname>Apollonio</keyname><forenames>Nicola</forenames></author><author><keyname>Caramia</keyname><forenames>Massimiliano</forenames></author><author><keyname>Franciosa</keyname><forenames>Paolo Giulio</forenames></author></authors><title>On the Galois Lattice of Bipartite Distance Hereditary Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a complete characterization of bipartite graphs having tree-like
Galois lattices. We prove that the poset obtained by deleting bottom and top
elements from the Galois lattice of a bipartite graph is tree-like if and only
if the graph is a Bipartite Distance Hereditary graph. By relying on the
interplay between bipartite distance hereditary graphs and series-parallel
graphs, we show that the lattice can be realized as the containment relation
among directed paths in an arborescence. Moreover, a compact encoding of
Bipartite Distance Hereditary graphs is proposed, that allows optimal time
computation of neighborhood intersections and maximal bicliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0155</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0155</id><created>2014-06-01</created><authors><author><keyname>Jabbour</keyname><forenames>Said</forenames></author><author><keyname>Ma</keyname><forenames>Yue</forenames></author><author><keyname>Raddaoui</keyname><forenames>Badran</forenames></author><author><keyname>Sais</keyname><forenames>Lakhdar</forenames></author><author><keyname>Salhi</keyname><forenames>Yakoub</forenames></author></authors><title>On the measure of conflicts: A MUS-Decomposition Based Framework</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring inconsistency is viewed as an important issue related to handling
inconsistencies. Good measures are supposed to satisfy a set of rational
properties. However, defining sound properties is sometimes problematic. In
this paper, we emphasize one such property, named Decomposability, rarely
discussed in the literature due to its modeling difficulties. To this end, we
propose an independent decomposition which is more intuitive than existing
proposals. To analyze inconsistency in a more fine-grained way, we introduce a
graph representation of a knowledge base and various MUSdecompositions. One
particular MUS-decomposition, named distributable MUS-decomposition leads to an
interesting partition of inconsistencies in a knowledge base such that multiple
experts can check inconsistencies in parallel, which is impossible under
existing measures. Such particular MUSdecomposition results in an inconsistency
measure that satisfies a number of desired properties. Moreover, we give an
upper bound complexity of the measure that can be computed using 0/1 linear
programming or Min Cost Satisfiability problems, and conduct preliminary
experiments to show its feasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0156</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0156</id><created>2014-06-01</created><updated>2014-11-20</updated><authors><author><keyname>Han</keyname><forenames>Sheng</forenames></author><author><keyname>Wang</keyname><forenames>Suzhen</forenames></author><author><keyname>Wu</keyname><forenames>Xinyu</forenames></author></authors><title>$l_1$-regularized Outlier Isolation and Regression</title><categories>cs.CV cs.LG stat.ML</categories><comments>Outlier Detection, Robust Regression, Robust Rank Factorization,
  $l_1$-regularization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a new regression model called $l_1$-regularized outlier
isolation and regression (LOIRE) and a fast algorithm based on block coordinate
descent to solve this model. Besides, assuming outliers are gross errors
following a Bernoulli process, this paper also presented a Bernoulli estimate
model which, in theory, should be very accurate and robust due to its complete
elimination of affections caused by outliers. Though this Bernoulli estimate is
hard to solve, it could be approximately achieved through a process which takes
LOIRE as an important intermediate step. As a result, the approximate Bernoulli
estimate is a good combination of Bernoulli estimate's accuracy and LOIRE
regression's efficiency with several simulations conducted to strongly verify
this point. Moreover, LOIRE can be further extended to realize robust rank
factorization which is powerful in recovering low-rank component from massive
corruptions. Extensive experimental results showed that the proposed method
outperforms state-of-the-art methods like RPCA and GoDec in the aspect of
computation speed with a competitive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0157</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0157</id><created>2014-06-01</created><authors><author><keyname>Applebaum</keyname><forenames>Benny</forenames></author><author><keyname>David</keyname><forenames>Liron</forenames></author><author><keyname>Even</keyname><forenames>Guy</forenames></author></authors><title>Deterministic Rateless Codes for BSC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rateless code encodes a finite length information word into an infinitely
long codeword such that longer prefixes of the codeword can tolerate a larger
fraction of errors. A rateless code achieves capacity for a family of channels
if, for every channel in the family, reliable communication is obtained by a
prefix of the code whose rate is arbitrarily close to the channel's capacity.
As a result, a universal encoder can communicate over all channels in the
family while simultaneously achieving optimal communication overhead. In this
paper, we construct the first \emph{deterministic} rateless code for the binary
symmetric channel. Our code can be encoded and decoded in $O(\beta)$ time per
bit and in almost logarithmic parallel time of $O(\beta \log n)$, where $\beta$
is any (arbitrarily slow) super-constant function. Furthermore, the error
probability of our code is almost exponentially small $\exp(-\Omega(n/\beta))$.
Previous rateless codes are probabilistic (i.e., based on code ensembles),
require polynomial time per bit for decoding, and have inferior asymptotic
error probabilities. Our main technical contribution is a constructive proof
for the existence of an infinite generating matrix that each of its prefixes
induce a weight distribution that approximates the expected weight distribution
of a random linear code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0167</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0167</id><created>2014-06-01</created><updated>2015-02-06</updated><authors><author><keyname>Paul</keyname><forenames>Saurabh</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Feature Selection for Linear SVM with Provable Guarantees</title><categories>stat.ML cs.LG</categories><comments>Appearing in Proceedings of 18th AISTATS, JMLR W&amp;CP, vol 38, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give two provably accurate feature-selection techniques for the linear
SVM. The algorithms run in deterministic and randomized time respectively. Our
algorithms can be used in an unsupervised or supervised setting. The supervised
approach is based on sampling features from support vectors. We prove that the
margin in the feature space is preserved to within $\epsilon$-relative error of
the margin in the full feature space in the worst-case. In the unsupervised
setting, we also provide worst-case guarantees of the radius of the minimum
enclosing ball, thereby ensuring comparable generalization as in the full
feature space and resolving an open problem posed in Dasgupta et al. We present
extensive experiments on real-world datasets to support our theory and to
demonstrate that our method is competitive and often better than prior
state-of-the-art, for which there are no known provable guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0173</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0173</id><created>2014-06-01</created><authors><author><keyname>Stankovic</keyname><forenames>Ljubisa</forenames></author></authors><title>On the ISAR Image Analysis and Recovery with Unavailable or Heavily
  Corrupted Data</title><categories>cs.IT math.IT</categories><comments>9 pages, 6 figures, submitted to the IEEE Transactions on Aerospace
  and Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common ISAR radar images and signals can be reconstructed from much fewer
samples than the sampling theorem requires since they are usually sparse.
Unavailable randomly positioned samples can result from heavily corrupted parts
of the signal. Since these samples can be omitted and declared as unavailable,
the application of the compressive sensing methods in the recovery of heavily
corrupted signal and radar images is possible. A\ simple direct method for the
recovery of unavailable signal samples and the calculation of the restored ISAR
image is reviewed. An analysis of the noise influence is performed. For fast
maneuvering ISAR targets the sparsity property is lost since the ISAR image is
blurred. A nonparametric quadratic time-frequency representations based method
is used to restore the ISAR image sparsity. However, the linear relation
between the signal and the sparsity domain transformation is lost. A recently
proposed gradient recovery algorithm is adapted for this kind of analysis. It
does not require the linear relation of the signal and its sparsity domain
transformation in the process of unavailable data recovery. The presented
methods and results are tested on several numerical examples proving the
expected accuracy and improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0175</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0175</id><created>2014-06-01</created><authors><author><keyname>Halim</keyname><forenames>Zahid</forenames></author></authors><title>Evolutionary Search in the Space of Rules for Creation of New Two-Player
  Board Games</title><categories>cs.NE cs.AI</categories><doi>10.1142/S0218213013500280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Games have always been a popular test bed for artificial intelligence
techniques. Game developers are always in constant search for techniques that
can automatically create computer games minimizing the developer's task. In
this work we present an evolutionary strategy based solution towards the
automatic generation of two player board games. To guide the evolutionary
process towards games, which are entertaining, we propose a set of metrics.
These metrics are based upon different theories of entertainment in computer
games. This work also compares the entertainment value of the evolved games
with the existing popular board based games. Further to verify the
entertainment value of the evolved games with the entertainment value of the
human user a human user survey is conducted. In addition to the user survey we
check the learnability of the evolved games using an artificial neural network
based controller. The proposed metrics and the evolutionary process can be
employed for generating new and entertaining board games, provided an initial
search space is given to the evolutionary algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0184</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0184</id><created>2014-06-01</created><authors><author><keyname>Latif</keyname><forenames>Kamran</forenames></author></authors><title>Parallelism Via Concurrency at Multiple Levels</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the key elements determining the best performance of
computing by increasing the frequency of a single chip and to get the minimum
latency during execution of the programs to achieve best possible output. It is
not enough to provide concurrent improvements in the hardware as Software also
have to introduce concurrency in order to exploit the parallelism. The software
parallelism is defined by the control and data dependency of programs whereas
Hardware refers to the type of parallelism defined by the machine architecture
and hardware multiplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0187</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0187</id><created>2014-06-01</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H\rakan</forenames></author></authors><title>Piecewise Toeplitz Matrices-based Sensing for Rank Minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a set of piecewise Toeplitz matrices as the linear
mapping/sensing operator $\mathcal{A}: \mathbf{R}^{n_1 \times n_2} \rightarrow
\mathbf{R}^M$ for recovering low rank matrices from few measurements. We prove
that such operators efficiently encode the information so there exists a unique
reconstruction matrix under mild assumptions. This work provides a significant
extension of the compressed sensing and rank minimization theory, and it
achieves a tradeoff between reducing the memory required for storing the
sampling operator from $\mathcal{O}(n_1n_2M)$ to $\mathcal{O}(\max(n_1,n_2)M)$
but at the expense of increasing the number of measurements by $r$. Simulation
results show that the proposed operator can recover low rank matrices
efficiently with a reconstruction performance close to the cases of using
random unstructured operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0189</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0189</id><created>2014-06-01</created><authors><author><keyname>Malioutov</keyname><forenames>Dmitry</forenames></author><author><keyname>Slavov</keyname><forenames>Nikolai</forenames></author></authors><title>Convex Total Least Squares</title><categories>stat.ML cs.LG q-bio.GN q-bio.QM stat.AP</categories><comments>9 pages, 4 figures</comments><journal-ref>JMLR W&amp;CP 32 (1) :109-117, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the total least squares (TLS) problem that generalizes least squares
regression by allowing measurement errors in both dependent and independent
variables. TLS is widely used in applied fields including computer vision,
system identification and econometrics. The special case when all dependent and
independent variables have the same level of uncorrelated Gaussian noise, known
as ordinary TLS, can be solved by singular value decomposition (SVD). However,
SVD cannot solve many important practical TLS problems with realistic noise
structure, such as having varying measurement noise, known structure on the
errors, or large outliers requiring robust error-norms. To solve such problems,
we develop convex relaxation approaches for a general class of structured TLS
(STLS). We show both theoretically and experimentally, that while the plain
nuclear norm relaxation incurs large approximation errors for STLS, the
re-weighted nuclear norm approach is very effective, and achieves better
accuracy on challenging STLS problems than popular non-convex solvers. We
describe a fast solution based on augmented Lagrangian formulation, and apply
our approach to an important class of biological problems that use population
average measurements to infer cell-type and physiological-state specific
expression levels that are very hard to measure directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0193</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0193</id><created>2014-06-01</created><authors><author><keyname>Slavov</keyname><forenames>Nikolai</forenames></author></authors><title>Inference of Sparse Networks with Unobserved Variables. Application to
  Gene Regulatory Networks</title><categories>stat.ML cs.LG q-bio.MN q-bio.QM stat.AP</categories><comments>8 pages, 5 figures</comments><journal-ref>JMLR W&amp;CP 9:757-764, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are a unifying framework for modeling complex systems and network
inference problems are frequently encountered in many fields. Here, I develop
and apply a generative approach to network inference (RCweb) for the case when
the network is sparse and the latent (not observed) variables affect the
observed ones. From all possible factor analysis (FA) decompositions explaining
the variance in the data, RCweb selects the FA decomposition that is consistent
with a sparse underlying network. The sparsity constraint is imposed by a novel
method that significantly outperforms (in terms of accuracy, robustness to
noise, complexity scaling, and computational efficiency) Bayesian methods and
MLE methods using l1 norm relaxation such as K-SVD and l1--based sparse
principle component analysis (PCA). Results from simulated models demonstrate
that RCweb recovers exactly the model structures for sparsity as low (as
non-sparse) as 50% and with ratio of unobserved to observed variables as high
as 2. RCweb is robust to noise, with gradual decrease in the parameter ranges
as the noise level increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0196</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0196</id><created>2014-06-01</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>A linear algorithm for the grundy number of a tree</title><categories>cs.DM math.CO</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 1, February 2014</journal-ref><doi>10.5121/ijcsit.2014.6112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coloring of a graph G = (V,E) is a partition {V1, V2, . . ., Vk} of V into
independent sets or color classes. A vertex v Vi is a Grundy vertex if it is
adjacent to at least one vertex in each color class Vj . A coloring is a Grundy
coloring if every color class contains at least one Grundy vertex, and the
Grundy number of a graph is the maximum number of colors in a Grundy coloring.
We derive a natural upper bound on this parameter and show that graphs with
sufficiently large girth achieve equality in the bound. In particular, this
gives a linear time algorithm to determine the Grundy number of a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0200</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0200</id><created>2014-06-01</created><updated>2014-07-30</updated><authors><author><keyname>Gomaa</keyname><forenames>Ahmad</forenames></author><author><keyname>Jalloul</keyname><forenames>Louay M. A.</forenames></author></authors><title>Efficient Soft-Input Soft-Output Detection of Dual-Layer MIMO Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><doi>10.1109/LWC.2014.2340853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dual-layer multiple-input multiple-output (MIMO) system with multi-level
modulation is considered. A computationally efficient soft-input soft-output
receiver based on the exact max-log maximum a posteriori (max-log-MAP)
principle is presented in the context of iterative detection and decoding. We
show that the computational complexity of our exact max-log-MAP solution grows
linearly with the constellation size and is, furthermore, less than that of the
best known methods of Turbo-LORD that only provide approximate solutions. Using
decoder feedback to change the decision thresholds of the constellation
symbols, we show that the exhaustive search operation boils down to a simple
slicing operation. For various simulation parameters, we verify that our
solution performs identically to the brute-force exhaustive search max-log MAP
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0214</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0214</id><created>2014-06-01</created><authors><author><keyname>Bendich</keyname><forenames>Paul</forenames></author><author><keyname>Chin</keyname><forenames>Sang</forenames></author><author><keyname>Clarke</keyname><forenames>Jesse</forenames></author><author><keyname>deSena</keyname><forenames>Jonathan</forenames></author><author><keyname>Harer</keyname><forenames>John</forenames></author><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Newman</keyname><forenames>Andrew</forenames></author><author><keyname>Porter</keyname><forenames>David</forenames></author><author><keyname>Rouse</keyname><forenames>David</forenames></author><author><keyname>Strawn</keyname><forenames>Nate</forenames></author><author><keyname>Watkins</keyname><forenames>Adam</forenames></author></authors><title>Topological and Statistical Behavior Classifiers for Tracking
  Applications</title><categories>cs.SY math.AT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the first unified theory for target tracking using Multiple
Hypothesis Tracking, Topological Data Analysis, and machine learning. Our
string of innovations are 1) robust topological features are used to encode
behavioral information, 2) statistical models are fitted to distributions over
these topological features, and 3) the target type classification methods of
Wigren and Bar Shalom et al. are employed to exploit the resulting likelihoods
for topological features inside of the tracking procedure. To demonstrate the
efficacy of our approach, we test our procedure on synthetic vehicular data
generated by the Simulation of Urban Mobility package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0215</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0215</id><created>2014-06-01</created><authors><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Agarwal</keyname><forenames>Shivali</forenames></author><author><keyname>Chee</keyname><forenames>Yi-Min</forenames></author><author><keyname>Sindhgatta</keyname><forenames>Renuka R.</forenames></author><author><keyname>Oppenheim</keyname><forenames>Daniel V.</forenames></author><author><keyname>Lee</keyname><forenames>Juhnyoung</forenames></author><author><keyname>Ratakonda</keyname><forenames>Krishna</forenames></author></authors><title>Cognitive Coordination of Global Service Delivery</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal coordination mechanisms are of growing importance as human-based
service delivery becomes more globalized and informal mechanisms are no longer
effective. Further it is becoming apparent that business environments,
communication among distributed teams, and work performance are all subject to
endogenous and exogenous uncertainty.
  This paper describes a stochastic model of service requests in global service
delivery and then puts forth a cognitive approach for coordination in the face
of uncertainty, based on a perception-action loop and receding horizon control.
Optimization algorithms used are a mix of myopic dynamic programming and
constraint-based programming. The coordination approach described has been
deployed by a globally integrated enterprise in a very large-scale global
delivery system and has been demonstrated to improve work efficiency by 10-15%
as compared to manual planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0216</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0216</id><created>2014-06-01</created><authors><author><keyname>Huber</keyname><forenames>Jakob</forenames></author><author><keyname>Sztyler</keyname><forenames>Timo</forenames></author><author><keyname>Noessner</keyname><forenames>Jan</forenames></author><author><keyname>Murdock</keyname><forenames>Jaimie</forenames></author><author><keyname>Allen</keyname><forenames>Colin</forenames></author><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author></authors><title>LODE: Linking Digital Humanities Content to the Web of Data</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous digital humanities projects maintain their data collections in the
form of text, images, and metadata. While data may be stored in many formats,
from plain text to XML to relational databases, the use of the resource
description framework (RDF) as a standardized representation has gained
considerable traction during the last five years. Almost every digital
humanities meeting has at least one session concerned with the topic of digital
humanities, RDF, and linked data. While most existing work in linked data has
focused on improving algorithms for entity matching, the aim of the
LinkedHumanities project is to build digital humanities tools that work &quot;out of
the box,&quot; enabling their use by humanities scholars, computer scientists,
librarians, and information scientists alike. With this paper, we report on the
Linked Open Data Enhancer (LODE) framework developed as part of the
LinkedHumanities project. With LODE we support non-technical users to enrich a
local RDF repository with high-quality data from the Linked Open Data cloud.
LODE links and enhances the local RDF repository without compromising the
quality of the data. In particular, LODE supports the user in the enhancement
and linking process by providing intuitive user-interfaces and by suggesting
high-quality linking candidates using tailored matching algorithms. We hope
that the LODE framework will be useful to digital humanities scholars
complementing other digital humanities tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0223</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0223</id><created>2014-06-01</created><authors><author><keyname>Aman</keyname><forenames>Saima</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Prasanna</keyname><forenames>Viktor K.</forenames></author></authors><title>Holistic Measures for Evaluating Prediction Models in Smart Grids</title><categories>cs.LG</categories><comments>14 Pages, 8 figures, Accepted and to appear in IEEE Transactions on
  Knowledge and Data Engineering, 2014. Authors' final version. Copyright
  transferred to IEEE</comments><doi>10.1109/TKDE.2014.2327022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of prediction models is often based on &quot;abstract metrics&quot;
that estimate the model's ability to limit residual errors between the observed
and predicted values. However, meaningful evaluation and selection of
prediction models for end-user domains requires holistic and
application-sensitive performance measures. Inspired by energy consumption
prediction models used in the emerging &quot;big data&quot; domain of Smart Power Grids,
we propose a suite of performance measures to rationally compare models along
the dimensions of scale independence, reliability, volatility and cost. We
include both application independent and dependent measures, the latter
parameterized to allow customization by domain experts to fit their scenario.
While our measures are generalizable to other domains, we offer an empirical
analysis using real energy use data for three Smart Grid applications:
planning, customer education and demand response, which are relevant for energy
sustainability. Our results underscore the value of the proposed measures to
offer a deeper insight into models' behavior and their impact on real
applications, which benefit both data mining researchers and practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0231</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0231</id><created>2014-06-01</created><authors><author><keyname>Wang</keyname><forenames>Quanquan</forenames></author><author><keyname>Li</keyname><forenames>Yongping</forenames></author></authors><title>Ambiguous Proximity Distribution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proximity Distribution Kernel is an effective method for bag-of-featues based
image representation. In this paper, we investigate the soft assignment of
visual words to image features for proximity distribution. Visual word
contribution function is proposed to model ambiguous proximity distributions.
Three ambiguous proximity distributions is developed by three ambiguous
contribution functions. The experiments are conducted on both classification
and retrieval of medical image data sets. The results show that the performance
of the proposed methods, Proximity Distribution Kernel (PDK), is better or
comparable to the state-of-the-art bag-of-features based image representation
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0234</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0234</id><created>2014-06-01</created><authors><author><keyname>Gu</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Joint PIC and relay selection based on greedy techniques for cooperative
  DS-CDMA systems</title><categories>cs.IT math.IT</categories><comments>5 figures, 2 tables, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a cross-layer design strategy based on the parallel
interference cancellation (PIC) detection technique and a multi-relay selection
algorithm for the uplink of cooperative direct-sequence code-division multiple
access (DS-CDMA) systems. We devise a low-cost greedy list-based PIC (GL-PIC)
strategy with RAKE receivers as the front-end that can approach the maximum
likelihood detector performance. We also present a low-complexity multi-relay
selection algorithm based on greedy techniques that can approach the
performance of an exhaustive search. Simulations show an excellent bit error
rate performance of the proposed detection and relay selection algorithms as
compared to existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0242</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0242</id><created>2014-06-02</created><updated>2015-04-08</updated><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Fotis</forenames></author></authors><title>Random Walks that Find Perfect Objects and the Lov\'{a}sz Local Lemma</title><categories>math.CO cs.DM math.PR</categories><comments>28 pages, added weighted version, added Independent Sets version,
  added Latin Squares Application</comments><msc-class>68W20</msc-class><acm-class>F.1.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithmic local lemma by establishing a sufficient condition for
the uniform random walk on a directed graph to reach a sink quickly. Our work
is inspired by Moser's entropic method proof of the Lov\'{a}sz Local Lemma
(LLL) for satisfiability and completely bypasses the Probabilistic Method
formulation of the LLL. In particular, our method works when the underlying
state space is entirely unstructured. Similarly to Moser's argument, the key
point is that the inevitability of reaching a sink is established by bounding
the entropy of the walk as a function of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0253</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0253</id><created>2014-06-02</created><authors><author><keyname>Soorajprasad</keyname><forenames>M H</forenames></author><author><keyname>N</keyname><forenames>Balapradeep K</forenames></author><author><keyname>J</keyname><forenames>Antony P</forenames></author></authors><title>VirtuMob : Remote Desktop Virtualization Solution for Smarphones</title><categories>cs.OH</categories><comments>5 Pages, 4 Figures, Published with International Journal of Computer
  Trends and Technology (IJCTT). arXiv admin note: text overlap with
  arXiv:1310.5850 by other authors without attribution</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V11(4):169-173,May 2014</journal-ref><doi>10.14445/22312803/IJCTT-V11P136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility is an important attribute in todays computing world. Mobile
devices,smartphone and tablet PC are becoming an integral part of human life
because they are most effective and convenient communication tools. This paper
proposes a system to connect and access the desktops of remote computer systems
using an android based Smartphone. Virtual Network Computing based architecture
is used to develop the proposed system. Through a VirtuMob viewer provided on
the users Smartphone, the user will be able to access and manipulate the
desktops of remote computers. Several functionality such as viewing the
desktop, mouse operations, keyboard operations, manipulation of documents can
be performed from the Smartphone. VirtuMob server should be running on the
remote system and it must be attached to a network. VirtuMob Accelerator is
used to process the RFB frames of the desktop, perform Encoding of the frames
and then relay the frames to the viewer over the internet. Several Encoding
techniques are studied and analysed to determine which is best suited for the
proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0256</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0256</id><created>2014-06-02</created><authors><author><keyname>Danquah</keyname><forenames>Wiseborn Manfe</forenames></author><author><keyname>Altilar</keyname><forenames>Turgay D</forenames></author></authors><title>HYBRIST Mobility Model- A Novel Hybrid Mobility Model for VANET
  Simulations</title><categories>cs.NI</categories><comments>International Journal of Computers Applications. 7 pages paper with
  10 figures presents a novel mobility model for VANET simulations which best
  suits the fast developing BRT system type of mobility in Vehicular
  communications systems</comments><report-no>IJCA Journal, Volume 86, Number 14 Year of Publication: 2014</report-no><journal-ref>IJCA 86(14):15-21, January 2014</journal-ref><doi>10.5120/15053-3408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulations play a vital role in implementing, testing and validating
proposed algorithms and protocols in VANET. Mobility model, defined as the
movement pattern of vehicles, is one of the main factors that contribute
towards the efficient implementation of VANET algorithms and protocols. Using
near reality mobility models ensure that accurate results are obtained from
simulations. Mobility models that have been proposed and used to implement and
test VANET protocols and algorithms are either the urban mobility model or
highway mobility model. Algorithms and protocols implemented using urban or
highway mobility models may not produce accurate results in hybrid mobility
models without enhancement due to the vast differences in mobility patterns. It
is on this score the Hybrist, a novel hybrid mobility model is proposed. The
realistic mobility pattern trace file of the proposed Hybrist hybrid mobility
model can be imported to VANET simulators such as Veins and network simulators
such as ns2 and Qualnet to simulate VANET algorithms and protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0263</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0263</id><created>2014-06-02</created><updated>2015-06-03</updated><authors><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Nakashima</keyname><forenames>Yuto</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author><author><keyname>Tsuruta</keyname><forenames>Kazuya</forenames></author></authors><title>The &quot;Runs&quot; Theorem</title><categories>cs.DM cs.DS</categories><comments>simple proof with some more bounds</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new characterization of maximal repetitions (or runs) in strings
based on Lyndon words. The characterization leads to a proof of what was known
as the &quot;runs&quot; conjecture (Kolpakov \&amp; Kucherov (FOCS '99)), which states that
the maximum number of runs $\rho(n)$ in a string of length $n$ is less than
$n$. The proof is remarkably simple, considering the numerous endeavors to
tackle this problem in the last 15 years, and significantly improves our
understanding of how runs can occur in strings. In addition, we obtain an upper
bound of $3n$ for the maximum sum of exponents $\sigma(n)$ of runs in a string
of length $n$, improving on the best known bound of $4.1n$ by Crochemore et al.
(JDA 2012), as well as other improved bounds on related problems. The
characterization also gives rise to a new, conceptually simple linear-time
algorithm for computing all the runs in a string. A notable characteristic of
our algorithm is that, unlike all existing linear-time algorithms, it does not
utilize the Lempel-Ziv factorization of the string. We also establish a
relationship between runs and nodes of the Lyndon tree, which gives a simple
optimal solution to the 2-Period Query problem that was recently solved by
Kociumaka et al. (SODA 2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0267</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0267</id><created>2014-06-02</created><updated>2014-06-14</updated><authors><author><keyname>Dharmawansa</keyname><forenames>Prathapasinghe</forenames></author><author><keyname>Johnstone</keyname><forenames>Iain M.</forenames></author></authors><title>Joint density of eigenvalues in spiked multivariate models</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical methods of multivariate analysis are based on the eigenvalues
of one or two sample covariance matrices. In many applications of these
methods, for example to high dimensional data, it is natural to consider
alternative hypotheses which are a low rank departure from the null hypothesis.
For rank one alternatives, this note provides a representation for the joint
eigenvalue density in terms of a single contour integral. This will be of use
for deriving approximate distributions for likelihood ratios and linear
statistics used in testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0271</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0271</id><created>2014-06-02</created><authors><author><keyname>Gherekhloo</keyname><forenames>Soheil</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Extended Generalized DoF Optimality Regime of Treating Interference as
  Noise in the X Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simple scheme of treating interference as noise (TIN) is studied in this
paper for the 3 x 2 X channel. A new sum-capacity upper bound is derived. This
upper bound is transformed into a generalized degrees-of-freedom (GDoF) upper
bound, and is shown to coincide with the achievable GDoF of scheme that
combines TDMA and TIN for some conditions on the channel parameters. These
conditions specify a noisy interference regime which extends noisy interference
regimes available in literature. As a by-product, the sum-capacity of the 3 x 2
X channel is characterized within a constant gap in the given noisy
interference regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0281</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0281</id><created>2014-06-02</created><updated>2014-10-07</updated><authors><author><keyname>Cheplygina</keyname><forenames>Veronika</forenames></author><author><keyname>Tax</keyname><forenames>David M. J.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>On Classification with Bags, Groups and Sets</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classification problems can be difficult to formulate directly in terms
of the traditional supervised setting, where both training and test samples are
individual feature vectors. There are cases in which samples are better
described by sets of feature vectors, that labels are only available for sets
rather than individual samples, or, if individual labels are available, that
these are not independent. To better deal with such problems, several
extensions of supervised learning have been proposed, where either training
and/or test objects are sets of feature vectors. However, having been proposed
rather independently of each other, their mutual similarities and differences
have hitherto not been mapped out. In this work, we provide an overview of such
learning scenarios, propose a taxonomy to illustrate the relationships between
them, and discuss directions for further research in these areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0285</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0285</id><created>2014-06-02</created><authors><author><keyname>Li</keyname><forenames>Quan-Lin</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Block-Structured Supermarket Models</title><categories>cs.PF</categories><comments>65 pages; 7 figures</comments><msc-class>68M20, 90B22, 90B18</msc-class><acm-class>C.4; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supermarket models are a class of parallel queueing networks with an adaptive
control scheme that play a key role in the study of resource management of,
such as, computer networks, manufacturing systems and transportation networks.
When the arrival processes are non-Poisson and the service times are
non-exponential, analysis of such a supermarket model is always limited,
interesting, and challenging.
  This paper describes a supermarket model with non-Poisson inputs: Markovian
Arrival Processes (MAPs) and with non-exponential service times: Phase-type
(PH) distributions, and provides a generalized matrix-analytic method which is
first combined with the operator semigroup and the mean-field limit. When
discussing such a more general supermarket model, this paper makes some new
results and advances as follows: (1) Providing a detailed probability analysis
for setting up an infinite-dimensional system of differential vector equations
satisfied by the expected fraction vector, where &quot;the invariance of environment
factors&quot; is given as an important result. (2) Introducing the phase-type
structure to the operator semigroup and to the mean-field limit, and a
Lipschitz condition can be obtained by means of a unified matrix-differential
algorithm. (3) The matrix-analytic method is used to compute the fixed point
which leads to performance computation of this system. Finally, we use some
numerical examples to illustrate how the performance measures of this
supermarket model depend on the non-Poisson inputs and on the non-exponential
service times. Thus the results of this paper give new highlight on
understanding influence of non-Poisson inputs and of non-exponential service
times on performance measures of more general supermarket models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0288</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0288</id><created>2014-06-02</created><authors><author><keyname>Kulkarni</keyname><forenames>Kaustubh</forenames></author><author><keyname>Evangelidis</keyname><forenames>Georgios</forenames></author><author><keyname>Cech</keyname><forenames>Jan</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Continuous Action Recognition Based on Sequence Alignment</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Vision 112(1), 90-114, 2015</journal-ref><doi>10.1007/s11263-014-0758-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous action recognition is more challenging than isolated recognition
because classification and segmentation must be simultaneously carried out. We
build on the well known dynamic time warping (DTW) framework and devise a novel
visual alignment technique, namely dynamic frame warping (DFW), which performs
isolated recognition based on per-frame representation of videos, and on
aligning a test sequence with a model sequence. Moreover, we propose two
extensions which enable to perform recognition concomitant with segmentation,
namely one-pass DFW and two-pass DFW. These two methods have their roots in the
domain of continuous recognition of speech and, to the best of our knowledge,
their extension to continuous visual action recognition has been overlooked. We
test and illustrate the proposed techniques with a recently released dataset
(RAVEL) and with two public-domain datasets widely used in action recognition
(Hollywood-1 and Hollywood-2). We also compare the performances of the proposed
isolated and continuous recognition algorithms with several recently published
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0289</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0289</id><created>2014-06-02</created><authors><author><keyname>Sarti</keyname><forenames>Alessandro</forenames></author><author><keyname>Citti</keyname><forenames>Giovanna</forenames></author></authors><title>The constitution of visual perceptual units in the functional
  architecture of V1</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scope of this paper is to consider a mean field neural model which takes into
account the functional neurogeometry of the visual cortex modelled as a group
of rotations and translations. The model generalizes well known results of
Bressloff and Cowan which, in absence of input, accounts for hallucination
patterns. The main result of our study consists in showing that in presence of
a visual input, the eigenmodes of the linearized operator which become stable
represent perceptual units present in the image. The result is strictly related
to dimensionality reduction and clustering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0292</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0292</id><created>2014-06-02</created><authors><author><keyname>Hupel</keyname><forenames>Lars</forenames></author></authors><title>Interactive Simplifier Tracing and Debugging in Isabelle</title><categories>cs.MS cs.LO</categories><comments>Conferences on Intelligent Computer Mathematics, 2014</comments><doi>10.1007/978-3-319-08434-3_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Isabelle proof assistant comes equipped with a very powerful tactic for
term simplification. While tremendously useful, the results of simplifying a
term do not always match the user's expectation: sometimes, the resulting term
is not in the form the user expected, or the simplifier fails to apply a rule.
We describe a new, interactive tracing facility which offers insight into the
hierarchical structure of the simplification with user-defined filtering,
memoization and search. The new simplifier trace is integrated into the
Isabelle/jEdit Prover IDE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0295</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0295</id><created>2014-06-02</created><authors><author><keyname>Pentiuc</keyname><forenames>Stefan Gheorghe</forenames></author><author><keyname>Giza</keyname><forenames>Felicia</forenames></author><author><keyname>Schipor</keyname><forenames>Ovidiu Andrei</forenames></author></authors><title>Mobile Agents for Distance Evaluation Procedures</title><categories>cs.CY</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of Internet has led to new approaches for distance education. The
main mechanisms involved in this process are distance learning and distance
evaluation. Distance learning have multiple forms, from browsing and finding
information when needed or collecting information onto ready-to-use packages,
to interactive content where the learner is able to affect and control the
content in some way. From the teacher's point of view, evaluation of learning
aims to determine whether the students have achieved the goals set for a
particular topic or course. Typically this is tested somehow and based on the
test results, evaluation is performed. This kind of method is always restricted
to the test, however. Many times the students may actually learn something
completely different from those that are tested. Furthermore, testing does not
necessarily take the students' individual needs into account. However, when
properly designed, the testing method is a practical and even fairly reliable
way of evaluating learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0296</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0296</id><created>2014-06-02</created><authors><author><keyname>Giza</keyname><forenames>Felicia Florentina</forenames></author><author><keyname>Turcu</keyname><forenames>Cristina Elena</forenames></author><author><keyname>Schipor</keyname><forenames>Ovidiu Andrei</forenames></author></authors><title>Using Mobile Agents for Information Retrival in B2B Systems</title><categories>cs.IR</categories><comments>6 pages, 2 figures, in Romanian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an architecture of an information retrieval system that
use the advantages offered by mobile agents to collect information from
different sources and bring the result to the calling user. Mobile agent
technology will be used for determine the traceability of a product and also
for searching information about a specific entity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0298</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0298</id><created>2014-06-02</created><authors><author><keyname>Alam</keyname><forenames>Sahabul</forenames></author><author><keyname>De</keyname><forenames>Debashis</forenames></author></authors><title>Analysis of Security Threats in Wireless Sensor Network</title><categories>cs.NI cs.CR</categories><comments>12 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:0712.4169 by other authors</comments><doi>10.5121/ijwmn.2014.6204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Network(WSN) is an emerging technology and explored field of
researchers worldwide in the past few years, so does the need for effective
security mechanisms. The sensing technology combined with processing power and
wireless communication makes it lucrative for being exploited in abundance in
future. The inclusion of wireless communication technology also incurs various
types of security threats due to unattended installation of sensor nodes as
sensor networks may interact with sensitive data and /or operate in hostile
unattended environments. These security concerns be addressed from the
beginning of the system design. The intent of this paper is to investigate the
security related issues in wireless sensor networks. In this paper we have
explored general security threats in wireless sensor network with extensive
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0303</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0303</id><created>2014-06-02</created><updated>2014-07-13</updated><authors><author><keyname>Echenim</keyname><forenames>Mnacho</forenames></author><author><keyname>Peltier</keyname><forenames>Nicolas</forenames></author></authors><title>A Superposition Calculus for Abductive Reasoning</title><categories>cs.LO cs.AI</categories><msc-class>68T27, 03B35, 03B10</msc-class><acm-class>I.2.3; F.3.1; F.4.1; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a modification of the superposition calculus that is meant to
generate consequences of sets of first-order axioms. This approach is proven to
be sound and deductive-complete in the presence of redundancy elimination
rules, provided the considered consequences are built on a given finite set of
ground terms, represented by constant symbols. In contrast to other approaches,
most existing results about the termination of the superposition calculus can
be carried over to our procedure. This ensures in particular that the calculus
is terminating for many theories of interest to the SMT community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0304</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0304</id><created>2014-06-02</created><authors><author><keyname>Schneider</keyname><forenames>Markus</forenames></author><author><keyname>Ramos</keyname><forenames>Fabio</forenames></author></authors><title>Transductive Learning for Multi-Task Copula Processes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of multi-task learning with copula process.
Multivariable prediction in spatial and spatial-temporal processes such as
natural resource estimation and pollution monitoring have been typically
addressed using techniques based on Gaussian processes and co-Kriging. While
the Gaussian prior assumption is convenient from analytical and computational
perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes
are an elegant and flexible solution to handle various non-Gaussian likelihoods
by capturing the dependence structure of random variables with cumulative
distribution functions rather than their marginals. We show how multi-task
learning for copula processes can be used to improve multivariable prediction
for problems where the simple Gaussianity prior assumption does not hold. Then,
we present a transductive approximation for multi-task learning and derive
analytical expressions for the copula process model. The approach is evaluated
and compared to other techniques in one artificial dataset and two publicly
available datasets for natural resource estimation and concrete slump
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0306</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0306</id><created>2014-06-02</created><updated>2015-02-03</updated><authors><author><keyname>Marussig</keyname><forenames>Benjamin</forenames></author><author><keyname>Zechner</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Beer</keyname><forenames>Gernot</forenames></author><author><keyname>Fries</keyname><forenames>Thomas-Peter</forenames></author></authors><title>Fast Isogeometric Boundary Element Method based on Independent Field
  Approximation</title><categories>cs.NA math.NA</categories><comments>32 pages, 27 figures</comments><journal-ref>Computer Methods in Applied Mechanics and Engineering, Volume 284,
  2015, Pages 458-488</journal-ref><doi>10.1016/j.cma.2014.09.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An isogeometric boundary element method for problems in elasticity is
presented, which is based on an independent approximation for the geometry,
traction and displacement field. This enables a flexible choice of refinement
strategies, permits an efficient evaluation of geometry related information, a
mixed collocation scheme which deals with discontinuous tractions along
non-smooth boundaries and a significant reduction of the right hand side of the
system of equations for common boundary conditions. All these benefits are
achieved without any loss of accuracy compared to conventional isogeometric
formulations. The system matrices are approximated by means of hierarchical
matrices to reduce the computational complexity for large scale analysis. For
the required geometrical bisection of the domain, a strategy for the evaluation
of bounding boxes containing the supports of NURBS basis functions is
presented. The versatility and accuracy of the proposed methodology is
demonstrated by convergence studies showing optimal rates and real world
examples in two and three dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0309</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0309</id><created>2014-06-02</created><authors><author><keyname>Kachris</keyname><forenames>Christoforos</forenames></author><author><keyname>Sirakoulis</keyname><forenames>Georgios</forenames></author><author><keyname>Soudris</keyname><forenames>Dimitrios</forenames></author></authors><title>Network Function Virtualization based on FPGAs:A Framework for
  all-Programmable network devices</title><categories>cs.NI cs.AR</categories><comments>Network function virtualizations, FPGA, dynamic reconfiguration</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Function Virtualization (NFV) refers to the use of commodity hardware
resources as the basic platform to perform specialized network functions as
opposed to specialized hardware devices. Currently, NFV is mainly implemented
based on general purpose processors, or general purpose network processors. In
this paper we propose the use of FPGAs as an ideal platform for NFV that can be
used to provide both the flexibility of virtualizations and the high
performance of the specialized hardware. We present the early attempts of using
FPGAs dynamic reconfiguration in network processing applications to provide
flexible network functions and we present the opportunities for an FPGA-based
NFV platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0312</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0312</id><created>2014-06-02</created><authors><author><keyname>Murray</keyname><forenames>Naila</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author></authors><title>Generalized Max Pooling</title><categories>cs.CV</categories><comments>(to appear) CVPR 2014 - IEEE Conference on Computer Vision &amp; Pattern
  Recognition (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art patch-based image representations involve a pooling
operation that aggregates statistics computed from local descriptors. Standard
pooling operations include sum- and max-pooling. Sum-pooling lacks
discriminability because the resulting representation is strongly influenced by
frequent yet often uninformative descriptors, but only weakly influenced by
rare yet potentially highly-informative ones. Max-pooling equalizes the
influence of frequent and rare descriptors but is only applicable to
representations that rely on count statistics, such as the bag-of-visual-words
(BOV) and its soft- and sparse-coding extensions. We propose a novel pooling
mechanism that achieves the same effect as max-pooling but is applicable beyond
the BOV and especially to the state-of-the-art Fisher Vector -- hence the name
Generalized Max Pooling (GMP). It involves equalizing the similarity between
each patch and the pooled representation, which is shown to be equivalent to
re-weighting the per-patch statistics. We show on five public image
classification benchmarks that the proposed GMP can lead to significant
performance gains with respect to heuristic alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0333</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0333</id><created>2014-06-02</created><authors><author><keyname>Spreer</keyname><forenames>Jonathan</forenames><affiliation>Organisers</affiliation></author><author><keyname>Wagner</keyname><forenames>Uli</forenames><affiliation>Organisers</affiliation></author><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Murai</keyname><forenames>Satoshi</forenames></author><author><keyname>Sedgwick</keyname><forenames>Eric</forenames></author><author><keyname>Segerman</keyname><forenames>Henry</forenames></author></authors><title>Collection of abstracts of the Workshop on Triangulations in Geometry
  and Topology at CG Week 2014 in Kyoto</title><categories>cs.CG cs.CC math.CO math.GT</categories><comments>4 x 6 page extended abstracts. The workshop was held as part of
  CG-Week 2014 at Kyoto University. June 10th, 2014</comments><msc-class>57M50, 57N35, 57Q35, 57N10, 57Q15, 68Q17, 68U05, 52B05,</msc-class><acm-class>F.1.3; F.4.1; G.2.1; G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This workshop about triangulations of manifolds in computational geometry and
topology was held at the 2014 CG-Week in Kyoto, Japan.
  It focussed on computational and combinatorial questions regarding
triangulations, with the goal of bringing together researchers working on
various aspects of triangulations and of fostering a closer collaboration
within the computational geometry and topology community.
  Triangulations are highly suitable for computations due to their clear
combinatorial structure. As a consequence, they have been successfully employed
in discrete algorithms to solve purely theoretical problems in a broad variety
of mathematical research areas (knot theory, polytope theory, 2- and 3-manifold
topology, geometry, and others). However, due to the large variety of
applications, requirements vary from field to field and thus different types of
triangulations, different tools, and different frameworks are used in different
areas of research. This is why today closely related research areas are
sometimes largely disjoint leaving potential reciprocal benefits unused.
  To address these potentials a workshop on Triangulations was held at
Oberwolfach Research Institute in 2012. Since then many new collaborations
between researchers of different mathematical communities have been
established. Regarding the computational geometry community, the theory of
manifolds continues to contribute to advances in more applied areas of the
field. Many researchers are interested in fundamental mathematical research
about triangulations and thus will benefit from a broad set of knowledge about
different research areas using different techniques.
  We hope that this workshop brought together researchers from many different
fields of computational geometry to have fruitful discussions which will lead
to new interdisciplinary collaborations and solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0342</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0342</id><created>2014-06-02</created><authors><author><keyname>Aono</keyname><forenames>Yoshinori</forenames></author></authors><title>A faster method for computing Gama-Nguyen-Regev's extreme pruning
  coefficients</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers Gama-Nguyen-Regev's strategy [GNR10] for optimizing
pruning coefficients for lattice vector enumeration. We give a table of
optimized coefficients and proposes a faster method for computing
near-optimized coefficients for any parameters by interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0349</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0349</id><created>2014-06-02</created><authors><author><keyname>Tan</keyname><forenames>Tony</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author></authors><title>Undecidability of satisfiability in the algebra of finite binary
  relations with union, composition, and difference</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider expressions built up from binary relation names using the
operators union, composition, and set difference. We show that it is
undecidable to test whether a given such expression $e$ is finitely
satisfiable, i.e., whether there exist finite binary relations that can be
substituted for the relation names so that $e$ evaluates to a nonempty result.
This result already holds in restriction to expressions that mention just a
single relation name, and where the difference operator can be nested at most
once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0370</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0370</id><created>2014-06-02</created><authors><author><keyname>Diewald</keyname><forenames>Stefan</forenames></author><author><keyname>M&#xf6;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Roalter</keyname><forenames>Luis</forenames></author><author><keyname>Kranz</keyname><forenames>Matthias</forenames></author></authors><title>Simulation and Virtual Prototyping of Tangible User Interfaces</title><categories>cs.HC</categories><comments>8 pages, 8 images</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prototyping is an important part in research and development of tangible user
interfaces (TUIs). On the way from the idea to a working prototype, new
hardware prototypes usually have to be crafted repeatedly in numerous
iterations. This brings us to think about virtual prototypes that exhibit the
same functionality as a real TUI, but reduce the amount of time and resources
that have to be spent.
  Building upon existing open-source software - the middleware Robot Operating
System (ROS) and the 3D simulator Gazebo - we have created a toolkit that can
be used for developing and testing fully functional implementations of a
tangible user interface as a virtual device. The entire interaction between the
TUI and other hardware and software components is controlled by the middleware,
while the human interaction with the TUI can be explored using the 3D simulator
and 3D input/output technologies. We argue that by simulating parts of the
hardware-software co-design process, the overall development effort can be
reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0373</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0373</id><created>2014-06-02</created><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Koshiba</keyname><forenames>Takeshi</forenames></author><author><keyname>Watanabe</keyname><forenames>Osamu</forenames></author><author><keyname>Yamamoto</keyname><forenames>Masaki</forenames></author></authors><title>Linear Programming Relaxations for Goldreich's Generators over
  Non-Binary Alphabets</title><categories>cs.CR cs.CC cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goldreich suggested candidates of one-way functions and pseudorandom
generators included in $\mathsf{NC}^0$. It is known that randomly generated
Goldreich's generator using $(r-1)$-wise independent predicates with $n$ input
variables and $m=C n^{r/2}$ output variables is not pseudorandom generator with
high probability for sufficiently large constant $C$. Most of the previous
works assume that the alphabet is binary and use techniques available only for
the binary alphabet. In this paper, we deal with non-binary generalization of
Goldreich's generator and derives the tight threshold for linear programming
relaxation attack using local marginal polytope for randomly generated
Goldreich's generators. We assume that $u(n)\in \omega(1)\cap o(n)$ input
variables are known. In that case, we show that when $r\ge 3$, there is an
exact threshold
$\mu_\mathrm{c}(k,r):=\binom{k}{r}^{-1}\frac{(r-2)^{r-2}}{r(r-1)^{r-1}}$ such
that for $m=\mu\frac{n^{r-1}}{u(n)^{r-2}}$, the LP relaxation can determine
linearly many input variables of Goldreich's generator if
$\mu&gt;\mu_\mathrm{c}(k,r)$, and that the LP relaxation cannot determine
$\frac1{r-2} u(n)$ input variables of Goldreich's generator if
$\mu&lt;\mu_\mathrm{c}(k,r)$. This paper uses characterization of LP solutions by
combinatorial structures called stopping sets on a bipartite graph, which is
related to a simple algorithm called peeling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0375</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0375</id><created>2014-06-02</created><authors><author><keyname>Moreira</keyname><forenames>Waldir</forenames></author><author><keyname>Mendes</keyname><forenames>Paulo</forenames></author><author><keyname>Sargento</keyname><forenames>Susana</forenames></author></authors><title>Assessment Model for Opportunistic Routing</title><categories>cs.NI</categories><comments>6 pages, 5 figures, in Portuguese</comments><journal-ref>IEEE Latin America Transactions, Vol 10 Issue 3 April 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an assessment model, based on a new taxonomy, which
comprises an evaluation guideline with performance metrics and experimental
setup to aid designers in evaluating solutions through fair comparisons.
Simulation results are provided based on the proposed model considering
Epidemic, PROPHET, Bubble Rap, and Spray and Wait, and showing how they perform
under the same set of metrics and scenario
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0378</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0378</id><created>2014-06-02</created><authors><author><keyname>De</keyname><forenames>Nilanjan</forenames></author><author><keyname>Pal</keyname><forenames>Anita</forenames></author><author><keyname>Nayeem</keyname><forenames>Sk. Md. Abu</forenames></author></authors><title>Connective eccentric index of some graph operations</title><categories>math.CO cs.DM</categories><comments>9 pages, 3 figures</comments><msc-class>Primary: 05C35, Secondary: 05C07, 05C40</msc-class><acm-class>J.2; G.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connective eccentric index of a graph is a topological index involving
degrees and eccentricities of vertices of the graph. In this paper, we have
studied the connective eccentric index for double graph and double cover. Also
we give the connective eccentric index for some graph operations such as joins,
symmetric difference, disjunction and splice of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0379</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0379</id><created>2014-06-02</created><authors><author><keyname>Gou</keyname><forenames>Li</forenames></author><author><keyname>Wei</keyname><forenames>Bo</forenames></author><author><keyname>Sadiq</keyname><forenames>Rehan</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>An improved vulnerability index of complex networks based on fractal
  dimension</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an increasing emphasis on network security, much more attention has been
attracted to the vulnerability of complex networks. The multi-scale evaluation
of vulnerability is widely used since it makes use of combined powers of the
links' betweenness and has an effective evaluation to vulnerability. However,
how to determine the coefficient in existing multi-scale evaluation model to
measure the vulnerability of different networks is still an open issue. In this
paper, an improved model based on the fractal dimension of complex networks is
proposed to obtain a more reasonable evaluation of vulnerability with more
physical significance. Not only the structure and basic physical properties of
networks is characterized, but also the covering ability of networks, which is
related to the vulnerability of the network, is taken into consideration in our
proposed method. The numerical examples and real applications are used to
illustrate the efficiency of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0380</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0380</id><created>2014-06-02</created><authors><author><keyname>Gugg</keyname><forenames>Christoph</forenames></author><author><keyname>Harker</keyname><forenames>Matthew</forenames></author><author><keyname>O'Leary</keyname><forenames>Paul</forenames></author><author><keyname>Rath</keyname><forenames>Gerhard</forenames></author></authors><title>An Algebraic Framework for the Real-Time Solution of Inverse Problems on
  Embedded Systems</title><categories>cs.DM</categories><comments>24 pages, journal article</comments><acm-class>C.3; G.1.7; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a new approach to the real-time solution of inverse
problems on embedded systems. The class of problems addressed corresponds to
ordinary differential equations (ODEs) with generalized linear constraints,
whereby the data from an array of sensors forms the forcing function. The
solution of the equation is formulated as a least squares (LS) problem with
linear constraints. The LS approach makes the method suitable for the explicit
solution of inverse problems where the forcing function is perturbed by noise.
The algebraic computation is partitioned into a initial preparatory step, which
precomputes the matrices required for the run-time computation; and the cyclic
run-time computation, which is repeated with each acquisition of sensor data.
The cyclic computation consists of a single matrix-vector multiplication, in
this manner computation complexity is known a-priori, fulfilling the definition
of a real-time computation. Numerical testing of the new method is presented on
perturbed as well as unperturbed problems; the results are compared with known
analytic solutions and solutions acquired from state-of-the-art implicit
solvers. The solution is implemented with model based design and uses only
fundamental linear algebra; consequently, this approach supports automatic code
generation for deployment on embedded systems. The targeting concept was tested
via software- and processor-in-the-loop verification on two systems with
different processor architectures. Finally, the method was tested on a
laboratory prototype with real measurement data for the monitoring of flexible
structures. The problem solved is: the real-time overconstrained reconstruction
of a curve from measured gradients. Such systems are commonly encountered in
the monitoring of structures and/or ground subsidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0403</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0403</id><created>2014-05-30</created><updated>2014-06-03</updated><authors><author><keyname>Pallister</keyname><forenames>James</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Hollis</keyname><forenames>Simon</forenames></author></authors><title>Optimizing the flash-RAM energy trade-off in deeply embedded systems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deeply embedded systems often have the tightest constraints on energy
consumption, requiring that they consume tiny amounts of current and run on
batteries for years. However, they typically execute code directly from flash,
instead of the more energy efficient RAM. We implement a novel compiler
optimization that exploits the relative efficiency of RAM by statically moving
carefully selected basic blocks from flash to RAM. Our technique uses integer
linear programming, with an energy cost model to select a good set of basic
blocks to place into RAM, without impacting stack or data storage.
  We evaluate our optimization on a common ARM microcontroller and succeed in
reducing the average power consumption by up to 41% and reducing energy
consumption by up to 22%, while increasing execution time. A case study is
presented, where an application executes code then sleeps for a period of time.
For this example we show that our optimization could allow the application to
run on battery for up to 32% longer. We also show that for this scenario the
total application energy can be reduced, even if the optimization increases the
execution time of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0416</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0416</id><created>2014-06-02</created><authors><author><keyname>Johnson</keyname><forenames>Anya Elaine</forenames></author><author><keyname>Strauss</keyname><forenames>Eli</forenames></author><author><keyname>Pickett</keyname><forenames>Rodney</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author><author><keyname>Dworkin</keyname><forenames>Ian</forenames></author><author><keyname>Goldsby</keyname><forenames>Heather J.</forenames></author></authors><title>More Bang For Your Buck: Quorum-Sensing Capabilities Improve the
  Efficacy of Suicidal Altruism</title><categories>cs.NE cs.CE q-bio.PE</categories><comments>8 pages, 8 figures, ALIFE '14 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the context of evolution, an altruistic act that benefits the
receiving individual at the expense of the acting individual is a puzzling
phenomenon. An extreme form of altruism can be found in colicinogenic E. coli.
These suicidal altruists explode, releasing colicins that kill unrelated
individuals, which are not colicin resistant. By committing suicide, the
altruist makes it more likely that its kin will have less competition. The
benefits of this strategy rely on the number of competitors and kin nearby. If
the organism explodes at an inopportune time, the suicidal act may not harm any
competitors. Communication could enable organisms to act altruistically when
environmental conditions suggest that that strategy would be most beneficial.
Quorum sensing is a form of communication in which bacteria produce a protein
and gauge the amount of that protein around them. Quorum sensing is one means
by which bacteria sense the biotic factors around them and determine when to
produce products, such as antibiotics, that influence competition. Suicidal
altruists could use quorum sensing to determine when exploding is most
beneficial, but it is challenging to study the selective forces at work in
microbes. To address these challenges, we use digital evolution (a form of
experimental evolution that uses self-replicating computer programs as
organisms) to investigate the effects of enabling altruistic organisms to
communicate via quorum sensing. We found that quorum-sensing altruists killed a
greater number of competitors per explosion, winning competitions against
non-communicative altruists. These findings indicate that quorum sensing could
increase the beneficial effect of altruism and the suite of conditions under
which it will evolve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0426</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0426</id><created>2014-06-02</created><authors><author><keyname>Li</keyname><forenames>Heng</forenames></author></authors><title>Fast construction of FM-index for long sequence reads</title><categories>q-bio.GN cs.DS</categories><comments>2 pages</comments><doi>10.1093/bioinformatics/btu541</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summary: We present a new method to incrementally construct the FM-index for
both short and long sequence reads, up to the size of a genome. It is the first
algorithm that can build the index while implicitly sorting the sequences in
the reverse (complement) lexicographical order without a separate sorting step.
The implementation is among the fastest for indexing short reads and the only
one that practically works for reads of averaged kilobases in length.
  Availability and implementation: https://github.com/lh3/ropebwt2
  Contact: hengli@broadinstitute.org
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0435</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0435</id><created>2014-06-02</created><authors><author><keyname>Kim</keyname><forenames>Jun-Sung</forenames></author><author><keyname>Whang</keyname><forenames>Kyu-Young</forenames></author><author><keyname>Kwon</keyname><forenames>Hyuk-Yoon</forenames></author><author><keyname>Song</keyname><forenames>Il-Yeol</forenames></author></authors><title>Odysseus/DFS: Integration of DBMS and Distributed File System for
  Transaction Processing of Big Data</title><categories>cs.DB</categories><comments>35 pages, 13 figures</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relational DBMS (RDBMS) has been widely used since it supports various
high-level functionalities such as SQL, schemas, indexes, and transactions that
do not exist in the O/S file system. But, a recent advent of big data
technology facilitates development of new systems that sacrifice the DBMS
functionality in order to efficiently manage large-scale data. Those so-called
NoSQL systems use a distributed file system, which support scalability and
reliability. They support scalability of the system by storing data into a
large number of low-cost commodity hardware and support reliability by storing
the data in replica. However, they have a drawback that they do not adequately
support high-level DBMS functionality. In this paper, we propose an
architecture of a DBMS that uses the DFS as storage. With this novel
architecture, the DBMS is capable of supporting scalability and reliability of
the DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a
virtually unlimited storage space provided by the DFS, rendering it to be
suitable for big data analytics. As part of the architecture of the DBMS, we
propose the notion of the meta DFS file, which allows the DBMS to use the DFS
as the storage, and an efficient transaction management method including
recovery and concurrency control. We implement this architecture in
Odysseus/DFS, an integration of the Odysseus relational DBMS, that has been
being developed at KAIST for over 24 years, with the DFS. Our experiments on
transaction processing show that, due to the high-level functionality of
Odysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL
system. We also show that, compared with an RDBMS with local storage, the
performance of Odysseus/DFS is comparable or marginally degraded, showing that
the overhead of Odysseus/DFS for supporting scalability by using the DFS as the
storage is not significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0440</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0440</id><created>2014-06-02</created><updated>2014-10-08</updated><authors><author><keyname>Kreutz</keyname><forenames>Diego</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author><author><keyname>Verissimo</keyname><forenames>Paulo</forenames></author><author><keyname>Rothenberg</keyname><forenames>Christian Esteve</forenames></author><author><keyname>Azodolmolky</keyname><forenames>Siamak</forenames></author><author><keyname>Uhlig</keyname><forenames>Steve</forenames></author></authors><title>Software-Defined Networking: A Comprehensive Survey</title><categories>cs.NI</categories><comments>Version 2.01: 61 pages, 11 figures, 17 tables, 579 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Networking (SDN) is an emerging paradigm that promises to
change this state of affairs, by breaking vertical integration, separating the
network's control logic from the underlying routers and switches, promoting
(logical) centralization of network control, and introducing the ability to
program the network. The separation of concerns introduced between the
definition of network policies, their implementation in switching hardware, and
the forwarding of traffic, is key to the desired flexibility: by breaking the
network control problem into tractable pieces, SDN makes it easier to create
and introduce new abstractions in networking, simplifying network management
and facilitating network evolution. In this paper we present a comprehensive
survey on SDN. We start by introducing the motivation for SDN, explain its main
concepts and how it differs from traditional networking, its roots, and the
standardization activities regarding this novel paradigm. Next, we present the
key building blocks of an SDN infrastructure using a bottom-up, layered
approach. We provide an in-depth analysis of the hardware infrastructure,
southbound and northbound APIs, network virtualization layers, network
operating systems (SDN controllers), network programming languages, and network
applications. We also look at cross-layer problems such as debugging and
troubleshooting. In an effort to anticipate the future evolution of this new
paradigm, we discuss the main ongoing research efforts and challenges of SDN.
In particular, we address the design of switches and control platforms -- with
a focus on aspects such as resiliency, scalability, performance, security and
dependability -- as well as new opportunities for carrier transport networks
and cloud providers. Last but not least, we analyze the position of SDN as a
key enabler of a software-defined environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0455</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0455</id><created>2014-06-02</created><updated>2014-06-13</updated><authors><author><keyname>Chen</keyname><forenames>Cheng</forenames></author><author><keyname>Zheng</keyname><forenames>Lan</forenames></author><author><keyname>Srinivasan</keyname><forenames>Venkatesh</forenames></author><author><keyname>Thomo</keyname><forenames>Alex</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Sukow</keyname><forenames>Anthony</forenames></author></authors><title>Buyer to Seller Recommendation under Constraints</title><categories>cs.SI cs.GT q-fin.GN q-fin.ST</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of recommender systems are designed to recommend items (such as
movies and products) to users. We focus on the problem of recommending buyers
to sellers which comes with new challenges: (1) constraints on the number of
recommendations buyers are part of before they become overwhelmed, (2)
constraints on the number of recommendations sellers receive within their
budget, and (3) constraints on the set of buyers that sellers want to receive
(e.g., no more than two people from the same household). We propose the
following critical problems of recommending buyers to sellers: Constrained
Recommendation (C-REC) capturing the first two challenges, and Conflict-Aware
Constrained Recommendation (CAC-REC) capturing all three challenges at the same
time. We show that C-REC can be modeled using linear programming and can be
efficiently solved using modern solvers. On the other hand, we show that
CAC-REC is NP-hard. We propose two approximate algorithms to solve CAC-REC and
show that they achieve close to optimal solutions via comprehensive experiments
using real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0482</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0482</id><created>2014-06-02</created><updated>2014-06-12</updated><authors><author><keyname>Davila</keyname><forenames>Randy</forenames></author><author><keyname>Kenter</keyname><forenames>Franklin</forenames></author></authors><title>Bounds for the Zero-Forcing Number of Graphs with Large Girth</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the zero-forcing number for triangle-free graphs. We improve
upon the trivial bound, $\delta \le Z(G)$ where $\delta$ is the minimum degree,
in the triangle-free case. In particular, we show that $2 \delta - 2 \le Z(G)$
for graphs with girth of at least 5, and this can be further improved when $G$
has a small cut set. Using these results, we are able to prove the Graph
Complement Conjecture on minimum rank for a large class of graphs. Lastly, we
make a conjecture that the lower bound for $Z(G)$ increases as a function of
the girth, $g$, and $\delta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0486</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0486</id><created>2014-06-02</created><updated>2014-06-19</updated><authors><author><keyname>Lanctot</keyname><forenames>Marc</forenames></author><author><keyname>Winands</keyname><forenames>Mark H. M.</forenames></author><author><keyname>Pepels</keyname><forenames>Tom</forenames></author><author><keyname>Sturtevant</keyname><forenames>Nathan R.</forenames></author></authors><title>Monte Carlo Tree Search with Heuristic Evaluations using Implicit
  Minimax Backups</title><categories>cs.AI</categories><comments>24 pages, 7 figures, 9 tables, expanded version of paper presented at
  IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo Tree Search (MCTS) has improved the performance of game engines
in domains such as Go, Hex, and general game playing. MCTS has been shown to
outperform classic alpha-beta search in games where good heuristic evaluations
are difficult to obtain. In recent years, combining ideas from traditional
minimax search in MCTS has been shown to be advantageous in some domains, such
as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new
way to use heuristic evaluations to guide the MCTS search by storing the two
sources of information, estimated win rates and heuristic evaluations,
separately. Rather than using the heuristic evaluations to replace the
playouts, our technique backs them up implicitly during the MCTS simulations.
These minimax values are then used to guide future simulations. We show that
using implicit minimax backups leads to stronger play performance in Kalah,
Breakthrough, and Lines of Action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0492</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0492</id><created>2014-06-02</created><updated>2015-09-08</updated><authors><author><keyname>Hougardy</keyname><forenames>Stefan</forenames></author><author><keyname>Silvanus</keyname><forenames>Jannik</forenames></author><author><keyname>Vygen</keyname><forenames>Jens</forenames></author></authors><title>Dijkstra meets Steiner: a fast exact goal-oriented Steiner tree
  algorithm</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new exact algorithm for the Steiner tree problem in
edge-weighted graphs. Our algorithm improves the classical dynamic programming
approach by Dreyfus and Wagner. We achieve a significantly better practical
performance via pruning and future costs, a generalization of a well-known
concept to speed up shortest path computations. Our algorithm matches the best
known worst-case run time and has a fast, often superior, practical
performance: on some large instances originating from VLSI design, previous
best run times are improved upon by orders of magnitudes. We are also able to
solve larger instances of the $d$-dimensional rectilinear Steiner tree problem
for $d \in \{3, 4, 5\}$, whose Hanan grids contain up to several millions of
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0495</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0495</id><created>2014-05-29</created><authors><author><keyname>Pentiuc</keyname><forenames>Stefan-Gheorghe</forenames></author><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Danubianu</keyname><forenames>Mirela</forenames></author><author><keyname>Schipor</keyname><forenames>Doina-Maria</forenames></author></authors><title>Automatic Recognition of Dyslalia Affecting Pre-Scholars</title><categories>cs.CY</categories><comments>9 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:0912.3969</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes the recognition part of a system that will be used for
personalized therapy of dyslalia affecting pre scholars. Dyslalia is a speech
disorder that affect pronunciation of one ore many sounds. The full system
targets interdisciplinary research (computer science, psychology, electronics)
- having as main objective the development of methods, models, algorithms,
System on Chip architectures with regards to the elaboration and implementation
of a complete system addressing the therapy of dyslalia affecting pre scholars,
in a personalized and user centered manner. The system addresses the number of
10% of children with age between 4 and 7 that, according to the statistics,
present different variations of speech impairments. Although these impairments
do not create major difficulties concerning common communication, it has been
noticed that problems are likely to appear affecting negatively the child's
personality as well as his social environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0496</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0496</id><created>2014-06-02</created><updated>2015-01-21</updated><authors><author><keyname>Musmeci</keyname><forenames>Nicolo</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author><author><keyname>Di Matteo</keyname><forenames>Tiziana</forenames></author></authors><title>Relation between Financial Market Structure and the Real Economy:
  Comparison between Clustering Methods</title><categories>q-fin.ST cs.CE</categories><comments>31 pages, 17 figures</comments><doi>10.1371/journal.pone.0116201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We quantify the amount of information filtered by different hierarchical
clustering methods on correlations between stock returns comparing it with the
underlying industrial activity structure. Specifically, we apply, for the first
time to financial data, a novel hierarchical clustering approach, the Directed
Bubble Hierarchical Tree and we compare it with other methods including the
Linkage and k-medoids. In particular, by taking the industrial sector
classification of stocks as a benchmark partition, we evaluate how the
different methods retrieve this classification. The results show that the
Directed Bubble Hierarchical Tree can outperform other methods, being able to
retrieve more information with fewer clusters. Moreover, we show that the
economic information is hidden at different levels of the hierarchical
structures depending on the clustering method. The dynamical analysis on a
rolling window also reveals that the different methods show different degrees
of sensitivity to events affecting financial markets, like crises. These
results can be of interest for all the applications of clustering methods to
portfolio optimization and risk hedging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0516</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0516</id><created>2014-06-02</created><updated>2015-06-11</updated><authors><author><keyname>Valera</keyname><forenames>Isabel</forenames></author><author><keyname>Gomez-Rodriguez</keyname><forenames>Manuel</forenames></author></authors><title>Modeling Adoption and Usage of Competing Products</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence and wide-spread use of online social networks has led to a
dramatic increase on the availability of social activity data. Importantly,
this data can be exploited to investigate, at a microscopic level, some of the
problems that have captured the attention of economists, marketers and
sociologists for decades, such as, e.g., product adoption, usage and
competition.
  In this paper, we propose a continuous-time probabilistic model, based on
temporal point processes, for the adoption and frequency of use of competing
products, where the frequency of use of one product can be modulated by those
of others. This model allows us to efficiently simulate the adoption and
recurrent usages of competing products, and generate traces in which we can
easily recognize the effect of social influence, recency and competition. We
then develop an inference method to efficiently fit the model parameters by
solving a convex program. The problem decouples into a collection of smaller
subproblems, thus scaling easily to networks with hundred of thousands of
nodes. We validate our model over synthetic and real diffusion data gathered
from Twitter, and show that the proposed model does not only provides a good
fit to the data and more accurate predictions than alternatives but also
provides interpretable model parameters, which allow us to gain insights into
some of the factors driving product adoption and frequency of use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0532</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0532</id><created>2014-06-02</created><authors><author><keyname>da Silva</keyname><forenames>Gon&#xe7;alo Amaral</forenames></author></authors><title>Multimodal vs. Unimodal Physiological Control in Videogames for Enhanced
  Realism and Depth</title><categories>cs.HC</categories><comments>MSc Thesis, English-written, 89 pages, University of Porto</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (arXiv abridged abstract) In the last two decades, videogames have evolved in
a nearly explosive way from the pixelated graphics to today's near-realistic 3D
environments. The interaction devices traditionally used in videogames have not
evolved with the same intensity, but recent HCI studies have explored
biofeedback interaction - the explicit manipulation of a person's physiological
data as input to a system - as an alternative to them. Traditional biofeedback
prototypes apply 1 sensor to each game mechanic (unimodality).
  In this dissertation, we introduce the combination of 2 physiological sensors
simultaneously per game mechanic (multimodality) and present a First-Person
Shooter game comprised of 8 game mechanics with three interaction flavours (no
biofeedback/vanilla, unimodal and multimodal). An empirical study with 32
regular players was employed to explore and study differences between the three
interaction types and where they can be best employed.
  Players compared the three games in terms of Fun, Ease of Use, Originality,
Playability and Favourite Condition. For the sake of completeness, other
evaluation methods were used as well: IMI Questionnaire, keywords association
and open-ended commentaries. The vanilla version was considered easier to use,
but both biofeedback versions were considered the most fun. Both versions were
praised differently: the unimodal version for its simplicity of use, and the
multimodal for its realism, activation safety of game mechanics and depth added
to the game. Our conclusion is that multimodal biofeedback can have a relevant
impact in terms of added depth, depending on the way it is used inside the
game. On a boundary case, it can be used to increase the feeling of empowerment
on the player when using certain abilities, or to intentionally make in-game
actions more difficult by demanding more physical effort from the player.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0533</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0533</id><created>2014-06-02</created><authors><author><keyname>Richert</keyname><forenames>Dean</forenames></author><author><keyname>Cortes</keyname><forenames>Jorge</forenames></author></authors><title>Distributed bargaining in dyadic-exchange networks</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers dyadic-exchange networks in which individual agents
autonomously form coalitions of size two and agree on how to split a
transferable utility. Valid results for this game include stable (if agents
have no unilateral incentive to deviate), balanced (if matched agents obtain
similar benefits from collaborating), or Nash (both stable and balanced)
outcomes. We design provably-correct continuous-time algorithms to find each of
these classes of outcomes in a distributed way. Our algorithmic design to find
Nash bargaining solutions builds on the other two algorithms by having the
dynamics for finding stable outcomes feeding into the one for finding balanced
ones. Our technical approach to establish convergence and robustness combines
notions and tools from optimization, graph theory, nonsmooth analysis, and
Lyapunov stability theory and provides a useful framework for further
extensions. We illustrate our results in a wireless communication scenario
where single-antenna devices have the possibility of working as 2-antenna
virtual devices to improve channel capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0554</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0554</id><created>2014-06-02</created><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author><author><keyname>Todorov</keyname><forenames>Emanuel</forenames></author></authors><title>Universal Convexification via Risk-Aversion</title><categories>cs.SY cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for convexifying a fairly general class of
optimization problems. Under additional assumptions, we analyze the
suboptimality of the solution to the convexified problem relative to the
original nonconvex problem and prove additive approximation guarantees. We then
develop algorithms based on stochastic gradient methods to solve the resulting
optimization problems and show bounds on convergence rates. %We show a simple
application of this framework to supervised learning, where one can perform
integration explicitly and can use standard (non-stochastic) optimization
algorithms with better convergence guarantees. We then extend this framework to
apply to a general class of discrete-time dynamical systems. In this context,
our convexification approach falls under the well-studied paradigm of
risk-sensitive Markov Decision Processes. We derive the first known model-based
and model-free policy gradient optimization algorithms with guaranteed
convergence to the optimal solution. Finally, we present numerical results
validating our formulation in different applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0574</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0574</id><created>2014-06-03</created><authors><author><keyname>Lee</keyname><forenames>Kyumin</forenames></author><author><keyname>Webb</keyname><forenames>Steve</forenames></author><author><keyname>Ge</keyname><forenames>Hancheng</forenames></author></authors><title>The Dark Side of Micro-Task Marketplaces: Characterizing Fiverr and
  Automatically Detecting Crowdturfing</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As human computation on crowdsourcing systems has become popular and powerful
for performing tasks, malicious users have started misusing these systems by
posting malicious tasks, propagating manipulated contents, and targeting
popular web services such as online social networks and search engines.
Recently, these malicious users moved to Fiverr, a fast-growing micro-task
marketplace, where workers can post crowdturfing tasks (i.e., astroturfing
campaigns run by crowd workers) and malicious customers can purchase those
tasks for only $5. In this paper, we present a comprehensive analysis of
Fiverr. First, we identify the most popular types of crowdturfing tasks found
in this marketplace and conduct case studies for these crowdturfing tasks.
Then, we build crowdturfing task detection classifiers to filter these tasks
and prevent them from becoming active in the marketplace. Our experimental
results show that the proposed classification approach effectively detects
crowdturfing tasks, achieving 97.35% accuracy. Finally, we analyze the real
world impact of crowdturfing tasks by purchasing active Fiverr tasks and
quantifying their impact on a target site. As part of this analysis, we show
that current security systems inadequately detect crowdsourced manipulation,
which confirms the necessity of our proposed crowdturfing task detection
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0576</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0576</id><created>2014-06-03</created><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Talgam-Cohen</keyname><forenames>Inbal</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>Welfare and Revenue Guarantees for Competitive Bundling Equilibrium</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study equilibria of markets with $m$ heterogeneous indivisible goods and
$n$ consumers with combinatorial preferences. It is well known that a
competitive equilibrium is not guaranteed to exist when valuations are not
gross substitutes. Given the widespread use of bundling in real-life markets,
we study its role as a stabilizing and coordinating device by considering the
notion of \emph{competitive bundling equilibrium}: a competitive equilibrium
over the market induced by partitioning the goods for sale into fixed bundles.
Compared to other equilibrium concepts involving bundles, this notion has the
advantage of simulatneous succinctness ($O(m)$ prices) and market clearance.
  Our first set of results concern welfare guarantees. We show that in markets
where consumers care only about the number of goods they receive (known as
multi-unit or homogeneous markets), even in the presence of complementarities,
there always exists a competitive bundling equilibrium that guarantees a
logarithmic fraction of the optimal welfare, and this guarantee is tight. We
also establish non-trivial welfare guarantees for general markets, two-consumer
markets, and markets where the consumer valuations are additive up to a fixed
budget (budget-additive).
  Our second set of results concern revenue guarantees. Motivated by the fact
that the revenue extracted in a standard competitive equilibrium may be zero
(even with simple unit-demand consumers), we show that for natural subclasses
of gross substitutes valuations, there always exists a competitive bundling
equilibrium that extracts a logarithmic fraction of the optimal welfare, and
this guarantee is tight. The notion of competitive bundling equilibrium can
thus be useful even in markets which possess a standard competitive
equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0582</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0582</id><created>2014-06-03</created><authors><author><keyname>Domagala</keyname><forenames>Lukasz</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Rastello</keyname><forenames>Fabrice</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Ponnuswany</keyname><forenames>Sadayappan</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Van Amstel</keyname><forenames>Duco</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>A Tiling Perspective for Register Optimization</title><categories>cs.PL</categories><proxy>ccsd</proxy><report-no>RR-8541</report-no><journal-ref>N&amp;deg; RR-8541 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Register allocation is a much studied problem. A particularly important
context for optimizing register allocation is within loops, since a significant
fraction of the execution time of programs is often inside loop code. A variety
of algorithms have been proposed in the past for register allocation, but the
complexity of the problem has resulted in a decoupling of several important
aspects, including loop unrolling, register promotion, and instruction
reordering. In this paper, we develop an approach to register allocation and
promotion in a unified optimization framework that simultaneously considers the
impact of loop unrolling and instruction scheduling. This is done via a novel
instruction tiling approach where instructions within a loop are represented
along one dimension and innermost loop iterations along the other dimension. By
exploiting the regularity along the loop dimension, and imposing essential
dependence based constraints on intra-tile execution order, the problem of
optimizing register pressure is cast in a constraint programming formalism.
Experimental results are provided from thousands of innermost loops extracted
from the SPEC benchmarks, demonstrating improvements over the current
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0588</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0588</id><created>2014-06-03</created><updated>2014-06-04</updated><authors><author><keyname>Bu</keyname><forenames>Shasha</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-Jin</forenames></author></authors><title>Image retrieval with hierarchical matching pursuit</title><categories>cs.CV</categories><comments>5 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel representation of images for image retrieval is introduced in this
paper, by using a new type of feature with remarkable discriminative power.
Despite the multi-scale nature of objects, most existing models perform feature
extraction on a fixed scale, which will inevitably degrade the performance of
the whole system. Motivated by this, we introduce a hierarchical sparse coding
architecture for image retrieval to explore multi-scale cues. Sparse codes
extracted on lower layers are transmitted to higher layers recursively. With
this mechanism, cues from different scales are fused. Experiments on the
Holidays dataset show that the proposed method achieves an excellent retrieval
performance with a small code length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0589</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0589</id><created>2014-06-03</created><authors><author><keyname>Gao</keyname><forenames>Fei</forenames></author><author><keyname>Liu</keyname><forenames>Bin</forenames></author><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Wen</keyname><forenames>Qiao-Yan</forenames></author></authors><title>Post-processing of the oblivious key in quantum private queries</title><categories>quant-ph cs.CR</categories><comments>11 pages, 5 figures, 3 tables</comments><msc-class>81P94, 81P45, 94A60</msc-class><journal-ref>Ieee Journal Of Selected Topics In Quantum Electronics, VOL. 21,
  NO. 3, 6600111, 2015</journal-ref><doi>10.1109/JSTQE.2014.2358192</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Quantum private query (QPQ) is a kind of quantum protocols to protect both
users' privacy in their communication. There is an interesting example, that
is, Alice wants to buy one item from Bob's database, which is composed of a
quantity of valuable messages. QPQ protocol is the communication procedure
ensuring that Alice can get only one item from Bob, and at the same time, Bob
cannot know which one was taken by Alice. Owing to its practicability,
quantum-key-distribution-based QPQ has draw much attention in recent years.
However, the post-processing of the key in such protocols, called oblivious
key, remains far from being satisfactorily known. Especially, the error
correction method for such special key is still missing. Here we focus on the
post-processing of the oblivious key, including both dilution and error
correction. On the one hand, we demonstrate that the previous dilution method,
which greatly reduces the communication complexity, will bring Alice the chance
to illegally obtain much additional information about Bob's database.
Simulations show that by very limited queries Alice can obtain the whole
database. On the other hand, we present an effective error-correction method
for the oblivious key, which completes its post-processing and makes such QPQ
more practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0599</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0599</id><created>2014-06-03</created><authors><author><keyname>Chen</keyname><forenames>Zhenghong</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Hierarchical Comprehensive Triangular Decomposition</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of comprehensive triangular decomposition (CTD) was first
introduced by Chen et al. in their CASC'2007 paper and could be viewed as an
analogue of comprehensive Grobner systems for parametric polynomial systems.
The first complete algorithm for computing CTD was also proposed in that paper
and implemented in the RegularChains library in Maple. Following our previous
work on generic regular decomposition for parametric polynomial systems, we
introduce in this paper a so-called hierarchical strategy for computing CTDs.
Roughly speaking, for a given parametric system, the parametric space is
divided into several sub-spaces of different dimensions and we compute CTDs
over those sub-spaces one by one. So, it is possible that, for some benchmarks,
it is difficult to compute CTDs in reasonable time while this strategy can
obtain some &quot;partial&quot; solutions over some parametric sub-spaces. The program
based on this strategy has been tested on a number of benchmarks from the
literature. Experimental results on these benchmarks with comparison to
RegularChains are reported and may be valuable for developing more efficient
triangularization tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0609</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0609</id><created>2014-06-03</created><updated>2015-01-04</updated><authors><author><keyname>Xu</keyname><forenames>Huanle</forenames></author><author><keyname>Lau</keyname><forenames>Wing Cheong</forenames></author></authors><title>Optimization for Speculative Execution of Multiple Jobs in a
  MapReduce-like Cluster</title><categories>cs.DC</categories><comments>This paper has been withdrawn due to the simulation part need to be
  strengthened</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, a computing cluster in a typical data center can easily consist of
hundreds of thousands of commodity servers, making component/ machine failures
the norm rather than exception. A parallel processing job can be delayed
substantially as long as one of its many tasks is being assigned to a failing
machine. To tackle this so-called straggler problem, most parallel processing
frameworks such as MapReduce have adopted various strategies under which the
system may speculatively launch additional copies of the same task if its
progress is abnormally slow or simply because extra idling resource is
available. In this paper, we focus on the design of speculative execution
schemes for a parallel processing cluster under different loading conditions.
For the lightly loaded case, we analyze and propose two optimization-based
schemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing
the job utility and the Straggler Detection Algorithm (SDA) which minimizes the
overall resource consumption of a job. We also derive the workload threshold
under which SCA or SDA should be used for speculative execution. Our simulation
results show both SCA and SDA can reduce the job flowtime by nearly 60%
comparing to the speculative execution strategy of Microsoft Mantri. For the
heavily loaded case, we propose the Enhanced Speculative Execution (ESE)
algorithm which is an extension of the Microsoft Mantri scheme. We show that
the ESE algorithm can beat the Mantri baseline scheme by 18% in terms of job
flowtime while consuming the same amount of resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0623</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0623</id><created>2014-06-03</created><authors><author><keyname>Merc&#xe8;re</keyname><forenames>Guillaume</forenames></author><author><keyname>Ramos</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Prot</keyname><forenames>Olivier</forenames></author></authors><title>Identification of parameterized gray-box state-space systems: from a
  black-box linear time-invariant representation to a structured one: detailed
  derivation of the gradients involved in the cost functions</title><categories>cs.SY</categories><comments>Companion paper of an eponymous article accepted for publication in
  IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating consistent parameters of a structured state-space representation
requires a reliable initialization when the vector of parameters is computed by
using a gradient-based algorithm. In the eponymous companion paper accepted for
publication in IEEE Transactions on Automatic Control, the problem of supplying
a reliable initial vector of parameters is tackled. More precisely, by assuming
that a reliable initial fully-parameterized state-space model of the system is
available, the aforementioned paper addresses the challenging problem of
transforming this initial fully-parameterized model into the structured
state-space parameterization satisfied by the system to be identified. Two
solutions to solve such a parameterization problem are more precisely
introduced in the IEEE TAC paper. First, a solution based on a null-space-based
reformulation of a set of equations arising from the aforementioned similarity
transformation problem is considered. Second, an algorithm dedicated to
non-convex optimization is presented in order to transform the initial
fully-parameterized model into the structured state-space parameterization of
the system to be identified. In this technical report, a specific attention is
paid to the gradient computation required by the optimization algorithms used
in aforementioned to solve the aforementioned problem. These gradient
formulations are indeed necessary to apply the quasi-Newton
Broyden-Fletcher-Goldfarb-Shanno (BFGS) methods used for the null-space based
as well as the least-squares-formulated optimization techniques introduced in
the IEEE TAC paper. For the sake of conciseness, we only focus on the smooth
version of the optimization problem introduced in aforementioned paper.
Interested readers can easily extend the following results by using the chain
rule as well as the sub-gradient computation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0641</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0641</id><created>2014-06-03</created><authors><author><keyname>Prisacariu</keyname><forenames>Cristian</forenames></author></authors><title>Extensions of Configuration Structures</title><categories>cs.DC cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The present paper defines ST-structures (and an extension of these, called
STC-structures). The main purpose is to provide concrete relationships between
highly expressive concurrency models coming from two different schools of
thought: the higher dimensional automata, a \textit{state-based} approach of
Pratt and van Glabbeek; and the configuration structures and (in)pure event
structures, an \textit{event-based} approach of van Glabbeek and Plotkin. In
this respect we make comparative studies of the expressive power of
ST-structures relative to the above models. Moreover, standard notions from
other concurrency models are defined for ST(C)-structures, like steps and
paths, bisimilarities, and action refinement, and related results are given.
These investigations of ST(C)-structures are intended to provide a better
understanding of the \textit{state-event duality} described by Pratt, and also
of the (a)cyclic structures of higher dimensional automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0647</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0647</id><created>2014-06-03</created><updated>2014-09-04</updated><authors><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Pentapods with Mobility 2</title><categories>cs.RO</categories><comments>18 pages, 5 figures</comments><msc-class>53A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a full classification of all pentapods with mobility 2,
where neither all platform anchor points nor all base anchor points are located
on a line. Therefore this paper solves the famous Borel-Bricard problem for
2-dimensional motions beside the excluded case of five collinear points with
spherical trajectories. But even for this special case we present three new
types as a side-result. Based on our study of pentapods, we also give a
complete list of all non-architecturally singular hexapods with 2-dimensional
self-motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0650</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0650</id><created>2014-06-03</created><updated>2015-07-16</updated><authors><author><keyname>Galindo</keyname><forenames>Carlos</forenames></author><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author></authors><title>New Quantum Codes from Evaluation and Matrix-Product Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stabilizer codes obtained via CSS code construction and Steane's enlargement
of subfield-subcodes and matrix-product codes coming from generalized
Reed-Muller, hyperbolic and affine variety codes are studied. Stabilizer codes
with good quantum parameters are supplied, in particular, some binary codes of
lengths 127 and 128 improve the parameters of the codes in
http://www.codetables.de. Moreover, non-binary codes are presented either with
parameters better than or equal to the quantum codes obtained from BCH codes by
La Guardia or with lengths that can not be reached by them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0670</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0670</id><created>2014-06-03</created><updated>2014-07-27</updated><authors><author><keyname>Du</keyname><forenames>Chen Fei</forenames></author><author><keyname>Mousavi</keyname><forenames>Hamoon</forenames></author><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Decision Algorithms for Fibonacci-Automatic Words, with Applications to
  Pattern Avoidance</title><categories>cs.FL cs.DM math.CO</categories><comments>inserted new section 9 on abelian properties</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implement a decision procedure for answering questions about a class of
infinite words that might be called (for lack of a better name)
&quot;Fibonacci-automatic&quot;. This class includes, for example, the famous Fibonacci
word f = 01001010..., the fixed point of the morphism 0 -&gt; 01 and 1 -&gt; 0. We
then recover many results about the Fibonacci word from the literature (and
improve some of them), such as assertions about the occurrences in f of
squares, cubes, palindromes, and so forth. As an application of our method we
prove a new result: there exists an aperiodic infinite binary word avoiding the
pattern x x x^R. This is the first avoidability result concerning a nonuniform
morphism proven purely mechanically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0671</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0671</id><created>2014-06-03</created><updated>2014-10-10</updated><authors><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Antonio-Rodr&#xed;guez</keyname><forenames>Emilio</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Modeling and Efficient Cancellation of Nonlinear Self-Interference in
  MIMO Full-Duplex Transceivers</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures. To be presented in the 2014 International
  Workshop on Emerging Technologies for 5G Wireless Cellular Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the modeling and digital cancellation of
self-interference in in-band full-duplex (FD) transceivers with multiple
transmit and receive antennas. The self-interference modeling and the proposed
nonlinear spatio-temporal digital canceller structure takes into account, by
design, the effects of I/Q modulator imbalances and power amplifier (PA)
nonlinearities with memory, in addition to the multipath self-interference
propagation channels and the analog RF cancellation stage. The proposed
solution is the first cancellation technique in the literature which can handle
such a self-interference scenario. It is shown by comprehensive simulations
with realistic RF component parameters and with two different PA models to
clearly outperform the current state-of-the-art digital self-interference
cancellers, and to clearly extend the usable transmit power range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0673</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0673</id><created>2014-06-03</created><authors><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Govezensky</keyname><forenames>Tzipe</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Barrio</keyname><forenames>Rafael A.</forenames></author></authors><title>Effects of Deception in Social Networks</title><categories>physics.soc-ph cs.CY cs.SI nlin.AO</categories><comments>19 pages, 3 figures; submitted for review</comments><journal-ref>Proc. R. Soc. B 281 (1790), 20141195 (2014)</journal-ref><doi>10.1098/rspb.2014.1195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Honesty plays a crucial role in any situation where organisms exchange
information or resources. Dishonesty can thus be expected to have damaging
effects on social coherence if agents cannot trust the information or goods
they receive. However, a distinction is often drawn between prosocial lies
('white' lies) and antisocial lying (i.e. deception for personal gain), with
the former being considered much less destructive than the latter. We use an
agent-based model to show that antisocial lying causes social networks to
become increasingly fragmented. Antisocial dishonesty thus places strong
constraints on the size and cohesion of social communities, providing a major
hurdle that organisms have to overcome (e.g. by evolving counter-deception
strategies) in order to evolve large, socially cohesive communities. In
contrast, 'white' lies can prove to be beneficial in smoothing the flow of
interactions and facilitating a larger, more integrated network. Our results
demonstrate that these group-level effects can arise as emergent properties of
interactions at the dyadic level. The balance between prosocial and antisocial
lies may set constraints on the structure of social networks, and hence the
shape of society as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0680</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0680</id><created>2014-06-03</created><authors><author><keyname>Liu</keyname><forenames>Ziqiong</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author><author><keyname>Zheng</keyname><forenames>Liang</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Visual Reranking with Improved Image Graph</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an improved reranking method for the Bag-of-Words (BoW)
based image search. Built on [1], a directed image graph robust to outlier
distraction is proposed. In our approach, the relevance among images is encoded
in the image graph, based on which the initial rank list is refined. Moreover,
we show that the rank-level feature fusion can be adopted in this reranking
method as well. Taking advantage of the complementary nature of various
features, the reranking performance is further enhanced. Particularly, we
exploit the reranking method combining the BoW and color information.
Experiments on two benchmark datasets demonstrate that ourmethod yields
significant improvements and the reranking results are competitive to the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0688</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0688</id><created>2014-06-03</created><authors><author><keyname>Mohamed</keyname><forenames>Mostafa Hosni</forenames></author><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>Reduced List-Decoding of Reed--Solomon Codes Using Reliability
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We decode Reed-Solomon codes using soft information provided at the receiver.
The Extended Euclidean Algorithm (EEA) is considered as an initial step to
obtain an intermediate result. The final decoding result is obtained by
interpolating the output of the EEA at the least reliable positions of the
received word. We refer to this decoding method as reduced list-decoding, since
not all received positions are used in the interpolation as in other
list-decoding methods, such as the Guruswami-Sudan and Wu algorithms.
Consequently the complexity of the interpolation step is reduced considerably.
The probability of failure can be minimised by adjusting certain parameters,
making it comparable with the K\&quot;otter-Vardy algorithm but having a much lower
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0690</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0690</id><created>2014-06-03</created><updated>2015-12-01</updated><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Niewerth</keyname><forenames>Matthias</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>On the state complexity of closures and interiors of regular languages
  with subwords and superwords</title><categories>cs.FL</categories><journal-ref>Theoretical Computer Science, 610:91-107, 2016</journal-ref><doi>10.1016/j.tcs.2015.09.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The downward and upward closures of a regular language $L$ are obtained by
collecting all the subwords and superwords of its elements, respectively. The
downward and upward interiors of $L$ are obtained dually by collecting words
having all their subwords and superwords in $L$, respectively. We provide lower
and upper bounds on the size of the smallest automata recognizing these
closures and interiors. We also consider the computational complexity of
decision problems for closures of regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0728</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0728</id><created>2014-06-03</created><updated>2014-06-04</updated><authors><author><keyname>He</keyname><forenames>Di</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>A Game-theoretic Machine Learning Approach for Revenue Maximization in
  Sponsored Search</title><categories>cs.GT cs.LG</categories><comments>Twenty-third International Conference on Artificial Intelligence
  (IJCAI 2013)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sponsored search is an important monetization channel for search engines, in
which an auction mechanism is used to select the ads shown to users and
determine the prices charged from advertisers. There have been several pieces
of work in the literature that investigate how to design an auction mechanism
in order to optimize the revenue of the search engine. However, due to some
unrealistic assumptions used, the practical values of these studies are not
very clear. In this paper, we propose a novel \emph{game-theoretic machine
learning} approach, which naturally combines machine learning and game theory,
and learns the auction mechanism using a bilevel optimization framework. In
particular, we first learn a Markov model from historical data to describe how
advertisers change their bids in response to an auction mechanism, and then for
any given auction mechanism, we use the learnt model to predict its
corresponding future bid sequences. Next we learn the auction mechanism through
empirical revenue maximization on the predicted bid sequences. We show that the
empirical revenue will converge when the prediction period approaches infinity,
and a Genetic Programming algorithm can effectively optimize this empirical
revenue. Our experiments indicate that the proposed approach is able to produce
a much more effective auction mechanism than several baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0767</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0767</id><created>2014-06-03</created><authors><author><keyname>Simonyi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>&#xc1;gnes</forenames></author></authors><title>A generalization of Witsenhausen's zero-error rate for directed graphs</title><categories>cs.IT math.CO math.IT</categories><comments>27 pages, 4 figures</comments><msc-class>94A15, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a communication setup where a source output is sent through a
free noisy channel first and an additional codeword is sent through a noiseless
but expensive channel later. With the help of the second message the decoder
should be able to decide with zero-error whether its decoding of the first
message was error-free. This scenario leads to the definition of a digraph
parameter that generalizes Witsenhausen's zero-error rate for directed graphs.
We investigate this new parameter for some specific directed graphs and explore
its relations to other digraph parameters like Sperner capacity and dichromatic
number.
  When the original problem is modified to require zero-error decoding of the
whole message then we arrive back to the Witsenhausen rate of an appropriately
defined undirected graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0769</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0769</id><created>2014-06-02</created><updated>2014-12-29</updated><authors><author><keyname>Wedin</keyname><forenames>Edvin</forenames></author><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author></authors><title>A quadratic lower bound for the convergence rate in the one-dimensional
  Hegselmann-Krause bounded confidence dynamics</title><categories>cs.SY math.CO</categories><comments>6 pages, one figure. Arxiv may flag for text overlap with a companion
  paper which we are uploading simoultaneously, as there is some similar text
  in the introductions to the two papers. Version 2: Some minor glitches fixed:
  an error in Footnote 2, some spelling mistakes and an incomplete reference in
  the bibliography. Version 3: This version accepted for publication</comments><msc-class>93A14, 39A60, 91D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let f_{k}(n) be the maximum number of time steps taken to reach equilibrium
by a system of n agents obeying the k-dimensional Hegselmann-Krause bounded
confidence dynamics. Previously, it was known that \Omega(n) = f_{1}(n) =
O(n^3). Here we show that f_{1}(n) = \Omega(n^2), which matches the best-known
lower bound in all dimensions k &gt;= 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0774</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0774</id><created>2014-06-01</created><authors><author><keyname>Caminati</keyname><forenames>Marco B.</forenames></author><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Rowat</keyname><forenames>Colin</forenames></author></authors><title>Set Theory or Higher Order Logic to Represent Auction Concepts in
  Isabelle?</title><categories>cs.LO</categories><comments>Preprint of a paper accepted for the forthcoming CICM 2014 conference
  (cicm-conference.org/2014): S.M. Watt et al. (Eds.): CICM 2014, LNAI 8543,
  Springer International Publishing Switzerland 2014. 16 pages, 1 figure</comments><msc-class>03E02, 03B35 (primary), 68W05 (Secondary)</msc-class><acm-class>F.4.1; D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When faced with the question of how to represent properties in a formal proof
system any user has to make design decisions. We have proved three of the
theorems from Maskin's 2004 survey article on Auction Theory using the
Isabelle/HOL system, and we have produced verified code for combinatorial
Vickrey auctions. A fundamental question in this was how to represent some
basic concepts: since set theory is available inside Isabelle/HOL, when
introducing new definitions there is often the issue of balancing the amount of
set-theoretical objects and of objects expressed using entities which are more
typical of higher order logic such as functions or lists. Likewise, a user has
often to answer the question whether to use a constructive or a
non-constructive definition. Such decisions have consequences for the proof
development and the usability of the formalization. For instance, sets are
usually closer to the representation that economists would use and recognize,
while the other objects are closer to the extraction of computational content.
In this paper we give examples of the advantages and disadvantages for these
approaches and their relationships. In addition, we present the corresponding
Isabelle library of definitions and theorems, most prominently those dealing
with relations and quotients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0775</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0775</id><created>2014-06-03</created><authors><author><keyname>Bi</keyname><forenames>Huibo</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>A Cooperative Emergency Navigation Framework using Mobile Cloud
  Computing</title><categories>cs.NI</categories><comments>This document contains 8 pages and 3 figures and has been accepted by
  ISCIS 2014 (29th International Symposium on Computer and Information
  Sciences)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of wireless sensor networks (WSNs) for emergency navigation systems
suffer disadvantages such as limited computing capacity, restricted battery
power and high likelihood of malfunction due to the harsh physical environment.
By making use of the powerful sensing ability of smart phones, this paper
presents a cloud-enabled emergency navigation framework to guide evacuees in a
coordinated manner and improve the reliability and resilience in both
communication and localization. By using social potential fields (SPF),
evacuees form clusters during an evacuation process and are directed to
egresses with the aid of a Cognitive Packet Networks (CPN) based algorithm.
Rather than just rely on the conventional telecommunications infrastructures,
we suggest an Ad hoc Cognitive Packet Network (AHCPN) based protocol to prolong
the life time of smart phones, that adaptively searches optimal communication
routes between portable devices and the egress node that provides access to a
cloud server with respect to the remaining battery power of smart phones and
the time latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0819</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0819</id><created>2014-06-02</created><updated>2014-06-04</updated><authors><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author><author><keyname>Wedin</keyname><forenames>Edvin</forenames></author></authors><title>The Hegselmann-Krause dynamics for equally spaced agents</title><categories>physics.soc-ph cs.MA math.CO</categories><comments>17 pages, one figure. Arxiv may flag for text overlap with a
  companion paper which we are uploading simoultaneously, as there is some
  similar text in the introductions to the two papers. Version 2: Some minor
  glitches fixed: spelling mistakes and an incomplete reference in the
  bibliography</comments><msc-class>93A14, 39A60, 91D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Hegselmann-Krause bounded confidence dynamics for n equally
spaced opinions on the real line, with gaps equal to the confidence bound r,
which we take to be 1. We prove rigorous results on the evolution of this
configuration, which confirm hypotheses previously made based on simulations
for small values of n. Namely, for every n, the system evolves as follows:
after every 5 time steps, a group of 3 agents become disconnected at either end
and collapse to a cluster at the subsequent step. This continues until there
are fewer than 6 agents left in the middle, and these finally collapse to a
cluster, if n is not a multiple of 6. In particular, the final configuration
consists of 2*[n/6] clusters of size 3, plus one cluster in the middle of size
n (mod 6), if n is not a multiple of 6, and the number of time steps before
freezing is 5n/6 + O(1). We also consider the dynamics for arbitrary, but
constant, inter-agent spacings d \in [0, 1] and present three main findings.
Firstly we prove that the evolution is periodic also at some other, but not
all, values of d, and present numerical evidence that for all d something
&quot;close&quot; to periodicity nevertheless holds. Secondly, we exhibit a value of d at
which the behaviour is periodic and the time to freezing is n + O(1), hence
slower than that for d = 1. Thirdly, we present numerical evidence that, as d
--&gt; 0, the time to freezing may be closer, in order of magnitude, to the
diameter d(n-1) of the configuration rather than the number of agents n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0824</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0824</id><created>2014-06-03</created><authors><author><keyname>Arik</keyname><forenames>Sercan</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Sukru Burc</forenames></author><author><keyname>Goldberg</keyname><forenames>Adam</forenames></author></authors><title>Supervised classification-based stock prediction and portfolio
  optimization</title><categories>q-fin.ST cs.CE cs.LG q-fin.PM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of publicly traded companies as well as the amount of their
financial data grows rapidly, it is highly desired to have tracking, analysis,
and eventually stock selections automated. There have been few works focusing
on estimating the stock prices of individual companies. However, many of those
have worked with very small number of financial parameters. In this work, we
apply machine learning techniques to address automated stock picking, while
using a larger number of financial parameters for individual companies than the
previous studies. Our approaches are based on the supervision of prediction
parameters using company fundamentals, time-series properties, and correlation
information between different stocks. We examine a variety of supervised
learning techniques and found that using stock fundamentals is a useful
approach for the classification problem, when combined with the high
dimensional data handling capabilities of support vector machine. The portfolio
our system suggests by predicting the behavior of stocks results in a 3% larger
growth on average than the overall market within a 3-month time period, as the
out-of-sample test suggests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0866</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0866</id><created>2014-06-03</created><authors><author><keyname>Kim</keyname><forenames>Jinsub</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Thomas</keyname><forenames>Robert J.</forenames></author></authors><title>Subspace Methods for Data Attack on State Estimation: A Data Driven
  Approach</title><categories>cs.CR</categories><comments>12 pages</comments><doi>10.1109/TSP.2014.2385670</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data attacks on state estimation modify part of system measurements such that
the tempered measurements cause incorrect system state estimates. Attack
techniques proposed in the literature often require detailed knowledge of
system parameters. Such information is difficult to acquire in practice. The
subspace methods presented in this paper, on the other hand, learn the system
operating subspace from measurements and launch attacks accordingly. Conditions
for the existence of an unobservable subspace attack are obtained under the
full and partial measurement models. Using the estimated system subspace, two
attack strategies are presented. The first strategy aims to affect the system
state directly by hiding the attack vector in the system subspace. The second
strategy misleads the bad data detection mechanism so that data not under
attack are removed. Performance of these attacks are evaluated using the IEEE
14-bus network and the IEEE 118-bus network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0879</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0879</id><created>2014-06-03</created><updated>2016-02-12</updated><authors><author><keyname>Finkelstein</keyname><forenames>Jeffrey</forenames></author></authors><title>Computing rank of finite algebraic structures with limited
  nondeterminism</title><categories>cs.CC</categories><comments>This document is distributed under the Creative Commons
  Attribution-ShareAlike 4.0 International License</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The rank of a finite algebraic structure with a single binary operation is
the minimum number of elements needed to express every other element under the
closure of the operation. In the case of groups, the previous best algorithm
for computing rank used polylogarithmic space. We reduce the best upper bounds
on the complexity of computing rank for groups and for quasigroups. This paper
proves that the rank problem for these algebraic structures can be verified by
highly restricted models of computation given only very short certificates of
correctness.
  Specifically, we prove that the problem of deciding whether the rank of a
finite quasigroup, given as a Cayley table, is smaller than a specified number
is decidable by a circuit of depth $O(\log \log n)$ augmented with $O(\log^2
n)$ nondeterministic bits. Furthermore, if the quasigroup is a group, then the
problem is also decidable by a Turing machine using $O(\log n)$ space and
$O(\log^2 n)$ bits of nondeterminism with the ability to read the
nondeterministic bits multiple times. Finally, we provide similar results for
related problems on other algebraic structures and other kinds of rank. These
new upper bounds are significant improvements, especially for groups. In
general, the lens of limited nondeterminism provides an easy way to improve
many simple algorithms, like the ones presented here, and we suspect it will be
especially useful for other algebraic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0888</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0888</id><created>2014-06-03</created><authors><author><keyname>Almeida</keyname><forenames>Jorge</forenames></author><author><keyname>Costa</keyname><forenames>Jos&#xe9; Carlos</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author></authors><title>McCammond's normal forms for free aperiodic semigroups revisited</title><categories>cs.FL</categories><journal-ref>LMS Journal of Computation and Mathematics 18 (2015) 130-147</journal-ref><doi>10.1112/S1461157014000448</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits the solution of the word problem for $\omega$-terms
interpreted over finite aperiodic semigroups, obtained by J. McCammond. The
original proof of correctness of McCammond's algorithm, based on normal forms
for such terms, uses McCammond's solution of the word problem for certain
Burnside semigroups. In this paper, we establish a new, simpler, correctness
proof of McCammond's algorithm, based on properties of certain regular
languages associated with the normal forms. This method leads to new
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0893</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0893</id><created>2014-06-03</created><authors><author><keyname>Joseph</keyname><forenames>Mathew</forenames></author><author><keyname>Kuper</keyname><forenames>Gabriel</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author></authors><title>Query Answering over Contextualized RDF/OWL Knowledge with
  Forall-Existential Bridge Rules: Attaining Decidability using Acyclicity
  (full version)</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent outburst of context-dependent knowledge on the Semantic Web (SW)
has led to the realization of the importance of the quads in the SW community.
Quads, which extend a standard RDF triple, by adding a new parameter of the
`context' of an RDF triple, thus informs a reasoner to distinguish between the
knowledge in various contexts. Although this distinction separates the triples
in an RDF graph into various contexts, and allows the reasoning to be decoupled
across various contexts, bridge rules need to be provided for inter-operating
the knowledge across these contexts. We call a set of quads together with the
bridge rules, a quad-system. In this paper, we discuss the problem of query
answering over quad-systems with expressive forall-existential bridge rules. It
turns out the query answering over quad-systems is undecidable, in general. We
derive a decidable class of quad-systems, namely context-acyclic quad-systems,
for which query answering can be done using forward chaining. Tight bounds for
data and combined complexity of query entailment has been established for the
derived class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0899</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0899</id><created>2014-06-03</created><updated>2015-08-26</updated><authors><author><keyname>Valls</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Max-Weight Revisited: Sequences of Non-Convex Optimisations Solving
  Convex Optimisations</title><categories>math.OC cs.IT math.IT</categories><comments>convex optimisation, max-weight scheduling, backpressure, subgradient
  methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connections between max-weight approaches and dual
subgradient methods for convex optimisation. We find that strong connections
exist and we establish a clean, unifying theoretical framework that includes
both max-weight and dual subgradient approaches as special cases. Our analysis
uses only elementary methods, and is not asymptotic in nature. It also allows
us to establish an explicit and direct connection between discrete queue
occupancies and Lagrange multipliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0900</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0900</id><created>2014-06-03</created><authors><author><keyname>Tak&#xe1;cs</keyname><forenames>K&#xe1;roly</forenames></author><author><keyname>Flache</keyname><forenames>Andreas</forenames></author><author><keyname>M&#xe4;s</keyname><forenames>Michael</forenames></author></authors><title>Is there negative social influence? Disentangling effects of
  dissimilarity and disliking on opinion shifts</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical studies are inconclusive about the underlying mechanisms that shape
the interrelated dynamics of opinions and interpersonal attraction. There is
strong evidence that others whom are liked have a positive influence on
opinions and similarities induce attraction (homophily). We know less about
&quot;negative&quot; mechanisms concerning whether disliked others induce shifts away
from consensus (negative influence), whether large differences (dissimilarity)
generate distancing, and whether dissimilarities induce disliking
(heterophobia). This study tests discriminating hypotheses about the presence
of positive and negative mechanisms in controlled experiments involving dyadic
interactions. Results confirm the presence of homophily, do not support the
existence of negative social influence, and show a robust positive linear
relationship between opinion distance and opinion shifts. This implies that
contact might provide the largest push towards consensus in case of large
initial differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0905</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0905</id><created>2014-06-03</created><authors><author><keyname>Missier</keyname><forenames>Paolo</forenames></author><author><keyname>Woodman</keyname><forenames>Simon</forenames></author><author><keyname>Hiden</keyname><forenames>Hugo</forenames></author><author><keyname>Watson</keyname><forenames>Paul</forenames></author></authors><title>Provenance and data differencing for workflow reproducibility analysis</title><categories>cs.DB</categories><journal-ref>Provenance and data differencing for workflow reproducibility
  analysis Missier, P.; Woodman, S.; Hiden, H.; and Watson, P. Concurrency and
  Computation: Practice and Experience, . 2013</journal-ref><doi>10.1002/cpe.3035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the foundations of science is that researchers must publish the
methodology used to achieve their results so that others can attempt to
reproduce them. This has the added benefit of allowing methods to be adopted
and adapted for other purposes. In the field of e-Science, services -- often
choreographed through workflow, process data to generate results. The
reproduction of results is often not straightforward as the computational
objects may not be made available or may have been updated since the results
were generated. For example, services are often updated to fix bugs or improve
algorithms. This paper addresses these problems in three ways. Firstly, it
introduces a new framework to clarify the range of meanings of
&quot;reproducibility&quot;. Secondly, it describes a new algorithm, \PDIFF, that uses a
comparison of workflow provenance traces to determine whether an experiment has
been reproduced; the main innovation is that if this is not the case then the
specific point(s) of divergence are identified through graph analysis,
assisting any researcher wishing to understand those differences. One key
feature is support for user-defined, semantic data comparison operators.
Finally, the paper describes an implementation of \PDIFF that leverages the
power of the e-Science Central platform which enacts workflows in the cloud. As
well as automatically generating a provenance trace for consumption by \PDIFF,
the platform supports the storage and re-use of old versions of workflows, data
and services; the paper shows how this can be powerfully exploited in order to
achieve reproduction and re-use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0907</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0907</id><created>2014-06-03</created><updated>2014-07-23</updated><authors><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Haraldson</keyname><forenames>Joseph</forenames></author></authors><title>Computing GCRDs of Approximate Differential Polynomials</title><categories>cs.SC cs.NA</categories><comments>To appear, Workshop on Symbolic-Numeric Computing (SNC'14) July 2014</comments><acm-class>I.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential (Ore) type polynomials with approximate polynomial coefficients
are introduced. These provide a useful representation of approximate
differential operators with a strong algebraic structure, which has been used
successfully in the exact, symbolic, setting. We then present an algorithm for
the approximate Greatest Common Right Divisor (GCRD) of two approximate
differential polynomials, which intuitively is the differential operator whose
solutions are those common to the two inputs operators. More formally, given
approximate differential polynomials $f$ and $g$, we show how to find &quot;nearby&quot;
polynomials $\widetilde f$ and $\widetilde g$ which have a non-trivial GCRD.
Here &quot;nearby&quot; is under a suitably defined norm. The algorithm is a
generalization of the SVD-based method of Corless et al. (1995) for the
approximate GCD of regular polynomials. We work on an appropriately
&quot;linearized&quot; differential Sylvester matrix, to which we apply a block SVD. The
algorithm has been implemented in Maple and a demonstration of its robustness
is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0909</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0909</id><created>2014-06-03</created><authors><author><keyname>Ildarabadi</keyname><forenames>S.</forenames></author><author><keyname>Ebrahimi</keyname><forenames>M.</forenames></author><author><keyname>Pourreza</keyname><forenames>H. R.</forenames></author></authors><title>Improvement Tracking Dynamic Programming using Replication Function for
  Continuous Sign Language Recognition</title><categories>cs.CV</categories><comments>5 pages, 13 figures, Published with &quot;International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><doi>10.14445/22315381/IJETT-V7P254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we used a Replication Function (R. F.)for improvement tracking
with dynamic programming. The R. F. transforms values of gray level [0 255] to
[0 1]. The resulting images of R. F. are more striking and visible in skin
regions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and
face. Results show that Tracking Error Rate 11% and Average Tracked Distance 7%
reduced
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0912</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0912</id><created>2014-06-03</created><updated>2015-03-04</updated><authors><author><keyname>Seeling</keyname><forenames>Patrick</forenames></author></authors><title>Towards Quality of Experience Determination for Video in Augmented
  Binocular Vision Scenarios</title><categories>cs.MM cs.HC</categories><comments>Accepted to Signal Processing: Image Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the continuous growth in the consumer markets of mobile smartphones and
increasingly in augmented reality wearable devices, several avenues of research
investigate the relationships between the quality perceived by mobile users and
the delivery mechanisms at play to support a high quality of experience for
mobile users. In this paper, we present the first study that evaluates the
relationships of mobile movie quality and the viewer-perceived quality thereof
in an augmented reality setting with see-through devices. We find that
participants tend to overestimate the video quality and exhibit a significant
variation of accuracy that leans onto the movie content and its dynamics. Our
findings, thus, can broadly impact future media adaptation and delivery
mechanisms for this new display format of mobile multimedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0919</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0919</id><created>2014-06-03</created><updated>2014-06-11</updated><authors><author><keyname>Lan</keyname><forenames>Guanghui</forenames></author></authors><title>Gradient Sliding for Composite Optimization</title><categories>math.OC cs.CC stat.ML</categories><msc-class>90C25, 90C06, 90C22, 49M37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider in this paper a class of composite optimization problems whose
objective function is given by the summation of a general smooth and nonsmooth
component, together with a relatively simple nonsmooth term. We present a new
class of first-order methods, namely the gradient sliding algorithms, which can
skip the computation of the gradient for the smooth component from time to
time. As a consequence, these algorithms require only ${\cal
O}(1/\sqrt{\epsilon})$ gradient evaluations for the smooth component in order
to find an $\epsilon$-solution for the composite problem, while still
maintaining the optimal ${\cal O}(1/\epsilon^2)$ bound on the total number of
subgradient evaluations for the nonsmooth component. We then present a
stochastic counterpart for these algorithms and establish similar complexity
bounds for solving an important class of stochastic composite optimization
problems. Moreover, if the smooth component in the composite function is
strongly convex, the developed gradient sliding algorithms can significantly
reduce the number of graduate and subgradient evaluations for the smooth and
nonsmooth component to ${\cal O} (\log (1/\epsilon))$ and ${\cal
O}(1/\epsilon)$, respectively. Finally, we generalize these algorithms to the
case when the smooth component is replaced by a nonsmooth one possessing a
certain bi-linear saddle point structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0924</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0924</id><created>2014-06-03</created><updated>2014-12-12</updated><authors><author><keyname>Felzenszwalb</keyname><forenames>Pedro F.</forenames></author><author><keyname>Oberlin</keyname><forenames>John G.</forenames></author></authors><title>Multiscale Fields of Patterns</title><categories>cs.CV</categories><comments>In NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a framework for defining high-order image models that can be used
in a variety of applications. The approach involves modeling local patterns in
a multiscale representation of an image. Local properties of a coarsened image
reflect non-local properties of the original image. In the case of binary
images local properties are defined by the binary patterns observed over small
neighborhoods around each pixel. With the multiscale representation we capture
the frequency of patterns observed at different scales of resolution. This
framework leads to expressive priors that depend on a relatively small number
of parameters. For inference and learning we use an MCMC method for block
sampling with very large blocks. We evaluate the approach with two example
applications. One involves contour detection. The other involves binary
segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0928</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0928</id><created>2014-06-03</created><authors><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Goratti</keyname><forenames>Leonardo</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Reynaud</keyname><forenames>Laurent</forenames></author></authors><title>Enabling Disaster Resilient 4G Mobile Communication Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 4G Long Term Evolution (LTE) is the cellular technology expected to
outperform the previous generations and to some extent revolutionize the
experience of the users by taking advantage of the most advanced radio access
techniques (i.e. OFDMA, SC-FDMA, MIMO). However, the strong dependencies
between user equipments (UEs), base stations (eNBs) and the Evolved Packet Core
(EPC) limit the flexibility, manageability and resiliency in such networks. In
case the communication links between UEs-eNB or eNB-EPC are disrupted, UEs are
in fact unable to communicate. In this article, we reshape the 4G mobile
network to move towards more virtual and distributed architectures for
improving disaster resilience, drastically reducing the dependency between UEs,
eNBs and EPC. The contribution of this work is twofold. We firstly present the
Flexible Management Entity (FME), a distributed entity which leverages on
virtualized EPC functionalities in 4G cellular systems. Second, we introduce a
simple and novel device-todevice (D2D) communication scheme allowing the UEs in
physical proximity to communicate directly without resorting to the
coordination with an eNB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0930</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0930</id><created>2014-06-03</created><authors><author><keyname>Lee</keyname><forenames>Aaron</forenames></author><author><keyname>King</keyname><forenames>Livia</forenames></author></authors><title>ACO Implementation for Sequence Alignment with Genetic Algorithms</title><categories>cs.CE cs.NE</categories><comments>Report 6 pages, 4 figures, Supplementary material 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we implement Ant Colony Optimization (ACO) for sequence
alignment. ACO is a meta-heuristic recently developed for nearest neighbor
approximations in large, NP-hard search spaces. Here we use a genetic algorithm
approach to evolve the best parameters for an ACO designed to align two
sequences. We then used the best parameters found to interpolate approximate
optimal parameters for a given string length within a range. The basis of our
comparison is the alignment given by the Needleman-Wunsch algorithm. We found
that ACO can indeed be applied to sequence alignment. While it is
computationally expensive compared to other equivalent algorithms, it is a
promising algorithm that can be readily applied to a variety of other
biological problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0936</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0936</id><created>2014-06-04</created><authors><author><keyname>Hu</keyname><forenames>Feng</forenames></author><author><keyname>Zhao</keyname><forenames>Hai-Xing</forenames></author><author><keyname>Zhan</keyname><forenames>Xiu-Xiu</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author></authors><title>Evolution of citation networks with the hypergraph formalism</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed an evolving model via the hypergraph to illustrate
the evolution of the citation network. In the evolving model, we consider the
mechanism combined with preferential attachment and the aging influence.
Simulation results show that the proposed model can characterize the citation
distribution of the real system very well. In addition, we give the analytical
result of the citation distribution using the master equation. Detailed
analysis showed that the time decay factor should be the origin of the same
citation distribution between the proposed model and the empirical result. The
proposed model might shed some lights in understanding the underlying laws
governing the structure of real citation networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0941</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0941</id><created>2014-06-04</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Rabbany</keyname><forenames>Reihaneh</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Augmentative Message Passing for Traveling Salesman Problem and Graph
  Partitioning</title><categories>cs.AI</categories><report-no>Advances in Neural Information Processing Systems 27 (NIPS 2014)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cutting plane method is an augmentative constrained optimization
procedure that is often used with continuous-domain optimization techniques
such as linear and convex programs. We investigate the viability of a similar
idea within message passing -- which produces integral solutions -- in the
context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP),
we propose a factor-graph based on Held-Karp formulation, with an exponential
number of constraint factors, each of which has an exponential but sparse
tabular form. 2) For graph-partitioning (a.k.a., community mining) using
modularity optimization, we introduce a binary variable model with a large
number of constraints that enforce formation of cliques. In both cases we are
able to derive surprisingly simple message updates that lead to competitive
solutions on benchmark instances. In particular for TSP we are able to find
near-optimal solutions in the time that empirically grows with N^3,
demonstrating that augmentation is practical and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0946</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0946</id><created>2014-06-04</created><authors><author><keyname>He</keyname><forenames>Fei</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author></authors><title>Beyond $\chi^2$ Difference: Learning Optimal Metric for Boundary
  Detection</title><categories>cs.CV</categories><comments>Submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2014.2346232</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This letter focuses on solving the challenging problem of detecting natural
image boundaries. A boundary usually refers to the border between two regions
with different semantic meanings. Therefore, a measurement of dissimilarity
between image regions plays a pivotal role in boundary detection of natural
images. To improve the performance of boundary detection, a Learning-based
Boundary Metric (LBM) is proposed to replace $\chi^2$ difference adopted by the
classical algorithm mPb. Compared with $\chi^2$ difference, LBM is composed of
a single layer neural network and an RBF kernel, and is fine-tuned by
supervised learning rather than human-crafted. It is more effective in
describing the dissimilarity between natural image regions while tolerating
large variance of image data. After substituting $\chi^2$ difference with LBM,
the F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to
0.71. Moreover, when image features are computed on a single scale, the
proposed LBM algorithm still achieves competitive results compared with
\emph{mPb}, which makes use of multi-scale image features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0955</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0955</id><created>2014-06-04</created><authors><author><keyname>Gu</keyname><forenames>Yan</forenames></author><author><keyname>Sun</keyname><forenames>Yihan</forenames></author><author><keyname>He</keyname><forenames>Yong</forenames></author></authors><title>Cascading A*: a Parallel Approach to Approximate Heuristic Search</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we proposed a new approximate heuristic search algorithm:
Cascading A*, which is a two-phrase algorithm combining A* and IDA* by a new
concept &quot;envelope ball&quot;. The new algorithm CA* is efficient, able to generate
approximate solution and any-time solution, and parallel friendly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0968</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0968</id><created>2014-06-04</created><authors><author><keyname>Kirk</keyname><forenames>Christopher S</forenames></author></authors><title>Integration of a Predictive, Continuous Time Neural Network into
  Securities Market Trading Operations</title><categories>q-fin.CP cs.CE cs.NE</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes recent development and test implementation of a
continuous time recurrent neural network that has been configured to predict
rates of change in securities. It presents outcomes in the context of popular
technical analysis indicators and highlights the potential impact of continuous
predictive capability on securities market trading operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0975</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0975</id><created>2014-06-04</created><authors><author><keyname>Skordas</keyname><forenames>Ioannis A.</forenames></author><author><keyname>Fragulis</keyname><forenames>George F.</forenames></author><author><keyname>Triantafyllou</keyname><forenames>Athanassios G.</forenames></author></authors><title>A.Q.M.E.I.S.: Air Quality Meteorological and Enviromental Information
  System in Western Macedonia, Hellas</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An operational monitoring, as well as high resolution local-scale
meteorological and air quality forecasting information system for Western
Macedonia, Hellas, has been developed and is operated by the Laboratory of
Atmospheric Pollution and Environmental Physics / TEI Western Macedonia since
2002, continuously improved. In this paper the novelty of information system is
presented, in a dynamic, easily accessible and user-friendly manner. It
consists of a structured system that users have access to and they can
manipulate thoroughly, as well as of a system for accessing and managing
results of measurements in a direct and dynamic way. It provides updates about
the weather and pollution forecast for the next few days (based on current day
information) in Western Macedonia. These forecasts are displayed through
dynamic-interactive web charts and the visual illustration of the atmospheric
pollution of the region in a map using images and animation images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0993</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0993</id><created>2014-06-04</created><updated>2014-08-27</updated><authors><author><keyname>Matsubara</keyname><forenames>Takamitsu</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Latent Kullback Leibler Control for Continuous-State Systems using
  Probabilistic Graphical Models</title><categories>cs.SY cs.RO</categories><comments>9 pages, 5 figures, accepted in Uncertainty in Artificial
  Intelligence (UAI '14)</comments><acm-class>I.2.8; I.2.9; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kullback Leibler (KL) control problems allow for efficient computation of
optimal control by solving a principal eigenvector problem. However, direct
applicability of such framework to continuous state-action systems is limited.
In this paper, we propose to embed a KL control problem in a probabilistic
graphical model where observed variables correspond to the continuous (possibly
high-dimensional) state of the system and latent variables correspond to a
discrete (low-dimensional) representation of the state amenable for KL control
computation. We present two examples of this approach. The first one uses
standard hidden Markov models (HMMs) and computes exact optimal control, but is
only applicable to low-dimensional systems. The second one uses factorial HMMs,
it is scalable to higher dimensional problems, but control computation is
approximate. We illustrate both examples in several robot motor control tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.0995</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.0995</id><created>2014-06-04</created><updated>2014-06-17</updated><authors><author><keyname>Ramanathan</keyname><forenames>Ravishankar</forenames></author><author><keyname>Kay</keyname><forenames>Alastair</forenames></author><author><keyname>Murta</keyname><forenames>Gl&#xe1;ucia</forenames></author><author><keyname>Horodecki</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Characterising the Performance of XOR Games and the Shannon Capacity of
  Graphs</title><categories>quant-ph cs.IT math.CO math.IT</categories><comments>5 pages. Clarified proof of theorem 1, typos corrected</comments><journal-ref>Phys. Rev. Lett. 113, 240401 (2014)</journal-ref><doi>10.1103/PhysRevLett.113.240401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a set of necessary and sufficient conditions such that
quantum players of a two-party {\sc xor} game cannot perform any better than
classical players. With any such game, we associate a graph and examine its
zero-error communication capacity. This allows us to specify a broad new class
of graphs for which the Shannon capacity can be calculated. The conditions also
enable the parametrisation of new families of games which have no quantum
advantage, for arbitrary input probability distributions up to certain
symmetries. In the future, these might be used in information-theoretic studies
on reproducing the set of quantum non-local correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1012</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1012</id><created>2014-06-04</created><authors><author><keyname>S</keyname><forenames>Lakshmi Prabha</forenames></author><author><keyname>Janakiraman</keyname><forenames>T. N.</forenames></author></authors><title>Comfortability of a Team in Social Networks</title><categories>cs.SI</categories><comments>24 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1405.4534</comments><msc-class>91D30, 05C82, 05C85, 05C69, 05C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many indexes (measures or metrics) in Social Network Analysis
(SNA), like density, cohesion, etc. We have defined a new SNA index called
&quot;comfortability&quot;. In this paper, core comfortable team of a social network is
defined based on graph theoretic concepts and some of their structural
properties are analyzed.
  Comfortability is one of the important attributes (characteristics) for a
successful team work. So, it is necessary to find a comfortable and successful
team in any given social network.
  It is proved that forming core comfortable team in any network is NP-Complete
using the concepts of domination in graph theory. Next, we give two
polynomial-time approximation algorithms for finding such a core comfortable
team in any given network with performance ratio O(ln \Delta), where \Delta is
the maximum degree of a given network (graph). The time complexity of the
algorithm is proved to be O(n^{3}), where n is the number of persons (vertices)
in the network (graph). It is also proved that the algorithms give good results
in scale-free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1022</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1022</id><created>2014-06-04</created><authors><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author><author><keyname>Sinaimeri</keyname><forenames>Blerina</forenames></author><author><keyname>Marchet</keyname><forenames>Camille</forenames></author><author><keyname>Miele</keyname><forenames>Vincent</forenames></author><author><keyname>Sagot</keyname><forenames>Marie-France</forenames></author><author><keyname>Lacroix</keyname><forenames>Vincent</forenames></author></authors><title>Navigating in a sea of repeats in RNA-seq without drowning</title><categories>cs.DS cs.CE q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main challenge in de novo assembly of NGS data is certainly to deal with
repeats that are longer than the reads. This is particularly true for RNA- seq
data, since coverage information cannot be used to flag repeated sequences, of
which transposable elements are one of the main examples. Most transcriptome
assemblers are based on de Bruijn graphs and have no clear and explicit model
for repeats in RNA-seq data, relying instead on heuristics to deal with them.
The results of this work are twofold. First, we introduce a formal model for
repre- senting high copy number repeats in RNA-seq data and exploit its
properties for inferring a combinatorial characteristic of repeat-associated
subgraphs. We show that the problem of identifying in a de Bruijn graph a
subgraph with this charac- teristic is NP-complete. In a second step, we show
that in the specific case of a local assembly of alternative splicing (AS)
events, we can implicitly avoid such subgraphs. In particular, we designed and
implemented an algorithm to efficiently identify AS events that are not
included in repeated regions. Finally, we validate our results using synthetic
data. We also give an indication of the usefulness of our method on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1033</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1033</id><created>2014-06-03</created><authors><author><keyname>Saini</keyname><forenames>Smita</forenames></author><author><keyname>Mann</keyname><forenames>Deep</forenames></author></authors><title>Identity Management issues in Cloud Computing</title><categories>cs.CR</categories><comments>3 pages.&quot;Published with International Journal of Computer Trends and
  Technology (IJCTT)&quot;</comments><journal-ref>IJCTT V9(8):414-416, March 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9P174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is providing a low cost on demand services to the users,
omnipresent network,large storage capacity due to these features of cloud
computing web applications are moving towards the cloud and due to this
migration of the web application,cloud computing platform is raised many issues
like privacy, security etc. Privacy issue are major concern for the cloud
computing. Privacy is to preserve the sensitive information of the cloud
consumer and the major issues to the privacy are unauthorized secondary usage,
lack of user control, unclear responsibility. For dealing with these privacy
issues Identity management method are used. This paper discusses the privacy
issue and different kind of identity management technique that are used for
preserving the privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1034</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1034</id><created>2014-06-04</created><authors><author><keyname>Salge</keyname><forenames>Christoph</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Don't Believe Everything You Hear; Preserving Relevant Information by
  Discarding Social Information</title><categories>cs.MA cs.SI nlin.AO</categories><comments>8 pages, 4 figures, accepted for publication in Proceedings of
  Alife14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating information gained by observing others via Social Bayesian
Learning can be beneficial for an agent's performance, but can also enable
population wide information cascades that perpetuate false beliefs through the
agent population. We show how agents can influence the observation network by
changing their probability of observing others, and demonstrate the existence
of a population-wide equilibrium, where the advantages and disadvantages of the
Social Bayesian update are balanced. We also use the formalism of relevant
information to illustrate how negative information cascades are characterized
by processing increasing amounts of non-relevant information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1035</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1035</id><created>2014-06-03</created><authors><author><keyname>Weil</keyname><forenames>Pascal</forenames><affiliation>LaBRI</affiliation></author></authors><title>From algebra to logic: there and back again -- the story of a hierarchy</title><categories>cs.FL cs.LO</categories><comments>Developments in Language Theory 2014, Ekaterinburg : Russian
  Federation (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an extended survey of the results concerning a hierarchy of languages
that is tightly connected with the quantifier alternation hierarchy within the
two-variable fragment of first order logic of the linear order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1036</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1036</id><created>2014-06-04</created><authors><author><keyname>Sarkar</keyname><forenames>Sumanta</forenames></author></authors><title>Some Results on Bent-Negabent Boolean Functions over Finite Fields</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider negabent Boolean functions that have Trace representation. We
completely characterize quadratic negabent monomial functions. We show the
relation between negabent functions and bent functions via a quadratic
function. Using this characterization, we give infinite classes of
bent-negabent Boolean functions over the finite field $\F_{2^n}$, with the
maximum possible degree, $n \over 2$. These are the first ever constructions of
negabent functions with trace representation that have optimal degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1041</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1041</id><created>2014-06-04</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames></author><author><keyname>Konstantinidis</keyname><forenames>Stavros</forenames></author><author><keyname>Kopecki</keyname><forenames>Steffen</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author></authors><title>An efficient algorithm for computing the edit distance of a regular
  language via input-altering transducers</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of computing the edit distance of a regular language
given via an NFA. This problem relates to the inherent maximal error-detecting
capability of the language in question. We present an efficient algorithm for
solving this problem which executes in time $O(r^2n^2d)$, where $r$ is the
cardinality of the alphabet involved, $n$ is the number of transitions in the
given NFA, and $d$ is the computed edit distance. We have implemented the
algorithm and present here performance tests. The correctness of the algorithm
is based on the result (also presented here) that the particular
error-detection property related to our problem can be defined via an
input-altering transducer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1049</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1049</id><created>2014-06-04</created><updated>2014-06-16</updated><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Xu</keyname><forenames>Bangteng</forenames></author></authors><title>Fourier Transforms and Bent Functions on Finite Abelian Group-Acted Sets</title><categories>cs.DM cs.CR math.RT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a finite abelian group acting faithfully on a finite set $X$. As a
natural generalization of the perfect nonlinearity of Boolean functions, the
$G$-bentness and $G$-perfect nonlinearity of functions on $X$ are studied by
Poinsot et al. [6,7] via Fourier transforms of functions on $G$. In this paper
we introduce the so-called $G$-dual set $\widehat X$ of $X$, which plays the
role similar to the dual group $\widehat G$ of $G$, and the Fourier transforms
of functions on $X$, a generalization of the Fourier transforms of functions on
finite abelian groups. Then we characterize the bent functions on $X$ in terms
of their own Fourier transforms on $\widehat X$. Bent (perfect nonlinear)
functions on finite abelian groups and $G$-bent ($G$-perfect nonlinear)
functions on $X$ are treated in a uniform way in this paper, and many known
results in [4,2,6,7] are obtained as direct consequences. Furthermore, we will
prove that the bentness of a function on $X$ can be determined by its distance
from the set of $G$-linear functions. In order to explain the main results
clearly, examples are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1055</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1055</id><created>2014-06-04</created><authors><author><keyname>Sok</keyname><forenames>Lin</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>Lattice Codes for the Binary Deletion Channel</title><categories>cs.IT math.IT</categories><comments>2 figs; presented in part in ISIT 2013; submitted to IEEE trans. on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of deletion codes for the Levenshtein metric is reduced to
the construction of codes over the integers for the Manhattan metric by run
length coding. The latter codes are constructed by expurgation of translates of
lattices. These lattices, in turn, are obtained from Construction~A applied to
binary codes and $\Z_4-$codes. A lower bound on the size of our codes for the
Manhattan distance are obtained through generalized theta series of the
corresponding lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1058</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1058</id><created>2014-06-04</created><authors><author><keyname>Mehraghdam</keyname><forenames>Sevil</forenames></author><author><keyname>Keller</keyname><forenames>Matthias</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Specifying and Placing Chains of Virtual Network Functions</title><categories>cs.NI</categories><doi>10.1109/CloudNet.2014.6968961</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network appliances perform different functions on network flows and
constitute an important part of an operator's network. Normally, a set of
chained network functions process network flows. Following the trend of
virtualization of networks, virtualization of the network functions has also
become a topic of interest. We define a model for formalizing the chaining of
network functions using a context-free language. We process deployment requests
and construct virtual network function graphs that can be mapped to the
network. We describe the mapping as a Mixed Integer Quadratically Constrained
Program (MIQCP) for finding the placement of the network functions and chaining
them together considering the limited network resources and requirements of the
functions. We have performed a Pareto set analysis to investigate the possible
trade-offs between different optimization objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1059</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1059</id><created>2014-06-03</created><authors><author><keyname>Arrobo</keyname><forenames>Gabriel E.</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>Minimizing Energy Consumption for Cooperative Network and Diversity
  Coded Sensor Networks</title><categories>cs.NI</categories><doi>10.1109/WTS.2014.6834989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach to minimize the energy consumption of
multihop wireless packet networks, while achieving the required level of
reliability. We consider networks that use Cooperative Network Coding (CNC),
which is a synergistic combination of Cooperative Communications and Network
Coding. Our approach is to optimize and balance the use of forward error
control, error detection, and retransmissions at the packet level for these
networks. Additionally, we introduce Cooperative Diversity Coding (CDC), which
is a novel means to code the information packets, with the aim of minimizing
the energy consumed for coding operations. The performance of CDC is similar to
CNC in terms of the probability of successful reception at the destination and
expected number of correctly received information packets at the destination.
However, CDC requires less energy at the source node because of its
implementation simplicity. Achieving minimal energy consumption, with the
required level of reliability is critical for the optimum functioning of many
wireless sensor and body area networks. For representative applications, the
optimized CDC or CNC network achieves &gt;= 25% energy savings compared to the
baseline CNC scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1061</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1061</id><created>2014-06-04</created><authors><author><keyname>Novacek</keyname><forenames>Vit</forenames></author></authors><title>A Methodology for Empirical Analysis of LOD Datasets</title><categories>cs.AI cs.SI</categories><comments>A current working draft of the paper submitted to the ISWC'14
  conference (track information available here:
  http://iswc2014.semanticweb.org/call-replication-benchmark-data-software-papers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CoCoE stands for Complexity, Coherence and Entropy, and presents an
extensible methodology for empirical analysis of Linked Open Data (i.e., RDF
graphs). CoCoE can offer answers to questions like: Is dataset A better than B
for knowledge discovery since it is more complex and informative?, Is dataset X
better than Y for simple value lookups due its flatter structure?, etc. In
order to address such questions, we introduce a set of well-founded measures
based on complementary notions from distributional semantics, network analysis
and information theory. These measures are part of a specific implementation of
the CoCoE methodology that is available for download. Last but not least, we
illustrate CoCoE by its application to selected biomedical RDF datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1062</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1062</id><created>2014-06-03</created><authors><author><keyname>Arrobo</keyname><forenames>Gabriel E.</forenames></author><author><keyname>Perumalla</keyname><forenames>Calvin A.</forenames></author><author><keyname>Hanke</keyname><forenames>Stanley B.</forenames></author><author><keyname>Ketterl</keyname><forenames>Thomas P.</forenames></author><author><keyname>Fabri</keyname><forenames>Peter J.</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>An Innovative Wireless Cardiac Rhythm Management (iCRM) System</title><categories>cs.CE cs.SY</categories><doi>10.1109/WTS.2014.6835035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a wireless Communicator to manage and enhance a
Cardiac Rhythm Management System. The system includes: (1) an on-body wireless
Electrocardiogram (ECG), (2) an Intracardiac Electrogram (EGM) embedded inside
an Implantable Cardioverter/Defibrillator, and (3) a Communicator (with a
resident Learning System). The first two devices are existing technology
available in the market and are emulated using data from the Physionet
database, while the Communicator was designed and implemented by our research
team. The value of the information obtained by combining the information
supplied by (1) and (2), presented to the Communicator, improves decision
making regarding use of the actuator or other actions. Preliminary results show
a high level of confidence in the decisions made by the Communicator. For
example, excellent accuracy is achieved in predicting atrial arrhythmia in 8
patients using only external ECG when we used a neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1065</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1065</id><created>2014-06-03</created><updated>2016-03-03</updated><authors><author><keyname>Orthuber</keyname><forenames>Wolfgang</forenames></author></authors><title>Uniform definition of comparable and searchable information on the web</title><categories>cs.IR</categories><comments>36 pages, 20 figures</comments><msc-class>68P05, 68P10, 68P20, 68P30</msc-class><acm-class>C.2.6; H.1.1; H.3.3; E.1; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basically information means selection within a domain (value or definition
set) of possibilities. For objectifiable, comparable and precise information
the domain should be the same for all. Therefore the global (online) definition
of the domain is proposed here. It is advantageous to define an ordered domain,
because this allows using numbers for addressing the elements and because
nature is ordered in many respects. The original data can be ordered in
multiple independent ways. We can define a domain with multiple independent
numeric dimensions to reflect this. Because we want to search information in
the domain, for quantification of similarity we define a distance function or
metric. Therefore we propose &quot;Domain Spaces&quot; (DSs) which are online defined
nestable metric spaces. Their elements are called &quot;Domain Vectors&quot; (DVs) and
have the simple form:
  URL (of common DS definition) plus sequence of numbers
  At this the sequence must be given so that the mapping of numbers to the DS
dimensions is clear. By help of appropriate software DVs can be represented
e.g. as words and numbers. Compared to words, however, DVs have (as original
information) important objectifiable advantages (clear definition, objectivity,
information content, range, resolution, efficiency, searchability). Using DSs
users can define which information they make searchable and how it is
searchable. DSs can be also used to make quantitative (numeric) data as uniform
DVs interoperable, comparable and searchable. The approach is demonstrated in
an online database with search engine (http://NumericSearch.com). The search
procedure is called &quot;Numeric Search&quot;. It consists of two systematic steps: 1.
Selection of the appropriate DS e.g. by conventional word based search within
the DS definitions. 2. Range and/or similarity search of DVs in the selected
DS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1066</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1066</id><created>2014-06-04</created><updated>2015-10-23</updated><authors><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author><author><keyname>Lukarski</keyname><forenames>Dimitar</forenames></author></authors><title>Fast Matlab compatible sparse assembly on multicore computers</title><categories>cs.DC cs.MS cs.NA</categories><msc-class>68W10, 65Y10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and implement in this paper a fast sparse assembly algorithm, the
fundamental operation which creates a compressed matrix from raw index data.
Since it is often a quite demanding and sometimes critical operation, it is of
interest to design a highly efficient implementation. We show how to do this,
and moreover, we show how our implementation can be parallelized to utilize the
power of modern multicore computers. Our freely available code, fully Matlab
compatible, achieves about a factor of 5 times in speedup on a typical 6-core
machine and 10 times on a dual-socket 16 core machine compared to the built-in
serial implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1069</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1069</id><created>2014-06-04</created><updated>2014-08-26</updated><authors><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames><affiliation>Universit&#xe4;t des Saarlandes</affiliation></author><author><keyname>Olderog</keyname><forenames>Ernst-R&#xfc;diger</forenames><affiliation>Carl von Ossietzky Universit&#xe4;t Oldenburg</affiliation></author></authors><title>Petri Games: Synthesis of Distributed Systems with Causal Memory</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 217-230</journal-ref><doi>10.4204/EPTCS.161.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new multiplayer game model for the interaction and the flow of
information in a distributed system. The players are tokens on a Petri net. As
long as the players move in independent parts of the net, they do not know of
each other; when they synchronize at a joint transition, each player gets
informed of the causal history of the other player. We show that for Petri
games with a single environment player and an arbitrary bounded number of
system players, deciding the existence of a safety strategy for the system
players is EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1077</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1077</id><created>2014-06-03</created><authors><author><keyname>Lerma</keyname><forenames>Miguel A.</forenames></author></authors><title>How inefficient can a sort algorithm be?</title><categories>cs.DS</categories><comments>8 pages</comments><msc-class>03D99</msc-class><acm-class>F.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find large lower bounds for a certain family of algorithms, and prove that
such bounds are limited only by natural computability arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1078</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1078</id><created>2014-06-03</created><updated>2014-09-02</updated><authors><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>van Merrienboer</keyname><forenames>Bart</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Bougares</keyname><forenames>Fethi</forenames></author><author><keyname>Schwenk</keyname><forenames>Holger</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>EMNLP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN
encodes a sequence of symbols into a fixed-length vector representation, and
the other decodes the representation into another sequence of symbols. The
encoder and decoder of the proposed model are jointly trained to maximize the
conditional probability of a target sequence given a source sequence. The
performance of a statistical machine translation system is empirically found to
improve by using the conditional probabilities of phrase pairs computed by the
RNN Encoder-Decoder as an additional feature in the existing log-linear model.
Qualitatively, we show that the proposed model learns a semantically and
syntactically meaningful representation of linguistic phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1081</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1081</id><created>2014-06-04</created><authors><author><keyname>Chen</keyname><forenames>Z.</forenames></author><author><keyname>Fan</keyname><forenames>P.</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>Throughput Optimized Multi-Source Cooperative Networks With
  Compute-and-Forward</title><categories>cs.IT math.IT</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate a multi-source multi-cast network with the aid
of an arbitrary number of relays, where it is assumed that no direct link is
available at each S-D pair. The aim is to find the fundamental limit on the
maximal common multicast throughput of all source nodes if resource allocations
are available. A transmission protocol employing the relaying strategy, namely,
compute-and-forward (CPF), is proposed. {We also adjust the methods in the
literature to obtain the integer network-constructed coefficient matrix (a
naive method, a local optimal method as well as a global optimal method) to fit
for the general topology with an arbitrary number of relays. Two transmission
scenarios are addressed. The first scenario is delay-stringent transmission
where each message must be delivered within one slot. The second scenario is
delay-tolerant transmission where no delay constraint is imposed. The
associated optimization problems to maximize the short-term and long-term
common multicast throughput are formulated and solved, and the optimal
allocation of power and time slots are presented. Performance comparisons show
that the CPF strategy outperforms conventional decode-and-forward (DF)
strategy. It is also shown that with more relays, the CPF strategy performs
even better due to the increased diversity. Finally, by simulation, it is
observed that for a large network in relatively high SNR regime, CPF with the
local optimal method for the network-constructed matrix can perform close to
that with the global optimal method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1090</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1090</id><created>2014-06-04</created><authors><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Varghese</keyname><forenames>Thomas</forenames></author></authors><title>Tight Bounds for Complementing Parity Automata</title><categories>cs.FL</categories><comments>Full version of paper accepted for publication at MFCS 2014</comments><journal-ref>Proceedings of MFCS 2014, Springer-Verlag Lecture Notes in
  Computer Science Vol. 8634(1): pp 499-510</journal-ref><doi>10.1007/978-3-662-44522-8_42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We follow a connection between tight determinisation and complementation and
establish a complementation procedure from parity automata to nondeterministic
B\&quot;uchi automata and prove it to be tight up to an $O(n)$ factor, where $n$ is
the size of the nondeterministic parity automaton. This factor does not depend
on the number of priorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1102</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1102</id><created>2014-06-04</created><updated>2015-07-10</updated><authors><author><keyname>Gong</keyname><forenames>Pinghua</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Linear Convergence of Variance-Reduced Stochastic Gradient without
  Strong Convexity</title><categories>cs.NA cs.LG stat.CO stat.ML</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient algorithms estimate the gradient based on only one or a
few samples and enjoy low computational cost per iteration. They have been
widely used in large-scale optimization problems. However, stochastic gradient
algorithms are usually slow to converge and achieve sub-linear convergence
rates, due to the inherent variance in the gradient computation. To accelerate
the convergence, some variance-reduced stochastic gradient algorithms, e.g.,
proximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have
recently been proposed to solve strongly convex problems. Under the strongly
convex condition, these variance-reduced stochastic gradient algorithms achieve
a linear convergence rate. However, many machine learning problems are convex
but not strongly convex. In this paper, we introduce Prox-SVRG and its
projected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)
to solve a class of non-strongly convex optimization problems widely used in
machine learning. As the main technical contribution of this paper, we show
that both VRPSG and Prox-SVRG achieve a linear convergence rate without strong
convexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)
inequality which is the first to be rigorously proved for a class of
non-strongly convex problems in both constrained and regularized settings.
Moreover, the SSC inequality is independent of algorithms and may be applied to
analyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which
may be of independent interest. To the best of our knowledge, this is the first
work that establishes the linear convergence rate for the variance-reduced
stochastic gradient algorithms on solving both constrained and regularized
problems without strong convexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1111</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1111</id><created>2014-06-04</created><authors><author><keyname>Calvert</keyname><forenames>Wesley</forenames></author></authors><title>PAC Learning, VC Dimension, and the Arithmetic Hierarchy</title><categories>math.LO cs.LG cs.LO</categories><msc-class>03D80, 03D45</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute that the index set of PAC-learnable concept classes is
$m$-complete $\Sigma^0_3$ within the set of indices for all concept classes of
a reasonable form. All concept classes considered are computable enumerations
of computable $\Pi^0_1$ classes, in a sense made precise here. This family of
concept classes is sufficient to cover all standard examples, and also has the
property that PAC learnability is equivalent to finite VC dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1120</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1120</id><created>2014-06-02</created><authors><author><keyname>sudhakar</keyname><forenames>Yuva</forenames></author><author><keyname>Tiwari</keyname><forenames>P. M.</forenames></author></authors><title>Online Rotor Resistance Adaptation Of Induction Motor Drive</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an adaptive control systems scheme is used to update the rotor
resistance of an induction motor drive. Rotor resistance of the induction motor
drive is dependent on the temperature where it is installed for one Induced
Draft Fan (I.D Fan) in one of the 500 MW generating thermal power plant. The
rotor resistance adaptation is of induction motor drive is achieved through
Indirect Field Oriented. The desired value of the rotor flux along the q - axis
should ideally be zero. This condition acts as a reference model for the
proposed Adaptive Control scheme. Inductance does not change with the
temperature. Therefore, the only parameter which changes with the temperature
in the adjustable model is the rotor resistance. This resistance is adjusted
such that the flux along the q - axis is driven to zero. The proposed method
effectively adjusts the rotor resistance on-line and keeps the machine field
oriented. The effectiveness of the proposed scheme is tested through simulation
and is experimentally verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1128</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1128</id><created>2014-06-04</created><authors><author><keyname>Placzek</keyname><forenames>Bartlomiej</forenames></author></authors><title>A self-organizing system for urban traffic control based on predictive
  interval microscopic model</title><categories>cs.AI cs.SY</categories><comments>29 pages, 8 figures</comments><journal-ref>Engineering Applications of Artificial Intelligence, vol. 34, pp.
  75-84, 2014</journal-ref><doi>10.1016/j.engappai.2014.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a self-organizing traffic signal system for an urban
road network. The key elements of this system are agents that control traffic
signals at intersections. Each agent uses an interval microscopic traffic model
to predict effects of its possible control actions in a short time horizon. The
executed control action is selected on the basis of predicted delay intervals.
Since the prediction results are represented by intervals, the agents can
recognize and suspend those control actions, whose positive effect on the
performance of traffic control is uncertain. Evaluation of the proposed traffic
control system was performed in a simulation environment. The simulation
experiments have shown that the proposed approach results in an improved
performance, particularly for non-uniform traffic streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1133</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1133</id><created>2014-06-04</created><authors><author><keyname>Marinho</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Petters</keyname><forenames>Stefan M.</forenames></author></authors><title>Timing Analysis for DAG-based and GFP Scheduled Tasks</title><categories>cs.DC cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern embedded systems have made the transition from single-core to
multi-core architectures, providing performance improvement via parallelism
rather than higher clock frequencies. DAGs are considered among the most
generic task models in the real-time domain and are well suited to exploit this
parallelism. In this paper we provide a schedulability test using response-time
analysis exploiting exploring and bounding the self interference of a DAG task.
Additionally we bound the interference a high priority task has on lower
priority ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1134</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1134</id><created>2014-06-04</created><updated>2014-11-03</updated><authors><author><keyname>Nam</keyname><forenames>Woonhyun</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author><author><keyname>Han</keyname><forenames>Joon Hee</forenames></author></authors><title>Local Decorrelation For Improved Detection</title><categories>cs.CV</categories><comments>To appear in Neural Information Processing Systems (NIPS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even with the advent of more sophisticated, data-hungry methods, boosted
decision trees remain extraordinarily successful for fast rigid object
detection, achieving top accuracy on numerous datasets. While effective, most
boosted detectors use decision trees with orthogonal (single feature) splits,
and the topology of the resulting decision boundary may not be well matched to
the natural topology of the data. Given highly correlated data, decision trees
with oblique (multiple feature) splits can be effective. Use of oblique splits,
however, comes at considerable computational expense. Inspired by recent work
on discriminative decorrelation of HOG features, we instead propose an
efficient feature transform that removes correlations in local neighborhoods.
The result is an overcomplete but locally decorrelated representation ideally
suited for use with orthogonal decision trees. In fact, orthogonal trees with
our locally decorrelated features outperform oblique trees trained over the
original features at a fraction of the computational cost. The overall
improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we
reduce false positives nearly tenfold over the previous state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1137</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1137</id><created>2014-06-04</created><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Tianyi</forenames></author><author><keyname>Wang</keyname><forenames>Bolun</forenames></author><author><keyname>Sambasivan</keyname><forenames>Divya</forenames></author><author><keyname>Zhang</keyname><forenames>Zengbin</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Crowds on Wall Street: Extracting Value from Social Investing Platforms</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For decades, the world of financial advisors has been dominated by large
investment banks such as Goldman Sachs. In recent years, user-contributed
investment services such as SeekingAlpha and StockTwits have grown to millions
of users. In this paper, we seek to understand the quality and impact of
content on social investment platforms, by empirically analyzing complete
datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years).
We develop sentiment analysis tools and correlate contributed content to the
historical performance of relevant stocks. While SeekingAlpha articles and
StockTwits messages provide minimal correlation to stock performance in
aggregate, a subset of authors contribute more valuable (predictive) content.
We show that these authors can be identified via both empirical methods or by
user interactions, and investments using their analysis significantly
outperform broader markets. Finally, we conduct a user survey that sheds light
on users views of SeekingAlpha content and stock manipulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1140</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1140</id><created>2014-06-04</created><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Lim</keyname><forenames>Tengjoon</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>Fading Two-Way Relay Channels: Physical-Layer Versus Digital Network
  Coding</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider three transmit strategies for the fading
three-node, two-way relay network (TWRN) -- physical-layer network coding
(PNC), digital network coding (DNC) and codeword superposition (CW-Sup). The
aim is to minimize the total average energy needed to deliver a given pair of
required average rates. Full channel state information is assumed to be
available at all transmitters and receivers. The optimization problems
corresponding to the various strategies in fading channels are formulated,
solved and compared. For the DNC-based strategies, a simple time sharing of
transmission of the network-coded message and the remaining bits of the larger
message (DNC-TS) is considered first. We extend this approach to include a
superposition strategy (DNC-Sup), in which the network-coded message and the
remainder of the longer source message are superimposed before transmission. It
is demonstrated theoretically that DNC-Sup outperforms DNC-TS and CW-Sup in
terms of total average energy usage. More importantly, it is shown in
simulation that DNC-Sup performs better than PNC if the required rate is low
and worse otherwise. Finally, an algorithm to select the optimal strategy in
terms of energy usage subject to different rate pair requirements is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1143</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1143</id><created>2014-06-04</created><authors><author><keyname>Weissman</keyname><forenames>Sarah</forenames></author><author><keyname>Ayhan</keyname><forenames>Samet</forenames></author><author><keyname>Bradley</keyname><forenames>Joshua</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author></authors><title>Identifying Duplicate and Contradictory Information in Wikipedia</title><categories>cs.IR cs.CL cs.DL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our study identifies sentences in Wikipedia articles that are either
identical or highly similar by applying techniques for near-duplicate detection
of web pages. This is accomplished with a MapReduce implementation of minhash
to identify clusters of sentences with high Jaccard similarity. We show that
these clusters can be categorized into six different types, two of which are
particularly interesting: identical sentences quantify the extent to which
content in Wikipedia is copied and pasted, and near-duplicate sentences that
state contradictory facts point to quality issues in Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1154</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1154</id><created>2014-06-04</created><updated>2014-09-21</updated><authors><author><keyname>Tams</keyname><forenames>Benjamin</forenames></author></authors><title>Decodability Attack against the Fuzzy Commitment Scheme with Public
  Feature Transforms</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fuzzy commitment scheme is a cryptographic primitive that can be used to
store biometric templates being encoded as fixed-length feature vectors
protected. If multiple related records generated from the same biometric
instance can be intercepted, their correspondence can be determined using the
decodability attack. In 2011, Kelkboom et al. proposed to pass the feature
vectors through a record-specific but public permutation process in order to
prevent this attack. In this paper, it is shown that this countermeasure
enables another attack also analyzed by Simoens et al. in 2009 which can even
ease an adversary to fully break two related records. The attack may only be
feasible if the protected feature vectors have a reasonably small Hamming
distance; yet, implementations and security analyses must account for this
risk. This paper furthermore discusses that by means of a public
transformation, the attack cannot be prevented in a binary fuzzy commitment
scheme based on linear codes. Fortunately, such transformations can be
generated for the non-binary case. In order to still be able to protect binary
feature vectors, one may consider to use the improved fuzzy vault scheme by
Dodis et al. which may be secured against linkability attacks using
observations made by Merkle and Tams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1158</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1158</id><created>2014-06-04</created><updated>2015-01-11</updated><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Komosa</keyname><forenames>Pawel</forenames></author><author><keyname>Mach</keyname><forenames>Lukas</forenames></author></authors><title>Kernelization lower bound for Permutation Pattern Matching</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A permutation $\pi$ contains a permutation $\sigma$ as a pattern if it
contains a subsequence of length $|\sigma|$ whose elements are in the same
relative order as in the permutation $\sigma$. This notion plays a major role
in enumerative combinatorics. We prove that the problem does not have a
polynomial kernel (under the widely believed complexity assumption $\mbox{NP}
\not\subseteq \mbox{co-NP}/\mbox{poly}$) by introducing a new polynomial
reduction from the clique problem to permutation pattern matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1167</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1167</id><created>2014-06-04</created><authors><author><keyname>Yin</keyname><forenames>Xu-Cheng</forenames></author><author><keyname>Yang</keyname><forenames>Chun</forenames></author><author><keyname>Hao</keyname><forenames>Hong-Wei</forenames></author></authors><title>Learning to Diversify via Weighted Kernels for Classifier Ensemble</title><categories>cs.LG cs.CV</categories><comments>Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence
  (TPAMI)</comments><acm-class>I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifier ensemble generally should combine diverse component classifiers.
However, it is difficult to give a definitive connection between diversity
measure and ensemble accuracy. Given a list of available component classifiers,
how to adaptively and diversely ensemble classifiers becomes a big challenge in
the literature. In this paper, we argue that diversity, not direct diversity on
samples but adaptive diversity with data, is highly correlated to ensemble
accuracy, and we propose a novel technology for classifier ensemble, learning
to diversify, which learns to adaptively combine classifiers by considering
both accuracy and diversity. Specifically, our approach, Learning TO Diversify
via Weighted Kernels (L2DWK), performs classifier combination by optimizing a
direct but simple criterion: maximizing ensemble accuracy and adaptive
diversity simultaneously by minimizing a convex loss function. Given a measure
formulation, the diversity is calculated with weighted kernels (i.e., the
diversity is measured on the component classifiers' outputs which are kernelled
and weighted), and the kernel weights are automatically learned. We minimize
this loss function by estimating the kernel weights in conjunction with the
classifier weights, and propose a self-training algorithm for conducting this
convex optimization procedure iteratively. Extensive experiments on a variety
of 32 UCI classification benchmark datasets show that the proposed approach
consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost,
Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via
Semi-Definite Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1169</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1169</id><created>2014-06-04</created><updated>2014-08-27</updated><authors><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>On The Impact of Time-Varying Interference-Channel on the Spatial
  Approach of Spectrum Sharing between S-band Radar and Communication System</title><categories>cs.IT math.IT</categories><comments>Accepted version: IEEE Military Communications Conference (MILCOM)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sharing is a new approach to solve the congestion problem in the RF
spectrum. A spatial approach for spectrum sharing between a radar and a
communication system was proposed, which mitigates the radar interference to
communication by projecting the radar waveform onto the null space of the
interference channel, between radar and communication system [1]. In this work,
we extend this approach to a maritime MIMO radar which experiences a time
varying interference channel due to the oscillatory motion of a ship, because
of the breaking of sea/ocean waves. We model this variation by using the matrix
perturbation theory and the statistical distribution of the breaking waves.
This model is then used to study the impact of perturbed interference channel
on the spatial approach of spectrum sharing. We use the maximum likelihood (ML)
estimate of a target's angle of arrival to study the radar's performance when
its waveform is projected onto the null space of the perturbed interference
channel. Through our analytical and simulation results, we study the loss in
the radar's performance due to the null space projection (NSP) of its waveform
on the perturbed interference channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1197</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1197</id><created>2014-06-04</created><updated>2015-01-05</updated><authors><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Mastrandrea</keyname><forenames>Rossana</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Unbiased sampling of network ensembles</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>MatLab code available at
  http://www.mathworks.it/matlabcentral/fileexchange/46912-max-sam-package-zip</comments><journal-ref>New J. Phys. 17, 023052 (2015)</journal-ref><doi>10.1088/1367-2630/17/2/023052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling random graphs with given properties is a key step in the analysis of
networks, as random ensembles represent basic null models required to identify
patterns such as communities and motifs. An important requirement is that the
sampling process is unbiased and efficient. The main approaches are
microcanonical, i.e. they sample graphs that match the enforced constraints
exactly. Unfortunately, when applied to strongly heterogeneous networks (like
most real-world examples), the majority of these approaches become biased
and/or time-consuming. Moreover, the algorithms defined in the simplest cases,
such as binary graphs with given degrees, are not easily generalizable to more
complicated ensembles. Here we propose a solution to the problem via the
introduction of a &quot;Maximize and Sample&quot; (&quot;Max &amp; Sam&quot; for short) method to
correctly sample ensembles of networks where the constraints are `soft', i.e.
realized as ensemble averages. Our method is based on exact maximum-entropy
distributions and is therefore unbiased by construction, even for strongly
heterogeneous networks. It is also more computationally efficient than most
microcanonical alternatives. Finally, it works for both binary and weighted
networks with a variety of constraints, including combined degree-strength
sequences and full reciprocity structure, for which no alternative method
exists. Our canonical approach can in principle be turned into an unbiased
microcanonical one, via a restriction to the relevant subset. Importantly, the
analysis of the fluctuations of the constraints suggests that the
microcanonical and canonical versions of all the ensembles considered here are
not equivalent. We show various real-world applications and provide a code
implementing all our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1203</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1203</id><created>2014-06-04</created><authors><author><keyname>Bhartiya</keyname><forenames>Divyanshu</forenames></author><author><keyname>Singh</keyname><forenames>Ashudeep</forenames></author></authors><title>A Semantic Approach to Summarization</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentence extraction based summarization methods has some limitations as it
doesn't go into the semantics of the document. Also, it lacks the capability of
sentence generation which is intuitive to humans. Here we present a novel
method to summarize text documents taking the process to semantic levels with
the use of WordNet and other resources, and using a technique for sentence
generation. We involve semantic role labeling to get the semantic
representation of text and use of segmentation to form clusters of the related
pieces of text. Picking out the centroids and sentence generation completes the
task. We evaluate our system against human composed summaries and also present
an evaluation done by humans to measure the quality attributes of our
summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1213</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1213</id><created>2014-06-04</created><authors><author><keyname>Hanspach</keyname><forenames>Michael</forenames></author><author><keyname>Goetz</keyname><forenames>Michael</forenames></author></authors><title>On Covert Acoustical Mesh Networks in Air</title><categories>cs.CR cs.NI cs.SD</categories><comments>10 pages, updated version</comments><journal-ref>Journal of Communications 8(11), Nov. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covert channels can be used to circumvent system and network policies by
establishing communications that have not been considered in the design of the
computing system. We construct a covert channel between different computing
systems that utilizes audio modulation/demodulation to exchange data between
the computing systems over the air medium. The underlying network stack is
based on a communication system that was originally designed for robust
underwater communication. We adapt the communication system to implement covert
and stealthy communications by utilizing the ultrasonic frequency range. We
further demonstrate how the scenario of covert acoustical communication over
the air medium can be extended to multi-hop communications and even to wireless
mesh networks. A covert acoustical mesh network can be conceived as a meshed
botnet or malnet that is accessible via inaudible audio transmissions.
Different applications of covert acoustical mesh networks are presented,
including the use for remote keylogging over multiple hops. It is shown that
the concept of a covert acoustical mesh network renders many conventional
security concepts useless, as acoustical communications are usually not
considered. Finally, countermeasures against covert acoustical mesh networks
are discussed, including the use of lowpass filtering in computing systems and
a host-based intrusion detection system for analyzing audio input and output in
order to detect any irregularities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1215</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1215</id><created>2014-06-04</created><updated>2015-05-25</updated><authors><author><keyname>Alam</keyname><forenames>Maksudul</forenames></author><author><keyname>Khan</keyname><forenames>Maleq</forenames></author></authors><title>Parallel Algorithms for Generating Random Networks with Given Degree
  Sequences</title><categories>cs.DC</categories><comments>Accepted in NPC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random networks are widely used for modeling and analyzing complex processes.
Many mathematical models have been proposed to capture diverse real-world
networks. One of the most important aspects of these models is degree
distribution. Chung--Lu (CL) model is a random network model, which can produce
networks with any given arbitrary degree distribution. The complex systems we
deal with nowadays are growing larger and more diverse than ever. Generating
random networks with any given degree distribution consisting of billions of
nodes and edges or more has become a necessity, which requires efficient and
parallel algorithms. We present an MPI-based distributed memory parallel
algorithm for generating massive random networks using CL model, which takes
$O(\frac{m+n}{P}+P)$ time with high probability and $O(n)$ space per processor,
where $n$, $m$, and $P$ are the number of nodes, edges and processors,
respectively. The time efficiency is achieved by using a novel load-balancing
algorithm. Our algorithms scale very well to a large number of processors and
can generate massive power--law networks with one billion nodes and $250$
billion edges in one minute using $1024$ processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1222</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1222</id><created>2014-06-04</created><updated>2014-10-30</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Discovering Structure in High-Dimensional Data Through Correlation
  Explanation</title><categories>cs.LG cs.AI stat.ML</categories><comments>15 pages, 6 figures. Includes supplementary material and link to
  code. Published in the proceedings of the 28th Annual Conference on Neural
  Information Processing Systems, NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method to learn a hierarchy of successively more abstract
representations of complex data based on optimizing an information-theoretic
objective. Intuitively, the optimization searches for a set of latent factors
that best explain the correlations in the data as measured by multivariate
mutual information. The method is unsupervised, requires no model assumptions,
and scales linearly with the number of variables which makes it an attractive
approach for very high dimensional systems. We demonstrate that Correlation
Explanation (CorEx) automatically discovers meaningful structure for data from
diverse sources including personality tests, DNA, and human language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1224</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1224</id><created>2014-06-04</created><authors><author><keyname>Li</keyname><forenames>Xuhui</forenames></author><author><keyname>Liu</keyname><forenames>Mengchi</forenames></author><author><keyname>Zhu</keyname><forenames>Shanfeng</forenames></author><author><keyname>Ghafoor</keyname><forenames>Arif</forenames></author></authors><title>XTQ: A Declarative Functional XML Query Language</title><categories>cs.PL cs.DB</categories><comments>65 pages</comments><acm-class>H.2.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Various query languages have been proposed to extract and restructure
information in XML documents. These languages, usually claiming to be
declarative, mainly consider the conjunctive relationships among data elements.
In order to present the operations where the hierarchical and the disjunctive
relationships need to be considered, such as restructuring hierarchy and
handling heterogeneity, the programs in these languages often exhibit a
procedural style and thus the declarativeness in them is not so prominent as in
conventional query languages like SQL.
  In this paper, we propose a declarative pattern-based functional XML query
language named XML Tree Query (XTQ). XTQ adopts expressive composite patterns
to present data extraction, meanwhile establishing the conjunctive, the
disjunctive and the hierarchical relationships among data elements. It uses the
matching terms, a composite structure of the variables bound to the matched
data elements, to present a global sketch of the extracted data, and develops a
deductive restructuring mechanism of matching terms to indicate data
transformation, especially for restructuring hierarchy and handling
heterogeneity. Based on matching terms, XTQ employs a coherent approach to
function declaration and invocation to consistently extract and construct
composite data structure, which integrates features of conventional functional
languages and pattern-based query languages. Additionally, XTQ also supports
data filtering on composite data structure such as hierarchical data, which is
seldom deliberately considered in other studies. We demonstrate with various
examples that XTQ can declaratively present complex XML queries which are
common in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1230</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1230</id><created>2014-06-04</created><authors><author><keyname>Akl</keyname><forenames>Naeem</forenames></author><author><keyname>Fahs</keyname><forenames>Jihad</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author></authors><title>Statistical Intercell Interference Modeling for Capacity-Coverage
  Tradeoff Analysis in Downlink Cellular Networks</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages, 7 figures, conference</comments><doi>10.1109/ICCSPA.2015.7081269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference shapes the interplay between capacity and coverage in cellular
networks. However, interference is non-deterministic and depends on various
system and channel parameters including user scheduling, frequency reuse, and
fading variations. We present an analytical approach for modeling the
distribution of intercell interference in the downlink of cellular networks as
a function of generic fading channel models and various scheduling schemes. We
demonstrate the usefulness of the derived expressions in calculating
location-based and average-based data rates in addition to capturing practical
tradeoffs between cell capacity and coverage in downlink cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1231</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1231</id><created>2014-06-04</created><authors><author><keyname>Dahl</keyname><forenames>George E.</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Multi-task Neural Networks for QSAR Predictions</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although artificial neural networks have occasionally been used for
Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in
the past, the literature has of late been dominated by other machine learning
techniques such as random forests. However, a variety of new neural net
techniques along with successful applications in other domains have renewed
interest in network approaches. In this work, inspired by the winning team's
use of neural networks in a recent QSAR competition, we used an artificial
neural network to learn a function that predicts activities of compounds for
multiple assays at the same time. We conducted experiments leveraging recent
methods for dealing with overfitting in neural networks as well as other tricks
from the neural networks literature. We compared our methods to alternative
methods reported to perform well on these tasks and found that our neural net
methods provided superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1234</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1234</id><created>2014-06-04</created><authors><author><keyname>Lijiang</keyname><forenames>Chen</forenames></author></authors><title>A Geometric Method to Obtain the Generation Probability of a Sentence</title><categories>cs.CL cs.AI math.ST stat.CO stat.ME stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;How to generate a sentence&quot; is the most critical and difficult problem in
all the natural language processing technologies. In this paper, we present a
new approach to explain the generation process of a sentence from the
perspective of mathematics. Our method is based on the premise that in our
brain a sentence is a part of a word network which is formed by many word
nodes. Experiments show that the probability of the entire sentence can be
obtained by the probabilities of single words and the probabilities of the
co-occurrence of word pairs, which indicate that human use the synthesis method
to generate a sentence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1238</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1238</id><created>2014-06-04</created><authors><author><keyname>Verbeek</keyname><forenames>Freek</forenames><affiliation>Open University of The Netherlands</affiliation></author><author><keyname>Schmaltz</keyname><forenames>Julien</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Proceedings Twelfth International Workshop on the ACL2 Theorem Prover
  and its Applications</title><categories>cs.LO cs.MS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014</journal-ref><doi>10.4204/EPTCS.152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Twelfth International Workshop on
the ACL2 Theorem Prover and Its Applications, ACL2'14, a two-day workshop held
in Vienna, Austria, on July 12-13, 2014. ACL2 workshops occur at approximately
18-month intervals and provide a major technical forum for researchers to
present and discuss improvements and extensions to the theorem prover,
comparisons of ACL2 with other systems, and applications of ACL2 in formal
verification. These proceedings include 13 peer reviewed technical papers.
  ACL2 is a state-of-the-art automated reasoning system that has been
successfully applied in academia, government, and industry for specification
and verification of computing systems and in teaching computer science courses.
In 2005, Boyer, Kaufmann, and Moore were awarded the 2005 ACM Software System
Award for their work in ACL2 and the other theorem provers in the Boyer-Moore
family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1241</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1241</id><created>2014-06-04</created><authors><author><keyname>El-Shishtawy</keyname><forenames>T.</forenames></author><author><keyname>El-Sammak</keyname><forenames>A.</forenames></author></authors><title>The Best Templates Match Technique For Example Based Machine Translation</title><categories>cs.CL</categories><comments>Eleventh International Conference on Artificial Intelligence
  Applications, 2003</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It has been proved that large scale realistic Knowledge Based Machine
Translation applications require acquisition of huge knowledge about language
and about the world. This knowledge is encoded in computational grammars,
lexicons and domain models. Another approach which avoids the need for
collecting and analyzing massive knowledge, is the Example Based approach,
which is the topic of this paper. We show through the paper that using Example
Based in its native form is not suitable for translating into Arabic. Therefore
a modification to the basic approach is presented to improve the accuracy of
the translation process. The basic idea of the new approach is to improve the
technique by which template-based approaches select the appropriate templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1244</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1244</id><created>2014-06-04</created><authors><author><keyname>Hochuli</keyname><forenames>Alexandra</forenames></author><author><keyname>Holzer</keyname><forenames>Stephan</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Distributed Approximation of Minimum Routing Cost Trees</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the NP-hard problem of approximating a Minimum Routing Cost Spanning
Tree in the message passing model with limited bandwidth (CONGEST model). In
this problem one tries to find a spanning tree of a graph $G$ over $n$ nodes
that minimizes the sum of distances between all pairs of nodes. In the
considered model every node can transmit a different (but short) message to
each of its neighbors in each synchronous round. We provide a randomized
$(2+\epsilon)$-approximation with runtime $O(D+\frac{\log n}{\epsilon})$ for
unweighted graphs. Here, $D$ is the diameter of $G$. This improves over both,
the (expected) approximation factor $O(\log n)$ and the runtime $O(D\log^2 n)$
of the best previously known algorithm.
  Due to stating our results in a very general way, we also derive an (optimal)
runtime of $O(D)$ when considering $O(\log n)$-approximations as done by the
best previously known algorithm. In addition we derive a deterministic
$2$-approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1247</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1247</id><created>2014-06-04</created><authors><author><keyname>Yi</keyname><forenames>Dong</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Liao</keyname><forenames>Shengcai</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Shared Representation Learning for Heterogeneous Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After intensive research, heterogenous face recognition is still a
challenging problem. The main difficulties are owing to the complex
relationship between heterogenous face image spaces. The heterogeneity is
always tightly coupled with other variations, which makes the relationship of
heterogenous face images highly nonlinear. Many excellent methods have been
proposed to model the nonlinear relationship, but they apt to overfit to the
training set, due to limited samples. Inspired by the unsupervised algorithms
in deep learning, this paper proposes an novel framework for heterogeneous face
recognition. We first extract Gabor features at some localized facial points,
and then use Restricted Boltzmann Machines (RBMs) to learn a shared
representation locally to remove the heterogeneity around each facial point.
Finally, the shared representations of local RBMs are connected together and
processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases
are selected to evaluate the proposed method. For Sketch-Photo problem, we
obtain perfect results on the CUFS database. For NIR-VIS problem, we produce
new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1253</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1253</id><created>2014-06-04</created><authors><author><keyname>Borggaard</keyname><forenames>Jeff T.</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author></authors><title>Model Reduction for DAEs with an Application to Flow Control</title><categories>math.DS cs.NA cs.SY math.NA</categories><comments>Accepted to appear in Active Flow and Combustion Control 2014,
  Springer-Verlag, in press</comments><journal-ref>Active Flow and Combustion Control 2014, R. King editors,
  Springer-Verlag, Notes on Numerical Fluid Mechanics and Multidisciplinary
  Design, Vol. 127, (ISBN 978-3-319-11966-3), pp. 381-396, 2015</journal-ref><doi>10.1007/978-3-319-11967-0_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct numerical simulation of dynamical systems is of fundamental importance
in studying a wide range of complex physical phenomena. However, the
ever-increasing need for accuracy leads to extremely large-scale dynamical
systems whose simulations impose huge computational demands. Model reduction
offers one remedy to this problem by producing simpler reduced models that are
both easier to analyze and faster to simulate while accurately replicating the
original behavior. Interpolatory model reduction methods have emerged as
effective candidates for very large-scale problems due to their ability to
produce high-fidelity (optimal in some cases) reduced models for linear and
bilinear dynamical systems with modest computational cost. In this paper, we
will briefly review the interpolation framework for model reduction and
describe a well studied flow control problem that requires model reduction of a
large scale system of differential algebraic equations. We show that
interpolatory model reduction produces a feedback control strategy that matches
the structure of much more expensive control design methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1255</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1255</id><created>2014-06-04</created><authors><author><keyname>Xu</keyname><forenames>Yuedong</forenames></author><author><keyname>Elayoubi</keyname><forenames>Salaheddine</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>El-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Yu</keyname><forenames>Yinghao</forenames></author></authors><title>Flow Level QoE of Video Streaming in Wireless Networks</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quality of Experience (QoE) of streaming service is often degraded by
frequent playback interruptions. To mitigate the interruptions, the media
player prefetches streaming contents before starting playback, at a cost of
delay. We study the QoE of streaming from the perspective of flow dynamics.
First, a framework is developed for QoE when streaming users join the network
randomly and leave after downloading completion. We compute the distribution of
prefetching delay using partial differential equations (PDEs), and the
probability generating function of playout buffer starvations using ordinary
differential equations (ODEs) for CBR streaming. Second, we extend our
framework to characterize the throughput variation caused by opportunistic
scheduling at the base station, and the playback variation of VBR streaming.
Our study reveals that the flow dynamics is the fundamental reason of playback
starvation. The QoE of streaming service is dominated by the first moments such
as the average throughput of opportunistic scheduling and the mean playback
rate. While the variances of throughput and playback rate have very limited
impact on starvation behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1256</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1256</id><created>2014-06-04</created><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques E.</forenames></author></authors><title>Output-Feedback Control of Nonlinear Systems using Control Contraction
  Metrics and Convex Optimization</title><categories>math.OC cs.SY</categories><comments>Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control contraction metrics (CCMs) are a new approach to nonlinear control
design based on contraction theory. The resulting design problems are expressed
as pointwise linear matrix inequalities and are and well-suited to solution via
convex optimization. In this paper, we extend the theory on CCMs by showing
that a pair of &quot;dual&quot; observer and controller problems can be solved using
pointwise linear matrix inequalities, and that when a solution exists a
separation principle holds. That is, a stabilizing output-feedback controller
can be found. The procedure is demonstrated using a benchmark problem of
nonlinear control: the Moore-Greitzer jet engine compressor model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1265</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1265</id><created>2014-06-05</created><authors><author><keyname>Jung</keyname><forenames>Yoon Mo</forenames></author><author><keyname>Shen</keyname><forenames>Jianhong Jackie</forenames></author></authors><title>Illusory Shapes via Phase Transition</title><categories>math.OC cs.CV q-bio.NC</categories><comments>New Work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new variational illusory shape (VIS) model via phase fields and
phase transitions. It is inspired by the first-order variational illusory
contour (VIC) model proposed by Jung and Shen [{\em J. Visual Comm. Image
Repres.}, {\bf 19}:42-55, 2008]. Under the new VIS model, illusory shapes are
represented by phase values close to 1 while the rest by values close to 0. The
0-1 transition is achieved by an elliptic energy with a double-well potential,
as in the theory of $\Gamma$-convergence. The VIS model is non-convex, with the
zero field as its trivial global optimum. To seek visually meaningful local
optima that can induce illusory shapes, an iterative algorithm is designed and
its convergence behavior is closely studied. Several generic numerical examples
confirm the versatility of the model and the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1273</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1273</id><created>2014-06-05</created><updated>2014-10-09</updated><authors><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author><author><keyname>Rosenbaum</keyname><forenames>Will</forenames></author></authors><title>On The Communication Complexity of Finding an (Approximate) Stable
  Marriage</title><categories>cs.CC cs.GT</categories><comments>This paper has been subsumed by arXiv:1405.7709</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the communication complexity of protocols that
compute stable matchings. We work within the context of Gale and Shapley's
original stable marriage problem\cite{GS62}: $n$ men and $n$ women each
privately hold a total and strict ordering on all of the members of the
opposite gender. They wish to collaborate in order to find a stable
matching---a pairing of the men and women such that no unmatched pair mutually
prefer each other to their assigned partners in the matching. We show that any
communication protocol (deterministic, nondeterministic, or randomized) that
correctly ouputs a stable matching requires $\Omega(n^2)$ bits of
communication. Thus, the original algorithm of Gale and Shapley is
communication-optimal up to a logarithmic factor. We then introduce a &quot;divorce
metric&quot; on the set of all matchings, which allows us to consider approximately
stable matchings. We describe an efficient algorithm to compute the &quot;distance
to stability&quot; of a given matching. We then show that even under the relaxed
requirement that a protocol only yield an approximate stable matching, the
$\Omega(n^2)$ communication lower bound still holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1275</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1275</id><created>2014-06-05</created><authors><author><keyname>Hossain</keyname><forenames>Ashraf</forenames></author><author><keyname>Mishra</keyname><forenames>Rashmita</forenames></author></authors><title>Sensing and Link Model for Wireless Sensor Network: Coverage and
  Connectivity Analysis</title><categories>cs.IT math.IT</categories><comments>5 pages, conference EAPE 2013, 2nd National Conference EAPE 2013,
  Kolkata, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage and connectivity both are important in wireless sensor network
(WSN). Coverage means how well an area of interest is being monitored by the
deployed network. It depends on sensing model that has been used to design the
network model. Connectivity ensures the establishment of a wireless link
between two nodes. A link model studies the connectivity between two nodes. The
probability of establishing a wireless link between two nodes is a
probabilistic phenomenon. The connectivity between two nodes plays an important
role in the determination of network connectivity. In this paper, we
investigate the impact of sensing model of nodes on the network coverage. Also,
we investigate the dependency of the connectivity and coverage on the shadow
fading parameters. It has been observed that shadowing effect reduces the
network coverage while it enhances connectivity in a multi-hop wireless
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1278</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1278</id><created>2014-06-05</created><updated>2014-12-29</updated><authors><author><keyname>Zeng</keyname><forenames>William</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames><affiliation>University of Singapore and University of Oxford</affiliation></author></authors><title>Abstract structure of unitary oracles for quantum algorithms</title><categories>quant-ph cs.LO math.CT</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 270-284</journal-ref><doi>10.4204/EPTCS.172.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a pair of complementary dagger-Frobenius algebras, equipped with
a self-conjugate comonoid homomorphism onto one of the algebras, produce a
nontrivial unitary morphism on the product of the algebras. This gives an
abstract understanding of the structure of an oracle in a quantum computation,
and we apply this understanding to develop a new algorithm for the
deterministic identification of group homomorphisms into abelian groups. We
also discuss an application to the categorical theory of signal-flow networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1280</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1280</id><created>2014-06-05</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>Laxminarayana</keyname><forenames>M</forenames></author></authors><title>Basis Identification for Automatic Creation of Pronunciation Lexicon for
  Proper Names</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of a proper names pronunciation lexicon is usually a manual
effort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,
in literature, are usually rule based and work best for non-proper names in a
particular language. Proper names are foreign to a G2P module. We follow an
optimization approach to enable automatic construction of proper names
pronunciation lexicon. The idea is to construct a small orthogonal set of words
(basis) which can span the set of names in a given database. We propose two
algorithms for the construction of this basis. The transcription lexicon of all
the proper names in a database can be produced by the manual transcription of
only the small set of basis words. We first construct a cost function and show
that the minimization of the cost function results in a basis. We derive
conditions for convergence of this cost function and validate them
experimentally on a very large proper name database. Experiments show the
transcription can be achieved by transcribing a set of small number of basis
words. The algorithms proposed are generic and independent of language; however
performance is better if the proper names have same origin, namely, same
language or geographical region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1281</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1281</id><created>2014-06-05</created><authors><author><keyname>Tufekci</keyname><forenames>Nesibe</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author></authors><title>On codes over R_{k,m} and constructions for new binary self-dual codes</title><categories>cs.IT math.IT</categories><comments>17 pages</comments><msc-class>94B05, 94B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study codes over the ring
R_{k,m}=F_2[u,v]/&lt;u^{k},v^{m},uv-vu&gt;, which is a family of Frobenius,
characteristic 2 extensions of the binary field. We introduce a distance and
duality preserving Gray map from R_{k,m} to F_2^{km} together with a Lee
weight. After proving the MacWilliams identities for codes over R_{k,m} for all
the relevant weight enumerators, we construct many binary self-dual codes as
the Gray images of self-dual codes over R_{k,m}. In addition to many extremal
binary self-dual codes obtained in this way, including a new construction for
the extended binary Golay code, we find 175 new Type I binary self-dual codes
of parameters [72,36,12] and 105 new Type II binary self-dual codes of
parameter [72,36,12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1284</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1284</id><created>2014-06-05</created><authors><author><keyname>Azimdoost</keyname><forenames>Bita</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Sadjadpour</keyname><forenames>Hamid R.</forenames></author></authors><title>The Price of Updating the Control Plane in Information-Centric Networks</title><categories>cs.NI</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are studying some fundamental properties of the interface between control
and data planes in Information-Centric Networks. We try to evaluate the traffic
between these two planes based on allowing a minimum level of acceptable
distortion in the network state representation in the control plane. We apply
our framework to content distribution, and see how we can compute the overhead
of maintaining the location of content in the control plane. This is of
importance to evaluate content-oriented network architectures: we identify
scenarios where the cost of updating the control plane for content routing
overwhelms the benefit of fetching a nearby copy. We also show how to minimize
the cost of this overhead when associating costs to peering traffic and to
internal traffic for operator-driven CDNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1286</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1286</id><created>2014-06-05</created><updated>2015-12-16</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>The Volume-Correlation Subspace Detector</title><categories>cs.IT math.IT</categories><comments>35 pages, submitted to IEEE Trans, part of this paper has been
  published by ISIT'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the presence of subspace signals with unknown clutter (or
interference) is a widely known difficult problem encountered in various signal
processing applications. Traditional methods fails to solve this problem
because they require knowledge of clutter subspace, which has to be learned or
estimated beforehand. In this paper, we propose a novel detector, named
volume-correlation subspace detector, that can detect signal from clutter
without any knowledge of clutter subspace. This detector effectively makes use
of the hidden geometrical connection between the known target signal subspace
to be detected and the subspace constructed from sampled data to ascertain the
existence of target signal. It is derived based upon a mathematical tool, which
basically calculates volume of parallelotope in high-dimensional linear space.
Theoretical analysis show that while the proposed detector is detecting the
known target signal, the unknown clutter signal can be explored and eliminated
simultaneously. This advantage is called &quot;detecting while learning&quot;, and
implies perfect performance of this detector in the clutter environment.
Numerical simulation validated our conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1305</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1305</id><created>2014-06-05</created><updated>2015-08-14</updated><authors><author><keyname>Garber</keyname><forenames>Dan</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth
optimization has regained much interest in recent years in the context of large
scale optimization and machine learning. A key advantage of the method is that
it avoids projections - the computational bottleneck in many applications -
replacing it by a linear optimization step. Despite this advantage, the known
convergence rates of the FW method fall behind standard first order methods for
most settings of interest. It is an active line of research to derive faster
linear optimization-based algorithms for various settings of convex
optimization.
  In this paper we consider the special case of optimization over strongly
convex sets, for which we prove that the vanila FW method converges at a rate
of $\frac{1}{t^2}$. This gives a quadratic improvement in convergence rate
compared to the general case, in which convergence is of the order
$\frac{1}{t}$, and known to be tight. We show that various balls induced by
$\ell_p$ norms, Schatten norms and group norms are strongly convex on one hand
and on the other hand, linear optimization over these sets is straightforward
and admits a closed-form solution. We further show how several previous
fast-rate results for the FW method follow easily from our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1308</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1308</id><created>2014-06-05</created><updated>2015-03-04</updated><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author></authors><title>Elias Bound for General Distances and Stable Sets in Edge-Weighted
  Graphs</title><categories>cs.IT math.CO math.IT</categories><comments>Accepted, IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an extension of the Elias bound on the minimum distance
of codes for discrete alphabets with general, possibly infinite-valued,
distances. The bound is obtained by combining a previous extension of the Elias
bound, introduced by Blahut, with an extension of a bound previously introduced
by the author which builds upon ideas of Gallager, Lov\'asz and Marton. The
result can in fact be interpreted as a unification of the Elias bound and of
Lov\'asz's bound on graph (or zero-error) capacity, both being recovered as
particular cases of the one presented here. Previous extensions of the Elias
bound by Berlekamp, Blahut and Piret are shown to be included as particular
cases of our bound. Applications to the reliability function are then
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1310</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1310</id><created>2014-06-05</created><authors><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Zdancewic</keyname><forenames>Steve</forenames></author></authors><title>Finite Vector Spaces as Model of Simply-Typed Lambda-Calculi</title><categories>cs.LO</categories><comments>Accepted at ICTAC 2014. The final publication will be available at
  link.springer.com</comments><journal-ref>Proc. of ICTAC'14, in LNCS vol. 8687, pp 442-459 (2014)</journal-ref><doi>10.1007/978-3-319-10882-7_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use finite vector spaces (finite dimension, over finite
fields) as a non-standard computational model of linear logic. We first define
a simple, finite PCF-like lambda-calculus with booleans, and then we discuss
two finite models, one based on finite sets and the other on finite vector
spaces. The first model is shown to be fully complete with respect to the
operational semantics of the language. The second model is not complete, but we
develop an algebraic extension of the finite lambda calculus that recovers
completeness. The relationship between the two semantics is described, and
several examples based on Church numerals are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1329</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1329</id><created>2014-06-05</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Mohamed Salim</forenames></author></authors><title>Algorithmes dynamiques pour la communication dans le r\'eseau ad hoc
  Coloration des graphes</title><categories>cs.DM</categories><comments>in French</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several authors modelled networks ad hoc by oriented or disoriented graphs,
whereby the problem of allowance (allocation) of the frequencies at the level
of the network was transformed into coloring problem of nodes in the graph.
Graph coloring is a tool to characterize the graphs. In our study, we were
interested in particular in the coloring of vertex. In this domain, a large
number of parameters of coloring were defined. A coloring for which two
neighboring summits do not have same color is called proper coloring. We
proposed to evaluate two other parameters vertex coloring maximizing the number
of colors to use: b-chromatic number and especially the number of Grundy. These
studies have focused on two types of graphs, which are the powers graphs and
Cartesian sum of graphs. In the first part, we determined borders among Grundy
for the Cartesian sum of graphs and finally we proposed algorithms for the
coloration of graphs. First of all, we gave some properties and results for
simple graphs (stable, chain, cycle, full, star, bipartisan). Secondly, we
studied the Cartesian sum of two graphs by giving exact values of the number of
Grundy for diverse classes of graphs (sum of a chain and a chain, a chain and a
cycle, a cycle and a cycle) then we estimated borders of this parameter for the
sum of a complete graph and any other graph. In the second part, we presented
the results of the Cartesian sum of several graphs and in the last part; we
also gave an algorithm for generating graphs with a given number of Grundy.
Thereafter, we studied another parameter namely the partial coloring Grundy.
The properties of this parameter are close to those of total Grundy. Finally,
we used parameters of coloring total Grundy and partial Grundy to generate
algorithms of coloring of graphs. We proposed algorithms of coloring basing on
the properties of certain graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1335</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1335</id><created>2014-06-05</created><authors><author><keyname>Uddin</keyname><forenames>Muhammad Moeen</forenames></author><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Sajjad</keyname><forenames>Hassan</forenames></author></authors><title>Understanding Types of Users on Twitter</title><categories>cs.SI cs.CY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People use microblogging platforms like Twitter to involve with other users
for a wide range of interests and practices. Twitter profiles run by different
types of users such as humans, bots, spammers, businesses and professionals.
This research work identifies six broad classes of Twitter users, and employs a
supervised machine learning approach which uses a comprehensive set of features
to classify users into the identified classes. For this purpose, we exploit
users' profile and tweeting behavior information. We evaluate our approach by
performing 10-fold cross validation using manually annotated 716 different
Twitter profiles. High classification accuracy (measured using AUC, and
precision, recall) reveals the significance of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1352</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1352</id><created>2014-06-05</created><updated>2015-03-03</updated><authors><author><keyname>Angius</keyname><forenames>Alessio</forenames></author><author><keyname>Balbo</keyname><forenames>Gianfranco</forenames></author><author><keyname>Beccuti</keyname><forenames>Marco</forenames></author><author><keyname>Bibbona</keyname><forenames>Enrico</forenames></author><author><keyname>Horvath</keyname><forenames>Andras</forenames></author><author><keyname>Sirovich</keyname><forenames>Roberta</forenames></author></authors><title>Approximate analysis of biological systems by hybrid switching jump
  diffusion</title><categories>cs.PF math.PR q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider large state space continuous time Markov chains
(MCs) arising in the field of systems biology. For density dependent families
of MCs that represent the interaction of large groups of identical objects,
Kurtz has proposed two kinds of approximations. One is based on ordinary
differential equations, while the other uses a diffusion process. The
computational cost of the deterministic approximation is significantly lower,
but the diffusion approximation retains stochasticity and is able to reproduce
relevant random features like variance, bimodality, and tail behavior. In a
recent paper, for particular stochastic Petri net models, we proposed a jump
diffusion approximation that aims at being applicable beyond the limits of
Kurtz's diffusion approximation, namely when the process reaches the boundary
with non-negligible probability. Other limitations of the diffusion
approximation in its original form are that it can provide inaccurate results
when the number of objects in some groups is often or constantly low and that
it can be applied only to pure density dependent Markov chains. In order to
overcome these drawbacks, in this paper we propose to apply the jump-diffusion
approximation only to those components of the model that are in density
dependent form and are associated with high population levels. The remaining
components are treated as discrete quantities. The resulting process is a
hybrid switching jump diffusion. We show that the stochastic differential
equations that characterize this process can be derived automatically both from
the description of the original Markov chains or starting from a higher level
description language, like stochastic Petri nets. The proposed approach is
illustrated on three models: one modeling the so called crazy clock reaction,
one describing viral infection kinetics and the last considering transcription
regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1362</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1362</id><created>2014-06-05</created><authors><author><keyname>Wang</keyname><forenames>Lan</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>An Implementation of Voice over the Cognitive Packet Network</title><categories>cs.NI</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over IP (VOIP) has strict Quality of Service (QoS) constraints and
requires real-time packet delivery, which poses a major challenge to IP
networks. The Cognitive Packet Network (CPN) has been designed as a QoS-driven
protocol that addresses user-oriented QoS demands by adaptively routing packets
based on online sensing and measurement. This paper presents our design,
implementation and evaluation of a &quot;Voice over CPN&quot; system where an extension
of the CPN routing algorithm has been developed to support the needs of voice
packet delivery in the presence of other background traffic flows with the same
or different QoS requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1368</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1368</id><created>2014-06-05</created><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Cibulka</keyname><forenames>Josef</forenames></author><author><keyname>Kyn&#x10d;l</keyname><forenames>Jan</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author></authors><title>Peeling potatoes near-optimally in near-linear time</title><categories>cs.CG cs.DS math.MG</categories><comments>Preliminary version to be presented at SoCG 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following geometric optimization problem: find a convex
polygon of maximum area contained in a given simple polygon $P$ with $n$
vertices. We give a randomized near-linear-time $(1-\varepsilon)$-approximation
algorithm for this problem: in $O((n/\varepsilon^4) \log^2 n \log(1/\delta))$
time we find a convex polygon contained in $P$ that, with probability at least
$1-\delta$, has area at least $(1-\varepsilon)$ times the area of an optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1385</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1385</id><created>2014-06-05</created><authors><author><keyname>Dikmen</keyname><forenames>Onur</forenames></author><author><keyname>Yang</keyname><forenames>Zhirong</forenames></author><author><keyname>Oja</keyname><forenames>Erkki</forenames></author></authors><title>Learning the Information Divergence</title><categories>cs.LG</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information divergence that measures the difference between two nonnegative
matrices or tensors has found its use in a variety of machine learning
problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic
Neighbor Embedding, topic models, and Bayesian network optimization. The
success of such a learning task depends heavily on a suitable divergence. A
large variety of divergences have been suggested and analyzed, but very few
results are available for an objective choice of the optimal divergence for a
given task. Here we present a framework that facilitates automatic selection of
the best divergence among a given family, based on standard maximum likelihood
estimation. We first propose an approximated Tweedie distribution for the
beta-divergence family. Selecting the best beta then becomes a machine learning
problem solved by maximum likelihood. Next, we reformulate alpha-divergence in
terms of beta-divergence, which enables automatic selection of alpha by maximum
likelihood with reuse of the learning principle for beta-divergence.
Furthermore, we show the connections between gamma and beta-divergences as well
as R\'enyi and alpha-divergences, such that our automatic selection framework
is extended to non-separable divergences. Experiments on both synthetic and
real-world data demonstrate that our method can quite accurately select
information divergence across different learning problems and various
divergence families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1393</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1393</id><created>2014-06-05</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author><author><keyname>Hamid</keyname><forenames>Fahmida</forenames></author></authors><title>Interclausal Logic Variables</title><categories>cs.PL</categories><comments>to appear as a ICLP'14 technical contribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unification of logic variables instantly connects present and future
observations of their value, independently of their location in the data areas
of the runtime system. The paper extends this property to &quot;interclausal logic
variables&quot;, an easy to implement Prolog extension that supports instant global
information exchanges without dynamic database updates. We illustrate their
usefulness with two of algorithms, {\em graph coloring} and {\em minimum
spanning tree}. Implementations of interclausal variables as source-level
transformations and as abstract machine adaptations are given. To address the
need for globally visible chained transitions of logic variables we describe a
DCG-based program transformation that extends the functionality of interclausal
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1395</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1395</id><created>2014-06-05</created><authors><author><keyname>Ferrucci</keyname><forenames>Luca</forenames></author><author><keyname>Bersani</keyname><forenames>Marcello M.</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author></authors><title>An LTL Semantics of Business Workflows with Recovery</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a business workflow case study with abnormal behavior management
(i.e. recovery) and demonstrate how temporal logics and model checking can
provide a methodology to iteratively revise the design and obtain a correct-by
construction system. To do so we define a formal semantics by giving a
compilation of generic workflow patterns into LTL and we use the bound model
checker Zot to prove specific properties and requirements validity. The working
assumption is that such a lightweight approach would easily fit into processes
that are already in place without the need for a radical change of procedures,
tools and people's attitudes. The complexity of formalisms and invasiveness of
methods have been demonstrated to be one of the major drawback and obstacle for
deployment of formal engineering techniques into mundane projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1404</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1404</id><created>2014-06-05</created><authors><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>On the satisfiability problem for SPARQL patterns</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The satisfiability problem for SPARQL patterns is undecidable in general,
since the expressive power of SPARQL 1.0 is comparable with that of the
relational algebra. The goal of this paper is to delineate the boundary of
decidability of satisfiability in terms of the constraints allowed in filter
conditions. The classes of constraints considered are bound-constraints,
negated bound-constraints, equalities, nonequalities, constant-equalities, and
constant-nonequalities. The main result of the paper can be summarized by
saying that, as soon as inconsistent filter conditions can be formed,
satisfiability is undecidable. The key insight in each case is to find a way to
emulate the set difference operation. Undecidability can then be obtained from
a known undecidability result for the algebra of binary relations with union,
composition, and set difference. When no inconsistent filter conditions can be
formed, satisfiability is efficiently decidable by simple checks on bound
variables and on the use of literals. The paper also points out that
satisfiability for the so-called `well-designed' patterns can be decided by a
check on bound variables and a check for inconsistent filter conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1411</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1411</id><created>2014-06-05</created><updated>2014-06-06</updated><authors><author><keyname>Nie</keyname><forenames>Siqi</forenames></author><author><keyname>Maua</keyname><forenames>Denis Deratani</forenames></author><author><keyname>de Campos</keyname><forenames>Cassio Polpo</forenames></author><author><keyname>Ji</keyname><forenames>Qiang</forenames></author></authors><title>Advances in Learning Bayesian Networks of Bounded Treewidth</title><categories>cs.AI cs.LG stat.ML</categories><comments>23 pages, 2 figures, 3 tables</comments><msc-class>68T37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents novel algorithms for learning Bayesian network structures
with bounded treewidth. Both exact and approximate methods are developed. The
exact method combines mixed-integer linear programming formulations for
structure learning and treewidth computation. The approximate method consists
in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and
subsequently selecting, exactly or approximately, the best structure whose
moral graph is a subgraph of that $k$-tree. Some properties of these methods
are discussed and proven. The approaches are empirically compared to each other
and to a state-of-the-art method for learning bounded treewidth structures on a
collection of public data sets with up to 100 variables. The experiments show
that our exact algorithm outperforms the state of the art, and that the
approximate approach is fairly accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1413</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1413</id><created>2014-06-05</created><updated>2015-08-14</updated><authors><author><keyname>Cherubini</keyname><forenames>Alessandra</forenames></author><author><keyname>Frigeri</keyname><forenames>Achille</forenames></author><author><keyname>Liu</keyname><forenames>Zuhua</forenames></author></authors><title>Composing short 3-compressing words on a 2 letter alphabet</title><categories>math.CO cs.FL</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite deterministic (semi)automaton $\mathcal{A} =(Q,\Sigma,\delta)$ is
$k$-compressible if there is some word $w\in \Sigma^+$ such that the image of
its state set $Q$ under the natural action of $w$ is reduced by at least $k$
states. Such word, if it exists, is called a $k$-compressing word for
$\mathcal{A}$. A word is $k$-collapsing if it is $k$-compressing for each
$k$-compressible automaton. We compute a set $W$ of short words such that each
$3$-compressible automata on a two letter alphabet is $3$-compressed at least
by a word in $W$. Then we construct a shortest common superstring of the words
in $W$ and, with a further refinement, we obtain a $3$-collapsing word of
length $53$. Moreover, as previously announced, we show that the shortest
$3$-synchronizing word is not $3$-collapsing, illustrating the new bounds
$34\leq c(2,3)\leq 53$ for the length $c(2,3)$ of the shortest $3$-collapsing
word on a two letter alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1414</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1414</id><created>2014-06-05</created><updated>2014-10-01</updated><authors><author><keyname>Wegner</keyname><forenames>Anatol E.</forenames></author></authors><title>Subgraph covers -- An information theoretic approach to motif analysis
  in networks</title><categories>cs.SI cs.DM physics.soc-ph q-bio.MN</categories><comments>10 pages, 7 tables, 1 Figure</comments><journal-ref>Phys. Rev. X 4, 041026 (2014)</journal-ref><doi>10.1103/PhysRevX.4.041026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world networks contain a statistically surprising number of certain
subgraphs, called network motifs. In the prevalent approach to motif analysis,
network motifs are detected by comparing subgraph frequencies in the original
network with a statistical null model. In this paper we propose an alternative
approach to motif analysis where network motifs are defined to be connectivity
patterns that occur in a subgraph cover that represents the network using
minimal total information. A subgraph cover is defined to be a set of subgraphs
such that every edge of the graph is contained in at least one of the subgraphs
in the cover. Some recently introduced random graph models that can incorporate
significant densities of motifs have natural formulations in terms of subgraph
covers and the presented approach can be used to match networks with such
models. To prove the practical value of our approach we also present a
heuristic for the resulting NP-hard optimization problem and give results for
several real world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1423</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1423</id><created>2014-06-05</created><authors><author><keyname>Amavi</keyname><forenames>Joshua</forenames></author><author><keyname>Chabin</keyname><forenames>Jacques</forenames></author><author><keyname>Ferrari</keyname><forenames>Mirian Halfeld</forenames></author><author><keyname>R&#xe9;ty</keyname><forenames>Pierre</forenames></author></authors><title>A ToolBox for Conservative XML Schema Evolution and Document Adaptation</title><categories>cs.DB</categories><comments>15 pages, DEXA'14</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes a set of tools to help dealing with XML database
evolution. It aims at establishing a multi-system environment where a global
integrated system works in harmony with some local original ones, allowing data
translation in both directions and, thus, activities on both levels. To deal
with schemas, we propose an algorithm that computes a mapping capable of
obtaining a global schema which is a conservative extension of original local
schemas. The role of the obtained mapping is then twofold: it ensures schema
evolution, via composition and inversion, and it guides the construction of a
document translator, allowing automatic data adaptation w.r.t. type evolution.
This paper applies, extends and put together some of our previous
contributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1431</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1431</id><created>2014-06-03</created><authors><author><keyname>Mansouri</keyname><forenames>Ali</forenames></author><author><keyname>Amghar</keyname><forenames>Youssef</forenames></author></authors><title>Int\'egration des r\`egles actives dans des documents</title><categories>cs.DB</categories><comments>master's thesis, in French</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The management of technical documentation is an unavoidable activity
interesting for the enterprises. Indeed, the need to manage documents during
all the life cycle is an important issue. For that, the need to enhance the
ability of document management systems is an interesting challenge. Despite
existing systems on market (electronic document management systems), they are
considered as non-flexible systems which are based on data models preventing
any extension or improvement. In addition, those systems do not allow a slight
description of documents elements and propose an insufficient mechanisms for
both links and consistency management. LIRIS laboratory has developed research
in this area and proposed an active system, termed SAGED, whose objectives is
to manage link and consistency using active rules. However SAGED is based on an
approach that split rules (for consistency management) and documents
description. The main drawback is the rigidity of such approach which is
highlighted whenever documents are moved from one server to another or during
exchanges of documents. To contribute to solve this problem, we propose to
develop an approach aiming at improve the document management including
consistency. This approach is based on the introduction of rules with the XML
description of the documents [BoCP01]. In this context we proposed a
XML-oriented storage level allowing the storing of documents and rules
uniformly through a native XML database. We defined an intelligent system
termed SIGED according a client/server architecture built around an intelligent
component for active rules execution. These rules are extracted from XML
document, compiled and executed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1433</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1433</id><created>2014-06-05</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author></authors><title>Reconfiguring Independent Sets in Cographs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two independent sets of a graph are adjacent if they differ on exactly one
vertex (i.e. we can transform one into the other by adding or deleting a
vertex). Let $k$ be an integer. We consider the reconfiguration graph
$TAR_k(G)$ on the set of independent sets of size at least $k$ in a graph $G$,
with the above notion of adjacency. Here we provide a cubic-time algorithm to
decide whether $TAR_k(G)$ is connected when $G$ is a cograph, thus solving an
open question of~[Bonsma 2014]. As a by-product, we also describe a linear-time
algorithm which decides whether two elements of $TAR_k(G)$ are in the same
connected component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1449</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1449</id><created>2014-06-05</created><updated>2015-03-13</updated><authors><author><keyname>Valdano</keyname><forenames>Eugenio</forenames></author><author><keyname>Poletto</keyname><forenames>Chiara</forenames></author><author><keyname>Giovannini</keyname><forenames>Armando</forenames></author><author><keyname>Palma</keyname><forenames>Diana</forenames></author><author><keyname>Savini</keyname><forenames>Lara</forenames></author><author><keyname>Colizza</keyname><forenames>Vittoria</forenames></author></authors><title>Predicting epidemic risk from past temporal contact data</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 5 figures + SI (18 pages, 15 figures)</comments><journal-ref>Valdano E, Poletto C, Giovannini A, Palma D, Savini L, et al.
  (2015) Predicting Epidemic Risk from Past Temporal Contact Data. PLoS Comput
  Biol 11(3): e1004152. doi:10.1371/journal.pcbi.1004152</journal-ref><doi>10.1371/journal.pcbi.1004152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how epidemics spread in a system is a crucial step to prevent
and control outbreaks, with broad implications on the system's functioning,
health, and associated costs. This can be achieved by identifying the elements
at higher risk of infection and implementing targeted surveillance and control
measures. One important ingredient to consider is the pattern of
disease-transmission contacts among the elements, however lack of data or
delays in providing updated records may hinder its use, especially for
time-varying patterns. Here we explore to what extent it is possible to use
past temporal data of a system's pattern of contacts to predict the risk of
infection of its elements during an emerging outbreak, in absence of updated
data. We focus on two real-world temporal systems; a livestock displacements
trade network among animal holdings, and a network of sexual encounters in
high-end prostitution. We define the node's loyalty as a local measure of its
tendency to maintain contacts with the same elements over time, and uncover
important non-trivial correlations with the node's epidemic risk. We show that
a risk assessment analysis incorporating this knowledge and based on past
structural and temporal pattern properties provides accurate predictions for
both systems. Its generalizability is tested by introducing a theoretical model
for generating synthetic temporal networks. High accuracy of our predictions is
recovered across different settings, while the amount of possible predictions
is system-specific. The proposed method can provide crucial information for the
setup of targeted intervention strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1469</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1469</id><created>2014-06-05</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author></authors><title>Diffusion of the Adoption of Online Retailing in Saudi Arabia</title><categories>cs.CY</categories><comments>PhD thesis, 312 pages including appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that Saudi Arabia is a leading producer of oil and natural
gas, a member of G-20, has the largest and fastest growth of ICT marketplaces
in the Arab region, and is very wealthy, online retailing activities are not
progressing at the same speed as its growing ICT marketplace. For this reason,
the attitudes of retailers in companies at different stages of e-commerce
maturity were investigated to determine what factors are causing this problem.
The data collection included two stages: first, interviews with 16 retailers
and, then, based on the outcomes of the interviews, a quantitative study
designed to test and further explore the qualitative interview findings with a
sample of 153 retailers in companies at different stages of e-commerce
maturity. The results showed striking differences between retailers as a
function of the stage of growth of their companies in regard to factors
relating to their perceptions of consumers and to organizational factors. At
the same time, the results showed agreement between retailers in companies at
different stages in relation to environmental factors. The investigation helped
to identify the incentives and barriers for each stage of maturity and makes
recommendations of what could be done to help companies move from one stage to
the next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1475</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1475</id><created>2014-06-05</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author></authors><title>Modeling CSFs of B2C E-commerce Systems Using the Enterprise
  Architecture Approach</title><categories>cs.CY</categories><comments>Master Thesis, 80 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study is to investigate the Critical Success Factors (CSFs) of the
Business to Customer (B2C) e-commerce system. These factors should be
considered comprehensively and expanded to all parties concerned to create and
provide the electronic service and ensure that the CSFs are satisfied. In order
to give an organized and inclusive view of the CSFs, an enterprise architecture
framework will be adopted to systemize this investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1476</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1476</id><created>2014-06-05</created><updated>2015-03-23</updated><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author><author><keyname>Chakraborty</keyname><forenames>Anirban</forenames></author><author><keyname>Plaza</keyname><forenames>Stephen</forenames></author><author><keyname>Scheffer</keyname><forenames>Lou</forenames></author></authors><title>A Context-aware Delayed Agglomeration Framework for Electron Microscopy
  Segmentation</title><categories>cs.CV</categories><journal-ref>PLoS ONE 10(5): e0125825, 2015</journal-ref><doi>10.1371/journal.pone.0125825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electron Microscopy (EM) image (or volume) segmentation has become
significantly important in recent years as an instrument for connectomics. This
paper proposes a novel agglomerative framework for EM segmentation. In
particular, given an over-segmented image or volume, we propose a novel
framework for accurately clustering regions of the same neuron. Unlike existing
agglomerative methods, the proposed context-aware algorithm divides superpixels
(over-segmented regions) of different biological entities into different
subsets and agglomerates them separately. In addition, this paper describes a
&quot;delayed&quot; scheme for agglomerative clustering that postpones some of the merge
decisions, pertaining to newly formed bodies, in order to generate a more
confident boundary prediction. We report significant improvements attained by
the proposed approach in segmentation accuracy over existing standard methods
on 2D and 3D datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1485</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1485</id><created>2014-06-05</created><updated>2014-12-05</updated><authors><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Iterative Neural Autoregressive Distribution Estimator (NADE-k)</title><categories>stat.ML cs.LG</categories><comments>Accepted at Neural Information Processing Systems (NIPS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training of the neural autoregressive density estimator (NADE) can be viewed
as doing one step of probabilistic inference on missing values in data. We
propose a new model that extends this inference scheme to multiple steps,
arguing that it is easier to learn to improve a reconstruction in $k$ steps
rather than to learn to reconstruct in a single inference step. The proposed
model is an unsupervised building block for deep learning that combines the
desirable properties of NADE and multi-predictive training: (1) Its test
likelihood can be computed analytically, (2) it is easy to generate independent
samples from it, and (3) it uses an inference engine that is a superset of
variational inference for Boltzmann machines. The proposed NADE-k is
competitive with the state-of-the-art in density estimation on the two datasets
tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1488</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1488</id><created>2014-06-05</created><authors><author><keyname>Cao</keyname><forenames>Yun-He</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Wang</keyname><forenames>Sheng-Hua</forenames></author></authors><title>IRCI Free Colocated MIMO Radar Based on Sufficient Cyclic Prefix OFDM
  Waveforms</title><categories>cs.IT math.IT</categories><comments>27 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose a cyclic prefix (CP) based MIMO-OFDM range
reconstruction method and its corresponding MIMO-OFDM waveform design for
co-located MIMO radar systems. Our proposed MIMO-OFDM waveform design achieves
the maximum signal-to-noise ratio (SNR) gain after the range reconstruction and
its peak-to-average power ratio (PAPR) in the discrete time domain is also
optimal, i.e., 0dB, when Zadoff-Chu sequences are used in the discrete
frequency domain as the weighting coefficients for the subcarriers. We also
investigate the performance when there are transmit and receive digital
beamforming (DBF) pointing errors. It is shown that our proposed CP based
MIMO-OFDM range reconstruction is inter-range-cell interference (IRCI) free no
matter whether there are transmit and receive DBF pointing errors or not.
Simulation results are presented to verify the theory and compare it with the
conventional OFDM and LFM co-located MIMO radars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1489</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1489</id><created>2014-06-03</created><authors><author><keyname>Kontzalis</keyname><forenames>Charalambos P.</forenames></author><author><keyname>Kalogeropoulos</keyname><forenames>Grigoris</forenames></author></authors><title>Controllability and reachability of singular linear discrete time
  systems</title><categories>math.OC cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this article is to develop a matrix pencil approach for
the study of the controllability and reachability of a class of linear singular
discrete time systems. The description equation of a practical system may be
established through selection of the proper state variables. Time domain
analysis is the method of analyzing the system based on this description
equation, through which we may gain a fair understanding of the system's
structural features as well as its internal properties. Using time domain
analysis, this article studies the fundamentals in system theory such as
reachability and controllability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1501</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1501</id><created>2014-06-05</created><updated>2014-06-20</updated><authors><author><keyname>Estreguil</keyname><forenames>Christine</forenames></author><author><keyname>Caudullo</keyname><forenames>Giovanni</forenames></author><author><keyname>de Rigo</keyname><forenames>Daniele</forenames></author></authors><title>Connectivity of Natura 2000 forest sites in Europe</title><categories>cs.CE q-bio.PE</categories><comments>9 pages, from a poster published in F1000Posters 2014, 5: 485</comments><journal-ref>F1000Posters 2014, 5: 485</journal-ref><doi>10.6084/m9.figshare.1063300</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Background/Purpose: In the context of the European Biodiversity policy, the
Green Infrastructure Strategy is one supporting tool to mitigate fragmentation,
inter-alia to increase the spatial and functional connectivity between
protected and unprotected areas. The Joint Research Centre has developed an
integrated model to provide a macro-scale set of indices to evaluate the
connectivity of the Natura 2000 network, which forms the backbone of a Green
Infrastructure for Europe. The model allows a wide assessment and comparison to
be performed across countries in terms of structural (spatially connected or
isolated sites) and functional connectivity (least-cost distances between sites
influenced by distribution, distance and land cover).
  Main conclusion: The Natura 2000 network in Europe shows differences among
countries in terms of the sizes and numbers of sites, their distribution as
well as distances between sites. Connectivity has been assessed on the basis of
a 500 m average inter-site distance, roads and intensive land use as barrier
effects as well as the presence of &quot;green&quot; corridors. In all countries the
Natura 2000 network is mostly made of sites which are not physically connected.
Highest functional connectivity values are found for Spain, Slovakia, Romania
and Bulgaria. The more natural landscape in Sweden and Finland does not result
in high inter-site network connectivity due to large inter-site distances. The
distribution of subnets with respect to roads explains the higher share of
isolated subnets in Portugal than in Belgium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1502</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1502</id><created>2014-06-05</created><authors><author><keyname>Biehl</keyname><forenames>Martin</forenames></author><author><keyname>Salge</keyname><forenames>Christoph</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Towards designing artificial universes for artificial agents under
  interaction closure</title><categories>cs.MA</categories><comments>8 pages, 3 figures; accepted for publication in ALIFE 14 proceedings</comments><msc-class>G.3, H.1.1, I.2.0, I.6.m, J.2, J.3</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in designing artificial universes for artifi- cial agents.
We view artificial agents as networks of high- level processes on top of of a
low-level detailed-description system. We require that the high-level processes
have some intrinsic explanatory power and we introduce an extension of
informational closure namely interaction closure to capture this. Then we
derive a method to design artificial universes in the form of finite Markov
chains which exhibit high-level pro- cesses that satisfy the property of
interaction closure. We also investigate control or information transfer which
we see as an building block for networks representing artificial agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1509</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1509</id><created>2014-06-05</created><updated>2014-06-25</updated><authors><author><keyname>Ja&#x15b;kowski</keyname><forenames>Wojciech</forenames></author></authors><title>Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in
  the Othello League</title><categories>cs.NE cs.AI cs.LG</categories><comments>Added technical report number</comments><report-no>RA-06/2014</report-no><msc-class>68T05</msc-class><journal-ref>ICGA Journal 37(2), 2014, pp. 85-96</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  N-tuple networks have been successfully used as position evaluation functions
for board games such as Othello or Connect Four. The effectiveness of such
networks depends on their architecture, which is determined by the placement of
constituent n-tuples, sequences of board locations, providing input to the
network. The most popular method of placing n-tuples consists in randomly
generating a small number of long, snake-shaped board location sequences. In
comparison, we show that learning n-tuple networks is significantly more
effective if they involve a large number of systematically placed, short,
straight n-tuples. Moreover, we demonstrate that in order to obtain the best
performance and the steepest learning curve for Othello it is enough to use
n-tuples of size just 2, yielding a network consisting of only 288 weights. The
best such network evolved in this study has been evaluated in the online
Othello League, obtaining the performance of nearly 96% --- more than any other
player to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1510</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1510</id><created>2014-06-05</created><authors><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Sneyers</keyname><forenames>Jon</forenames></author></authors><title>Proceedings of the Eleventh Workshop on Constraint Handling Rules</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the eleventh Workshop on
Constraint Handling Rules (CHR 2014), which will be held in Vienna at the
occasion of the Vienna Summer of Logic (VSL)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1516</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1516</id><created>2014-06-05</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Yang</keyname><forenames>Zheng</forenames></author><author><keyname>Fan</keyname><forenames>Pingzhi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Performance of Non-Orthogonal Multiple Access in 5G Systems with
  Randomly Deployed Users</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2014.2343971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the performance of non-orthogonal multiple access (NOMA) is
investigated in a cellular downlink scenario with randomly deployed users. The
developed analytical results show that NOMA can achieve superior performance in
terms of ergodic sum rates; however, the outage performance of NOMA depends
critically on the choices of the users' targeted data rates and allocated
power. In particular, a wrong choice of the targeted data rates and allocated
power can lead to a situation in which the user's outage probability is always
one, i.e. the user's targeted quality of service will never be met.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1520</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1520</id><created>2014-06-02</created><authors><author><keyname>Mlinar</keyname><forenames>Vladan</forenames></author></authors><title>Inherent limits on optimization and discovery in physical systems</title><categories>cs.SI cond-mat.mtrl-sci physics.soc-ph</categories><journal-ref>Ann. Phys. 351, 837-849 (2014)</journal-ref><doi>10.1016/j.aop.2014.10.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological mapping of a large physical system on a graph, and its
decomposition using universal measures is proposed. We find inherent limits to
the potential for optimization of a given system and its approximate
representations by motifs, and the ability to reconstruct the full system given
approximate representations. The approximate representation of the system most
suited for optimization may be different from that which most accurately
describes the full system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1528</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1528</id><created>2014-06-05</created><authors><author><keyname>Lang</keyname><forenames>Dustin</forenames></author><author><keyname>Hogg</keyname><forenames>David W.</forenames></author><author><keyname>Scholkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Towards building a Crowd-Sourced Sky Map</title><categories>cs.CV astro-ph.IM</categories><comments>Appeared at AI-STATS 2014</comments><journal-ref>JMLR Workshop and Conference Proceedings, 33 (AI &amp; Statistics
  2014), 549</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a system that builds a high dynamic-range and wide-angle image of
the night sky by combining a large set of input images. The method makes use of
pixel-rank information in the individual input images to improve a &quot;consensus&quot;
pixel rank in the combined image. Because it only makes use of ranks and the
complexity of the algorithm is linear in the number of images, the method is
useful for large sets of uncalibrated images that might have undergone unknown
non-linear tone mapping transformations for visualization or aesthetic reasons.
We apply the method to images of the night sky (of unknown provenance)
discovered on the Web. The method permits discovery of astronomical objects or
features that are not visible in any of the input images taken individually.
More importantly, however, it permits scientific exploitation of a huge source
of astronomical images that would not be available to astronomical research
without our automatic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1534</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1534</id><created>2014-06-05</created><authors><author><keyname>Levy</keyname><forenames>Paul</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Krishnaswami</keyname><forenames>Neel</forenames><affiliation>University of Birmingham</affiliation></author></authors><title>Proceedings 5th Workshop on Mathematically Structured Functional
  Programming</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.3; F.4</acm-class><journal-ref>EPTCS 153, 2014</journal-ref><doi>10.4204/EPTCS.153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Fifth Workshop on Mathematically
Structured Functional Programming (MSFP 2014), taking place on 12 April, 2014
in Grenoble, France, as a satellite event of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2014.
  MSFP is devoted to the derivation of functionality from structure. It
highlights concepts from algebra, semantics and type theory as they are
increasingly reflected in programming practice, especially functional
programming. As the range of papers presented in this year's workshop shows,
this continues to be a fruitful interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1543</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1543</id><created>2014-06-05</created><updated>2015-07-08</updated><authors><author><keyname>Barbosa</keyname><forenames>Geraldo A.</forenames></author><author><keyname>van de Graaf</keyname><forenames>Jeroen</forenames></author></authors><title>Untappable key distribution system: a one-time-pad booster</title><categories>cs.CR quant-ph</categories><comments>12 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-time-pad (OTP) encryption simply cannot be cracked, even by a quantum
computer. The need of sharing in a secure way supplies of symmetric random keys
turned the method almost obsolete as a standing-alone method for fast and large
volume telecommunication. Basically, this secure sharing of keys and their
renewal, once exhausted, had to be done through couriers, in a slow and costly
process. This paper presents a solution for this problem providing a fast and
unlimited renewal of secure keys: An untappable key distribution system is
presented and detailed. This fast key distribution system utilizes two layers
of confidentially protection: 1) Physical noise intrinsic to the optical
channel that turn the coded signals into stealth signals and 2) Privacy
amplification using a bit pool of refreshed entropy run after run, to eliminate
any residual information.
  The resulting level of security is rigorously calculated and demonstrates
that the level of information an eavesdropper could obtain is completely
negligible. The random bit sequences, fast and securely distributed, can be
used to encrypt text, data or voice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1547</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1547</id><created>2014-06-05</created><authors><author><keyname>Palasek</keyname><forenames>Stan</forenames></author></authors><title>Arbitrage-free exchange rate ensembles over a general trade network</title><categories>q-fin.EC cs.SI q-fin.TR</categories><comments>6 pages</comments><msc-class>91</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is assumed that under suitable economic and information-theoretic
conditions, market exchange rates are free from arbitrage. Commodity markets in
which trades occur over a complete graph are shown to be trivial. We therefore
examine the vector space of no-arbitrage exchange rate ensembles over an
arbitrary connected undirected graph. Consideration is given for the minimal
information for determination of an exchange rate ensemble. We conclude with a
topical discussion of exchanges in which our analyses may be relevant,
including the emergent but highly-regulated (and therefore not a complete
graph) market for digital currencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1548</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1548</id><created>2014-06-05</created><updated>2014-08-13</updated><authors><author><keyname>Bliss</keyname><forenames>Catherine A.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Estimation of global network statistics from incomplete data</title><categories>physics.soc-ph cs.SI</categories><comments>63 pages, 32 figures, 26 tables; Submitted to PLOSONE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks underlie an enormous variety of social, biological,
physical, and virtual systems. A profound complication for the science of
complex networks is that in most cases, observing all nodes and all network
interactions is impossible. Previous work addressing the impacts of partial
network data is surprisingly limited, focuses primarily on missing nodes, and
suggests that network statistics derived from subsampled data are not suitable
estimators for the same network statistics describing the overall network
topology. We generate scaling methods to predict true network statistics,
including the degree distribution, from only partial knowledge of nodes, links,
or weights. Our methods are transparent and do not assume a known generating
process for the network, thus enabling prediction of network statistics for a
wide variety of applications. We validate analytical results on four simulated
network classes and empirical data sets of various sizes. We perform
subsampling experiments by varying proportions of sampled data and demonstrate
that our scaling methods can provide very good estimates of true network
statistics while acknowledging limits. Lastly, we apply our techniques to a set
of rich and evolving large-scale social networks, Twitter reply networks. Based
on 100 million tweets, we use our scaling techniques to propose a statistical
characterization of the Twitter Interactome from September 2008 to November
2008. Our treatment allows us to find support for Dunbar's hypothesis in
detecting an upper threshold for the number of active social contacts that
individuals maintain over the course of one week.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1556</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1556</id><created>2014-06-05</created><authors><author><keyname>Kaufmann</keyname><forenames>Matt</forenames><affiliation>UT Austin</affiliation></author><author><keyname>Moore</keyname><forenames>J Strother</forenames><affiliation>UT Austin</affiliation></author></authors><title>Enhancements to ACL2 in Versions 6.2, 6.3, and 6.4</title><categories>cs.AI cs.LO cs.MS</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 1-7</journal-ref><doi>10.4204/EPTCS.152.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on improvements to ACL2 made since the 2013 ACL2 Workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1557</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1557</id><created>2014-06-05</created><authors><author><keyname>Chamarthi</keyname><forenames>Harsh Raju</forenames><affiliation>Northeastern Univeristy</affiliation></author><author><keyname>Dillinger</keyname><forenames>Peter C.</forenames><affiliation>Northeastern Univeristy</affiliation></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames><affiliation>Northeastern Univeristy</affiliation></author></authors><title>Data Definitions in the ACL2 Sedan</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 27-48</journal-ref><doi>10.4204/EPTCS.152.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a data definition framework that enables the convenient
specification of data types in ACL2s, the ACL2 Sedan. Our primary motivation
for developing the data definition framework was pedagogical. We were teaching
undergraduate students how to reason about programs using ACL2s and wanted to
provide them with an effective method for defining, testing, and reasoning
about data types in the context of an untyped theorem prover. Our framework is
now routinely used not only for pedagogical purposes, but also by advanced
users.
  Our framework concisely supports common data definition patterns, e.g. list
types, map types, and record types. It also provides support for polymorphic
functions. A distinguishing feature of our approach is that we maintain both a
predicative and an enumerative characterization of data definitions.
  In this paper we present our data definition framework via a sequence of
examples. We give a complete characterization in terms of tau rules of the
inclusion/exclusion relations a data definition induces, under suitable
restrictions. The data definition framework is a key component of
counterexample generation support in ACL2s, but can be independently used in
ACL2, and is available as a community book.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1558</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1558</id><created>2014-06-05</created><authors><author><keyname>Selfridge</keyname><forenames>Benjamin</forenames><affiliation>University of Texas at Austin</affiliation></author><author><keyname>Smith</keyname><forenames>Eric</forenames><affiliation>Kestrel Institute</affiliation></author></authors><title>Polymorphic Types in ACL2</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 49-59</journal-ref><doi>10.4204/EPTCS.152.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a tool suite for the ACL2 programming language which
incorporates certain ideas from the Hindley-Milner paradigm of functional
programming (as exemplified in popular languages like ML and Haskell),
including a &quot;typed&quot; style of programming with the ability to define polymorphic
types. These ideas are introduced via macros into the language of ACL2, taking
advantage of ACL2's guard-checking mechanism to perform type checking on both
function definitions and theorems. Finally, we discuss how these macros were
used to implement features of Specware, a software specification and
implementation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1559</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1559</id><created>2014-06-05</created><authors><author><keyname>Joosten</keyname><forenames>Sebastiaan</forenames><affiliation>Technical University of Eindhoven/Radboud University Nijmegen</affiliation></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames><affiliation>University of Innsbruck</affiliation></author><author><keyname>Urban</keyname><forenames>Josef</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>Initial Experiments with TPTP-style Automated Theorem Provers on ACL2
  Problems</title><categories>cs.AI cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 77-85</journal-ref><doi>10.4204/EPTCS.152.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports our initial experiments with using external ATP on some
corpora built with the ACL2 system. This is intended to provide the first
estimate about the usefulness of such external reasoning and AI systems for
solving ACL2 problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1560</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1560</id><created>2014-06-05</created><authors><author><keyname>Cowles</keyname><forenames>John</forenames><affiliation>University of Wyoming</affiliation></author><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Equivalence of the Traditional and Non-Standard Definitions of Concepts
  from Real Analysis</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 89-100</journal-ref><doi>10.4204/EPTCS.152.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ACL2(r) is a variant of ACL2 that supports the irrational real and complex
numbers. Its logical foundation is based on internal set theory (IST), an
axiomatic formalization of non-standard analysis (NSA). Familiar ideas from
analysis, such as continuity, differentiability, and integrability, are defined
quite differently in NSA-some would argue the NSA definitions are more
intuitive. In previous work, we have adopted the NSA definitions in ACL2(r),
and simply taken as granted that these are equivalent to the traditional
analysis notions, e.g., to the familiar epsilon-delta definitions. However, we
argue in this paper that there are circumstances when the more traditional
definitions are advantageous in the setting of ACL2(r), precisely because the
traditional notions are classical, so they are unencumbered by IST limitations
on inference rules such as induction or the use of pseudo-lambda terms in
functional instantiation. To address this concern, we describe a formal proof
in ACL2(r) of the equivalence of the traditional and non-standards definitions
of these notions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1561</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1561</id><created>2014-06-05</created><authors><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author><author><keyname>Cowles</keyname><forenames>John</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Formal Verification of Medina's Sequence of Polynomials for
  Approximating Arctangent</title><categories>cs.LO cs.MS</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 101-110</journal-ref><doi>10.4204/EPTCS.152.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verification of many algorithms for calculating transcendental functions
is based on polynomial approximations to these functions, often Taylor series
approximations. However, computing and verifying approximations to the
arctangent function are very challenging problems, in large part because the
Taylor series converges very slowly to arctangent-a 57th-degree polynomial is
needed to get three decimal places for arctan(0.95). Medina proposed a series
of polynomials that approximate arctangent with far faster convergence-a
7th-degree polynomial is all that is needed to get three decimal places for
arctan(0.95). We present in this paper a proof in ACL2(r) of the correctness
and convergence rate of this sequence of polynomials. The proof is particularly
beautiful, in that it uses many results from real analysis. Some of these
necessary results were proven in prior work, but some were proven as part of
this effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1562</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1562</id><created>2014-06-05</created><authors><author><keyname>Puri</keyname><forenames>Disha</forenames><affiliation>Dept. of Computer Science, Portland State University</affiliation></author><author><keyname>Ray</keyname><forenames>Sandip</forenames><affiliation>Strategic CAD Labs, Intel Corporation</affiliation></author><author><keyname>Hao</keyname><forenames>Kecheng</forenames><affiliation>Dept. of Computer Science, Portland State University</affiliation></author><author><keyname>Xie</keyname><forenames>Fei</forenames><affiliation>Dept. of Computer Science, Portland State University</affiliation></author></authors><title>Using ACL2 to Verify Loop Pipelining in Behavioral Synthesis</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 111-128</journal-ref><doi>10.4204/EPTCS.152.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavioral synthesis involves compiling an Electronic System-Level (ESL)
design into its Register-Transfer Level (RTL) implementation. Loop pipelining
is one of the most critical and complex transformations employed in behavioral
synthesis. Certifying the loop pipelining algorithm is challenging because
there is a huge semantic gap between the input sequential design and the output
pipelined implementation making it infeasible to verify their equivalence with
automated sequential equivalence checking techniques. We discuss our ongoing
effort using ACL2 to certify loop pipelining transformation. The completion of
the proof is work in progress. However, some of the insights developed so far
may already be of value to the ACL2 community. In particular, we discuss the
key invariant we formalized, which is very different from that used in most
pipeline proofs. We discuss the needs for this invariant, its formalization in
ACL2, and our envisioned proof using the invariant. We also discuss some
trade-offs, challenges, and insights developed in course of the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1563</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1563</id><created>2014-06-05</created><authors><author><keyname>Selfridge</keyname><forenames>Benjamin</forenames><affiliation>University of Texas at Austin</affiliation></author></authors><title>An ACL2 Mechanization of an Axiomatic Framework for Weak Memory</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 129-144</journal-ref><doi>10.4204/EPTCS.152.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proving the correctness of programs written for multiple processors is a
challenging problem, due in no small part to the weaker memory guarantees
afforded by most modern architectures. In particular, the existence of store
buffers means that the programmer can no longer assume that writes to different
locations become visible to all processors in the same order. However, all
practical architectures do provide a collection of weaker guarantees about
memory consistency across processors, which enable the programmer to write
provably correct programs in spite of a lack of full sequential consistency. In
this work, we present a mechanization in the ACL2 theorem prover of an
axiomatic weak memory model (introduced by Alglave et al.). In the process, we
provide a new proof of an established theorem involving these axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1565</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1565</id><created>2014-06-05</created><authors><author><keyname>O'Leary</keyname><forenames>John W.</forenames><affiliation>Intel Corp.</affiliation></author><author><keyname>Russinoff</keyname><forenames>David M.</forenames><affiliation>Intel Corp</affiliation></author></authors><title>Modeling Algorithms in SystemC and ACL2</title><categories>cs.AR cs.PL</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 145-162</journal-ref><doi>10.4204/EPTCS.152.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the formal language MASC, based on a subset of SystemC and
intended for modeling algorithms to be implemented in hardware. By means of a
special-purpose parser, an algorithm coded in SystemC is converted to a MASC
model for the purpose of documentation, which in turn is translated to ACL2 for
formal verification. The parser also generates a SystemC variant that is
suitable as input to a high-level synthesis tool. As an illustration of this
methodology, we describe a proof of correctness of a simple 32-bit radix-4
multiplier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1566</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1566</id><created>2014-06-05</created><authors><author><keyname>Hardin</keyname><forenames>David S.</forenames><affiliation>Rockwell Collins</affiliation></author><author><keyname>Davis</keyname><forenames>Jennifer A.</forenames><affiliation>Rockwell Collins</affiliation></author><author><keyname>Greve</keyname><forenames>David A.</forenames><affiliation>Rockwell Collins</affiliation></author><author><keyname>McClurg</keyname><forenames>Jedidiah R.</forenames><affiliation>University of Colorado</affiliation></author></authors><title>Development of a Translator from LLVM to ACL2</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><acm-class>F.3.1;F.4.1</acm-class><journal-ref>EPTCS 152, 2014, pp. 163-177</journal-ref><doi>10.4204/EPTCS.152.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our current work a library of formally verified software components is to
be created, and assembled, using the Low-Level Virtual Machine (LLVM)
intermediate form, into subsystems whose top-level assurance relies on the
assurance of the individual components. We have thus undertaken a project to
build a translator from LLVM to the applicative subset of Common Lisp accepted
by the ACL2 theorem prover. Our translator produces executable ACL2 formal
models, allowing us to both prove theorems about the translated models as well
as validate those models by testing. The resulting models can be translated and
certified without user intervention, even for code with loops, thanks to the
use of the def::ung macro which allows us to defer the question of termination.
Initial measurements of concrete execution for translated LLVM functions
indicate that performance is nearly 2.4 million LLVM instructions per second on
a typical laptop computer. In this paper we overview the translation process
and illustrate the translator's capabilities by way of a concrete example,
including both a functional correctness theorem as well as a validation test
for that example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1567</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1567</id><created>2014-06-05</created><authors><author><keyname>Bertrand</keyname><forenames>Nathalie</forenames><affiliation>INRIA, Rennes</affiliation></author><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>University of Trieste</affiliation></author></authors><title>Proceedings Twelfth International Workshop on Quantitative Aspects of
  Programming Languages and Systems</title><categories>cs.LO cs.CE cs.PF</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014</journal-ref><doi>10.4204/EPTCS.154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Twelfth Workshop on Quantitative
Aspects of Programming Languages and Systems (QAPL 2014), held in Grenoble,
France, on 12 and 13 April, 2014. QAPL 2014 was a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS). The
central theme of the workshop is that of quantitative aspects of computation.
These aspects are related to the use of physical quantities (storage space,
time, bandwidth, etc.) as well as mathematical quantities (e.g. probability and
measures for reliability, security and trust), and play an important (sometimes
essential) role in characterising the behaviour and determining the properties
of systems. Such quantities are central to the definition of both the model of
systems (architecture, language design, semantics) and the methodologies and
tools for the analysis and verification of the systems properties. The aim of
this workshop is to discuss the explicit use of quantitative information such
as time and probabilities either directly in the model or as a tool for the
analysis of systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1568</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1568</id><created>2014-06-05</created><updated>2015-01-26</updated><authors><author><keyname>An&#xe9;</keyname><forenames>C&#xe9;cile</forenames></author><author><keyname>Ho</keyname><forenames>Lam Si Tung</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Phase transition on the convergence rate of parameter estimation under
  an Ornstein-Uhlenbeck diffusion on a tree</title><categories>q-bio.PE cs.CE math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion processes on trees are commonly used in evolutionary biology to
model the joint distribution of continuous traits, such as body mass, across
species. Estimating the parameters of such processes from tip values presents
challenges because of the intrinsic correlation between the observations
produced by the shared evolutionary history, thus violating the standard
independence assumption of large-sample theory. For instance Ho and An\'e
\cite{HoAne13} recently proved that the mean (also known in this context as
selection optimum) of an Ornstein-Uhlenbeck process on a tree cannot be
estimated consistently from an increasing number of tip observations if the
tree height is bounded. Here, using a fruitful connection to the so-called
reconstruction problem in probability theory, we study the convergence rate of
parameter estimation in the unbounded height case. For the mean of the process,
we provide a necessary and sufficient condition for the consistency of the
maximum likelihood estimator (MLE) and establish a phase transition on its
convergence rate in terms of the growth of the tree. In particular we show that
a loss of $\sqrt{n}$-consistency (i.e., the variance of the MLE becomes
$\Omega(n^{-1})$, where $n$ is the number of tips) occurs when the tree growth
is larger than a threshold related to the phase transition of the
reconstruction problem. For the covariance parameters, we give a novel,
efficient estimation method which achieves $\sqrt{n}$-consistency under natural
assumptions on the tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1569</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1569</id><created>2014-06-05</created><updated>2014-09-28</updated><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Two-Part Reconstruction with Noisy-Sudocodes</title><categories>cs.IT math.IT</categories><comments>23 pages, 7 figures, to appear in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2362892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a two-part reconstruction framework for signal recovery in
compressed sensing (CS), where a fast algorithm is applied to provide partial
recovery in Part 1, and a CS algorithm is applied to complete the residual
problem in Part 2. Partitioning the reconstruction process into two
complementary parts provides a natural trade-off between runtime and
reconstruction quality. To exploit the advantages of the two-part framework, we
propose a Noisy-Sudocodes algorithm that performs two-part reconstruction of
sparse signals in the presence of measurement noise. Specifically, we design a
fast algorithm for Part 1 of Noisy-Sudocodes that identifies the zero
coefficients of the input signal from its noisy measurements. Many existing CS
algorithms could be applied to Part 2, and we investigate approximate message
passing (AMP) and binary iterative hard thresholding (BIHT). For
Noisy-Sudocodes with AMP in Part 2, we provide a theoretical analysis that
characterizes the trade-off between runtime and reconstruction quality. In a
1-bit CS setting where a new 1-bit quantizer is constructed for Part 1 and BIHT
is applied to Part 2, numerical results show that the Noisy-Sudocodes algorithm
improves over BIHT in both runtime and reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1571</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1571</id><created>2014-06-05</created><authors><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Haghpanah</keyname><forenames>Nima</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>Optimal Auctions for Correlated Buyers with Sampling</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cr\'emer and McLean [1985] showed that, when buyers' valuations are drawn
from a correlated distribution, an auction with full knowledge on the
distribution can extract the full social surplus. We study whether this
phenomenon persists when the auctioneer has only incomplete knowledge of the
distribution, represented by a finite family of candidate distributions, and
has sample access to the real distribution. We show that the naive approach
which uses samples to distinguish candidate distributions may fail, whereas an
extended version of the Cr\'emer-McLean auction simultaneously extracts full
social surplus under each candidate distribution. With an algebraic argument,
we give a tight bound on the number of samples needed by this auction, which is
the difference between the number of candidate distributions and the dimension
of the linear space they span.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1572</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1572</id><created>2014-06-05</created><authors><author><keyname>Kibangou</keyname><forenames>Alain Y.</forenames></author><author><keyname>de Almeida</keyname><forenames>Andr&#xe9; L. F.</forenames></author></authors><title>Consensus-based In-Network Computation of the PARAFAC Decomposition</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a new approach for the distributed computation of
the PARAFAC decomposition of a third-order tensor across a network of
collaborating nodes. We are interested in the case where the overall data
gathered across the network can be modeled as a data tensor admitting an
essentially unique PARAFAC decomposition, while each node only observes a
sub-tensor with not necessarily enough diversity so that identifiability
conditions are not locally fulfilled at each node. In this situation,
conventional (centralized) tensor based methods cannot be applied individually
at each node. By allowing collaboration between neighboring nodes of the
network, we propose distributed versions of the alternating least squares (ALS)
and Levenberg-Marquardt (LM) algorithms for the in-network estimation of the
factor matrices of a third-order tensor. We assume that one of the factor
matrices contains parameters that are local to each node, while the two
remaining factor matrices contain global parameters that are common to the
whole network. The proposed algorithms combine the estimation of the local
factors with an in-network computation of the global factors of the PARAFAC
decomposition using average consensus over graphs. They emulate their
centralized counterparts in the case of ideal data exchange and ideal consensus
computations. The performance of the proposed algorithms are evaluated in both
ideal and imperfect cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1579</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1579</id><created>2014-06-06</created><updated>2015-04-20</updated><authors><author><keyname>Hegde</keyname><forenames>Chinmay</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Schmidt</keyname><forenames>Ludwig</forenames></author></authors><title>Approximation Algorithms for Model-Based Compressive Sensing</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing (CS) stipulates that a sparse signal can be recovered
from a small number of linear measurements, and that this recovery can be
performed efficiently in polynomial time. The framework of model-based
compressive sensing (model-CS) leverages additional structure in the signal and
prescribes new recovery schemes that can reduce the number of measurements even
further. However, model-CS requires an algorithm that solves the
model-projection problem: given a query signal, produce the signal in the model
that is also closest to the query signal. Often, this optimization can be
computationally very expensive. Moreover, an approximation algorithm is not
sufficient for this optimization task. As a result, the model-projection
problem poses a fundamental obstacle for extending model-CS to many interesting
models.
  In this paper, we introduce a new framework that we call
approximation-tolerant model-based compressive sensing. This framework includes
a range of algorithms for sparse recovery that require only approximate
solutions for the model-projection problem. In essence, our work removes the
aforementioned obstacle to model-based compressive sensing, thereby extending
the applicability of model-CS to a much wider class of models. We instantiate
this new framework for the Constrained Earth Mover Distance (CEMD) model, which
is particularly useful for signal ensembles where the positions of the nonzero
coefficients do not change significantly as a function of spatial (or temporal)
location. We develop novel approximation algorithms for both the maximization
and the minimization versions of the model-projection problem via graph
optimization techniques. Leveraging these algorithms into our framework results
in a nearly sample-optimal sparse recovery scheme for the CEMD model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1580</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1580</id><created>2014-06-06</created><authors><author><keyname>Bijalwan</keyname><forenames>Vishwanath</forenames></author><author><keyname>Kumari</keyname><forenames>Pinki</forenames></author><author><keyname>Pascual</keyname><forenames>Jordan</forenames></author><author><keyname>Semwal</keyname><forenames>Vijay Bhaskar</forenames></author></authors><title>Machine learning approach for text and document mining</title><categories>cs.IR cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1003.1795, arXiv:1212.2065
  by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text Categorization (TC), also known as Text Classification, is the task of
automatically classifying a set of text documents into different categories
from a predefined set. If a document belongs to exactly one of the categories,
it is a single-label classification task; otherwise, it is a multi-label
classification task. TC uses several tools from Information Retrieval (IR) and
Machine Learning (ML) and has received much attention in the last years from
both researchers in the academia and industry developers. In this paper, we
first categorize the documents using KNN based machine learning approach and
then return the most relevant documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1583</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1583</id><created>2014-06-06</created><authors><author><keyname>kumar</keyname><forenames>Satendra</forenames></author><author><keyname>kathuria</keyname><forenames>Mamta</forenames></author><author><keyname>Gupta</keyname><forenames>Alok Kumar</forenames></author><author><keyname>Rani</keyname><forenames>Monika</forenames></author></authors><title>Fuzzy clustering of web documents using equivalence relations and fuzzy
  hierarchical clustering</title><categories>cs.IR</categories><comments>5 pages, Software Engineering (CONSEG), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional clustering algorithms have difficulties in handling the
challenges posed by the collection of natural data which is often vague and
uncertain. Fuzzy clustering methods have the potential to manage such
situations efficiently. Fuzzy clustering method is offered to construct
clusters with uncertain boundaries and allows that one object belongs to one or
more clusters with some membership degree. In this paper, an algorithm and
experimental results are presented for fuzzy clustering of web documents using
equivalence relations and fuzzy hierarchical clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1584</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1584</id><created>2014-06-06</created><updated>2014-11-05</updated><authors><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Kurach</keyname><forenames>Karol</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Learning to Discover Efficient Mathematical Identities</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore how machine learning techniques can be applied to
the discovery of efficient mathematical identities. We introduce an attribute
grammar framework for representing symbolic expressions. Given a set of grammar
rules we build trees that combine different rules, looking for branches which
yield compositions that are analytically equivalent to a target expression, but
of lower computational complexity. However, as the size of the trees grows
exponentially with the complexity of the target expression, brute force search
is impractical for all but the simplest of expressions. Consequently, we
introduce two novel learning approaches that are able to learn from simpler
expressions to guide the tree search. The first of these is a simple n-gram
model, the other being a recursive neural-network. We show how these approaches
enable us to derive complex identities, beyond reach of brute-force search, or
human derivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1605</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1605</id><created>2014-06-06</created><authors><author><keyname>Berger</keyname><forenames>Achim</forenames></author><author><keyname>Pichler</keyname><forenames>Markus</forenames></author><author><keyname>Haslmayr</keyname><forenames>Werner</forenames></author><author><keyname>Springer</keyname><forenames>Andreas</forenames></author></authors><title>Energy Efficient and Reliable Wireless Sensor Networks - An Extension to
  IEEE 802.15.4e</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collecting sensor data in industrial environments from up to some tenth of
battery powered sensor nodes with sampling rates up to 100Hz requires energy
aware protocols, which avoid collisions and long listening phases. The IEEE
802.15.4 standard focuses on energy aware wireless sensor networks (WSNs) and
the Task Group 4e has published an amendment to fulfill up to 100 sensor value
transmissions per second per sensor node (Low Latency Deterministic Network
(LLDN) mode) to satisfy demands of factory automation. To improve the
reliability of the data collection in the star topology of the LLDN mode, we
propose a relay strategy, which can be performed within the LLDN schedule.
Furthermore we propose an extension of the star topology to collect data from
two-hop sensor nodes. The proposed Retransmission Mode enables power savings in
the sensor node of more than 33%, while reducing the packet loss by up to 50%.
To reach this performance, an optimum spatial distribution is necessary, which
is discussed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1619</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1619</id><created>2014-06-06</created><updated>2014-06-18</updated><authors><author><keyname>Diemer</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author></authors><title>An Invariant Linear Quadratic Gaussian controller for a simplified car</title><categories>cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of tracking a reference trajectory for
a simplified car model based on unicycle kinematics, whose position only is
measured, and where the control input and the measurements are corrupted by
independent Gaussian noises. To tackle this problem we devise a novel
observer-controller: the invariant Linear Quadratic Gaussian controller (ILQG).
It is based on the Linear Quadratic Gaussian controller, but the equations are
slightly modified to account for, and to exploit, the symmetries of the
problem. The gain tuning exhibits a reduced dependency on the estimated
trajectory, and is thus less sensitive to misestimates. Beyond the fact the
invariant approach is sensible (there is no reason why the controller
performance should depend on whether the reference trajectory is heading west
or south), we show through simulations that the ILQG outperforms the
conventional LQG controller in case of large noises or large initial
uncertainties. We show that those robustness properties may also prove useful
for motion planning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1621</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1621</id><created>2014-06-06</created><authors><author><keyname>Seibert</keyname><forenames>Matthias</forenames></author><author><keyname>W&#xf6;rmann</keyname><forenames>Julian</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Separable Cosparse Analysis Operator Learning</title><categories>cs.LG stat.ML</categories><comments>5 pages, 3 figures, accepted at EUSIPCO 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability of having a sparse representation for a certain class of signals
has many applications in data analysis, image processing, and other research
fields. Among sparse representations, the cosparse analysis model has recently
gained increasing interest. Many signals exhibit a multidimensional structure,
e.g. images or three-dimensional MRI scans. Most data analysis and learning
algorithms use vectorized signals and thereby do not account for this
underlying structure. The drawback of not taking the inherent structure into
account is a dramatic increase in computational cost. We propose an algorithm
for learning a cosparse Analysis Operator that adheres to the preexisting
structure of the data, and thus allows for a very efficient implementation.
This is achieved by enforcing a separable structure on the learned operator.
Our learning algorithm is able to deal with multidimensional data of arbitrary
order. We evaluate our method on volumetric data at the example of
three-dimensional MRI scans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1623</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1623</id><created>2014-06-06</created><authors><author><keyname>Kudahl</keyname><forenames>Christian</forenames></author></authors><title>Deciding the On-line Chromatic Number of a Graph with Pre-Coloring is
  PSPACE-Complete</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of determining if the on-line chromatic number of a graph is less
than or equal to k, given a pre-coloring, is shown to be PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1626</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1626</id><created>2014-06-06</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Kohli</keyname><forenames>Mahish</forenames></author></authors><title>Ant Colony Optimization for Inferring Key Gene Interactions</title><categories>cs.NE cs.CE</categories><comments>8 pages, 2 figures and 4 tables</comments><journal-ref>Proc. of 9th INDIACom-2015, 2nd International Conference on
  Computing for Sustainable Global Development, March 11-13, 2015 pp. 1242-1246</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring gene interaction network from gene expression data is an important
task in systems biology research. The gene interaction network, especially key
interactions, plays an important role in identifying biomarkers for disease
that further helps in drug design. Ant colony optimization is an optimization
algorithm based on natural evolution and has been used in many optimization
problems. In this paper, we applied ant colony optimization algorithm for
inferring the key gene interactions from gene expression data. The algorithm
has been tested on two different kinds of benchmark datasets and observed that
it successfully identify some key gene interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1633</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1633</id><created>2014-06-06</created><updated>2014-12-29</updated><authors><author><keyname>Atzemoglou</keyname><forenames>Philip</forenames><affiliation>University of Oxford</affiliation></author></authors><title>The dagger lambda calculus</title><categories>cs.LO cs.PL math.CT quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 217-235</journal-ref><doi>10.4204/EPTCS.172.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel lambda calculus that casts the categorical approach to the
study of quantum protocols into the rich and well established tradition of type
theory. Our construction extends the linear typed lambda calculus with a linear
negation of &quot;trivialised&quot; De Morgan duality. Reduction is realised through
explicit substitution, based on a symmetric notion of binding of global scope,
with rules acting on the entire typing judgement instead of on a specific
subterm. Proofs of subject reduction, confluence, strong normalisation and
consistency are provided, and the language is shown to be an internal language
for dagger compact categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1638</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1638</id><created>2014-06-06</created><authors><author><keyname>Chen</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Song</keyname><forenames>Dan</forenames></author><author><keyname>Wang</keyname><forenames>Dongming</forenames></author></authors><title>Automated Generation of Geometric Theorems from Images of Diagrams</title><categories>cs.AI</categories><comments>31 pages. Submitted to Annals of Mathematics and Artificial
  Intelligence (special issue on Geometric Reasoning)</comments><msc-class>68T10, 68T15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to generate geometric theorems from electronic images
of diagrams automatically. The approach makes use of techniques of Hough
transform to recognize geometric objects and their labels and of numeric
verification to mine basic geometric relations. Candidate propositions are
generated from the retrieved information by using six strategies and geometric
theorems are obtained from the candidates via algebraic computation.
Experiments with a preliminary implementation illustrate the effectiveness and
efficiency of the proposed approach for generating nontrivial theorems from
images of diagrams. This work demonstrates the feasibility of automated
discovery of profound geometric knowledge from simple image data and has
potential applications in geometric knowledge management and education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1655</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1655</id><created>2014-06-06</created><updated>2014-09-30</updated><authors><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author></authors><title>Variational inference of latent state sequences using Recurrent Networks</title><categories>stat.ML cs.LG</categories><comments>This paper has been withdrawn due to a derivation/implementation
  error and the resulting invalidation of the results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in the estimation of deep directed graphical models and
recurrent networks let us contribute to the removal of a blind spot in the area
of probabilistc modelling of time series. The proposed methods i) can infer
distributed latent state-space trajectories with nonlinear transitions, ii)
scale to large data sets thanks to the use of a stochastic objective and fast,
approximate inference, iii) enable the design of rich emission models which iv)
will naturally lead to structured outputs. Two different paths of introducing
latent state sequences are pursued, leading to the variational recurrent auto
encoder (VRAE) and the variational one step predictor (VOSP). The use of
independent Wiener processes as priors on the latent state sequence is a viable
compromise between efficient computation of the Kullback-Leibler divergence
from the variational approximation of the posterior and maintaining a
reasonable belief in the dynamics. We verify our methods empirically, obtaining
results close or superior to the state of the art. We also show qualitative
results for denoising and missing value imputation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1677</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1677</id><created>2014-06-06</created><authors><author><keyname>Chadha</keyname><forenames>Ankit R.</forenames></author><author><keyname>Misal</keyname><forenames>Rishikesh</forenames></author><author><keyname>Mokashi</keyname><forenames>Tanaya</forenames></author></authors><title>Modified Binary Search Algorithm</title><categories>cs.DS cs.IT math.IT</categories><journal-ref>International Journal of Applied Information Systems 7(2):37-40,
  April 2014</journal-ref><doi>10.5120/ijais14-451131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a modification to the traditional binary search algorithm
in which it checks the presence of the input element with the middle element of
the given set of elements at each iteration. Modified binary search algorithm
optimizes the worst case of the binary search algorithm by comparing the input
element with the first &amp; last element of the data set along with the middle
element and also checks the input number belongs to the range of numbers
present in the given data set at each iteration there by reducing the time
taken by the worst cases of binary search algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1691</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1691</id><created>2014-06-06</created><authors><author><keyname>Lange</keyname><forenames>Vanessa</forenames></author><author><keyname>Schmitt</keyname><forenames>Manuel</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>Towards a Better Understanding of the Local Attractor in Particle Swarm
  Optimization: Speed and Solution Quality</title><categories>cs.NE</categories><acm-class>I.2.8</acm-class><doi>10.1007/978-3-319-11298-5_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic
for solving continuous optimization problems. Although this technique is widely
used, the understanding of the mechanisms that make swarms so successful is
still limited. We present the first substantial experimental investigation of
the influence of the local attractor on the quality of exploration and
exploitation. We compare in detail classical PSO with the social-only variant
where local attractors are ignored. To measure the exploration capabilities, we
determine how frequently both variants return results in the neighborhood of
the global optimum. We measure the quality of exploitation by considering only
function values from runs that reached a search point sufficiently close to the
global optimum and then comparing in how many digits such values still deviate
from the global minimum value. It turns out that the local attractor
significantly improves the exploration, but sometimes reduces the quality of
the exploitation. As a compromise, we propose and evaluate a hybrid PSO which
switches off its local attractors at a certain point in time. The effects
mentioned can also be observed by measuring the potential of the swarm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1695</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1695</id><created>2014-06-06</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>Tsallis information dimension of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 4 figures</comments><doi>10.1016/j.physa.2014.10.071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fractal and self-similarity properties are revealed in many complex
networks. In order to show the influence of different part in the complex
networks to the information dimension, we have proposed a new information
dimension based on Tsallis entropy namely Tsallis information dimension. The
Tsallis information dimension can show the fractal property from different
perspective by set different value of q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1697</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1697</id><created>2014-06-06</created><authors><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Multiscale probability transformation of basic probability assignment</title><categories>cs.AI</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making is still an open issue in the application of Dempster-Shafer
evidence theory. A lot of works have been presented for it. In the transferable
belief model (TBM), pignistic probabilities based on the basic probability as-
signments are used for decision making. In this paper, multiscale probability
transformation of basic probability assignment based on the belief function and
the plausibility function is proposed, which is a generalization of the
pignistic probability transformation. In the multiscale probability function, a
factor q based on the Tsallis entropy is used to make the multiscale prob-
abilities diversified. An example is shown that the multiscale probability
transformation is more reasonable in the decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1701</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1701</id><created>2014-06-06</created><authors><author><keyname>Kirk</keyname><forenames>Nathan</forenames></author><author><keyname>Benson</keyname><forenames>Alan</forenames></author><author><keyname>Goodyer</keyname><forenames>Christopher</forenames></author><author><keyname>Hubbard</keyname><forenames>Matthew</forenames></author></authors><title>A computational study of the effects of remodelled electrophysiology and
  mechanics on initiation of ventricular fibrillation in human heart failure</title><categories>cs.CE cs.NA math.NA q-bio.TO</categories><comments>20 pages, 8 figures, 3 tables</comments><msc-class>35Q74, 35Q92, 65F08, 65M60, 65N30, 92-08, 74H15, 74S05</msc-class><acm-class>G.1.0; G.1.3; G.1.8; G.4; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of pathological cardiac conditions such as arrhythmias, a major
cause of mortality in heart failure, is becoming increasingly informed by
computational simulation, numerically modelling the governing equations. This
can provide insight where experimental work is constrained by technical
limitations and/or ethical issues.
  As the models become more realistic, the construction of efficient and
accurate computational models becomes increasingly challenging. In particular,
recent developments have started to couple the electrophysiology models with
mechanical models in order to investigate the effect of tissue deformation on
arrhythmogenesis, thus introducing an element of nonlinearity into the
mathematical representation. This paper outlines a biophysically-detailed
computational model of coupled electromechanical cardiac activity which uses
the finite element method to approximate both electrical and mechanical systems
on unstructured, deforming, meshes. An ILU preconditioner is applied to improve
performance of the solver.
  This software is used to examine the role of electrophysiology, fibrosis and
mechanical deformation on the stability of spiral wave dynamics in human
ventricular tissue by applying it to models of both healthy and failing tissue.
The latter was simulated by modifying (i) cellular electrophysiological
properties, to generate an increased action potential duration and altered
intracellular calcium handling, and (ii) tissue-level properties, to simulate
the gap junction remodelling, fibrosis and increased tissue stiffness seen in
heart failure. The resulting numerical experiments suggest that, for the chosen
mathematical models of electrophysiology and mechanical response, introducing
tissue level fibrosis can have a destabilising effect on the dynamics, while
the net effect of the electrophysiological remodelling stabilises the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1704</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1704</id><created>2014-06-06</created><authors><author><keyname>Gnang</keyname><forenames>Edinah K.</forenames></author><author><keyname>Radziwill</keyname><forenames>Maksym</forenames></author><author><keyname>Sanna</keyname><forenames>Carlo</forenames></author></authors><title>Counting arithmetic formulas</title><categories>math.CO cs.DM math.NT</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arithmetic formula is an expression involving only the constant $1$, and
the binary operations of addition and multiplication, with multiplication by
$1$ not allowed. We obtain an asymptotic formula for the number of arithmetic
formulas evaluating to $n$ as $n$ goes to infinity, solving a conjecture of E.
K. Gnang and D. Zeilberger. We give also an asymptotic formula for the number
of arithmetic formulas evaluating to $n$ and using exactly $k$ multiplications.
Finally we analyze three specific encodings for producing arithmetic formulas.
For almost all integers $n$, we compare the lengths of the arithmetic formulas
for $n$ that each encoding produces with the length of the shortest formula for
$n$ (which we estimate from below). We briefly discuss the time-space tradeoff
offered by each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1717</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1717</id><created>2014-06-06</created><authors><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Median Filtering is Equivalent to Sorting</title><categories>cs.DS</categories><comments>1 + 24 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows that the following problems are equivalent, both in theory
and in practice:
  - median filtering: given an $n$-element vector, compute the sliding window
median with window size $k$,
  - piecewise sorting: given an $n$-element vector, divide it in $n/k$ blocks
of length $k$ and sort each block.
  By prior work, median filtering is known to be at least as hard as piecewise
sorting: with a single median filter operation we can sort $\Theta(n/k)$ blocks
of length $\Theta(k)$. The present work shows that median filtering is also as
easy as piecewise sorting: we can do median filtering with one piecewise
sorting operation and linear-time postprocessing. In particular, median
filtering can directly benefit from the vast literature on sorting
algorithms---for example, adaptive sorting algorithms imply adaptive median
filtering algorithms.
  The reduction is very efficient in practice---for random inputs the
performance of the new sorting-based algorithm is on a par with the fastest
heap-based algorithms, and for benign data distributions it typically
outperforms prior algorithms.
  The key technical idea is that we can represent the sliding window with a
pair of sorted doubly-linked lists: we delete items from one list and add items
to the other list. Deletions are easy; additions can be done efficiently if we
reverse the time twice: First we construct the full list and delete the items
in the reverse order. Then we undo each deletion with Knuth's dancing links
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1719</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1719</id><created>2014-06-03</created><updated>2014-07-11</updated><authors><author><keyname>Yomdin</keyname><forenames>Y.</forenames></author></authors><title>Smooth Parametrizations in Dynamics, Analysis, Diophantine and
  Computational Geometry</title><categories>cs.CG math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smooth parametrization consists in a subdivision of the mathematical objects
under consideration into simple pieces, and then parametric representation of
each piece, while keeping control of high order derivatives. The main goal of
the present paper is to provide a short overview of some results and open
problems on smooth parametrization and its applications in several apparently
rather separated domains: Smooth Dynamics, Diophantine Geometry, Approximation
Theory, and Computational Geometry.
  The structure of the results, open problems, and conjectures in each of these
domains shows in many cases a remarkable similarity, which we try to stress.
Sometimes this similarity can be easily explained, sometimes the reasons remain
somewhat obscure, and it motivates some natural questions discussed in the
paper. We present also some new results, stressing interconnection between
various types and various applications of smooth parametrization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1724</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1724</id><created>2014-06-06</created><updated>2015-08-01</updated><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author><author><keyname>Tawfik</keyname><forenames>Hazim</forenames></author></authors><title>Random Aerial Beamforming for Underlay Cognitive Radio with Exposed
  Secondary Users</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the exposed secondary users problem in underlay
cognitive radio systems, where both the secondary-to-primary and
primary-to-secondary channels have a Line-of-Sight (LoS) component. Based on a
Rician model for the LoS channels, we show, analytically and numerically, that
LoS interference hinders the achievable secondary user capacity when
interference constraints are imposed at the primary user receiver. This is
caused by the poor dynamic range of the interference channels fluctuations when
a dominant LoS component exists. In order to improve the capacity of such
system, we propose the usage of an Electronically Steerable Parasitic Array
Radiator (ESPAR) antennas at the secondary terminals. An ESPAR antenna involves
a single RF chain and has a reconfigurable radiation pattern that is controlled
by assigning arbitrary weights to M orthonormal basis radiation patterns via
altering a set of reactive loads. By viewing the orthonormal patterns as
multiple virtual dumb antennas, we randomly vary their weights over time
creating artificial channel fluctuations that can perfectly eliminate the
undesired impact of LoS interference. This scheme is termed as Random Aerial
Beamforming (RAB), and is well suited for compact and low cost mobile terminals
as it uses a single RF chain. Moreover, we investigate the exposed secondary
users problem in a multiuser setting, showing that LoS interference hinders
multiuser interference diversity and affects the growth rate of the SU capacity
as a function of the number of users. Using RAB, we show that LoS interference
can actually be exploited to improve multiuser diversity via opportunistic
nulling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1725</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1725</id><created>2014-05-29</created><updated>2015-08-05</updated><authors><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Zhou</keyname><forenames>Jiantao</forenames></author></authors><title>Bi-level Protected Compressive Sampling</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some pioneering works have investigated embedding cryptographic properties in
compressive sampling (CS) in a way similar to one-time pad symmetric cipher.
This paper tackles the problem of constructing a CS-based symmetric cipher
under the key reuse circumstance, i.e., the cipher is resistant to common
attacks even a fixed measurement matrix is used multiple times. To this end, we
suggest a bi-level protected CS (BLP-CS) model which makes use of the advantage
of the non-RIP measurement matrix construction. Specifically, two kinds of
artificial basis mismatch techniques are investigated to construct key-related
sparsifying bases. It is demonstrated that the encoding process of BLP-CS is
simply a random linear projection, which is the same as the basic CS model.
However, decoding the linear measurements requires knowledge of both the
key-dependent sensing matrix and its sparsifying basis. The proposed model is
exemplified by sampling images as a joint data acquisition and protection layer
for resource-limited wireless sensors. Simulation results and numerical
analyses have justified that the new model can be applied in circumstances
where the measurement matrix can be re-used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1727</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1727</id><created>2014-06-03</created><authors><author><keyname>He</keyname><forenames>Chao</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Ketterl</keyname><forenames>Thomas P.</forenames></author><author><keyname>Arrobo</keyname><forenames>Gabriel E.</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>MIMO In Vivo</title><categories>cs.IT math.IT</categories><comments>WAMICON 2014</comments><doi>10.1109/WAMICON.2014.6857757</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the performance of MIMO for in vivo environments, using ANSYS HFSS
and their complete human body model, to determine the maximum data rates that
can be achieved using an IEEE 802.11n system. Due to the lossy nature of the in
vivo medium, achieving high data rates with reliable performance will be a
challenge, especially since the in vivo antenna performance is strongly
affected by near field coupling to the lossy medium and the signals levels will
be limited by specified specific absorption rate (SAR) levels. We analyzed the
bit error rate (BER) of a MIMO system with one pair of antennas placed in vivo
and the second pair placed inside and outside the body at various distances
from the in vivo antennas. The results were compared to SISO simulations and
showed that by using MIMO in vivo, significant performance gain can be
achieved, and at least two times the data rate can be supported with SAR
limited transmit power levels, making it possible to achieve target data rates
in the 100 Mbps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1738</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1738</id><created>2014-06-06</created><authors><author><keyname>Garone</keyname><forenames>Emanuele</forenames></author><author><keyname>Nicotra</keyname><forenames>Marco M.</forenames></author></authors><title>Explicit Reference Governor for Continuous Time Nonlinear Systems
  Subject to Convex Constraints</title><categories>cs.SY</categories><comments>Submitted to: IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel closed-form strategy that dynamically modifies
the reference of a pre-compensated nonlinear system to ensure the satisfaction
of a set of convex constraints. The main idea consists of translating
constraints in the state space into constraints on the Lyapunov function and
then modulating the reference velocity so as to limit the value of the Lyapunov
function. The theory is introduced for general nonlinear systems subject to
convex constraints. In the case of polyhedric constraints, an explicit solution
is provided for the large and highly relevant class of nonlinear systems whose
Lyapunov function is lower-bounded by a quadratic form. In view of improving
performances, further specializations are provided for the relevant cases of
linear systems and robotic manipulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1754</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1754</id><created>2014-06-05</created><authors><author><keyname>Sprunger</keyname><forenames>David</forenames></author><author><keyname>Tune</keyname><forenames>William</forenames></author><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Moss</keyname><forenames>Lawrence S.</forenames></author></authors><title>Eigenvalues and Transduction of Morphic Sequences: Extended Version</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study finite state transduction of automatic and morphic sequences.
Dekking proved that morphic sequences are closed under transduction and in
particular morphic images. We present a simple proof of this fact, and use the
construction in the proof to show that non-erasing transductions preserve a
condition called alpha-substitutivity. Roughly, a sequence is
alpha-substitutive if the sequence can be obtained as the limit of iterating a
substitution with dominant eigenvalue alpha. Our results culminate in the
following fact: for multiplicatively independent real numbers alpha and beta,
if v is an alpha-substitutive sequence and w is a beta-substitutive sequence,
then v and w have no common non-erasing transducts except for the ultimately
periodic sequences. We rely on Cobham's theorem for substitutions, a recent
result of Durand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1758</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1758</id><created>2014-06-06</created><authors><author><keyname>Curien</keyname><forenames>Nicolas</forenames></author><author><keyname>Duquesne</keyname><forenames>Thomas</forenames></author><author><keyname>Kortchemski</keyname><forenames>Igor</forenames></author><author><keyname>Manolescu</keyname><forenames>Ioan</forenames></author></authors><title>Scaling limits and influence of the seed graph in preferential
  attachment trees</title><categories>math.PR cs.DM math.ST stat.TH</categories><comments>32 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the asymptotics of random trees built by linear
preferential attachment, also known in the literature as Barab\'asi-Albert
trees or plane-oriented recursive trees. We first prove a conjecture of Bubeck,
Mossel \&amp; R\'acz concerning the influence of the seed graph on the asymptotic
behavior of such trees. Separately we study the geometric structure of nodes of
large degrees in a plane version of Barab\'asi-Albert trees via their
associated looptrees. As the number of nodes grows, we show that these
looptrees, appropriately rescaled, converge in the Gromov-Hausdorff sense
towards a random compact metric space which we call the Brownian looptree. The
latter is constructed as a quotient space of Aldous' Brownian Continuum Random
Tree and is shown to have almost sure Hausdorff dimension $2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1765</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1765</id><created>2014-06-06</created><authors><author><keyname>Condamines</keyname><forenames>Anne</forenames></author><author><keyname>Warnier</keyname><forenames>Maxime</forenames></author></authors><title>Linguistic Analysis of Requirements of a Space Project and their
  Conformity with the Recommendations Proposed by a Controlled Natural Language</title><categories>cs.SE cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long term aim of the project carried out by the French National Space
Agency (CNES) is to design a writing guide based on the real and regular
writing of requirements. As a first step in the project, this paper proposes a
lin-guistic analysis of requirements written in French by CNES engineers. The
aim is to determine to what extent they conform to two rules laid down in
INCOSE, a recent guide for writing requirements. Although CNES engineers are
not obliged to follow any Controlled Natural Language in their writing of
requirements, we believe that language regularities are likely to emerge from
this task, mainly due to the writers' experience. The issue is approached using
natural language processing tools to identify sentences that do not comply with
INCOSE rules. We further review these sentences to understand why the
recommendations cannot (or should not) always be applied when specifying
large-scale projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1767</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1767</id><created>2014-06-03</created><authors><author><keyname>Salge</keyname><forenames>Christoph</forenames></author><author><keyname>Glackin</keyname><forenames>Cornelius</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Changing the Environment Based on Empowerment as Intrinsic Motivation</title><categories>cs.AI nlin.AO</categories><comments>31 pages, 8 figures, published in Entropy
  (http://www.mdpi.com/1099-4300/16/5/2789), much extended version of
  http://arxiv.org/abs/1310.3692</comments><journal-ref>Entropy 16, no. 5: 2789-2819 (2014)</journal-ref><doi>10.3390/e16052789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One aspect of intelligence is the ability to restructure your own environment
so that the world you live in becomes more beneficial to you. In this paper we
investigate how the information-theoretic measure of agent empowerment can
provide a task-independent, intrinsic motivation to restructure the world. We
show how changes in embodiment and in the environment change the resulting
behaviour of the agent and the artefacts left in the world. For this purpose,
we introduce an approximation of the established empowerment formalism based on
sparse sampling, which is simpler and significantly faster to compute for
deterministic dynamics. Sparse sampling also introduces a degree of randomness
into the decision making process, which turns out to beneficial for some cases.
We then utilize the measure to generate agent behaviour for different agent
embodiments in a Minecraft-inspired three dimensional block world. The
paradigmatic results demonstrate that empowerment can be used as a suitable
generic intrinsic motivation to not only generate actions in given static
environments, as shown in the past, but also to modify existing environmental
conditions. In doing so, the emerging strategies to modify an agent's
environment turn out to be meaningful to the specific agent capabilities, i.e.,
de facto to its embodiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1770</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1770</id><created>2014-06-06</created><authors><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author><author><keyname>Mutch</keyname><forenames>Jim</forenames></author><author><keyname>Isik</keyname><forenames>Leyla</forenames></author></authors><title>Computational role of eccentricity dependent cortical magnification</title><categories>cs.LG q-bio.NC</categories><report-no>CBMM memo 17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sampling extension of M-theory focused on invariance to scale
and translation. Quite surprisingly, the theory predicts an architecture of
early vision with increasing receptive field sizes and a high resolution fovea
-- in agreement with data about the cortical magnification factor, V1 and the
retina. From the slope of the inverse of the magnification factor, M-theory
predicts a cortical &quot;fovea&quot; in V1 in the order of $40$ by $40$ basic units at
each receptive field size -- corresponding to a foveola of size around $26$
minutes of arc at the highest resolution, $\approx 6$ degrees at the lowest
resolution. It also predicts uniform scale invariance over a fixed range of
scales independently of eccentricity, while translation invariance should
depend linearly on spatial frequency. Bouma's law of crowding follows in the
theory as an effect of cortical area-by-cortical area pooling; the Bouma
constant is the value expected if the signature responsible for recognition in
the crowding experiments originates in V2. From a broader perspective, the
emerging picture suggests that visual recognition under natural conditions
takes place by composing information from a set of fixations, with each
fixation providing recognition from a space-scale image fragment -- that is an
image patch represented at a set of increasing sizes and decreasing
resolutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1774</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1774</id><created>2014-06-06</created><updated>2014-06-13</updated><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames><affiliation>Janelia Farm Research Campus- HHMI</affiliation></author><author><keyname>Plaza</keyname><forenames>Stephen</forenames><affiliation>Janelia Farm Research Campus- HHMI</affiliation></author><author><keyname>Scheffer</keyname><forenames>Louis</forenames><affiliation>Janelia Farm Research Campus- HHMI</affiliation></author></authors><title>Small Sample Learning of Superpixel Classifiers for EM Segmentation-
  Extended Version</title><categories>cs.CV</categories><comments>Accepted for MICCAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pixel and superpixel classifiers have become essential tools for EM
segmentation algorithms. Training these classifiers remains a major bottleneck
primarily due to the requirement of completely annotating the dataset which is
tedious, error-prone and costly. In this paper, we propose an interactive
learning scheme for the superpixel classifier for EM segmentation. Our
algorithm is &quot;active semi-supervised&quot; because it requests the labels of a small
number of examples from user and applies label propagation technique to
generate these queries. Using only a small set ($&lt;20\%$) of all datapoints, the
proposed algorithm consistently generates a classifier almost as accurate as
that estimated from a complete groundtruth. We provide segmentation results on
multiple datasets to show the strength of these classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1777</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1777</id><created>2014-06-06</created><authors><author><keyname>Krivulin</keyname><forenames>N.</forenames></author></authors><title>Algebraic solutions of tropical optimization problems</title><categories>math.OC cs.SY</categories><comments>25 pages, presented at Intern. Conf. &quot;Algebra and Mathematical Logic:
  Theory and Applications&quot;, June 2-6, 2014, Kazan, Russia</comments><msc-class>65K05 (Primary), 15A80, 90C48, 65K10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multidimensional optimization problems, which are formulated and
solved in terms of tropical mathematics. The problems are to minimize
(maximize) a linear or nonlinear function defined on vectors of a
finite-dimensional semimodule over an idempotent semifield, and may have
constraints in the form of linear equations and inequalities. The aim of the
paper is twofold: first to give a broad overview of known tropical optimization
problems and solution methods, including recent results; and second, to derive
a direct, complete solution to a new constrained optimization problem as an
illustration of the algebraic approach recently proposed to solve tropical
optimization problems with nonlinear objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1784</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1784</id><created>2014-06-06</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Degree mixing in multilayer networks impedes the evolution of
  cooperation</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>8 two-column pages, 9 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 89 (2014) 052813</journal-ref><doi>10.1103/PhysRevE.89.052813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, the evolution of cooperation has been studied on single,
isolated networks. Yet a player, especially in human societies, will typically
be a member of many different networks, and those networks will play a
different role in the evolutionary process. Multilayer networks are therefore
rapidly gaining on popularity as the more apt description of a networked
society. With this motivation, we here consider 2-layer scale-free networks
with all possible combinations of degree mixing, wherein one network layer is
used for the accumulation of payoffs and the other is used for strategy
updating. We find that breaking the symmetry through assortative mixing in one
layer and/or disassortative mixing in the other layer, as well as preserving
the symmetry by means of assortative mixing in both layers, impedes the
evolution of cooperation. We use degree-dependent distributions of strategies
and cluster-size analysis to explain these results, which highlight the
importance of hubs and the preservation of symmetry between multilayer networks
for the successful resolution of social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1790</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1790</id><created>2014-06-06</created><authors><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>Behavioral Mechanism Design: Optimal Contests for Simple Agents</title><categories>cs.GT</categories><comments>This is the full version of a paper in the ACM Conference on
  Economics and Computation (ACM-EC), 2014</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incentives are more likely to elicit desired outcomes when they are designed
based on accurate models of agents' strategic behavior. A growing literature,
however, suggests that people do not quite behave like standard economic agents
in a variety of environments, both online and offline. What consequences might
such differences have for the optimal design of mechanisms in these
environments? In this paper, we explore this question in the context of optimal
contest design for simple agents---agents who strategically reason about
whether or not to participate in a system, but not about the input they provide
to it. Specifically, consider a contest where $n$ potential contestants with
types $(q_i,c_i)$ each choose between participating and producing a submission
of quality $q_i$ at cost $c_i$, versus not participating at all, to maximize
their utilities. How should a principal distribute a total prize $V$ amongst
the $n$ ranks to maximize some increasing function of the qualities of elicited
submissions in a contest with such simple agents?
  We first solve the optimal contest design problem for settings with
homogenous participation costs $c_i = c$. Here, the optimal contest is always a
simple contest, awarding equal prizes to the top $j^*$ contestants for a
suitable choice of $j^*$. (In comparable models with strategic effort choices,
the optimal contest is either a winner-take-all contest or awards possibly
unequal prizes, depending on the curvature of agents' effort cost functions.)
We next address the general case with heterogeneous costs where agents' types
are inherently two-dimensional, significantly complicating equilibrium
analysis. Our main result here is that the winner-take-all contest is a
3-approximation of the optimal contest when the principal's objective is to
maximize the quality of the best elicited contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1794</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1794</id><created>2014-06-07</created><authors><author><keyname>B</keyname><forenames>Anita</forenames></author><author><keyname>Sheril</keyname><forenames>Beena</forenames></author><author><keyname>E</keyname><forenames>Ramesh B.</forenames></author></authors><title>Advanced vehicle safety and content distribution system</title><categories>cs.NI</categories><comments>4 pages, 2 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>Volume 11 Number 9 - May 2014, IJETT-V11P284</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced vehicle content distribution system (ACDS)is complemented by
improved network connectivity with Mobile Network 3G, 4G network. Advanced
content distribution system uses Access Points deployed along roadside. APs
co-ordinate and collaborate to distribute content to vehicles in mobility. The
infrastructure of deployed APs solves real time issues like predicting errors
in movement, limited information shared to vehicles on movement due to limited
resources. The advances vehicle content distribution system structures APs in
to form a map which is considering the vehicle contact pattern which is
analyzed by APs. The system is more effective by optimizing the network
consumption by sharing prefectched data between APs. The process depends on APs
storage, bandwidth and load on the origin APs which is connected to internet.
With the features Advanced System to distribute the content the system takes
care of improving road safety, delivery accuracy and important content
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1795</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1795</id><created>2014-06-06</created><authors><author><keyname>Fatema</keyname><forenames>Nusrat</forenames></author><author><keyname>Brad</keyname><forenames>Remus</forenames></author></authors><title>Security Requirements, Counterattacks and Projects in Healthcare
  Applications Using WSNs - A Review</title><categories>cs.CR cs.CY</categories><journal-ref>International Journal of Computer Networking and Communication,
  vol. 2 (2), pp. 1-9, May 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Healthcare applications are well thought-out as interesting fields for WSN
where patients can be examine using wireless medical sensor networks. Inside
the hospital or extensive care surroundings there is a tempting need for steady
monitoring of essential body functions and support for patient mobility. Recent
research cantered on patient reliable communication, mobility, and
energy-efficient routing. Yet deploying new expertise in healthcare
applications presents some understandable security concerns which are the
important concern in the inclusive deployment of wireless patient monitoring
systems. This manuscript presents a survey of the security features, its
counter attacks in healthcare applications including some proposed projects
which have been done recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1796</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1796</id><created>2014-06-06</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>A Generic Numbering System based on Catalan Families of Combinatorial
  Objects</title><categories>cs.MS cs.DS</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study novel arithmetic algorithms on a canonical number representation
based on the Catalan family of combinatorial objects.
  Our algorithms work on a generic representation that we illustrate on
instances like ordered binary and multiway trees, balanced parentheses
languages as well as the usual bitstring-based natural numbers seen through the
same generic interface as members of the Catalan family.
  For numbers corresponding to Catalan objects of low representation
complexity, our algorithms provide super-exponential gains while their average
and worst case complexity is within constant factors of their traditional
counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1803</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1803</id><created>2014-06-06</created><authors><author><keyname>Chen</keyname><forenames>Yen-Chi</forenames></author><author><keyname>Genovese</keyname><forenames>Christopher R.</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Generalized Mode and Ridge Estimation</title><categories>stat.ME cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized density is a product of a density function and a weight
function. For example, the average local brightness of an astronomical image is
the probability of finding a galaxy times the mean brightness of the galaxy. We
propose a method for studying the geometric structure of generalized densities.
In particular, we show how to find the modes and ridges of a generalized
density function using a modification of the mean shift algorithm and its
variant, subspace constrained mean shift. Our method can be used to perform
clustering and to calculate a measure of connectivity between clusters. We
establish consistency and rates of convergence for our estimator and apply the
methods to data from two astronomical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1808</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1808</id><created>2014-06-06</created><authors><author><keyname>Lacerda</keyname><forenames>Gustavo</forenames></author></authors><title>Upper-Bounding Proof Length with the Busy Beaver</title><categories>math.LO cs.LO</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a short theorem, i.e. one that can be written down using just a few
symbols. Can its shortest proof be arbitrarily long? We answer this question in
the negative. Inspired by arguments by Calude et al (1999) and Chaitin (1984)
that construct an upper bound on the first counterexample of a $\Pi_1$ sentence
as a function of the sentence's length, we present a similar argument about
proof length for arbitrary statements. As with the above, our bound is
uncomputable, since it uses a Busy Beaver oracle. Unlike the above, our result
is not restricted to any complexity class. Finally, we combine the above search
procedures into an automatic (albeit uncomputable) procedure for discovering
G\&quot;{o}del sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1818</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1818</id><created>2014-06-06</created><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Multi-Application Resource Allocation with Users Discrimination in
  Cellular Networks</title><categories>cs.NI</categories><comments>submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider resource allocation optimization problem in
cellular networks for different types of users running multiple applications
simultaneously. In our proposed model, each user application is assigned a
utility function that represents the application type running on the user
equipment (UE). The network operators assign a subscription weight to each UE
based on its subscription. Each UE assigns an application weight to each of its
applications based on the instantaneous usage percentage of the application.
Additionally, UEs with higher priority assign applications target rates to
their applications. Our objective is to allocate the resources optimally among
the UEs and their applications from a single evolved node B (eNodeB) based on a
utility proportional fairness policy with priority to real-time application
users. A minimum quality of service (QoS) is guaranteed to each UE application
based on the UE subscription weight, the UE application weight and the UE
application target rate. We propose a two-stage rate allocation algorithm to
allocate the eNodeB resources among users and their applications. Finally, we
present simulation results for the performance of our rate allocation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1822</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1822</id><created>2014-06-06</created><updated>2015-11-14</updated><authors><author><keyname>Choromanska</keyname><forenames>Anna</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Logarithmic Time Online Multiclass prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of multiclass classification with an extremely large
number of classes (k), with the goal of obtaining train and test time
complexity logarithmic in the number of classes. We develop top-down tree
construction approaches for constructing logarithmic depth trees. On the
theoretical front, we formulate a new objective function, which is optimized at
each node of the tree and creates dynamic partitions of the data which are both
pure (in terms of class labels) and balanced. We demonstrate that under
favorable conditions, we can construct logarithmic depth trees that have leaves
with low label entropy. However, the objective function at the nodes is
challenging to optimize computationally. We address the empirical problem with
a new online decision tree construction procedure. Experiments demonstrate that
this online algorithm quickly achieves improvement in test error compared to
more common logarithmic training time approaches, which makes it a plausible
method in computationally constrained large-k applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1823</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1823</id><created>2014-06-06</created><authors><author><keyname>Dara</keyname><forenames>Sashank</forenames></author></authors><title>Multi-user protocols with access control for computational privacy in
  public clouds</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational privacy is a property of cryptographic system that ensures the
privacy of data being processed at an untrusted server. Fully Homomorphic
Encryption Schemes (FHE) promise to provide such property. Contemporary FHE
schemes are suited for applications that have single user and server. In
reality many of the cloud applications involve multiple users with various
degrees of trust and the server need not necessarily be aware of it too. We
present a Complementary Key Pairs technique and protocols based on that to
scale any generic FHE schemes to multi user scenarios. We also use such
technique along with FHE to show how attribute based access control can be
achieved while server being oblivious of the same. We analyze the protocols and
their security. Our protocols don't make any assumptions on how FHE scheme
itself works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1827</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1827</id><created>2014-06-06</created><updated>2015-05-14</updated><authors><author><keyname>Bowman</keyname><forenames>Samuel R.</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Recursive Neural Networks Can Learn Logical Semantics</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-structured recursive neural networks (TreeRNNs) for sentence meaning
have been successful for many applications, but it remains an open question
whether the fixed-length representations that they learn can support tasks as
demanding as logical deduction. We pursue this question by evaluating whether
two such models---plain TreeRNNs and tree-structured neural tensor networks
(TreeRNTNs)---can correctly learn to identify logical relationships such as
entailment and contradiction using these representations. In our first set of
experiments, we generate artificial data from a logical grammar and use it to
evaluate the models' ability to learn to handle basic relational reasoning,
recursive structures, and quantification. We then evaluate the models on the
more natural SICK challenge data. Both models perform competitively on the SICK
data and generalize well in all three experiments on simulated data, suggesting
that they can learn suitable representations for logical inference in natural
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1831</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1831</id><created>2014-06-06</created><authors><author><keyname>Poole</keyname><forenames>Ben</forenames></author><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author></authors><title>Analyzing noise in autoencoders and deep networks</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoencoders have emerged as a useful framework for unsupervised learning of
internal representations, and a wide variety of apparently conceptually
disparate regularization techniques have been proposed to generate useful
features. Here we extend existing denoising autoencoders to additionally inject
noise before the nonlinearity, and at the hidden unit activations. We show that
a wide variety of previous methods, including denoising, contractive, and
sparse autoencoders, as well as dropout can be interpreted using this
framework. This noise injection framework reaps practical benefits by providing
a unified strategy to develop new internal representations by designing the
nature of the injected noise. We show that noisy autoencoders outperform
denoising autoencoders at the very task of denoising, and are competitive with
other single-layer techniques on MNIST, and CIFAR-10. We also show that types
of noise other than dropout improve performance in a deep network through
sparsifying, decorrelating, and spreading information across representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1833</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1833</id><created>2014-06-06</created><updated>2014-06-09</updated><authors><author><keyname>Szerlip</keyname><forenames>Paul A.</forenames></author><author><keyname>Morse</keyname><forenames>Gregory</forenames></author><author><keyname>Pugh</keyname><forenames>Justin K.</forenames></author><author><keyname>Stanley</keyname><forenames>Kenneth O.</forenames></author></authors><title>Unsupervised Feature Learning through Divergent Discriminative Feature
  Accumulation</title><categories>cs.NE cs.LG</categories><comments>Corrected citation formatting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike unsupervised approaches such as autoencoders that learn to reconstruct
their inputs, this paper introduces an alternative approach to unsupervised
feature learning called divergent discriminative feature accumulation (DDFA)
that instead continually accumulates features that make novel discriminations
among the training set. Thus DDFA features are inherently discriminative from
the start even though they are trained without knowledge of the ultimate
classification problem. Interestingly, DDFA also continues to add new features
indefinitely (so it does not depend on a hidden layer size), is not based on
minimizing error, and is inherently divergent instead of convergent, thereby
providing a unique direction of research for unsupervised feature learning. In
this paper the quality of its learned features is demonstrated on the MNIST
dataset, where its performance confirms that indeed DDFA is a viable technique
for learning useful features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1837</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1837</id><created>2014-06-06</created><updated>2015-06-05</updated><authors><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>Daum&#xe9;</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Ross</keyname><forenames>Stephane</forenames></author></authors><title>Efficient programmable learning to search</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve &quot;learning to search&quot; approaches to structured prediction in two
ways. First, we show that the search space can be defined by an arbitrary
imperative program, reducing the number of lines of code required to develop
new structured prediction tasks by orders of magnitude. Second, we make
structured prediction orders of magnitude faster through various algorithmic
improvements. We demonstrate the feasibility of our approach on three
structured prediction tasks: two variants of sequence labeling and
entity-relation resolution. In all cases we obtain accuracies at least as high
as alternative approaches, at drastically reduced execution and programming
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1844</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1844</id><created>2014-06-06</created><authors><author><keyname>Chetty</keyname><forenames>Vasu</forenames></author><author><keyname>Warnick</keyname><forenames>Sean</forenames></author></authors><title>Meanings and Applications of Structure in Networks of Dynamic Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter reviews four notions of system structure, three of which are
contextual and classic (i.e. the complete computational structure linked to a
state space model, the sparsity pattern of a transfer function, and the
interconnection of subsystems) and one which is relatively new (i.e. the signal
structure of a system's dynamical structure function). Although each of these
structural concepts apply to the nonlinear and stochastic setting, this work
will focus on linear time invariant systems to distill the key concepts and
make their relationships clear. We then discusses three applications of the
newest structural form (the signal structure of a system's dynamical structure
function): network reconstruction, vulnerability analysis, and a recent result
in distributed control that guarantees the synthesis of a stabilizing
controller with a specified structure or proves that no such controller exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1848</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1848</id><created>2014-06-06</created><authors><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Dinh</keyname><forenames>Hai Q.</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>Repeated-root constacyclic codes of length $2\ell^mp^n$</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><msc-class>11T71, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any different odd primes $\ell$ and $p$, structure of constacyclic codes
of length $2\ell^mp^n$ over a finite field $\mathbb F_q$ of characteritic $p$
and their duals is established in term of their generator polynomials. Among
other results, all linear complimentary dual and self-dual constacyclic codes
of length $2\ell^mp^n$ over $\mathbb F_q$ are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1853</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1853</id><created>2014-06-06</created><updated>2014-10-31</updated><authors><author><keyname>Osband</keyname><forenames>Ian</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Model-based Reinforcement Learning and the Eluder Dimension</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning to optimize an unknown Markov decision
process (MDP). We show that, if the MDP can be parameterized within some known
function class, we can obtain regret bounds that scale with the dimensionality,
rather than cardinality, of the system. We characterize this dependence
explicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is
the Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. These
represent the first unified regret bounds for model-based reinforcement
learning and provide state of the art guarantees in several important settings.
Moreover, we present a simple and computationally efficient algorithm
\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies
these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1855</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1855</id><created>2014-06-06</created><authors><author><keyname>Ramya</keyname><forenames>P.</forenames></author><author><keyname>Sasirekha</keyname><forenames>S.</forenames></author></authors><title>Text Mining System for Non-Expert Miners</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service oriented architecture integrated with text mining allows services to
extract information in a well defined manner. In this paper, it is proposed to
design a knowledge extracting system for the Ocean Information Data System.
Deployed ARGO floating sensors of INCOIS (Indian National Council for Ocean
Information Systems) organization reflects the characteristics of ocean. This
is forwarded to the OIDS (Ocean Information Data System). For the data received
from OIDS, pre-processing techniques are applied. Pre-processing involves the
header retrieval and data separation. Header information is used to identify
the region of sensor, whereas data is used in the analysis process of Ocean
Information System. Analyzed data is segmented based on the region, by the
header value. Mining technique and composition principle is applied on the
segments for further analysis. Index Terms-- Service oriented architecture;
Text Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1856</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1856</id><created>2014-06-06</created><updated>2014-10-30</updated><authors><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>A Drifting-Games Analysis for Online Learning and Applications to
  Boosting</title><categories>cs.LG</categories><comments>In NIPS2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general mechanism to design online learning algorithms based on
a minimax analysis within a drifting-games framework. Different online learning
settings (Hedge, multi-armed bandit problems and online convex optimization)
are studied by converting into various kinds of drifting games. The original
minimax analysis for drifting games is then used and generalized by applying a
series of relaxations, starting from choosing a convex surrogate of the 0-1
loss function. With different choices of surrogates, we not only recover
existing algorithms, but also propose new algorithms that are totally
parameter-free and enjoy other useful properties. Moreover, our drifting-games
framework naturally allows us to study high probability bounds without
resorting to any concentration results, and also a generalized notion of regret
that measures how good the algorithm is compared to all but the top small
fraction of candidates. Finally, we translate our new Hedge algorithm into a
new adaptive boosting algorithm that is computationally faster as shown in
experiments, since it ignores a large number of examples on each round.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1867</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1867</id><created>2014-06-07</created><authors><author><keyname>Nie</keyname><forenames>Weili</forenames></author><author><keyname>Zheng</keyname><forenames>Fuchun</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoming</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Energy Efficiency of Cross-Tier Base Station Cooperation in
  Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures, Submitted to the IEEE Transactions on Wireless
  Communications (TWC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous cellular networks (HetNets) are to be deployed for future
wireless communication to meet the ever-increasing mobile traffic demand.
However, the dense and random deployment of small cells and their uncoordinated
operation raise important concerns about energy efficiency. Base station (BS)
cooperation is set to play a key role in managing interference in the HetNets.
In this paper, we consider BS cooperation in the downlink HetNets where BSs
from different tiers within the respective cooperative clusters jointly
transmit the same data to a typical user, and further optimize the energy
efficiency performance. First, based on the proposed clustering model, we
derive the spatial average rate using tools from stochastic geometry.
Furthermore, we formulate a power minimization problem with a minimum spatial
average rate constraint and derive an approximate result of the optimal
received signal strength (RSS) thresholds. Building upon these results, we
effectively address the problem of how to design appropriate RSS thresholds,
taking into account the trade-off between spatial average rate and energy
efficiency. Simulations show that our proposed clustering model is more
energy-saving than the geometric clustering model, and under our proposed
clustering model, deploying a two-tier HetNet is significantly more
energy-saving compared to a macro-only network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1870</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1870</id><created>2014-06-07</created><authors><author><keyname>Keet</keyname><forenames>C. Maria</forenames></author><author><keyname>Khumalo</keyname><forenames>Langa</forenames></author></authors><title>Toward verbalizing ontologies in isiZulu</title><categories>cs.CL</categories><comments>12 pages, 1 figure; CNL 2014</comments><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IsiZulu is one of the eleven official languages of South Africa and roughly
half the population can speak it. It is the first (home) language for over 10
million people in South Africa. Only a few computational resources exist for
isiZulu and its related Nguni languages, yet the imperative for tool
development exists. We focus on natural language generation, and the grammar
options and preferences in particular, which will inform verbalization of
knowledge representation languages and could contribute to machine translation.
The verbalization pattern specification shows that the grammar rules are
elaborate and there are several options of which one may have preference. We
devised verbalization patterns for subsumption, basic disjointness, existential
and universal quantification, and conjunction. This was evaluated in a survey
among linguists and non-linguists. Some differences between linguists and
non-linguists can be observed, with the former much more in agreement, and
preferences depend on the overall structure of the sentence, such as singular
for subsumption and plural in other cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1875</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1875</id><created>2014-06-07</created><authors><author><keyname>Zheng</keyname><forenames>Xi</forenames></author><author><keyname>Bansal</keyname><forenames>Akanksha</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author></authors><title>Bullseye: Structured Passage Retrieval and Document Highlighting for
  Scholarly Search</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Bullseye system for scholarly search. Given a collection of
research papers, Bullseye: 1) identifies relevant passages using any
on-the-shelf algorithm; 2) automatically detects document structure and
restricts retrieved passages to user-specifed sections; and 3) highlights those
passages for each PDF document retrieved. We evaluate Bullseye with regard to
three aspects: system effectiveness, user effectiveness, and user effort. In a
system-blind evaluation, users were asked to compare passage retrieval using
Bullseye vs. a baseline which ignores document structure, in regard to four
types of graded assessments. Results show modest improvement in system
effectiveness while both user effectiveness and user effort show substantial
improvement. Users also report very strong demand for passage highlighting in
scholarly search across both systems considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1876</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1876</id><created>2014-06-07</created><updated>2014-06-12</updated><authors><author><keyname>Turek</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Abelian properties of Parry words</title><categories>math.CO cs.FL</categories><comments>19 pages</comments><msc-class>68R15, 11B85</msc-class><journal-ref>Theor. Comput. Sci. 566 (2015), 26-38</journal-ref><doi>10.1016/j.tcs.2014.11.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abelian complexity of a word $\mathbf{u}$ is a function that counts the
number of pairwise non-abelian-equivalent factors of $\mathbf{u}$ of length
$n$. We prove that for any $c$-balanced Parry word $\mathbf{u}$, the values of
the abelian complexity function can be computed by a finite-state automaton.
The proof is based on the notion of relative Parikh vectors. The approach works
for any function $F(n)$ that can be expressed in terms of the set of relative
Parikh vectors corresponding to the length $n$. For example, we show that the
balance function of a $c$-balanced Parry word is computable by a finite-state
automaton as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1880</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1880</id><created>2014-06-07</created><updated>2014-09-08</updated><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Spectral Clustering of Graphs with the Bethe Hessian</title><categories>cond-mat.dis-nn cs.SI physics.soc-ph stat.ML</categories><comments>8 pages, 2 figures</comments><journal-ref>Advances in Neural Information Processing Systems 27 (NIPS 2014)
  pp 406-414</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is a standard approach to label nodes on a graph by
studying the (largest or lowest) eigenvalues of a symmetric real matrix such as
e.g. the adjacency or the Laplacian. Recently, it has been argued that using
instead a more complicated, non-symmetric and higher dimensional operator,
related to the non-backtracking walk on the graph, leads to improved
performance in detecting clusters, and even to optimal performance for the
stochastic block model. Here, we propose to use instead a simpler object, a
symmetric real matrix known as the Bethe Hessian operator, or deformed
Laplacian. We show that this approach combines the performances of the
non-backtracking operator, thus detecting clusters all the way down to the
theoretical limit in the stochastic block model, with the computational,
theoretical and memory advantages of real symmetric matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1881</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1881</id><created>2014-06-07</created><updated>2014-07-28</updated><authors><author><keyname>Pishchulin</keyname><forenames>Leonid</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Fine-grained Activity Recognition with Holistic and Pose based Features</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holistic methods based on dense trajectories are currently the de facto
standard for recognition of human activities in video. Whether holistic
representations will sustain or will be superseded by higher level video
encoding in terms of body pose and motion is the subject of an ongoing debate.
In this paper we aim to clarify the underlying factors responsible for good
performance of holistic and pose-based representations. To that end we build on
our recent dataset leveraging the existing taxonomy of human activities. This
dataset includes 24,920 video snippets covering 410 human activities in total.
Our analysis reveals that holistic and pose-based methods are highly
complementary, and their performance varies significantly depending on the
activity. We find that holistic methods are mostly affected by the number and
speed of trajectories, whereas pose-based methods are mostly influenced by
viewpoint of the person. We observe striking performance differences across
activities: for certain activities results with pose-based features are more
than twice as accurate compared to holistic features, and vice versa. The best
performing approach in our comparison is based on the combination of holistic
and pose-based approaches, which again underlines their complementarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1886</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1886</id><created>2014-06-07</created><authors><author><keyname>Rojas</keyname><forenames>Raul</forenames></author></authors><title>The Z1: Architecture and Algorithms of Konrad Zuse's First Computer</title><categories>cs.AR</categories><comments>24 pages, 20 figures</comments><report-no>DCIS-14-1</report-no><msc-class>68Mxx</msc-class><acm-class>K.2; B.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides the first comprehensive description of the Z1, the
mechanical computer built by the German inventor Konrad Zuse in Berlin from
1936 to 1938. The paper describes the main structural elements of the machine,
the high-level architecture, and the dataflow between components. The computer
could perform the four basic arithmetic operations using floating-point
numbers. Instructions were read from punched tape. A program consisted of a
sequence of arithmetical operations, intermixed with memory store and load
instructions, interrupted possibly by input and output operations. Numbers were
stored in a mechanical memory. The machine did not include conditional
branching in the instruction set. While the architecture of the Z1 is similar
to the relay computer Zuse finished in 1941 (the Z3) there are some significant
differences. The Z1 implements operations as sequences of microinstructions, as
in the Z3, but does not use rotary switches as micro-steppers. The Z1 uses a
digital incrementer and a set of conditions which are translated into
microinstructions for the exponent and mantissa units, as well as for the
memory blocks. Microinstructions select one out of 12 layers in a machine with
a 3D mechanical structure of binary mechanical elements. The exception circuits
for mantissa zero, necessary for normalized floating-point, were lacking; they
were first implemented in the Z3. The information for this article was
extracted from careful study of the blueprints drawn by Zuse for the
reconstruction of the Z1 for the German Technology Museum in Berlin, from some
letters, and from sketches in notebooks. Although the machine has been in
exhibition since 1989 (non-operational), no detailed high-level description of
the machine's architecture had been available. This paper fills that gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1901</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1901</id><created>2014-06-07</created><authors><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Michel</keyname><forenames>Bertrand</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Subsampling Methods for Persistent Homology</title><categories>math.AT cs.CG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is a multiscale method for analyzing the shape of sets
and functions from point cloud data arising from an unknown distribution
supported on those sets. When the size of the sample is large, direct
computation of the persistent homology is prohibitive due to the combinatorial
nature of the existing algorithms. We propose to compute the persistent
homology of several subsamples of the data and then combine the resulting
estimates. We study the risk of two estimators and we prove that the
subsampling approach carries stable topological information while achieving a
great reduction in computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1906</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1906</id><created>2014-06-07</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>Refinement-Cut: User-Guided Segmentation Algorithm for Translational
  Science</title><categories>cs.CV</categories><comments>6 figures, 50 references</comments><journal-ref>Sci. Rep. 4, 5164, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, a semi-automatic segmentation algorithm for (medical)
image analysis is presented. More precise, the approach belongs to the category
of interactive contouring algorithms, which provide real-time feedback of the
segmentation result. However, even with interactive real-time contouring
approaches there are always cases where the user cannot find a satisfying
segmentation, e.g. due to homogeneous appearances between the object and the
background, or noise inside the object. For these difficult cases the algorithm
still needs additional user support. However, this additional user support
should be intuitive and rapid integrated into the segmentation process, without
breaking the interactive real-time segmentation feedback. I propose a solution
where the user can support the algorithm by an easy and fast placement of one
or more seed points to guide the algorithm to a satisfying segmentation result
also in difficult cases. These additional seed(s) restrict(s) the calculation
of the segmentation for the algorithm, but at the same time, still enable to
continue with the interactive real-time feedback segmentation. For a practical
and genuine application in translational science, the approach has been tested
on medical data from the clinical routine in 2D and 3D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1907</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1907</id><created>2014-06-07</created><authors><author><keyname>Preece</keyname><forenames>Alun</forenames></author><author><keyname>Gwilliams</keyname><forenames>Chris</forenames></author><author><keyname>Parizas</keyname><forenames>Christos</forenames></author><author><keyname>Pizzocaro</keyname><forenames>Diego</forenames></author><author><keyname>Bakdash</keyname><forenames>Jonathan Z.</forenames></author><author><keyname>Braines</keyname><forenames>Dave</forenames></author></authors><title>Conversational Sensing</title><categories>cs.HC</categories><acm-class>H.5.2</acm-class><doi>10.1117/12.2053283</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Recent developments in sensing technologies, mobile devices and context-aware
user interfaces have made it possible to represent information fusion and
situational awareness as a conversational process among actors - human and
machine agents - at or near the tactical edges of a network. Motivated by use
cases in the domain of security, policing and emergency response, this paper
presents an approach to information collection, fusion and sense-making based
on the use of natural language (NL) and controlled natural language (CNL) to
support richer forms of human-machine interaction. The approach uses a
conversational protocol to facilitate a flow of collaborative messages from NL
to CNL and back again in support of interactions such as: turning eyewitness
reports from human observers into actionable information (from both trained and
untrained sources); fusing information from humans and physical sensors (with
associated quality metadata); and assisting human analysts to make the best use
of available sensing assets in an area of interest (governed by management and
security policies). CNL is used as a common formal knowledge representation for
both machine and human agents to support reasoning, semantic information fusion
and generation of rationale for inferences, in ways that remain transparent to
human users. Examples are provided of various alternative styles for user
feedback, including NL, CNL and graphical feedback. A pilot experiment with
human subjects shows that a prototype conversational agent is able to gather
usable CNL information from untrained human subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1910</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1910</id><created>2014-06-07</created><updated>2015-10-18</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Context-Aware Resource Allocation in Cellular Networks</title><categories>cs.NI</categories><comments>(c) 2015 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define and propose a resource allocation architecture for cellular
networks. The architecture combines content-aware, time-aware and
location-aware resource allocation for next generation broadband wireless
systems. The architecture ensures content-aware resource allocation by
prioritizing real-time applications users over delay-tolerant applications
users when allocating resources. It enables time-aware resource allocation via
traffic-dependent pricing that varies during different hours of day (e.g. peak
and off-peak traffic hours). Additionally, location-aware resource allocation
is integrable in this architecture by including carrier aggregation of various
frequency bands. The context-aware resource allocation is an optimal and
flexible architecture that can be easily implemented in practical cellular
networks. We highlight the advantages of the proposed network architecture with
a discussion on the future research directions for context-aware resource
allocation architecture. We also provide experimental results to illustrate a
general proof of concept for this new architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1915</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1915</id><created>2014-06-07</created><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Gerstlauer</keyname><forenames>Andreas</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Real-Time Rate-Distortion Optimized Streaming of Wireless Video</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile cyberphysical systems have received considerable attention over the
last decade, as communication, computing and control come together on a common
platform. Understanding the complex interactions that govern the behavior of
large complex cyberphysical systems is not an easy task. The goal of this paper
is to address this challenge in the particular context of multimedia delivery
over an autonomous aerial vehicle (AAV) network. Bandwidth requirements and
stringent delay constraints of real-time video streaming, paired with
limitations on computational complexity and power consumptions imposed by the
underlying implementation platform, make cross-layer and cross-domain co-design
approaches a necessity. In this paper, we propose a novel, low-complexity
rate-distortion optimized (RDO) algorithms specifically targeted at video
streaming over mobile embedded networks. We test the performance of our RDO
algorithms using a network of AAVs both in simulation and implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1920</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1920</id><created>2014-06-07</created><updated>2015-05-11</updated><authors><author><keyname>Shmarov</keyname><forenames>Fedor</forenames></author><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author></authors><title>Probabilistic bounded reachability for hybrid systems with continuous
  nondeterministic and probabilistic parameters</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an algorithm for computing bounded reachability probability for
hybrid systems, i.e., the probability that the system reaches an unsafe region
within a finite number of discrete transitions. In particular, we focus on
hybrid systems with continuous dynamics given by solutions of nonlinear
ordinary differential equations (with possibly nondeterministic initial
conditions and parameters), and probabilistic behaviour given by initial
parameters distributed as continuous (with possibly infinite support) and
discrete random variables. Our approach is to define an appropriate relaxation
of the (undecidable) reachability problem, so that it can be solved by
$\delta$-complete decision procedures. In particular, for systems with
continuous random parameters only, we develop a validated integration procedure
which computes an arbitrarily small interval that is guaranteed to contain the
reachability probability. In the more general case of systems with both
nondeterministic and probabilistic parameters, our procedure computes a
guaranteed enclosure for the range of reachability probabilities. We have
applied our approach to a number of nonlinear hybrid models and validated the
results by comparison with Monte Carlo simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1923</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1923</id><created>2014-06-07</created><authors><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Paquette</keyname><forenames>Michel</forenames></author></authors><title>Broadcasting in Networks of Unknown Topology in the Presence of Swamping</title><categories>cs.DC cs.CC cs.NI</categories><comments>A preliminary version of this paper appeared in Proc. 12th
  International Symposium on Stabilization, Safety, and Security of Distributed
  Systems (SSS 2010), New York City, USA, September 20-22, Vol 6366, pp.
  267-281, LNCS Springer, 2010. Full proofs for this paper published 2010:
  https://scs.carleton.ca/content/communication-networks-spatially-correlated-faults</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of broadcasting in a wireless network
under a novel communication model: the {\em swamping} communication model. In
this model, nodes communicate only with those nodes at geometric distance
greater than $s$ and at most $r$ from them. Communication between nearby nodes
under this model can be very time consuming, as the length of the path between
two nodes within distance $s$ is only bounded above by the diameter $D$, in
many cases. For the $n$-node lattice networks, we present algorithms of optimal
time complexity, respectively $O(n/r + r/(r-s))$ for the lattice line and
$O(\sqrt{n}/r + r/(r-s))$ for the two-dimensional lattice. We also consider
networks of unknown topology of diameter $D$ and of a parameter $g$ ({\em
granularity}). More specifically, we consider networks with $\gamma$ the
minimum distance between any two nodes and $g = 1/\gamma$. We present broadcast
algorithms for networks of nodes placed on the line and on the plane with
respective time complexities $O(D/l + g^2)$ and $O(Dg/l + g^4)$, where $l \in
\Theta(\max\{(1-s),\gamma\})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1925</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1925</id><created>2014-06-07</created><authors><author><keyname>Boscaini</keyname><forenames>Davide</forenames></author><author><keyname>Eynard</keyname><forenames>Davide</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author></authors><title>Shape-from-intrinsic operator</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape-from-X is an important class of problems in the fields of geometry
processing, computer graphics, and vision, attempting to recover the structure
of a shape from some observations. In this paper, we formulate the problem of
shape-from-operator (SfO), recovering an embedding of a mesh from intrinsic
differential operators defined on the mesh. Particularly interesting instances
of our SfO problem include synthesis of shape analogies, shape-from-Laplacian
reconstruction, and shape exaggeration. Numerically, we approach the SfO
problem by splitting it into two optimization sub-problems that are applied in
an alternating scheme: metric-from-operator (reconstruction of the discrete
metric from the intrinsic operator) and embedding-from-metric (finding a shape
embedding that would realize a given metric, a setting of the multidimensional
scaling problem).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1928</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1928</id><created>2014-06-07</created><authors><author><keyname>Buer</keyname><forenames>Tobias</forenames></author></authors><title>An exact and two heuristic strategies for truthful bidding in
  combinatorial transport auctions</title><categories>cs.GT cs.AI</categories><msc-class>91B26 (Primary) 90B06, 90B40 (Secondary)</msc-class><acm-class>I.2.8; I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support a freight carrier in a combinatorial transport auction, we
proposes an exact and two heuristic strategies for bidding on subsets of
requests. The exact bidding strategy is based on the concept of elementary
request combinations. We show that it is sufficient and necessary for a carrier
to bid on each elementary request combination in order to guarantee the same
result as bidding on each element of the powerset of the set of tendered
requests. Both heuristic bidding strategies identify promising request
combinations. For this, pairwise synergies based on saving values as well as
the capacitated p-median problem are used. The bidding strategies are evaluated
by a computational study that simulates an auction. It is based on 174
benchmark instances and therefore easily extendable by other researchers. On
average, the two heuristic strategies achieve 91 percent and 81 percent of the
available sales potential while generating 36 and only 4 percent of the bundle
bids of the exact strategy. Therefore, the proposed bidding strategies help a
carrier to increase her chance to win and at the same time reduce the
computational burden to participate in a combinatorial transport auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1933</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1933</id><created>2014-06-07</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author><author><keyname>Ostermann</keyname><forenames>Alexander</forenames></author></authors><title>On the error propagation of semi-Lagrange and Fourier methods for
  advection problems</title><categories>math.NA cs.NA</categories><comments>submitted to Computers &amp; Mathematics with Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the error propagation of numerical schemes for the
advection equation in the case where high precision is desired. The numerical
methods considered are based on the fast Fourier transform, polynomial
interpolation (semi-Lagrangian methods using a Lagrange or spline
interpolation), and a discontinuous Galerkin semi-Lagrangian approach (which is
conservative and has to store more than a single value per cell).
  We demonstrate, by carrying out numerical experiments, that the worst case
error estimates given in the literature provide a good explanation for the
error propagation of the interpolation-based semi-Lagrangian methods. For the
discontinuous Galerkin semi-Lagrangian method, however, we find that the
characteristic property of semi-Lagrangian error estimates (namely the fact
that the error increases proportionally to the number of time steps) is not
observed. We provide an explanation for this behavior and conduct numerical
simulations that corroborate the different qualitative features of the error in
the two respective types of semi-Lagrangian methods.
  The method based on the fast Fourier transform is exact but, due to round-off
errors, susceptible to a linear increase of the error in the number of time
steps. We show how to modify the Cooley--Tukey algorithm in order to obtain an
error growth that is proportional to the square root of the number of time
steps.
  Finally, we show, for a simple model, that our conclusions hold true if the
advection solver is used as part of a splitting scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1938</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1938</id><created>2014-06-07</created><updated>2014-09-17</updated><authors><author><keyname>Lipowski</keyname><forenames>Adam</forenames></author><author><keyname>Lipowska</keyname><forenames>Dorota</forenames></author><author><keyname>Ferreira</keyname><forenames>Antonio Luis</forenames></author></authors><title>Emergence of Social Structures via Preferential Selection</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>4 pages,Phys. Rev. E (accepted)</comments><journal-ref>Phys. Rev. E 90, 032817 (2014)</journal-ref><doi>10.1103/PhysRevE.90.032817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a weighted-network multi-agent model with preferential selection
such that agents choose partners with the probability $p(w)$, where $w$ is the
number of their past selections. When $p(w)$ increases sublinearly with the
number of past selections ($p(w)\sim w^{\alpha}, \ \alpha&lt;1$), agents develop a
uniform preference for all other agents. At $\alpha=1$, this state looses
stability and more complex structures form. For a superlinear increase
($\alpha&gt;1$), strong heterogeneities emerge and agents make selections mainly
within small and sometimes asymmetric clusters. Even in a few-agent case,
formation of such clusters resembles phase transitions with spontaneous
symmetry breaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1943</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1943</id><created>2014-06-07</created><authors><author><keyname>Suo</keyname><forenames>Yuanming</forenames></author><author><keyname>Dao</keyname><forenames>Minh</forenames></author><author><keyname>Srinivas</keyname><forenames>Umamahesh</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Structured Dictionary Learning for Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparsity driven signal processing has gained tremendous popularity in the
last decade. At its core, the assumption is that the signal of interest is
sparse with respect to either a fixed transformation or a signal dependent
dictionary. To better capture the data characteristics, various dictionary
learning methods have been proposed for both reconstruction and classification
tasks. For classification particularly, most approaches proposed so far have
focused on designing explicit constraints on the sparse code to improve
classification accuracy while simply adopting $l_0$-norm or $l_1$-norm for
sparsity regularization. Motivated by the success of structured sparsity in the
area of Compressed Sensing, we propose a structured dictionary learning
framework (StructDL) that incorporates the structure information on both group
and task levels in the learning process. Its benefits are two-fold: (i) the
label consistency between dictionary atoms and training data are implicitly
enforced; and (ii) the classification performance is more robust in the cases
of a small dictionary size or limited training data than other techniques.
Using the subspace model, we derive the conditions for StructDL to guarantee
the performance and show theoretically that StructDL is superior to $l_0$-norm
or $l_1$-norm regularized dictionary learning for classification. Extensive
experiments have been performed on both synthetic simulations and real world
applications, such as face recognition and object classification, to
demonstrate the validity of the proposed DL framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1949</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1949</id><created>2014-06-08</created><updated>2015-05-19</updated><authors><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author></authors><title>Distinct Distances: Open Problems and Current Bounds</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the variants of Erd\H{o}s' distinct distances problem and the
current best bounds for each of those.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1953</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1953</id><created>2014-06-08</created><updated>2014-08-22</updated><authors><author><keyname>Liu</keyname><forenames>Peilei</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>Automatic Extraction of Protein Interaction in Literature</title><categories>cs.CL cs.CE</categories><comments>This paper has been withdrawn by the author due to its lack of
  academic value</comments><acm-class>H.2.8; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein-protein interaction extraction is the key precondition of the
construction of protein knowledge network, and it is very important for the
research in the biomedicine. This paper extracted directional protein-protein
interaction from the biological text, using the SVM-based method. Experiments
were evaluated on the LLL05 corpus with good results. The results show that
dependency features are import for the protein-protein interaction extraction
and features related to the interaction word are effective for the interaction
direction judgment. At last, we analyzed the effects of different features and
planed for the next step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1957</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1957</id><created>2014-06-08</created><updated>2014-08-03</updated><authors><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>SINR in wireless networks and the two-parameter Poisson-Dirichlet
  process</title><categories>math.PR cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic geometry models of wireless networks based on Poisson point
processes are increasingly being developed with a focus on studying various
signal-to-interference-plus-noise ratio (SINR) values. We show that the SINR
values experienced by a typical user with respect to different base stations of
a Poissonian cellular network are related to a specific instance of the
so-called two-parameter Poisson-Dirichlet process. This process has many
interesting properties as well as applications in various fields. We give
examples of several results proved for this process that are of immediate or
potential interest in the development of analytic tools for cellular networks.
Some of them simplify or are akin to certain results that are recently being
developed in wireless networks literature. By doing this we hope to motivate
further research and use of Poisson-Dirichlet processes in this new setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1965</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1965</id><created>2014-06-08</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>An Algebraic Characterisation of Concurrent Composition</title><categories>cs.LO</categories><comments>This is an old technical report from 1981. I submitted it to a
  special issue of HOSC in honour of Peter Landin, as explained in the Prelude,
  added in 2008. However, at an advanced stage, the handling editor became
  unresponsive, and the paper was never published. I am making it available via
  the arXiv for the same reasons given in the Prelude</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algebraic characterization of a form of synchronized parallel
composition allowing for true concurrency, using ideas based on Peter Landin's
&quot;Program-Machine Symmetric Automata Theory&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1967</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1967</id><created>2014-06-08</created><updated>2015-01-01</updated><authors><author><keyname>Harase</keyname><forenames>Shin</forenames></author></authors><title>Quasi-Monte Carlo point sets with small $t$-values and WAFOM</title><categories>math.NA cs.NA</categories><comments>17 pages</comments><msc-class>65C05, 65D30</msc-class><doi>10.1016/j.amc.2014.12.144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $t$-value of a $(t, m, s)$-net is an important criterion of point sets
for quasi-Monte Carlo integration, and many point sets are constructed in terms
of the $t$-values, as this leads to small integration error bounds. Recently,
Matsumoto, Saito, and Matoba proposed the Walsh figure of merit (WAFOM) as a
quickly computable criterion of point sets that ensures higher order
convergence for function classes of very high smoothness. In this paper, we
consider a search algorithm for point sets whose $t$-value and WAFOM are both
small, so as to be effective for a wider range of function classes. For this,
we fix digital $(t, m, s)$-nets with small $t$-values (e.g., Sobol' or
Niederreiter--Xing nets) in advance, apply random linear scrambling, and select
scrambled digital $(t, m, s)$-nets in terms of WAFOM. Experiments show that the
resulting point sets improve the rates of convergence for smooth functions and
are robust for non-smooth functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1969</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1969</id><created>2014-06-08</created><authors><author><keyname>Akanbi</keyname><forenames>Adeyinka K.</forenames></author><author><keyname>Agunbiade</keyname><forenames>Olusanya Y.</forenames></author><author><keyname>Kuti</keyname><forenames>Sadiq</forenames></author><author><keyname>Dehinbo</keyname><forenames>Olumuyiwa J.</forenames></author></authors><title>A Semantic Enhanced Model for effective Spatial Information Retrieval</title><categories>cs.IR cs.AI cs.IT math.IT</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of information on the web is geographically referenced. Discovering and
retrieving this geographic information to satisfy various users needs across
both open and distributed Spatial Data Infrastructures (SDI) poses eminent
research challenges. However, this is mostly caused by semantic heterogeneity
in users query and lack of semantic referencing of the Geographic Information
(GI) metadata. To addressing these challenges, this paper discusses ontology
based semantic enhanced model, which explicitly represents GI metadata, and
provides linked RDF instances of each entity. The system focuses on semantic
search, ontology, and efficient spatial information retrieval. In particular,
an integrated model that uses specific domain information extraction to improve
the searching and retrieval of ranked spatial search results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1974</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1974</id><created>2014-06-08</created><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Turkiyyah</keyname><forenames>George</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Communication Complexity of the Fast Multipole Method and its Algebraic
  Variants</title><categories>cs.DC cs.NA</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combination of hierarchical tree-like data structures and data access
patterns from fast multipole methods and hierarchical low-rank approximation of
linear operators from H-matrix methods appears to form an algorithmic path
forward for efficient implementation of many linear algebraic operations of
scientific computing at the exascale. The combination provides asymptotically
optimal computational and communication complexity and applicability to large
classes of operators that commonly arise in scientific computing applications.
A convergence of the mathematical theories of the fast multipole and H-matrix
methods has been underway for over a decade. We recap this mathematical
unification and describe implementation aspects of a hybrid of these two
compelling hierarchical algorithms on hierarchical distributed-shared memory
architectures, which are likely to be the first to reach the exascale. We
present a new communication complexity estimate for fast multipole methods on
such architectures. We also show how the data structures and access patterns of
H-matrices for low-rank operators map onto those of fast multipole, leading to
an algebraically generalized form of fast multipole that compromises none of
its architecturally ideal properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.1998</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.1998</id><created>2014-06-08</created><authors><author><keyname>Missier</keyname><forenames>Paolo</forenames></author><author><keyname>Bryans</keyname><forenames>Jeremy</forenames></author><author><keyname>Gamble</keyname><forenames>Carl</forenames></author><author><keyname>Curcin</keyname><forenames>Vasa</forenames></author><author><keyname>Danger</keyname><forenames>Roxana</forenames></author></authors><title>ProvAbs: model, policy, and tooling for abstracting PROV graphs</title><categories>cs.DB</categories><comments>In Procs. IPAW 2014 (Provenance and Annotations). Koln, Germany:
  Springer, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance metadata can be valuable in data sharing settings, where it can be
used to help data consumers form judgements regarding the reliability of the
data produced by third parties. However, some parts of provenance may be
sensitive, requiring access control, or they may need to be simplified for the
intended audience. Both these issues can be addressed by a single mechanism for
creating abstractions over provenance, coupled with a policy model to drive the
abstraction. Such mechanism, which we refer to as abstraction by grouping,
simultaneously achieves partial disclosure of provenance, and facilitates its
consumption. In this paper we introduce a formal foundation for this type of
abstraction, grounded in the W3C PROV model; describe the associated policy
model; and briefly present its implementation, the Provabs tool for interactive
experimentation with policies and abstractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2000</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2000</id><created>2014-06-08</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Introduction to Neutrosophic Statistics</title><categories>cs.AI</categories><comments>122 pages, many geometrical figures, many tables</comments><msc-class>28E10</msc-class><journal-ref>Published as a book by Sitech in 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neutrosophic Statistics means statistical analysis of population or sample
that has indeterminate (imprecise, ambiguous, vague, incomplete, unknown) data.
For example, the population or sample size might not be exactly determinate
because of some individuals that partially belong to the population or sample,
and partially they do not belong, or individuals whose appurtenance is
completely unknown. Also, there are population or sample individuals whose data
could be indeterminate. In this book, we develop the 1995 notion of
neutrosophic statistics. We present various practical examples. It is possible
to define the neutrosophic statistics in many ways, because there are various
types of indeterminacies, depending on the problem to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2008</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2008</id><created>2014-06-08</created><authors><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames></author><author><keyname>Klasing</keyname><forenames>Ralf</forenames></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames></author><author><keyname>Kuszner</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Rendezvous of Heterogeneous Mobile Agents in Edge-weighted Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a variant of the deterministic rendezvous problem for a pair of
heterogeneous agents operating in an undirected graph, which differ in the time
they require to traverse particular edges of the graph. Each agent knows the
complete topology of the graph and the initial positions of both agents. The
agent also knows its own traversal times for all of the edges of the graph, but
is unaware of the corresponding traversal times for the other agent. The goal
of the agents is to meet on an edge or a node of the graph. In this scenario,
we study the time required by the agents to meet, compared to the meeting time
$T_{OPT}$ in the offline scenario in which the agents have complete knowledge
about each others speed characteristics. When no additional assumptions are
made, we show that rendezvous in our model can be achieved after time $O(n
T_{OPT})$ in a $n$-node graph, and that such time is essentially in some cases
the best possible. However, we prove that the rendezvous time can be reduced to
$\Theta (T_{OPT})$ when the agents are allowed to exchange $\Theta(n)$ bits of
information at the start of the rendezvous process. We then show that under
some natural assumption about the traversal times of edges, the hardness of the
heterogeneous rendezvous problem can be substantially decreased, both in terms
of time required for rendezvous without communication, and the communication
complexity of achieving rendezvous in time $\Theta (T_{OPT})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2010</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2010</id><created>2014-06-08</created><updated>2014-06-11</updated><authors><author><keyname>Stich</keyname><forenames>Sebastian U.</forenames></author></authors><title>On low complexity Acceleration Techniques for Randomized Optimization:
  Supplementary Online Material</title><categories>math.OC cs.NA</categories><comments>15 pages, 9 figures; the main part without the appendix is a preprint
  of a conference publication that will appear at PPSN XIII (2014); the
  appendix reports the complete numerical data that could not fit into the main
  part</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it was shown by Nesterov (2011) that techniques form convex
optimization can be used to successfully accelerate simple derivative-free
randomized optimization methods. The appeal of those schemes lies in their low
complexity, which is only $\Theta(n)$ per iteration---compared to $\Theta(n^2)$
for algorithms storing second-order information or covariance matrices. From a
high-level point of view, those accelerated schemes employ correlations between
successive iterates---a concept looking similar to the evolution path used in
Covariance Matrix Adaptation Evolution Strategies (CMA-ES).
  In this contribution, we (i) implement and empirically test a simple
accelerated random search scheme (SARP). Our study is the first to provide
numerical evidence that SARP can effectively be implemented with adaptive step
size control and does not require access to gradient or advanced line search
oracles. We (ii) try to empirically verify the supposed analogy between the
evolution path and SARP. We propose an algorithm CMA-EP that uses only the
evolution path to bias the search. This algorithm can be generalized to a
family of low memory schemes, with complexity $\Theta(mn)$ per iteration,
following a recent approach by Loshchilov (2014). The study shows that the
performance of CMA-EP heavily depends on the spectra of the objective function
and thus it cannot accelerate as consistently as SARP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2015</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2015</id><created>2014-06-08</created><authors><author><keyname>Veeramachaneni</keyname><forenames>Kalyan</forenames></author><author><keyname>Halawa</keyname><forenames>Sherif</forenames></author><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author><author><keyname>O'Reilly</keyname><forenames>Una-May</forenames></author><author><keyname>Taylor</keyname><forenames>Colin</forenames></author><author><keyname>Do</keyname><forenames>Chuong</forenames></author></authors><title>MOOCdb: Developing Standards and Systems to Support MOOC Data Science</title><categories>cs.IR cs.CY cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a shared data model for enabling data science in Massive Open
Online Courses (MOOCs). The model captures students interactions with the
online platform. The data model is platform agnostic and is based on some basic
core actions that students take on an online learning platform. Students
usually interact with the platform in four different modes: Observing,
Submitting, Collaborating and giving feedback. In observing mode students are
simply browsing the online platform, watching videos, reading material, reading
book or watching forums. In submitting mode, students submit information to the
platform. This includes submissions towards quizzes, homeworks, or any
assessment modules. In collaborating mode students interact with other students
or instructors on forums, collaboratively editing wiki or chatting on google
hangout or other hangout venues. With this basic definitions of activities, and
a data model to store events pertaining to these activities, we then create a
common terminology to map Coursera and edX data into this shared data model.
This shared data model called MOOCdb becomes the foundation for a number of
collaborative frameworks that enable progress in data science without the need
to share the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2017</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2017</id><created>2014-06-08</created><authors><author><keyname>Higham</keyname><forenames>Desmond J.</forenames></author><author><keyname>Grindrod</keyname><forenames>Peter</forenames></author><author><keyname>Mantzaris</keyname><forenames>Alexander V.</forenames></author><author><keyname>Otley</keyname><forenames>Amanda</forenames></author><author><keyname>Laflin</keyname><forenames>Peter</forenames></author></authors><title>Anticipating Activity in Social Media Spikes</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel mathematical model for the activity of microbloggers
during an external, event-driven spike. The model leads to a testable
prediction of who would become most active if a spike were to take place. This
type of information is of great interest to commercial organisations,
governments and charities, as it identifies key players who can be targeted
with information in real time when the network is most receptive. The model
takes account of the fact that dynamic interactions evolve over an underlying,
static network that records who listens to whom. The model is based on the
assumption that, in the case where the entire community has become aware of an
external news event, a key driver of activity is the motivation to participate
by responding to incoming messages. We test the model on a large scale Twitter
conversation concerning the appointment of a UK Premier League football club
manager. We also present further results for a Bundesliga football match, a
marketing event and a television programme. In each case we find that
exploiting the underlying connectivity structure improves the prediction of who
will be active during a spike. We also show how the half-life of a spike in
activity can be quantified in terms of the network size and the typical
response rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2018</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2018</id><created>2014-06-08</created><authors><author><keyname>Ou</keyname><forenames>Yen-Fu</forenames></author><author><keyname>Lin</keyname><forenames>Wenzhi</forenames></author><author><keyname>Zeng</keyname><forenames>Huiqi</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Perceptual Quality of Video with Periodic Frame Rate and Quantization
  Variation-Subjective Studies and Analytical Modeling</title><categories>cs.MM</categories><comments>Keywords: perceptual video quality, frame rate, QS, temporal
  variation, quality metrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In networked video applications, the frame rate (FR) and quantization
stepsize (QS) of a compressed video are often adapted in response to the
changes of the available bandwidth. It is important to understand how do the
variation of FR and QS and their variation pattern affect the video quality. In
this paper, we investigate the impact of temporal variation of FR and QS on the
perceptual video quality. Among all possible variation patterns, we focus on
videos in which two FR's (or QS's) alternate over a fixed interval. We explore
the human responses to such variation by conducting subjective evaluation of
test videos with different variation magnitudes and frequencies. We further
analyze statistical significance of the impact of variation magnitude,
variation frequency, video content, and their interactions. By analyzing the
subjective ratings, we propose two models for predicting the quality of video
with alternating FR and QS, respectively, The proposed models have simple
mathematical forms with a few content-dependent parameters. The models fit the
measured data very well using parameters determined by least square fitting
with the measured data. We further propose some guidelines for adaptation of FR
and QS based on trends observed from subjective test results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2021</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2021</id><created>2014-06-08</created><authors><author><keyname>Whiting</keyname><forenames>James G. H.</forenames></author><author><keyname>Costello</keyname><forenames>Ben P. J. de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Slime Mould Logic Gates Based on Frequency Changes of Electrical
  Potential Oscillation</title><categories>cs.ET</categories><comments>10 Pages, 3 Figures, 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physarum polycephalum is a large single amoeba cell, which in its plasmodial
phase,forages and connects nearby food sources with protoplasmic tubes. The
organism forages for food by growing these tubes towards detected food stuffs,
this foraging behaviour is governed by simple rules of photoavoidance and
chemotaxis. The electrical activity of the tubes oscillates, creating a
peristaltic like action within the tubes, forcing cytoplasm along the lumen;
the frequency of this oscillation controls the speed and direction of growth.
External stimuli such as light and food cause changes in the oscillation
frequency. We demonstrate that using these stimuli as logical inputs we can
approximate logic gates using these tubes and derive combinational logic
circuits by cascading the gates, with software analysis providing the output of
each gate and determining the input of the following gate. Basic gates OR, AND
and NOT were correct 90%, 77.8% and 91.7% of the time respectively. Derived
logic circuits XOR, Half Adder and Full Adder were 70.8%, 65% and 58.8%
accurate respectively. Accuracy of the combinational logic decreases as the
number of gates is increased, however they are at least as accurate as previous
logic approximations using spatial growth of Physarum polycephalum and up to 30
times as fast at computing the logical output. The results shown here
demonstrate a significant advancement in organism-based computing, providing a
solid basis for hybrid computers of the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2022</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2022</id><created>2014-06-08</created><authors><author><keyname>Tejwani</keyname><forenames>Rahul</forenames><affiliation>University at Buffalo</affiliation></author></authors><title>Two-dimensional Sentiment Analysis of text</title><categories>cs.IR cs.CL</categories><comments>sentiment analysis, two-dimensional</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment Analysis aims to get the underlying viewpoint of the text, which
could be anything that holds a subjective opinion, such as an online review,
Movie rating, Comments on Blog posts etc. This paper presents a novel approach
that classify text in two-dimensional Emotional space, based on the sentiments
of the author. The approach uses existing lexical resources to extract feature
set, which is trained using Supervised Learning techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2023</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2023</id><created>2014-06-08</created><authors><author><keyname>Giordano</keyname><forenames>Laura</forenames></author><author><keyname>Gliozzi</keyname><forenames>Valentina</forenames></author><author><keyname>Olivetti</keyname><forenames>Nicola</forenames></author><author><keyname>Pozzato</keyname><forenames>Gian Luca</forenames></author></authors><title>Rational Closure in SHIQ</title><categories>cs.AI</categories><comments>30 pages, extended version of paper accepted to DL2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a notion of rational closure for the logic SHIQ, which does not
enjoys the finite model property, building on the notion of rational closure
introduced by Lehmann and Magidor in [23]. We provide a semantic
characterization of rational closure in SHIQ in terms of a preferential
semantics, based on a finite rank characterization of minimal models. We show
that the rational closure of a TBox can be computed in EXPTIME using entailment
in SHIQ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2031</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2031</id><created>2014-06-08</created><authors><author><keyname>Chen</keyname><forenames>Xianjie</forenames></author><author><keyname>Mottaghi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Liu</keyname><forenames>Xiaobai</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Detect What You Can: Detecting and Representing Objects using Holistic
  Models and Body Parts</title><categories>cs.CV</categories><comments>CBMM memo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting objects becomes difficult when we need to deal with large shape
deformation, occlusion and low resolution. We propose a novel approach to i)
handle large deformations and partial occlusions in animals (as examples of
highly deformable objects), ii) describe them in terms of body parts, and iii)
detect them when their body parts are hard to detect (e.g., animals depicted at
low resolution). We represent the holistic object and body parts separately and
use a fully connected model to arrange templates for the holistic object and
body parts. Our model automatically decouples the holistic object or body parts
from the model when they are hard to detect. This enables us to represent a
large number of holistic object and body part combinations to better deal with
different &quot;detectability&quot; patterns caused by deformations, occlusion and/or low
resolution.
  We apply our method to the six animal categories in the PASCAL VOC dataset
and show that our method significantly improves state-of-the-art (by 4.1% AP)
and provides a richer representation for objects. During training we use
annotations for body parts (e.g., head, torso, etc), making use of a new
dataset of fully annotated object parts for PASCAL VOC 2010, which provides a
mask for each part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2035</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2035</id><created>2014-06-08</created><updated>2014-11-06</updated><authors><author><keyname>Yogatama</keyname><forenames>Dani</forenames></author><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Learning Word Representations with Hierarchical Sparse Coding</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for learning word representations using hierarchical
regularization in sparse coding inspired by the linguistic study of word
meanings. We show an efficient learning algorithm based on stochastic proximal
methods that is significantly faster than previous approaches, making it
possible to perform hierarchical sparse coding on a corpus of billions of word
tokens. Experiments on various benchmark tasks---word similarity ranking,
analogies, sentence completion, and sentiment analysis---demonstrate that the
method outperforms or is competitive with state-of-the-art methods. Our word
representations are available at
\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2040</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2040</id><created>2014-06-08</created><updated>2014-06-18</updated><authors><author><keyname>Wiebe</keyname><forenames>Nathan</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>Quantum arithmetic and numerical analysis using Repeat-Until-Success
  circuits</title><categories>quant-ph cs.NA</categories><journal-ref>QIC 16, pp. 134-178 (2016)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method for approximate synthesis of single--qubit rotations of
the form $e^{-i f(\phi_1,\ldots,\phi_k)X}$ that is based on the
Repeat-Until-Success (RUS) framework for quantum circuit synthesis. We
demonstrate how smooth computable functions $f$ can be synthesized from two
basic primitives. This synthesis approach constitutes a manifestly quantum form
of arithmetic that differs greatly from the approaches commonly used in quantum
algorithms. The key advantage of our approach is that it requires far fewer
qubits than existing approaches: as a case in point, we show that using as few
as $3$ ancilla qubits, one can obtain RUS circuits for approximate
multiplication and reciprocals. We also analyze the costs of performing
multiplication and inversion on a quantum computer using conventional
approaches and find that they can require too many qubits to execute on a small
quantum computer, unlike our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2041</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2041</id><created>2014-06-08</created><authors><author><keyname>Kuester</keyname><forenames>Jan-Christoph</forenames></author><author><keyname>Bauer</keyname><forenames>Andreas</forenames></author></authors><title>Platform-Centric Android Monitoring---Modular and Efficient</title><categories>cs.SE cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an add-on for the Android platform, capable of intercepting nearly
all interactions between apps or apps with the platform, including arguments of
method invocations in a human-readable format. A preliminary performance
evaluation shows that the performance penalty of our solution is roughly
comparable with similar tools in that area. The advantage of our solution,
however, is that it is truly modular in the sense that we do not actually
modify the Android platform itself, and can include it even with an already
running system. Possible uses of such an add-on are manifold; we discuss one
from the area of runtime verification that aims at improving system security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2049</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2049</id><created>2014-06-08</created><authors><author><keyname>Li</keyname><forenames>Xue</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-Jin</forenames></author><author><keyname>Shen</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Bao-Di</forenames></author></authors><title>Image Tag Completion by Low-rank Factorization with Dual Reconstruction
  Structure Preserved</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel tag completion algorithm is proposed in this paper, which is designed
with the following features: 1) Low-rank and error s-parsity: the incomplete
initial tagging matrix D is decomposed into the complete tagging matrix A and a
sparse error matrix E. However, instead of minimizing its nuclear norm, A is
further factor-ized into a basis matrix U and a sparse coefficient matrix V,
i.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our
algorithm to recover latent structures from noisy initial data and avoid
performing too much denoising; 2) Local reconstruction structure consistency:
to steer the completion of D, the local linear reconstruction structures in
feature space and tag space are obtained and preserved by U and V respectively.
Such a scheme could alleviate the negative effect of distances measured by
low-level features and incomplete tags. Thus, we can seek a balance between
exploiting as much information and not being mislead to suboptimal performance.
Experiments conducted on Corel5k dataset and the newly issued Flickr30Concepts
dataset demonstrate the effectiveness and efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2058</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2058</id><created>2014-06-08</created><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames><affiliation>Queen Mary University of London</affiliation></author></authors><title>Monad Transformers for Backtracking Search</title><categories>cs.PL</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 153, 2014, pp. 31-50</journal-ref><doi>10.4204/EPTCS.153.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends Escardo and Oliva's selection monad to the selection monad
transformer, a general monadic framework for expressing backtracking search
algorithms in Haskell. The use of the closely related continuation monad
transformer for similar purposes is also discussed, including an implementation
of a DPLL-like SAT solver with no explicit recursion. Continuing a line of work
exploring connections between selection functions and game theory, we use the
selection monad transformer with the nondeterminism monad to obtain an
intuitive notion of backward induction for a certain class of nondeterministic
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2059</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2059</id><created>2014-06-08</created><authors><author><keyname>Abel</keyname><forenames>Andreas</forenames><affiliation>Gothenburg University</affiliation></author><author><keyname>Chapman</keyname><forenames>James</forenames><affiliation>Institute of Cybernetics, Tallinn</affiliation></author></authors><title>Normalization by Evaluation in the Delay Monad: A Case Study for
  Coinduction via Copatterns and Sized Types</title><categories>cs.LO cs.PL</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><acm-class>D.3.3; F.3.2; F.3.3; F.4.1</acm-class><journal-ref>EPTCS 153, 2014, pp. 51-67</journal-ref><doi>10.4204/EPTCS.153.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an Agda formalization of a normalizer for
simply-typed lambda terms. The normalizer consists of two coinductively defined
functions in the delay monad: One is a standard evaluator of lambda terms to
closures, the other a type-directed reifier from values to eta-long beta-normal
forms. Their composition, normalization-by-evaluation, is shown to be a total
function a posteriori, using a standard logical-relations argument.
  The successful formalization serves as a proof-of-concept for coinductive
programming and reasoning using sized types and copatterns, a new and presently
experimental feature of Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2060</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2060</id><created>2014-06-08</created><authors><author><keyname>Hicks</keyname><forenames>Michael</forenames><affiliation>University of Maryland, College Park</affiliation></author><author><keyname>Bierman</keyname><forenames>Gavin</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Guts</keyname><forenames>Nataliya</forenames><affiliation>University of Maryland, College Park</affiliation></author><author><keyname>Leijen</keyname><forenames>Daan</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Swamy</keyname><forenames>Nikhil</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>Polymonadic Programming</title><categories>cs.PL</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><acm-class>D.3.1</acm-class><journal-ref>EPTCS 153, 2014, pp. 79-99</journal-ref><doi>10.4204/EPTCS.153.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monads are a popular tool for the working functional programmer to structure
effectful computations. This paper presents polymonads, a generalization of
monads. Polymonads give the familiar monadic bind the more general type forall
a,b. L a -&gt; (a -&gt; M b) -&gt; N b, to compose computations with three different
kinds of effects, rather than just one. Polymonads subsume monads and
parameterized monads, and can express other constructions, including precise
type-and-effect systems and information flow tracking; more generally,
polymonads correspond to Tate's productoid semantic model. We show how to equip
a core language (called lambda-PM) with syntactic support for programming with
polymonads. Type inference and elaboration in lambda-PM allows programmers to
write polymonadic code directly in an ML-like syntax--our algorithms compute
principal types and produce elaborated programs wherein the binds appear
explicitly. Furthermore, we prove that the elaboration is coherent: no matter
which (type-correct) binds are chosen, the elaborated program's semantics will
be the same. Pleasingly, the inferred types are easy to read: the polymonad
laws justify (sometimes dramatic) simplifications, but with no effect on a
type's generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2061</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2061</id><created>2014-06-08</created><authors><author><keyname>Leijen</keyname><forenames>Daan</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>Koka: Programming with Row Polymorphic Effect Types</title><categories>cs.PL</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 153, 2014, pp. 100-126</journal-ref><doi>10.4204/EPTCS.153.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a programming model where effects are treated in a disciplined
way, and where the potential side-effects of a function are apparent in its
type signature. The type and effect of expressions can also be inferred
automatically, and we describe a polymorphic type inference system based on
Hindley-Milner style inference. A novel feature is that we support polymorphic
effects through row-polymorphism using duplicate labels. Moreover, we show that
our effects are not just syntactic labels but have a deep semantic connection
to the program. For example, if an expression can be typed without an exn
effect, then it will never throw an unhandled exception. Similar to Haskell's
`runST` we show how we can safely encapsulate stateful operations. Through the
state effect, we can also safely combine state with let-polymorphism without
needing either imperative type variables or a syntactic value restriction.
Finally, our system is implemented fully in a new language called Koka and has
been used successfully on various small to medium-sized sample programs ranging
from a Markdown processor to a tier-splitted chat application. You can try out
Koka live at www.rise4fun.com/koka/tutorial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2062</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2062</id><created>2014-06-08</created><authors><author><keyname>Jeltsch</keyname><forenames>Wolfgang</forenames><affiliation>TT&#xdc; K&#xfc;berneetika Instituut</affiliation></author></authors><title>Categorical Semantics for Functional Reactive Programming with Temporal
  Recursion and Corecursion</title><categories>cs.PL cs.LO</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><acm-class>F.3.2; D.1.1; F.4.1</acm-class><journal-ref>EPTCS 153, 2014, pp. 127-142</journal-ref><doi>10.4204/EPTCS.153.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional reactive programming (FRP) makes it possible to express temporal
aspects of computations in a declarative way. Recently we developed two kinds
of categorical models of FRP: abstract process categories (APCs) and concrete
process categories (CPCs). Furthermore we showed that APCs generalize CPCs. In
this paper, we extend APCs with additional structure. This structure models
recursion and corecursion operators that are related to time. We show that the
resulting categorical models generalize those CPCs that impose an additional
constraint on time scales. This constraint boils down to ruling out
$\omega$-supertasks, which are closely related to Zeno's paradox of Achilles
and the tortoise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2063</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2063</id><created>2014-06-08</created><authors><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames><affiliation>Ilmenau University of Technology</affiliation></author><author><keyname>Lepper</keyname><forenames>Markus</forenames><affiliation>&lt;semantics/&gt; GmbH</affiliation></author></authors><title>Foundations of Total Functional Data-Flow Programming</title><categories>cs.PL cs.DM</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><acm-class>D.1.1; D.1.7; D.3.2; D.3.3; F.1.2; F.3.2</acm-class><journal-ref>EPTCS 153, 2014, pp. 143-167</journal-ref><doi>10.4204/EPTCS.153.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of declarative stream programming (discrete time, clocked
synchronous, modular, data-centric) is divided between the data-flow graph
paradigm favored by domain experts, and the functional reactive paradigm
favored by academics. In this paper, we describe the foundations of a framework
for unifying functional and data-flow styles that differs from FRP proper in
significant ways: It is based on set theory to match the expectations of domain
experts, and the two paradigms are reduced symmetrically to a low-level middle
ground, with strongly compositional semantics. The design of the framework is
derived from mathematical first principles, in particular coalgebraic
coinduction and a standard relational model of stateful computation. The
abstract syntax and semantics introduced here constitute the full core of a
novel stream programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2064</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2064</id><created>2014-06-08</created><authors><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames></author></authors><title>Coherence for Skew-Monoidal Categories</title><categories>cs.LO cs.PL math.CT</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 153, 2014, pp. 68-77</journal-ref><doi>10.4204/EPTCS.153.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I motivate a variation (due to K. Szlach\'{a}nyi) of monoidal categories
called skew-monoidal categories where the unital and associativity laws are not
required to be isomorphisms, only natural transformations. Coherence has to be
formulated differently than in the well-known monoidal case. In my (to my
knowledge new) version, it becomes a statement of uniqueness of normalizing
rewrites. I present a proof of this coherence theorem and also formalize it
fully in the dependently typed programming language Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2065</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2065</id><created>2014-06-08</created><authors><author><keyname>Latella</keyname><forenames>Diego</forenames><affiliation>ISTI - CNR</affiliation></author><author><keyname>Loreti</keyname><forenames>Michele</forenames><affiliation>Universit&#xe0; di Firenze</affiliation></author><author><keyname>Massink</keyname><forenames>Mieke</forenames><affiliation>ISTI - CNR</affiliation></author><author><keyname>Senni</keyname><forenames>Valerio</forenames><affiliation>IMT Lucca</affiliation></author></authors><title>Stochastically timed predicate-based communication primitives for
  autonomic computing</title><categories>cs.PL cs.LO cs.SE</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 1-16</journal-ref><doi>10.4204/EPTCS.154.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicate-based communication allows components of a system to send messages
and requests to ensembles of components that are determined at execution time
through the evaluation of a predicate, in a multicast fashion. Predicate-based
communication can greatly simplify the programming of autonomous and adaptive
systems. We present a stochastically timed extension of the Software Component
Ensemble Language (SCEL) that was introduced in previous work. Such an
extension raises a number of non-trivial design and formal semantics issues
with different options as possible solutions at different levels of
abstraction. We discuss four of these options, of which two in more detail. We
provide a formal semantics definition and an illustration of the use of the
language modeling a bike sharing system, together with some preliminary
analysis of the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2066</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2066</id><created>2014-06-08</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames><affiliation>DiMI, University of Udine</affiliation></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames><affiliation>DiMI, University of Udine</affiliation></author></authors><title>GSOS for non-deterministic processes with quantitative aspects</title><categories>cs.LO cs.PL</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 154, 2014, pp. 17-33</journal-ref><doi>10.4204/EPTCS.154.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, some general frameworks have been proposed as unifying theories for
processes combining non-determinism with quantitative aspects (such as
probabilistic or stochastically timed executions), aiming to provide general
results and tools. This paper provides two contributions in this respect.
First, we present a general GSOS specification format (and a corresponding
notion of bisimulation) for non-deterministic processes with quantitative
aspects. These specifications define labelled transition systems according to
the ULTraS model, an extension of the usual LTSs where the transition relation
associates any source state and transition label with state reachability weight
functions (like, e.g., probability distributions). This format, hence called
Weight Function SOS (WFSOS), covers many known systems and their bisimulations
(e.g. PEPA, TIPP, PCSP) and GSOS formats (e.g. GSOS, Weighted GSOS,
Segala-GSOS, among others).
  The second contribution is a characterization of these systems as coalgebras
of a class of functors, parametric on the weight structure. This result allows
us to prove soundness of the WFSOS specification format, and that
bisimilarities induced by these specifications are always congruences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2067</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2067</id><created>2014-06-08</created><authors><author><keyname>Tschaikowski</keyname><forenames>Max</forenames><affiliation>University of Southampton</affiliation></author><author><keyname>Tribastone</keyname><forenames>Mirco</forenames><affiliation>University of Southampton</affiliation></author></authors><title>Extended Differential Aggregations in Process Algebra for Performance
  and Biology</title><categories>cs.PF cs.CE cs.DC</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 34-47</journal-ref><doi>10.4204/EPTCS.154.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study aggregations for ordinary differential equations induced by fluid
semantics for Markovian process algebra which can capture the dynamics of
performance models and chemical reaction networks. Whilst previous work has
required perfect symmetry for exact aggregation, we present approximate fluid
lumpability, which makes nearby processes perfectly symmetric after a
perturbation of their parameters. We prove that small perturbations yield
nearby differential trajectories. Numerically, we show that many heterogeneous
processes can be aggregated with negligible errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2068</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2068</id><created>2014-06-08</created><authors><author><keyname>Braitling</keyname><forenames>Bettina</forenames></author><author><keyname>Fioriti</keyname><forenames>Luis Mar&#xed;a Ferrer</forenames></author><author><keyname>Hatefi</keyname><forenames>Hassan</forenames></author><author><keyname>Wimmer</keyname><forenames>Ralf</forenames></author><author><keyname>Becker</keyname><forenames>Bernd</forenames></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author></authors><title>MeGARA: Menu-based Game Abstraction and Abstraction Refinement of Markov
  Automata</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 48-63</journal-ref><doi>10.4204/EPTCS.154.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov automata combine continuous time, probabilistic transitions, and
nondeterminism in a single model. They represent an important and powerful way
to model a wide range of complex real-life systems. However, such models tend
to be large and difficult to handle, making abstraction and abstraction
refinement necessary. In this paper we present an abstraction and abstraction
refinement technique for Markov automata, based on the game-based and
menu-based abstraction of probabilistic automata. First experiments show that a
significant reduction in size is possible using abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2069</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2069</id><created>2014-06-08</created><authors><author><keyname>Feng</keyname><forenames>Cheng</forenames><affiliation>Laboratory for Foundations of Computer Science, University of Edinburg, Scotland, UK</affiliation></author></authors><title>Patch-based Hybrid Modelling of Spatially Distributed Systems by Using
  Stochastic HYPE - ZebraNet as an Example</title><categories>cs.PF</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 64-77</journal-ref><doi>10.4204/EPTCS.154.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individual-based hybrid modelling of spatially distributed systems is usually
expensive. Here, we consider a hybrid system in which mobile agents spread over
the space and interact with each other when in close proximity. An
individual-based model for this system needs to capture the spatial attributes
of every agent and monitor the interaction between each pair of them. As a
result, the cost of simulating this model grows exponentially as the number of
agents increases. For this reason, a patch-based model with more abstraction
but better scalability is advantageous. In a patch-based model, instead of
representing each agent separately, we model the agents in a patch as an
aggregation. This property significantly enhances the scalability of the model.
In this paper, we convert an individual-based model for a spatially distributed
network system for wild-life monitoring, ZebraNet, to a patch-based stochastic
HYPE model with accurate performance evaluation. We show the ease and
expressiveness of stochastic HYPE for patch-based modelling of hybrid systems.
Moreover, a mean-field analytical model is proposed as the fluid flow
approximation of the stochastic HYPE model, which can be used to investigate
the average behaviour of the modelled system over an infinite number of
simulation runs of the stochastic HYPE model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2071</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2071</id><created>2014-06-08</created><authors><author><keyname>Kempf</keyname><forenames>Jean-Francois</forenames><affiliation>VERIMAG</affiliation></author><author><keyname>Lebeltel</keyname><forenames>Olivier</forenames><affiliation>VERIMAG</affiliation></author><author><keyname>Maler</keyname><forenames>Oded</forenames><affiliation>VERIMAG</affiliation></author></authors><title>Formal and Informal Methods for Multi-Core Design Space Exploration</title><categories>cs.SE cs.PF</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 78-92</journal-ref><doi>10.4204/EPTCS.154.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a tool-supported methodology for design-space exploration for
embedded systems. It provides means to define high-level models of applications
and multi-processor architectures and evaluate the performance of different
deployment (mapping, scheduling) strategies while taking uncertainty into
account. We argue that this extension of the scope of formal verification is
important for the viability of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2075</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2075</id><created>2014-06-09</created><updated>2015-02-15</updated><authors><author><keyname>Nedic</keyname><forenames>Angelia</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Stochastic Gradient-Push for Strongly Convex Functions on Time-Varying
  Directed Graphs</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the convergence rate of the recently proposed subgradient-push
method for distributed optimization over time-varying directed graphs. The
subgradient-push method can be implemented in a distributed way without
requiring knowledge of either the number of agents or the graph sequence; each
node is only required to know its out-degree at each time. Our main result is a
convergence rate of $O \left((\ln t)/t \right)$ for strongly convex functions
with Lipschitz gradients even if only stochastic gradient samples are
available; this is asymptotically faster than the $O \left((\ln t)/\sqrt{t}
\right)$ rate previously known for (general) convex functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2076</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2076</id><created>2014-06-09</created><updated>2014-10-10</updated><authors><author><keyname>Nguyen</keyname><forenames>Hieu D.</forenames></author><author><keyname>Coxson</keyname><forenames>Greg E.</forenames></author></authors><title>Doppler Tolerance, Complementary Code Sets and the Generalized
  Thue-Morse Sequence</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the construction of Doppler-tolerant Golay complementary
waveforms by Pezeshki-Calderbank-Moran-Howard to complementary code sets having
more than two codes. This is accomplished by exploiting number-theoretic
results involving the sum-of-digits function, equal sums of like powers, and a
generalization to more than two symbols of the classical two-symbol
Prouhet-Thue-Morse sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2079</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2079</id><created>2014-06-09</created><updated>2014-11-20</updated><authors><author><keyname>Pantelis</keyname><forenames>Garry</forenames></author></authors><title>Program Verification of Numerical Computation - Part 2</title><categories>cs.LO cs.MS</categories><comments>arXiv admin note: text overlap with arXiv:1401.1290</comments><msc-class>03Fxx</msc-class><acm-class>F.4.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes present some extensions of a formal method introduced in an
earlier paper. The formal method is designed as a tool for program verification
of numerical computation and forms the basis of the software package VPC.
Included in the extensions that are presented here are disjunctions and methods
for detecting non-computable programs. A more comprehensive list of the
construction rules as higher order constructs is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2080</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2080</id><created>2014-06-09</created><updated>2015-04-10</updated><authors><author><keyname>Sukhbaatar</keyname><forenames>Sainbayar</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Training Convolutional Networks with Noisy Labels</title><categories>cs.CV cs.LG cs.NE</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of large labeled datasets has allowed Convolutional Network
models to achieve impressive recognition results. However, in many settings
manual annotation of the data is impractical; instead our data has noisy
labels, i.e. there is some freely available label for each image which may or
may not be accurate. In this paper, we explore the performance of
discriminatively-trained Convnets when trained on such noisy data. We introduce
an extra noise layer into the network which adapts the network outputs to match
the noisy label distribution. The parameters of this noise layer can be
estimated as part of the training process and involve simple modifications to
current training infrastructures for deep networks. We demonstrate the
approaches on several datasets, including large scale experiments on the
ImageNet classification benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2082</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2082</id><created>2014-06-09</created><updated>2015-08-28</updated><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Tibshirani</keyname><forenames>Ryan J.</forenames></author></authors><title>Fast and Flexible ADMM Algorithms for Trend Filtering</title><categories>stat.ML cs.LG cs.NA math.OC stat.AP</categories><comments>22 pages, 10 figures; published in Journal of Computational and
  Graphical Statistics, 2015</comments><doi>10.1080/10618600.2015.1054033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fast and robust algorithm for trend filtering, a
recently developed nonparametric regression tool. It has been shown that, for
estimating functions whose derivatives are of bounded variation, trend
filtering achieves the minimax optimal error rate, while other popular methods
like smoothing splines and kernels do not. Standing in the way of a more
widespread practical adoption, however, is a lack of scalable and numerically
stable algorithms for fitting trend filtering estimates. This paper presents a
highly efficient, specialized ADMM routine for trend filtering. Our algorithm
is competitive with the specialized interior point methods that are currently
in use, and yet is far more numerically robust. Furthermore, the proposed ADMM
implementation is very simple, and importantly, it is flexible enough to extend
to many interesting related problems, such as sparse trend filtering and
isotonic trend filtering. Software for our method is freely available, in both
the C and R languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2083</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2083</id><created>2014-06-09</created><updated>2014-11-23</updated><authors><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>P&#xf3;czos</keyname><forenames>Barnab&#xe1;s</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>On the Decreasing Power of Kernel and Distance based Nonparametric
  Hypothesis Tests in High Dimensions</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH</categories><comments>19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference
  on Artificial Intelligence (with author order reversed from ArXiv)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about two related decision theoretic problems, nonparametric
two-sample testing and independence testing. There is a belief that two
recently proposed solutions, based on kernels and distances between pairs of
points, behave well in high-dimensional settings. We identify different sources
of misconception that give rise to the above belief. Specifically, we
differentiate the hardness of estimation of test statistics from the hardness
of testing whether these statistics are zero or not, and explicitly discuss a
notion of &quot;fair&quot; alternative hypotheses for these problems as dimension
increases. We then demonstrate that the power of these tests actually drops
polynomially with increasing dimension against fair alternatives. We end with
some theoretical insights and shed light on the \textit{median heuristic} for
kernel bandwidth selection. Our work advances the current understanding of the
power of modern nonparametric hypothesis tests in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2092</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2092</id><created>2014-06-09</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Division by zero in non-involutive meadows</title><categories>math.RA cs.LO</categories><comments>14 pages</comments><msc-class>12E12, 12L12, 68Q65</msc-class><journal-ref>Journal of Applied Logic, 13(1):1--12, 2015</journal-ref><doi>10.1016/j.jal.2014.10.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meadows have been proposed as alternatives for fields with a purely
equational axiomatization. At the basis of meadows lies the decision to make
the multiplicative inverse operation total by imposing that the multiplicative
inverse of zero is zero. Thus, the multiplicative inverse operation of a meadow
is an involution. In this paper, we study `non-involutive meadows', i.e.\
variants of meadows in which the multiplicative inverse of zero is not zero,
and pay special attention to non-involutive meadows in which the multiplicative
inverse of zero is one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2096</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2096</id><created>2014-06-09</created><authors><author><keyname>Njonko</keyname><forenames>Paul Brillant Feuto</forenames></author><author><keyname>Cardey</keyname><forenames>Sylviane</forenames></author><author><keyname>Greenfield</keyname><forenames>Peter</forenames></author><author><keyname>Abed</keyname><forenames>Walid El</forenames></author></authors><title>RuleCNL: A Controlled Natural Language for Business Rule Specifications</title><categories>cs.SE cs.CL</categories><comments>12 pages, 7 figures, Fourth Workshop on Controlled Natural Language
  (CNL 2014) Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business rules represent the primary means by which companies define their
business, perform their actions in order to reach their objectives. Thus, they
need to be expressed unambiguously to avoid inconsistencies between business
stakeholders and formally in order to be machine-processed. A promising
solution is the use of a controlled natural language (CNL) which is a good
mediator between natural and formal languages. This paper presents RuleCNL,
which is a CNL for defining business rules. Its core feature is the alignment
of the business rule definition with the business vocabulary which ensures
traceability and consistency with the business domain. The RuleCNL tool
provides editors that assist end-users in the writing process and automatic
mappings into the Semantics of Business Vocabulary and Business Rules (SBVR)
standard. SBVR is grounded in first order logic and includes constructs called
semantic formulations that structure the meaning of rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2099</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2099</id><created>2014-06-09</created><authors><author><keyname>Muhammad</keyname><forenames>Tufail</forenames></author><author><keyname>Halim</keyname><forenames>Zahid</forenames></author><author><keyname>Khan</keyname><forenames>Majid Ali</forenames></author></authors><title>ClassSpy: Java Object Pattern Visualization Tool</title><categories>cs.PL cs.SE</categories><comments>ICOMS-2013. International Conference on Modeling and Simulation,
  25-27 November, Islamabad</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern java programs consist of large number of classes as well as vast
amount of objects instantiated during program execution. Software developers
are always keen to know the number of objects created for each class. This
information is helpful for a developer in understanding the packages/classes of
a program and optimizing their code. However, understanding such a vast amount
of information is not a trivial task. Visualization helps to depict this
information on a single screen and to comprehend it efficiently. This paper
presents a visualization approach that depicts information about all the
objects instantiated during the program execution. The proposed technique is
more space efficient and scalable to handle vast datasets, at the same time
helpful to identify the key program components. This easy to use interface
provides user an environment to glimpse the entire objects on a single screen.
The proposed approach allows sorting objects at class, thread and method
levels. Effectiveness and usability of the proposed approach is shown through
case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2103</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2103</id><created>2014-06-09</created><authors><author><keyname>French</keyname><forenames>Tim</forenames></author><author><keyname>Hales</keyname><forenames>James</forenames></author><author><keyname>Tay</keyname><forenames>Edwin</forenames></author></authors><title>A composable language for action models</title><categories>cs.LO</categories><comments>Extended version of a paper to appear in the 10th International
  Conference on Advances in Modal Logic (AiML)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action models are semantic structures similar to Kripke models that represent
a change in knowledge in an epistemic setting. Whereas the language of action
model logic embeds the semantic structure of an action model directly within
the language, this paper introduces a language that represents action models
using syntactic operators inspired by relational actions. This language admits
an intuitive description of the action models it represents, and we show in
several settings that it is sufficient to represent any action model up to a
given modal depth and to represent the results of action model synthesis, and
give a sound and complete axiomatisation in some of these settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2107</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2107</id><created>2014-06-09</created><authors><author><keyname>Ben-Moshe</keyname><forenames>Boaz</forenames></author><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Omri</keyname><forenames>Eran</forenames></author></authors><title>Optimizing Budget Allocation in Graphs</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical facility location problem we consider a graph $G$ with fixed
weights on the edges of $G$. The goal is then to find an optimal positioning
for a set of facilities on the graph with respect to some objective function.
We introduce a new framework for facility location problems, where the weights
on the graph edges are not fixed, but rather should be assigned. The goal is to
find a valid assignment for which the resulting weighted graph optimizes the
facility location objective function. We present algorithms for finding the
optimal {\em budget allocation} for the center point problem and for the median
point problem on trees. Our algorithms run in linear time, both for the case
where a candidate vertex is given as part of the input, and for the case where
finding a vertex that optimizes the solution is part of the problem. We also
present a hardness result for the general graph case of the center point
problem, followed by an $O(\log^2(n))$ approximation algorithm on graphs - with
general metric spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2108</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2108</id><created>2014-06-09</created><updated>2014-06-11</updated><authors><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>Linear time Constructions of some $d$-Restriction Problems</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new linear time globally explicit constructions for perfect hash
families, cover-free families and separating hash functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2110</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2110</id><created>2014-06-09</created><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Bagnol</keyname><forenames>Marc</forenames></author><author><keyname>Pistone</keyname><forenames>Paolo</forenames></author><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Logic Programming and Logarithmic Space</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present an algebraic view on logic programming, related to proof theory
and more specifically linear logic and geometry of interaction. Within this
construction, a characterization of logspace (deterministic and
non-deterministic) computation is given via a synctactic restriction, using an
encoding of words that derives from proof theory.
  We show that the acceptance of a word by an observation (the counterpart of a
program in the encoding) can be decided within logarithmic space, by reducing
this problem to the acyclicity of a graph. We show moreover that observations
are as expressive as two-ways multi-heads finite automata, a kind of pointer
machines that is a standard model of logarithmic space computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2121</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2121</id><created>2014-06-09</created><authors><author><keyname>Lam</keyname><forenames>Edmund S. L.</forenames></author><author><keyname>Cervesato</keyname><forenames>Iliano</forenames></author></authors><title>Constraint Handling Rules with Multiset Comprehension Patterns</title><categories>cs.PL</categories><comments>Part of CHR 2014 proceedings (arXiv:1406.1510)</comments><report-no>CHR/2014/3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CHR is a declarative, concurrent and committed choice rule-based constraint
programming language. We extend CHR with multiset comprehension patterns,
providing the programmer with the ability to write multiset rewriting rules
that can match a variable number of constraints in the store. This enables
writing more readable, concise and declarative code for algorithms that
coordinate large amounts of data or require aggregate operations. We call this
extension $\mathit{CHR}^\mathit{cp}$. We give a high-level abstract semantics
of $\mathit{CHR}^\mathit{cp}$, followed by a lower-level operational semantics.
We then show the soundness of this operational semantics with respect to the
abstract semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2122</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2122</id><created>2014-06-09</created><authors><author><keyname>Gerlich</keyname><forenames>Ralf</forenames></author></authors><title>Automatic Test Data Generation and Model Checking with CHR</title><categories>cs.SE cs.PL</categories><comments>Part of CHR 2014 proceedings (arXiv:1406.1510)</comments><report-no>CHR/2014/1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an example for application of Constraint Handling Rules to
automated test data generation and model checking in verification of mission
critical software for satellite control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2125</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2125</id><created>2014-06-09</created><authors><author><keyname>Nogatz</keyname><forenames>Falco</forenames></author><author><keyname>Fr&#xfc;hwirth</keyname><forenames>Thom</forenames></author></authors><title>From XML Schema to JSON Schema: Translation with CHR</title><categories>cs.DB cs.PL</categories><comments>Part of CHR 2014 proceedings (arXiv:1406.1510)</comments><report-no>CHR/2014/2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its rising popularity as data format especially for web services, the
software ecosystem around the JavaScript Object Notation (JSON) is not as
widely distributed as that of XML. For both data formats there exist schema
languages to specify the structure of instance documents, but there is
currently no opportunity to translate already existing XML Schema documents
into equivalent JSON Schemas.
  In this paper we introduce an implementation of a language translator. It
takes an XML Schema and creates its equivalent JSON Schema document. Our
approach is based on Prolog and CHR. By unfolding the XML Schema document into
CHR constraints, it is possible to specify the concrete translation rules in a
declarative way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2128</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2128</id><created>2014-06-09</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoge</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A bio-inspired algorithm for fuzzy user equilibrium problem by aid of
  Physarum Polycephalum</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The user equilibrium in traffic assignment problem is based on the fact that
travelers choose the minimum-cost path between every origin-destination pair
and on the assumption that such a behavior will lead to an equilibrium of the
traffic network. In this paper, we consider this problem when the traffic
network links are fuzzy cost. Therefore, a Physarum-type algorithm is developed
to unify the Physarum network and the traffic network for taking full of
advantage of Physarum Polycephalum's adaptivity in network design to solve the
user equilibrium problem. Eventually, some experiments are used to test the
performance of this method. The results demonstrate that our approach is
competitive when compared with other existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2132</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2132</id><created>2014-06-09</created><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames></author><author><keyname>Benzi</keyname><forenames>Michele</forenames></author></authors><title>Are Social Networks Really Balanced?</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a long-standing belief that in social networks with simultaneous
friendly/hostile interactions (signed networks) there is a general tendency to
a global balance. Balance represents a state of the network with lack of
contentious situations. Here we introduce a method to quantify the degree of
balance of any signed (social) network. It accounts for the contribution of all
signed cycles in the network and gives, in agreement with empirical evidences,
more weight to the shorter than to the longer cycles. We found that, contrary
to what is believed, many signed social networks -- in particular very large
directed online social networks -- are in general very poorly balanced. We also
show that unbalanced states can be changed by tuning the weights of the social
interactions among the agents in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2134</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2134</id><created>2014-06-09</created><authors><author><keyname>Raj</keyname><forenames>Manish</forenames></author><author><keyname>Chakraborty</keyname><forenames>P.</forenames></author><author><keyname>Nandi</keyname><forenames>G. C.</forenames></author></authors><title>Rescue Robotics in Bore well Environment</title><categories>cs.RO cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique for rescue task in bore well environment has been proposed. India
is facing a distressed cruel situation where in the previous years a number of
child deaths have been reported falling in the bore well. As the diameter of
the bore well is quiet narrow for any adult person and the lights goes dark
inside it, the rescue task in those situations is a challenging task. Here we
are proposing a robotic system which will attach a harness to the child using
pneumatic arms for picking up. A teleconferencing system will also be attached
to the robot for communicating with the child.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2135</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2135</id><created>2014-06-09</created><authors><author><keyname>Granstrom</keyname><forenames>Karl</forenames></author></authors><title>An extended target tracking model with multiple random matrices and
  unified kinematics</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model for tracking of extended targets, where each
target is represented by a given number of elliptic subobjects. A gamma
Gaussian inverse Wishart implementation is derived, and necessary
approximations are suggested to alleviate the data association complexity. A
simulation study shows the merits of the model compared to previous work on the
topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2139</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2139</id><created>2014-06-09</created><updated>2014-07-08</updated><authors><author><keyname>Faraki</keyname><forenames>Masoud</forenames></author><author><keyname>Palhang</keyname><forenames>Maziar</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Log-Euclidean Bag of Words for Human Action Recognition</title><categories>cs.CV</categories><acm-class>I.4.9; I.5.4</acm-class><doi>10.1049/iet-cvi.2014.0018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing videos by densely extracted local space-time features has
recently become a popular approach for analysing actions. In this paper, we
tackle the problem of categorising human actions by devising Bag of Words (BoW)
models based on covariance matrices of spatio-temporal features, with the
features formed from histograms of optical flow. Since covariance matrices form
a special type of Riemannian manifold, the space of Symmetric Positive Definite
(SPD) matrices, non-Euclidean geometry should be taken into account while
discriminating between covariance matrices. To this end, we propose to embed
SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW
approach to its Riemannian version. The proposed BoW approach takes into
account the manifold geometry of SPD matrices during the generation of the
codebook and histograms. Experiments on challenging human action datasets show
that the proposed method obtains notable improvements in discrimination
accuracy, in comparison to several state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2140</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2140</id><created>2014-06-09</created><updated>2014-10-07</updated><authors><author><keyname>Sendra</keyname><forenames>J. Rafael</forenames></author><author><keyname>Sevilla</keyname><forenames>David</forenames></author><author><keyname>Villarino</keyname><forenames>Carlos</forenames></author></authors><title>Covering Rational Ruled Surfaces</title><categories>math.AG cs.SC</categories><comments>19 pages, 2 figures in jpg. v2: minor correction of Example 1. v3:
  updated acknowledgements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that covers any given rational ruled surface with two
rational parametrizations. In addition, we present an algorithm that transforms
any rational surface parametrization into a new rational surface
parametrization without affine base points and such that the degree of the
corresponding maps is preserved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2144</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2144</id><created>2014-06-09</created><updated>2015-09-20</updated><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author><author><keyname>Sombra</keyname><forenames>Martin</forenames></author></authors><title>Polynomial partitioning on varieties of codimension two and
  point-hypersurface incidences in four dimensions</title><categories>math.AG cs.CG math.AC math.CO</categories><comments>23 pages. Some explanations added</comments><msc-class>Primary 52C10, Secondary 13D40, 14P25</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial partitioning theorem for finite sets of points in the
real locus of an irreducible complex algebraic variety of codimension at most
two. This result generalizes the polynomial partitioning theorem on the
Euclidean space of Guth and Katz, and its extension to hypersurfaces by Zahl
and by Kaplan, Matou\v{s}ek, Sharir and Safernov\'a.
  We also present a bound for the number of incidences between points and
hypersurfaces in the four-dimensional Euclidean space. It is an application of
our partitioning theorem together with the refined bounds for the number of
connected components of a semi-algebraic set by Barone and Basu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2146</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2146</id><created>2014-06-09</created><updated>2014-10-13</updated><authors><author><keyname>Sarkar</keyname><forenames>Tanmoy</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Digital Watermarking Techniques in Spatial and Frequency Domain</title><categories>cs.CR</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarking is the act of hiding information in multimedia data, for
the purposes of content protection or authentication. In ordinary digital
watermarking, the secret information is embedded into the multimedia data
(cover data) with minimum distortion of the cover data. Due to these
watermarking techniques the watermark image is almost negligible visible. In
this paper we will discuss about various techniques of Digital Watermarking
techniques in spatial and frequency domains
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2150</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2150</id><created>2014-06-09</created><updated>2014-12-30</updated><authors><author><keyname>Hilman</keyname><forenames>Fathurrahman</forenames></author><author><keyname>Baek</keyname><forenames>Jong-Hyen</forenames></author><author><keyname>Chae</keyname><forenames>Eun-Kyung</forenames></author><author><keyname>KyungchunLee</keyname></author></authors><title>ML Detection for MIMO Systems under Channel Estimation Errors</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the erroneous
  simulation results of Figs.1-7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless communication systems, the use of multiple antennas at both the
transmitter and receiver is a widely known method for improving both
reliability and data rates, as it increases the former through transmit or
receive diversity and the latter by spatial multiplexing. In order to detect
signals, channel state information (CSI) is typically required at the receiver;
however, the estimation of CSI is not perfect in practical systems, which
causes performance degradation. In this paper, we propose a novel maximum
likelihood (ML) scheme that is robust to channel information errors. By
assuming a bound on the total power of channel estimation errors, we apply an
optimization method to estimate the instantaneous covariance of channel
estimation errors in order to minimize the ML cost function. To reduce
computational complexity, we also propose an iterative sphere decoding scheme
based on the proposed ML detection method. Simulation results show that the
proposed algorithm provides a performance gain in terms of error probability
relative to existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2154</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2154</id><created>2014-06-09</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Rusak</keyname><forenames>Damian</forenames></author></authors><title>Euclidean TSP with few inner points in linear space</title><categories>cs.DS</categories><comments>under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of $n$ points in the Euclidean plane, such that just $k$ points
are strictly inside the convex hull of the whole set, we want to find the
shortest tour visiting every point. The fastest known algorithm for the version
when $k$ is significantly smaller than $n$, i.e., when there are just few inner
points, works in $O(k^{11\sqrt{k}} k^{1.5} n^{3})$ time [Knauer and Spillner,
WG 2006], but also requires space of order $k^{c\sqrt{k}}n^{2}$. The best
linear space algorithm takes $O(k! k n)$ time [Deineko, Hoffmann, Okamoto,
Woeginer, Oper. Res. Lett. 34(1), 106-110]. We construct a linear space
$O(nk^2+k^{O(\sqrt{k})})$ time algorithm. The new insight is extending the
known divide-and-conquer method based on planar separators with a
matching-based argument to shrink the instance in every recursive call. This
argument also shows that the problem admits a quadratic bikernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2161</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2161</id><created>2014-06-09</created><authors><author><keyname>de Lima</keyname><forenames>Tiago</forenames></author><author><keyname>Herzig</keyname><forenames>Andreas</forenames></author></authors><title>Tableaux for Dynamic Logic of Propositional Assignments</title><categories>cs.LO cs.AI</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dynamic Logic for Propositional Assignments (DL-PA) has recently been
studied as an alternative to Propositional Dynamic Logic (PDL). In DL-PA, the
abstract atomic programs of PDL are replaced by assignments of propositional
variables to truth values. This makes DL-PA enjoy some interesting meta-logical
properties that PDL does not, such as eliminability of the Kleene star,
compactness and interpolation. We define and analytic tableaux calculus for
DL-PA and show that it matches the known complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2180</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2180</id><created>2014-06-09</created><authors><author><keyname>Rojas</keyname><forenames>Esteban Zapata</forenames></author></authors><title>Selecting interesting zones at Aburr\'a valley and valley of St.
  Nicholas Valley's using the identification method of Density-based Clustering
  and Improved Nearest Neighbor applied on social networks</title><categories>cs.CY</categories><comments>9 pages that explain an interesting method to Clustering Social
  Network's data, in Spanish</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  More than ever, social networks have become an important place in the
interaction and behaviour of humans in the last decade. This valuable position,
makes it imperative analyze different aspects of everyday life and science in
general. In this article illustrates how was the process of capturing and
storing information, the application of Density-based Cluster and improved
nearest neighbor and a review of the results, showing the elements used in the
identification of areas of interest through clusters, circumferences and cirle
radios of coverage obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2199</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2199</id><created>2014-06-09</created><updated>2014-11-12</updated><authors><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Two-Stream Convolutional Networks for Action Recognition in Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate architectures of discriminatively trained deep Convolutional
Networks (ConvNets) for action recognition in video. The challenge is to
capture the complementary information on appearance from still frames and
motion between frames. We also aim to generalise the best performing
hand-crafted features within a data-driven learning framework.
  Our contribution is three-fold. First, we propose a two-stream ConvNet
architecture which incorporates spatial and temporal networks. Second, we
demonstrate that a ConvNet trained on multi-frame dense optical flow is able to
achieve very good performance in spite of limited training data. Finally, we
show that multi-task learning, applied to two different action classification
datasets, can be used to increase the amount of training data and improve the
performance on both.
  Our architecture is trained and evaluated on the standard video actions
benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of
the art. It also exceeds by a large margin previous attempts to use deep nets
for video classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2203</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2203</id><created>2014-06-09</created><updated>2014-09-16</updated><authors><author><keyname>Liu</keyname><forenames>Zhen</forenames></author><author><keyname>Dong</keyname><forenames>Weike</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author></authors><title>Local degree blocking model for link prediction in complex networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1063/1.4906371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering and reconstructing networks by accurately identifying missing and
unreliable links is a vital task in the domain of network analysis and mining.
In this article, by studying a specific local structure, namely a degree block
having a node and its all immediate neighbors, we find it contains important
statistical features of link formation for complex networks. We therefore
propose a parameter-free local blocking (LB) predictor to quantitatively detect
link formation in given networks via local link density calculations. The
promising experimental results performed on six real-world networks suggest
that the new index can outperform other traditional local similarity-based
methods on most of tested networks. After further analyzing the scores'
correlations between LB and two other methods, we find that the features of LB
index are analogous to those of both PA index and short-path-based index, which
empirically verify that large degree principle and short path principle
simultaneously captured by the LB index are jointly driving link formation in
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2204</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2204</id><created>2014-06-09</created><authors><author><keyname>Williams</keyname><forenames>Sandra</forenames></author><author><keyname>Power</keyname><forenames>Richard</forenames></author><author><keyname>Third</keyname><forenames>Allan</forenames></author></authors><title>How Easy is it to Learn a Controlled Natural Language for Building a
  Knowledge Base?</title><categories>cs.CL</categories><comments>CNL 2014 : Fourth Workshop on Controlled Natural Language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in controlled natural language editors for knowledge
engineering (KE) have given rise to expectations that they will make KE tasks
more accessible and perhaps even enable non-engineers to build knowledge bases.
This exploratory research focussed on novices and experts in knowledge
engineering during their attempts to learn a controlled natural language (CNL)
known as OWL Simplified English and use it to build a small knowledge base.
Participants' behaviours during the task were observed through eye-tracking and
screen recordings. This was an attempt at a more ambitious user study than in
previous research because we used a naturally occurring text as the source of
domain knowledge, and left them without guidance on which information to
select, or how to encode it. We have identified a number of skills
(competencies) required for this difficult task and key problems that authors
face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2205</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2205</id><created>2014-06-09</created><authors><author><keyname>Loe</keyname><forenames>Chuan Wen</forenames></author><author><keyname>Jensen</keyname><forenames>Henrik Jeldtoft</forenames></author></authors><title>Comparison of Communities Detection Algorithms for Multiplex</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.physa.2015.02.089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex is a set of graphs on the same vertex set, i.e.
$\{G(V,E_1),\ldots,G(V,E_m)\}$. It is a generalized graph to model multiple
relationships with parallel edges between vertices. This paper is a literature
review of existing communities detection algorithms for multiplex and a
comparative analysis of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2210</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2210</id><created>2014-06-09</created><updated>2014-07-14</updated><authors><author><keyname>Carbajal</keyname><forenames>Juan Pablo</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author><author><keyname>Hermans</keyname><forenames>Michiel</forenames></author><author><keyname>Schrauwen</keyname><forenames>Benjamin</forenames></author></authors><title>Memristor models for machine learning</title><categories>cs.LG cond-mat.mtrl-sci</categories><comments>4 figures, no tables. Submitted to neural computation</comments><doi>10.1162/NECO_a_00694</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the quest for alternatives to traditional CMOS, it is being suggested that
digital computing efficiency and power can be improved by matching the
precision to the application. Many applications do not need the high precision
that is being used today. In particular, large gains in area- and power
efficiency could be achieved by dedicated analog realizations of approximate
computing engines. In this work, we explore the use of memristor networks for
analog approximate computation, based on a machine learning framework called
reservoir computing. Most experimental investigations on the dynamics of
memristors focus on their nonvolatile behavior. Hence, the volatility that is
present in the developed technologies is usually unwanted and it is not
included in simulation models. In contrast, in reservoir computing, volatility
is not only desirable but necessary. Therefore, in this work, we propose two
different ways to incorporate it into memristor simulation models. The first is
an extension of Strukov's model and the second is an equivalent Wiener model
approximation. We analyze and compare the dynamical properties of these models
and discuss their implications for the memory and the nonlinear processing
capacity of memristor networks. Our results indicate that device variability,
increasingly causing problems in traditional computer design, is an asset in
the context of reservoir computing. We conclude that, although both models
could lead to useful memristor based reservoir computing systems, their
computational performance will differ. Therefore, experimental modeling
research is required for the development of accurate volatile memristor models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2222</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2222</id><created>2014-06-09</created><authors><author><keyname>Lenczycki</keyname><forenames>Timothy T.</forenames></author><author><keyname>Suto</keyname><forenames>Kelly</forenames></author><author><keyname>Williams</keyname><forenames>Christina</forenames></author><author><keyname>Strout</keyname><forenames>Michelle Mills</forenames></author></authors><title>The Chemistry Between High School Students and Computer Science</title><categories>cs.CY</categories><comments>8 pages, 2 figures</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer science enrollments have started to rise again, but the percentage
of women undergraduates in computer science is still low. Some studies indicate
this might be due to a lack of awareness of computer science at the high school
level. We present our experiences running a 5-year, high school outreach
program that introduces information about computer science within the context
of required chemistry courses. We developed interactive worksheets using
Molecular Workbench that help the students learn chemistry and computer science
concepts related to relevant events such as the gulf oil spill. Our evaluation
of the effectiveness of this approach indicates that the students do become
more aware of computer science as a discipline, but system support issues in
the classroom can make the approach difficult for teachers and discouraging for
the students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2227</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2227</id><created>2014-06-09</created><updated>2014-12-09</updated><authors><author><keyname>Jaderberg</keyname><forenames>Max</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Synthetic Data and Artificial Neural Networks for Natural Scene Text
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a framework for the recognition of natural scene
text. Our framework does not require any human-labelled data, and performs word
recognition on the whole image holistically, departing from the character based
recognition systems of the past. The deep neural network models at the centre
of this framework are trained solely on data produced by a synthetic text
generation engine -- synthetic data that is highly realistic and sufficient to
replace real data, giving us infinite amounts of training data. This excess of
data exposes new possibilities for word recognition models, and here we
consider three models, each one &quot;reading&quot; words in a different way: via 90k-way
dictionary encoding, character sequence encoding, and bag-of-N-grams encoding.
In the scenarios of language based and completely unconstrained text
recognition we greatly improve upon state-of-the-art performance on standard
datasets, using our fast, simple machinery and requiring zero data-acquisition
costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2234</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2234</id><created>2014-06-09</created><authors><author><keyname>Knowles</keyname><forenames>Bryan</forenames></author><author><keyname>Atici</keyname><forenames>Mustafa</forenames></author></authors><title>Fault-Tolerant, but Paradoxical Path-Finding in Physical and Conceptual
  Systems</title><categories>cs.SE cs.AI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report our initial investigations into reliability and path-finding based
models and propose future areas of interest. Inspired by broken sidewalks
during on-campus construction projects, we develop two models for navigating
this &quot;unreliable network.&quot; These are based on a concept of &quot;accumulating risk&quot;
backward from the destination, and both operate on directed acyclic graphs with
a probability of failure associated with each edge. The first serves to
introduce and has faults addressed by the second, more conservative model.
Next, we show a paradox when these models are used to construct polynomials on
conceptual networks, such as design processes and software development life
cycles. When the risk of a network increases uniformly, the most reliable path
changes from wider and longer to shorter and narrower. If we let professional
inexperience--such as with entry level cooks and software developers--represent
probability of edge failure, does this change in path imply that the novice
should follow instructions with fewer &quot;back-up&quot; plans, yet those with
alternative routes should be followed by the expert?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2235</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2235</id><created>2014-06-09</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author><author><keyname>Gashler</keyname><forenames>Michael</forenames></author></authors><title>A Hybrid Latent Variable Neural Network Model for Item Recommendation</title><categories>cs.LG cs.IR cs.NE stat.ML</categories><comments>10 pages, 3 tables. arXiv admin note: text overlap with
  arXiv:1312.5394</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is used to recommend items to a user without
requiring a knowledge of the item itself and tends to outperform other
techniques. However, collaborative filtering suffers from the cold-start
problem, which occurs when an item has not yet been rated or a user has not
rated any items. Incorporating additional information, such as item or user
descriptions, into collaborative filtering can address the cold-start problem.
In this paper, we present a neural network model with latent input variables
(latent neural network or LNN) as a hybrid collaborative filtering technique
that addresses the cold-start problem. LNN outperforms a broad selection of
content-based filters (which make recommendations based on item descriptions)
and other hybrid approaches while maintaining the accuracy of state-of-the-art
collaborative filtering techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2237</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2237</id><created>2014-06-09</created><updated>2014-10-14</updated><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>Reducing the Effects of Detrimental Instances</title><categories>stat.ML cs.LG</categories><comments>6 pages, 5 tables, 2 figures. arXiv admin note: substantial text
  overlap with arXiv:1403.1893</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not all instances in a data set are equally beneficial for inducing a model
of the data. Some instances (such as outliers or noise) can be detrimental.
However, at least initially, the instances in a data set are generally
considered equally in machine learning algorithms. Many current approaches for
handling noisy and detrimental instances make a binary decision about whether
an instance is detrimental or not. In this paper, we 1) extend this paradigm by
weighting the instances on a continuous scale and 2) present a methodology for
measuring how detrimental an instance may be for inducing a model of the data.
We call our method of identifying and weighting detrimental instances reduced
detrimental instance learning (RDIL). We examine RIDL on a set of 54 data sets
and 5 learning algorithms and compare RIDL with other weighting and filtering
approaches. RDIL is especially useful for learning algorithms where every
instance can affect the classification boundary and the training instances are
considered individually, such as multilayer perceptrons trained with
backpropagation (MLPs). Our results also suggest that a more accurate estimate
of which instances are detrimental can have a significant positive impact for
handling them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2255</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2255</id><created>2014-06-09</created><updated>2014-07-08</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy-Efficient Cooperative Cognitive Relaying Protocols for
  Full-Duplex Cognitive Radio Users and Delay-Aware Primary Users</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a network in which a primary user (PU) may cooperate
with a cognitive radio (CR) user for transmission of its data packets. The PU
is assumed to be a buffered terminal operating in a time-slotted fashion. We
develop two protocols which involve cooperation between primary and secondary
users. To satisfy certain quality of service requirements, users share time
slot duration and frequency bandwidth. Moreover, the secondary user (SU) may
leverage the primary feedback signal. The proposed protocols are designed such
that the secondary rate is maximized and the primary queueing delay is
maintained less than the queueing delay in case of non-cooperative PU. In
addition, the proposed protocols guarantee the stability of the primary queue
and maintain the average energy emitted by the CR user below a certain value.
The proposed protocols also provide more robust and potentially continuous
service for SUs compared to the conventional practice in cognitive networks
where SUs transmit in the spectrum holes and silence sessions of the PUs. We
include primary source burstiness, sensing errors, and feedback reception
errors to the analysis of the proposed cooperative cognitive protocols.
Numerical results show the beneficial gains of the cooperative protocols in
terms of secondary rate and primary throughput, queueing delay, and average
energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2262</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2262</id><created>2014-06-06</created><authors><author><keyname>Chadha</keyname><forenames>Ankit</forenames></author><author><keyname>Misal</keyname><forenames>Rishikesh</forenames></author><author><keyname>Mokashi</keyname><forenames>Tanaya</forenames></author><author><keyname>Chadha</keyname><forenames>Aman</forenames></author></authors><title>ARC Sort: Enhanced and Time Efficient Sorting Algorithm</title><categories>cs.DS</categories><journal-ref>International Journal of Applied Information Systems 7(2), 1-7,
  March 2014</journal-ref><doi>10.5120/ijais14-451130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses about a sorting algorithm which uses the concept of
buckets where each bucket represents a certain number of digits. A two
dimensional data structure is used where one dimension represents buckets i. e;
number of digits and each bucket's corresponding dimensions represents the
input numbers that belong to that bucket. Each bucket is then individually
sorted. Since every preceding bucket elements will always be smaller than the
succeeding buckets no comparison between them is required. By doing this we can
significantly reduced the time complexity of any sorting algorithm used to sort
the given set of inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2266</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2266</id><created>2014-06-05</created><authors><author><keyname>Davis</keyname><forenames>Jared</forenames><affiliation>Centaur Technology</affiliation></author><author><keyname>Kaufmann</keyname><forenames>Matt</forenames><affiliation>University of Texas at Austin</affiliation></author></authors><title>Industrial-Strength Documentation for ACL2</title><categories>cs.SE cs.MS</categories><comments>In Proceedings ACL2 2014, arXiv:1406.1238</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 152, 2014, pp. 9-25</journal-ref><doi>10.4204/EPTCS.152.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ACL2 theorem prover is a complex system. Its libraries are vast.
Industrial verification efforts may extend this base with hundreds of thousands
of lines of additional modeling tools, specifications, and proof scripts. High
quality documentation is vital for teams that are working together on projects
of this scale. We have developed XDOC, a flexible, scalable documentation tool
for ACL2 that can incorporate the documentation for ACL2 itself, the Community
Books, and an organization's internal formal verification projects, and which
has many features that help to keep the resulting manuals up to date. Using
this tool, we have produced a comprehensive, publicly available ACL2+Books
Manual that brings better documentation to all ACL2 users. We have also
developed an extended manual for use within Centaur Technology that extends the
public manual to cover Centaur's internal books. We expect that other
organizations using ACL2 will wish to develop similarly extended manuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2277</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2277</id><created>2014-06-09</created><authors><author><keyname>Martinez</keyname><forenames>Genaro J.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Alonso-Sanz</keyname><forenames>Ramon</forenames></author></authors><title>Designing Complex Dynamics in Cellular Automata with Memory</title><categories>nlin.CG cs.ET nlin.AO physics.comp-ph</categories><comments>55 pages, 26 figures</comments><journal-ref>International Journal of Bifurcation and Chaos, 23(10), 1330035
  (131 pages), 2013</journal-ref><doi>10.1142/S0218127413300358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since their inception at {\it Macy conferences} in later 1940s complex
systems remain the most controversial topic of inter-disciplinary sciences. The
term `complex system' is the most vague and liberally used scientific term.
Using elementary cellular automata (ECA), and exploiting the CA classification,
we demonstrate elusiveness of `complexity' by shifting space-time dynamics of
the automata from simple to complex by enriching cells with {\it memory}. This
way, we can transform any ECA class to another ECA class --- without changing
skeleton of cell-state transition function --- and vice versa by just selecting
a right kind of memory. A systematic analysis display that memory helps
`discover' hidden information and behaviour on trivial --- uniform, periodic,
and non-trivial --- chaotic, complex --- dynamical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2282</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2282</id><created>2014-06-09</created><authors><author><keyname>Wang</keyname><forenames>Chunyu</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Robust Estimation of 3D Human Poses from a Single Image</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human pose estimation is a key step to action recognition. We propose a
method of estimating 3D human poses from a single image, which works in
conjunction with an existing 2D pose/joint detector. 3D pose estimation is
challenging because multiple 3D poses may correspond to the same 2D pose after
projection due to the lack of depth information. Moreover, current 2D pose
estimators are usually inaccurate which may cause errors in the 3D estimation.
We address the challenges in three ways: (i) We represent a 3D pose as a linear
combination of a sparse set of bases learned from 3D human skeletons. (ii) We
enforce limb length constraints to eliminate anthropomorphically implausible
skeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error
between the projection of the 3D pose and the corresponding 2D detection. The
$L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the
alternating direction method (ADM) to solve the optimization problem
efficiently. Our approach outperforms the state-of-the-arts on three benchmark
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2283</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2283</id><created>2014-06-09</created><authors><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Puhrsch</keyname><forenames>Christian</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Depth Map Prediction from a Single Image using a Multi-Scale Deep
  Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting depth is an essential component in understanding the 3D geometry
of a scene. While for stereo images local correspondence suffices for
estimation, finding depth relations from a single image is less
straightforward, requiring integration of both global and local information
from various cues. Moreover, the task is inherently ambiguous, with a large
source of uncertainty coming from the overall scale. In this paper, we present
a new method that addresses this task by employing two deep network stacks: one
that makes a coarse global prediction based on the entire image, and another
that refines this prediction locally. We also apply a scale-invariant error to
help measure depth relations rather than scale. By leveraging the raw datasets
as large sources of training data, our method achieves state-of-the-art results
on both NYU Depth and KITTI, and matches detailed depth boundaries without the
need for superpixelation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2285</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2285</id><created>2014-06-09</created><authors><author><keyname>Chodisetti</keyname><forenames>Navya</forenames></author></authors><title>A Piggybank Protocol for Quantum Cryptography</title><categories>cs.CR</categories><comments>6 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a quantum mechanical version of the piggy-bank
cryptography protocol. The basic piggybank cryptography idea is to use two
communications: one with the encrypted message, and the other regarding the
encryption transformation which the receiver must decipher first. In the
quantum mechanical version of the protocol, the encrypting unitary
transformation information is sent separately but just deciphering it is not
enough to break the system. The proposed quantum protocol consists of two
stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2293</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2293</id><created>2014-06-09</created><updated>2016-02-14</updated><authors><author><keyname>Banerjee</keyname><forenames>Abhijit</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Arun G.</forenames></author><author><keyname>Duflo</keyname><forenames>Esther</forenames></author><author><keyname>Jackson</keyname><forenames>Matthew O.</forenames></author></authors><title>Gossip: Identifying Central Individuals in a Social Network</title><categories>physics.soc-ph cs.SI</categories><comments>46 pages, 3 figures, 11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible, simply by asking a few members of a community, to identify
individuals who are best placed to diffuse information? A simple model of
diffusion shows how boundedly rational individuals can, just by tracking gossip
about people, identify those who are most central in a network according to
&quot;diffusion centrality&quot; (a measure of network centrality which nests existing
ones, and predicts the extent to which piece of information seeded to a network
member diffuses in finite time). Using rich network data from 35 Indian
villages, we find that respondents accurately nominate those who are diffusion
central -- not just traditional leaders or those with many friends. In a
subsequent randomized field experiment in 213 villages, we track the diffusion
of a piece of information initially given to a small number of &quot;seeds&quot; in each
community. Seeds who are nominated by others lead to a near tripling of the
spread of information relative to randomly chosen seeds. Diffusion centrality
accounts for some, but not all, of the extra diffusion from these nominated
seeds compared to other seeds (including those with high social status) in our
experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2294</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2294</id><created>2014-06-09</created><authors><author><keyname>Lamping</keyname><forenames>John</forenames></author><author><keyname>Veach</keyname><forenames>Eric</forenames></author></authors><title>A Fast, Minimal Memory, Consistent Hash Algorithm</title><categories>cs.DS</categories><acm-class>E.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present jump consistent hash, a fast, minimal memory, consistent hash
algorithm that can be expressed in about 5 lines of code. In comparison to the
algorithm of Karger et al., jump consistent hash requires no storage, is
faster, and does a better job of evenly dividing the key space among the
buckets and of evenly dividing the workload when the number of buckets changes.
Its main limitation is that the buckets must be numbered sequentially, which
makes it more suitable for data storage applications than for distributed web
caching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2296</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2296</id><created>2014-06-09</created><updated>2015-04-12</updated><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author></authors><title>Approximating Nash Equilibria and Dense Subgraphs via an Approximate
  Version of Carath\'{e}odory's Theorem</title><categories>cs.GT cs.DS</categories><comments>28 pages; added references and extensions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithmic applications of an approximate version of
Carath\'{e}odory's theorem. The theorem states that given a set of vectors $X$
in $\mathbb{R}^d$, for every vector in the convex hull of $X$ there exists an
$\varepsilon$-close (under the $p$-norm distance, for $2\leq p &lt; \infty$)
vector that can be expressed as a convex combination of at most $b$ vectors of
$X$, where the bound $b$ depends on $\varepsilon$ and the norm $p$ and is
independent of the dimension $d$. This theorem can be derived by instantiating
Maurey's lemma, early references to which can be found in the work of Pisier
(1981) and Carl (1985). However, in this paper we present a self-contained
proof of this result.
  Using this theorem we establish that in a bimatrix game with $ n \times n$
payoff matrices $A, B$, if the number of non-zero entries in any column of
$A+B$ is at most $s$ then an $\varepsilon$-Nash equilibrium of the game can be
computed in time $n^{O\left(\frac{\log s }{\varepsilon^2}\right)}$. This, in
particular, gives us a polynomial-time approximation scheme for Nash
equilibrium in games with fixed column sparsity $s$. Moreover, for arbitrary
bimatrix games---since $s$ can be at most $n$---the running time of our
algorithm matches the best-known upper bound, which was obtained by Lipton,
Markakis, and Mehta (2003).
  The approximate Carath\'{e}odory's theorem also leads to an additive
approximation algorithm for the normalized densest $k$-subgraph problem. Given
a graph with $n$ vertices and maximum degree $d$, the developed algorithm
determines a subgraph with exactly $k$ vertices with normalized density within
$\varepsilon$ (in the additive sense) of the optimal in time $n^{O\left(
\frac{\log d}{\varepsilon^2}\right)}$. Additionally, we show that a similar
approximation result can be achieved for the problem of finding a $k \times
k$-bipartite subgraph of maximum normalized density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2298</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2298</id><created>2014-06-09</created><authors><author><keyname>Pace</keyname><forenames>Gordon J.</forenames></author><author><keyname>Rosner</keyname><forenames>Michael</forenames></author></authors><title>Explaining Violation Traces with Finite State Natural Language
  Generation Models</title><categories>cs.SE cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An essential element of any verification technique is that of identifying and
communicating to the user, system behaviour which leads to a deviation from the
expected behaviour. Such behaviours are typically made available as long traces
of system actions which would benefit from a natural language explanation of
the trace and especially in the context of business logic level specifications.
In this paper we present a natural language generation model which can be used
to explain such traces. A key idea is that the explanation language is a CNL
that is, formally speaking, regular language susceptible transformations that
can be expressed with finite state machinery. At the same time it admits
various forms of abstraction and simplification which contribute to the
naturalness of explanations that are communicated to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2311</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2311</id><created>2014-06-08</created><authors><author><keyname>Kaur</keyname><forenames>Manpreet</forenames></author><author><keyname>Abhilasha</keyname></author></authors><title>Survey on Variants of Distributed Energy efficient Clustering Protocols
  in heterogeneous Wireless Sensor Network</title><categories>cs.NI</categories><comments>17 Pages, IJCEA, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are composed of low cost and extremely power
constrained sensor nodes which are scattered over a region forming self
organized networks, making energy consumption a crucial design issue. Thus,
finite network lifetime is widely regarded as a fundamental performance
bottleneck. These networks are used for various applications such as field
monitoring, home automation, medical data collection or surveillance. Research
has shown that clustering sensor nodes is an efficient method to manage energy
consumption for prolonging the network lifetime. Presence of heterogeneity
enhances the lifetime and reliability in network. In this paper, we present the
distributed and energy efficient clustering protocols which follow the thoughts
of Distributed Energy Efficient Clustering protocol. Objective of our work is
to analyze that how these extended routing protocols work in order to optimize
network lifetime and how routing protocols are improved. We emphasizes on
issues experienced by various protocols and how these issues are tackled by
other enhanced protocols. This provides a survey of work done on distributed
energy efficient protocols
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2313</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2313</id><created>2014-06-09</created><authors><author><keyname>Giadom</keyname><forenames>Vigale Leelanubari</forenames></author><author><keyname>Edem</keyname><forenames>Williams E.</forenames></author></authors><title>Context management strategies in wireless network</title><categories>cs.NI</categories><journal-ref>International Journal of advanced studies in Computer Science and
  Engineering IJASCSE, Volume 3, Issue 5, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context management strategies in wireless technology are dependent upon the
collection of accurate information from the individual nodes. This information
(called context information) can be exploited by administrators or automated
systems in order to decide on specific management concerns. While traditional
approaches for fixed networks are more or less centralized, more complex
management strategies have been proposed for wireless networks, such as
hierarchical, fully distributed and hybrid ones. The reason for the
introduction of new strategies is based on the dynamic and unpredictable nature
of wireless networks and their (usually) limited resources, which do not
support centralized management solutions. In this work, efforts is being made
to uncovered some specific strategies that can be used to managed context
information that reaches the centre of decision making, the work is concluded
with a detail comparison of the strategies to enable context application
developers make right choice of strategy to be employed in a specific
situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2348</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2348</id><created>2014-06-09</created><updated>2014-12-03</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Raniszewski</keyname><forenames>Marcin</forenames></author></authors><title>Sampling the suffix array with minimizers</title><categories>cs.DS</categories><comments>One new SamSAMi variant; extended experimental results</comments><msc-class>68W32</msc-class><acm-class>F.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling (evenly) the suffixes from the suffix array is an old idea trading
the pattern search time for reduced index space. A few years ago Claude et al.
showed an alphabet sampling scheme allowing for more efficient pattern searches
compared to the sparse suffix array, for long enough patterns. A drawback of
their approach is the requirement that sought patterns need to contain at least
one character from the chosen subalphabet. In this work we propose an
alternative suffix sampling approach with only a minimum pattern length as a
requirement, which seems more convenient in practice. Experiments show that our
algorithm achieves competitive time-space tradeoffs on most standard benchmark
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2358</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2358</id><created>2014-06-06</created><authors><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Conjunction and Negation of Natural Concepts: A Quantum-theoretic
  Modeling</title><categories>cs.AI q-bio.NC</categories><comments>32 pages, standard latex, no figures, 16 tables. arXiv admin note:
  text overlap with arXiv:1311.6050; and text overlap with arXiv:0805.3850 by
  other authors</comments><journal-ref>Journal of Mathematical Psychology 66, 83-102 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform two experiments with the aim to investigate the effects of
negation on the combination of natural concepts. In the first experiment, we
test the membership weights of a list of exemplars with respect to two
concepts, e.g., {\it Fruits} and {\it Vegetables}, and their conjunction {\it
Fruits And Vegetables}. In the second experiment, we test the membership
weights of the same list of exemplars with respect to the same two concepts,
but negating the second, e.g., {\it Fruits} and {\it Not Vegetables}, and again
their conjunction {\it Fruits And Not Vegetables}. The collected data confirm
existing results on conceptual combination, namely, they show dramatic
deviations from the predictions of classical (fuzzy set) logic and probability
theory. More precisely, they exhibit conceptual vagueness, gradeness of
membership, overextension and double overextension of membership weights with
respect to the given conjunctions. Then, we show that the quantum probability
model in Fock space recently elaborated to model Hampton's data on concept
conjunction (Hampton, 1988a) and disjunction (Hampton, 1988b) faithfully
accords with the collected data. Our quantum-theoretic modeling enables to
describe these non-classical effects in terms of genuine quantum effects,
namely `contextuality', `superposition', `interference' and `emergence'. The
obtained results confirm and strenghten the analysis in Aerts (2009a) and Sozzo
(2014) on the identification of quantum aspects in experiments on conceptual
vagueness. Our results can be inserted within the general research on the
identification of quantum structures in cognitive and decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2370</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2370</id><created>2014-06-09</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames></author><author><keyname>Barenbaum</keyname><forenames>Pablo</forenames></author><author><keyname>Mazza</keyname><forenames>Damiano</forenames></author></authors><title>Distilling Abstract Machines (Long Version)</title><categories>cs.PL</categories><comments>63 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that many environment-based abstract machines can be seen as
strategies in lambda calculi with explicit substitutions (ES). Recently,
graphical syntaxes and linear logic led to the linear substitution calculus
(LSC), a new approach to ES that is halfway between big-step calculi and
traditional calculi with ES. This paper studies the relationship between the
LSC and environment-based abstract machines. While traditional calculi with ES
simulate abstract machines, the LSC rather distills them: some transitions are
simulated while others vanish, as they map to a notion of structural
congruence. The distillation process unveils that abstract machines in fact
implement weak linear head reduction, a notion of evaluation having a central
role in the theory of linear logic. We show that such a pattern applies
uniformly in call-by-name, call-by-value, and call-by-need, catching many
machines in the literature. We start by distilling the KAM, the CEK, and the
ZINC, and then provide simplified versions of the SECD, the lazy KAM, and
Sestoft's machine. Along the way we also introduce some new machines with
global environments. Moreover, we show that distillation preserves the time
complexity of the executions, i.e. the LSC is a complexity-preserving
abstraction of abstract machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2375</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2375</id><created>2014-06-09</created><updated>2014-06-11</updated><authors><author><keyname>Lu</keyname><forenames>Wenhao</forenames></author><author><keyname>Lian</keyname><forenames>Xiaochen</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Parsing Semantic Parts of Cars Using Graphical Models and Segment
  Appearance Consistency</title><categories>cs.CV</categories><comments>12 pages, CBMM memo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of semantic part parsing (segmentation) of
cars, i.e.assigning every pixel within the car to one of the parts (e.g.body,
window, lights, license plates and wheels). We formulate this as a landmark
identification problem, where a set of landmarks specifies the boundaries of
the parts. A novel mixture of graphical models is proposed, which dynamically
couples the landmarks to a hierarchy of segments. When modeling pairwise
relation between landmarks, this coupling enables our model to exploit the
local image contents in addition to spatial deformation, an aspect that most
existing graphical models ignore. In particular, our model enforces appearance
consistency between segments within the same part. Parsing the car, including
finding the optimal coupling between landmarks and segments in the hierarchy,
is performed by dynamic programming. We evaluate our method on a subset of
PASCAL VOC 2010 car images and on the car subset of 3D Object Category dataset
(CAR3D). We show good results and, in particular, quantify the effectiveness of
using the segment appearance consistency in terms of accuracy of part
localization and segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2390</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2390</id><created>2014-06-09</created><updated>2014-11-03</updated><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Cheng</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Unsupervised Deep Haar Scattering on Graphs</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification of high-dimensional data defined on graphs is particularly
difficult when the graph geometry is unknown. We introduce a Haar scattering
transform on graphs, which computes invariant signal descriptors. It is
implemented with a deep cascade of additions, subtractions and absolute values,
which iteratively compute orthogonal Haar wavelet transforms. Multiscale
neighborhoods of unknown graphs are estimated by minimizing an average total
variation, with a pair matching algorithm of polynomial complexity. Supervised
classification with dimension reduction is tested on data bases of scrambled
images, and for signals sampled on unknown irregular grids on a sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2392</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2392</id><created>2014-06-09</created><authors><author><keyname>Compton</keyname><forenames>Ryan</forenames></author><author><keyname>Keegan</keyname><forenames>Matthew S.</forenames></author><author><keyname>Xu</keyname><forenames>Jiejun</forenames></author></authors><title>Inferring the geographic focus of online documents from social media
  sharing patterns</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 10 figures, Computational Approaches to Social Modeling
  (ChASM) Workshop, WebSci 2014, Bloomington, Indiana-June 24-26 2014</comments><acm-class>H.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the geographic focus of digital media is an essential first step
for modern geographic information retrieval. However, publicly-visible location
annotations are remarkably sparse in online data. In this work, we demonstrate
a method which infers the geographic focus of an online document by examining
the locations of Twitter users who share links to the document.
  We apply our geotagging technique to multiple datasets built from different
content: manually-annotated news articles, GDELT, YouTube, Flickr, Twitter, and
Tumblr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2395</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2395</id><created>2014-06-09</created><authors><author><keyname>Almeida</keyname><forenames>Ezilda</forenames></author><author><keyname>Ferreira</keyname><forenames>Pedro</forenames></author><author><keyname>Vinhoza</keyname><forenames>Tiago</forenames></author><author><keyname>Dutra</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Li</keyname><forenames>Jingwei</forenames></author><author><keyname>Wu</keyname><forenames>Yirong</forenames></author><author><keyname>Burnside</keyname><forenames>Elizabeth</forenames></author></authors><title>ExpertBayes: Automatically refining manually built Bayesian networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian network structures are usually built using only the data and
starting from an empty network or from a naive Bayes structure. Very often, in
some domains, like medicine, a prior structure knowledge is already known. This
structure can be automatically or manually refined in search for better
performance models. In this work, we take Bayesian networks built by
specialists and show that minor perturbations to this original network can
yield better classifiers with a very small computational cost, while
maintaining most of the intended meaning of the original model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2398</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2398</id><created>2014-06-09</created><authors><author><keyname>Minkus</keyname><forenames>Tehila</forenames></author><author><keyname>Memon</keyname><forenames>Nasir</forenames></author></authors><title>Leveraging Personalization To Facilitate Privacy</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks have enabled new methods and modalities of
collaboration and sharing. These advances bring privacy concerns: online social
data is more accessible and persistent and simultaneously less contextualized
than traditional social interactions. To allay these concerns, many web
services allow users to configure their privacy settings based on a set of
multiple-choice questions.
  We suggest a new paradigm for privacy options. Instead of suggesting the same
defaults to each user, services can leverage knowledge of users' traits to
recommend a machine-learned prediction of their privacy preferences for
Facebook. As a case study, we build and evaluate MyPrivacy, a publicly
available web application that suggests personalized privacy settings. An
evaluation with 199 users shows that users find the suggestions to be
appropriate and private; furthermore, they express intent to implement the
recommendations made by MyPrivacy. This supports the proposal to put
personalization to work in online communities to promote privacy and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2400</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2400</id><created>2014-06-09</created><authors><author><keyname>Dann&#xe9;lls</keyname><forenames>Dana</forenames></author><author><keyname>Gr&#x16b;z&#x12b;tis</keyname><forenames>Normunds</forenames></author></authors><title>Controlled Natural Language Generation from a Multilingual
  FrameNet-based Grammar</title><categories>cs.CL</categories><journal-ref>Lecture Notes in Computer Science, Vol. 8625, Springer, 2014, pp.
  155-166</journal-ref><doi>10.1007/978-3-319-10223-8_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a currently bilingual but potentially multilingual
FrameNet-based grammar library implemented in Grammatical Framework. The
contribution of this paper is two-fold. First, it offers a methodological
approach to automatically generate the grammar based on semantico-syntactic
valence patterns extracted from FrameNet-annotated corpora. Second, it provides
a proof of concept for two use cases illustrating how the acquired multilingual
grammar can be exploited in different CNL applications in the domains of arts
and tourism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2407</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2407</id><created>2014-06-09</created><authors><author><keyname>Bristow</keyname><forenames>Hilton</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Optimization Methods for Convolutional Sparse Coding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse and convolutional constraints form a natural prior for many
optimization problems that arise from physical processes. Detecting motifs in
speech and musical passages, super-resolving images, compressing videos, and
reconstructing harmonic motions can all leverage redundancies introduced by
convolution. Solving problems involving sparse and convolutional constraints
remains a difficult computational problem, however. In this paper we present an
overview of convolutional sparse coding in a consistent framework. The
objective involves iteratively optimizing a convolutional least-squares term
for the basis functions, followed by an L1-regularized least squares term for
the sparse coefficients. We discuss a range of optimization methods for solving
the convolutional sparse coding objective, and the properties that make each
method suitable for different applications. In particular, we concentrate on
computational complexity, speed to {\epsilon} convergence, memory usage, and
the effect of implied boundary conditions. We present a broad suite of examples
covering different signal and application domains to illustrate the general
applicability of convolutional sparse coding, and the efficacy of the available
optimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2419</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2419</id><created>2014-06-10</created><authors><author><keyname>Bristow</keyname><forenames>Hilton</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Why do linear SVMs trained on HOG features perform so well?</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Support Vector Machines trained on HOG features are now a de facto
standard across many visual perception tasks. Their popularisation can largely
be attributed to the step-change in performance they brought to pedestrian
detection, and their subsequent successes in deformable parts models. This
paper explores the interactions that make the HOG-SVM symbiosis perform so
well. By connecting the feature extraction and learning processes rather than
treating them as disparate plugins, we show that HOG features can be viewed as
doing two things: (i) inducing capacity in, and (ii) adding prior to a linear
SVM trained on pixels. From this perspective, preserving second-order
statistics and locality of interactions are key to good performance. We
demonstrate surprising accuracy on expression recognition and pedestrian
detection tasks, by assuming only the importance of preserving such local
second-order interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2426</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2426</id><created>2014-06-10</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Srinivasan</keyname><forenames>Sriram</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author><author><keyname>Bhowmick</keyname><forenames>Sanjukta</forenames></author></authors><title>On the Permanence of Vertices in Network Communities</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 5 figures, 8 tables, Accepted in 20th ACM SIGKDD Conference
  on Knowledge Discovery and Data Mining</comments><acm-class>H.2.8; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the prevalence of community detection algorithms, relatively less
work has been done on understanding whether a network is indeed modular and how
resilient the community structure is under perturbations. To address this
issue, we propose a new vertex-based metric called &quot;permanence&quot;, that can
quantitatively give an estimate of the community-like structure of the network.
  The central idea of permanence is based on the observation that the strength
of membership of a vertex to a community depends upon the following two
factors: (i) the distribution of external connectivity of the vertex to
individual communities and not the total external connectivity, and (ii) the
strength of its internal connectivity and not just the total internal edges.
  In this paper, we demonstrate that compared to other metrics, permanence
provides (i) a more accurate estimate of a derived community structure to the
ground-truth community and (ii) is more sensitive to perturbations in the
network. As a by-product of this study, we have also developed a community
detection algorithm based on maximizing permanence. For a modular network
structure, the results of our algorithm match well with ground-truth
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2428</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2428</id><created>2014-06-10</created><authors><author><keyname>Roy</keyname><forenames>Bodhayan</forenames></author></authors><title>Point visibility graph recognition is NP-hard</title><categories>cs.CG cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a 3-SAT formula, a graph can be constructed in polynomial time such
that the graph is a point visibility graph if and only if the 3-SAT formula is
satisfiable. This reduction establishes that the problem of recognition of
point visibility graphs is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2431</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2431</id><created>2014-06-10</created><updated>2014-11-19</updated><authors><author><keyname>Anava</keyname><forenames>Oren</forenames></author><author><keyname>Golan</keyname><forenames>Shahar</forenames></author><author><keyname>Golbandi</keyname><forenames>Nadav</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Lempel</keyname><forenames>Ronny</forenames></author><author><keyname>Rokhlenko</keyname><forenames>Oleg</forenames></author><author><keyname>Somekh</keyname><forenames>Oren</forenames></author></authors><title>Budget-Constrained Item Cold-Start Handling in Collaborative Filtering
  Recommenders via Optimal Design</title><categories>cs.IR cs.LG</categories><comments>11 pages, 2 figures</comments><msc-class>62K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that collaborative filtering (CF) based recommender systems
provide better modeling of users and items associated with considerable rating
history. The lack of historical ratings results in the user and the item
cold-start problems. The latter is the main focus of this work. Most of the
current literature addresses this problem by integrating content-based
recommendation techniques to model the new item. However, in many cases such
content is not available, and the question arises is whether this problem can
be mitigated using CF techniques only. We formalize this problem as an
optimization problem: given a new item, a pool of available users, and a budget
constraint, select which users to assign with the task of rating the new item
in order to minimize the prediction error of our model. We show that the
objective function is monotone-supermodular, and propose efficient optimal
design based algorithms that attain an approximation to its optimum. Our
findings are verified by an empirical study using the Netflix dataset, where
the proposed algorithms outperform several baselines for the problem at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2454</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2454</id><created>2014-06-10</created><authors><author><keyname>Hu</keyname><forenames>Chunhe</forenames></author><author><keyname>Chen</keyname><forenames>Zongji</forenames></author></authors><title>Distributed consensus on minimum time rendezvous via cyclic alternating
  projection</title><categories>cs.SY math.OC</categories><comments>6 pages, 6 figures, submitted to CDC 2014</comments><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a distributed algorithm to solve planar minimum
time multi-vehicle rendezvous problem with non-identical velocity constraints
on cyclic digraph (topology). Motivated by the cyclic alternating projection
method that can compute a point's projection on the intersection of some convex
sets, we transform the minimum time rendezvous problem into finding the
distance between the position plane and the intersection of several
second-order cones in position-time space. The distance can be achieved by
metric projecting onto the plane and the intersection persistently from any
initial point, where the projection onto the intersection is obtained by
Dykstra's alternating projection algorithm. It is shown that during the
procedure, vehicles use only the information from neighbors and can apply the
projection onto the plane asynchronously. Demonstrations are worked out to
illustrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2456</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2456</id><created>2014-06-10</created><authors><author><keyname>Yu</keyname><forenames>Li</forenames></author><author><keyname>Perez-Delgado</keyname><forenames>Carlos A.</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>Limitations on information theoretically secure quantum homomorphic
  encryption</title><categories>quant-ph cs.CR</categories><comments>4 pages, 1 figure</comments><journal-ref>Phys. Rev. A 90, 050303 (2014)</journal-ref><doi>10.1103/PhysRevA.90.050303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homomorphic encryption is a form of encryption which allows computation to be
carried out on the encrypted data without the need for decryption. The success
of quantum approaches to related tasks in a delegated computation setting has
raised the question of whether quantum mechanics may be used to achieve
information theoretically secure fully homomorphic encryption. Here we show,
via an information localisation argument, that deterministic fully homomorphic
encryption necessarily incurs exponential overhead if perfect security is
required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2459</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2459</id><created>2014-06-10</created><authors><author><keyname>Hu</keyname><forenames>Chunhe</forenames></author><author><keyname>Chen</keyname><forenames>Zongji</forenames></author></authors><title>Distributed MIN-MAX Optimization Application to Time-optimal Consensus:
  An Alternating Projection Approach</title><categories>cs.SY cs.NA math.OC</categories><comments>11 pages, 6 figures, submitted to AIAA GNC 2015</comments><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed an alternating projection based algorithm to solve
a class of distributed MIN-MAX convex optimization problems. We firstly
transform this MINMAX problem into the problem of searching for the minimum
distance between some hyper-plane and the intersection of the epigraphs of
convex functions. The Bregman's alternating method is employed in our algorithm
to achieve the distance by iteratively projecting onto the hyper-plane and the
intersection. The projection onto the intersection is obtained by cyclic
Dykstra's projection method. We further apply our algorithm to the minimum time
multi-agent consensus problem. The attainable states set for the agent can be
transformed into the epigraph of some convex functions, and the search for
time-optimal state for consensus satisfies the MIN-MAX problem formulation.
Finally, the numerous simulation proves the validity of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2464</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2464</id><created>2014-06-10</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>Pandharipande</keyname><forenames>Meghna</forenames></author><author><keyname>Sita</keyname><forenames>G</forenames></author></authors><title>Music and Vocal Separation Using Multi-Band Modulation Based Features</title><categories>cs.SD cs.AI</categories><comments>5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics
  Applications (ISIEA)</comments><doi>10.1109/ISIEA.2010.5679370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential use of non-linear speech features has not been investigated for
music analysis although other commonly used speech features like Mel Frequency
Ceptral Coefficients (MFCC) and pitch have been used extensively. In this
paper, we assume an audio signal to be a sum of modulated sinusoidal and then
use the energy separation algorithm to decompose the audio into amplitude and
frequency modulation components using the non-linear Teager-Kaiser energy
operator. We first identify the distribution of these non-linear features for
music only and voice only segments in the audio signal in different Mel spaced
frequency bands and show that they have the ability to discriminate. The
proposed method based on Kullback-Leibler divergence measure is evaluated using
a set of Indian classical songs from three different artists. Experimental
results show that the discrimination ability is evident in certain low and mid
frequency bands (200 - 1500 Hz).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2470</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2470</id><created>2014-06-10</created><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Detti</keyname><forenames>Andrea</forenames></author><author><keyname>Pisa</keyname><forenames>Claudio</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Blefari-Melazzi</keyname><forenames>Nicola</forenames></author></authors><title>Controller selection in a Wireless Mesh SDN under network partitioning
  and merging scenarios</title><categories>cs.NI</categories><comments>Submitted paper (May 26th, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a Wireless Mesh Network (WMN) integrating SDN
principles. The Wireless Mesh Routers (WMR) are OpenFlow capable switches that
can be controlled by SDN controllers, according to the wmSDN (wireless mesh
SDN) architecture that we have introduced in a previous work. We consider the
issue of controller selection in a scenario with intermittent connectivity. We
assume that over time a single WMN can become split in two or more partitions
and that separate partitions can merge into a larger one. We assume that a set
of SDN controllers can potentially take control of the WMRs. At a given time
only one controller should be the master of a WMR and it should be the most
appropriate one according to some metric. We argue that the state of the art
solutions for &quot;master election&quot; among distributed controllers are not suitable
in a mesh networking environment, as they could easily be affected by
inconsistencies. We envisage a &quot;master selection&quot; approach which is under the
control of each WMR, and guarantees that at a given time only one controller
will be master of a WMR. We designed a specific master selection procedure
which is very simple in terms of the control logic to be executed in the WMR.
We have implemented the proposed solution and deployed it over a network
emulator (CORE) and over the combination of two physical wireless testbeds
(NITOS and wiLab.t).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2479</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2479</id><created>2014-06-10</created><authors><author><keyname>Mostafavi</keyname><forenames>Seyedakbar</forenames></author><author><keyname>Dehghan</keyname><forenames>Mehdi</forenames></author></authors><title>Decentralized Adaptive Helper Selection in Multi-channel P2P Streaming
  Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Peer-to-Peer (P2P) multichannel live streaming, helper peers with surplus
bandwidth resources act as micro-servers to compensate the server deficiencies
in balancing the resources between different channel overlays. With deployment
of helper level between server and peers, optimizing the user/helper topology
becomes a challenging task since applying well-known reciprocity-based choking
algorithms is impossible due to the one-directional nature of video streaming
from helpers to users. Because of selfish behavior of peers and lack of central
authority among them, selection of helpers requires coordination. In this
paper, we design a distributed online helper selection mechanism which is
adaptable to supply and demand pattern of various video channels. Our solution
for strategic peers' exploitation from the shared resources of helpers is to
guarantee the convergence to correlated equilibria (CE) among the helper
selection strategies. Online convergence to the set of CE is achieved through
the regret-tracking algorithm which tracks the equilibrium in the presence of
stochastic dynamics of helpers' bandwidth. The resulting CE can help us select
proper cooperation policies. Simulation results demonstrate that our algorithm
achieves good convergence, load distribution on helpers and sustainable
streaming rates for peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2480</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2480</id><created>2014-06-10</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author><author><keyname>Pi&#xf3;ro</keyname><forenames>Michal</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Optimization of Free Space Optical Wireless Network for Cellular
  Backhauling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With densification of nodes in cellular networks, free space optic (FSO)
connections are becoming an appealing low cost and high rate alternative to
copper and fiber as the backhaul solution for wireless communication systems.
To ensure a reliable cellular backhaul, provisions for redundant, disjoint
paths between the nodes must be made in the design phase. This paper aims at
finding a cost-effective solution to upgrade the cellular backhaul with
pre-deployed optical fibers using FSO links and mirror components. Since the
quality of the FSO links depends on several factors, such as transmission
distance, power, and weather conditions, we adopt an elaborate formulation to
calculate link reliability. We present a novel integer linear programming model
to approach optimal FSO backhaul design, guaranteeing $K$-disjoint paths
connecting each node pair. Next, we derive a column generation method to a
path-oriented mathematical formulation. Applying the method in a sequential
manner enables high computational scalability. We use realistic scenarios to
demonstrate our approaches efficiently provide optimal or near-optimal
solutions, and thereby allow for accurately dealing with the trade-off between
cost and reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2495</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2495</id><created>2014-06-10</created><authors><author><keyname>Firth</keyname><forenames>Hugo</forenames></author><author><keyname>Missier</keyname><forenames>Paolo</forenames></author></authors><title>ProvGen: generating synthetic PROV graphs with predictable structure</title><categories>cs.DB</categories><comments>IPAW'14 paper, In Procs. IPAW 2014 (Provenance and Annotations).
  Koln, Germany: Springer, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces provGen, a generator aimed at producing large synthetic
provenance graphs with predictable properties and of arbitrary size. Synthetic
provenance graphs serve two main purposes. Firstly, they provide a variety of
controlled workloads that can be used to test storage and query capabilities of
provenance management systems at scale. Secondly, they provide challenging
testbeds for experimenting with graph algorithms for provenance analytics, an
area of increasing research interest. provGen produces PROV graphs and stores
them in a graph DBMS (Neo4J). A key feature is to let users control the
relationship makeup and topological features of the graph, by providing a seed
provenance pattern along with a set of constraints, expressed using a custom
Domain Specific Language. We also propose a simple method for evaluating the
quality of the generated graphs, by measuring how realistically they simulate
the structure of real-world patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2504</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2504</id><created>2014-06-10</created><updated>2015-07-01</updated><authors><author><keyname>Xin</keyname><forenames>Bo</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author></authors><title>Exploring Algorithmic Limits of Matrix Rank Minimization under Affine
  Constraints</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require recovering a matrix of minimal rank within an
affine constraint set, with matrix completion a notable special case. Because
the problem is NP-hard in general, it is common to replace the matrix rank with
the nuclear norm, which acts as a convenient convex surrogate. While elegant
theoretical conditions elucidate when this replacement is likely to be
successful, they are highly restrictive and convex algorithms fail when the
ambient rank is too high or when the constraint set is poorly structured.
Non-convex alternatives fare somewhat better when carefully tuned; however,
convergence to locally optimal solutions remains a continuing source of
failure. Against this backdrop we derive a deceptively simple and
parameter-free probabilistic PCA-like algorithm that is capable, over a wide
battery of empirical tests, of successful recovery even at the theoretical
limit where the number of measurements equal the degrees of freedom in the
unknown low-rank matrix. Somewhat surprisingly, this is possible even when the
affine constraint set is highly ill-conditioned. While proving general recovery
guarantees remains evasive for non-convex algorithms, Bayesian-inspired or
otherwise, we nonetheless show conditions whereby the underlying cost function
has a unique stationary point located at the global optimum; no existing cost
function we are aware of satisfies this same property. We conclude with a
simple computer vision application involving image rectification and a standard
collaborative filtering benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2507</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2507</id><created>2014-06-10</created><updated>2014-08-07</updated><authors><author><keyname>Taylor</keyname><forenames>Tim</forenames></author></authors><title>WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings</title><categories>cs.NE cs.MA</categories><comments>Editors: Tim Taylor, Josh Auerbach, Josh Bongard, Jeff Clune, Simon
  Hickinbotham, Greg Hornby</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held at
the 14th International Conference on the Synthesis and Simulation of Living
Systems (ALIFE 14), New York, NY, 31 July 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2516</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2516</id><created>2014-06-10</created><authors><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author></authors><title>Subsidization Competition: Vitalizing the Neutral Internet</title><categories>cs.NI cs.CY</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Unlike telephone operators, which pay termination fees to reach the users of
another network, Internet Content Providers (CPs) do not pay the Internet
Service Providers (ISPs) of users they reach. While the consequent cross
subsidization to CPs has nurtured content innovations at the edge of the
Internet, it reduces the investment incentives for the access ISPs to expand
capacity. As potential charges for terminating CPs' traffic are criticized
under the net neutrality debate, we propose to allow CPs to voluntarily
subsidize the usagebased fees induced by their content traffic for end-users.
We model the regulated subsidization competition among CPs under a neutral
network and show how deregulation of subsidization could increase an access
ISP's utilization and revenue, strengthening its investment incentives.
Although the competition might harm certain CPs, we find that the main cause
comes from high access prices rather than the existence of subsidization. Our
results suggest that subsidization competition will increase the
competitiveness and welfare of the Internet content market; however, regulators
might need to regulate access prices if the access ISP market is not
competitive enough. We envision that subsidization competition could become a
viable model for the future Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2518</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2518</id><created>2014-06-10</created><authors><author><keyname>Campigotto</keyname><forenames>Romain</forenames></author><author><keyname>C&#xe9;spedes</keyname><forenames>Patricia Conde</forenames></author><author><keyname>Guillaume</keyname><forenames>Jean-Loup</forenames></author></authors><title>A Generalized and Adaptive Method for Community Detection</title><categories>cs.SI physics.soc-ph stat.AP</categories><acm-class>G.4; G.2.2; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks represent interactions between entities. They appear in
various contexts such as sociology, biology, etc., and they generally contain
highly connected subgroups called communities. Community detection is a
well-studied problem and most of the algorithms aim to maximize the
Newman-Girvan modularity function, the most popular being the Louvain method
(it is well-suited on very large graphs). However, the classical modularity has
many drawbacks: we can find partitions of high quality in graphs without
community structure, e.g., on random graphs; it promotes large communities.
Then, we have adapted the Louvain method to other quality functions. In this
paper, we describe a generic version of the Louvain method. In particular, we
give a sufficient condition to plug a quality function into it. We also show
that global performance of this new version is similar to the classical Louvain
algorithm, that promotes it to the best rank of the community detection
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2519</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2519</id><created>2014-06-10</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author><author><keyname>Villares</keyname><forenames>Ignacio Azagra</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>On Importance of Steganographic Cost For Network Steganography</title><categories>cs.MM cs.CR</categories><comments>15 pages, 14 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network steganography encompasses the information hiding techniques that can
be applied in communication network environments and that utilize hidden data
carriers for this purpose. In this paper we introduce a characteristic called
steganographic cost which is an indicator for the degradation or distortion of
the carrier caused by the application of the steganographic method. Based on
exemplary cases for single- and multi-method steganographic cost analyses we
observe that it can be an important characteristic that allows to express
hidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR
(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.
Steganographic cost can moreover be helpful to analyse the relationships
between two or more steganographic methods applied to the same hidden data
carrier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2528</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2528</id><created>2014-06-10</created><authors><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author></authors><title>Denosing Using Wavelets and Projections onto the L1-Ball</title><categories>math.OC cs.CV</categories><comments>Submitted to Signal Processing Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both wavelet denoising and denosing methods using the concept of sparsity are
based on soft-thresholding. In sparsity based denoising methods, it is assumed
that the original signal is sparse in some transform domains such as the
wavelet domain and the wavelet subsignals of the noisy signal are projected
onto L1-balls to reduce noise. In this lecture note, it is shown that the size
of the L1-ball or equivalently the soft threshold value can be determined using
linear algebra. The key step is an orthogonal projection onto the epigraph set
of the L1-norm cost function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2534</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2534</id><created>2014-06-10</created><authors><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Prokop</keyname><forenames>Christoph</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Load Hiding of Household's Power Demand</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development and introduction of smart metering, the energy
information for costumers will change from infrequent manual meter readings to
fine-grained energy consumption data. On the one hand these fine-grained
measurements will lead to an improvement in costumers' energy habits, but on
the other hand the fined-grained data produces information about a household
and also households' inhabitants, which are the basis for many future privacy
issues. To ensure household privacy and smart meter information owned by the
household inhabitants, load hiding techniques were introduced to obfuscate the
load demand visible at the household energy meter. In this work, a
state-of-the-art battery-based load hiding (BLH) technique, which uses a
controllable battery to disguise the power consumption and a novel load hiding
technique called load-based load hiding (LLH) are presented. An LLH system uses
an controllable household appliance to obfuscate the household's power demand.
We evaluate and compare both load hiding techniques on real household data and
show that both techniques can strengthen household privacy but only LLH can
increase appliance level privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2538</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2538</id><created>2014-06-10</created><authors><author><keyname>Barzdins</keyname><forenames>Guntis</forenames></author></authors><title>FrameNet CNL: a Knowledge Representation and Information Extraction
  Language</title><categories>cs.CL cs.AI cs.IR cs.LG</categories><comments>CNL-2014 camera-ready version. The final publication is available at
  link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a FrameNet-based information extraction and knowledge
representation framework, called FrameNet-CNL. The framework is used on natural
language documents and represents the extracted knowledge in a tailor-made
Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be
generated automatically in multiple languages. This approach brings together
the fields of information extraction and CNL, because a source text can be
considered belonging to FrameNet-CNL, if information extraction parser produces
the correct knowledge representation as a result. We describe a
state-of-the-art information extraction parser used by a national news agency
and speculate that FrameNet-CNL eventually could shape the natural language
subset used for writing the newswire articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2539</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2539</id><created>2014-06-10</created><authors><author><keyname>de Franca</keyname><forenames>Fabricio Olivetti</forenames></author></authors><title>Maximizing Diversity for Multimodal Optimization</title><categories>cs.NE</categories><comments>submitted to PPSN'14 Workshop Advances in Multimodal Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most multimodal optimization algorithms use the so called \textit{niching
methods}~\cite{mahfoud1995niching} in order to promote diversity during
optimization, while others, like \textit{Artificial Immune
Systems}~\cite{de2010conceptual} try to find multiple solutions as its main
objective. One of such algorithms, called
\textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance that
measures the distance between two solutions regarding their basis of
attraction. In this short abstract I propose the use of the Line Distance
measure as the main objective-function in order to locate multiple optima at
once in a population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2541</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2541</id><created>2014-06-10</created><authors><author><keyname>Hern&#xe1;ndez-Lobato</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew W.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Predictive Entropy Search for Efficient Global Optimization of Black-box
  Functions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel information-theoretic approach for Bayesian optimization
called Predictive Entropy Search (PES). At each iteration, PES selects the next
evaluation point that maximizes the expected information gained with respect to
the global maximum. PES codifies this intractable acquisition function in terms
of the expected reduction in the differential entropy of the predictive
distribution. This reformulation allows PES to obtain approximations that are
both more accurate and efficient than other alternatives such as Entropy Search
(ES). Furthermore, PES can easily perform a fully Bayesian treatment of the
model hyperparameters while ES cannot. We evaluate PES in both synthetic and
real-world applications, including optimization problems in machine learning,
finance, biotechnology, and robotics. We show that the increased accuracy of
PES leads to significant gains in optimization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2544</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2544</id><created>2014-06-10</created><authors><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Najvirt</keyname><forenames>Robert</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>Faithful Glitch Propagation in Binary Circuit Models</title><categories>cs.OH</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern digital circuit design relies on fast digital timing simulation tools
and, hence, on accurate binary-valued circuit models that faithfully model
signal propagation, even throughout a complex design. Unfortunately, it was
recently proved [F\&quot;ugger et al., ASYNC'13] that no existing binary-valued
circuit model proposed so far, including the two most commonly used pure and
inertial delay channels, faithfully captures glitch propagation: For the simple
Short-Pulse Filtration (SPF) problem, which is related to a circuit's ability
to suppress a single glitch, we showed that the quite broad class of bounded
single-history channels either contradict the unsolvability of SPF in bounded
time or the solvability of SPF in unbounded time in physical circuits.
  In this paper, we propose a class of binary circuit models that do not suffer
from this deficiency: Like bounded single-history channels, our involution
channels involve delays that may depend on the time of the previous output
transition. Their characteristic property are delay functions which are based
on involutions, i.e., functions that form their own inverse. A concrete example
of such a delay function, which is derived from a generalized first-order
analog circuit model, reveals that this is not an unrealistic assumption. We
prove that, in sharp contrast to what is possible with bounded single-history
channels, SPF cannot be solved in bounded time due to the nonexistence of a
lower bound on the delay of involution channels, whereas it is easy to provide
an unbounded SPF implementation. It hence follows that binary-valued circuit
models based on involution channels allow to solve SPF precisely when this is
possible in physical circuits. To the best of our knowledge, our model is hence
the very first candidate for a model that indeed guarantees faithful glitch
propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2545</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2545</id><created>2014-06-10</created><authors><author><keyname>de Franca</keyname><forenames>Fabricio Olivetti</forenames></author><author><keyname>Coelho</keyname><forenames>Guilherme Palermo</forenames></author></authors><title>A Flexible Fitness Function for Community Detection in Complex Networks</title><categories>cs.NE cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most community detection algorithms from the literature work as optimization
tools that minimize a given \textit{fitness function}, while assuming that each
node belongs to a single community. Since there is no hard concept of what a
community is, most proposed fitness functions focus on a particular definition.
As such, these functions do not always lead to partitions that correspond to
those observed in practice. This paper proposes a new flexible fitness function
that allows the identification of communities with distinct characteristics.
Such flexibility was evaluated through the adoption of an immune-inspired
optimization algorithm, named cob-aiNet[C], to identify both disjoint and
overlapping communities in a set of benchmark networks. The results have shown
that the obtained partitions are much closer to the ground-truth than those
obtained by the optimization of the modularity function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2565</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2565</id><created>2014-06-10</created><authors><author><keyname>Arghira</keyname><forenames>N.</forenames></author><author><keyname>Fagarasan</keyname><forenames>I.</forenames></author><author><keyname>Stamatescu</keyname><forenames>G.</forenames></author><author><keyname>Iliescu</keyname><forenames>S. St.</forenames></author><author><keyname>Stamatescu</keyname><forenames>I.</forenames></author><author><keyname>Calofir</keyname><forenames>V.</forenames></author></authors><title>Some Aspects concerning the Cyber-Physical Systems Approach in Power
  Systems</title><categories>cs.SY</categories><comments>5 pages, 2 figures, 3rd International Workshop on Cyber-Physical
  Systems (IWoCPS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper carries out a review of the main functional aspects of an
electro-energetic system, principles which lead to an evolution or even to a
paradigm change the the control of such complex systems. The repositioning of
the physical and information systems, mainly through the development of the
computing and communication distributed entities, lead to a new structuring of
the control approach in power systems, in accordance to the modern
Cyber-Physical Systems (CPS) paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2568</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2568</id><created>2014-05-22</created><updated>2015-05-26</updated><authors><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>C&#xe1;rdenas</keyname><forenames>Alvaro A.</forenames></author><author><keyname>Ratliff</keyname><forenames>Lillian J.</forenames></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Quantifying the Utility-Privacy Tradeoff in the Smart Grid</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modernization of the electrical grid and the installation of smart meters
come with many advantages to control and monitoring. However, in the wrong
hands, the data might pose a privacy threat. In this paper, we consider the
tradeoff between smart grid operations and the privacy of consumers. We analyze
the tradeoff between smart grid operations and how often data is collected by
considering a realistic direct-load control example using thermostatically
controlled loads, and we give simulation results to show how its performance
degrades as the sampling frequency decreases. Additionally, we introduce a new
privacy metric, which we call inferential privacy. This privacy metric assumes
a strong adversary model, and provides an upper bound on the adversary's
ability to infer a private parameter, independent of the algorithm he uses.
Combining these two results allow us to directly consider the tradeoff between
better load control and consumer privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2572</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2572</id><created>2014-06-10</created><authors><author><keyname>Dauphin</keyname><forenames>Yann</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Identifying and attacking the saddle point problem in high-dimensional
  non-convex optimization</title><categories>cs.LG math.OC stat.ML</categories><comments>The theoretical review and analysis in this article draw heavily from
  arXiv:1405.4604 [cs.LG]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central challenge to many fields of science and engineering involves
minimizing non-convex error functions over continuous, high dimensional spaces.
Gradient descent or quasi-Newton methods are almost ubiquitously used to
perform such minimizations, and it is often thought that a main source of
difficulty for these local methods to find the global minimum is the
proliferation of local minima with much higher error than the global minimum.
Here we argue, based on results from statistical physics, random matrix theory,
neural network theory, and empirical evidence, that a deeper and more profound
difficulty originates from the proliferation of saddle points, not local
minima, especially in high dimensional problems of practical interest. Such
saddle points are surrounded by high error plateaus that can dramatically slow
down learning, and give the illusory impression of the existence of a local
minimum. Motivated by these arguments, we propose a new approach to
second-order optimization, the saddle-free Newton method, that can rapidly
escape high dimensional saddle points, unlike gradient descent and quasi-Newton
methods. We apply this algorithm to deep or recurrent neural network training,
and provide numerical evidence for its superior optimization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2580</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2580</id><created>2014-06-10</created><authors><author><keyname>Apriyanti</keyname><forenames>D. H.</forenames></author><author><keyname>Arymurthy</keyname><forenames>A. A.</forenames></author><author><keyname>Handoko</keyname><forenames>L. T.</forenames></author></authors><title>Identification of Orchid Species Using Content-Based Flower Image
  Retrieval</title><categories>cs.CV cs.IR cs.LG</categories><comments>Proceeding of International Conference on Computer, Control,
  Informatics and its Applications 2013, pp. 53-57</comments><report-no>KRPURWODADILIPI-13044</report-no><doi>10.1109/IC3INA.2013.6819148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we developed the system for recognizing the orchid species by
using the images of flower. We used MSRM (Maximal Similarity based on Region
Merging) method for segmenting the flower object from the background and
extracting the shape feature such as the distance from the edge to the centroid
point of the flower, aspect ratio, roundness, moment invariant, fractal
dimension and also extract color feature. We used HSV color feature with
ignoring the V value. To retrieve the image, we used Support Vector Machine
(SVM) method. Orchid is a unique flower. It has a part of flower called lip
(labellum) that distinguishes it from other flowers even from other types of
orchids. Thus, in this paper, we proposed to do feature extraction not only on
flower region but also on lip (labellum) region. The result shows that our
proposed method can increase the accuracy value of content based flower image
retrieval for orchid species up to $\pm$ 14%. The most dominant feature is
Centroid Contour Distance, Moment Invariant and HSV Color. The system accuracy
is 85,33% in validation phase and 79,33% in testing phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2582</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2582</id><created>2014-06-10</created><updated>2014-10-24</updated><authors><author><keyname>Schober</keyname><forenames>Michael</forenames></author><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Probabilistic ODE Solvers with Runge-Kutta Means</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>18 pages (9 page conference paper, plus supplements); appears in
  Advances in Neural Information Processing Systems (NIPS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runge-Kutta methods are the classic family of solvers for ordinary
differential equations (ODEs), and the basis for the state of the art. Like
most numerical methods, they return point estimates. We construct a family of
probabilistic numerical methods that instead return a Gauss-Markov process
defining a probability distribution over the ODE solution. In contrast to prior
work, we construct this family such that posterior means match the outputs of
the Runge-Kutta family exactly, thus inheriting their proven good properties.
Remaining degrees of freedom not identified by the match to Runge-Kutta are
chosen such that the posterior probability measure fits the observed structure
of the ODE. Our results shed light on the structure of Runge-Kutta solvers from
a new direction, provide a richer, probabilistic output, have low computational
cost, and raise new research questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2587</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2587</id><created>2014-06-10</created><updated>2015-11-05</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Rossmanith</keyname><forenames>Peter</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando Sanchez</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>Structural Sparsity of Complex Networks: Bounded Expansion in Random
  Models and Real-World Graphs</title><categories>cs.SI cs.DM cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research aims to identify strong structural features of real-world
complex networks, sufficient to enable a host of graph algorithms that are much
more efficient than what is possible for general graphs (and currently used for
network analysis). Specifically, we study the property of bounded expansion.
This is the strongest formalization of the well-observed notion of &quot;sparsity&quot;
that might possibly apply to real-world networks and allow many previously
intractable problems to become tractable. On the theoretical side, we analyze
many previously proposed models for random networks and characterize which ones
have bounded expansion. We show that, w.h.p., (1) graphs sampled with either
the Molloy-Reed configuration model (including a variation of the model which
achieves high clustering) or the Chung-Lu model with a prescribed sparse degree
sequence (including heavy-tailed degree distributions); (2) perturbed
bounded-degree graphs; (3) stochastic block models with small probabilities;
result in graphs of bounded expansion. We also prove that the Kleinberg model
and the Barabasi-Albert model, in typical setups, contain large
one-subdivisions of cliques and thus do not result in graphs of bounded
expansion. On the practical side, we give experimental evidence that many
complex networks have bounded expansion, by measuring the closely related &quot;low
treedepth coloring number&quot; on a corpus of real-world data. On the algorithmic
side, we show how tools provided by the bounded expansion framework can be used
to efficiently solve the following common network analysis problems: for a
fixed graph H, we obtain the fastest-known algorithm for counting the number of
induced H-subgraphs and the number of H-homomorphisms; and we design linear
algorithms for computing several centrality measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2590</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2590</id><created>2014-06-10</created><updated>2014-07-28</updated><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Halfon</keyname><forenames>Simon</forenames></author></authors><title>Integer Vector Addition Systems with States</title><categories>cs.FL</categories><comments>17 pages, 2 figures</comments><acm-class>F.1.1</acm-class><doi>10.1007/978-3-319-11439-2_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies reachability, coverability and inclusion problems for
Integer Vector Addition Systems with States (ZVASS) and extensions and
restrictions thereof. A ZVASS comprises a finite-state controller with a finite
number of counters ranging over the integers. Although it is folklore that
reachability in ZVASS is NP-complete, it turns out that despite their
naturalness, from a complexity point of view this class has received little
attention in the literature. We fill this gap by providing an in-depth analysis
of the computational complexity of the aforementioned decision problems. Most
interestingly, it turns out that while the addition of reset operations to
ordinary VASS leads to undecidability and Ackermann-hardness of reachability
and coverability, respectively, they can be added to ZVASS while retaining
NP-completness of both coverability and reachability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2602</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2602</id><created>2014-06-10</created><authors><author><keyname>Fetaya</keyname><forenames>Ethan</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Ullman</keyname><forenames>Shimon</forenames></author></authors><title>Graph Approximation and Clustering on a Budget</title><categories>stat.ML cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning from a similarity matrix (such as
spectral clustering and lowd imensional embedding), when computing pairwise
similarities are costly, and only a limited number of entries can be observed.
We provide a theoretical analysis using standard notions of graph
approximation, significantly generalizing previous results (which focused on
spectral clustering with two clusters). We also propose a new algorithmic
approach based on adaptive sampling, which experimentally matches or improves
on previous methods, while being considerably more general and computationally
cheaper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2613</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2613</id><created>2014-06-07</created><authors><author><keyname>Ansari</keyname><forenames>Shahab U.</forenames></author><author><keyname>Mansha</keyname><forenames>Sameen</forenames></author></authors><title>Simulation based Hardness Evaluation of a Multi-Objective Genetic
  Algorithm</title><categories>cs.NE</categories><comments>International Conference on Modeling &amp; Simulation, November, 25-27,
  2013, Islamabad</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies have shown that multi-objective optimization problems are hard
problems. Such problems either require longer time to converge to an optimum
solution, or may not converge at all. Recently some researchers have claimed
that real culprit for increasing the hardness of multi-objective problems are
not the number of objectives themselves rather it is the increased size of
solution set, incompatibility of solutions, and high probability of finding
suboptimal solution due to increased number of local maxima. In this work, we
have setup a simple framework for the evaluation of hardness of multi-objective
genetic algorithms (MOGA). The algorithm is designed for a pray-predator game
where a player is to improve its lifespan, challenging level and usability of
the game arena through number of generations. A rigorous set of experiments are
performed for quantifying the hardness in terms of evolution for increasing
number of objective functions. In genetic algorithm, crossover and mutation
with equal probability are applied to create offspring in each generation.
First, each objective function is maximized individually by ranking the
competing players on the basis of the fitness (cost) function, and then a
multi-objective cost function (sum of individual cost functions) is maximized
with ranking, and also without ranking where dominated solutions are also
allowed to evolve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2614</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2614</id><created>2014-06-07</created><updated>2015-02-26</updated><authors><author><keyname>Kalsoom</keyname><forenames>Rizwana</forenames></author><author><keyname>Qureshi</keyname><forenames>Moomal</forenames></author></authors><title>Application and Verification of Algorithm Learning Based Neural Network</title><categories>cs.NE</categories><comments>This paper has been withdrawn by the author due to a crucial accuracy
  error in Fig. 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial accuracy error
in Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work
in the following article. Yoon, H., Park, C. S., Kim, J. S., &amp; Baek, J. G.
(2013). Algorithm learning based neural network integrating feature selection
and classification. Expert Systems with Applications, 40(1), 231-241.
http://www.sciencedirect.com/science/article/pii/S0957417412008731
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2616</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2616</id><created>2014-06-10</created><updated>2016-01-05</updated><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Das</keyname><forenames>Debarghya</forenames></author><author><keyname>Gupta</keyname><forenames>Jayesh K</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large
  Scale Preference Feedback</title><categories>cs.RO cs.AI cs.LG</categories><comments>PlanIt Camera Ready ICRA'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning user preferences over robot trajectories
for environments rich in objects and humans. This is challenging because the
criterion defining a good trajectory varies with users, tasks and interactions
in the environment. We represent trajectory preferences using a cost function
that the robot learns and uses it to generate good trajectories in new
environments. We design a crowdsourcing system - PlanIt, where non-expert users
label segments of the robot's trajectory. PlanIt allows us to collect a large
amount of user feedback, and using the weak and noisy labels from PlanIt we
learn the parameters of our model. We test our approach on 122 different
environments for robotic navigation and manipulation tasks. Our extensive
experiments show that the learned cost function generates preferred
trajectories in human environments. Our crowdsourcing system is publicly
available for the visualization of the learned costs and for providing
preference feedback: \url{http://planit.cs.cornell.edu}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2622</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2622</id><created>2014-06-10</created><authors><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>CMLA</affiliation></author><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>LIF</affiliation></author></authors><title>Equivalence of Learning Algorithms</title><categories>cs.LG stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.2451</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to introduce a concept of equivalence between
machine learning algorithms. We define two notions of algorithmic equivalence,
namely, weak and strong equivalence. These notions are of paramount importance
for identifying when learning prop erties from one learning algorithm can be
transferred to another. Using regularized kernel machines as a case study, we
illustrate the importance of the introduced equivalence concept by analyzing
the relation between kernel ridge regression (KRR) and m-power regularized
least squares regression (M-RLSR) algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2623</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2623</id><created>2014-06-10</created><updated>2014-06-11</updated><authors><author><keyname>Loshchilov</keyname><forenames>Ilya</forenames><affiliation>LIS</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames><affiliation>LRI</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES</title><categories>cs.NE cs.AI</categories><comments>13th International Conference on Parallel Problem Solving from Nature
  (PPSN 2014) (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widely
accepted as a robust derivative-free continuous optimization algorithm for
non-linear and non-convex optimization problems. CMA-ES is well known to be
almost parameterless, meaning that only one hyper-parameter, the population
size, is proposed to be tuned by the user. In this paper, we propose a
principled approach called self-CMA-ES to achieve the online adaptation of
CMA-ES hyper-parameters in order to improve its overall performance.
Experimental results show that for larger-than-default population size, the
default settings of hyper-parameters of CMA-ES are far from being optimal, and
that self-CMA-ES allows for dynamically approaching optimal settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2628</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2628</id><created>2014-06-10</created><updated>2014-06-20</updated><authors><author><keyname>Green</keyname><forenames>Oded</forenames></author><author><keyname>Odeh</keyname><forenames>Saher</forenames></author><author><keyname>Birk</keyname><forenames>Yitzhak</forenames></author></authors><title>Merge Path - A Visually Intuitive Approach to Parallel Merging</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merging two sorted arrays is a prominent building block for sorting and other
functions. Its efficient parallelization requires balancing the load among
compute cores, minimizing the extra work brought about by parallelization, and
minimizing inter-thread synchronization requirements. Efficient use of memory
is also important.
  We present a novel, visually intuitive approach to partitioning two input
sorted arrays into pairs of contiguous sequences of elements, one from each
array, such that 1) each pair comprises any desired total number of elements,
and 2) the elements of each pair form a contiguous sequence in the output
merged sorted array. While the resulting partition and the computational
complexity are similar to those of certain previous algorithms, our approach is
different, extremely intuitive, and offers interesting insights. Based on this,
we present a synchronization-free, cache-efficient merging (and sorting)
algorithm.
  While we use a shared memory architecture as the basis, our algorithm is
easily adaptable to additional architectures. In fact, our approach is even
relevant to cache-efficient sequential sorting. The algorithms are presented,
along with important cache-related insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2630</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2630</id><created>2014-06-09</created><authors><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>A Utility Proportional Fairness Radio Resource Block Allocation in
  Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a radio resource block allocation optimization problem
for cellular communications systems with users running delay-tolerant and
real-time applications, generating elastic and inelastic traffic on the network
and being modelled as logarithmic and sigmoidal utilities respectively. The
optimization is cast under a utility proportional fairness framework aiming at
maximizing the cellular systems utility whilst allocating users the resource
blocks with an eye on application quality of service requirements and on the
procedural temporal and computational efficiency. Ultimately, the sensitivity
of the proposed modus operandi to the resource variations is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2631</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2631</id><created>2014-06-09</created><authors><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>A Utility Proportional Fairness Resource Allocation in Spectrally
  Radar-Coexistent Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sharing is an elegant solution to addressing the scarcity of the
bandwidth for wireless communications systems. This research studies the
feasibility of sharing the spectrum between sectorized cellular systems and
stationary radars interfering with certain sectors of the communications
infrastructure. It also explores allocating optimal resources to mobile devices
in order to provide with the quality of service for all running applications
whilst growing the communications network spectrally coexistent with the radar
systems. The rate allocation problem is formulated as two convex optimizations,
where the radar-interfering sector assignments are extracted from the portion
of the spectrum non-overlapping with the radar operating frequency. Such a
double-stage resource allocation procedure inherits the fairness into the rate
allocation scheme by first assigning the spectrally radar-overlapping
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2636</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2636</id><created>2014-06-10</created><authors><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author></authors><title>Intersection graphs of segments and $\exists\mathbb{R}$</title><categories>cs.CG math.CO</categories><comments>36 pages, expository paper</comments><msc-class>68R10, 68Q17, 05C62, 52C30, 52C45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ with vertex set $\{v_1,v_2,\ldots,v_n\}$ is an intersection graph
of segments if there are segments $s_1,\ldots,s_n$ in the plane such that $s_i$
and $s_j$ have a common point if and only if $\{v_i,v_j\}$ is an edge of~$G$.
In this expository paper, we consider the algorithmic problem of testing
whether a given abstract graph is an intersection graph of segments.
  It turned out that this problem is complete for an interesting recently
introduced class of computational problems, denoted by $\exists\mathbb{R}$.
This class consists of problems that can be reduced, in polynomial time, to
solvability of a system of polynomial inequalities in several variables over
the reals. We discuss some subtleties in the definition of $\exists\mathbb{R}$,
and we provide a complete and streamlined account of a proof of the
$\exists\mathbb{R}$-completeness of the recognition problem for segment
intersection graphs. Along the way, we establish
$\exists\mathbb{R}$-completeness of several other problems. We also present a
decision algorithm, due to Muchnik, for the first-order theory of the reals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2639</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2639</id><created>2014-06-06</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Cherry</keyname><forenames>Kevin M.</forenames></author><author><keyname>Hoffman</keyname><forenames>Joanne</forenames></author><author><keyname>Wang</keyname><forenames>Shijun</forenames></author><author><keyname>Liu</keyname><forenames>Jiamin</forenames></author><author><keyname>Turkbey</keyname><forenames>Evrim</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>A New 2.5D Representation for Lymph Node Detection using Random Sets of
  Deep Convolutional Neural Network Observations</title><categories>cs.CV cs.LG cs.NE</categories><comments>This article will be presented at MICCAI (Medical Image Computing and
  Computer-Assisted Interventions) 2014</comments><journal-ref>Medical Image Computing and Computer-Assisted Intervention -
  MICCAI 2014 Volume 8673 of the series Lecture Notes in Computer Science pp
  520-527</journal-ref><doi>10.1007/978-3-319-10404-1_65</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automated Lymph Node (LN) detection is an important clinical diagnostic task
but very challenging due to the low contrast of surrounding structures in
Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely
distributed locations. State-of-the-art studies show the performance range of
52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1
FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this
paper, we first operate a preliminary candidate generation stage, towards 100%
sensitivity at the cost of high FP levels (40 per patient), to harvest volumes
of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by
resampling 2D reformatted orthogonal views N times, via scale, random
translations, and rotations with respect to the VOI centroid coordinates. These
random views are then used to train a deep Convolutional Neural Network (CNN)
classifier. In testing, the CNN is employed to assign LN probabilities for all
N random views that can be simply averaged (as a set) to compute the final
classification probability per VOI. We validate the approach on two datasets:
90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs.
We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in
mediastinum and abdomen respectively, which drastically improves over the
previous state-of-the-art work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2644</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2644</id><created>2014-06-10</created><authors><author><keyname>Elouafiq</keyname><forenames>Ali</forenames></author><author><keyname>Abid</keyname><forenames>Redouan</forenames></author></authors><title>Geographical Asynchronous Information Access for Distributed Systems</title><categories>cs.DC cs.DB</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Non-relational databases are the common means of data storage in the Cloud,
and optimizing the data access is of paramount importance into determining the
overall Cloud system performance. In this paper, we present GAIA, a novel model
for retrieving and managing correlated geo-localized data in the cloud
environment. We survey and compare the existing models used mostly in
Geographical Information Systems (GIS), mainly the Grid model and the
Coordinates Projection model. Besides, we present a benchmark comparing the
efficiency of the models. Using extensive experimentation, we show that GAIA
outperforms the existing models by its high efficiency which is of O(log(n)),
and this mainly thanks to its combination of projection with cell
decomposition. The other models have a linear efficiency of O(n). The presented
model is designed from the ground up to support GIS and is designed to suit
both cloud and parallel computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2646</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2646</id><created>2014-06-10</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J</forenames></author><author><keyname>Kreuzer</keyname><forenames>Martin</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Learning with Cross-Kernels and Ideal PCA</title><categories>cs.LG math.AC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how cross-kernel matrices, that is, kernel matrices between the
data and a custom chosen set of `feature spanning points' can be used for
learning. The main potential of cross-kernels lies in the fact that (a) only
one side of the matrix scales with the number of data points, and (b)
cross-kernels, as opposed to the usual kernel matrices, can be used to certify
for the data manifold. Our theoretical framework, which is based on a duality
involving the feature space and vanishing ideals, indicates that cross-kernels
have the potential to be used for any kind of kernel learning. We present a
novel algorithm, Ideal PCA (IPCA), which cross-kernelizes PCA. We demonstrate
on real and synthetic data that IPCA allows to (a) obtain PCA-like features
faster and (b) to extract novel and empirically validated features certifying
for the data manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2648</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2648</id><created>2014-06-09</created><updated>2014-06-11</updated><authors><author><keyname>Wang</keyname><forenames>Huan</forenames></author><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author></authors><title>Fast Matrix Multiplication with Sketching</title><categories>cs.NA cs.DS math.NA</categories><comments>Theorem 1 may be problematic, and more careful thought is required.
  The authors are discussing a solution on it. Currently it is better to
  withdraw the draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approximate algorithm for matrix multiplication based on matrix
sketching techniques. First one of the matrix is chosen and sparsified using
the online matrix sketching algorithm, and then the matrix product is
calculated using the sparsified matrix. We prove when the sample number grows
large compared to the sample dimensions the proposed algorithm achieves similar
accuracy bound with a smaller computational cost compared to the
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2661</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2661</id><created>2014-06-10</created><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Pouget-Abadie</keyname><forenames>Jean</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Ozair</keyname><forenames>Sherjil</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Generative Adversarial Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a generative
model G that captures the data distribution, and a discriminative model D that
estimates the probability that a sample came from the training data rather than
G. The training procedure for G is to maximize the probability of D making a
mistake. This framework corresponds to a minimax two-player game. In the space
of arbitrary functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1/2 everywhere. In the case where G
and D are defined by multilayer perceptrons, the entire system can be trained
with backpropagation. There is no need for any Markov chains or unrolled
approximate inference networks during either training or generation of samples.
Experiments demonstrate the potential of the framework through qualitative and
quantitative evaluation of the generated samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2671</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2671</id><created>2014-06-10</created><authors><author><keyname>Jaeger</keyname><forenames>Herbert</forenames></author></authors><title>Conceptors: an easy introduction</title><categories>cs.NE</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptors provide an elementary neuro-computational mechanism which sheds a
fresh and unifying light on a diversity of cognitive phenomena. A number of
demanding learning and processing tasks can be solved with unprecedented ease,
robustness and accuracy. Some of these tasks were impossible to solve before.
This entirely informal paper introduces the basic principles of conceptors and
highlights some of their usages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2673</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2673</id><created>2014-06-10</created><updated>2015-02-16</updated><authors><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Mondrian Forests: Efficient Online Random Forests</title><categories>stat.ML cs.LG</categories><journal-ref>Advances in Neural Information Processing Systems 27 (NIPS), pages
  3140-3148, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensembles of randomized decision trees, usually referred to as random
forests, are widely used for classification and regression tasks in machine
learning and statistics. Random forests achieve competitive predictive
performance and are computationally efficient to train and test, making them
excellent candidates for real-world prediction tasks. The most popular random
forest variants (such as Breiman's random forest and extremely randomized
trees) operate on batches of training data. Online methods are now in greater
demand. Existing online random forests, however, require more training data
than their batch counterpart to achieve comparable predictive performance. In
this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles
of random decision trees we call Mondrian forests. Mondrian forests can be
grown in an incremental/online fashion and remarkably, the distribution of
online Mondrian forests is the same as that of batch Mondrian forests. Mondrian
forests achieve competitive predictive performance comparable with existing
online random forests and periodically re-trained batch random forests, while
being more than an order of magnitude faster, thus representing a better
computation vs accuracy tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2685</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2685</id><created>2014-06-10</created><authors><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author><author><keyname>Lu</keyname><forenames>Yang</forenames></author></authors><title>Non-Markovian Character in Human Mobility: Online and Offline</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 6figures</comments><journal-ref>Chaos 25, 063106 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of human mobility characterizes the trajectories humans follow
during their daily activities and is the foundation of processes from epidemic
spreading to traffic prediction and information recommendation. In this paper,
we investigate a massive data set of human activity including both online
behavior of browsing websites and offline one of visiting towers based mobile
terminations. The non-Markovian character observed from both online and offline
cases is suggested by the scaling law in the distribution of dwelling time at
individual and collective levels, respectively. Furthermore, we argue that the
lower entropy and higher predictability in human mobility for both online and
offline cases may origin from this non-Markovian character. However, the
distributions of individual entropy and predictability show the different
degrees of non-Markovian character from online to offline cases. To accounting
for non-Markovian character in human mobility, we introduce a protype model
with three basic ingredients, \emph{preferential return, inertial effect, and
exploration} to reproduce the dynamic process of online and offline human
mobility. In comparison with standard and biased random walk models with
assumption of Markov process, the proposed model is able to obtain characters
much closer to these empirical observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2710</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2710</id><created>2014-06-10</created><authors><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>A Multiplicative Model for Learning Distributed Text-Based Attribute
  Representations</title><categories>cs.LG cs.CL</categories><comments>11 pages. An earlier version was accepted to the ICML-2014 Workshop
  on Knowledge-Powered Deep Learning for Text Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a general framework for learning distributed
representations of attributes: characteristics of text whose representations
can be jointly learned with word embeddings. Attributes can correspond to
document indicators (to learn sentence vectors), language indicators (to learn
distributed language representations), meta-data and side information (such as
the age, gender and industry of a blogger) or representations of authors. We
describe a third-order model where word context and attribute vectors interact
multiplicatively to predict the next word in a sequence. This leads to the
notion of conditional word similarity: how meanings of words change when
conditioned on different attributes. We perform several experimental tasks
including sentiment classification, cross-lingual document classification, and
blog authorship attribution. We also qualitatively evaluate conditional word
neighbours and attribute-conditioned text generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2720</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2720</id><created>2014-06-10</created><authors><author><keyname>Marriott</keyname><forenames>Chris</forenames></author><author><keyname>Chebib</keyname><forenames>Jobran</forenames></author></authors><title>The Effect of Social Learning on Individual Learning and Evolution</title><categories>cs.AI q-bio.PE</categories><comments>Accepted to ALIFE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effects of social learning on the individual learning and
genetic evolution of a colony of artificial agents capable of genetic,
individual and social modes of adaptation. We confirm that there is strong
selection pressure to acquire traits of individual learning and social learning
when these are adaptive traits. We show that selection pressure for learning of
either kind can supress selection pressure for reproduction or greater fitness.
We show that social learning differs from individual learning in that it can
support a second evolutionary system that is decoupled from the biological
evolutionary system. This decoupling leads to an emergent interaction where
immature agents are more likely to engage in learning activities than mature
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2721</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2721</id><created>2014-06-10</created><authors><author><keyname>Meng</keyname><forenames>Zhaoshi</forenames></author><author><keyname>Eriksson</keyname><forenames>Brian</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Learning Latent Variable Gaussian Graphical Models</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>To appear in The 31st International Conference on Machine Learning
  (ICML 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian graphical models (GGM) have been widely used in many
high-dimensional applications ranging from biological and financial data to
recommender systems. Sparsity in GGM plays a central role both statistically
and computationally. Unfortunately, real-world data often does not fit well to
sparse graphical models. In this paper, we focus on a family of latent variable
Gaussian graphical models (LVGGM), where the model is conditionally sparse
given latent variables, but marginally non-sparse. In LVGGM, the inverse
covariance matrix has a low-rank plus sparse structure, and can be learned in a
regularized maximum likelihood framework. We derive novel parameter estimation
error bounds for LVGGM under mild conditions in the high-dimensional setting.
These results complement the existing theory on the structural learning, and
open up new possibilities of using LVGGM for statistical inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2726</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2726</id><created>2014-06-10</created><updated>2015-08-22</updated><authors><author><keyname>Ruiz-Vargas</keyname><forenames>Andres J.</forenames></author><author><keyname>Suk</keyname><forenames>Andrew</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Disjoint edges in topological graphs and the tangled-thrackle conjecture</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that for a constant $t\in \mathbb{N}$, every simple topological
graph on $n$ vertices has $O(n)$ edges if it has no two sets of $t$ edges such
that every edge in one set is disjoint from all edges of the other set (i.e.,
the complement of the intersection graph of the edges is $K_{t,t}$-free). As an
application, we settle the \emph{tangled-thrackle} conjecture formulated by
Pach, Radoi\v{c}i\'c, and T\'oth: Every $n$-vertex graph drawn in the plane
such that every pair of edges have precisely one point in common, where this
point is either a common endpoint, a crossing, or a point of tangency, has at
most $O(n)$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2728</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2728</id><created>2014-06-10</created><updated>2014-07-21</updated><authors><author><keyname>Furma&#x144;czyk</keyname><forenames>Hanna</forenames></author><author><keyname>Kubale</keyname><forenames>Marek</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanis&#x142;aw</forenames></author></authors><title>On bipartization of cubic graphs by removal of an independent set</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new problem for cubic graphs: bipartization of a cubic graph $Q$
by deleting sufficiently large independent set $I$. It can be expressed as
follows: \emph{Given a connected $n$-vertex tripartite cubic graph $Q=(V,E)$
with independence number $\alpha(Q)$, does $Q$ contain an independent set $I$
of size $k$ such that $Q-I$ is bipartite?} We are interested for which value of
$k$ the answer to this question is affirmative. We prove constructively that if
$\alpha(Q) \geq 4n/10$, then the answer is positive for each $k$ fulfilling
$\lfloor (n-\alpha(Q))/2 \rfloor \leq k \leq \alpha(Q)$. It remains an open
question if a similar construction is possible for cubic graphs with
$\alpha(Q)&lt;4n/10$.
  Next, we show that this problem with $\alpha(Q)\geq 4n/10$ and $k$ fulfilling
inequalities $\lfloor n/3 \rfloor \leq k \leq \alpha(Q)$ can be related to
semi-equitable graph 3-coloring, where one color class is of size $k$, and the
subgraph induced by the remaining vertices is equitably 2-colored. This means
that $Q$ has a coloring of type $(k, \lceil(n-k)/2\rceil, \lfloor (n-k)/2
\rfloor)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2732</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2732</id><created>2014-06-10</created><authors><author><keyname>Papandreou</keyname><forenames>George</forenames></author></authors><title>Deep Epitomic Convolutional Neural Networks</title><categories>cs.CV cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have recently proven extremely competitive
in challenging image recognition tasks. This paper proposes the epitomic
convolution as a new building block for deep neural networks. An epitomic
convolution layer replaces a pair of consecutive convolution and max-pooling
layers found in standard deep convolutional neural networks. The main version
of the proposed model uses mini-epitomes in place of filters and computes
responses invariant to small translations by epitomic search instead of
max-pooling over image positions. The topographic version of the proposed model
uses large epitomes to learn filter maps organized in translational
topographies. We show that error back-propagation can successfully learn
multiple epitomic layers in a supervised fashion. The effectiveness of the
proposed method is assessed in image classification tasks on standard
benchmarks. Our experiments on Imagenet indicate improved recognition
performance compared to standard convolutional neural networks of similar
architecture. Our models pre-trained on Imagenet perform excellently on
Caltech-101. We also obtain competitive image classification results on the
small-image MNIST and CIFAR-10 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2738</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2738</id><created>2014-06-10</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Wireless Backhaul Networks: Capacity Bound, Scalability Analysis and
  Design Guidelines</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the scalability of a wireless backhaul network modeled as
a random extended network with multi-antenna base stations (BSs), where the
number of antennas per BS is allowed to scale as a function of the network
size. The antenna scaling is justified by the current trend towards the use of
higher carrier frequencies, which allows to pack large number of antennas in
small form factors. The main goal is to study the per-BS antenna requirement
that ensures scalability of this network, i.e., its ability to deliver
non-vanishing rate to each source-destination pair. We first derive an
information theoretic upper bound on the capacity of this network under a
general propagation model, which provides a lower bound on the per-BS antenna
requirement. Then, we characterize the scalability requirements for two
competing strategies of interest: (i) long hop: each source-destination pair
minimizes the number of hops by sacrificing multiplexing gain while achieving
full beamforming (power) gain over each hop, and (ii) short hop: each
source-destination pair communicates through a series of short hops, each
achieving full multiplexing gain. While long hop may seem more intuitive in the
context of massive multiple-input multiple-output (MIMO) transmission, we show
that the short hop strategy is significantly more efficient in terms of per-BS
antenna requirement for throughput scalability. As a part of the proof, we
construct a scalable short hop strategy and show that it does not violate any
fundamental limits on the spatial degrees of freedom (DoFs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2741</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2741</id><created>2014-06-10</created><authors><author><keyname>Cai</keyname><forenames>Jun</forenames></author><author><keyname>Macready</keyname><forenames>William G.</forenames></author><author><keyname>Roy</keyname><forenames>Aidan</forenames></author></authors><title>A practical heuristic for finding graph minors</title><categories>quant-ph cs.DS math.CO</categories><comments>16 pages, 7 figures</comments><msc-class>05C83, 81P68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a heuristic algorithm for finding a graph $H$ as a minor of a
graph $G$ that is practical for sparse $G$ and $H$ with hundreds of vertices.
We also explain the practical importance of finding graph minors in mapping
quadratic pseudo-boolean optimization problems onto an adiabatic quantum
annealer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2744</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2744</id><created>2014-06-10</created><authors><author><keyname>Davies</keyname><forenames>Krispin A.</forenames></author><author><keyname>Ramirez-Serrano</keyname><forenames>Alejandro</forenames></author><author><keyname>Wilson</keyname><forenames>Graeme N.</forenames></author><author><keyname>Mustafa</keyname><forenames>Mahmoud</forenames></author></authors><title>Rapid Control Selection through Hill-Climbing Methods</title><categories>cs.SY</categories><comments>5th International Conference on Intelligent Robotics and Applications
  (ICIRA 2012) Original text available at:
  http://link.springer.com/chapter/10.1007%2F978-3-642-33515-0_55</comments><journal-ref>5th International Conference, ICIRA 2012, Montreal, Canada,
  October 3-5, 2012, Proceedings, Part II, pp. 561-570, Oct. 2012</journal-ref><doi>10.1007/978-3-642-33515-0_55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of control selection in complex dynamical and
environmental scenarios where model predictive control (MPC) proves
particularly effective. As the performance of MPC is highly dependent on the
efficiency of its incorporated search algorithm, this work examined hill
climbing as an alternative to traditional systematic or random search
algorithms. The relative performance of a candidate hill climbing algorithm was
compared to representative systematic and random algorithms in a set of
systematic tests and in a real-world control scenario. These tests indicated
that hill climbing can provide significantly improved search efficiency when
the control space has a large number of dimensions or divisions along each
dimension. Furthermore, this demonstrated that there was little increase in
search times associated with a significant increase in the number of control
configurations considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2746</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2746</id><created>2014-06-10</created><updated>2014-09-08</updated><authors><author><keyname>Almishari</keyname><forenames>Mishari</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Oguz</keyname><forenames>Ekin</forenames></author></authors><title>Are 140 Characters Enough? A Large-Scale Linkability Study of Tweets</title><categories>cs.IR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microblogging is a very popular Internet activity that informs and entertains
great multitudes of people world-wide via quickly and scalably disseminated
terse messages containing all kinds of newsworthy utterances. Even though
microblogging is neither designed nor meant to emphasize privacy, numerous
contributors hide behind pseudonyms and compartmentalize their different
incarnations via multiple accounts within the same, or across multiple,
site(s). Prior work has shown that stylometric analysis is a very powerful tool
capable of linking product or service reviews and blogs that are produced by
the same author when the number of authors is large. In this paper, we explore
linkability of tweets. Our results, based on a very large corpus of tweets,
clearly demonstrate that, at least for relatively active tweeters, linkability
of tweets by the same author is easily attained even when the number of
tweeters is large. We also show that our linkability results hold for a set of
actual Twitter users who tweet from multiple accounts. This has some obvious
privacy implications, both positive and negative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2751</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2751</id><created>2014-06-10</created><updated>2015-04-16</updated><authors><author><keyname>Bornschein</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Reweighted Wake-Sleep</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training deep directed graphical models with many hidden variables and
performing inference remains a major challenge. Helmholtz machines and deep
belief networks are such models, and the wake-sleep algorithm has been proposed
to train them. The wake-sleep algorithm relies on training not just the
directed generative model but also a conditional generative model (the
inference network) that runs backward from visible to latent, estimating the
posterior distribution of latent given visible. We propose a novel
interpretation of the wake-sleep algorithm which suggests that better
estimators of the gradient can be obtained by sampling latent variables
multiple times from the inference network. This view is based on importance
sampling as an estimator of the likelihood, with the approximate inference
network as a proposal distribution. This interpretation is confirmed
experimentally, showing that better likelihood can be achieved with this
reweighted wake-sleep procedure. Based on this interpretation, we propose that
a sigmoidal belief network is not sufficiently powerful for the layers of the
inference network in order to recover a good estimator of the posterior
distribution of latent variables. Our experiments show that using a more
powerful layer model, such as NADE, yields substantially better generative
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2752</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2752</id><created>2014-06-10</created><updated>2015-03-27</updated><authors><author><keyname>Sun</keyname><forenames>Hongguang</forenames></author><author><keyname>Wildemeersch</keyname><forenames>Matthias</forenames></author><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>D2D Enhanced Heterogeneous Cellular Networks with Dynamic TDD</title><categories>cs.NI</categories><comments>15 pages; 9 figures; submitted to IEEE Transactions on Wireless
  Communications</comments><doi>10.1109/TWC.2015.2418192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, the growing amount of UL and DL mobile data traffic has
been characterized by substantial asymmetry and time variations. Dynamic
time-division duplex (TDD) has the capability to accommodate to the traffic
asymmetry by adapting the UL/DL configuration to the current traffic demands.
In this work, we study a two-tier heterogeneous cellular network (HCN) where
the macro tier and small cell tier operate according to a dynamic TDD scheme on
orthogonal frequency bands. To offload the network infrastructure, mobile users
in proximity can engage in D2D communications, whose activity is determined by
a carrier sensing multiple access (CSMA) scheme to protect the ongoing
infrastructure-based and D2D transmissions. We present an analytical framework
to evaluate the network performance in terms of load-aware coverage probability
and network throughput. The proposed framework allows to quantify the effect on
the coverage probability of the most important TDD system parameters, such as
the UL/DL configuration, the base station density, and the bias factor. In
addition, we evaluate how the bandwidth partition and the D2D network access
scheme affect the total network throughput. Through the study of the tradeoff
between coverage probability and D2D user activity, we provide guidelines for
the optimal design of D2D network access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2758</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2758</id><created>2014-06-10</created><authors><author><keyname>Yang</keyname><forenames>Xuezhi</forenames></author></authors><title>A multi-level soft frequency reuse technique for wireless communication
  systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-level soft frequency reuse (ML-SFR) scheme and a resource allocation
methodology are proposed for wireless communication systems in this letter. In
the proposed ML-SFR scheme, there are 2N power density limit levels, achieving
better interference pattern and further improving the cell edge and overall
data rate, compared to the traditional 2-level SFR scheme. The detailed design
of an 8-level SFR scheme is demonstrated. Numeral results show that the cell
edge spectrum efficiency is increased to 5 times of that of reuse 1 and the
overall spectrum efficiency is improved by 31%. ML-SFR can be utilized in the
current 4G system and would be a candidate key technology for future 5G
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2773</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2773</id><created>2014-06-11</created><authors><author><keyname>Taruna</keyname><forenames>S</forenames></author><author><keyname>Singh</keyname><forenames>Pratibha</forenames></author><author><keyname>Joshi</keyname><forenames>Soshya</forenames></author></authors><title>Green Computing In Developed And Developing Countries</title><categories>cs.CY</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.3, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today e-waste are becoming a major problem for the developing countries.
E-waste is defined something as a discarded parts of electronic devices which
contains most of the times, hazardous chemicals which is deadly for our
environment, example is computer components. Green Computing is the study and
practice of designing, using, disposing and manufacturing electronic components
in an eco-friendly manner and Green Computing is one of the solution to tackle
with this hazardous e-waste problem which is an emerging concern towards the
environment. The objective of this paper is to draw the attention towards the
lack of awareness about green computing or we can say how green computing
policies is being ignored by developing countries and how developed countries
are adopting green IT policies seriously. This paper also discusses the
analysis which has been done on how the amount of e-waste has been increased in
developing countries in past years
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2775</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2775</id><created>2014-06-11</created><authors><author><keyname>Zhao</keyname><forenames>Jian</forenames></author><author><keyname>Liu</keyname><forenames>Hengzhu</forenames></author><author><keyname>Chen</keyname><forenames>Xucan</forenames></author><author><keyname>Liang</keyname><forenames>Zhengfa</forenames></author></authors><title>Realization and design of a pilot assist decision-making system based on
  speech recognition</title><categories>cs.HC cs.SD</categories><comments>10 pages, 8 figures</comments><msc-class>93B05</msc-class><doi>10.5121/csit.2014.4526</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A system based on speech recognition is proposed for pilot assist
decision-making. It is based on a HIL aircraft simulation platform and uses the
microcontroller SPCE061A as the central processor to achieve better reliability
and higher cost-effect performance. Technologies of LPCC (linear predictive
cepstral coding) and DTW (Dynamic Time Warping) are applied for isolated-word
speech recognition to gain a smaller amount of calculation and a better
real-time performance. Besides, we adopt the PWM (Pulse Width Modulation)
regulation technology to effectively regulate each control surface by speech,
and thus to assist the pilot to make decisions. By trial and error, it is
proved that we have a satisfactory accuracy rate of speech recognition and
control effect. More importantly, our paper provides a creative idea for
intelligent human-computer interaction and applications of speech recognition
in the field of aviation control. Our system is also very easy to be extended
and applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2777</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2777</id><created>2014-06-11</created><authors><author><keyname>Charan</keyname><forenames>A. Sai</forenames></author><author><keyname>Manasa</keyname><forenames>N. K.</forenames></author><author><keyname>Sarma</keyname><forenames>Prof. N. V. S. N.</forenames></author></authors><title>Out Performance Of Cuckoo Search Algorithm Among Nature Inspired
  Algorithms in Planar Antenna Arrays</title><categories>cs.ET cs.IT math.IT</categories><comments>10 PAGES, 9 FIGURES, WIMON-2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this modern era a great deal of metamorphism is observed around us which
eventuate due to some minute modifications and innovations in the area of
Science and Technology. This paper deals with the application of a meta
heuristic optimization algorithm namely the Cuckoo Search Algorithm in the
design of an optimized planar antenna array which ensures high
gain,directivity, suppression of side lobes, increased efficiency and improves
other antenna parameters as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2779</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2779</id><created>2014-06-11</created><authors><author><keyname>Aliwi</keyname><forenames>Hadeel Saleh Haj</forenames></author><author><keyname>Sumari</keyname><forenames>Putra</forenames></author></authors><title>real-time audio translation module between iax and rsw</title><categories>cs.MM</categories><comments>7 pages, 5 figures</comments><msc-class>90B18</msc-class><journal-ref>Hadeel Saleh Haj Aliwi and Putra Sumari (2014),&quot;REAL-TIME AUDIO
  TRANSLATION MODULE BETWEEN IAX AND RSW&quot;, International Journal of Computer
  Networks &amp; Communications (IJCNC) Vol.6, No.3, pp.125-133</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the last few years, multimedia communication has been developed and
improved rapidly in order to enable users to communicate between each other
over the internet. Generally, multimedia communication consists of audio and
video communication. However, this research concentrates on audio conferencing
only. The audio translation between protocols is a very critical issue, because
it solves the communication problems between any two protocols. So, it enables
people around the world to talk with each other even they use different
protocols. In this research, a real time audio translation module between two
protocols has been done. These two protocols are: InterAsterisk eXchange
Protocol (IAX) and Real-Time Switching Control Protocol (RSW), which they are
widely used to provide two ways audio transfer feature. The solution here is to
provide inter-working between the two protocols which they have different media
transports, audio codecs, header formats and different transport protocols for
the audio transmission. This translation will help bridging the gap between the
two protocols by providing inter-working capability between the two audio
streams of IAX and RSW. Some related works have been done to provide
translation between IAX and RSW control signalling messages. But, this research
paper concentrates on the translation that depends on the media transfer. The
proposed translation module was tested and evaluated in different scenarios in
order to examine its performance. The obtained results showed that the
Real-Time Audio Translation Module produces lower rates of packet delay and
jitter than the acceptance values for each of the mentioned performance
metrics.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="61000" completeListSize="102538">1122234|62001</resumptionToken>
</ListRecords>
</OAI-PMH>
