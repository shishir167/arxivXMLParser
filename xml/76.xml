<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:45:45Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|75001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07224</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07224</id><created>2015-03-24</created><updated>2015-04-02</updated><authors><author><keyname>Sevlian</keyname><forenames>Raffi</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Feeder Topology Identification</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced control of distributed energy resources at the consumer level
requires full situational awareness of the the distribution system. One
important problem is that of feeder topology identification, due to changing
switch configurations, given a sparse number of measurements. We formulate the
problem for residential feeders as a spanning tree identification problem over
a general graph. Given a set of power flow measurements and load pseudo
measurements, we show that the underlying graph structure is crucial in
defining identifiability of the correct spanning tree on the graph. First we
solve the deterministic case of known true loads and measured power flow. We
show that the placement of sensors on the network alone determine whether the
set of spanning trees can be correctly identified. In the stochastic case where
loads are in the form of noisy forecasts, we present a locally optimal sensor
placement algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07232</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07232</id><created>2015-03-24</created><authors><author><keyname>Maidens</keyname><forenames>John</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author></authors><title>A note on optimal experiment design for nonlinear systems using dynamic
  programming</title><categories>cs.SY math.OC</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method of solving the T-optimal design problem for nonlinear
dynamical systems using dynamic programming. In contrast with previous dynamic
programming formulations, we avoid adding an equation for the dispersion to the
system state, allowing for more efficient solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07235</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07235</id><created>2015-03-24</created><updated>2015-09-10</updated><authors><author><keyname>Shen</keyname><forenames>Jun</forenames></author><author><keyname>Bazzi</keyname><forenames>Rida A.</forenames></author></authors><title>A Formal Study on Backward Compatible Dynamic Software Updates</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamic software update problem for programs interacting with an
environment that is not necessarily updated. We argue that such updates should
be backward compatible. We propose a general definition of backward
compatibility and cases of backward compatible program update. Based on our
detailed study of real world program evolution, we propose classes of backward
compatible update for interactive programs, which are included at an average of
32% of all studied program changes. The definitions of update classes are
parameterized by our novel framework of program equivalence, which generalizes
existing results on program equivalence to non-terminating executions. Our
study of backward compatible updates is based on a typed extension of W
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07236</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07236</id><created>2015-03-24</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Isotropically Random Orthogonal Matrices: Performance of LASSO and
  Minimum Conic Singular Values</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the precise performance of the Generalized LASSO algorithm for
recovering structured signals from compressed noisy measurements, obtained via
i.i.d. Gaussian matrices, has been characterized. The analysis is based on a
framework introduced by Stojnic and heavily relies on the use of Gordon's
Gaussian min-max theorem (GMT), a comparison principle on Gaussian processes.
As a result, corresponding characterizations for other ensembles of measurement
matrices have not been developed. In this work, we analyze the corresponding
performance of the ensemble of isotropically random orthogonal (i.r.o.)
measurements. We consider the constrained version of the Generalized LASSO and
derive a sharp characterization of its normalized squared error in the
large-system limit. When compared to its Gaussian counterpart, our result
analytically confirms the superiority in performance of the i.r.o. ensemble.
Our second result, derives an asymptotic lower bound on the minimum conic
singular values of i.r.o. matrices. This bound is larger than the corresponding
bound on Gaussian matrices. To prove our results we express i.r.o. matrices in
terms of Gaussians and show that, with some modifications, the GMT framework is
still applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07240</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07240</id><created>2015-03-24</created><authors><author><keyname>Zhou</keyname><forenames>Dengyong</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Platt</keyname><forenames>John C.</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author></authors><title>Regularized Minimax Conditional Entropy for Crowdsourcing</title><categories>cs.LG stat.ML</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a rapidly increasing interest in crowdsourcing for data labeling. By
crowdsourcing, a large number of labels can be often quickly gathered at low
cost. However, the labels provided by the crowdsourcing workers are usually not
of high quality. In this paper, we propose a minimax conditional entropy
principle to infer ground truth from noisy crowdsourced labels. Under this
principle, we derive a unique probabilistic labeling model jointly
parameterized by worker ability and item difficulty. We also propose an
objective measurement principle, and show that our method is the only method
which satisfies this objective measurement principle. We validate our method
through a variety of real crowdsourcing datasets with binary, multiclass or
ordinal labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07241</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07241</id><created>2015-03-24</created><authors><author><keyname>Sundaram</keyname><forenames>Narayanan</forenames></author><author><keyname>Satish</keyname><forenames>Nadathur Rajagopalan</forenames></author><author><keyname>Patwary</keyname><forenames>Md Mostofa Ali</forenames></author><author><keyname>Dulloor</keyname><forenames>Subramanya R</forenames></author><author><keyname>Vadlamudi</keyname><forenames>Satya Gautam</forenames></author><author><keyname>Das</keyname><forenames>Dipankar</forenames></author><author><keyname>Dubey</keyname><forenames>Pradeep</forenames></author></authors><title>GraphMat: High performance graph analytics made productive</title><categories>cs.PF cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the growing importance of large-scale graph analytics, there is a need
to improve the performance of graph analysis frameworks without compromising on
productivity. GraphMat is our solution to bridge this gap between a
user-friendly graph analytics framework and native, hand-optimized code.
GraphMat functions by taking vertex programs and mapping them to high
performance sparse matrix operations in the backend. We get the productivity
benefits of a vertex programming framework without sacrificing performance.
GraphMat is in C++, and we have been able to write a diverse set of graph
algorithms in this framework with the same effort compared to other vertex
programming frameworks. GraphMat performs 1.2-7X faster than high performance
frameworks such as GraphLab, CombBLAS and Galois. It achieves better multicore
scalability (13-15X on 24 cores) than other frameworks and is 1.2X off native,
hand-optimized code on a variety of different graph algorithms. Since GraphMat
performance depends mainly on a few scalable and well-understood sparse matrix
operations, GraphMatcan naturally benefit from the trend of increasing
parallelism on future hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07253</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07253</id><created>2015-03-24</created><authors><author><keyname>Chen</keyname><forenames>Mo</forenames></author><author><keyname>Hu</keyname><forenames>Qie</forenames></author><author><keyname>Mackin</keyname><forenames>Casey</forenames></author><author><keyname>Fisac</keyname><forenames>Jaime F.</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire J.</forenames></author></authors><title>Safe Platooning of Unmanned Aerial Vehicles via Reachability</title><categories>cs.SY</categories><comments>Submission to 54th IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been immense interest in using unmanned aerial vehicles
(UAVs) for civilian operations such as package delivery, firefighting, and fast
disaster response. As a result, UAV traffic management systems are needed to
support potentially thousands of UAVs flying simultaneously in the airspace, in
order to ensure their liveness and safety requirements are met. Hamilton-Jacobi
(HJ) reachability is a powerful framework for providing conditions under which
these requirements can be met, and for synthesizing the optimal controller for
meeting them. However, due to the curse of dimensionality, HJ reachability is
only tractable for a small number of vehicles if their set of maneuvers is
unrestricted. In this paper, we define a platoon to be a group of UAVs in a
single-file formation. We model each vehicle as a hybrid system with modes
corresponding to its role in the platoon, and specify the set of allowed
maneuvers in each mode to make the analysis tractable. We propose several
liveness controllers based on HJ reachability, and wrap a safety controller,
also based on HJ reachability, around the liveness controllers. For a single
altitude range, our approach guarantees safety for one safety breach; in the
unlikely event of multiple safety breaches, safety can be guaranteed over
multiple altitude ranges. We demonstrate the satisfaction of liveness and
safety requirements through simulations of three common scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07261</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07261</id><created>2015-03-24</created><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Dual Polynomials for Collision and Element Distinctness</title><categories>cs.CC quant-ph</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approximate degree of a Boolean function $f: \{-1, 1\}^n \to \{-1, 1\}$
is the minimum degree of a real polynomial that approximates $f$ to within
error $1/3$ in the $\ell_\infty$ norm. In an influential result, Aaronson and
Shi (J. ACM 2004) proved tight $\tilde{\Omega}(n^{1/3})$ and
$\tilde{\Omega}(n^{2/3})$ lower bounds on the approximate degree of the
Collision and Element Distinctness functions, respectively. Their proof was
non-constructive, using a sophisticated symmetrization argument and tools from
approximation theory.
  More recently, several open problems in the study of approximate degree have
been resolved via the construction of dual polynomials. These are explicit dual
solutions to an appropriate linear program that captures the approximate degree
of any function. We reprove Aaronson and Shi's results by constructing explicit
dual polynomials for the Collision and Element Distinctness functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07266</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07266</id><created>2015-03-24</created><authors><author><keyname>Hansen</keyname><forenames>Conner</forenames></author><author><keyname>Syriani</keyname><forenames>Eugene</forenames></author><author><keyname>Lucio</keyname><forenames>Levi</forenames></author></authors><title>Towards Controlling Refinements of Statecharts</title><categories>cs.SE</categories><comments>In Poster Proceedings of 6th Conference on Software Language
  Engineering (SLE) 2013 (http://www.sleconf.org/2013/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In incremental development strategies, modelers frequently refine Statecharts
models to satisfy requirements and changes. Although several solutions exist to
the problem of Statecharts refinement, they provide such levels of freedom that
a statechart cannot make assumptions or guarantees about its future structure.
In this paper, we propose a set of bounding rules to limit the allowable
Statecharts refinement operations such that certain assumptions will hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07274</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07274</id><created>2015-03-24</created><authors><author><keyname>Mansimov</keyname><forenames>Elman</forenames></author><author><keyname>Srivastava</keyname><forenames>Nitish</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Initialization Strategies of Spatio-Temporal Convolutional Neural
  Networks</title><categories>cs.CV cs.LG</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new way of incorporating temporal information present in videos
into Spatial Convolutional Neural Networks (ConvNets) trained on images, that
avoids training Spatio-Temporal ConvNets from scratch. We describe several
initializations of weights in 3D Convolutional Layers of Spatio-Temporal
ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it
is important to initialize 3D Convolutional Weights judiciously in order to
learn temporal representations of videos. We evaluate our methods on the
UCF-101 dataset and demonstrate improvement over Spatial ConvNets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07276</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07276</id><created>2015-03-25</created><authors><author><keyname>Gostar</keyname><forenames>Amirali K.</forenames></author><author><keyname>Hoseinnezhad</keyname><forenames>Reza</forenames></author><author><keyname>Bab-Hadiashar</keyname><forenames>Alireza</forenames></author></authors><title>Multi-Bernoulli Sensor-Control via Minimization of Expected Estimation
  Errors</title><categories>cs.SY cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a sensor-control method for choosing the best next state
of the sensor(s), that provide(s) accurate estimation results in a multi-target
tracking application. The proposed solution is formulated for a multi-Bernoulli
filter and works via minimization of a new estimation error-based cost
function. Simulation results demonstrate that the proposed method can
outperform the state-of-the-art methods in terms of computation time and
robustness to clutter while delivering similar accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07283</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07283</id><created>2015-03-25</created><authors><author><keyname>Korobov</keyname><forenames>Mikhail</forenames></author></authors><title>Morphological Analyzer and Generator for Russian and Ukrainian Languages</title><categories>cs.CL</categories><comments>AIST 2015 (http://aistconf.org/2015); 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian
languages. It uses large efficiently encoded lexi- cons built from OpenCorpora
and LanguageTool data. A set of linguistically motivated rules is developed to
enable morphological analysis and generation of out-of-vocabulary words
observed in real-world documents. For Russian pymorphy2 provides
state-of-the-arts morphological analysis quality. The analyzer is implemented
in Python programming language with optional C++ extensions. Emphasis is put on
ease of use, documentation and extensibility. The package is distributed under
a permissive open-source license, encouraging its use in both academic and
commercial setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07284</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07284</id><created>2015-03-25</created><authors><author><keyname>De</keyname><forenames>Arijit</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>A Rule-Based Short Query Intent Identification System</title><categories>cs.IR cs.AI</categories><comments>5 pages, 2010 International Conference on Signal and Image Processing
  (ICSIP)</comments><doi>10.1109/ICSIP.2010.5697471</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using SMS (Short Message System), cell phones can be used to query for
information about various topics. In an SMS based search system, one of the key
problems is to identify a domain (broad topic) associated with the user query;
so that a more comprehensive search can be carried out by the domain specific
search engine. In this paper we use a rule based approach, to identify the
domain, called Short Query Intent Identification System (SQIIS). We construct
two different rule-bases using different strategies to suit query intent
identification. We evaluate the two rule-bases experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07288</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07288</id><created>2015-03-25</created><updated>2015-08-21</updated><authors><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>The Social System Identification Problem</title><categories>cs.SI</categories><comments>Revised version submitted to IEEE CDC 2015, to be presented</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The focus of this paper is modeling what we call a Social Radar, i.e. a
method to estimate the relative influence between social agents, by sampling
their opinions and as they evolve, after injecting in the network stubborn
agents. The stubborn agents opinion is not influenced by the peers they seek to
sway, and their opinion bias is the known input to the social network system.
The novelty is in the model presented to probe a social network and the
solution of the associated regression problem. The model allows to map the
observed opinion onto system equations that can be used to infer the social
graph and the amount of trust that characterizes the links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07291</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07291</id><created>2015-03-25</created><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Wireless Power Charging Control in Multiuser Broadband Networks</title><categories>cs.NI</categories><comments>This paper had been accepted by IEEE ICC 2015, Workshop on Green
  Communications and Networks with Energy Harvesting, Smart Grids, and
  Renewable Energies</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent advances in wireless power transfer (WPT) technology provide a
cost-effective solution to charge wireless devices remotely without disruption
to the use. In this paper, we propose an efficient wireless charging control
method for exploiting the frequency diversity in multiuser broadband wireless
networks, to reduce energy outage and keep the system operating in an efficient
and sustainable state. In particular, we first analyze the impact of charging
control method to the operating lifetime of a WPT-enabled broadband system.
Based on the analysis, we then propose a multi-criteria charging control policy
that optimizes the transmit power allocation over frequency by jointly
considering the channel state information (CSI) and the battery state
information (BSI) of wireless devices. For practical implementation, the
proposed scheme is realized by a novel limited CSI estimation mechanism
embedded with partial BSI, which significantly reduces the energy cost of CSI
and BSI feedback. Simulation results show that the proposed method could
significantly increase the network lifetime under stringent transmit power
constraint. Reciprocally, it also consumes lower transmit power to achieve
near-perpetual network operation than other single-criterion based charging
control methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07294</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07294</id><created>2015-03-25</created><authors><author><keyname>Syn</keyname><forenames>Wendy Tan Wei</forenames></author><author><keyname>How</keyname><forenames>Bong Chih</forenames></author><author><keyname>Atoum</keyname><forenames>Issa</forenames></author></authors><title>Using Latent Semantic Analysis to Identify Quality in Use (QU)
  Indicators from User Reviews</title><categories>cs.CL cs.AI cs.IR</categories><comments>4 Figures in The International Conference on Artificial Intelligence
  and Pattern Recognition (AIPR2014),2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a novel approach to categorize users' reviews according
to the three Quality in Use (QU) indicators defined in ISO: effectiveness,
efficiency and freedom from risk. With the tremendous amount of reviews
published each day, there is a need to automatically summarize user reviews to
inform us if any of the software able to meet requirement of a company
according to the quality requirements. We implemented the method of Latent
Semantic Analysis (LSA) and its subspace to predict QU indicators. We build a
reduced dimensionality universal semantic space from Information System
journals and Amazon reviews. Next, we projected set of indicators' measurement
scales into the universal semantic space and represent them as subspace. In the
subspace, we can map similar measurement scales to the unseen reviews and
predict the QU indicators. Our preliminary study able to obtain the average of
F-measure, 0.3627.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07297</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07297</id><created>2015-03-25</created><authors><author><keyname>Pal</keyname><forenames>Chandrajit</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>A Brief Survey of Recent Edge-Preserving Smoothing Algorithms on Digital
  Images</title><categories>cs.CV</categories><comments>Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge preserving filters preserve the edges and its information while blurring
an image. In other words they are used to smooth an image, while reducing the
edge blurring effects across the edge like halos, phantom etc. They are
nonlinear in nature. Examples are bilateral filter, anisotropic diffusion
filter, guided filter, trilateral filter etc. Hence these family of filters are
very useful in reducing the noise in an image making it very demanding in
computer vision and computational photography applications like denoising,
video abstraction, demosaicing, optical-flow estimation, stereo matching, tone
mapping, style transfer, relighting etc. This paper provides a concrete
introduction to edge preserving filters starting from the heat diffusion
equation in olden to recent eras, an overview of its numerous applications, as
well as mathematical analysis, various efficient and optimized ways of
implementation and their interrelationships, keeping focus on preserving the
boundaries, spikes and canyons in presence of noise. Furthermore it provides a
realistic notion for efficient implementation with a research scope for
hardware realization for further acceleration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07301</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07301</id><created>2015-03-25</created><authors><author><keyname>Buda</keyname><forenames>Andrzej</forenames></author><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author></authors><title>Exploring patterns in European singles charts</title><categories>physics.soc-ph cs.SI nlin.PS stat.AP</categories><comments>7p+appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  European singles charts are important part of the music industry responsible
for creating popularity of songs. After modeling and exploring dynamics of
global album sales in previous papers, we investigate patterns of hit singles
popularity according to all data (1966-2015) from weekly charts (polls) in 12
Western European countries. The dynamics of building popularity in various
national charts is more than the economy because it depends on spread of
information. In our research we have shown how countries may be affected by
their neighbourhood and influenced by technological era. We have also computed
correlations with geographical and cultural distances between countries in
analog, digital and Internet era. We have shown that time delay between the
single premiere and the peak of popularity has become shorter under the
influence of technology and the popularity of songs depends on geographical
distances in analog (1966-1987) and Internet (2004-2015) era. On the other
hand, cultural distances between nations have influenced the peaks of
popularity, but in the Compact Disc era only (1988-2003). We have also
indicated the European countries in line with global trends e.g. The
Netherlands, the United Kingdom and outsiders like Italy and Spain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07310</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07310</id><created>2015-03-25</created><updated>2015-06-03</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Van Pham</keyname><forenames>Trung</forenames></author></authors><title>The Complexity of Phylogeny Constraint Satisfaction</title><categories>cs.CC</categories><comments>45 pages, 1 figure. In this version we correct the proof Lemma 82</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We systematically study the computational complexity of a broad class of
computational problems in phylogenetic reconstruction. The class contains for
example the rooted triple consistency problem, forbidden subtree problems, the
quartet consistency problem, and many other problems studied in the
bioinformatics literature. The studied problems can be described as constraint
satisfaction problems where the constraints have a first-order definition over
the rooted triple relation. We show that every such phylogeny problem can be
solved in polynomial time or is NP-complete. On the algorithmic side, we
generalize a well-known polynomial-time algorithm of Aho, Sagiv, Szymanski, and
Ullman for the rooted triple consistency problem. Our algorithm repeatedly
solves linear equation systems to construct a solution in polynomial time. We
then show that every phylogeny problem that cannot be solved by our algorithm
is NP-complete. Our classification establishes a dichotomy for a large class of
infinite structures that we believe is of independent interest in universal
algebra, model theory, and topology. The proof of our main result combines
results and techniques from various research areas: a recent classification of
the model-complete cores of the reducts of the homogeneous binary branching
C-relation, Leeb's Ramsey theorem for rooted trees, and universal algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07321</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07321</id><created>2015-03-25</created><updated>2015-06-16</updated><authors><author><keyname>Atzeni</keyname><forenames>Italo</forenames></author><author><keyname>Arnau</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Fractional Pilot Reuse in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Paper presented at the IEEE ICC 2015 Workshop on 5G &amp; Beyond -
  Enabling Technologies and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pilot contamination is known to be one of the main impairments for massive
MIMO multi-cell communications. Inspired by the concept of fractional frequency
reuse and by recent contributions on pilot reutilization among non-adjacent
cells, we propose a new pilot allocation scheme to mitigate this effect. The
key idea is to allow users in neighboring cells that are closest to their base
stations to reuse the same pilot sequences. Focusing on the uplink, we obtain
expressions for the overall spectral efficiency per cell for different linear
combining techniques at the base station and use them to obtain both the
optimal pilot reuse parameters and the optimal number of scheduled users.
Numerical results show a remarkable improvement in terms of spectral efficiency
with respect to the existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07341</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07341</id><created>2015-03-25</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author></authors><title>An Experiment on Using Bayesian Networks for Process Mining</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process mining is a technique that performs an automatic analysis of business
processes from a log of events with the promise of understanding how processes
are executed in an organisation.
  Several models have been proposed to address this problem, however, here we
propose a different approach to deal with uncertainty. By uncertainty, we mean
estimating the probability of some sequence of tasks occurring in a business
process, given that only a subset of tasks may be observable.
  In this sense, this work proposes a new approach to perform process mining
using Bayesian Networks. These structures can take into account the probability
of a task being present or absent in the business process. Moreover, Bayesian
Networks are able to automatically learn these probabilities through mechanisms
such as the maximum likelihood estimate and EM clustering.
  Experiments made over a Loan Application Case study suggest that Bayesian
Networks are adequate structures for process mining and enable a deep analysis
of the business process model that can be used to answer queries about that
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07342</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07342</id><created>2015-03-25</created><authors><author><keyname>Eferina</keyname><forenames>E. G.</forenames></author><author><keyname>Korolkova</keyname><forenames>A. V.</forenames></author><author><keyname>Gevorkyan</keyname><forenames>M. N.</forenames></author><author><keyname>Kulyabov</keyname><forenames>D. S.</forenames></author><author><keyname>Sevastyanov</keyname><forenames>L. A.</forenames></author></authors><title>One-Step Stochastic Processes Simulation Software Package</title><categories>cs.SC</categories><comments>in Russian; in English</comments><journal-ref>Bulletin of PFUR. Series Mathematics. Information Sciences.
  Physics (3) (2014) 46-59</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Background. It is assumed that the introduction of stochastic in mathematical
model makes it more adequate. But there is virtually no methods of coordinated
(depended on structure of the system) stochastic introduction into
deterministic models. Authors have improved the method of stochastic models
construction for the class of one-step processes and illustrated by models of
population dynamics. Population dynamics was chosen for study because its
deterministic models were sufficiently well explored that allows to compare the
results with already known ones.
  Purpose. To optimize the models creation as much as possible some routine
operations should be automated. In this case, the process of drawing up the
model equations can be algorithmized and implemented in the computer algebra
system. Furthermore, on the basis of these results a set of programs for
numerical experiment can be obtained.
  Method. The computer algebra system Axiom is used for analytical calculations
implementation. To perform the numerical experiment FORTRAN and Julia languages
are used. The method Runge--Kutta method for stochastic differential equations
is used as numerical method.
  Results. The program compex for creating stochastic one-step processes models
is constructed. Its application is illustrated by the predator-prey population
dynamic system.
  Conclusions. Computer algebra systems are very convenient for the purposes of
rapid prototyping in mathematical models design and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07356</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07356</id><created>2015-03-25</created><authors><author><keyname>Moraga</keyname><forenames>Claudio</forenames></author></authors><title>Mixed polarity reversible Peres gates</title><categories>cs.ET</categories><comments>2 pages, 5 figures</comments><acm-class>B.1.2; B.6.1; B.6.3; G.2.3</acm-class><journal-ref>Electronics Letters 50 (14):987-989, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible Peres gates with more than two all over binary-valued control
signals are discussed. Methods are disclosed for the low cost realization of
this kind of Peres gates without requiring ancillary lines. Proper distribution
of the controlled gates and their inverses allow driving the reversible Peres
gate with control signals of different polarities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07372</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07372</id><created>2015-03-25</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author></authors><title>The Two-user Causal Cognitive Interference Channel: Novel Outer Bounds
  and Constant Gap Result for the Symmetric Gaussian Noise Channel in Weak
  Interference</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the two-user Causal Cognitive Interference Channel (CCIC),
where two transmitters aim to communicate independent messages to two different
receivers via a common channel. One source, referred to as the cognitive, is
capable of overhearing the other source, referred to as the primary, through a
noisy in-band link and thus can assist in sending the primary's data. Two novel
outer bounds of the type $2R_p+R_c$ and $R_p+2R_c$ are derived for the class of
injective semi-deterministic CCICs where the noises at the different
source-destination pairs are independent. An achievable rate region is derived
based on Gelfand-Pinsker binning, superposition coding and simultaneous
decoding at the receivers.
  The lower and outer bounds are then specialized to the practically relevant
Gaussian noise case. The authors of this paper recently characterized to within
a constant gap the capacity of the symmetric Gaussian CCIC in (a) the strong
interference regime, and (b) for a subset of the weak interference regime when
the cooperation link is larger than a given threshold. This work characterizes
to within a constant gap the capacity for the symmetric Gaussian CCIC in the
regime that was still open. In particular, it is shown that the novel outer
bounds are necessary to characterize the capacity to within a constant gap when
the cooperation link is weaker than the direct links, that is, in this regime
unilateral cooperation leaves some system resources underutilized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07376</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07376</id><created>2015-03-25</created><authors><author><keyname>Cisneros</keyname><forenames>R.</forenames></author><author><keyname>Pirro</keyname><forenames>M.</forenames></author><author><keyname>Bergna</keyname><forenames>G.</forenames></author><author><keyname>Ortega</keyname><forenames>R.</forenames></author><author><keyname>Ippoliti</keyname><forenames>G.</forenames></author><author><keyname>Molinas</keyname><forenames>M.</forenames></author></authors><title>Global Tracking Passivity--based PI Control of Bilinear Systems and its
  Application to the Boost and Modular Multilevel Converters</title><categories>cs.SY</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of trajectory tracking of a class of
bilinear systems with time--varying measurable disturbance. A set of matrices
{A,B_i} has been identified, via a linear matrix inequality, for which it is
possible to ensure global tracking of (admissible, differentiable) trajectories
with a simple linear time--varying PI controller. Instrumental to establish the
result is the construction of an output signal with respect to which the
incremental model is passive. The result is applied to the boost and the
modular multilevel converter for which experimental results are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07377</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07377</id><created>2015-03-25</created><authors><author><keyname>Naghizadeh</keyname><forenames>Parinaz</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>A Tale of Two Mechanisms: Incentivizing Investments in Security Games</title><categories>cs.GT</categories><comments>45 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a system of interdependent users, the security of an entity is affected
not only by that user's investment in security measures, but also by the
positive externality of the security decisions of (some of) the other users.
The provision of security in such system is therefore modeled as a public good
provision problem, and is referred to as a security game. In this paper, we
compare two well-known incentive mechanisms in this context for incentivizing
optimal security investments among users, namely the Pivotal and the
Externality mechanisms. The taxes in a Pivotal mechanism are designed to ensure
users' voluntary participation, while those in an Externality mechanism are
devised to maintain a balanced budget. We first show the more general result
that, due to the non-excludable nature of security, no mechanism can
incentivize the socially optimal investment profile, while at the same time
ensuring voluntary participation and maintaining a balanced budget for all
instances of security games. To further illustrate, we apply the Pivotal and
Externality mechanisms to the special case of weighted total effort
interdependence models, and identify some of the effects of varying
interdependency between users on the budget deficit in the Pivotal mechanism,
as well as on the participation incentives in the Externality mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07384</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07384</id><created>2015-03-25</created><authors><author><keyname>Hot</keyname><forenames>Elma</forenames></author><author><keyname>Sekuli&#x107;</keyname><forenames>Petar</forenames></author></authors><title>Compressed sensing MRI using masked DCT and DFT measurements</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents modification of the TwIST algorithm for Compressive
Sensing MRI images reconstruction. Compressive Sensing is new approach in
signal processing whose basic idea is recovering signal form small set of
available samples. The application of the Compressive Sensing in biomedical
imaging has found great importance. It allows significant lowering of the
acquisition time, and therefore, save the patient from the negative impact of
the MR apparatus. TwIST is commonly used algorithm for 2D signals
reconstruction using Compressive Sensing principle. It is based on the Total
Variation minimization. Standard version of the TwIST uses masked 2D Discrete
Fourier Transform coefficients as Compressive Sensing measurements. In this
paper, different masks and different transformation domains for coefficients
selection are tested. Certain percent of the measurements is used from the
mask, as well as small number of coefficients outside the mask. Comparative
analysis using 2D DFT and 2D DCT coefficients, with different mask shapes is
performed. The theory is proved with experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07387</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07387</id><created>2015-02-20</created><updated>2015-12-22</updated><authors><author><keyname>Plaisance</keyname><forenames>Jeff</forenames></author><author><keyname>Kurz</keyname><forenames>Nathan</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Vectorized VByte Decoding</title><categories>cs.IR</categories><comments>First International Symposium on Web Algorithms (June 2015)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the ubiquitous technique of VByte compression, which represents
each integer as a variable length sequence of bytes. The low 7 bits of each
byte encode a portion of the integer, and the high bit of each byte is reserved
as a continuation flag. This flag is set to 1 for all bytes except the last,
and the decoding of each integer is complete when a byte with a high bit of 0
is encountered. VByte decoding can be a performance bottleneck especially when
the unpredictable lengths of the encoded integers cause frequent branch
mispredictions. Previous attempts to accelerate VByte decoding using SIMD
vector instructions have been disappointing, prodding search engines such as
Google to use more complicated but faster-to-decode formats for
performance-critical code. Our decoder (Masked VByte) is 2 to 4 times faster
than a conventional scalar VByte decoder, making the format once again
competitive with regard to speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07389</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07389</id><created>2015-03-25</created><updated>2015-03-31</updated><authors><author><keyname>Bjerre-Nielsen</keyname><forenames>Andreas</forenames></author></authors><title>Network formation with value heterogeneity: centrality, segregation and
  adverse effects</title><categories>q-fin.EC cs.GT</categories><comments>28 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate formation of economic and social networks where agents may
form or cut ties. The novelty is combining a setup where agents are
heterogeneous in their talent for generating value in the links they form and
value may also accrue from indirect ties. We provide sufficient conditions for
assortative matching: agents of greater talent have partners of greater talent.
A novel feature is that agents with higher talent are more central in networks.
Another novel feature is degree assortativity: partnered agents have a similar
number of partners. Two suboptimal network structures are noteworthy. One
network displays excess assortativity as high and low talented types fail to
connect, and thus inefficient due to payoff externalities despite otherwise
obeying the conditions of Becker (1973). In another suboptimal network an agent
of low talent becomes excessively central.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07401</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07401</id><created>2015-03-24</created><updated>2015-10-01</updated><authors><author><keyname>Hasegawa</keyname><forenames>Keisuke</forenames></author><author><keyname>Sakurai</keyname><forenames>Tatsuma</forenames></author><author><keyname>Makino</keyname><forenames>Yasutoshi</forenames></author><author><keyname>Shinoda</keyname><forenames>Hiroyuki</forenames></author></authors><title>Manual Character Transmission by Presenting Trajectories of 7mm-high
  Letters in One Second</title><categories>cs.HC</categories><comments>Submitted in IEEE Transactions on Haptics</comments><doi>10.1109/TOH.2016.2517625</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report a method of intuitively transmitting symbolic
information to untrained users via only their hands without using any visual or
auditory cues. Our simple concept is presenting three-dimensional letter
trajectories to the user's hand via a stylus which is mechanically manipulated.
By this simple method, in our experiments, participants were able to read 14
mm-high lower-case letters displayed at a rate of one letter per second with an
accuracy rate of 71.9% in their first trials, which was improved to 91.3% after
a five-minute training period. These results showed small individual
differences among participants (standard deviation of 12.7% in the first trials
and 6.7% after training). We also found that this accuracy was still retained
to a high level (85.1% with SD of 8.2%) even when the letters were reduced to a
height of 7 mm. Thus, we revealed that sighted adults potentially possess the
ability to read small letters accurately at normal writing speed using their
hands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07405</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07405</id><created>2015-03-25</created><authors><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Liakata</keyname><forenames>Maria</forenames></author><author><keyname>Procter</keyname><forenames>Rob</forenames></author></authors><title>Making the Most of Tweet-Inherent Features for Social Spam Detection on
  Twitter</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social spam produces a great amount of noise on social media services such as
Twitter, which reduces the signal-to-noise ratio that both end users and data
mining applications observe. Existing techniques on social spam detection have
focused primarily on the identification of spam accounts by using extensive
historical and network-based data. In this paper we focus on the detection of
spam tweets, which optimises the amount of data that needs to be gathered by
relying only on tweet-inherent features. This enables the application of the
spam detection system to a large set of tweets in a timely fashion, potentially
applicable in a real-time or near real-time setting. Using two large
hand-labelled datasets of tweets containing spam, we study the suitability of
five classification algorithms and four different feature sets to the social
spam detection task. Our results show that, by using the limited set of
features readily available in a tweet, we can achieve encouraging results which
are competitive when compared against existing spammer detection systems that
make use of additional, costly user features. Our study is the first that
attempts at generalising conclusions on the optimal classifiers and sets of
features for social spam detection over different datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07414</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07414</id><created>2015-03-25</created><updated>2015-04-30</updated><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Shi</keyname><forenames>Dayu</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Comparing Graphs via Persistence Distortion</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric graphs are ubiquitous in science and engineering. For example, many
data are drawn from hidden spaces that are graph-like, such as the cosmic web.
A metric graph offers one of the simplest yet still meaningful ways to
represent the non-linear structure hidden behind the data. In this paper, we
propose a new distance between two finite metric graphs, called the
persistence-distortion distance, which draws upon a topological idea. This
topological perspective along with the metric space viewpoint provide a new
angle to the graph matching problem. Our persistence-distortion distance has
two properties not shared by previous methods: First, it is stable against the
perturbations of the input graph metrics. Second, it is a continuous distance
measure, in the sense that it is defined on an alignment of the underlying
spaces of input graphs, instead of merely their nodes. This makes our
persistence-distortion distance robust against, for example, different
discretizations of the same underlying graph. Despite considering the input
graphs as continuous spaces, that is, taking all points into account, we show
that we can compute the persistence-distortion distance in polynomial time. The
time complexity for the discrete case where only graph nodes are considered is
much faster. We also provide some preliminary experimental results to
demonstrate the use of the new distance measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07424</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07424</id><created>2015-03-23</created><updated>2015-05-20</updated><authors><author><keyname>Bryant</keyname><forenames>Darryn</forenames></author><author><keyname>Colbourn</keyname><forenames>Charles</forenames></author><author><keyname>Horsley</keyname><forenames>Daniel</forenames></author><author><keyname>Cath&#xe1;in</keyname><forenames>Padraig &#xd3;</forenames></author></authors><title>Compressed sensing with combinatorial designs: theory and simulations</title><categories>cs.IT math.CO math.IT</categories><comments>18 pages, 3 figures</comments><msc-class>05B05, 94A12, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 'An asymptotic result on compressed sensing matrices', a new construction
for compressed sensing matrices using combinatorial design theory was
introduced. In this paper, we use deterministic and probabilistic methods to
analyse the performance of matrices obtained from this construction. We provide
new theoretical results and detailed simulations. These simulations indicate
that the construction is competitive with Gaussian random matrices, and that
recovery is tolerant to noise. A new recovery algorithm tailored to the
construction is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07426</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07426</id><created>2015-03-24</created><authors><author><keyname>Cheng</keyname><forenames>Yukun</forenames></author><author><keyname>Zhou</keyname><forenames>Sanming</forenames></author></authors><title>A Survey on Approximation Mechanism Design without Money for Facility
  Games</title><categories>cs.GT math.CO</categories><msc-class>05C57, 91A43, 91A46</msc-class><journal-ref>Advances in Global Optimization, Springer Proceedings in
  Mathematics &amp; Statistics Volume 95, 2015, pp 117-128</journal-ref><doi>10.1007/978-3-319-08377-3_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a facility game one or more facilities are placed in a metric space to
serve a set of selfish agents whose addresses are their private information. In
a classical facility game, each agent wants to be as close to a facility as
possible, and the cost of an agent can be defined as the distance between her
location and the closest facility. In an obnoxious facility game, each agent
wants to be far away from all facilities, and her utility is the distance from
her location to the facility set. The objective of each agent is to minimize
her cost or maximize her utility. An agent may lie if, by doing so, more
benefit can be obtained. We are interested in social choice mechanisms that do
not utilize payments. The game designer aims at a mechanism that is
strategy-proof, in the sense that any agent cannot benefit by misreporting her
address, or, even better, group strategy-proof, in the sense that any coalition
of agents cannot all benefit by lying. Meanwhile, it is desirable to have the
mechanism to be approximately optimal with respect to a chosen objective
function. Several models for such approximation mechanism design without money
for facility games have been proposed. In this paper we briefly review these
models and related results for both deterministic and randomized mechanisms,
and meanwhile we present a general framework for approximation mechanism design
without money for facility games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07431</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07431</id><created>2015-03-25</created><authors><author><keyname>Romero</keyname><forenames>Daniel M.</forenames></author><author><keyname>Huttenlocher</keyname><forenames>Dan</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>Coordination and Efficiency in Decentralized Collaboration</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 6 figures, ICWSM 2015, in Proc. 9th International AAAI
  Conference on Weblogs and Social Media</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environments for decentralized on-line collaboration are now widespread on
the Web, underpinning open-source efforts, knowledge creation sites including
Wikipedia, and other experiments in joint production. When a distributed group
works together in such a setting, the mechanisms they use for coordination can
play an important role in the effectiveness of the group's performance.
  Here we consider the trade-offs inherent in coordination in these on-line
settings, balancing the benefits to collaboration with the cost in effort that
could be spent in other ways. We consider two diverse domains that each contain
a wide range of collaborations taking place simultaneously -- Wikipedia and
GitHub -- allowing us to study how coordination varies across different
projects. We analyze trade-offs in coordination along two main dimensions,
finding similar effects in both our domains of study: first we show that, in
aggregate, high-status projects on these sites manage the coordination
trade-off at a different level than typical projects; and second, we show that
projects use a different balance of coordination when they are &quot;crowded,&quot; with
relatively small size but many participants. We also develop a stylized
theoretical model for the cost-benefit trade-off inherent in coordination and
show that it qualitatively matches the trade-offs we observe between
crowdedness and coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07439</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07439</id><created>2015-03-25</created><updated>2015-04-03</updated><authors><author><keyname>Whang</keyname><forenames>Joyce Jiyoung</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Overlapping Community Detection Using Neighborhood-Inflated Seed
  Expansion</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is an important task in network analysis. A community
(also referred to as a cluster) is a set of cohesive vertices that have more
connections inside the set than outside. In many social and information
networks, these communities naturally overlap. For instance, in a social
network, each vertex in a graph corresponds to an individual who usually
participates in multiple communities. In this paper, we propose an efficient
overlapping community detection algorithm using a seed expansion approach. The
key idea of our algorithm is to find good seeds, and then greedily expand these
seeds based on a community metric. Within this seed expansion method, we
investigate the problem of how to determine good seed nodes in a graph. In
particular, we develop new seeding strategies for a personalized PageRank
clustering scheme that optimizes the conductance community score. Experimental
results show that our seed expansion algorithm outperforms other
state-of-the-art overlapping community detection methods in terms of producing
cohesive clusters and identifying ground-truth communities. We also show that
our new seeding strategies are better than existing strategies, and are thus
effective in finding good overlapping communities in real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07444</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07444</id><created>2015-03-25</created><updated>2015-11-02</updated><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames></author><author><keyname>S.</keyname><forenames>Karthik C.</forenames></author><author><keyname>Tavenas</keyname><forenames>Sebastien</forenames></author></authors><title>Building Efficient and Compact Data Structures for Simplicial Complexes</title><categories>cs.CG cs.DS</categories><comments>An extended abstract appeared in the proceedings of SoCG 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Simplex Tree (ST) is a recently introduced data structure that can
represent abstract simplicial complexes of any dimension and allows efficient
implementation of a large range of basic operations on simplicial complexes. In
this paper, we show how to optimally compress the Simplex Tree while retaining
its functionalities. In addition, we propose two new data structures called the
Maximal Simplex Tree (MxST) and the Simplex Array List (SAL). We analyze the
compressed Simplex Tree, the Maximal Simplex Tree, and the Simplex Array List
under various settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07455</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07455</id><created>2015-03-25</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Sum Secrecy Rate in MISO Full-Duplex Wiretap Channel with Imperfect CSI</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.3918</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the achievable sum secrecy rate in MISO
(multiple-input-single-output) {\em full-duplex} wiretap channel in the
presence of a passive eavesdropper and imperfect channel state information
(CSI). We assume that the users participating in full-duplex communication have
multiple transmit antennas, and that the users and the eavesdropper have single
receive antenna each. The users have individual transmit power constraints.
They also transmit jamming signals to improve the secrecy rates. We obtain the
achievable perfect secrecy rate region by maximizing the worst case sum secrecy
rate. We also obtain the corresponding transmit covariance matrices associated
with the message signals and the jamming signals. Numerical results that show
the impact of imperfect CSI on the achievable secrecy rate region are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07460</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07460</id><created>2015-01-30</created><authors><author><keyname>Xu</keyname><forenames>Shenghui</forenames></author></authors><title>RANSAC based three points algorithm for ellipse fitting of spherical
  object's projection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the spherical object can be seen everywhere, we should extract the ellipse
image accurately and fit it by implicit algebraic curve in order to finish the
3D reconstruction. In this paper, we propose a new ellipse fitting algorithm
which only needs three points to fit the projection of spherical object and is
different from the traditional algorithms that need at least five point. The
fitting procedure is just similar as the estimation of Fundamental Matrix
estimation by seven points, and the RANSAC algorithm has also been used to
exclude the interference of noise and scattered points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07463</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07463</id><created>2015-03-25</created><updated>2015-05-19</updated><authors><author><keyname>Barvinok</keyname><forenames>Alexander</forenames></author></authors><title>Computing the partition function of a polynomial on the Boolean cube</title><categories>cs.DS math.CO math.OC</categories><comments>results on optimization of polynomials on the Boolean cube are
  included</comments><msc-class>90C09, 68C25, 68W25, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a polynomial f: {-1, 1}^n --&gt; C, we define the partition function as the
average of exp{ lambda f(x) } over all points x in {-1, 1}^n, where lambda in C
is a parameter. We present an algorithm, which, given such f, lambda and
epsilon &gt;0 approximates the partition function within a relative error of
epsilon in N^{ O (ln n -ln epsilon) } time provided |lambda| &lt; ( 2L sqrt{d}
)^{-1}, where d is the degree, L is (roughly) the Lipschitz constant of f and N
is the number of monomials in f. We apply the algorithm to approximate the
maximum of a polynomial f: {-1, 1}^n --&gt; R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07469</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07469</id><created>2015-03-25</created><authors><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author><author><keyname>Hawkins</keyname><forenames>Jeff</forenames></author></authors><title>Properties of Sparse Distributed Representations and their Application
  to Hierarchical Temporal Memory</title><categories>q-bio.NC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical evidence demonstrates that every region of the neocortex represents
information using sparse activity patterns. This paper examines Sparse
Distributed Representations (SDRs), the primary information representation
strategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We
derive a number of properties that are core to scaling, robustness, and
generalization. We use the theory to provide practical guidelines and
illustrate the power of SDRs as the basis of HTM. Our goal is to help create a
unified mathematical and practical framework for SDRs as it relates to cortical
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07473</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07473</id><created>2015-03-25</created><authors><author><keyname>Raje</keyname><forenames>Manali</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>A Survey on Backup of Data on Remote Server</title><categories>cs.DC</categories><comments>4 pages, 3 figures in IJSR, Vol.3, Issue 12, December 2014, ISSN:
  2319-7064</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large amount of electronic data is generated in Cloud computing every day.
Efficient maintenance of this data requires proper services. Hence a method to
collect data securely, by protecting and developing backups is mentioned. The
Objective is to provide Auto Response Server, better solutions for data backup
and restoring using Cloud. Data can be collected and sent to a centralized
repository in a platform independent format without any network consideration.
This data can then be used according to the requirement. The purpose of this
particular Remote Backup Server is to collect information from any remote
location even if network connectivity is not available at that point of time
and provide proper services as well as to recover data in case of loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07474</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07474</id><created>2015-03-25</created><authors><author><keyname>Kanoje</keyname><forenames>Sumitkumar</forenames></author><author><keyname>Girase</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>User Profiling Trends, Techniques and Applications</title><categories>cs.IR</categories><comments>6 pages, 1 figure in IJAFRC, Vol.1, Issue 11, November 2014, ISSN:
  2348-4853. arXiv admin note: text overlap with arXiv:1503.06555</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Personalization of information has taken recommender systems at a very
high level. With personalization these systems can generate user specific
recommendations accurately and efficiently. User profiling helps
personalization, where information retrieval is done to personalize a scenario
which maintains a separate user profile for individual user. The main objective
of this paper is to explore this field of personalization in context of user
profiling, to help researchers make aware of the user profiling. Various
trends, techniques and Applications have been discussed in paper which will
fulfill this motto.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07475</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07475</id><created>2015-03-25</created><authors><author><keyname>Bokde</keyname><forenames>Dheeraj kumar</forenames></author><author><keyname>Girase</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Role of Matrix Factorization Model in Collaborative Filtering Algorithm:
  A Survey</title><categories>cs.IR</categories><comments>8 pages, 1 figure in IJAFRC, Vol.1, Issue 12, December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation Systems apply Information Retrieval techniques to select the
online information relevant to a given user. Collaborative Filtering is
currently most widely used approach to build Recommendation System. CF
techniques uses the user behavior in form of user item ratings as their
information source for prediction. There are major challenges like sparsity of
rating matrix and growing nature of data which is faced by CF algorithms. These
challenges are been well taken care by Matrix Factorization. In this paper we
attempt to present an overview on the role of different MF model to address the
challenges of CF algorithms, which can be served as a roadmap for research in
this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07477</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07477</id><created>2015-03-25</created><authors><author><keyname>Koturwar</keyname><forenames>Praful</forenames></author><author><keyname>Girase</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>A Survey of Classification Techniques in the Area of Big Data</title><categories>cs.LG</categories><comments>7 pages, 3 figures, 2 tables in IJAFRC, Vol.1, Issue 11, November
  2014, ISSN: 2348-4853</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data concern large-volume, growing data sets that are complex and have
multiple autonomous sources. Earlier technologies were not able to handle
storage and processing of huge data thus Big Data concept comes into existence.
This is a tedious job for users unstructured data. So, there should be some
mechanism which classify unstructured data into organized form which helps user
to easily access required data. Classification techniques over big
transactional database provide required data to the users from large datasets
more simple way. There are two main classification techniques, supervised and
unsupervised. In this paper we focused on to study of different supervised
classification techniques. Further this paper shows a advantages and
limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07487</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07487</id><created>2015-03-25</created><updated>2015-07-14</updated><authors><author><keyname>Mullen</keyname><forenames>Gary L.</forenames></author><author><keyname>Muratovi&#x107;-Ribi&#x107;</keyname><forenames>Amela</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author></authors><title>On coefficients of powers of polynomials and their compositions over
  finite fields</title><categories>math.NT cs.IT math.IT</categories><msc-class>11T06, 11T35, 15B33</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any given polynomial $f$ over the finite field $\mathbb{F}_q$ with degree
at most $q-1$, we associate it with a $q\times q$ matrix $A(f)=(a_{ik})$
consisting of coefficients of its powers $(f(x))^k=\sum_{i=0}^{q-1}a_{ik} x^i$
modulo $x^q -x$ for $k=0,1,\ldots,q-1$. This matrix has some interesting
properties such as $A(g\circ f)=A(f)A(g)$ where $(g\circ f)(x) = g(f(x))$ is
the composition of the polynomial $g$ with the polynomial $f$. In particular,
$A(f^{(k)})=(A(f))^k$ for any $k$-th composition $f^{(k)}$ of $f$ with $k \geq
0$. As a consequence, we prove that the rank of $A(f)$ gives the cardinality of
the value set of $f$. Moreover, if $f$ is a permutation polynomial then the
matrix associated with its inverse $A(f^{(-1)})=A(f)^{-1}=PA(f)P$ where $P$ is
an antidiagonal permutation matrix. As an application, we study the period of a
nonlinear congruential pseduorandom sequence $\bar{a} = \{a_0, a_1, a_2, ...
\}$ generated by $a_n = f^{(n)}(a_0)$ with initial value $a_0$, in terms of the
order of the associated matrix. Finally we show that $A(f)$ is diagonalizable
in some extension field of $\mathbb{F}_q$ when $f$ is a permutation polynomial
over $\mathbb{F}_q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07490</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07490</id><created>2015-02-28</created><updated>2015-04-13</updated><authors><author><keyname>Sengupta</keyname><forenames>Subhajit</forenames></author><author><keyname>Gurumoorthy</keyname><forenames>Karthik S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Arunava</forenames></author></authors><title>Sensitivity Analysis for additive STDP rule</title><categories>q-bio.NC cs.NE</categories><comments>On Computational Neuroscience/ 11 pages</comments><msc-class>91E40, 68T05, 92C42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning
rule. The basis of STDP has strong experimental evidences and it depends on
precise input and output spike timings. In this paper we show that under
biologically plausible spiking regime, slight variability in the spike timing
leads to drastically different evolution of synaptic weights when its dynamics
are governed by the additive STDP rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07496</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07496</id><created>2015-03-11</created><authors><author><keyname>Ribas</keyname><forenames>Sabir</forenames></author><author><keyname>Ribeiro-Neto</keyname><forenames>Berthier</forenames></author><author><keyname>Silva</keyname><forenames>Edmundo de Souza e</forenames></author><author><keyname>Ueda</keyname><forenames>Alberto</forenames></author><author><keyname>Ziviani</keyname><forenames>Nivio</forenames></author></authors><title>P-score: A Publication-based Metric for Academic Productivity</title><categories>cs.DL</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a metric to assess academic productivity based on
publication outputs. We are interested in knowing how well a research group in
an area of knowledge is doing relatively to a pre-selected set of reference
groups, where each group is composed by academics or researchers. To assess
academic productivity we propose a new metric, which we call P-score. Our
metric P-score assigns weights to venues using only the publication patterns of
selected reference groups. This implies that P-score does not depend on
citation data and thus, that it is simpler to compute particularly in contexts
in which citation data is not easily available. Also, preliminary experiments
suggest that P-score preserves strong correlation with citation-based metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07508</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07508</id><created>2015-03-25</created><authors><author><keyname>Xin</keyname><forenames>Bo</forenames></author><author><keyname>Hu</keyname><forenames>Lingjing</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Stable Feature Selection from Brain sMRI</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuroimage analysis usually involves learning thousands or even millions of
variables using only a limited number of samples. In this regard, sparse
models, e.g. the lasso, are applied to select the optimal features and achieve
high diagnosis accuracy. The lasso, however, usually results in independent
unstable features. Stability, a manifest of reproducibility of statistical
results subject to reasonable perturbations to data and the model, is an
important focus in statistics, especially in the analysis of high dimensional
data. In this paper, we explore a nonnegative generalized fused lasso model for
stable feature selection in the diagnosis of Alzheimer's disease. In addition
to sparsity, our model incorporates two important pathological priors: the
spatial cohesion of lesion voxels and the positive correlation between the
features and the disease labels. To optimize the model, we propose an efficient
algorithm by proving a novel link between total variation and fast network flow
algorithms via conic duality. Experiments show that the proposed nonnegative
model performs much better in exploring the intrinsic structure of data via
selecting stable features compared with other state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07542</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07542</id><created>2015-03-25</created><authors><author><keyname>Chaitanya</keyname><forenames>Tumula V. K.</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author></authors><title>Energy-Efficient Adaptive Power Allocation for Incremental MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider energy-efficient adaptive power allocation for three incremental
multiple-input multiple-output (IMIMO) systems employing ARQ, hybrid ARQ (HARQ)
with Chase combining (CC), and HARQ with incremental redundancy (IR), to
minimize their rate-outage probability (or equivalently packet drop rate) under
a constraint on average energy consumption per data packet. We first provide
the rate-outage probability expressions for the three IMIMO systems, and then
propose methods to convert them into a tractable form and formulate the
corresponding non-convex optimization problems that can be solved by an
interior-point algorithm for finding a local optimum. To further reduce the
solution complexity, using an asymptotically equivalent approximation of the
rate-outage probability expressions, we approximate the non-convex optimization
problems as a unified geometric programming problem (GPP), for which we derive
the closed-form solution. Illustrative results indicate that the proposed power
allocation (PPA) offers significant gains in energy savings as compared to the
equal-power allocation (EPA), and the simple closed-form GPP solution can
provide closer performance to the exact method at lower values of rate-outage
probability, for the three IMIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07545</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07545</id><created>2015-02-05</created><authors><author><keyname>George</keyname><forenames>Pandya</forenames></author></authors><title>An Investigation into the Correlation between a Presidents Approval
  Rating and the Performance of His Party in the Midterm Elections</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, American politics have become increasingly polarized. In the
current political landscape, a president cannot easily collaborate with the
opposite party and pass legislature. Ideologies between parties have drifted
apart to the point that one party generally stonewalls any legislature proposed
by the other party. Because of this political landscape, it is paramount for a
president to have a majority of his party in Congress. Political parties invest
a great deal of time and effort into making sure that first their Presidential
candidate wins and is popular, and then their congressional candidates win
seats in Congress. In this study, the effect of the former on the latter was
investigated - how the approval rating of the president influences the number
of seats won or lost in Congress during the midterm elections. The data used
was collected from Gallup. An analysis of the data yielded the statistically
significant linear model y = -107.423+1.594x, where x is the approval rating of
the president and y is the number of Congressional seats won or lost by the
party of the president. Further analysis yielded a 20 percent more
statistically useful model for approval ratings greater than 50 percent: y =
-275.461+4.37551x. As of the eve of the 2014 Midterms, President Barrack Obama
had an approval rating of 44 percent. Using the originally derived linear
model, it can be said with 95 percent confidence that the Democratic Party will
lose between 27 and 48 seats in Congress, rounded to the nearest whole seat.
This prediction has since been proven correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07551</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07551</id><created>2015-02-04</created><authors><author><keyname>Carrion</keyname><forenames>P.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>A Low-throughput Wavelet-based Steganography Audio Scheme</title><categories>cs.MM cs.CR</categories><comments>2 pages, 1 figure, conference: 8th Brazilian Symposium on Information
  and Computer System Security, 2008, Gramado, RS, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the preliminary of a novel scheme of steganography, and
introduces the idea of combining two secret keys in the operation. The first
secret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,
SAFER+, etc.) prior to the wavelet audio decomposition. The way in which the
cipher text is embedded in the file requires another key, namely a stego-key,
which is associated with features of the audio wavelet analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07554</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07554</id><created>2015-03-08</created><authors><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author></authors><title>An Analytical Formulation of Power System Oscillation Frequency</title><categories>cs.SY</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes an analytical approach to formulate the power system
oscillation frequency under a large disturbance. A fact is revealed that the
oscillation frequency is only the function of the oscillation amplitude when
the system's model and operating condition are fixed. Case studies also show
that this function is damping-insensitive and could be applied to an inter-area
model of a multi-machine power system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07561</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07561</id><created>2015-03-25</created><authors><author><keyname>You</keyname><forenames>Seungil</forenames></author><author><keyname>Gattami</keyname><forenames>Ather</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>Primal robustness and semidefinite cones</title><categories>cs.SY math.OC</categories><comments>A shorter version submitted to CDC 15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reformulates and streamlines the core tools of robust stability
and performance for LTI systems using now-standard methods in convex
optimization. In particular, robustness analysis can be formulated directly as
a primal convex (semidefinite program or SDP) optimization problem using sets
of gramians whose closure is a semidefinite cone. This allows various
constraints such as structured uncertainty to be included directly, and
worst-case disturbances and perturbations constructed directly from the primal
variables. Well known results such as the KYP lemma and various scaled small
gain tests can also be obtained directly through standard SDP duality. To
readers familiar with robustness and SDPs, the framework should appear obvious,
if only in retrospect. But this is also part of its appeal and should enhance
pedagogy, and we hope suggest new research. There is a key lemma proving
closure of a grammian that is also obvious but our current proof appears
unnecessarily cumbersome, and a final aim of this paper is to enlist the help
of experts in robust control and convex optimization in finding simpler
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07563</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07563</id><created>2015-03-25</created><updated>2015-07-09</updated><authors><author><keyname>Amir</keyname><forenames>Amihood</forenames></author><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Levy</keyname><forenames>Avivit</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Shalom</keyname><forenames>B. Riva</forenames></author></authors><title>Mind the Gap</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the complexity of the online Dictionary Matching with One Gap
Problem (DMOG) which is the following. Preprocess a dictionary $D$ of $d$
patterns, where each pattern contains a special gap symbol that can match any
string, so that given a text that arrives online, a character at a time, we can
report all of the patterns from $D$ that are suffixes of the text that has
arrived so far, before the next character arrives. In more general versions the
gap symbols are associated with bounds determining the possible lengths of
matching strings. Finding efficient algorithmic solutions for (online) DMOG has
proven to be a difficult algorithmic challenge. We demonstrate that the
difficulty in obtaining efficient solutions for the DMOG problem even, in the
offline setting, can be traced back to the infamous 3SUM conjecture.
Interestingly, our reduction deviates from the known reduction paths that
follow from 3SUM. In particular, most reductions from 3SUM go through the
set-disjointness problem, which corresponds to the problem of preprocessing a
graph to answer edge-triangles queries. We use a new path of reductions by
considering the complementary, although structurally very different,
vertex-triangles queries. Using this new path we show a conditional lower bound
of $\Omega(\delta(G_D)+op)$ time per text character, where $G_D$ is a bipartite
graph that captures the structure of $D$, $\delta(G_D)$ is the degeneracy of
this graph, and $op$ is the output size. We also provide matching upper-bounds
(up to sub-polynomial factors) for the vertex-triangles problem, and then
extend these techniques to the online DMOG problem. In particular, we introduce
algorithms whose time cost depends linearly on $\delta(G_D)$. Our algorithms
make use of graph orientations, together with some additional techniques.
Finally, when $\delta(G_D)$ is large we are able to obtain even more efficient
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07568</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07568</id><created>2015-03-25</created><updated>2015-03-27</updated><authors><author><keyname>Beir&#xf3;</keyname><forenames>Mariano G.</forenames></author><author><keyname>Grynberg</keyname><forenames>Sebasti&#xe1;n P.</forenames></author><author><keyname>Alvarez-Hamelin</keyname><forenames>J. Ignacio</forenames></author></authors><title>Router-level community structure of the Internet Autonomous Systems</title><categories>cs.NI cs.DS cs.SI physics.data-an</categories><msc-class>68U99, 05C85</msc-class><acm-class>I.5.3; G.2.2</acm-class><doi>10.1140/epjds/s13688-015-0048-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet is composed of routing devices connected between them and
organized into independent administrative entities: the Autonomous Systems. The
existence of different types of Autonomous Systems (like large connectivity
providers, Internet Service Providers or universities) together with
geographical and economical constraints, turns the Internet into a complex
modular and hierarchical network. This organization is reflected in many
properties of the Internet topology, like its high degree of clustering and its
robustness.
  In this work, we study the modular structure of the Internet router-level
graph in order to assess to what extent the Autonomous Systems satisfy some of
the known notions of community structure. We show that the modular structure of
the Internet is much richer than what can be captured by the current community
detection methods, which are severely affected by resolution limits and by the
heterogeneity of the Autonomous Systems. Here we overcome this issue by using a
multiresolution detection algorithm combined with a small sample of nodes. We
also discuss recent work on community structure in the light of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07576</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07576</id><created>2015-03-25</created><updated>2015-03-26</updated><authors><author><keyname>Ruhi</keyname><forenames>Navid Azizan</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>SIRS Epidemics on Complex Networks: Concurrence of Exact Markov Chain
  and Approximated Models</title><categories>cs.SI math.DS math.OC</categories><comments>A short version of this paper has been submitted to CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the SIRS (Susceptible-Infected-Recovered-Susceptible) spreading
processes over complex networks, by considering its exact $3^n$-state Markov
chain model. The Markov chain model exhibits an interesting connection with its
$2n$-state nonlinear &quot;mean-field&quot; approximation and the latter's corresponding
linear approximation. We show that under the specific threshold where the
disease-free state is a globally stable fixed point of both the linear and
nonlinear models, the exact underlying Markov chain has an $O(\log n)$ mixing
time, which means the epidemic dies out quickly. In fact, the epidemic
eradication condition coincides for all the three models. Furthermore, when the
threshold condition is violated, which indicates that the linear model is not
stable, we show that there exists a unique second fixed point for the nonlinear
model, which corresponds to the endemic state. We also investigate the effect
of adding immunization to the SIRS epidemics by introducing two different
models, depending on the efficacy of the vaccine. Our results indicate that
immunization improves the threshold of epidemic eradication. Furthermore, the
common threshold for fast-mixing of the Markov chain and global stability of
the disease-free fixed point improves by the same factor for the
vaccination-dominant model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07587</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07587</id><created>2015-03-25</created><authors><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>Universal Psychometrics Tasks: difficulty, composition and decomposition</title><categories>cs.AI</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note revisits the concepts of task and difficulty. The notion of
cognitive task and its use for the evaluation of intelligent systems is still
replete with issues. The view of tasks as MDP in the context of reinforcement
learning has been especially useful for the formalisation of learning tasks.
However, this alternate interaction does not accommodate well for some other
tasks that are usual in artificial intelligence and, most especially, in animal
and human evaluation. In particular, we want to have a more general account of
episodes, rewards and responses, and, most especially, the computational
complexity of the algorithm behind an agent solving a task. This is crucial for
the determination of the difficulty of a task as the (logarithm of the) number
of computational steps required to acquire an acceptable policy for the task,
which includes the exploration of policies and their verification. We introduce
a notion of asynchronous-time stochastic tasks. Based on this interpretation,
we can see what task difficulty is, what instance difficulty is (relative to a
task) and also what task compositions and decompositions are.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07590</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07590</id><created>2015-03-25</created><authors><author><keyname>Lakshmana</keyname><forenames>T. R.</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>A.</forenames></author><author><keyname>Devassy</keyname><forenames>R.</forenames></author><author><keyname>Svensson</keyname><forenames>T.</forenames></author></authors><title>Precoder Design with Limited Feedback and Backhauling for Joint
  Transmission</title><categories>cs.IT math.IT</categories><comments>Under Review IEEE Transactions on Wireless Communications, Feb. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A centralized coordinated multipoint downlink joint transmission in a
frequency division duplex system requires channel state information (CSI) to be
fed back from the cell-edge users to their serving BS, and aggregated at the
central coordination node for precoding, so that interference can be mitigated.
The control signals comprising of CSI and the precoding weights can easily
overwhelm the backhaul resources. Relative thresholding has been proposed to
alleviate the burden; however, this is at the cost of reduction in throughput.
In this paper, we propose utilizing the long term channel statistics comprising
of pathloss and shadow fading in the precoder design to model the statistical
interference for the unknown CSI. In this regard, a successive second order
cone programming (SSOCP) based precoder for maximizing the weighted sum rate is
proposed. The accuracy of the solution obtained is bounded with the branch and
bound technique. An alternative optimization framework via weighted mean square
error minimization is also derived. Both these approaches provide an efficient
solution close to the optimal, and also achieve efficient backhauling, in a
sense that the precoding weights are generated only for the active links. For
comparison, a stochastic approach based on particle swarm optimization is also
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07604</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07604</id><created>2015-03-25</created><authors><author><keyname>Zhou</keyname><forenames>Mingxin</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Simultaneous Bidirectional Link Selection in Full Duplex MIMO Systems</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a point to point full duplex (FD) MIMO
communication system. We assume that each node is equipped with an arbitrary
number of antennas which can be used for transmission or reception. With FD
radios, bidirectional information exchange between two nodes can be achieved at
the same time. In this paper we design bidirectional link selection schemes by
selecting a pair of transmit and receive antenna at both ends for
communications in each direction to maximize the weighted sum rate or minimize
the weighted sum symbol error rate (SER). The optimal selection schemes require
exhaustive search, so they are highly complex. To tackle this problem, we
propose a Serial-Max selection algorithm, which approaches the exhaustive
search methods with much lower complexity. In the Serial-Max method, the
antenna pairs with maximum &quot;obtainable SINR&quot; at both ends are selected in a
two-step serial way. The performance of the proposed Serial-Max method is
analyzed, and the closed-form expressions of the average weighted sum rate and
the weighted sum SER are derived. The analysis is validated by simulations.
Both analytical and simulation results show that as the number of antennas
increases, the Serial-Max method approaches the performance of the
exhaustive-search schemes in terms of sum rate and sum SER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07609</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07609</id><created>2015-03-25</created><authors><author><keyname>Liu</keyname><forenames>Yanping</forenames></author><author><keyname>Reichle</keyname><forenames>Erik D.</forenames></author></authors><title>An Evolutionary Algorithm for Error-Driven Learning via Reinforcement</title><categories>cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although different learning systems are coordinated to afford complex
behavior, little is known about how this occurs. This article describes a
theoretical framework that specifies how complex behaviors that might be
thought to require error-driven learning might instead be acquired through
simple reinforcement. This framework includes specific assumptions about the
mechanisms that contribute to the evolution of (artificial) neural networks to
generate topologies that allow the networks to learn large-scale complex
problems using only information about the quality of their performance. The
practical and theoretical implications of the framework are discussed, as are
possible biological analogs of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07612</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07612</id><created>2015-03-25</created><updated>2015-06-22</updated><authors><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>MacCartney</keyname><forenames>George R.</forenames><suffix>Jr</suffix></author></authors><title>Probabilistic Omnidirectional Path Loss Models for Millimeter-Wave
  Outdoor Communications</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, IEEE Wireless Communications Letters (March 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a probabilistic omnidirectional millimeter-wave path
loss model based on real-world 28 GHz and 73 GHz measurements collected in New
York City. The probabilistic path loss approach uses a free space line-of-sight
propagation model, and for non-line-of-sight conditions uses either a close-in
free space reference distance path loss model or a floating-intercept path loss
model. The probabilistic model employs a weighting function that specifies the
line-of-sight probability for a given transmitter-receiver separation distance.
Results show that the probabilistic path loss model offers virtually identical
results whether one uses a non-line-of-sight close-in free space reference
distance path loss model, with a reference distance of 1 meter, or a
floating-intercept path loss model. This letter also shows that site-specific
environmental information may be used to yield the probabilistic weighting
function for choosing between line-of-sight and non-line-of-sight conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07613</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07613</id><created>2015-03-26</created><authors><author><keyname>Fifield</keyname><forenames>David</forenames></author><author><keyname>Follan</keyname><forenames>Torbj&#xf8;rn</forenames></author><author><keyname>Lunde</keyname><forenames>Emil</forenames></author></authors><title>Unsupervised authorship attribution</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We describe a technique for attributing parts of a written text to a set of
unknown authors. Nothing is assumed to be known a priori about the writing
styles of potential authors. We use multiple independent clusterings of an
input text to identify parts that are similar and dissimilar to one another. We
describe algorithms necessary to combine the multiple clusterings into a
meaningful output. We show results of the application of the technique on texts
having multiple writing styles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07619</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07619</id><created>2015-03-26</created><updated>2015-04-17</updated><authors><author><keyname>Javdani</keyname><forenames>Shervin</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha S.</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Shared Autonomy via Hindsight Optimization</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In shared autonomy, user input and robot autonomy are combined to control a
robot to achieve a goal. Often, the robot does not know a priori which goal the
user wants to achieve, and must both predict the user's intended goal, and
assist in achieving that goal. We formulate the problem of shared autonomy as a
Partially Observable Markov Decision Process with uncertainty over the user's
goal. We utilize maximum entropy inverse optimal control to estimate a
distribution over the user's goal based on the history of inputs. Ideally, the
robot assists the user by solving for an action which minimizes the expected
cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal
action is intractable, we use hindsight optimization to approximate the
solution. In a user study, we compare our method to a standard
predict-then-blend approach. We find that our method enables users to
accomplish tasks more quickly while utilizing less input. However, when asked
to rate each system, users were mixed in their assessment, citing a tradeoff
between maintaining control authority and accomplishing tasks quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07621</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07621</id><created>2015-03-26</created><authors><author><keyname>Fu</keyname><forenames>Shuangshuang</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>James</keyname><forenames>Matthew R.</forenames></author></authors><title>The Evolution of Network Entropy in Classical and Quantum Consensus
  Dynamics</title><categories>quant-ph cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the evolution of the network entropy for
consensus dynamics in classical or quantum networks. We show that in the
classical case, the network entropy decreases at the consensus limit if the
node initial values are i.i.d. Bernoulli random variables, and the network
differential entropy is monotonically non-increasing if the node initial values
are i.i.d. Gaussian. While for quantum consensus dynamics, the network's von
Neumann entropy is in contrast non-decreasing. In light of this inconsistency,
we compare several gossiping algorithms with random or deterministic
coefficients for classical or quantum networks, and show that quantum gossiping
algorithms with deterministic coefficients are physically related to classical
gossiping algorithms with random coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07624</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07624</id><created>2015-03-26</created><authors><author><keyname>Shao</keyname><forenames>Leo</forenames></author><author><keyname>Consuegra</keyname><forenames>Mario E.</forenames></author><author><keyname>Rangaswami</keyname><forenames>Raju</forenames></author><author><keyname>Narasimhan</keyname><forenames>Giri</forenames></author></authors><title>Analyzing Adaptive Cache Replacement Strategies</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive Replacement Cache (ARC) and Clock with Adaptive Replacement (CAR)
are state-of-the-art &quot;adaptive&quot; cache replacement algorithms invented to
improve on the shortcomings of classical cache replacement policies such as LRU
and Clock. Both ARC and CAR have been shown to outperform their classical and
popular counterparts in practice. However, for over a decade, no theoretical
proof of the competitiveness of ARC and CAR is known. We prove that for a cache
of size N, (a) ARC is 4N-competitive, and (b) CAR is 21N-competitive, thus
proving that no &quot;pathological&quot; worst-case request sequence exists that could
make them perform much worse than LRU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07626</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07626</id><created>2015-03-26</created><authors><author><keyname>Bychkov</keyname><forenames>Igor</forenames></author><author><keyname>Ruzhnikov</keyname><forenames>Gennady</forenames></author><author><keyname>Fedorov</keyname><forenames>Roman</forenames></author><author><keyname>Shumilov</keyname><forenames>Alexander</forenames></author></authors><title>Building the distributed WPS-services execution environment</title><categories>cs.SE</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes the environment of WPS-based (Web Processing Service)
distributed services, that uses scenarios in JavaScript programming language in
order to integrate services with each other. The environment standardizes data
processing procedures, stores all services-related information and offers the
set of basic WPS-services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07628</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07628</id><created>2015-03-26</created><authors><author><keyname>Zhang</keyname><forenames>Kaiqing</forenames></author><author><keyname>Hu</keyname><forenames>Hong</forenames></author><author><keyname>Dai</keyname><forenames>Wenhan</forenames></author><author><keyname>Shen</keyname><forenames>Yuan</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Indoor Localization Algorithm For Smartphones</title><categories>cs.NI</categories><comments>17 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Increasing sources of sensor measurements and prior knowledge have become
available for indoor localization on smartphones. How to effectively utilize
these sources for enhancing localization accuracy is an important yet
challenging problem. In this paper, we present an area state-aided localization
algorithm that exploits various sources of information. Specifically, we
introduce the concept of area state to indicate the area where the user is on
an indoor map. The position of the user is then estimated using inertial
measurement unit (IMU) measurements with the aid of area states. The area
states are in turn updated based on the position estimates. To avoid
accumulated errors of IMU measurements, our algorithm uses WiFi received signal
strength indicator (RSSI) to indicate the vicinity of the user to the routers.
The experiment results show that our system can achieve satisfactory
localization accuracy in a typical indoor environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07630</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07630</id><created>2015-03-26</created><authors><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhan</keyname><forenames>Xiu-Xiu</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Sun</keyname><forenames>Gui-Quan</forenames></author><author><keyname>Hui</keyname><forenames>Pak Ming</forenames></author></authors><title>Events Determine Spreading Patterns: Information Transmission via
  Internal and External Influences on Social Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1088/1367-2630/17/11/113045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, information transmission models motivated by the classical epidemic
propagation, have been applied to a wide-range of social systems, generally
assume that information mainly transmits among individuals via peer-to-peer
interactions on social networks. In this paper, we consider one more approach
for users to get information: the out-of-social-network influence. Empirical
analyses of eight typical events' diffusion on a very large micro-blogging
system, \emph{Sina Weibo}, show that the external influence has significant
impact on information spreading along with social activities. In addition, we
propose a theoretical model to interpret the spreading process via both
internal and external channels, considering three essential properties: (i)
memory effect; (ii) role of spreaders; and (iii) non-redundancy of contacts.
Experimental and mathematical results indicate that the information indeed
spreads much quicker and broader with mutual effects of the internal and
external influences. More importantly, the present model reveals that the event
characteristic would highly determine the essential spreading patterns once the
network structure is established. The results may shed some light on the
in-depth understanding of the underlying dynamics of information transmission
on real social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07640</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07640</id><created>2015-03-26</created><authors><author><keyname>Chen</keyname><forenames>Qinqin</forenames></author><author><keyname>Zhao</keyname><forenames>Hui</forenames></author><author><keyname>Li</keyname><forenames>Lin</forenames></author><author><keyname>Long</keyname><forenames>Hang</forenames></author><author><keyname>Wang</keyname><forenames>Jianquan</forenames></author><author><keyname>Hou</keyname><forenames>Xiaoyue</forenames></author></authors><title>A Closed-Loop UL Power Control Scheme for Interference Mitigation in
  Dynamic TD-LTE Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 4 figures,conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The TD-LTE system is envisaged to adopt dynamic time division duplexing (TDD)
transmissions for small cells to adapt their communication service to the fast
variation of downlink (DL) and uplink (UL) traffic demands. However, different
DL/UL directions for the same subframe in adjacent cells will result in new
destructive interference components, i.e., eNB-to-eNB and UE-to-UE, with levels
that can significantly differ from one subframe to another. In this paper, a
feasible UL power control mechanism is proposed to manage eNB-to-eNB
interference, where different UL power control parameters are set based on
different interference level. We consider the geometric location information
and the subframe set selection process about adjacent eNBs when the
interference level is estimated. The performance of the proposed scheme is
evaluated through system level simulations and it is shown that the scheme can
achieve preferable improvement in terms of UL average and 5%-ile packet
throughputs compared with the original scheme without power control. Also, the
UE-to-UE interference is not worse when the UE transmit power become higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07645</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07645</id><created>2015-03-26</created><authors><author><keyname>Sabri</keyname><forenames>Khair Eddin</forenames></author></authors><title>Automated Verification Of Role-Based Access Control Policies Constraints
  Using Prover9</title><categories>cs.CR</categories><comments>10 pages in International Journal of Security, Privacy and Trust
  Management (IJSPTM) Vol 4, No 1, February 2015</comments><doi>10.5121/ijsptm.2015.4101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access control policies are used to restrict access to sensitive records for
authorized users only. One approach for specifying policies is using role based
access control (RBAC) where authorization is given to roles instead of users.
Users are assigned to roles such that each user can access all the records that
are allowed to his/her role. RBAC has a great interest because of its
flexibility. One issue in RBAC is dealing with constraints. Usually, policies
should satisfy pre-defined constraints as for example separation of duty (SOD)
which states that users are not allowed to play two conflicting roles.
Verifying the satisfiability of constraints based on policies is time consuming
and may lead to errors. Therefore, an automated verification is essential. In
this paper, we propose a theory for specifying policies and constraints in
first order logic. Furthermore, we present a comprehensive list of constraints.
We identity constraints based on the relation between users and roles, between
roles and permission on records, between users and permission on records, and
between users, roles, and permission on records. Then, we use a general purpose
theorem prover tool called Prover9 for proving the satisfaction of constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07652</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07652</id><created>2015-03-26</created><updated>2015-04-23</updated><authors><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Upper Bound on the Capacity of a Cascade of Nonlinear and Noisy Channels</title><categories>cs.IT math.IT</categories><comments>The main change is to define the noise as bandlimited already in (8)
  rather than before (15). This serves to clarify subsequent steps</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An upper bound on the capacity of a cascade of nonlinear and noisy channels
is presented. The cascade mimics the split-step Fourier method for computing
waveform propagation governed by the stochastic generalized nonlinear
Schroedinger equation. It is shown that the spectral efficiency of the cascade
is at most log(1+SNR), where SNR is the receiver signal-to-noise ratio. The
results may be applied to optical fiber channels. However, the definition of
bandwidth is subtle and leaves open interpretations of the bound. Some of these
interpretations are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07653</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07653</id><created>2015-03-26</created><updated>2015-03-28</updated><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Sequence complexity and work extraction</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>17 pages, 1 figure. Submitted for publication. Results of section 6
  were improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simplified version of a solvable model by Mandal and Jarzynski,
which constructively demonstrates the interplay between work extraction and the
increase of the Shannon entropy of an information reservoir which is in contact
with the physical system. We extend Mandal and Jarzynski's main findings in
several directions: First, we allow sequences of correlated bits rather than
just independent bits. Secondly, at least for the case of binary information,
we show that, in fact, the Shannon entropy is only one measure of complexity of
the information that must increase in order for work to be extracted. The
extracted work can also be upper bounded in terms of the increase in other
quantities that measure complexity, like the predictability of future bits from
past ones. Third, we provide an extension to the case of non-binary information
(i.e., a larger alphabet), and finally, we extend the scope to the case where
the incoming bits (before the interaction) form an individual sequence, rather
than a random one. In this case, the entropy before the interaction can be
replaced by the Lempel-Ziv (LZ) complexity of the incoming sequence, a fact
that gives rise to an entropic meaning of the LZ complexity, not only in
information theory, but also in physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07659</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07659</id><created>2015-03-26</created><updated>2015-05-16</updated><authors><author><keyname>Kl&#xf6;ckner</keyname><forenames>Andreas</forenames></author></authors><title>Loo.py: From Fortran to performance via transformation and substitution
  rules</title><categories>cs.PL cs.CE cs.MS</categories><comments>ARRAY 2015 - 2nd ACM SIGPLAN International Workshop on Libraries,
  Languages and Compilers for Array Programming (ARRAY 2015)</comments><acm-class>D.3.4, D.1.3, G.4</acm-class><doi>10.1145/2774959.2774969</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large amount of numerically-oriented code is written and is being written
in legacy languages. Much of this code could, in principle, make good use of
data-parallel throughput-oriented computer architectures. Loo.py, a
transformation-based programming system targeted at GPUs and general
data-parallel architectures, provides a mechanism for user-controlled
transformation of array programs. This transformation capability is designed to
not just apply to programs written specifically for Loo.py, but also those
imported from other languages such as Fortran. It eases the trade-off between
achieving high performance, portability, and programmability by allowing the
user to apply a large and growing family of transformations to an input
program. These transformations are expressed in and used from Python and may be
applied from a variety of settings, including a pragma-like manner from other
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07680</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07680</id><created>2015-03-26</created><authors><author><keyname>Bras</keyname><forenames>Florent Le</forenames></author><author><keyname>Hamel</keyname><forenames>Tarek</forenames></author><author><keyname>Mahony</keyname><forenames>Robert</forenames></author><author><keyname>Samson</keyname><forenames>Claude</forenames></author></authors><title>Observer design for position and velocity bias estimation from a single
  direction output</title><categories>cs.SY</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of estimating the position of an object
moving in $R^n$ from direction and velocity measurements. After addressing
observability issues associated with this problem, a nonlinear observer is
designed so as to encompass the case where the measured velocity is corrupted
by a constant bias. Global exponential convergence of the estimation error is
proved under a condition of persistent excitation upon the direction
measurements. Simulation results illustrate the performance of the observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07691</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07691</id><created>2015-03-26</created><updated>2015-06-08</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Pr&#x16b;sis</keyname><forenames>Kri&#x161;j&#x101;nis</forenames></author><author><keyname>Vihrovs</keyname><forenames>Jevg&#x113;nijs</forenames></author></authors><title>Sensitivity versus Certificate Complexity of Boolean Functions</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity, block sensitivity and certificate complexity are basic
complexity measures of Boolean functions. The famous sensitivity conjecture
claims that sensitivity is polynomially related to block sensitivity. However,
it has been notoriously hard to obtain even exponential bounds. Since block
sensitivity is known to be polynomially related to certificate complexity, an
equivalent of proving this conjecture would be showing that certificate
complexity is polynomially related to sensitivity. Previously, it has been
shown that $bs(f) \leq C(f) \leq 2^{s(f)-1} s(f) - (s(f)-1)$. In this work, we
give a better upper bound of $bs(f) \leq C(f) \leq
\max\left(2^{s(f)-1}\left(s(f)-\frac 1 3\right), s(f)\right)$ using a recent
theorem limiting the structure of function graphs. We also examine relations
between these measures for functions with small 1-sensitivity $s_1(f)$ and
arbitrary 0-sensitivity $s_0(f)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07692</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07692</id><created>2015-03-26</created><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Sampled-data $H^{\infty}$ Optimization for Self-interference Suppression
  in Baseband Signal Subspaces</title><categories>cs.SY</categories><comments>submitted; 6pages, 13 figures. arXiv admin note: text overlap with
  arXiv:1503.02379</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a design method of selfinterference cancelers for
wireless relay stations taking account of the baseband signal subspace. The
problem is first formulated as a sampled-data $H^{\infty}$ control problem with
a generalized sampler and a generalized hold, which can be reduced to a
discretetime $\ell^2$-induced norm minimization problem. Taking account of the
implementation of the generalized sampler and hold, we adopt the filter-sampler
structure for the generalized sampler, and the uspampler-filter-hold structure
for the generalized hold. Under these implementation constraints, we
reformulate the problem as a standard discrete-time $H^{\infty}$ control
problem by using the discrete-time lifting technique. A simulation result is
shown to illustrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07693</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07693</id><created>2015-03-26</created><updated>2015-04-02</updated><authors><author><keyname>Talebi</keyname><forenames>Mahmoud</forenames></author><author><keyname>Groote</keyname><forenames>Jan Friso</forenames></author><author><keyname>Linnartz</keyname><forenames>Jean-Paul</forenames></author></authors><title>Communication Patterns in Mean Field Models for Wireless Sensor Networks</title><categories>cs.PF</categories><comments>22 pages, in LNCS format, Submitted to QEST'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are usually composed of a large number of nodes, and
with the increasing processing power and power consumption efficiency they are
expected to run more complex protocols in the future. These pose problems in
the field of verification and performance evaluation of wireless networks. In
this paper, we tailor the mean-field theory as a modeling technique to analyze
their behavior. We apply this method to the slotted ALOHA protocol, and
establish results on the long term trends of the protocol within a very large
network, specially regarding the stability of ALOHA-type protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07697</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07697</id><created>2015-03-26</created><authors><author><keyname>Florea</keyname><forenames>Laura</forenames></author><author><keyname>Florea</keyname><forenames>Corneliu</forenames></author><author><keyname>Vertan</keyname><forenames>Constantin</forenames></author></authors><title>Robust Eye Centers Localization with Zero--Crossing Encoded Image
  Projections</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new framework for the eye centers localization by the
joint use of encoding of normalized image projections and a Multi Layer
Perceptron (MLP) classifier. The encoding is novel and it consists in
identifying the zero-crossings and extracting the relevant parameters from the
resulting modes. The compressed normalized projections produce feature
descriptors that are inputs to a properly-trained MLP, for discriminating among
various categories of image regions. The proposed framework forms a fast and
reliable system for the eye centers localization, especially in the context of
face expression analysis in unconstrained environments. We successfully test
the proposed method on a wide variety of databases including BioID,
Cohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07705</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07705</id><created>2015-03-26</created><authors><author><keyname>Garc&#xed;a-Marco</keyname><forenames>Ignacio</forenames><affiliation>LIP</affiliation></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames><affiliation>LIP</affiliation></author><author><keyname>Tavenas</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Log-concavity and lower bounds for arithmetic circuits</title><categories>cs.CC cs.DM math.AC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One question that we investigate in this paper is, how can we build
log-concave polynomials using sparse polynomials as building blocks? More
precisely, let $f = \sum\_{i = 0}^d a\_i X^i \in \mathbb{R}^+[X]$ be a
polynomial satisfying the log-concavity condition $a\_i^2 \textgreater{} \tau
a\_{i-1}a\_{i+1}$ for every $i \in \{1,\ldots,d-1\},$ where $\tau
\textgreater{} 0$. Whenever $f$ can be written under the form $f = \sum\_{i =
1}^k \prod\_{j = 1}^m f\_{i,j}$ where the polynomials $f\_{i,j}$ have at most
$t$ monomials, it is clear that $d \leq k t^m$. Assuming that the $f\_{i,j}$
have only non-negative coefficients, we improve this degree bound to $d =
\mathcal O(k m^{2/3} t^{2m/3} {\rm log^{2/3}}(kt))$ if $\tau \textgreater{} 1$,
and to $d \leq kmt$ if $\tau = d^{2d}$.
  This investigation has a complexity-theoretic motivation: we show that a
suitable strengthening of the above results would imply a separation of the
algebraic complexity classes VP and VNP. As they currently stand, these results
are strong enough to provide a new example of a family of polynomials in VNP
which cannot be computed by monotone arithmetic circuits of polynomial size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07706</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07706</id><created>2015-03-26</created><authors><author><keyname>Florea</keyname><forenames>Corneliu</forenames></author><author><keyname>Florea</keyname><forenames>Laura</forenames></author><author><keyname>Boia</keyname><forenames>Raluca</forenames></author><author><keyname>Bandrabur</keyname><forenames>Alessandra</forenames></author><author><keyname>Vertan</keyname><forenames>Constantin</forenames></author></authors><title>Pain Intensity Estimation by a Self--Taught Selection of Histograms of
  Topographical Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pain assessment through observational pain scales is necessary for special
categories of patients such as neonates, patients with dementia, critically ill
patients, etc. The recently introduced Prkachin-Solomon score allows pain
assessment directly from facial images opening the path for multiple assistive
applications. In this paper, we introduce the Histograms of Topographical (HoT)
features, which are a generalization of the topographical primal sketch, for
the description of the face parts contributing to the mentioned score. We
propose a semi-supervised, clustering oriented self--taught learning procedure
developed on the emotion oriented Cohn-Kanade database. We use this procedure
to improve the discrimination between different pain intensity levels and the
generalization with respect to the monitored persons, while testing on the UNBC
McMaster Shoulder Pain database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07711</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07711</id><created>2015-03-26</created><authors><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Abisheva</keyname><forenames>Adiya</forenames></author><author><keyname>Schweighofer</keyname><forenames>Simon</forenames></author><author><keyname>Serd&#xfc;lt</keyname><forenames>Uwe</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Ideological and Temporal Components of Network Polarization in Online
  Political Participatory Media</title><categories>stat.AP cs.SI physics.soc-ph</categories><comments>35 pages, 11 figures, Internet, Policy &amp; Politics Conference,
  University of Oxford, Oxford, UK, 25-26 September 2014</comments><journal-ref>Policy &amp; Internet, 7(1) (2015)</journal-ref><doi>10.1002/poi3.82</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Political polarization is traditionally analyzed through the ideological
stances of groups and parties, but it also has a behavioral component that
manifests in the interactions between individuals. We present an empirical
analysis of the digital traces of politicians in politnetz.ch, a Swiss online
platform focused on political activity, in which politicians interact by
creating support links, comments, and likes. We analyze network polarization as
the level of intra- party cohesion with respect to inter-party connectivity,
finding that supports show a very strongly polarized structure with respect to
party alignment. The analysis of this multiplex network shows that each layer
of interaction contains relevant information, where comment groups follow
topics related to Swiss politics. Our analysis reveals that polarization in the
layer of likes evolves in time, increasing close to the federal elections of
2011. Furthermore, we analyze the internal social network of each party through
metrics related to hierarchical structures, information efficiency, and social
resilience. Our results suggest that the online social structure of a party is
related to its ideology, and reveal that the degree of connectivity across two
parties increases when they are close in the ideological space of a multi-party
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07713</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07713</id><created>2015-02-12</created><authors><author><keyname>Bahramnejad</keyname><forenames>Pedram</forenames></author><author><keyname>Sharafi</keyname><forenames>Sayed Mehran</forenames></author><author><keyname>Nabiollahi</keyname><forenames>Akbar</forenames></author></authors><title>A method for business process reengineering based on enterprise ontology</title><categories>cs.SE</categories><comments>15 pages, published in IJSEA</comments><doi>10.5121/ijsea.2015.6103</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Business Process Reengineering increases enterprise's chance to survive in
competition among organizations , but failure rate among reengineering efforts
is high, so new methods that decrease failure, are needed, in this paper a
business process reengineering method is presented that uses Enterprise
Ontology for modelling the current system and its goal is to improve analysing
current system and decreasing failure rate of BPR, and cost and time of
performing processes, In this method instead of just modelling processes,
processes with their : interactions and relations, environment, staffs and
customers will be modelled in enterprise ontology. Also in choosing processes
for reengineering step, after choosing them, processes which, according to the
enterprise ontology, has the most connection with the chosen ones, will also be
chosen to reengineer, finally this method is implemented on a company and As-Is
and To-Be processes are simulated and compared by ARIS tools, Report and
Simulation Experiment
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07715</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07715</id><created>2014-12-24</created><authors><author><keyname>Kovach</keyname><forenames>Daniel</forenames></author></authors><title>The Computational Theory of Intelligence: Data Aggregation</title><categories>cs.AI</categories><comments>Published in IJMNTA</comments><msc-class>68T27, 97C40, 97R40</msc-class><acm-class>I.2.0; I.2.1; I.2.11</acm-class><doi>10.4236/ijmnta.2014.34016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will expound upon the concepts proffered in [1], where we
proposed an information theoretic approach to intelligence in the computational
sense. We will examine data and meme aggregation, and study the effect of
limited resources on the resulting meme amplitudes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07716</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07716</id><created>2015-03-26</created><authors><author><keyname>Venugopal</keyname><forenames>Gayatri</forenames></author></authors><title>A Review of Popular Applications on Google Play - Do They Cater to
  Visually Impaired Users?</title><categories>cs.HC</categories><comments>19 pages, 1 figure, 3 tables</comments><journal-ref>Indian Journal of Science and Technology, Vol 8 S4, 221 to 239,
  February 2015</journal-ref><doi>10.17485/ijst/2015/v8iS4/61436</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of applications on online mobile application stores is increasing
at a rapid rate. Smart-phones are used by a wide range of people varying in
age, and also in the ability to use a smart phone. With the increasing
dependency on smart-phones, the paper aims to determine whether the popular
applications on Google Play, the official store for Android applications, can
be used by people with vision impairment. The accessibility of the applications
was tested using an external keyboard, and TalkBack, an accessibility tool
developed by Google. It was found that several popular applications on the
store were not designed keeping accessibility in mind. It was observed that
there exists a weak positive relationship between the popularity of the
application and its accessibility. A framework is proposed that can be used by
developers to improve the accessibility of an application. The paper also
discusses the programming aspects to be considered while developing an Android
application, so that the application can be used by sighted as well as visually
impaired users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07717</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07717</id><created>2015-03-26</created><authors><author><keyname>Lef&#xe8;vre</keyname><forenames>Claire</forenames></author><author><keyname>B&#xe9;atrix</keyname><forenames>Christopher</forenames></author><author><keyname>St&#xe9;phan</keyname><forenames>Igor</forenames></author><author><keyname>Garcia</keyname><forenames>Laurent</forenames></author></authors><title>ASPeRiX, a First Order Forward Chaining Approach for Answer Set
  Computing</title><categories>cs.LO cs.AI</categories><comments>49 pages. To appear in Theory and Practice of Logic Programming
  (TPLP)</comments><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The natural way to use Answer Set Programming (ASP) to represent knowledge in
Artificial Intelligence or to solve a combinatorial problem is to elaborate a
first order logic program with default negation. In a preliminary step this
program with variables is translated in an equivalent propositional one by a
first tool: the grounder. Then, the propositional program is given to a second
tool: the solver. This last one computes (if they exist) one or many answer
sets (stable models) of the program, each answer set encoding one solution of
the initial problem. Until today, almost all ASP systems apply this two steps
computation. In this article, the project ASPeRiX is presented as a first order
forward chaining approach for Answer Set Computing. This project was amongst
the first to introduce an approach of answer set computing that escapes the
preliminary phase of rule instantiation by integrating it in the search
process. The methodology applies a forward chaining of first order rules that
are grounded on the fly by means of previously produced atoms. Theoretical
foundations of the approach are presented, the main algorithms of the ASP
solver ASPeRiX are detailed and some experiments and comparisons with existing
systems are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07722</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07722</id><created>2014-12-18</created><authors><author><keyname>Parisi</keyname><forenames>Daniel R.</forenames></author><author><keyname>Negri</keyname><forenames>Pablo A.</forenames></author></authors><title>Sequential evacuation strategy for multiple rooms toward the same means
  of egress</title><categories>physics.soc-ph cs.MA</categories><comments>8 pages, 4 figures</comments><proxy>Luis Pugnaloni</proxy><journal-ref>Papers in Physics 6, 060013 (2014)</journal-ref><doi>10.4279/PIP.060013</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper examines different evacuation strategies for systems where several
rooms evacuate trough the same means of egress, using microscopic pedestrian
simulation.As a case study, a medium-rise office building is considered. It was
found that the standard strategy, whereby the simultaneous evacuation of all
levels is performed, can be improved by a sequential evacuation, beginning with
the lowest floor and continuing successively with each one of the upper floors
after a certain delay. The importance of the present research is that it
provides the basis for the design and implementation of new evacuation
strategies and alarm systems that could significantly improve the evacuation of
multiple rooms trough a common means of escape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07723</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07723</id><created>2015-03-26</created><authors><author><keyname>Kling</keyname><forenames>Christoph Carl</forenames></author><author><keyname>Kunegis</keyname><forenames>Jerome</forenames></author><author><keyname>Hartmann</keyname><forenames>Heinrich</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Staab</keyname><forenames>Steffen</forenames></author></authors><title>Voting Behaviour and Power in Online Democracy: A Study of
  LiquidFeedback in Germany's Pirate Party</title><categories>cs.CY cs.SI</categories><comments>11 pages, 11 figures, appeared in ICWSM 2015</comments><acm-class>K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, political parties have adopted Online Delegative Democracy
platforms such as LiquidFeedback to organise themselves and their political
agendas via a grassroots approach. A common objection against the use of these
platforms is the delegation system, where a user can delegate his vote to
another user, giving rise to so-called super-voters, i.e. powerful users who
receive many delegations. It has been asserted in the past that the presence of
these super-voters undermines the democratic process, and therefore delegative
democracy should be avoided. In this paper, we look at the emergence of
super-voters in the largest delegative online democracy platform worldwide,
operated by Germany's Pirate Party. We investigate the distribution of power
within the party systematically, study whether super-voters exist, and explore
the influence they have on the outcome of votings conducted online. While we
find that the theoretical power of super-voters is indeed high, we also observe
that they use their power wisely. Super-voters do not fully act on their power
to change the outcome of votes, but they vote in favour of proposals with the
majority of voters in many cases thereby exhibiting a stabilising effect on the
system. We use these findings to present a novel class of power indices that
considers observed voting biases and gives significantly better predictions
than state-of-the-art measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07737</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07737</id><created>2015-03-19</created><authors><author><keyname>Roy</keyname><forenames>Sohon</forenames></author></authors><title>Business Rule Mining from Spreadsheets</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Business rules represent the knowledge that guides the operations of a
business organization. They are implemented in software applications used by
organizations, and the activity of extracting them from software is known as
business rule mining. It has various purposes amongst which migration and
generating documentation are the most common. However, apart from conventional
software, organizations also use spreadsheets for a large part of their
operations and decision-making activities. Therefore we believe that
spreadsheets are also rich in business rules. We thus propose to develop an
automated system for extracting business rules from spreadsheets in a human
comprehensible natural language format. This position paper describes our
motivation, the problem description, related work, and challenges we foresee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07757</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07757</id><created>2015-03-26</created><updated>2015-08-13</updated><authors><author><keyname>Lengyel</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Varga</keyname><forenames>Attila</forenames></author><author><keyname>S&#xe1;gv&#xe1;ri</keyname><forenames>Bence</forenames></author><author><keyname>Jakobi</keyname><forenames>&#xc1;kos</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Geographies of an online social network</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages pdf file. Minor changes mainly about users abroad plus maps
  redrawn</comments><doi>10.1371/journal.pone.0137248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How is online social media activity structured in the geographical space?
Recent studies have shown that in spite of earlier visions about the &quot;death of
distance&quot;, physical proximity is still a major factor in social tie formation
and maintenance in virtual social networks. Yet, it is unclear, what are the
characteristics of the distance dependence in online social networks. In order
to explore this issue the complete network of the former major Hungarian online
social network is analyzed. We find that the distance dependence is weaker for
the online social network ties than what was found earlier for phone
communication networks. For a further analysis we introduced a coarser
granularity: We identified the settlements with the nodes of a network and
assigned two kinds of weights to the links between them. When the weights are
proportional to the number of contacts we observed weakly formed, but spatially
based modules resembling to the borders of macro-regions, the highest level of
regional administration in the country. If the weights are defined relative to
an uncorrelated null model, the next level of administrative regions, counties
are reflected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07759</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07759</id><created>2015-03-26</created><updated>2016-02-22</updated><authors><author><keyname>Pedersen</keyname><forenames>Edvard</forenames></author><author><keyname>Bongo</keyname><forenames>Lars Ailo</forenames></author></authors><title>Large-scale Biological Meta-database Management</title><categories>cs.DC cs.DB</categories><comments>10 pages, 6 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Up-to-date meta-databases are vital for the analysis of biological data.
However,the current exponential increase in biological data leads to
exponentially increasing meta-database sizes. Large-scale meta-database
management is therefore an important challenge for production platforms
providing services for biological data analysis. In particular, there is often
a need either to run an analysis with a particular version of a meta-database,
or to rerun an analysis with an updated meta-database. We present our GeStore
approach for biological meta-database management. It provides efficient storage
and runtime generation of specific meta-database versions, and efficient
incremental updates for biological data analysis tools. The approach is
transparent to the tools, and we provide a framework that makes it easy to
integrate GeStore with biological data analysis frameworks. We present the
GeStore system, an evaluation of the performance characteristics of the system,
and an evaluation of the benefits for a biological data analysis workflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07768</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07768</id><created>2015-03-26</created><authors><author><keyname>Davarpanah</keyname><forenames>Kourosh</forenames></author><author><keyname>Kaufman</keyname><forenames>Dan</forenames></author><author><keyname>Pubellier</keyname><forenames>Ophelie</forenames></author></authors><title>NeuCoin: the First Secure, Cost-efficient and Decentralized
  Cryptocurrency</title><categories>cs.CR</categories><comments>39 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NeuCoin is a decentralized peer-to-peer cryptocurrency derived from Sunny
King's Peercoin, which itself was derived from Satoshi Nakamoto's Bitcoin. As
with Peercoin, proof-of-stake replaces proof-of-work as NeuCoin's security
model, effectively replacing the operating costs of Bitcoin miners
(electricity, computers) with the capital costs of holding the currency.
Proof-of-stake also avoids proof-of-work's inherent tendency towards
centralization resulting from competition for coinbase rewards among miners
based on lowest cost electricity and hash power.
  NeuCoin increases security relative to Peercoin and other existing
proof-of-stake currencies in numerous ways, including: (1) incentivizing nodes
to continuously stake coins over time through substantially higher mining
rewards and lower minimum stake age; (2) abandoning the use of coin age in the
mining formula; (3) causing the stake modifier parameter to change over time
for each stake; and (4) utilizing a client that punishes nodes that attempt to
mine on multiple branches with duplicate stakes.
  This paper demonstrates how NeuCoin's proof-of-stake implementation addresses
all commonly raised &quot;nothing at stake&quot; objections to generic proof-of-stake
systems. It also reviews many of the flaws of proof-of-work designs to
highlight the potential for an alternate cryptocurrency that solves these
flaws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07780</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07780</id><created>2015-03-26</created><authors><author><keyname>Deville</keyname><forenames>Sylvain</forenames></author><author><keyname>Stevenson</keyname><forenames>Adam J.</forenames></author></authors><title>Mapping ceramics research and its evolution</title><categories>cond-mat.mtrl-sci cs.DL</categories><comments>15 pages, 8 figures, 4 supplementary materials figures. We are
  looking for a journal to publish this paper. If you are an editor and are
  interested that we submit this paper to your journal, please contact us</comments><journal-ref>Journal of the American Ceramic Society Volume 98, Issue 8, pages
  2324-2332, August 2015</journal-ref><doi>10.1111/jace.13699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show here how a simple data mining of bibliographic records can be used to
follow and help understand the evolution of a research domain, at a level that
cannot be captured by reading individual papers in a field of this size. We
illustrate the approach by investigating 43 years of research on ceramic
materials, covered by 253k bibliographic records. The patterns of keywords used
reveal the trends and the evolution of research ideas and priorities within the
field. Simple, interactive tools based on co-word network analysis help us
better appreciate the organization and relationships of ideas or individuals,
and hopefully allow identification of unexplored concepts, connections, or
approaches on a given topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07783</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07783</id><created>2015-03-26</created><authors><author><keyname>Saeedan</keyname><forenames>Faraz</forenames></author><author><keyname>Caputo</keyname><forenames>Barbara</forenames></author></authors><title>Towards Learning free Naive Bayes Nearest Neighbor-based Domain
  Adaptation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As of today, object categorization algorithms are not able to achieve the
level of robustness and generality necessary to work reliably in the real
world. Even the most powerful convolutional neural network we can train fails
to perform satisfactorily when trained and tested on data from different
databases. This issue, known as domain adaptation and/or dataset bias in the
literature, is due to a distribution mismatch between data collections. Methods
addressing it go from max-margin classifiers to learning how to modify the
features and obtain a more robust representation. Recent work showed that by
casting the problem into the image-to-class recognition framework, the domain
adaptation problem is significantly alleviated \cite{danbnn}. Here we follow
this approach, and show how a very simple, learning free Naive Bayes Nearest
Neighbor (NBNN)-based domain adaptation algorithm can significantly alleviate
the distribution mismatch among source and target data, especially when the
number of classes and the number of sources grow. Experiments on standard
benchmarks used in the literature show that our approach (a) is competitive
with the current state of the art on small scale problems, and (b) achieves the
current state of the art as the number of classes and sources grows, with
minimal computational requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07790</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07790</id><created>2015-03-26</created><authors><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Tim</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Transductive Multi-label Zero-shot Learning</title><categories>cs.LG cs.CV</categories><comments>12 pages, 6 figures, Accepted to BMVC 2014 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-shot learning has received increasing interest as a means to alleviate
the often prohibitive expense of annotating training data for large scale
recognition problems. These methods have achieved great success via learning
intermediate semantic representations in the form of attributes and more
recently, semantic word vectors. However, they have thus far been constrained
to the single-label case, in contrast to the growing popularity and importance
of more realistic multi-label data. In this paper, for the first time, we
investigate and formalise a general framework for multi-label zero-shot
learning, addressing the unique challenge therein: how to exploit multi-label
correlation at test time with no training data for those classes? In
particular, we propose (1) a multi-output deep regression model to project an
image into a semantic word space, which explicitly exploits the correlations in
the intermediate semantic layer of word vectors; (2) a novel zero-shot learning
algorithm for multi-label data that exploits the unique compositionality
property of semantic word vector representations; and (3) a transductive
learning strategy to enable the regression model learned from seen classes to
generalise well to unseen classes. Our zero-shot learning experiments on a
number of standard multi-label datasets demonstrate that our method outperforms
a variety of baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07792</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07792</id><created>2015-03-26</created><updated>2015-10-09</updated><authors><author><keyname>Hammer</keyname><forenames>Matthew A.</forenames></author><author><keyname>Dunfield</keyname><forenames>Joshua</forenames></author><author><keyname>Headley</keyname><forenames>Kyle</forenames></author><author><keyname>Labich</keyname><forenames>Nicholas</forenames></author><author><keyname>Foster</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Hicks</keyname><forenames>Michael</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Incremental Computation with Names</title><categories>cs.PL</categories><comments>OOPSLA '15, October 25-30, 2015, Pittsburgh, PA, USA</comments><acm-class>D.3.1; D.3.3; F.3.2</acm-class><doi>10.1145/2814270.2814305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past thirty years, there has been significant progress in developing
general-purpose, language-based approaches to incremental computation, which
aims to efficiently update the result of a computation when an input is
changed. A key design challenge in such approaches is how to provide efficient
incremental support for a broad range of programs. In this paper, we argue that
first-class names are a critical linguistic feature for efficient incremental
computation. Names identify computations to be reused across differing runs of
a program, and making them first class gives programmers a high level of
control over reuse. We demonstrate the benefits of names by presenting NOMINAL
ADAPTON, an ML-like language for incremental computation with names. We
describe how to use NOMINAL ADAPTON to efficiently incrementalize several
standard programming patterns---including maps, folds, and unfolds---and show
how to build efficient, incremental probabilistic trees and tries. Since
NOMINAL ADAPTON's implementation is subtle, we formalize it as a core calculus
and prove it is from-scratch consistent, meaning it always produces the same
answer as simply re-running the computation. Finally, we demonstrate that
NOMINAL ADAPTON can provide large speedups over both from-scratch computation
and ADAPTON, a previous state-of-the-art incremental computation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07793</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07793</id><created>2015-03-26</created><updated>2015-03-27</updated><authors><author><keyname>Das</keyname><forenames>Srinjoy</forenames></author><author><keyname>Pedroni</keyname><forenames>Bruno Umbria</forenames></author><author><keyname>Merolla</keyname><forenames>Paul</forenames></author><author><keyname>Arthur</keyname><forenames>John</forenames></author><author><keyname>Cassidy</keyname><forenames>Andrew S.</forenames></author><author><keyname>Jackson</keyname><forenames>Bryan L.</forenames></author><author><keyname>Modha</keyname><forenames>Dharmendra</forenames></author><author><keyname>Cauwenberghs</keyname><forenames>Gert</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Ken</forenames></author></authors><title>Gibbs Sampling with Low-Power Spiking Digital Neurons</title><categories>cs.NE</categories><comments>Accepted at ISCAS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines and Deep Belief Networks have been successfully
used in a wide variety of applications including image classification and
speech recognition. Inference and learning in these algorithms uses a Markov
Chain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms
the kernel of this sampler which can be realized from the firing statistics of
noisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper
demonstrates such an implementation on an array of digital spiking neurons with
stochastic leak and threshold properties for inference tasks and presents some
key performance metrics for such a hardware-based sampler in both the
generative and discriminative contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07795</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07795</id><created>2015-03-26</created><authors><author><keyname>Cotha</keyname><forenames>Naveen Kumar Parachur</forenames></author><author><keyname>Sokolova</keyname><forenames>Marina</forenames></author></authors><title>Multi-Labeled Classification of Demographic Attributes of Patients: a
  case study of diabetics patients</title><categories>cs.LG</categories><comments>16 pages, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated learning of patients demographics can be seen as multi-label
problem where a patient model is based on different race and gender groups. The
resulting model can be further integrated into Privacy-Preserving Data Mining,
where it can be used to assess risk of identification of different patient
groups. Our project considers relations between diabetes and demographics of
patients as a multi-labelled problem. Most research in this area has been done
as binary classification, where the target class is finding if a person has
diabetes or not. But very few, and maybe no work has been done in multi-labeled
analysis of the demographics of patients who are likely to be diagnosed with
diabetes. To identify such groups, we applied ensembles of several multi-label
learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07798</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07798</id><created>2015-03-26</created><authors><author><keyname>Gerola</keyname><forenames>M.</forenames></author><author><keyname>Santuari</keyname><forenames>M.</forenames></author><author><keyname>Salvadori</keyname><forenames>E.</forenames></author><author><keyname>Salsano</keyname><forenames>S.</forenames></author><author><keyname>Campanella</keyname><forenames>M.</forenames></author><author><keyname>Ventre</keyname><forenames>P. L.</forenames></author><author><keyname>Al-Shabibi</keyname><forenames>A.</forenames></author><author><keyname>Snow</keyname><forenames>W.</forenames></author></authors><title>ICONA: Inter Cluster ONOS Network Application</title><categories>cs.NI</categories><comments>Paper submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several Network Operating Systems (NOS) have been proposed in the last few
years for Software Defined Networks; however, a few of them are currently
offering the resiliency, scalability and high availability required for
production environments. Open Networking Operating System (ONOS) is an open
source NOS, designed to be reliable and to scale up to thousands of managed
devices. It supports multiple concurrent instances (a cluster of controllers)
with distributed data stores. A tight requirement of ONOS is that all instances
must be close enough to have negligible communication delays, which means they
are typically installed within a single datacenter or a LAN network. However in
certain wide area network scenarios, this constraint may limit the speed of
responsiveness of the controller toward network events like failures or
congested links, an important requirement from the point of view of a Service
Provider. This paper presents ICONA, a tool developed on top of ONOS and
designed in order to extend ONOS capability in network scenarios where there
are stringent requirements in term of control plane responsiveness. In
particular the paper describes the architecture behind ICONA and provides some
initial evaluation obtained on a preliminary version of the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07809</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07809</id><created>2015-03-14</created><authors><author><keyname>Nayak</keyname><forenames>S.</forenames></author><author><keyname>Chakraverty</keyname><forenames>S.</forenames></author></authors><title>Numerical solution of moving plate problem with uncertain parameters</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with uncertain parabolic fluid flow problem where the
uncertainty occurs due to the initial conditions and parameters involved in the
system. Uncertain values are considered as fuzzy and these are handled through
a recently developed method. Here the concepts of fuzzy numbers are combined
with Finite Difference Method (FDM) and then Fuzzy Finite Difference Method
(FFDM) has been proposed. The proposed FFDM has been used to solve the fluid
flow problem bounded by two parallel plates. Finally sensitivity of the fuzzy
parameters has also been analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07816</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07816</id><created>2015-03-14</created><authors><author><keyname>Abdelkhalak</keyname><forenames>Bahri</forenames></author><author><keyname>Zouaki</keyname><forenames>Hamid</forenames></author></authors><title>Content-Based Bird Retrieval using Shape context, Color moments and Bag
  of Features</title><categories>cs.CV cs.IR</categories><comments>5 pages, 2 figures, IJCSI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new descriptor for birds search. First, our work
was carried on the choice of a descriptor. This choice is usually driven by the
application requirements such as robustness to noise, stability with respect to
bias, the invariance to geometrical transformations or tolerance to occlusions.
In this context, we introduce a descriptor which combines the shape and color
descriptors to have an effectiveness description of birds. The proposed
descriptor is an adaptation of a descriptor based on the contours defined in
article Belongie et al. [5] combined with color moments [19]. Specifically,
points of interest are extracted from each image and information's in the
region in the vicinity of these points are represented by descriptors of shape
context concatenated with color moments. Thus, the approach bag of visual words
is applied to the latter. The experimental results show the effectiveness of
our descriptor for the bird search by content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07826</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07826</id><created>2015-03-26</created><updated>2015-03-26</updated><authors><author><keyname>He</keyname><forenames>Hao</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Fusing Censored Dependent Data for Distributed Detection</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a distributed detection problem for a censoring
sensor network where each sensor's communication rate is significantly reduced
by transmitting only &quot;informative&quot; observations to the Fusion Center (FC), and
censoring those deemed &quot;uninformative&quot;. While the independence of data from
censoring sensors is often assumed in previous research, we explore spatial
dependence among observations. Our focus is on designing the fusion rule under
the Neyman-Pearson (NP) framework that takes into account the spatial
dependence among observations. Two transmission scenarios are considered, one
where uncensored observations are transmitted directly to the FC and second
where they are first quantized and then transmitted to further improve
transmission efficiency. Copula-based Generalized Likelihood Ratio Test (GLRT)
for censored data is proposed with both continuous and discrete messages
received at the FC corresponding to different transmission strategies. We
address the computational issues of the copula-based GLRTs involving
multidimensional integrals by presenting more efficient fusion rules, based on
the key idea of injecting controlled noise at the FC before fusion. Although,
the signal-to-noise ratio (SNR) is reduced by introducing controlled noise at
the receiver, simulation results demonstrate that the resulting noise-aided
fusion approach based on adding artificial noise performs very closely to the
exact copula-based GLRTs. Copula-based GLRTs and their noise-aided counterparts
by exploiting the spatial dependence greatly improve detection performance
compared with the fusion rule under independence assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07828</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07828</id><created>2015-03-05</created><authors><author><keyname>Pulford</keyname><forenames>Graham W.</forenames></author></authors><title>A Survey of Manoeuvring Target Tracking Methods</title><categories>cs.SY</categories><comments>38 pages, 6 figures. Original manuscript dated 23/12/1998. Submitted
  to IEEE Transactions on Aerospace &amp; Electronic Systems. Withdrawn following a
  review process lasting 3 years. By the end of this period, this survey paper
  was out of date</comments><msc-class>05C85, 60J10, 60J27, 60J28, 60K15, 62F03, 62L12, 62M02, 62M05,
  68W27, 65C50, 65D10, 60G35, 93C05, 93C30, 93C15, 93C55, 93C57, 93C85, 93C95,
  93E10, 93E11, 93E14, 94A13</msc-class><acm-class>G.2.1; G.2.3; G.3; G.4; I.2.8; I.4.3; I.4.4; I.4.8; I.5.4; J.2</acm-class><doi>10.13140/2.1.4994.3846</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comprehensive review of the literature on manoeuvring target tracking for
both uncluttered and cluttered measurements is presented. Various discrete-time
dynamical models including non-random input, random-input and switching or
hybrid system manoeuvre models are presented. The problem of manoeuvre
detection is covered. Classical and current filtering methods for manoeuvre
tracking are presented including multi-level process noise, input estimation,
variable dimension filtering, two-stage filter, the interacting multiple model
algorithm, and generalised pseudo-Bayesian algorithms. Various extensions of
these algorithms to the case of cluttered measurements are also described and
these include: joint manoeuvre and measurement association, probabilistic data
association and multi-hypothesis tracking. Smoothing schemes, including IMM
smoothing and batch expectation-maximisation using the Viterbi algorithm, are
also described. The use of amplitude information for target measurement
discrimination is discussed. It is noted that although many manoeuvre tacking
techniques exist, the literature contains surprisingly few performance
comparisons to guide the design engineer although a performance benchmark has
recently been introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07845</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07845</id><created>2015-03-26</created><authors><author><keyname>Marti</keyname><forenames>Luis</forenames></author><author><keyname>Grimme</keyname><forenames>Christian</forenames></author><author><keyname>Kerschke</keyname><forenames>Pascal</forenames></author><author><keyname>Trautmann</keyname><forenames>Heike</forenames></author><author><keyname>Rudolph</keyname><forenames>G&#xfc;nter</forenames></author></authors><title>Averaged Hausdorff Approximations of Pareto Fronts based on
  Multiobjective Estimation of Distribution Algorithms</title><categories>cs.AI</categories><comments>13 pages</comments><acm-class>I.2.8; I.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the a posteriori approach of multiobjective optimization the Pareto front
is approximated by a finite set of solutions in the objective space. The
quality of the approximation can be measured by different indicators that take
into account the approximation's closeness to the Pareto front and its
distribution along the Pareto front. In particular, the averaged Hausdorff
indicator prefers an almost uniform distribution. An observed drawback of
multiobjective estimation of distribution algorithms (MEDAs) is that - as
common for randomized metaheuristics - the final population usually is not
uniformly distributed along the Pareto front. Therefore, we propose a
postprocessing strategy which consists of applying the averaged Hausdorff
indicator to the complete archive of generated solutions after optimization in
order to select a uniformly distributed subset of nondominated solutions from
the archive. In this paper, we put forward a strategy for extracting the above
described subset. The effectiveness of the proposal is contrasted in a series
of experiments that involve different MEDAs and filtering techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07881</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07881</id><created>2015-03-26</created><authors><author><keyname>Perez</keyname><forenames>Yonathan</forenames></author><author><keyname>Sosic</keyname><forenames>Rok</forenames></author><author><keyname>Banerjee</keyname><forenames>Arijit</forenames></author><author><keyname>Puttagunta</keyname><forenames>Rohan</forenames></author><author><keyname>Raison</keyname><forenames>Martin</forenames></author><author><keyname>Shah</keyname><forenames>Pararth</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Ringo: Interactive Graph Analytics on Big-Memory Machines</title><categories>cs.DB</categories><comments>6 pages, 2 figures</comments><acm-class>H.2.4</acm-class><doi>10.1145/2723372.2735369</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Ringo, a system for analysis of large graphs. Graphs provide a way
to represent and analyze systems of interacting objects (people, proteins,
webpages) with edges between the objects denoting interactions (friendships,
physical interactions, links). Mining graphs provides valuable insights about
individual objects as well as the relationships among them.
  In building Ringo, we take advantage of the fact that machines with large
memory and many cores are widely available and also relatively affordable. This
allows us to build an easy-to-use interactive high-performance graph analytics
system. Graphs also need to be built from input data, which often resides in
the form of relational tables. Thus, Ringo provides rich functionality for
manipulating raw input data tables into various kinds of graphs. Furthermore,
Ringo also provides over 200 graph analytics functions that can then be applied
to constructed graphs.
  We show that a single big-memory machine provides a very attractive platform
for performing analytics on all but the largest graphs as it offers excellent
performance and ease of use as compared to alternative approaches. With Ringo,
we also demonstrate how to integrate graph analytics with an iterative process
of trial-and-error data exploration and rapid experimentation, common in data
mining workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07884</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07884</id><created>2015-03-26</created><authors><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy M.</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Transductive Multi-class and Multi-label Zero-shot Learning</title><categories>cs.LG cs.CV</categories><comments>4 pages, 4 figures, ECCV 2014 Workshop on Parts and Attributes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, zero-shot learning (ZSL) has received increasing interest. The key
idea underpinning existing ZSL approaches is to exploit knowledge transfer via
an intermediate-level semantic representation which is assumed to be shared
between the auxiliary and target datasets, and is used to bridge between these
domains for knowledge transfer. The semantic representation used in existing
approaches varies from visual attributes to semantic word vectors and semantic
relatedness. However, the overall pipeline is similar: a projection mapping
low-level features to the semantic representation is learned from the auxiliary
dataset by either classification or regression models and applied directly to
map each instance into the same semantic representation space where a zero-shot
classifier is used to recognise the unseen target class instances with a single
known 'prototype' of each target class. In this paper we discuss two related
lines of work improving the conventional approach: exploiting transductive
learning ZSL, and generalising ZSL to the multi-label case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07889</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07889</id><created>2015-03-07</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Pancaldi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author></authors><title>Design and Implementation of an Inertial Navigation System for
  Pedestrians Based on a Low-Cost MEMS IMU</title><categories>cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inertial navigation systems for pedestrians are infrastructure-less and can
achieve sub-meter accuracy in the short/medium period. However, when low-cost
inertial measurement units (IMU) are employed for their implementation, they
suffer from a slowly growing drift between the true pedestrian position and the
corresponding estimated position. In this paper we illustrate a novel solution
to mitigate such a drift by: a) using only accelerometer and gyroscope
measurements (no magnetometers required); b) including the sensor error model
parameters in the state vector of an extended Kalman filter; c) adopting a
novel soft heuristic for foot stance detection and for zero-velocity updates.
Experimental results evidence that our inertial-only navigation system can
achieve similar or better performance with respect to pedestrian dead-reckoning
systems presented in related studies, although the adopted IMU is less accurate
than more expensive counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07901</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07901</id><created>2015-03-26</created><updated>2016-02-02</updated><authors><author><keyname>Imbach</keyname><forenames>R&#xe9;mi</forenames><affiliation>VEGAS</affiliation></author><author><keyname>Mathis</keyname><forenames>Pascal</forenames><affiliation>ICube</affiliation></author><author><keyname>Schreck</keyname><forenames>Pascal</forenames><affiliation>ICube</affiliation></author></authors><title>A Robust and Efficient Method for Solving Point Distance Problems by
  Homotopy</title><categories>cs.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of Point Distance Solving Problems is to find 2D or 3D placements of
points knowing distances between some pairs of points. The common guideline is
to solve them by a numerical iterative method e.g. Newton-Raphson method). A
sole solution is obtained whereas many exist. However the number of solutions
can be exponential and methods should provide solutions close to a sketch drawn
by the user.Geometric reasoning can help to simplify the underlying system of
equations by changing a few equations and triangularizing it.This
triangularization is a geometric construction of solutions, called construction
plan. We aim at finding several solutions close to the sketch on a
one-dimensional path defined by a global parameter-homotopy using a
construction plan. Some numerical instabilities may be encountered due to
specific geometric configurations. We address this problem by changing
on-the-fly the construction plan.Numerical results show that this hybrid method
is efficient and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07903</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07903</id><created>2015-03-26</created><authors><author><keyname>Haloui</keyname><forenames>Safia</forenames></author></authors><title>Codes from Jacobian surfaces</title><categories>cs.IT math.AG math.IT math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with some Algebraic Geometry codes on Jacobians of
genus 2 curves. We derive a lower bound for the minimum distance of these codes
from an upper &quot;Weil type&quot; bound for the number of rational points on
irreducible (possibly singular or non-absolutely irreducible) curves lying on
an abelian surface over a finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07905</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07905</id><created>2015-03-26</created><authors><author><keyname>Sourla</keyname><forenames>Efrosini</forenames></author><author><keyname>Sioutas</keyname><forenames>Spyros</forenames></author><author><keyname>Tsichlas</keyname><forenames>Kostas</forenames></author><author><keyname>Zaroliagis</keyname><forenames>Christos</forenames></author></authors><title>D3-Tree: A Dynamic Distributed Deterministic Load - Balancer for
  decentralized tree structures</title><categories>cs.DS cs.DC</categories><comments>32 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose D3-Tree, a dynamic distributed deterministic
structure for data management in decentralized networks. We present in brief
the theoretical algorithmic analysis, in which our proposed structure is based
on, and we describe thoroughly the key aspects of the implementation.
Conducting experiments, we verify that the implemented structure outperforms
other well-known hierarchical tree-based structures, since it provides better
complexities regarding load-balancing operations. More specifically, the
structure achieves a logarithmic amortized bound, using an efficient
deterministic load-balancing mechanism, which is general enough to be applied
to other hierarchical tree-based structures. Moreover, we investigate the
structure's fault tolerance, which hasn't been sufficiently tackled in previous
work, both theoretically and through rigorous experimentation. We prove that
D3-Tree is highly fault tolerant, since, even for massive node failures, it
achieves a significant success rate in element queries. Afterwards we go one
step further, in order to achieve sub-logarithmic complexity and propose the
ART+ structure (Autonomous Range Tree), exploiting the excellent performance of
D3-Tree. ART+ is a fully dynamic and fault-tolerant structure, which achieves
sub-logarithmic performance for query and update operations and performs
load-balancing in sub-logarithmic amortized cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07906</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07906</id><created>2015-03-26</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Srihari</keyname><forenames>Sargur N.</forenames></author></authors><title>Generalized K-fan Multimodal Deep Model with Shared Representations</title><categories>cs.LG stat.ML</categories><comments>11 pages, 5 figures</comments><msc-class>68T10</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multimodal learning with deep Boltzmann machines (DBMs) is an generative
approach to fuse multimodal inputs, and can learn the shared representation via
Contrastive Divergence (CD) for classification and information retrieval tasks.
However, it is a 2-fan DBM model, and cannot effectively handle multiple
prediction tasks. Moreover, this model cannot recover the hidden
representations well by sampling from the conditional distribution when more
than one modalities are missing. In this paper, we propose a K-fan deep
structure model, which can handle the multi-input and muti-output learning
problems effectively. In particular, the deep structure has K-branch for
different inputs where each branch can be composed of a multi-layer deep model,
and a shared representation is learned in an discriminative manner to tackle
multimodal tasks. Given the deep structure, we propose two objective functions
to handle two multi-input and multi-output tasks: joint visual restoration and
labeling, and the multi-view multi-calss object recognition tasks. To estimate
the model parameters, we initialize the deep model parameters with CD to
maximize the joint distribution, and then we use backpropagation to update the
model according to specific objective function. The experimental results
demonstrate that the model can effectively leverages multi-source information
and predict multiple tasks well over competitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07911</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07911</id><created>2015-03-26</created><updated>2015-10-15</updated><authors><author><keyname>Tallapragada</keyname><forenames>Pavankumar</forenames></author><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author><author><keyname>Cortes</keyname><forenames>Jorge</forenames></author></authors><title>Event-triggered control under time-varying rates and channel blackouts</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies event-triggered stabilization of linear time-invariant
systems over time-varying rate-limited communication channels. We explicitly
account for the possibility of channel blackouts, i.e., intervals of time when
the communication channel is unavailable for feedback. Assuming prior knowledge
of the channel evolution, we study the data capacity, which is the maximum
total number of bits that could be communicated over a given time interval, and
provide an efficient real-time algorithm to lower bound it for a deterministic
time-slotted model of channel evolution. Building on these results, we design
an event-triggering strategy that guarantees Zeno-free, exponential
stabilization at a desired convergence rate even in the presence of
intermittent channel blackouts. The contributions are the notion of channel
blackouts, the effective event-triggered control despite their occurrence, and
the analysis and quantification of the data capacity for a class of
time-varying continuous-time channels. Various simulations illustrate the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07919</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07919</id><created>2015-03-26</created><authors><author><keyname>Guri</keyname><forenames>Mordechai</forenames></author><author><keyname>Monitz</keyname><forenames>Matan</forenames></author><author><keyname>Mirski</keyname><forenames>Yisroel</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>BitWhisper: Covert Signaling Channel between Air-Gapped Computers using
  Thermal Manipulations</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been assumed that the physical separation (air-gap) of computers
provides a reliable level of security, such that should two adjacent computers
become compromised, the covert exchange of data between them would be
impossible. In this paper, we demonstrate BitWhisper, a method of bridging the
air-gap between adjacent compromised computers by using their heat emissions
and built-in thermal sensors to create a covert communication channel. Our
method is unique in two respects: it supports bidirectional communication, and
it requires no additional dedicated peripheral hardware. We provide
experimental results based on implementation of BitWhisper prototype, and
examine the channel properties and limitations. Our experiments included
different layouts, with computers positioned at varying distances from one
another, and several sensor types and CPU configurations (e.g., Virtual
Machines). We also discuss signal modulation and communication protocols,
showing how BitWhisper can be used for the exchange of data between two
computers in a close proximity (at distance of 0-40cm) at an effective rate of
1-8 bits per hour, a rate which makes it possible to infiltrate brief commands
and exfiltrate small amount of data (e.g., passwords) over the covert channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07921</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07921</id><created>2015-03-26</created><updated>2015-04-16</updated><authors><author><keyname>Reis</keyname><forenames>Julio</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#x131;cio</forenames></author><author><keyname>de Melo</keyname><forenames>Pedro O. S. Vaz</forenames></author><author><keyname>Prates</keyname><forenames>Raquel</forenames></author><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>An</keyname><forenames>Jisun</forenames></author></authors><title>Breaking the News: First Impressions Matter on Online News</title><categories>cs.CY cs.CL</categories><comments>The paper appears in ICWSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing number of people are changing the way they consume news, replacing
the traditional physical newspapers and magazines by their virtual online
versions or/and weblogs. The interactivity and immediacy present in online news
are changing the way news are being produced and exposed by media corporations.
News websites have to create effective strategies to catch people's attention
and attract their clicks. In this paper we investigate possible strategies used
by online news corporations in the design of their news headlines. We analyze
the content of 69,907 headlines produced by four major global media
corporations during a minimum of eight consecutive months in 2014. In order to
discover strategies that could be used to attract clicks, we extracted features
from the text of the news headlines related to the sentiment polarity of the
headline. We discovered that the sentiment of the headline is strongly related
to the popularity of the news and also with the dynamics of the posted comments
on that particular news.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07931</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07931</id><created>2015-03-26</created><authors><author><keyname>Karmakar</keyname><forenames>Prasenjit</forenames></author><author><keyname>Gopinath</keyname><forenames>K.</forenames></author></authors><title>Are Markov Models Effective for Storage Reliability Modelling?</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous Time Markov Chains (CTMC) have been used extensively to model
reliability of storage systems. While the exponentially distributed sojourn
time of Markov models is widely known to be unrealistic (and it is necessary to
consider Weibull-type models for components such as disks), recent work has
also highlighted some additional infirmities with the CTMC model, such as the
ability to handle repair times. Due to the memoryless property of these models,
any failure or repair of one component resets the &quot;clock&quot; to zero with any
partial repair or aging in some other subsystem forgotten. It has therefore
been argued that simulation is the only accurate technique available for
modelling the reliability of a storage system with multiple components.
  We show how both the above problematic aspects can be handled when we
consider a careful set of approximations in a detailed model of the system. A
detailed model has many states, and the transitions between them and the
current state captures the &quot;memory&quot; of the various components. We model a
non-exponential distribution using a sum of exponential distributions, along
with the use of a CTMC solver in a probabilistic model checking tool that has
support for reducing large state spaces. Furthermore, it is possible to get
results close to what is obtained through simulation and at much lower cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07932</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07932</id><created>2015-03-26</created><authors><author><keyname>Sun</keyname><forenames>Jingchao</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Jin</keyname><forenames>Xiaocong</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchao</forenames></author></authors><title>SecureFind: Secure and Privacy-Preserving Object Finding via Mobile
  Crowdsourcing</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The plummeting cost of Bluetooth tags and the ubiquity of mobile devices are
revolutionizing the traditional lost-and-found service. This paper presents
SecureFind, a secure and privacy-preserving object-finding system via mobile
crowdsourcing. In SecureFind, a unique Bluetooth tag is attached to every
valuable object, and the owner of a lost object submits an object-finding
request to many mobile users via the SecureFind service provider. Each mobile
user involved searches his vicinity for the lost object on behalf of the object
owner who can infer the location of his lost object based on the responses from
mobile users. SecureFind is designed to ensure strong object security such that
only the object owner can discover the location of his lost object as well as
offering strong location privacy to mobile users involved. The high efficacy
and efficiency of SecureFind are confirmed by extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07933</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07933</id><created>2015-03-26</created><authors><author><keyname>Tatarevic</keyname><forenames>Milos</forenames></author></authors><title>On Limits of Dense Packing of Equal Spheres in a Cube</title><categories>cs.CG math.CO</categories><msc-class>52C17, 05B40</msc-class><journal-ref>The Electronic Journal of Combinatorics 22(1) (2015) #P1.35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine packing of $n$ congruent spheres in a cube when $n$ is close but
less than the number of spheres in a regular cubic close-packed (ccp)
arrangement of $\lceil p^{3}/2\rceil$ spheres. For this family of packings, the
previous best-known arrangements were usually derived from a ccp by omission of
a certain number of spheres without changing the initial structure. In this
paper, we show that better arrangements exist for all $n\leq\lceil
p^{3}/2\rceil-2$. We introduce an optimization method to reveal improvements of
these packings, and present many new improvements for $n\leq4629$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07939</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07939</id><created>2015-03-26</created><authors><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Robust State Feedback Control Design with Probabilistic System
  Parameters</title><categories>cs.SY</categories><comments>Published in CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new polynomial chaos based framework for analyzing linear
systems with probabilistic parameters is presented. Stability analysis and
synthesis of optimal quadratically stabilizing controllers for such systems are
presented as convex optimization problems, with exponential mean square
stability guarantees. A Monte-Carlo approach for analysis and synthesis is also
presented, which is used to benchmark the polynomial chaos based approach. The
computational advantage of the polynomial chaos approach is shown with an
example based on the design of an optimal EMS-stabilizing controller, for an
F-16 aircraft model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07940</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07940</id><created>2015-03-26</created><authors><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Competitive Distribution Estimation</title><categories>cs.IT cs.DS cs.LG math.IT math.ST stat.TH</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating an unknown distribution from its samples is a fundamental problem
in statistics. The common, min-max, formulation of this goal considers the
performance of the best estimator over all distributions in a class. It shows
that with $n$ samples, distributions over $k$ symbols can be learned to a KL
divergence that decreases to zero with the sample size $n$, but grows
unboundedly with the alphabet size $k$.
  Min-max performance can be viewed as regret relative to an oracle that knows
the underlying distribution. We consider two natural and modest limits on the
oracle's power. One where it knows the underlying distribution only up to
symbol permutations, and the other where it knows the exact distribution but is
restricted to use natural estimators that assign the same probability to
symbols that appeared equally many times in the sample.
  We show that in both cases the competitive regret reduces to
$\min(k/n,\tilde{\mathcal{O}}(1/\sqrt n))$, a quantity upper bounded uniformly
for every alphabet size. This shows that distributions can be estimated nearly
as well as when they are essentially known in advance, and nearly as well as
when they are completely known in advance but need to be estimated via a
natural estimator. We also provide an estimator that runs in linear time and
incurs competitive regret of $\tilde{\mathcal{O}}(\min(k/n,1/\sqrt n))$, and
show that for natural estimators this competitive regret is inevitable. We also
demonstrate the effectiveness of competitive estimators using simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07943</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07943</id><created>2015-03-26</created><updated>2015-05-13</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>A Polynomial Chaos Framework for Designing Linear Parameter Varying
  Control Systems</title><categories>cs.SY</categories><comments>Published in ACC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we use polynomial chaos framework to design controllers for linear
parameter varying (LPV) dynamical systems. We assume the scheduling variable to
be random and use polynomial chaos approach to synthesize the controller for
the resulting linear stochastic dynamical system. The stability of the LPV
system is formulated as an exponential mean-square (EMS) stability problem. Two
algorithms are presented that guarantee EMS stability of the stochastic system
and correspond to parameter dependent and independent Lyapunov functions,
respectively. LPV controllers from the polynomial chaos based framework is
shown to outperform LPV controller from classical design for an example
nonlinear system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07948</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07948</id><created>2015-03-26</created><authors><author><keyname>Xing</keyname><forenames>Minyao</forenames></author><author><keyname>Peng</keyname><forenames>Yuexing</forenames></author><author><keyname>Xia</keyname><forenames>Teng</forenames></author><author><keyname>Long</keyname><forenames>Hang</forenames></author><author><keyname>Zheng</keyname><forenames>Kan</forenames></author></authors><title>Adaptive Spectrum Sharing of LTE Co-existing with WLAN in Unlicensed
  Frequency Bands</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 7 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase of wireless communication demands, licensed spectrum for
long term evolution (LTE) is no longer enough. The research effort has focused
on implementing LTE to unlicensed frequency bands in recent years, which
unavoidably brings the problem of LTE co-existence with other existing systems
on the same band. This paper proposes an adaptive co-existence mechanism for
LTE and wireless local area networks (WLAN) to enable a significant system
performance of WLAN while LTE does not lose much as well. LTE realizes the
co-existence by allocating time resources dynamically according to the traffic
load of WLAN system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07970</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07970</id><created>2015-03-27</created><authors><author><keyname>Watanabe</keyname><forenames>Sumio</forenames></author></authors><title>Bayesian Cross Validation and WAIC for Predictive Prior Design in
  Regular Asymptotic Theory</title><categories>cs.LG stat.ML</categories><comments>33 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior design is one of the most important problems in both statistics and
machine learning. The cross validation (CV) and the widely applicable
information criterion (WAIC) are predictive measures of the Bayesian
estimation, however, it has been difficult to apply them to find the optimal
prior because their mathematical properties in prior evaluation have been
unknown and the region of the hyperparameters is too wide to be examined. In
this paper, we derive a new formula by which the theoretical relation among CV,
WAIC, and the generalization loss is clarified and the optimal hyperparameter
can be directly found.
  By the formula, three facts are clarified about predictive prior design.
Firstly, CV and WAIC have the same second order asymptotic expansion, hence
they are asymptotically equivalent to each other as the optimizer of the
hyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes
the average generalization loss to be minimized asymptotically but does not the
random generalization loss. And lastly, by using the mathematical relation
between priors, the variances of the optimized hyperparameters by CV and WAIC
are made smaller with small computational costs. Also we show that the
optimized hyperparameter by DIC or the marginal likelihood does not minimize
the average or random generalization loss in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07989</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07989</id><created>2015-03-27</created><authors><author><keyname>Akhtar</keyname><forenames>Naveed</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Discriminative Bayesian Dictionary Learning for Classification</title><categories>cs.CV cs.LG</categories><comments>15 pages</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian approach to learn discriminative dictionaries for
sparse representation of data. The proposed approach infers probability
distributions over the atoms of a discriminative dictionary using a Beta
Process. It also computes sets of Bernoulli distributions that associate class
labels to the learned dictionary atoms. This association signifies the
selection probabilities of the dictionary atoms in the expansion of
class-specific data. Furthermore, the non-parametric character of the proposed
approach allows it to infer the correct size of the dictionary. We exploit the
aforementioned Bernoulli distributions in separately learning a linear
classifier. The classifier uses the same hierarchical Bayesian model as the
dictionary, which we present along the analytical inference solution for Gibbs
sampling. For classification, a test instance is first sparsely encoded over
the learned dictionary and the codes are fed to the classifier. We performed
experiments for face and action recognition; and object and scene-category
classification using five public datasets and compared the results with
state-of-the-art discriminative sparse representation approaches. Experiments
show that the proposed Bayesian approach consistently outperforms the existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07991</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07991</id><created>2015-03-27</created><authors><author><keyname>Derakhshandeh</keyname><forenames>Zahra</forenames></author><author><keyname>Gmyr</keyname><forenames>Robert</forenames></author><author><keyname>Strothmann</keyname><forenames>Thim</forenames></author><author><keyname>Bazzi</keyname><forenames>Rida</forenames></author><author><keyname>Richa</keyname><forenames>Andr&#xe9;a W.</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author></authors><title>Leader Election and Shape Formation with Self-Organizing Programmable
  Matter</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many proposals have already been made for realizing programmable matter,
ranging from shape-changing molecules, DNA tiles, and synthetic cells to
reconfigurable modular robotics. We are particularly interested in programmable
matter consisting of simple computational elements, called particles, that can
establish and release bonds and can actively move in a self-organized way, and
in the feasibility of solving basic problems relevant for programmable matter
with them. As a model for such self-organizing particle systems, we will use a
general form of the amoebot model first proposed at SPAA 2014. Based on that
model, we present efficient local-control algorithms for leader election and
path formation requiring only particles with constant size memory, and we also
discuss the limitations of solving these problems within the amoebot model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07994</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07994</id><created>2015-03-27</created><authors><author><keyname>Damiani</keyname><forenames>Ernesto</forenames></author><author><keyname>Pagano</keyname><forenames>Francesco</forenames></author><author><keyname>Pagano</keyname><forenames>Davide</forenames></author></authors><title>iPrivacy: a Distributed Approach to Privacy on the Cloud</title><categories>cs.CR cs.DC</categories><comments>13 pages, International Journal on Advances in Security 2011 vol.4 no
  3 &amp; 4. arXiv admin note: substantial text overlap with arXiv:1012.0759,
  arXiv:1109.3555</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing adoption of Cloud storage poses a number of privacy issues.
Users wish to preserve full control over their sensitive data and cannot accept
that it to be accessible by the remote storage provider. Previous research was
made on techniques to protect data stored on untrusted servers; however we
argue that the cloud architecture presents a number of open issues. To handle
them, we present an approach where confidential data is stored in a highly
distributed database, partly located on the cloud and partly on the clients.
Data is shared in a secure manner using a simple grant-and-revoke permission of
shared data and we have developed a system test implementation, using an
in-memory RDBMS with row-level data encryption for fine-grained data access
control
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07998</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07998</id><created>2015-03-27</created><authors><author><keyname>Schmid</keyname><forenames>Benjamin</forenames></author><author><keyname>Huisken</keyname><forenames>Jan</forenames></author></authors><title>Real-time multi-view deconvolution</title><categories>q-bio.QM cs.CV</categories><comments>8 pages, 5 figures, submitted to Bioinformatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In light-sheet microscopy, overall image content and resolution are improved
by acquiring and fusing multiple views of the sample from different directions.
State-of-the-art multi-view (MV) deconvolution employs the point spread
functions (PSF) of the different views to simultaneously fuse and deconvolve
the images in 3D, but processing takes a multiple of the acquisition time and
constitutes the bottleneck in the imaging pipeline. Here we show that MV
deconvolution in 3D can finally be achieved in real-time by reslicing the
acquired data and processing cross-sectional planes individually on the
massively parallel architecture of a graphics processing unit (GPU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08007</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08007</id><created>2015-03-27</created><authors><author><keyname>Thenozhi</keyname><forenames>Suresh</forenames></author><author><keyname>Tang</keyname><forenames>Yu</forenames></author></authors><title>Vibration Control Design for Nonlinear Systems using Frequency Response
  Function</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonlinear frequency response based adaptive vibration controller is
proposed for a class of nonlinear mechanical systems. In order to obtain the
nonlinear Frequency Response Function (FRF), the convergence properties of the
system are studied by using the convergence (contraction) theory. If the system
under consideration is: 1) convergent, it directly enables to derive a
nonlinear FRF for a band of excitation inputs, 2) non-convergent, first a
controller is used to obtain the convergence and then the corresponding FRF for
a band of excitation inputs is derived. Now the gains of the proposed adaptive
controller are tuned such that a desired closed-loop frequency response, in the
presence of excitation inputs is achieved. Finally, a building structure with
nonlinear cubic stiffness and a satellite system are considered to illustrate
the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08012</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08012</id><created>2015-03-27</created><authors><author><keyname>Li</keyname><forenames>Hui-Jia</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Luonan</forenames></author></authors><title>Measuring robustness of community structure in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1303.7434 by other authors</comments><journal-ref>EPL (Europhysics Letters),108(6),2015</journal-ref><doi>10.1209/0295-5075/108/68009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of community structure is a powerful tool for real networks, which
can simplify their topological and functional analysis considerably. However,
since community detection methods have random factors and real social networks
obtained from complex systems always contain error edges, evaluating the
robustness of community structure is an urgent and important task. In this
letter, we employ the critical threshold of resolution parameter in Hamiltonian
function, $\gamma_C$, to measure the robustness of a network. According to
spectral theory, a rigorous proof shows that the index we proposed is inversely
proportional to robustness of community structure. Furthermore, by utilizing
the co-evolution model, we provides a new efficient method for computing the
value of $\gamma_C$. The research can be applied to broad clustering problems
in network analysis and data mining due to its solid mathematical basis and
experimental effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08018</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08018</id><created>2015-03-27</created><authors><author><keyname>Li</keyname><forenames>Hui-Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang-Sun</forenames></author></authors><title>Analysis of stability of community structure across multiple
  hierarchical levels</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 3 figures</comments><journal-ref>EPL (Europhysics Letters),103(5),2013</journal-ref><doi>10.1209/0295-5075/103/58002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of stability of community structure is an important problem for
scientists from many fields. Here, we propose a new framework to reveal hidden
properties of community structure by quantitatively analyzing the dynamics of
Potts model. Specifically we model the Potts procedure of community structure
detection by a Markov process, which has a clear mathematical explanation.
Critical topological information regarding to multivariate spin configuration
could also be inferred from the spectral significance of the Markov process. We
test our framework on some example networks and find it doesn't have resolute
limitation problem at all. Results have shown the model we proposed is able to
uncover hierarchical structure in different scales effectively and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08019</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08019</id><created>2015-03-27</created><authors><author><keyname>Faradonbeh</keyname><forenames>Mohamad Kazem Shirani</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Optimality of Fast Matching Algorithms for Random Networks with
  Applications to Structural Controllability</title><categories>cs.DS cs.SY stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network control refers to a very large and diverse set of problems including
controllability of linear time-invariant dynamical systems evolving over time
that have inputs and outputs. The network control problem in this setting is to
select the appropriate input to steer the network into a desired output state.
Examples of the output state include the throughput of a communications
network, transcription factor concentration in a gene regulatory network,
customer purchases in a marketing context subject to social influences and the
amount of flux flowing through a biochemical network.
  We focus on control of linear dynamical systems under the notion of
structural controllability which is intimately connected to finding maximum
matchings. Hence, a natural objective is studying scalable and fast algorithms
for this task. We first show the convergence of matching algorithms for
different random networks and then analyze a popular, fast and practical
heuristic due to Karp and Sipser. We establish the optimality of both the
Karp-Sipser Algorithm as well as a simplification of it, and provide results
concerning the asymptotic size of maximum matchings for an extensive class of
random networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08024</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08024</id><created>2015-03-27</created><authors><author><keyname>Li</keyname><forenames>Hui-Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Junhua</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Ping</forenames></author><author><keyname>Chen</keyname><forenames>Luonan</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang-Sun</forenames></author></authors><title>Identifying overlapping communities in social networks using multi-scale
  local information expansion</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1004.4268, arXiv:cond-mat/0308217 by other authors; text overlap with
  arXiv:1104.5247 by other authors without attribution</comments><journal-ref>European Physical Journal B, 85(6), 109, 2012</journal-ref><doi>10.1140/epjb/e2012-30015-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing approaches for community detection require complete information
of the graph in a specific scale, which is impractical for many social
networks. We propose a novel algorithm that does not embrace the universal
approach but instead of trying to focus on local social ties and modeling
multi-scales of social interactions occurring in those networks. Our method for
the first time optimizes the topological entropy of a network and uncovers
communities through a novel dynamic system converging to a local minimum by
simply updating the membership vector with very low computational complexity.
It naturally supports overlapping communities through associating each node
with a membership vector which describes node's involvement in each community.
This way, in addition to uncover overlapping communities, we can also describe
different multi-scale partitions by tuning the characteristic size of modules
from the optimal partition. Because of the high efficiency and accuracy of the
algorithm, it is feasible to be used for the accurate detection of community
structure in real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08035</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08035</id><created>2015-03-27</created><authors><author><keyname>Li</keyname><forenames>Hui-Jia</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Wu</keyname><forenames>Ling-Yun</forenames></author><author><keyname>Zhang</keyname><forenames>Junhua</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang-Sun</forenames></author></authors><title>Potts model based on a Markov process computation solves the community
  structure problem effectively</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:0911.2308 by other authors</comments><journal-ref>Physical Review E, 86(1), 012801, 2012</journal-ref><doi>10.1103/PhysRevE.86.016109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potts model is a powerful tool to uncover community structure in complex
networks. Here, we propose a new framework to reveal the optimal number of
communities and stability of network structure by quantitatively analyzing the
dynamics of Potts model. Specifically we model the community structure
detection Potts procedure by a Markov process, which has a clear mathematical
explanation. Then we show that the local uniform behavior of spin values across
multiple timescales in the representation of the Markov variables could
naturally reveal the network's hierarchical community structure. In addition,
critical topological information regarding to multivariate spin configuration
could also be inferred from the spectral signatures of the Markov process.
Finally an algorithm is developed to determine fuzzy communities based on the
optimal number of communities and the stability across multiple timescales. The
effectiveness and efficiency of our algorithm are theoretically analyzed as
well as experimentally validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08039</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08039</id><created>2015-03-27</created><updated>2015-05-08</updated><authors><author><keyname>Li</keyname><forenames>Hui-Jia</forenames></author><author><keyname>Daniels</keyname><forenames>J J.</forenames></author></authors><title>Social significance of community structure: Statistical view</title><categories>physics.soc-ph cs.SI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation (17) and high overlapping rate</comments><journal-ref>Physical Review E, 91(1), 012801, 2015</journal-ref><doi>10.1103/PhysRevE.91.012801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure analysis is a powerful tool for social networks, which
can simplify their topological and functional analysis considerably. However,
since community detection methods have random factors and real social networks
obtained from complex systems always contain error edges, evaluating the
significance of community structure partitioned is an urgent and important
question. In this paper, integrating the specific characteristics of real
society, we present a novel framework analyzing the significance of social
community specially. The dynamics of social interactions are modeled by
identifying social leaders and corresponding hierarchical structures. Instead
of a direct comparison with the average outcome of a random model, we compute
the similarity of a given node with the leader by the number of common
neighbors. To determine the membership vector, an efficient community detection
algorithm is proposed based on the position of nodes and their corresponding
leaders. Then, using log-likelihood score, the tightness of community can be
derived. Based on the distribution of community tightness, we establish a new
connection between $p$-value theory and network analysis and then get a novel
statistical form significance measure. Finally, the framework is applied to
both benchmark networks and real social networks. Experimental results show
that our work can be used in many fields, such as determining the optimal
number of communities, analyzing the social significance of a given community,
comparing the performance among various algorithms and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08040</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08040</id><created>2015-03-27</created><updated>2015-03-31</updated><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Approximate message-passing decoder and capacity-achieving sparse
  superposition codes</title><categories>cs.IT math.IT</categories><comments>32 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximate message-passing decoder for sparse superposition
coding on the additive white Gaussian noise channel and extend our preliminary
work. While this coding scheme asymptotically reach the Shannon capacity, we
show that our iterative decoder is limited by a phase transition similar to
what happen in LDPC codes. We present and study two solutions to this problem,
that both allow to reach the Shannon capacity: i) a non constant power
allocation and ii) the use of spatially coupled codes. We also present
extensive simulations that suggest that spatial coupling is more robust and
allows for better reconstruction at finite code lengths. Finally, we show
empirically that the use of a fast Hadamard-based operator allows for an
efficient reconstruction, both in terms of computational time and memory, and
the ability to deal with large signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08048</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08048</id><created>2015-03-27</created><authors><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Shu</keyname><forenames>Pan-Pan</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Small</keyname><forenames>Michael</forenames></author></authors><title>Preferential imitation of vaccinating behavior can invalidate the
  targeted subsidy on complex network</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider the effect of inducement to vaccinate during the spread of an
infectious disease on complex networks. Suppose that public resources are
finite and that only a small proportion of individuals can be vaccinated freely
(complete subsidy), for the remainder of the population vaccination is a
voluntary behavior --- and each vaccinated individual carries a perceived cost.
We ask whether the classical targeted subsidy strategy is definitely better
than the random strategy: does targeting subsidy at individuals perceived to be
with the greatest risk actually help? With these questions, we propose a model
to investigate the \emph{interaction effects} of the subsidy policies and
individuals responses when facing subsidy policies on the epidemic dynamics on
complex networks. In the model, a small proportion of individuals are freely
vaccinated according to either the targeted or random subsidy policy, the
remainder choose to vaccinate (or not) based on voluntary principle and update
their vaccination decision via an imitation rule. Our findings show that the
targeted strategy is only advantageous when individuals prefer to imitate the
subsidized individuals' strategy. Otherwise, the effect of the targeted policy
is worse than the random immunization, since individuals preferentially select
non-subsidized individuals as the imitation objects. More importantly, we find
that under the targeted subsidy policy, increasing the proportion of subsidized
individuals may increase the final epidemic size. We further define social cost
as the sum of the costs of vaccination and infection, and study how each of the
two policies affect the social cost. Our result shows that there exist some
optimal intermediate regions leading to the minimal social cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08057</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08057</id><created>2015-03-27</created><updated>2015-06-22</updated><authors><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames></author></authors><title>Coloring graphs with no even hole $\geq 6$: the triangle-free case</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that the class of graphs with no triangle and no
induced cycle of even length at least 6 has bounded chromatic number. It is
well-known that even-hole-free graphs are $\chi$-bounded but we allow here the
existence of $C_4$. The proof relies on the concept of Parity Changing Path, an
adaptation of Trinity Changing Path which was recently introduced by Bonamy,
Charbit and Thomass\'e to prove that graphs with no induced cycle of length
divisible by three have bounded chromatic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08078</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08078</id><created>2015-03-27</created><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Kronegger</keyname><forenames>Martin</forenames></author><author><keyname>Pfandler</keyname><forenames>Andreas</forenames></author><author><keyname>Popa</keyname><forenames>Alexandru</forenames></author></authors><title>Parameterized Complexity of Asynchronous Border Minimization</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microarrays are research tools used in gene discovery as well as disease and
cancer diagnostics. Two prominent but challenging problems related to
microarrays are the Border Minimization Problem (BMP) and the Border
Minimization Problem with given placement (P-BMP).
  In this paper we investigate the parameterized complexity of natural variants
of BMP and P-BMP under several natural parameters. We show that BMP and P-BMP
are in FPT under the following two combinations of parameters: 1) the size of
the alphabet (c), the maximum length of a sequence (string) in the input (l)
and the number of rows of the microarray (r); and, 2) the size of the alphabet
and the size of the border length (o). Furthermore, P-BMP is in FPT when
parameterized by c and l. We complement our tractability results with
corresponding hardness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08081</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08081</id><created>2015-03-27</created><authors><author><keyname>P&#xf6;chacker</keyname><forenames>Manfred</forenames></author><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Proficiency of Power Values for Load Disaggregation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load disaggregation techniques infer the operation of different power
consuming devices from a single measurement point that records the total power
draw over time. Thus, a device consuming power at the moment can be understood
as information encoded in the power draw. However, similar power draws or
similar combinations of power draws limit the ability to detect the currently
active device set. We present an information coding perspective of load
disaggregation to enable a better understanding of this process and to support
its future improvement. In typical cases of quantity and type of devices and
their respective power consumption, not all possible device configurations can
be mapped to distinguishable power values. We introduce the term of proficiency
to describe the suitability of a device set for load disaggregation. We provide
the notion and calculation of entropy of initial device states, mutual
information of power values and the resulting uncertainty coefficient or
proficiency. We show that the proficiency is highly dependent from the device
running probability especially for devices with multiple states of power
consumption. The application of the concept is demonstrated by exemplary
artificial data as well as with actual power consumption data from real-world
power draw datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08085</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08085</id><created>2015-03-27</created><authors><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Evolutionary Poisson Games for Controlling Large Population Behaviors</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging applications in engineering such as crowd-sourcing and
(mis)information propagation involve a large population of heterogeneous users
or agents in a complex network who strategically make dynamic decisions. In
this work, we establish an evolutionary Poisson game framework to capture the
random, dynamic and heterogeneous interactions of agents in a holistic fashion,
and design mechanisms to control their behaviors to achieve a system-wide
objective. We use the antivirus protection challenge in cyber security to
motivate the framework, where each user in the network can choose whether or
not to adopt the software. We introduce the notion of evolutionary Poisson
stable equilibrium for the game, and show its existence and uniqueness. Online
algorithms are developed using the techniques of stochastic approximation
coupled with the population dynamics, and they are shown to converge to the
optimal solution of the controller problem. Numerical examples are used to
illustrate and corroborate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08090</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08090</id><created>2015-03-27</created><updated>2016-02-10</updated><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames></author><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames></author><author><keyname>Magron</keyname><forenames>Victor</forenames></author></authors><title>A Sums-of-Squares Extension of Policy Iterations</title><categories>cs.LO math.OC</categories><comments>24 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to address the imprecision often introduced by widening operators in
static analysis, policy iteration based on min-computations amounts to
considering the characterization of reachable value set of a program as an
iterative computation of policies, starting from a post-fixpoint. Computing
each policy and the associated invariant relies on a sequence of numerical
optimizations. While the early research efforts relied on linear programming
(LP) to address linear properties of linear programs, the current state of the
art is still limited to the analysis of linear programs with at most quadratic
invariants, relying on semidefinite programming (SDP) solvers to compute
policies, and LP solvers to refine invariants.
  We propose here to extend the class of programs considered through the use of
Sums-of-Squares (SOS) based optimization. Our approach enables the precise
analysis of switched systems with polynomial updates and guards. The analysis
presented has been implemented in Matlab and applied on existing programs
coming from the system control literature, improving both the range of
analyzable systems and the precision of previously handled ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08104</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08104</id><created>2015-03-27</created><authors><author><keyname>Chalios</keyname><forenames>Charalampos</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Dimitrios S.</forenames></author><author><keyname>Quintana-Orti</keyname><forenames>Enrique S.</forenames></author></authors><title>Evaluating Asymmetric Multicore Systems-on-Chip using Iso-Metrics</title><categories>cs.DC cs.AR</categories><comments>Presented at HiPEAC EEHCO '15, 6 pages</comments><acm-class>C.1.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The end of Dennard scaling has pushed power consumption into a first order
concern for current systems, on par with performance. As a result,
near-threshold voltage computing (NTVC) has been proposed as a potential means
to tackle the limited cooling capacity of CMOS technology. Hardware operating
in NTV consumes significantly less power, at the cost of lower frequency, and
thus reduced performance, as well as increased error rates. In this paper, we
investigate if a low-power systems-on-chip, consisting of ARM's asymmetric
big.LITTLE technology, can be an alternative to conventional high performance
multicore processors in terms of power/energy in an unreliable scenario. For
our study, we use the Conjugate Gradient solver, an algorithm representative of
the computations performed by a large range of scientific and engineering
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08109</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08109</id><created>2015-02-12</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Miranda</keyname><forenames>J. P. C. L.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Spread-Spectrum Based on Finite Field Fourier Transforms</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures. Int. Conf. on System Engineering, Comm. and.
  Info. Technol., Punta Arenas, Chile, 2001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spread-spectrum systems are presented, which are based on Finite Field
Fourier Transforms. Orthogonal spreading sequences defined over a finite field
are derived. New digital multiplex schemes based on such spread-spectrum
systems are also introduced, which are multilevel Coding Division Multiplex.
These schemes termed Galois-field Division Multiplex (GDM) offer compact
bandwidth requirements because only leaders of cyclotomic cosets are needed to
be transmitted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08115</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08115</id><created>2015-03-27</created><authors><author><keyname>Pagano</keyname><forenames>Francesco</forenames></author></authors><title>A Distributed Approach to Privacy on the Cloud</title><categories>cs.CR cs.DB</categories><comments>PhD Thesis in Computer Science at University of Milan - Italy 2012
  March 6th</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing adoption of Cloud-based data processing and storage poses a
number of privacy issues. Users wish to preserve full control over their
sensitive data and cannot accept it to be fully accessible to an external
storage provider. Previous research in this area was mostly addressed at
techniques to protect data stored on untrusted database servers; however, I
argue that the Cloud architecture presents a number of specific problems and
issues. This dissertation contains a detailed analysis of open issues. To
handle them, I present a novel approach where confidential data is stored in a
highly distributed partitioned database, partly located on the Cloud and partly
on the clients. In my approach, data can be either private or shared; the
latter is shared in a secure manner by means of simple grant-and-revoke
permissions. I have developed a proof-of-concept implementation using an
in-memory RDBMS with row-level data encryption in order to achieve fine-grained
data access control. This type of approach is rarely adopted in conventional
outsourced RDBMSs because it requires several complex steps. Benchmarks of my
proofof-concept implementation show that my approach overcomes most of the
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08131</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08131</id><created>2015-03-27</created><authors><author><keyname>Yazicioglu</keyname><forenames>A. Yasin</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>Formation of Robust Multi-Agent Networks Through Self-Organizing Random
  Regular Graphs</title><categories>cs.MA cs.SI cs.SY math.CO</categories><doi>10.1109/TNSE.2015.2503983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent networks are often modeled as interaction graphs, where the nodes
represent the agents and the edges denote some direct interactions. The
robustness of a multi-agent network to perturbations such as failures, noise,
or malicious attacks largely depends on the corresponding graph. In many
applications, networks are desired to have well-connected interaction graphs
with relatively small number of links. One family of such graphs is the random
regular graphs. In this paper, we present a decentralized scheme for
transforming any connected interaction graph with a possibly non-integer
average degree of k into a connected random m-regular graph for some m in [k, k
+ 2]. Accordingly, the agents improve the robustness of the network with a
minimal change in the overall sparsity by optimizing the graph connectivity
through the proposed local operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08134</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08134</id><created>2015-03-27</created><authors><author><keyname>Khanafer</keyname><forenames>Ali</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>Context-Aware Wireless Small Cell Networks: How to Exploit User
  Information for Resource Allocation</title><categories>cs.GT cs.IT cs.MA math.IT</categories><comments>To be presented at the IEEE International Conference on
  Communications (ICC), London, U.K., 2015</comments><msc-class>91A10</msc-class><acm-class>C.2.1; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel context-aware approach for resource allocation in
two-tier wireless small cell networks~(SCNs) is proposed. In particular, the
SCN's users are divided into two types: frequent users, who are regular users
of certain small cells, and occasional users, who are one-time or infrequent
users of a particular small cell. Given such \emph{context} information, each
small cell base station (SCBS) aims to maximize the overall performance
provided to its frequent users, while ensuring that occasional users are also
well serviced. We formulate the problem as a noncooperative game in which the
SCBSs are the players. The strategy of each SCBS is to choose a proper power
allocation so as to optimize a utility function that captures the tradeoff
between the users' quality-of-service gains and the costs in terms of resource
expenditures. We provide a sufficient condition for the existence and
uniqueness of a pure strategy Nash equilibrium for the game, and we show that
this condition is independent of the number of users in the network. Simulation
results show that the proposed context-aware resource allocation game yields
significant performance gains, in terms of the average utility per SCBS,
compared to conventional techniques such as proportional fair allocation and
sum-rate maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08139</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08139</id><created>2015-03-27</created><authors><author><keyname>Seshadreesan</keyname><forenames>Kaushik P.</forenames></author><author><keyname>Takeoka</keyname><forenames>Masahiro</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Bounds on entanglement distillation and secret key agreement for quantum
  broadcast channels</title><categories>quant-ph cs.IT math.IT</categories><comments>34 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The squashed entanglement of a quantum channel is an additive function of
quantum channels, which finds application as an upper bound on the rate at
which secret key and entanglement can be generated when using a quantum channel
a large number of times in addition to unlimited classical communication. This
quantity has led to an upper bound of $\log((1+\eta )/(1-\eta))$ on the
capacity of an optical communication channel for such a task, where $\eta$ is
the average fraction of photons that make it from the input to the output of
the channel. The purpose of the present paper is to extend these results beyond
the single-sender single-receiver setting to the more general case of a single
sender and multiple receivers (a quantum broadcast channel). We employ
multipartite generalizations of the squashed entanglement to constrain the
rates at which secret key and entanglement can be generated between any subset
of the users of such a channel, along the way developing several new properties
of these measures. We apply our results to the case of an optical broadcast
channel with one sender and two receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08141</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08141</id><created>2015-03-27</created><authors><author><keyname>Baltag</keyname><forenames>Alexandru</forenames></author><author><keyname>Renne</keyname><forenames>Bryan</forenames></author><author><keyname>Smets</keyname><forenames>Sonja</forenames></author></authors><title>Revisable Justified Belief: Preliminary Report</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory $\mathsf{CDL}$ of Conditional Doxastic Logic is the single-agent
version of Board's multi-agent theory $\mathsf{BRSIC}$ of conditional belief.
$\mathsf{CDL}$ may be viewed as a version of AGM belief revision theory in
which Boolean combinations of revisions are expressible in the language. We
introduce a theory $\mathsf{JCDL}$ of Justified Conditional Doxastic Logic that
replaces conditional belief formulas $B^\psi\varphi$ by expressions
$t{\,:^{\psi}}\varphi$ made up of a term $t$ whose syntactic structure suggests
a derivation of the belief $\varphi$ after revision by $\psi$. This allows us
to think of terms $t$ as reasons justifying a belief in various formulas after
a revision takes place. We show that $\mathsf{JCDL}$-theorems are the exact
analogs of $\mathsf{CDL}$-theorems, and that this result holds the other way
around as well. This allows us to think of $\mathsf{JCDL}$ as a theory of
revisable justified belief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08154</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08154</id><created>2015-03-27</created><updated>2016-02-18</updated><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Pudl&#xe1;k</keyname><forenames>Pavel</forenames></author></authors><title>On the Joint Entropy of $d$-Wise-Independent Variables</title><categories>cs.DM</categories><comments>An updated version that takes into account some comments received by
  the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How low can be the joint entropy of $n$ $d$-wise independent (for $d\ge2$)
discrete random variables? This question has been posed and partially answered
in a recent work of Babai.
  In this paper we improve some of his bounds, prove new bounds in a wider
range of parameters and show matching upper bounds in some special cases. In
particular, we prove tight lower bounds for the min-entropy (as well as the
entropy) of pairwise and three-wise independent balanced binary variables for
infinitely many values of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08155</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08155</id><created>2015-03-27</created><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author></authors><title>Learning Embedding Representations for Knowledge Inference on Imperfect
  and Incomplete Repositories</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of knowledge inference on large-scale
imperfect repositories with incomplete coverage by means of embedding entities
and relations at the first attempt. We propose IIKE (Imperfect and Incomplete
Knowledge Embedding), a probabilistic model which measures the probability of
each belief, i.e. $\langle h,r,t\rangle$, in large-scale knowledge bases such
as NELL and Freebase, and our objective is to learn a better low-dimensional
vector representation for each entity ($h$ and $t$) and relation ($r$) in the
process of minimizing the loss of fitting the corresponding confidence given by
machine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\bf
h} + {\bf r} - {\bf t}||$ to assess the plausibility of a belief when
conducting inference. We use subsets of those inexact knowledge bases to train
our model and test the performances of link prediction and triplet
classification on ground truth beliefs, respectively. The results of extensive
experiments show that IIKE achieves significant improvement compared with the
baseline and state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08158</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08158</id><created>2015-03-27</created><authors><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Justice</keyname><forenames>Emuoyibofarhe</forenames></author></authors><title>A Secure Intelligent Decision Support System for Prescribing Medication</title><categories>cs.CY</categories><journal-ref>CISDI 3:3 9-18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of electronic approach to writing and sending medical
prescription promises to improve patient safety, health outcomes, maintaining
patients privacy, promoting clinician acceptance and prescription security when
compared with the customary paper method. Traditionally, medical prescriptions
are typically handwritten or printed on paper and hand-delivered to
pharmacists. Paper-based medical prescriptions are generating major concerns as
the incidences of prescription errors have been increasing and causing minor to
serious problems to patients, including deaths. In this paper, intelligent
eprescription model that comprises a knowledge base of drug details and an
inference engine that can help in decision making when writing a prescription
was developed. The research implements the e-prescription model with
multifactor authentication techniques which comprises password and biometric
technology. Microsoft Visual Studio 2008, using C-Sharp programming language,
and Microsoft SQL Server 2005 database were employed in developing the systems
front end and back end respectively. This work implements a knowledge base to
the e-prescription system which has added intelligence for validating doctors
prescription and also added security feature to the e-prescription system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08163</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08163</id><created>2015-03-27</created><authors><author><keyname>Olaniyi</keyname><forenames>Mikail</forenames></author><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Robert</keyname><forenames>Jane</forenames></author><author><keyname>Oke</keyname><forenames>Alice</forenames></author></authors><title>Development of an Electronic Medical Image Archiving System for Health
  Care in Nigeria</title><categories>cs.CY</categories><journal-ref>International Journal of Computer and Information
  Technology,Volume 02, Issue 04, 2013 p.706-711</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical images require immediate access by several physicians in different
places within a medical facility and access to a critically injured persons
medical image, such as the x-ray, on time can be a key factor in the diagnosis
and treatment of the patient. The electronic medical image archive system can
help to solve the problem faced in previous physical medium archiving, thus
increasing productivity and time which patients are attended to. In this paper,
simple but functional electronic medical image archive architecture was
proposed and implemented. The system was further evaluated in a hospital
setting by medical experts using sample patient image data. Results of the
system evaluation shows that electronic medical image archiving systems can
actually promote efficiency, quality improvement, provide timely availability
of radiologic images, image consultation, and image interpretation in emergent
and no emergent clinical care areas among other benefits
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08167</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08167</id><created>2015-03-27</created><updated>2015-03-30</updated><authors><author><keyname>Beliga</keyname><forenames>Slobodan</forenames></author><author><keyname>Pobar</keyname><forenames>Miran</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Normalization of Non-Standard Words in Croatian Texts</title><categories>cs.CL</categories><comments>8 pages, 3 figures in Text, Speech and Dialogue extension to Lecture
  Notes in Artificial Intelligence LNAI6836. Hebernal, Ivan; Matou\v{s}ek,
  V\'aclav (ed). - Plzen: University of West Bohemia, 2011. 1-8 (ISBN:
  987-80-261-0069-0)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents text normalization which is an integral part of any
text-to-speech synthesis system. Text normalization is a set of methods with a
task to write non-standard words, like numbers, dates, times, abbreviations,
acronyms and the most common symbols, in their full expanded form are
presented. The whole taxonomy for classification of non-standard words in
Croatian language together with rule-based normalization methods combined with
a lookup dictionary are proposed. Achieved token rate for normalization of
Croatian texts is 95%, where 80% of expanded words are in correct morphological
form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08169</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08169</id><created>2015-03-27</created><authors><author><keyname>Mirhoseini</keyname><forenames>Azalia</forenames></author><author><keyname>Dyer</keyname><forenames>Eva. L.</forenames></author><author><keyname>Songhori</keyname><forenames>Ebrahim. M.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author><author><keyname>Koushanfar</keyname><forenames>Farinaz</forenames></author></authors><title>RankMap: A Platform-Aware Framework for Distributed Learning from Dense
  Datasets</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces RankMap, a platform-aware end-to-end framework for
efficient execution of a broad class of iterative learning algorithms for
massive and dense datasets. In contrast to the existing dense (iterative) data
analysis methods that are oblivious to the platform, for the first time, we
introduce novel scalable data transformation and mapping algorithms that enable
optimizing for the underlying computing platforms' cost/constraints. The cost
is defined by the number of arithmetic and (within-platform) message passing
operations incurred by the variable updates in each iteration, while the
constraints are set by the available memory resources. RankMap's transformation
scalably factorizes data into an ensemble of lower dimensional subspaces, while
its mapping schedules the flow of iterative computation on the transformed data
onto the pertinent computing machine. We show a trade-off between the desired
level of accuracy for the learning algorithm and the achieved efficiency.
RankMap provides two APIs, one matrix-based and one graph-based, which
facilitate automated adoption of the framework for performing several
contemporary iterative learning applications optimized to the platform. To
demonstrate the utility of RankMap, we solve sparse recovery and power
iteration problems on various real-world datasets with up to 1.8 billion
non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex
platforms using up to 244 cores. The results demonstrate up to 2 orders of
magnitude improvements in memory usage, execution speed, and bandwidth compared
with the best reported prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08175</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08175</id><created>2015-03-27</created><updated>2015-10-27</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author><author><keyname>Xu</keyname><forenames>Zhi</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Distributed Evaluation and Convergence of Self-Appraisals in Social
  Networks</title><categories>cs.SY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider in this paper a networked system of opinion dynamics in
continuous time, where the agents are able to evaluate their self-appraisals in
a distributed way. In the model we formulate, the underlying network topology
is described by a rooted digraph. For each ordered pair of agents $(i,j)$, we
assign a function of self-appraisal to agent $i$, which measures the level of
importance of agent $i$ to agent $j$. Thus, by communicating only with her
neighbors, each agent is able to calculate the difference between her level of
importance to others and others' level of importance to her. The dynamical
system of self-appraisals is then designed to drive these differences to zero.
We show that for almost all initial conditions, the trajectory generated by
this dynamical system asymptotically converges to an equilibrium point which is
exponentially stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08192</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08192</id><created>2015-03-27</created><authors><author><keyname>Yang</keyname><forenames>Mu</forenames></author><author><keyname>Tang</keyname><forenames>Choon Yik</forenames></author></authors><title>Distributed Estimation of Graph Spectrum</title><categories>cs.SY cs.DC</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a two-stage distributed algorithm that enables
nodes in a graph to cooperatively estimate the spectrum of a matrix $W$
associated with the graph, which includes the adjacency and Laplacian matrices
as special cases. In the first stage, the algorithm uses a discrete-time linear
iteration and the Cayley-Hamilton theorem to convert the problem into one of
solving a set of linear equations, where each equation is known to a node. In
the second stage, if the nodes happen to know that $W$ is cyclic, the algorithm
uses a Lyapunov approach to asymptotically solve the equations with an
exponential rate of convergence. If they do not know whether $W$ is cyclic, the
algorithm uses a random perturbation approach and a structural controllability
result to approximately solve the equations with an error that can be made
small. Finally, we provide simulation results that illustrate the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08196</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08196</id><created>2015-02-10</created><authors><author><keyname>Pham</keyname><forenames>Gia-Thuy</forenames></author><author><keyname>Loubaton</keyname><forenames>Philippe</forenames></author><author><keyname>Vallet</keyname><forenames>Pascal</forenames></author></authors><title>Performance analysis of spatial smoothing schemes in the context of
  large arrays</title><categories>stat.ME cs.IT math.IT math.PR</categories><comments>28 pages, 5 figures</comments><doi>10.1109/TSP.2015.2480044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper adresses the statistical behaviour of spatial smoothing subspace
DoA estimation schemes using a sensor array in the case where the number of
observations $N$ is significantly smaller than the number of sensors $M$, and
that the smoothing parameter $L$ is such that $M$ and $NL$ are of the same
order of magnitude. This context is modelled by an asymptotic regime in which
$NL$ and $M$ both converge towards $\infty$ at the same rate. As in recent
works devoted to the study of (unsmoothed) subspace methods in the case where
$M$ and $N$ are of the same order of magnitude, it is shown that it is still
possible to derive improved DoA estimators termed as Generalized-MUSIC with
spatial smoothing (G-MUSIC SS). The key ingredient of this work is a technical
result showing that the largest singular values and corresponding singular
vectors of low rank deterministic perturbation of certain Gaussian block-Hankel
large random matrices behave as if the entries of the latter random matrices
were independent identically distributed. This allows to conclude that when the
number of sources and their DoA do not scale with $M,N,L,$ a situation
modelling widely spaced DoA scenarios, then both traditional and Generalized
spatial smoothing subspace methods provide consistent DoA estimators whose
convergence speed is faster than $\frac{1}{M}$. The case of DoA that are spaced
of the order of a beamwidth, which models closely spaced sources, is also
considered. It is shown that the convergence speed of G-MUSIC SS estimates is
unchanged, but that it is no longer the case for MUSIC SS ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08223</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08223</id><created>2015-03-27</created><authors><author><keyname>Arathorn</keyname><forenames>David W.</forenames></author></authors><title>A System View of the Recognition and Interpretation of Observed Human
  Shape, Pose and Action</title><categories>cs.CV</categories><comments>41 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is physiological evidence that our ability to interpret human pose and
action from 2D visual imagery (binocular or monocular) engages the circuitry of
the motor cortices as well as the visual areas of the brain. This implies that
the capability of the motor cortices to solve inverse kinematics is flexible
enough to apply to both motion planning as well as serving as a generative
model for the visual processing of human figures, despite the differing
functional requirements of the two tasks. This paper provides a computational
model of the cooperation between visual and motor areas: in other words, a
system view of an important class of brain computations. The model unifies the
solution of the separate inverse problems involved in the task, visual
transformation discovery, inverse kinematics, and adaptation to morphology
variations, using several instances of the Map-seeking Circuit algorithm. While
the paper is weighted toward the exposition of a neurobiological hypothesis,
from mathematical formalization of the problem to neuronal circuitry, the
algorithmic expression of the solution is also a functional machine vision
system for human figure recognition, and 3D pose and body morphology
reconstruction from monocular, perspective-less input imagery. With an inverse
kinematic generative model capable of imposing a variety of endogenous and
exogenous constraints the machine vision implementation acquires
characteristics currently unique among such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08227</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08227</id><created>2015-03-27</created><updated>2015-05-02</updated><authors><author><keyname>Ye</keyname><forenames>Qiaoyang</forenames></author><author><keyname>Bursalioglu</keyname><forenames>Ozgun Y.</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos C.</forenames></author></authors><title>Harmonized Cellular and Distributed Massive MIMO: Load Balancing and
  Scheduling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tier networks with large-array base stations (BSs) that are able to
operate in the &quot;massive MIMO&quot; regime are envisioned to play a key role in
meeting the exploding wireless traffic demands. Operated over small cells with
reciprocity-based training, massive MIMO promises large spectral efficiencies
per unit area with low overheads. Also, near-optimal user-BS association and
resource allocation are possible in cellular massive MIMO HetNets using simple
admission control mechanisms and rudimentary BS schedulers, since scheduled
user rates can be predicted a priori with massive MIMO.
  Reciprocity-based training naturally enables coordinated multi-point
transmission (CoMP), as each uplink pilot inherently trains antenna arrays at
all nearby BSs. In this paper we consider a distributed-MIMO form of CoMP,
which improves cell-edge performance without requiring channel state
information exchanges among cooperating BSs. We present methods for harmonized
operation of distributed and cellular massive MIMO in the downlink that
optimize resource allocation at a coarser time scale across the network. We
also present scheduling policies at the resource block level which target
approaching the optimal allocations. Simulations reveal that the proposed
methods can significantly outperform the network-optimized cellular-only
massive MIMO operation (i.e., operation without CoMP), especially at the cell
edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08237</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08237</id><created>2015-03-27</created><authors><author><keyname>Mara&#x161;evi&#x107;</keyname><forenames>Jelena</forenames></author><author><keyname>Zhou</keyname><forenames>Jin</forenames></author><author><keyname>Krishnaswamy</keyname><forenames>Harish</forenames></author><author><keyname>Zhong</keyname><forenames>Yuan</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Resource Allocation and Rate Gains in Practical Full-Duplex Systems</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex communication has the potential to substantially increase the
throughput in wireless networks. However, the benefits of full-duplex are still
not well understood. In this paper, we characterize the full-duplex rate gains
in both single-channel and multi-channel use cases. For the single-channel
case, we quantify the rate gain as a function of the remaining
self-interference and SNR values. We also provide a sufficient condition under
which the sum of uplink and downlink rates on a full-duplex channel is concave
in the transmission power levels. Building on these results, we consider the
multi-channel case. For that case, we introduce a new realistic model of a
small form-factor (e.g., smartphone) full-duplex receiver and demonstrate its
accuracy via measurements. We study the problem of jointly allocating power
levels to different channels and selecting the frequency of maximum
self-interference suppression, where the objective is maximizing the sum of the
rates over uplink and downlink OFDM channels. We develop a polynomial time
algorithm which is nearly optimal under very mild restrictions. To reduce the
running time, we develop an efficient nearly-optimal algorithm under the high
SINR approximation. Finally, we demonstrate via numerical evaluations the
capacity gains in the different use cases and obtain insights into the impact
of the remaining self-interference and wireless channel states on the
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08244</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08244</id><created>2015-03-27</created><authors><author><keyname>Sevlian</keyname><forenames>Raffi</forenames></author><author><keyname>Zhao</keyname><forenames>Yue</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Outage Detection in Power Distribution Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An outage detection framework for power distribution networks is proposed.
Given the tree structure of the distribution system, a framework is developed
combining the use of real-time power flow measurements on edges of the tree
with load forecasts at the nodes of the tree. Components of the network are
modeled. A framework for the optimality of the detection problem is proposed
relying on the maximum missed detection probability. An algorithm is proposed
to solve the sensor placement problem for the general tree case. Finally, a set
of case studies is considered using feeder data from the Pacific Northwest
National Laboratories and Pacific Gas and Electric. We show that a 10 \% loss
in mean detection reliability network wide reduces the required sensor density
by $60 \%$ for a typical feeder if efficient use of measurements is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08248</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08248</id><created>2015-03-27</created><updated>2015-10-09</updated><authors><author><keyname>Li</keyname><forenames>Xirong</forenames></author><author><keyname>Uricchio</keyname><forenames>Tiberio</forenames></author><author><keyname>Ballan</keyname><forenames>Lamberto</forenames></author><author><keyname>Bertini</keyname><forenames>Marco</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author><author><keyname>Del Bimbo</keyname><forenames>Alberto</forenames></author></authors><title>Socializing the Semantic Gap: A Comparative Survey on Image Tag
  Assignment, Refinement and Retrieval</title><categories>cs.IR cs.CV cs.MM cs.SI</categories><acm-class>H.3.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Where previous reviews on content-based image retrieval emphasize on what can
be seen in an image to bridge the semantic gap, this survey considers what
people tag about an image. A comprehensive treatise of three closely linked
problems, i.e., image tag assignment, refinement, and tag-based image retrieval
is presented. While existing works vary in terms of their targeted tasks and
methodology, they rely on the key functionality of tag relevance, i.e.
estimating the relevance of a specific tag with respect to the visual content
of a given image and its social context. By analyzing what information a
specific method exploits to construct its tag relevance function and how such
information is exploited, this paper introduces a taxonomy to structure the
growing literature, understand the ingredients of the main works, clarify their
connections and difference, and recognize their merits and limitations. For a
head-to-head comparison between the state-of-the-art, a new experimental
protocol is presented, with training sets containing 10k, 100k and 1m images
and an evaluation on three test sets, contributed by various research groups.
Eleven representative works are implemented and evaluated. Putting all this
together, the survey aims to provide an overview of the past and foster
progress for the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08263</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08263</id><created>2015-03-28</created><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author></authors><title>CRF Learning with CNN Features for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Random Rields (CRF) have been widely applied in image
segmentations. While most studies rely on hand-crafted features, we here
propose to exploit a pre-trained large convolutional neural network (CNN) to
generate deep features for CRF learning. The deep CNN is trained on the
ImageNet dataset and transferred to image segmentations here for constructing
potentials of superpixels. Then the CRF parameters are learnt using a
structured support vector machine (SSVM). To fully exploit context information
in inference, we construct spatially related co-occurrence pairwise potentials
and incorporate them into the energy function. This prefers labelling of object
pairs that frequently co-occur in a certain spatial layout and at the same time
avoids implausible labellings during the inference. Extensive experiments on
binary and multi-class segmentation benchmarks demonstrate the promise of the
proposed method. We thus provide new baselines for the segmentation performance
on the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC
2011 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08264</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08264</id><created>2015-03-28</created><authors><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Wigand</keyname><forenames>Rolf T.</forenames></author></authors><title>Collective Dynamics of Hierarchical Networks</title><categories>cs.CY cs.SI</categories><comments>43 pages, 17 figures, 10 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an increasingly complex, mobile and interconnected world, we face growing
threats of disasters, whether by chance or deliberately. Disruption of
coordinated response and recovery efforts due to organizational, technical,
procedural, random or deliberate attack could result in the risk of massive
loss of life. This requires urgent action to explore the development of optimal
information-sharing environments for promoting collective disaster response and
preparedness using multijurisdictional hierarchical networks. Innovative
approaches to information flow modeling and analysis for dealing with
challenges of coordinating across multi layered agency structures as well as
development of early warnings through social systems using social media
analytics may be pivotal to timely responses to dealing with large scale
disasters where response strategies need to be viewed as a shared
responsibility. How do facilitate the development of collective disaster
response in a multijurisdictional setting? How do we develop and test the level
and effectiveness of shared multijurisdictional hierarchical networks for
improved preparedness and response? What is the role of multi layered training
and exercises in building the shared learning space for collective disaster
preparedness and response? The aim of this is therefore to determine factors
that may be responsible for affecting disaster response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08265</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08265</id><created>2015-03-28</created><authors><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Wigand</keyname><forenames>Rolf T.</forenames></author><author><keyname>Uddin</keyname><forenames>Shahadat</forenames></author></authors><title>Statistical Network Topology for Crisis Informetrics</title><categories>cs.SI</categories><comments>9 pages, 4 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crisis informetrics is considered to be a relatively new and emerging area of
research, which deals with the application of analytical approaches of network
and information science combined with experimental learning approaches of
statistical mechanics to explore communication and information flow, robustness
as well as tolerance of complex crisis networks under threats. In this paper,
we discuss the scale free network property of an organizational communication
network and test both traditional (static) and dynamic topology of social
networks during organizational crises Both types of topologies exhibit similar
characteristics of prominent actors reinforcing the power law distribution
nature of scale free networks. There are no significant fluctuations among the
actor prominence in daily and aggregated networks. We found that email
communication network display a high degree of scale free behavior described by
power law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08271</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08271</id><created>2015-03-28</created><authors><author><keyname>Paredes</keyname><forenames>Martha C. Paredes</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>M. Julia Fern&#xe1;ndez-Getino</forenames></author></authors><title>The Problem of Peak-to-Average Power Ratio in OFDM Systems</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><journal-ref>Revista Digital Facultad de Ingenier\'ia de Sistemas, ReDiFIS,
  Departamento de Inform\'atica y Ciencias de la Computaci\'on - Escuela
  Polit\'ecnica Nacional, (Quito - Ecuador), Volumen 1, No. 2, 2012. ISSN:
  1390:7239</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Orthogonal Frequency Division Multiplexing (OFDM) is widely used in many
digital communication systems due to its advantages such us high bit rate,
strong immunity to multipath and high spectral efficiency but it suffers a high
Peak-to-Average Power Ratio (PAPR) at the transmitted signal. It is very
important to deal with PAPR reduction in OFDM systems to avoid signal
degradation. Currently, the PAPR problem is an active area of research and in
this paper we present several techniques and that mathematically analyzed.
Moreover their advantages and disadvantages have been enumerated in order to
provide the readers the actual situation of the PAPR problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08275</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08275</id><created>2015-03-28</created><authors><author><keyname>Velik</keyname><forenames>Rosemarie</forenames></author><author><keyname>Nicolay</keyname><forenames>Pascal</forenames></author></authors><title>Energy Management in Storage-Augmented, Grid-Connected Prosumer
  Buildings and Neighbourhoods Using a Modified Simulated Annealing
  Optimization</title><categories>cs.AI</categories><comments>Computers &amp; Operations Research, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a modified simulated annealing optimization approach
for automatically determining optimal energy management strategies in
grid-connected, storage-augmented, photovoltaics-supplied prosumer buildings
and neighbourhoods based on user-specific goals. For evaluating the modified
simulated annealing optimizer, a number of test scenarios in the field of
energy self-consumption maximization are defined and results are compared to a
gradient descent and a total state space search approach. The benchmarking
against these two reference methods demonstrates that the modified simulated
annealing approach is able to find significantly better solutions than the
gradient descent algorithm - being equal or very close to the global optimum -
with significantly less computational effort and processing time than the total
state space search approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08286</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08286</id><created>2015-03-28</created><authors><author><keyname>Kuo</keyname><forenames>Chun-Yen</forenames></author><author><keyname>Lin</keyname><forenames>Gang-Xuan</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author></authors><title>The Necessary And Sufficient Condition for Generalized Demixing</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demixing is the problem of identifying multiple structured signals from a
superimposed observation. This work analyzes a general framework, based on
convex optimization, for solving demixing problems. We present a new solution
to determine whether or not a specific convex optimization problem built for
generalized demixing is successful. This solution will also bring about the
possibility to estimate the probability of success by the approximate kinematic
formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08289</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08289</id><created>2015-03-28</created><updated>2015-07-28</updated><authors><author><keyname>Brunelli</keyname><forenames>Matteo</forenames></author></authors><title>Recent advances on inconsistency indices for pairwise comparisons - a
  commentary</title><categories>cs.AI</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper recalls the definition of consistency for pairwise comparison
matrices and briefly presents the concept of inconsistency index in connection
to other aspects of the theory of pairwise comparisons. By commenting on a
recent contribution by Koczkodaj and Szwarc, it will be shown that the
discussion on inconsistency indices is far from being over, and the ground is
still fertile for debates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08294</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08294</id><created>2015-03-28</created><authors><author><keyname>Parigi</keyname><forenames>Giacomo</forenames></author><author><keyname>Stramieri</keyname><forenames>Angelo</forenames></author><author><keyname>Pau</keyname><forenames>Danilo</forenames></author><author><keyname>Piastra</keyname><forenames>Marco</forenames></author></authors><title>A Multi-signal Variant for the GPU-based Parallelization of Growing
  Self-Organizing Networks</title><categories>cs.DC cs.NE</categories><comments>17 pages</comments><journal-ref>Informatics in Control, Automation and Robotics - 9th
  International Conference, ICINCO 2012 Rome, Italy, July 28-31, 2012 Revised
  Selected Papers. Part I, pp. 83-100</journal-ref><doi>10.1007/978-3-319-03500-0_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the many possible approaches for the parallelization of self-organizing
networks, and in particular of growing self-organizing networks, perhaps the
most common one is producing an optimized, parallel implementation of the
standard sequential algorithms reported in the literature. In this paper we
explore an alternative approach, based on a new algorithm variant specifically
designed to match the features of the large-scale, fine-grained parallelism of
GPUs, in which multiple input signals are processed at once. Comparative tests
have been performed, using both parallel and sequential implementations of the
new algorithm variant, in particular for a growing self-organizing network that
reconstructs surfaces from point clouds. The experimental results show that
this approach allows harnessing in a more effective way the intrinsic
parallelism that the self-organizing networks algorithms seem intuitively to
suggest, obtaining better performances even with networks of smaller size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08310</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08310</id><created>2015-03-28</created><authors><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>P&#xe9;rez-Gim&#xe9;nez</keyname><forenames>Xavier</forenames></author><author><keyname>Pra&#x142;at</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Strong-majority bootstrap percolation on regular graphs with low
  dissemination threshold</title><categories>math.CO cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following model of strong-majority bootstrap percolation on a
graph. Let r be some positive integer, and p in [0,1]. Initially, every vertex
is active with probability p, independently from all other vertices. Then, at
every step of the process, each vertex v of degree deg(v) becomes active if at
least (deg(v)+r)/2 of its neighbours are active. Given any arbitrarily small
p&gt;0 and any integer r, we construct a family of d=d(p,r)-regular graphs such
that with high probability all vertices become active in the end. In
particular, the case r=1 answers a question and disproves a conjecture of
Rapaport, Suchan, Todinca, and Verstraete (Algorithmica, 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08312</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08312</id><created>2015-03-28</created><authors><author><keyname>Wu</keyname><forenames>Yan</forenames></author><author><keyname>Venkatramanan</keyname><forenames>Srinivasan</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>A Population Model for the Academic Ecosystem</title><categories>cs.SI cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, the academic ecosystem has seen a tremendous growth in
number of authors and publications. While most temporal studies in this area
focus on evolution of co-author and citation network structure, this systemic
inflation has received very little attention. In this paper, we address this
issue by proposing a population model for academia, derived from publication
records in the Computer Science domain. We use a generalized branching process
as an overarching framework, which enables us to describe the evolution and
composition of the research community in a systematic manner. Further, the
observed patterns allow us to shed light on researchers' lifecycle encompassing
arrival, academic life expectancy, activity, productivity and offspring
distribution in the ecosystem. We believe such a study will help develop better
bibliometric indices which account for the inflation, and also provide insights
into sustainable and efficient resource management for academia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08316</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08316</id><created>2015-03-28</created><updated>2015-06-09</updated><authors><author><keyname>Lucchi</keyname><forenames>Aurelien</forenames></author><author><keyname>McWilliams</keyname><forenames>Brian</forenames></author><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author></authors><title>A Variance Reduced Stochastic Newton Method</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-Newton methods are widely used in practise for convex loss minimization
problems. These methods exhibit good empirical performance on a wide variety of
tasks and enjoy super-linear convergence to the optimal solution. For
large-scale learning problems, stochastic Quasi-Newton methods have been
recently proposed. However, these typically only achieve sub-linear convergence
rates and have not been shown to consistently perform well in practice since
noisy Hessian approximations can exacerbate the effect of high-variance
stochastic gradient estimates. In this work we propose Vite, a novel stochastic
Quasi-Newton algorithm that uses an existing first-order technique to reduce
this variance. Without exploiting the specific form of the approximate Hessian,
we show that Vite reaches the optimum at a geometric rate with a constant
step-size when dealing with smooth strongly convex functions. Empirically, we
demonstrate improvements over existing stochastic Quasi-Newton and variance
reduced stochastic gradient methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08322</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08322</id><created>2015-03-28</created><authors><author><keyname>Parigi</keyname><forenames>Giacomo</forenames></author><author><keyname>Pedrini</keyname><forenames>Andrea</forenames></author><author><keyname>Piastra</keyname><forenames>Marco</forenames></author></authors><title>Some Further Evidence about Magnification and Shape in Neural Gas</title><categories>cs.NE</categories><doi>10.1109/IJCNN.2015.7280550</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Neural gas (NG) is a robust vector quantization algorithm with a well-known
mathematical model. According to this, the neural gas samples the underlying
data distribution following a power law with a magnification exponent that
depends on data dimensionality only. The effects of shape in the input data
distribution, however, are not entirely covered by the NG model above, due to
the technical difficulties involved. The experimental work described here shows
that shape is indeed relevant in determining the overall NG behavior; in
particular, some experiments reveal richer and complex behaviors induced by
shape that cannot be explained by the power law alone. Although a more
comprehensive analytical model remains to be defined, the evidence collected in
these experiments suggests that the NG algorithm has an interesting potential
for detecting complex shapes in noisy datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08323</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08323</id><created>2015-03-28</created><authors><author><keyname>Junosza-Szaniawski</keyname><forenames>Konstanty</forenames></author><author><keyname>Tuczynski</keyname><forenames>Michal</forenames></author></authors><title>Counting independent sets via Divide Measure and Conquer method</title><categories>cs.DM math.CO</categories><comments>20 pages</comments><msc-class>05C15</msc-class><acm-class>G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give an algorithm for counting the number of all independent
sets in a given graph which works in time $O^*(1.1394^n)$ for subcubic graphs
and in time $O^*(1.2369^n)$ for general graphs, where $n$ is the number of
vertices in the instance graph, and polynomial space. The result comes from
combining two well known methods &quot;Divide and Conquer&quot; and &quot;Measure and
Conquer&quot;. We introduce this new concept of Divide, Measure and Conquer method
and expect it will find applications in other problems.
  The algorithm of Bj\&quot;orklund, Husfeldt and Koivisto for graph colouring with
our algorithm used as a subroutine has complexity $O^*(2.2369^n)$ and is
currently the fastest graph colouring algorithm in polynomial space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08329</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08329</id><created>2015-03-28</created><updated>2015-07-28</updated><authors><author><keyname>Germain</keyname><forenames>Pascal</forenames></author><author><keyname>Lacasse</keyname><forenames>Alexandre</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Roy</keyname><forenames>Jean-Francis</forenames></author></authors><title>Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a
  Learning Algorithm</title><categories>stat.ML cs.LG</categories><comments>Published in JMLR http://jmlr.org/papers/v16/germain15a.html</comments><journal-ref>Journal of Machine Learning Research 2015, vol. 16, p. 787-860</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an extensive analysis of the behavior of majority votes in binary
classification. In particular, we introduce a risk bound for majority votes,
called the C-bound, that takes into account the average quality of the voters
and their average disagreement. We also propose an extensive PAC-Bayesian
analysis that shows how the C-bound can be estimated from various observations
contained in the training data. The analysis intends to be self-contained and
can be used as introductory material to PAC-Bayesian statistical learning
theory. It starts from a general PAC-Bayesian perspective and ends with
uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler
divergence and others allow kernel functions to be used as voters (via the
sample compression setting). Finally, out of the analysis, we propose the MinCq
learning algorithm that basically minimizes the C-bound. MinCq reduces to a
simple quadratic program. Aside from being theoretically grounded, MinCq
achieves state-of-the-art performance, as shown in our extensive empirical
comparison with both AdaBoost and the Support Vector Machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08345</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08345</id><created>2015-03-28</created><updated>2015-08-22</updated><authors><author><keyname>Singh</keyname><forenames>Pravendra</forenames></author></authors><title>Implementing an intelligent version of the classical sliding-puzzle game
  for unix terminals using Golang's concurrency primitives</title><categories>cs.AI</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An intelligent version of the sliding-puzzle game is developed using the new
Go programming language, which uses a concurrent version of the A* Informed
Search Algorithm to power solver-bot that runs in the background. The game runs
in computer system's terminals. Mainly, it was developed for UNIX-type systems
but it works pretty well in nearly all the operating systems because of
cross-platform compatibility of the programming language used. The game uses
language's concurrency primitives to simplify most of the hefty parts of the
game. A real-time notification delivery architecture is developed using
language's built-in concurrency support, which performs similar to event based
context aware invocations like we see on the web platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08348</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08348</id><created>2015-03-28</created><authors><author><keyname>Ganti</keyname><forenames>Ravi</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author></authors><title>Sparse Linear Regression With Missing Data</title><categories>stat.ML cs.LG stat.ME</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fast and accurate method for sparse regression in the
presence of missing data. The underlying statistical model encapsulates the
low-dimensional structure of the incomplete data matrix and the sparsity of the
regression coefficients, and the proposed algorithm jointly learns the
low-dimensional structure of the data and a linear regressor with sparse
coefficients. The proposed stochastic optimization method, Sparse Linear
Regression with Missing Data (SLRM), performs an alternating minimization
procedure and scales well with the problem size. Large deviation inequalities
shed light on the impact of the various problem-dependent parameters on the
expected squared loss of the learned regressor. Extensive simulations on both
synthetic and real datasets show that SLRM performs better than competing
algorithms in a variety of contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08360</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08360</id><created>2015-03-28</created><updated>2015-04-09</updated><authors><author><keyname>Karimi</keyname><forenames>S.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>Do current lattice Boltzmann methods for diffusion and diffusion-type
  equations respect maximum principles and the non-negative constraint?</title><categories>cs.NA math-ph math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lattice Boltzmann method (LBM) has established itself as a valid
numerical method in computational fluid dynamics. Recently,
multiple-relaxation-time LBM has been proposed to simulate anisotropic
advection-diffusion processes. The governing differential equations of
advective-diffusive systems are known to satisfy maximum principles, comparison
principles, the non-negative constraint, and the decay property. In this paper,
it will be shown that current single- and multiple-relaxation-time lattice
Boltzmann methods fail to preserve these mathematical properties for transient
diffusion-type equations. It will also be shown that the discretization of
Dirichlet boundary conditions will affect the performance of lattice Boltzmann
methods in meeting these mathematical principles. A new way of discretizing the
Dirichlet boundary conditions is also proposed. Several benchmark problems have
been solved to illustrate the performance of lattice Boltzmann methods and the
effect of discretization of boundary conditions with respect to the
aforementioned mathematical properties for transient diffusion and
advection-diffusion equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08363</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08363</id><created>2015-03-28</created><authors><author><keyname>Ganti</keyname><forenames>Ravi</forenames></author></authors><title>Active Model Aggregation via Stochastic Mirror Descent</title><categories>stat.ML cs.AI cs.LG</categories><comments>12 pages, 20 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning convex aggregation of models, that is as
good as the best convex aggregation, for the binary classification problem.
Working in the stream based active learning setting, where the active learner
has to make a decision on-the-fly, if it wants to query for the label of the
point currently seen in the stream, we propose a stochastic-mirror descent
algorithm, called SMD-AMA, with entropy regularization. We establish an excess
risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of
the order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in
[0,1)$ is an algorithm dependent parameter, that trades-off the number of
labels queried, and excess risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08370</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08370</id><created>2015-03-28</created><authors><author><keyname>Atan</keyname><forenames>Onur</forenames></author><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Global Bandits</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.7890</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard multi-armed bandits model decision problems in which the
consequences of each action choice are unknown and independent of each other.
But in a wide variety of decision problems - from drug dosage to dynamic
pricing - the consequences (rewards) of different actions are correlated, so
that selecting one action provides information about the consequences (rewards)
of other actions as well. We propose and analyze a class of models of such
decision problems; we call this class of models global bandits. When rewards
across actions (arms) are sufficiently correlated we construct a greedy policy
that achieves bounded regret, with a bound that depends on the true parameters
of the problem. In the special case in which rewards of all arms are
deterministic functions of a single unknown parameter, we construct a (more
sophisticated) greedy policy that achieves bounded regret, with a bound that
depends on the single true parameter of the problem. For this special case we
also obtain a bound on regret that is independent of the true parameter; this
bound is sub-linear, with an exponent that depends on the informativeness of
the arms (which measures the strength of correlation between arm rewards).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08376</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08376</id><created>2015-03-28</created><authors><author><keyname>Botchkarev</keyname><forenames>Alexei</forenames></author></authors><title>Assessing Excel VBA Suitability for Monte Carlo Simulation</title><categories>cs.MS stat.CO</categories><journal-ref>Spreadsheets in Education (eJSiE): 2015, Vol. 8: Iss. 2, Article 3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo (MC) simulation includes a wide range of stochastic techniques
used to quantitatively evaluate the behavior of complex systems or processes.
Microsoft Excel spreadsheets with Visual Basic for Applications (VBA) software
is, arguably, the most commonly employed general purpose tool for MC
simulation. Despite the popularity of the Excel in many industries and
educational institutions, it has been repeatedly criticized for its flaws and
often described as questionable, if not completely unsuitable, for statistical
problems. The purpose of this study is to assess suitability of the Excel
(specifically its 2010 and 2013 versions) with VBA programming as a tool for MC
simulation. The results of the study indicate that Microsoft Excel (versions
2010 and 2013) is a strong Monte Carlo simulation application offering a solid
framework of core simulation components including spreadsheets for data input
and output, VBA development environment and summary statistics functions. This
framework should be complemented with an external high-quality pseudo-random
number generator added as a VBA module. A large and diverse category of Excel
incidental simulation components that includes statistical distributions,
linear and non-linear regression and other statistical, engineering and
business functions require execution of due diligence to determine their
suitability for a specific MC project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08379</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08379</id><created>2015-03-28</created><authors><author><keyname>Lou</keyname><forenames>Taishan</forenames></author></authors><title>Consider Uncertain Parameters based on Sensitivity Matrix</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertain parameters of state-space models have always been a considerable
problem. Consider Kalman filter (CKF) and desensitized Kalman filter (DKF) are
two methods to solve this problem. Based on the sensitivity matrix respected to
the uncertain parameter vector, a special DKF with an analytical gain is given
and a new form of the CKF is derived. The mathematical equivalence between the
special DKF and the CKF is demonstrated when the sensitivity-weighting matrix
is set to the covariance of the uncertain parameter and the problem how to
select and obtain the sensitivity-weighting matrix in the DKF is solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08381</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08381</id><created>2015-03-28</created><authors><author><keyname>Sun</keyname><forenames>Xu</forenames></author></authors><title>Towards Shockingly Easy Structured Classification: A Search-based
  Probabilistic Online Learning Framework</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two major approaches for structured classification. One is the
probabilistic gradient-based methods such as conditional random fields (CRF),
which has high accuracy but with drawbacks: slow training, and no support of
search-based optimization (which is important in many cases). The other one is
the search-based learning methods such as perceptrons and margin infused
relaxed algorithm (MIRA), which have fast training but also with drawbacks: low
accuracy, no probabilistic information, and non-convergence in real-world
tasks. We propose a novel and &quot;shockingly easy&quot; solution, a search-based
probabilistic online learning method, to address most of those issues. This
method searches the output candidates, derives probabilities, and conduct
efficient online learning. We show that this method is with fast training,
support search-based optimization, very easy to implement, with top accuracy,
with probabilities, and with theoretical guarantees of convergence. Experiments
on well-known tasks show that our method has better accuracy than CRF and
almost as fast training speed as perceptron and MIRA. Results also show that
SAPO can easily beat the state-of-the-art systems on those highly-competitive
tasks, achieving record-breaking accuracies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08383</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08383</id><created>2015-03-29</created><authors><author><keyname>Cezar</keyname><forenames>Gustavo</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author></authors><title>Stability of Interconnected DC Converters</title><categories>math.OC cs.SY</categories><comments>Submitted to CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses stability issues of DC networks with constant power
loads (CPL). Common DC networks, such as automotive electrical systems and DC
microgrids, typically have a step-up/down converter connected in one side to
the main bus and, on the other, to the load. When load is constant power it can
generate destabilizing effects if not proper controlled. This paper shows that
converters driving CPLs can make the system unstable, even if they are
individually stable, depending on network parameters. We mitigate this problem
by means of passive components externally connected to the converter/CPL
subsystem. The analysis is verified through simulations. We are able to show
that certain converter circuit configurations achieve the so called
plug-and-play property, which stabilizes the interconnected system for all
network parameters. This property is desirable since it is does not require the
knowledge of detailed system topology and parameters, which can be time varying
and difficult to obtain. This method also contrasts to existing practices of
load augmentation, which can lead to severe efficiency losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08393</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08393</id><created>2015-03-29</created><updated>2015-09-23</updated><authors><author><keyname>Su</keyname><forenames>Weijie</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author></authors><title>SLOPE is Adaptive to Unknown Sparsity and Asymptotically Minimax</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>To appear in the Annals of Statistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider high-dimensional sparse regression problems in which we observe
$y = X \beta + z$, where $X$ is an $n \times p$ design matrix and $z$ is an
$n$-dimensional vector of independent Gaussian errors, each with variance
$\sigma^2$. Our focus is on the recently introduced SLOPE estimator ((Bogdan et
al., 2014)), which regularizes the least-squares estimates with the
rank-dependent penalty $\sum_{1 \le i \le p} \lambda_i |\hat \beta|_{(i)}$,
where $|\hat \beta|_{(i)}$ is the $i$th largest magnitude of the fitted
coefficients. Under Gaussian designs, where the entries of $X$ are
i.i.d.~$\mathcal{N}(0, 1/n)$, we show that SLOPE, with weights $\lambda_i$ just
about equal to $\sigma \cdot \Phi^{-1}(1-iq/(2p))$ ($\Phi^{-1}(\alpha)$ is the
$\alpha$th quantile of a standard normal and $q$ is a fixed number in $(0,1)$)
achieves a squared error of estimation obeying \[ \sup_{\| \beta\|_0 \le k}
\,\, \mathbb{P} \left(\| \hat{\beta}_{\text{SLOPE}} - \beta \|^2 &gt; (1+\epsilon)
\, 2\sigma^2 k \log(p/k) \right) \longrightarrow 0 \] as the dimension $p$
increases to $\infty$, and where $\epsilon &gt; 0$ is an arbitrary small constant.
This holds under a weak assumption on the $\ell_0$-sparsity level, namely, $k/p
\rightarrow 0$ and $(k\log p)/n \rightarrow 0$, and is sharp in the sense that
this is the best possible error any estimator can achieve. A remarkable feature
is that SLOPE does not require any knowledge of the degree of sparsity, and yet
automatically adapts to yield optimal total squared errors over a wide range of
$\ell_0$-sparsity classes. We are not aware of any other estimator with this
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08395</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08395</id><created>2015-03-29</created><updated>2015-05-15</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Towards More Efficient Symmetric Matrix Sketching and the CUR Matrix
  Decomposition</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix sketching schemes and the Nystr\&quot;om method have both been extensively
used to speed up large-scale eigenvalue computation and kernel learning
methods. Matrix sketching methods produce accurate matrix approximations, but
they are only computationally efficient on skinny matrices where one of the
matrix dimensions is relatively small. In particular, they are not efficient on
large square matrices. The Nystr\&quot;om method, on the other hand, is highly
efficient on symmetric (and thus square) matrices, but can only achieve low
matrix approximation accuracy. In this paper we propose a novel combination of
the sketching method and the Nystr\&quot;om method to improve their
efficiency/effectiveness, leading to a novel approximation which we call the
Sketch-Nystr\&quot;om method. The Sketch-Nystr\&quot;om method is computationally nearly
as efficient as the Nystr\&quot;om method on symmetric matrices with approximation
accuracy comparable to that of the sketching method. We show theoretically that
the Sketch-Nystr\&quot;om method can potentially solve eigenvalue problems and
kernel learning problems in linear time with respect to the matrix size to
achieve $1+\epsilon$ relative-error, whereas the sketch methods and the
Nystr\&quot;om method cost at least quadratic time to attain comparable error bound.
Our technique can be straightforwardly applied to make the CUR matrix
decomposition more efficiently computed without much affecting the accuracy.
Empirical experiments demonstrate the effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08396</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08396</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author></authors><title>Flow Demands Oriented Node Placement in Multi-Hop Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-hop wireless networks, flow demands mean that some nodes have
routing demands of transmitting their data to other nodes with a certain level
of transmission rate. When a set of nodes have been deployed with flow demands,
it is worth to know how to construct paths to satisfy these flow demands with
nodes placed as few as possible. In this paper, we study this flow demands
oriented node placement problem that has not been addressed before. In
particular, we divide and conquer the problem by three steps: calculating the
maximal flow for single routing demand, calculating the maximal flow for
multiple routing demands, and finding the minimal number of nodes for multiple
routing demands with flow requirement. During the above solving procedure, we
prove that the second and third step are NP-hard and propose two algorithms
that have polynomial-time complexity. The proposed algorithms are evaluated
under practical scenarios. The experiments show that the proposed algorithms
can achieve satisfactory results on both flow demands and total number of
wireless nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08398</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08398</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author></authors><title>A Cyber-Human Interaction Based System on Mobile Phone for Indoor
  Localization</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study the Cyber-Human Interaction (CHI) based approach
that the &quot;Human&quot; part sets a list of location-based objectives and makes the
pathway decision whereas the &quot;Cyber&quot; part provides the pathway suggestion,
infer heuristics from the environment along the pathway and incrementally
resolve the location-based objectives with new heuristics for indoor
localization. For this study, we implement a CHI-based system on mobile phone.
The CHI-based system offers the pathway suggestion and the solution of the
location-based objectives based on its trajectory management. Without any
priori knowledge on the area of interest and any aid from other equipments, a
laborer can achieve his location-based objectives by walking through the area
of interest and simultaneously online interacting with the CHI-based system
installed in his phone. In evaluation, we conduct the experiments and show the
advantage CHI in reducing the time cost and the expense cost for the laborer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08400</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08400</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author><author><keyname>Guo</keyname><forenames>Shusheng</forenames></author></authors><title>Online Query Scheduling on Source Permutation for Big Data Integration</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data integration could involve a large number of sources with
unpredictable redundancy information between them. The approach of building a
central warehousing to integrate big data from all sources then becomes
infeasible because of so large number of sources and continuous updates
happening. A practical approach is to apply online query scheduling that
inquires data from sources at runtime upon receiving a query. In this paper, we
address the Time-Cost Minimization Problem for online query scheduling, and
tackle the challenges of source permutation and statistics estimation to
minimize the time cost of retrieving answers for the real-time receiving query.
We propose the online scheduling strategy that enables the improvement of
statistics, the construction of source permutation and the execution of query
working in parallel. Experimental results show high efficiency and scalability
of our scheduling strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08404</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08404</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author><author><keyname>Zhao</keyname><forenames>Wei</forenames></author></authors><title>Beacon Node Placement for Minimal Localization Error</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beacon node placement, node-to-node measurement, and target node positioning
are the three key steps for a localization process. However, compared with the
other two steps, beacon node placement still lacks a comprehensive, systematic
study in research literatures. To fill this gap, we address the Beacon Node
Placment (BNP) problem that deploys beacon nodes for minimal localization error
in this paper. BNP is difficult in that the localization error is determined by
a complicated combination of factors, i.e., the localization error differing
greatly under a different environment, with a different algorithm applied, or
with a different type of beacon node used. In view of the hardness of BNP, we
propose an approximate function to reduce time cost in localization error
calculation, and also prove its time complexity and error bound. By
approximation, a sub-optimal distribution of beacon nodes could be found within
acceptable time cost for placement. In the experiment, we test our method and
compare it with other node placement methods under various settings and
environments. The experimental results show feasibility and effectiveness of
our method in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08407</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08407</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author></authors><title>CIUV: Collaborating Information Against Unreliable Views</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real world applications, the information of an object can be obtained
from multiple sources. The sources may provide different point of views based
on their own origin. As a consequence, conflicting pieces of information are
inevitable, which gives rise to a crucial problem: how to find the truth from
these conflicts. Many truth-finding methods have been proposed to resolve
conflicts based on information trustworthy (i.e. more appearance means more
trustworthy) as well as source reliability. However, the factor of men's
involvement, i.e., information may be falsified by men with malicious
intension, is more or less ignored in existing methods. Collaborating the
possible relationship between information's origins and men's participation are
still not studied in research. To deal with this challenge, we propose a method
-- Collaborating Information against Unreliable Views (CIUV) --- in dealing
with men's involvement for finding the truth. CIUV contains 3 stages for
interactively mitigating the impact of unreliable views, and calculate the
truth by weighting possible biases between sources. We theoretically analyze
the error bound of CIUV, and conduct intensive experiments on real dataset for
evaluation. The experimental results show that CIUV is feasible and has the
smallest error compared with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08413</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08413</id><created>2015-03-29</created><authors><author><keyname>Yemini</keyname><forenames>Michal</forenames></author><author><keyname>Somekh-Baruch</keyname><forenames>Anelia</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>On the Multiple Access Channel with Asynchronous Cognition</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1402.1617</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the two-user asynchronous cognitive multiple
access channel (ACMAC). This channel model includes two transmitters, an
uninformed one, and an informed one which knows prior to the beginning of a
transmission the message which the uninformed transmitter is about to send. We
assume that the channel from the uninformed transmitter to the receiver suffers
a fixed but unknown delay. We further introduce a modified model, referred to
as the ACC-MAC, which differs from the ACMAC in that the informed user knows
the signal that is to be transmitted by the other user, rather than the message
that it is about to transmit. We state single-letter inner and outer bounds on
the ACMAC and the ACC-MAC capacity regions, and we specialize the results to
the Gaussian case. Further, we characterize the capacity regions of these
channels in terms of multi-letter expressions. Finally, we provide an example
which instantiates the difference between message side-information and codeword
side-information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08421</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08421</id><created>2015-03-29</created><updated>2015-04-10</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>On Resilient Behaviors in Computational Systems and Environments</title><categories>cs.SY</categories><comments>The final publication is available at Springer via
  http://dx.doi.org/10.1007/s40860-015-0002-6 The paper considerably extends
  the results of two conference papers that are available at http://ow.ly/KWfkj
  and http://ow.ly/KWfgO. Text and formalism in those papers has been used or
  adapted in the herewith submitted paper</comments><doi>10.1007/s40860-015-0002-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present article introduces a reference framework for discussing
resilience of computational systems. Rather than a property that may or may not
be exhibited by a system, resilience is interpreted here as the emerging result
of a dynamic process. Said process represents the dynamic interplay between the
behaviors exercised by a system and those of the environment it is set to
operate in. As a result of this interpretation, coherent definitions of several
aspects of resilience can be derived and proposed, including elasticity, change
tolerance, and antifragility. Definitions are also provided for measures of the
risk of unresilience as well as for the optimal match of a given resilient
design with respect to the current environmental conditions. Finally, a
resilience strategy based on our model is exemplified through a simple
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08434</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08434</id><created>2015-03-29</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Full-Duplex Radio for Uplink/Downlink Transmission with Spatial
  Randomness</title><categories>cs.IT math.IT</categories><comments>Accepted for the IEEE International Conference on Communications (ICC
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless system with a full-duplex (FD) access point (AP) that
transmits to a scheduled user in the downlink (DL) channel, while receiving
data from an user in the uplink (UL) channel at the same time on the same
frequency. In this system, loopback interference (LI) at the AP and inter user
interference between the uplink (UL) user and downlink (DL) user can cause
performance degradation. In order to characterize the effects of LI and inter
user interference, we derive closed-form expressions for the outage probability
and achievable sum rate of the system. In addition an asymptotic analysis that
reveals insights into the system behavior and performance degradation is
presented. Our results indicate that under certain conditions, FD transmissions
yield performance gains over half-duplex (HD) mode of operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08436</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08436</id><created>2015-03-29</created><authors><author><keyname>Zhang</keyname><forenames>Xinlin</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author></authors><title>Impact of Residual Transmit RF Impairments on Training-Based MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>31 pages, submitted to the IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio-frequency (RF) impairments, which intimately exist in wireless
communication systems, can severely limit the performance of multiple-input
multiple-output (MIMO) systems. Although we can resort to compensation schemes
to mitigate part of these impairments, a certain amount of residual impairments
always persists. In this paper, we consider a training-based point-to-point
MIMO system with residual transmit RF impairments (RTRI) using spatial
multiplexing transmission. Specifically, we derive a new linear channel
estimator for the proposed model, and show that RTRI create an estimation error
floor in the high signal-to-noise ratio (SNR) regime. Moreover, we derive
closed-form expressions for the signal-to-noise-and-interference ratio (SINR)
distributions, along with analytical expressions for the ergodic achievable
rates of zero-forcing, maximum ratio combining, and minimum mean-squared error
receivers, respectively. In addition, we optimize the ergodic achievable rates
with respect to the training sequence length, and demonstrate that finite
dimensional systems with RTRI generally require more training at high SNRs than
those with ideal hardware. At last, we extend our analysis to large-scale MIMO
configurations, and derive deterministic equivalents of the ergodic achievable
rates. It is shown that, by deploying large receive antenna arrays, the extra
training requirements due to RTRI can be eliminated. In fact, with sufficiently
large number of receive antennas, systems with RTRI may even need less training
than systems with ideal hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08453</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08453</id><created>2015-03-29</created><authors><author><keyname>Romanelli</keyname><forenames>Alejandro</forenames></author></authors><title>Quantum walk, entanglement and thermodynamic laws</title><categories>quant-ph cs.IT math.IT</categories><comments>It was accepted to publish in Physica A</comments><msc-class>00</msc-class><doi>10.1016/j.physa.2015.03.084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an special dynamics of a quantum walk (QW) on a line. Initially,
the walker localized at the origin of the line with arbitrary chirality,
evolves to an asymptotic stationary state. In this stationary state a
measurement is performed and the state resulting from this measurement is used
to start a second QW evolution to achieve a second asymptotic stationary state.
In previous works, we developed the thermodynamics associated with the
entanglement between the coin and position degrees of freedom in the QW. Here
we study the application of the ?rst and second laws of thermodynamics to the
process between the two stationary states mentioned above. We show that: i) the
entropy change has upper and lower bounds that are obtained analytically as a
function of the initial conditions. ii) the energy change is associated to a
heat-transfer process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08454</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08454</id><created>2015-03-29</created><authors><author><keyname>Arif</keyname><forenames>M. Fareed</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>Towards Efficient Axiom Pinpointing of EL+ Ontologies</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EL family of Description Logics (DLs) has been the subject of interest in
recent years. On the one hand, these DLs are tractable, but fairly
inexpressive. On the other hand, these DLs can be used for designing different
classes of ontologies, most notably ontologies from the medical domain.
Unfortunately, building ontologies is error-prone. As a result, inferable
subsumption relations among concepts may be unintended. In recent years, the
problem of axiom pinpointing has been studied with the purpose of providing
minimal sets of axioms that explain unintended subsumption relations. For the
concrete case of EL and EL+, the most efficient approaches consist of encoding
the problem into propositional logic, specifically as a Horn formula, which is
then analyzed with a dedicated algorithm. This paper builds on this earlier
work, but exploits the important relationship between minimal axioms sets and
minimal unsatisfiable subformulas in the propositional domain. In turn, this
relationship allows applying a vast body of recent work in the propositional
domain to the concrete case of axiom pinpointing for EL and its variants. From
a practical perspective, the algorithms described in this paper are often
several orders of magnitude more efficient that the current state of the art in
axiom pinpointing for the EL family of DLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08463</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08463</id><created>2015-03-29</created><authors><author><keyname>Roul</keyname><forenames>Rajendra Kumar</forenames></author><author><keyname>Varshneya</keyname><forenames>Saransh</forenames></author><author><keyname>Kalra</keyname><forenames>Ashu</forenames></author><author><keyname>Sahay</keyname><forenames>Sanjay Kumar</forenames></author></authors><title>A Novel Modified Apriori Approach for Web Document Clustering</title><categories>cs.IR</categories><comments>11 Pages, 5 Figures</comments><journal-ref>Springer, Smart Innovation Systems and Technologies, Vol. 33,
  2015, p. 159-171; Proceedings of the ICCIDM, Dec. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional apriori algorithm can be used for clustering the web
documents based on the association technique of data mining. But this algorithm
has several limitations due to repeated database scans and its weak association
rule analysis. In modern world of large databases, efficiency of traditional
apriori algorithm would reduce manifolds. In this paper, we proposed a new
modified apriori approach by cutting down the repeated database scans and
improving association analysis of traditional apriori algorithm to cluster the
web documents. Further we improve those clusters by applying Fuzzy C-Means
(FCM), K-Means and Vector Space Model (VSM) techniques separately. For
experimental purpose, we use Classic3 and Classic4 datasets of Cornell
University having more than 10,000 documents and run both traditional apriori
and our modified apriori approach on it. Experimental results show that our
approach outperforms the traditional apriori algorithm in terms of database
scan and improvement on association of analysis. We found out that FCM is
better than K-Means and VSM in terms of F-measure of clusters of different
sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08471</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08471</id><created>2015-03-29</created><updated>2015-12-13</updated><authors><author><keyname>Shimodaira</keyname><forenames>Hidetoshi</forenames></author></authors><title>Cross-validation of matching correlation analysis by resampling matching
  weights</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The strength of association between a pair of data vectors is represented by
a nonnegative real number, called matching weight. For dimensionality
reduction, we consider a linear transformation of data vectors, and define a
matching error as the weighted sum of squared distances between transformed
vectors with respect to the matching weights. Given data vectors and matching
weights, the optimal linear transformation minimizing the matching error is
solved by the spectral graph embedding of Yan et al. (2007). This method is a
generalization of the canonical correlation analysis, and will be called as
matching correlation analysis (MCA). In this paper, we consider a novel
sampling scheme where the observed matching weights are randomly sampled from
underlying true matching weights with small probability, whereas the data
vectors are treated as constants. We then investigate a cross-validation by
resampling the matching weights. Our asymptotic theory shows that the
cross-validation, if rescaled properly, computes an unbiased estimate of the
matching error with respect to the true matching weights. Existing ideas of
cross-validation for resampling data vectors, instead of resampling matching
weights, are not applicable here. MCA can be used for data vectors from
multiple domains with different dimensions via an embarrassingly simple idea of
coding the data vectors. This method will be called as cross-domain matching
correlation analysis (CDMCA), and an interesting connection to the classical
associative memory model of neural networks is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08473</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08473</id><created>2015-03-29</created><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Bearing-Based Distributed Control and Estimation of Multi-Agent Systems</title><categories>cs.SY</categories><comments>6 pages, to appear in the 2015 European Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the distributed control and estimation of multi-agent
systems based on bearing information. In particular, we consider two problems:
(i) the distributed control of bearing-constrained formations using relative
position measurements and (ii) the distributed localization of sensor networks
using bearing measurements. Both of the two problems are considered in
arbitrary dimensional spaces. The analyses of the two problems rely on the
recently developed bearing rigidity theory. We show that the two problems have
the same mathematical formulation and can be solved by identical protocols. The
proposed controller and estimator can globally solve the two problems without
ambiguity. The results are supported with illustrative simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08476</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08476</id><created>2015-03-29</created><authors><author><keyname>Zaytsev</keyname><forenames>Vadim</forenames></author></authors><title>Guided Grammar Convergence</title><categories>cs.SE cs.FL cs.PL</categories><comments>In Poster Proceedings of 6th Conference on Software Language
  Engineering (SLE) 2013, http://www.sleconf.org/2013/</comments><acm-class>F.4.2; F.4.3; I.2.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Relating formal grammars is a hard problem that balances between language
equivalence (which is known to be undecidable) and grammar identity (which is
trivial). In this paper, we investigate several milestones between those two
extremes and propose a methodology for inconsistency management in grammar
engineering. While conventional grammar convergence is a practical approach
relying on human experts to encode differences as transformation steps, guided
grammar convergence is a more narrowly applicable technique that infers such
transformation steps automatically by normalising the grammars and establishing
a structural equivalence relation between them. This allows us to perform a
case study with automatically inferring bidirectional transformations between
11 grammars (in a broad sense) of the same artificial functional language:
parser specifications with different combinator libraries, definite clause
grammars, concrete syntax definitions, algebraic data types, metamodels, XML
schemata, object models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08479</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08479</id><created>2015-03-29</created><authors><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author><author><keyname>Greenstadt</keyname><forenames>Rachel</forenames></author><author><keyname>Kam</keyname><forenames>Moshe</forenames></author></authors><title>Active Authentication on Mobile Devices via Stylometry, Application
  Usage, Web Browsing, and GPS Location</title><categories>cs.CR stat.ML</categories><comments>Accepted for Publication in the IEEE Systems Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active authentication is the problem of continuously verifying the identity
of a person based on behavioral aspects of their interaction with a computing
device. In this study, we collect and analyze behavioral biometrics data from
200subjects, each using their personal Android mobile device for a period of at
least 30 days. This dataset is novel in the context of active authentication
due to its size, duration, number of modalities, and absence of restrictions on
tracked activity. The geographical colocation of the subjects in the study is
representative of a large closed-world environment such as an organization
where the unauthorized user of a device is likely to be an insider threat:
coming from within the organization. We consider four biometric modalities: (1)
text entered via soft keyboard, (2) applications used, (3) websites visited,
and (4) physical location of the device as determined from GPS (when outdoors)
or WiFi (when indoors). We implement and test a classifier for each modality
and organize the classifiers as a parallel binary decision fusion architecture.
We are able to characterize the performance of the system with respect to
intruder detection time and to quantify the contribution of each modality to
the overall performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08482</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08482</id><created>2015-03-29</created><authors><author><keyname>Blanas</keyname><forenames>Spyros</forenames></author><author><keyname>Byna</keyname><forenames>Surendra</forenames></author></authors><title>Towards Exascale Scientific Metadata Management</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in technology and computing hardware are enabling scientists from
all areas of science to produce massive amounts of data using large-scale
simulations or observational facilities. In this era of data deluge, effective
coordination between the data production and the analysis phases hinges on the
availability of metadata that describe the scientific datasets. Existing
workflow engines have been capturing a limited form of metadata to provide
provenance information about the identity and lineage of the data. However,
much of the data produced by simulations, experiments, and analyses still need
to be annotated manually in an ad hoc manner by domain scientists. Systematic
and transparent acquisition of rich metadata becomes a crucial prerequisite to
sustain and accelerate the pace of scientific innovation. Yet, ubiquitous and
domain-agnostic metadata management infrastructure that can meet the demands of
extreme-scale science is notable by its absence.
  To address this gap in scientific data management research and practice, we
present our vision for an integrated approach that (1) automatically captures
and manipulates information-rich metadata while the data is being produced or
analyzed and (2) stores metadata within each dataset to permeate
metadata-oblivious processes and to query metadata through established and
standardized data access interfaces. We motivate the need for the proposed
integrated approach using applications from plasma physics, climate modeling
and neuroscience, and then discuss research challenges and possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08485</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08485</id><created>2015-03-29</created><authors><author><keyname>Nguyen</keyname><forenames>PhuongBang</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar</forenames></author></authors><title>Fair Scheduling Policies Exploiting Multiuser Diversity in Cellular
  Systems with Device-to-Device Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the resource allocation problem in cellular networks which
support Device-to-Device Communications (D2D). For systems that enable D2D via
only orthogonal resource sharing, we propose and analyze two resource
allocation policies that guarantee access fairness among all users, while
taking advantage of multi-user diversity and local D2D communications, to
provide marked improvements over existing cellular-only policies. The first
policy, the Cellular Fairness Scheduling (CFS) Policy, provides the simplest
D2D extension to existing cellular systems, while the second policy, the D2D
Fairness Scheduling (DFS) Policy, harnesses maximal performance from
D2D-enabled systems under the orthogonal sharing setting. For even higher
spectral efficiency, cellular systems with D2D can schedule the same frequency
resource for more than one D2D pairs. Under this non-orthogonal sharing
environment, we propose a novel group scheduling policy, the Group Fairness
Scheduling (GFS) Policy, that exploits both spatial frequency reuse and
multiuser diversity in order to deliver dramatic improvements to system
performance with perfect fairness among the users, regardless of whether they
are cellular or D2D users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08490</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08490</id><created>2015-03-29</created><authors><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>D&#xe1;n</keyname><forenames>Gy&#xf6;rgy</forenames></author><author><keyname>Thobaben</keyname><forenames>Ragnar</forenames></author></authors><title>Differentially Private State Estimation in Distribution Networks with
  Smart Meters</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State estimation is routinely being performed in high-voltage power
transmission grids in order to assist in operation and to detect faulty
equipment. In low- and medium-voltage power distribution grids, on the other
hand, few real-time measurements are traditionally available, and operation is
often conducted based on predicted and historical data. Today, in many parts of
the world, smart meters have been deployed at many customers, and their
measurements could in principle be shared with the operators in real time to
enable improved state estimation. However, customers may feel reluctance in
doing so due to privacy concerns. We therefore propose state estimation schemes
for a distribution grid model, which ensure differential privacy to the
customers. In particular, the state estimation schemes optimize different
performance criteria, and a trade-off between a lower bound on the estimation
performance versus the customers' differential privacy is derived. The proposed
framework is general enough to be applicable also to other distribution
networks, such as water and gas networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08498</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08498</id><created>2015-03-29</created><authors><author><keyname>Iliopoulos</keyname><forenames>Vasileios</forenames></author><author><keyname>Penman</keyname><forenames>David B.</forenames></author></authors><title>Dual pivot Quicksort</title><categories>cs.DS</categories><comments>Post print of the article &quot;Dual pivot Quicksort&quot; published on 1
  August of 2012 in the journal of Discrete Mathematics, Algorithms and
  Applications</comments><msc-class>68W40, 60F05</msc-class><journal-ref>Discrete Math. Algorithm. Appl. 04, 1250041 (2012) [13 pages]</journal-ref><doi>10.1142/S1793830912500413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyse the dual pivot Quicksort, a variant of the standard
Quicksort algorithm, in which two pivots are used for the partitioning of the
array. We are solving recurrences of the expected number of key comparisons and
exchanges performed by the algorithm, obtaining the exact and asymptotic total
average values contributing to its time complexity. Further, we compute the
average number of partitioning stages and the variance of the number of key
comparisons. In terms of mean values, dual pivot Quicksort does not appear to
be faster than ordinary algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08504</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08504</id><created>2015-03-29</created><authors><author><keyname>Assi</keyname><forenames>Rawad Abou</forenames></author></authors><title>Investigating the Impact of Metric Aggregation Techniques on Defect
  Prediction</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code metrics collected at the method level are often aggregated using
summation to capture system properties at higher levels (e.g., file- or
package-level). Since defect data is often available at these higher levels,
this aggregation allows researchers to build defect prediction models. Recent
findings by Landman et al. indicate that aggregation is likely to inflate the
correlation between size and complexity metrics. In this paper, we explore the
effect of nine aggregation techniques on the correlation between three types of
code metrics, namely Lines of Code, McCabe, and Halstead metrics. In addition
to summation, we study aggregation techniques that are measures of: (1) central
tendency (average and median), (2) dispersion (standard deviation and
inter-quartile range), (3) shape (skewness and kurtosis), and (4) income
inequality (Theil index and Gini coefficient). Our results show that defect
prediction models built using summation outperform those built using other
aggregation techniques. We also find that more complex aggregations are no
different than much simpler ones and that incorporating all aggregation types
in the same model does not provide a significant improvement over using
summation alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08509</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08509</id><created>2015-03-29</created><authors><author><keyname>Beams</keyname><forenames>Natalie N.</forenames></author><author><keyname>Olson</keyname><forenames>Luke N.</forenames></author><author><keyname>Freund</keyname><forenames>Jonathan B.</forenames></author></authors><title>A Finite Element Based P3M Method for N-body Problems</title><categories>cs.NA math.NA</categories><comments>20 pages, submitted to SISC</comments><msc-class>7008, 70F10, 65N30, 65N99</msc-class><acm-class>G.1.0; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a fast mesh-based method for computing N-body interactions that
is both scalable and accurate. The method is founded on a
particle-particle--particle-mesh P3M approach, which decomposes a potential
into rapidly decaying short-range interactions and smooth, mesh-resolvable
long-range interactions. However, in contrast to the traditional approach of
using Gaussian screen functions to accomplish this decomposition, our method
employs specially designed polynomial bases to construct the screened
potentials. Because of this form of the screen, the long-range component of the
potential is then solved exactly with a finite element method, leading
ultimately to a sparse matrix problem that is solved efficiently with standard
multigrid methods. Moreover, since this system represents an exact
discretization, the optimal resolution properties of the FFT are unnecessary,
though the short-range calculation is now more involved than P3M/PME methods.
We introduce the method, analyze its key properties, and demonstrate the
accuracy of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08513</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08513</id><created>2015-03-29</created><authors><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Varia</keyname><forenames>Mayank</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Zeger</keyname><forenames>Linda M.</forenames></author></authors><title>Hiding Symbols and Functions: New Metrics and Constructions for
  Information-Theoretic Security</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present information-theoretic definitions and results for analyzing
symmetric-key encryption schemes beyond the perfect secrecy regime, i.e. when
perfect secrecy is not attained. We adopt two lines of analysis, one based on
lossless source coding, and another akin to rate-distortion theory. We start by
presenting a new information-theoretic metric for security, called symbol
secrecy, and derive associated fundamental bounds. We then introduce
list-source codes (LSCs), which are a general framework for mapping a key
length (entropy) to a list size that an eavesdropper has to resolve in order to
recover a secret message. We provide explicit constructions of LSCs, and
demonstrate that, when the source is uniformly distributed, the highest level
of symbol secrecy for a fixed key length can be achieved through a construction
based on minimum-distance separable (MDS) codes. Using an analysis related to
rate-distortion theory, we then show how symbol secrecy can be used to
determine the probability that an eavesdropper correctly reconstructs functions
of the original plaintext. We illustrate how these bounds can be applied to
characterize security properties of symmetric-key encryption schemes, and, in
particular, extend security claims based on symbol secrecy to a functional
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08518</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08518</id><created>2015-03-29</created><updated>2015-10-08</updated><authors><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>The Dual Diameter of Triangulations</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Poly$ be a simple polygon with $n$ vertices. The \emph{dual graph}
$\triang^*$ of a triangulation~$\triang$ of~$\Poly$ is the graph whose vertices
correspond to the bounded faces of $\triang$ and whose edges connect those
faces of~$\triang$ that share an edge. We consider triangulations of~$\Poly$
that minimize or maximize the diameter of their dual graph. We show that both
triangulations can be constructed in $O(n^3\log n)$ time using dynamic
programming. If $\Poly$ is convex, we show that any minimizing triangulation
has dual diameter exactly $2\cdot\lceil\log_2(n/3)\rceil$ or
$2\cdot\lceil\log_2(n/3)\rceil -1$, depending on~$n$. Trivially, in this case
any maximizing triangulation has dual diameter $n-2$. Furthermore, we
investigate the relationship between the dual diameter and the number of
\emph{ears} (triangles with exactly two edges incident to the boundary of
$\Poly$) in a triangulation. For convex $\Poly$, we show that there is always a
triangulation that simultaneously minimizes the dual diameter and maximizes the
number of ears. In contrast, we give examples of general simple polygons where
every triangulation that maximizes the number of ears has dual diameter that is
quadratic in the minimum possible value. We also consider the case of point
sets in general position in the plane. We show that for any such set of $n$
points there are triangulations with dual diameter in~$O(\log n)$ and
in~$\Omega(\sqrt n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08528</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08528</id><created>2015-03-29</created><updated>2015-06-26</updated><authors><author><keyname>Chechik</keyname><forenames>Shiri</forenames></author><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author></authors><title>Average Distance Queries through Weighted Samples in Graphs and Metric
  Spaces: High Scalability with Tight Statistical Guarantees</title><categories>cs.SI cs.LG</categories><comments>21 pages, will appear in the Proceedings of RANDOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The average distance from a node to all other nodes in a graph, or from a
query point in a metric space to a set of points, is a fundamental quantity in
data analysis. The inverse of the average distance, known as the (classic)
closeness centrality of a node, is a popular importance measure in the study of
social networks. We develop novel structural insights on the sparsifiability of
the distance relation via weighted sampling. Based on that, we present highly
practical algorithms with strong statistical guarantees for fundamental
problems. We show that the average distance (and hence the centrality) for all
nodes in a graph can be estimated using $O(\epsilon^{-2})$ single-source
distance computations. For a set $V$ of $n$ points in a metric space, we show
that after preprocessing which uses $O(n)$ distance computations we can compute
a weighted sample $S\subset V$ of size $O(\epsilon^{-2})$ such that the average
distance from any query point $v$ to $V$ can be estimated from the distances
from $v$ to $S$. Finally, we show that for a set of points $V$ in a metric
space, we can estimate the average pairwise distance using $O(n+\epsilon^{-2})$
distance computations. The estimate is based on a weighted sample of
$O(\epsilon^{-2})$ pairs of points, which is computed using $O(n)$ distance
computations. Our estimates are unbiased with normalized mean square error
(NRMSE) of at most $\epsilon$. Increasing the sample size by a $O(\log n)$
factor ensures that the probability that the relative error exceeds $\epsilon$
is polynomially small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08535</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08535</id><created>2015-03-30</created><authors><author><keyname>Xuan</keyname><forenames>Junyu</forenames></author><author><keyname>Lu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Guangquan</forenames></author><author><keyname>Da Xu</keyname><forenames>Richard Yi</forenames></author><author><keyname>Luo</keyname><forenames>Xiangfeng</forenames></author></authors><title>Infinite Author Topic Model based on Mixed Gamma-Negative Binomial
  Process</title><categories>stat.ML cs.IR cs.LG</categories><comments>10 pages, 5 figures, submitted to KDD conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating the side information of text corpus, i.e., authors, time
stamps, and emotional tags, into the traditional text mining models has gained
significant interests in the area of information retrieval, statistical natural
language processing, and machine learning. One branch of these works is the
so-called Author Topic Model (ATM), which incorporates the authors's interests
as side information into the classical topic model. However, the existing ATM
needs to predefine the number of topics, which is difficult and inappropriate
in many real-world settings. In this paper, we propose an Infinite Author Topic
(IAT) model to resolve this issue. Instead of assigning a discrete probability
on fixed number of topics, we use a stochastic process to determine the number
of topics from the data itself. To be specific, we extend a gamma-negative
binomial process to three levels in order to capture the
author-document-keyword hierarchical structure. Furthermore, each document is
assigned a mixed gamma process that accounts for the multi-author's
contribution towards this document. An efficient Gibbs sampling inference
algorithm with each conditional distribution being closed-form is developed for
the IAT model. Experiments on several real-world datasets show the capabilities
of our IAT model to learn the hidden topics, authors' interests on these topics
and the number of topics simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08542</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08542</id><created>2015-03-30</created><authors><author><keyname>Xuan</keyname><forenames>Junyu</forenames></author><author><keyname>Lu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Guangquan</forenames></author><author><keyname>Da Xu</keyname><forenames>Richard Yi</forenames></author><author><keyname>Luo</keyname><forenames>Xiangfeng</forenames></author></authors><title>Nonparametric Relational Topic Models through Dependent Gamma Processes</title><categories>stat.ML cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Relational Topic Models provide a way to discover the hidden
topics from a document network. Many theoretical and practical tasks, such as
dimensional reduction, document clustering, link prediction, benefit from this
revealed knowledge. However, existing relational topic models are based on an
assumption that the number of hidden topics is known in advance, and this is
impractical in many real-world applications. Therefore, in order to relax this
assumption, we propose a nonparametric relational topic model in this paper.
Instead of using fixed-dimensional probability distributions in its generative
model, we use stochastic processes. Specifically, a gamma process is assigned
to each document, which represents the topic interest of this document.
Although this method provides an elegant solution, it brings additional
challenges when mathematically modeling the inherent network structure of
typical document network, i.e., two spatially closer documents tend to have
more similar topics. Furthermore, we require that the topics are shared by all
the documents. In order to resolve these challenges, we use a subsampling
strategy to assign each document a different gamma process from the global
gamma process, and the subsampling probabilities of documents are assigned with
a Markov Random Field constraint that inherits the document network structure.
Through the designed posterior inference algorithm, we can discover the hidden
topics and its number simultaneously. Experimental results on both synthetic
and real-world network datasets demonstrate the capabilities of learning the
hidden topics and, more importantly, the number of topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08548</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08548</id><created>2015-03-30</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Piunovskiy</keyname><forenames>Alexei</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author></authors><title>Hitting with Restart: A Reason for Sisyphus Labour</title><categories>cs.PF cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in telecommunications, computer science and
physics, we consider a discrete-time Markov process with restart in the Borel
state space. At each step the process either with a positive probability
restarts from a given distribution, or with the complementary probability
continues according to a Markov transition kernel. The main focus of the
present work is the expectation of the hitting time (to a given target set) of
the process with restart, for which we obtain the explicit formula. Observing
that the process with restart is positive Harris recurrent, we obtain the
expression of its unique invariant probability. Then we show the equivalence of
the following statements: (a) the boundedness (with respect to the initial
state) of the expectation of the hitting time; (b) the finiteness of the
expectation of the hitting time for almost all the initial states with respect
to the unique invariant probability; and (c) the target set is of positive
measure with respect to the invariant probability. We illustrate our results
with two examples in uncountable and countable state spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08550</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08550</id><created>2015-03-30</created><authors><author><keyname>Grabisch</keyname><forenames>Michel</forenames><affiliation>EEP-PSE, CES</affiliation></author><author><keyname>Miranda</keyname><forenames>Pedro</forenames></author></authors><title>Exact bounds of the M{\&quot;o}bius inverse of monotone set functions</title><categories>cs.DM</categories><proxy>ccsd</proxy><journal-ref>Discrete Applied Mathematics, Elsevier, 2015, pp.7-12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the exact upper and lower bounds of the M{\&quot;o}bius inverse of
monotone and normalized set functions (a.k.a. normalized capacities) on a
finite set of n elements. We find that the absolute value of the bounds tend to
4 n/2 $\sqrt$ $\pi$n/2 when n is large. We establish also the exact bounds of
the interaction transform and Banzhaf interaction transform, as well as the
exact bounds of the M{\&quot;o}bius inverse for the subfamilies of k-additive
normalized capacities and p-symmetric normalized capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08551</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08551</id><created>2015-03-30</created><authors><author><keyname>Cerna</keyname><forenames>David</forenames></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames></author></authors><title>Analysis of Clause set Schema Aided by Automated Theorem Proving: A Case
  Study [Extended Paper]</title><categories>cs.LO</categories><comments>Submitted to Cade 2015. if published, will be published without
  appendix. Full paper, as provided here, is 23 pages. Published will be 15
  pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The schematic CERES method [8] is a recently developed method of cut
elimination for proof schemata, that is a sequence of proofs with a recursive
construction. Proof schemata can be thought of as a way to circumvent adding an
induction rule to the LK-calculus. In this work, we formalize a schematic
version of the infinitary pigeonhole principle, which we call the
Non-injectivity Assertion schema (NiA-schema), in the LKS-calculus [8], and
analyse the clause set schema extracted from the NiA-schema using some of the
structure provided by the schematic CERES method. To the best of our knowledge,
this is the first appli- cation of the constructs built for proof analysis of
proof schemata to a mathematical argument since its publication. We discuss the
role of Automated Theorem Proving (ATP) in schematic proof analysis, as well as
the shortcomings of the schematic CERES method concerning the formalization of
the NiA-schema, namely, the expressive power of the schematic resolution
calculus. We conclude with a discussion concerning the usage of ATP in
schematic proof analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08558</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08558</id><created>2015-03-30</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Borkar</keyname><forenames>Vivek</forenames><affiliation>EE-IIT</affiliation></author></authors><title>Whittle Index Policy for Crawling Ephemeral Content</title><categories>cs.IR cs.SY math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a task of scheduling a crawler to retrieve content from several
sites with ephemeral content. A user typically loses interest in ephemeral
content, like news or posts at social network groups, after several days or
hours. Thus, development of timely crawling policy for such ephemeral
information sources is very important. We first formulate this problem as an
optimal control problem with average reward. The reward can be measured in the
number of clicks or relevant search requests. The problem in its initial
formulation suffers from the curse of dimensionality and quickly becomes
intractable even with moderate number of information sources. Fortunately, this
problem admits a Whittle index, which leads to problem decomposition and to a
very simple and efficient crawling policy. We derive the Whittle index and
provide its theoretical justification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08570</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08570</id><created>2015-03-30</created><authors><author><keyname>Ning</keyname><forenames>Li</forenames></author><author><keyname>Yu</keyname><forenames>Dongxiao</forenames></author><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Yuexuan</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author><author><keyname>Feng</keyname><forenames>Shenzhong</forenames></author></authors><title>Uniform Information Exchange in Multi-channel Wireless Ad Hoc Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the information exchange problem, k packets that are initially maintained
by k nodes need to be disseminated to the whole network as quickly as possible.
We consider this problem in single-hop multi- channel networks of n nodes, and
propose a uniform protocol that with high probability accomplishes the
dissemination in O(k/F + F \cdot log n) rounds, assuming F available channels
and collision detection. This result is asymptotically optimal when k is large
(k \geq F^2 \cdot log n). To our knowledge, this is the ?1st uniform protocol
for information exchange in multi-channel networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08572</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08572</id><created>2015-03-30</created><updated>2015-05-27</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Mottet</keyname><forenames>Antoine</forenames></author></authors><title>Constraint Satisfaction Problems over the Integers with Successor</title><categories>math.LO cs.CC</categories><comments>47 pages</comments><msc-class>03C99</msc-class><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A distance constraint satisfaction problem is a constraint satisfaction
problem (CSP) whose constraint language consists of relations that are
first-order definable over $(\Bbb Z,succ)$, i.e., over the integers with the
successor function. Our main result says that every distance CSP is in Ptime or
NP-complete, unless it can be formulated as a finite domain CSP in which case
the computational complexity is not known in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08577</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08577</id><created>2015-03-30</created><authors><author><keyname>Duval</keyname><forenames>Vincent</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Sparse Spikes Deconvolution on Thin Grids</title><categories>cs.IT math.IT math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article analyzes the recovery performance of two popular finite
dimensional approximations of the sparse spikes deconvolution problem over
Radon measures. We examine in a unified framework both the L1 regularization
(often referred to as Lasso or Basis-Pursuit) and the Continuous Basis-Pursuit
(C-BP) methods. The Lasso is the de-facto standard for the sparse
regularization of inverse problems in imaging. It performs a nearest neighbor
interpolation of the spikes locations on the sampling grid. The C-BP method,
introduced by Ekanadham, Tranchina and Simoncelli, uses a linear interpolation
of the locations to perform a better approximation of the infinite-dimensional
optimization problem, for positive measures. We show that, in the small noise
regime, both methods estimate twice the number of spikes as the number of
original spikes. Indeed, we show that they both detect two neighboring spikes
around the locations of an original spikes. These results for deconvolution
problems are based on an abstract analysis of the so-called extended support of
the solutions of L1-type problems (including as special cases the Lasso and
C-BP for deconvolution), which are of an independent interest. They precisely
characterize the support of the solutions when the noise is small and the
regularization parameter is selected accordingly. We illustrate these findings
to analyze for the first time the support instability of compressed sensing
recovery when the number of measurements is below the critical limit (well
documented in the literature) where the support is provably stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08581</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08581</id><created>2015-03-30</created><authors><author><keyname>Partalas</keyname><forenames>Ioannis</forenames></author><author><keyname>Kosmopoulos</keyname><forenames>Aris</forenames></author><author><keyname>Baskiotis</keyname><forenames>Nicolas</forenames></author><author><keyname>Artieres</keyname><forenames>Thierry</forenames></author><author><keyname>Paliouras</keyname><forenames>George</forenames></author><author><keyname>Gaussier</keyname><forenames>Eric</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Amini</keyname><forenames>Massih-Reza</forenames></author><author><keyname>Galinari</keyname><forenames>Patrick</forenames></author></authors><title>LSHTC: A Benchmark for Large-Scale Text Classification</title><categories>cs.IR cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LSHTC is a series of challenges which aims to assess the performance of
classification systems in large-scale classification in a a large number of
classes (up to hundreds of thousands). This paper describes the dataset that
have been released along the LSHTC series. The paper details the construction
of the datsets and the design of the tracks as well as the evaluation measures
that we implemented and a quick overview of the results. All of these datasets
are available online and runs may still be submitted on the online server of
the challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08585</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08585</id><created>2015-03-30</created><updated>2015-08-09</updated><authors><author><keyname>Rost</keyname><forenames>Peter</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>The Complexity-Rate Tradeoff of Centralized Radio Access Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>13 pages, 8 figures, 25 references, accepted to IEEE Transactions on
  Wireless Communications</comments><msc-class>94A05</msc-class><doi>10.1109/TWC.2015.2449321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a centralized RAN, the signals from multiple RAPs are processed centrally
in a data center. Centralized RAN enables advanced interference coordination
strategies while leveraging the elastic provisioning of data processing
resources. It is particularly well suited for dense deployments, such as within
a large building where the RAPs are connected via fibre and many cells are
underutilized. This paper considers the computational requirements of
centralized RAN with the goal of illuminating the benefits of pooling
computational resources. A new analytical framework is proposed for quantifying
the computational load associated with the centralized processing of uplink
signals in the presence of block Rayleigh fading, distance-dependent path-loss,
and fractional power control. Several new performance metrics are defined,
including computational outage probability, outage complexity, computational
gain, computational diversity, and the complexity-rate tradeoff. The validity
of the analytical framework is confirmed by comparing it numerically with a
simulator compliant with the 3GPP LTE standard. Using the developed metrics, it
is shown that centralizing the computing resources provides a higher net
throughput per computational resource as compared to local processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08596</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08596</id><created>2015-03-30</created><updated>2015-04-09</updated><authors><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames></author><author><keyname>Cuturi</keyname><forenames>Marco</forenames></author></authors><title>Fast Optimal Transport Averaging of Neuroimaging Data</title><categories>cs.CV</categories><comments>Information Processing in Medical Imaging (IPMI), Jun 2015, Isle of
  Skye, United Kingdom. Springer, 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing how the Human brain is anatomically and functionally organized at the
level of a group of healthy individuals or patients is the primary goal of
neuroimaging research. Yet computing an average of brain imaging data defined
over a voxel grid or a triangulation remains a challenge. Data are large, the
geometry of the brain is complex and the between subjects variability leads to
spatially or temporally non-overlapping effects of interest. To address the
problem of variability, data are commonly smoothed before group linear
averaging. In this work we build on ideas originally introduced by Kantorovich
to propose a new algorithm that can average efficiently non-normalized data
defined over arbitrary discrete domains using transportation metrics. We show
how Kantorovich means can be linked to Wasserstein barycenters in order to take
advantage of an entropic smoothing approach. It leads to a smooth convex
optimization problem and an algorithm with strong convergence guarantees. We
illustrate the versatility of this tool and its empirical behavior on
functional neuroimaging data, functional MRI and magnetoencephalography (MEG)
source estimates, defined on voxel grids and triangulations of the folded
cortical surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08601</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08601</id><created>2015-03-30</created><authors><author><keyname>Nakatsukasa</keyname><forenames>Yuji</forenames></author><author><keyname>Soma</keyname><forenames>Tasuku</forenames></author><author><keyname>Uschmajew</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Finding a low-rank basis in a matrix subspace</title><categories>cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given matrix subspace, how can we find a basis that consists of
low-rank matrices? This is a generalization of the sparse vector problem. It
turns out that when the subspace is spanned by rank-1 matrices, the matrices
can be obtained by the tensor CP decomposition. For the higher rank case, the
situation is not as straightforward. In this work we present an algorithm based
on a greedy process applicable to higher rank problems. Our algorithm first
estimates the minimum rank by applying soft singular value thresholding to a
nuclear norm relaxation, and then computes a matrix with that rank using the
method of alternating projections. We provide local convergence results, and
compare our algorithm with several alternative approaches. Applications include
data compression beyond the classical truncated SVD, computing accurate
eigenvectors of a near-multiple eigenvalue, image separation and graph
Laplacian eigenproblems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08602</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08602</id><created>2015-03-30</created><authors><author><keyname>Ochsenschl&#xe4;ger</keyname><forenames>Peter</forenames></author><author><keyname>Rieke</keyname><forenames>Roland</forenames></author></authors><title>Pairs of Languages Closed under Shuffle Projection</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shuffle projection is motivated by the verification of safety properties of
special parameterized systems. Basic definitions and properties, especially
related to alphabetic homomorphisms, are presented. The relation between
iterated shuffle products and shuffle projections is shown. A special class of
multi-counter automata is introduced, to formulate shuffle projection in terms
of computations of these automata represented by transductions. This
reformulation of shuffle projection leads to construction principles for pairs
of languages closed under shuffle projection. Additionally, it is shown that
under certain conditions these transductions are rational, which implies
decidability of closure against shuffle projection. Decidability of these
conditions is proven for regular languages. Finally, without additional
conditions, decidability of the question, whether a pair of regular languages
is closed under shuffle projection, is shown. In an appendix the relation
between shuffle projection and the shuffle product of two languages is
discussed. Additionally, a kind of shuffle product for computations in
S-automata is defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08604</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08604</id><created>2015-03-30</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Monti</keyname><forenames>Corrado</forenames></author><author><keyname>Santini</keyname><forenames>Massimo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Liquid FM: Recommending Music through Viscous Democracy</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most modern recommendation systems use the approach of collaborative
filtering: users that are believed to behave alike are used to produce
recommendations. In this work we describe an application (Liquid FM) taking a
completely different approach. Liquid FM is a music recommendation system that
makes the user responsible for the recommended items. Suggestions are the
result of a voting scheme, employing the idea of viscous democracy. Liquid FM
can also be thought of as the first testbed for this voting system. In this
paper we outline the design and architecture of the application, both from the
theoretical and from the implementation viewpoints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08623</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08623</id><created>2015-03-30</created><updated>2015-12-09</updated><authors><author><keyname>Barrett</keyname><forenames>Edd</forenames></author><author><keyname>Bolz</keyname><forenames>Carl Friedrich</forenames></author><author><keyname>Diekmann</keyname><forenames>Lukas</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author></authors><title>Fine-grained Language Composition</title><categories>cs.PL</categories><comments>27 pages, 4 tables, 5 figures</comments><acm-class>D.3.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Although run-time language composition is common, it normally takes the form
of a crude Foreign Function Interface (FFI). While useful, such compositions
tend to be coarse-grained and slow. In this paper we introduce a novel
fine-grained syntactic composition of PHP and Python which allows users to
embed each language inside the other, including referencing variables across
languages. This composition raises novel design and implementation challenges.
We show that good solutions can be found to the design challenges; and that the
resulting implementation imposes an acceptable performance overhead of, at
most, just over 2x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08627</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08627</id><created>2015-03-30</created><authors><author><keyname>Pollakis</keyname><forenames>Emmanuel</forenames></author><author><keyname>Cavalcante</keyname><forenames>Renato L. G.</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Traffic Demand-Aware Topology Control for Enhanced Energy-Efficiency of
  Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications - Green
  Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The service provided by mobile networks operated today is not adapted to
spatio-temporal fluctuations in traffic demand, although such fluctuations
offer opportunities for energy savings. In particular, significant gains in
energy efficiency are realizable by disengaging temporarily redundant hardware
components of base stations. We therefore propose a novel optimization
framework that considers both the load-dependent energy radiated by the
antennas and the remaining forms of energy needed for operating the base
stations. The objective is to reduce the energy consumption of mobile networks,
while ensuring that the data rate requirements of the users are met throughout
the coverage area. Building upon sparse optimization techniques, we develop a
majorization-minimization algorithm with the ability to identify
energy-efficient network configurations. The iterative algorithm is load-aware,
has low computational complexity, and can be implemented in an online fashion
to exploit load fluctuations on a short time scale. Simulations show that the
algorithm can find network configurations with the energy consumption similar
to that obtained with global optimization tools, which cannot be applied to
real large networks. Although we consider only one currently deployed cellular
technology, the optimization framework is general, potentially applicable to a
large class of access technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08636</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08636</id><created>2015-03-30</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author><author><keyname>Ghosh</keyname><forenames>Santanu</forenames></author><author><keyname>Das</keyname><forenames>Mridul</forenames></author><author><keyname>Ray</keyname><forenames>Bhaswati</forenames></author></authors><title>Design &amp; Implementation Approach for Error Free Clinical Data Repository
  for the Medical Practitioners</title><categories>cs.DB</categories><comments>04 pages, 04 Figures, International Journal of Computer Trends and
  Technology, Volume-21 Number-2,2015, ISSN 2231-2803</comments><acm-class>H.4.0</acm-class><doi>10.14445/22312803/IJCTT-V21P113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern treatment of any disease is heavily dependent on the medical
diagnosis. Clinical data obtained through the diagnostics tests need to be
collected and entered into the computer database in order to make a clinical
data repository. In most of the cases, manual entry is an absolute necessity.
However, manual entry can cause errors also, leading to wrong diagnosis. This
paper explains how data could be entered free of error to reduce the chances of
wrong diagnosis by designing and implementation of a simple database driven
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08639</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08639</id><created>2015-03-30</created><authors><author><keyname>Li&#xe9;geois</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Mishra</keyname><forenames>Bamdev</forenames></author><author><keyname>Zorzi</keyname><forenames>Mattia</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>Sparse plus low-rank autoregressive identification in neuroimaging time
  series</title><categories>cs.LG cs.SY</categories><comments>6 pages paper submitted to CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of identifying multivariate autoregressive
(AR) sparse plus low-rank graphical models. Based on the corresponding problem
formulation recently presented, we use the alternating direction method of
multipliers (ADMM) to efficiently solve it and scale it to sizes encountered in
neuroimaging applications. We apply this decomposition on synthetic and real
neuroimaging datasets with a specific focus on the information encoded in the
low-rank structure of our model. In particular, we illustrate that this
information captures the spatio-temporal structure of the original data,
generalizing classical component analysis approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08642</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08642</id><created>2015-03-30</created><updated>2015-05-21</updated><authors><author><keyname>Sanjari</keyname><forenames>S.</forenames></author><author><keyname>Ozgoli</keyname><forenames>S.</forenames></author></authors><title>Generalized Integral Siding Mode Manifold Design: A Sum of Squares
  Approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general form of integral sliding mode manifold, and
proposes an algorithmic approach based on Sum of Squares (SOS) programming to
design generalized integral sliding mode manifold and controller for nonlinear
systems with both matched and unmatched uncertainties. The approach also gives
a sufficient condition for successful design of controller and manifold
parameters. The result of the paper is then verified by several simulation
examples and two practical applications, namely Glucose-insulin regulation
problem and the unicycle dynamics steering problem are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08643</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08643</id><created>2015-03-30</created><updated>2015-09-30</updated><authors><author><keyname>Ben-Aryeh</keyname><forenames>Y.</forenames></author></authors><title>Strong and weak separability conditions for two-qubits density matrices</title><categories>quant-ph cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explicit separable density matrices, for mixed two qubits states, are derived
by the use of Hilbert Schmidt decompositions and Peres Horodecki criterion. A
strongly separable two qubits mixed state is defined by multiplications of two
density matrices given with pure states while weakly separable two qubits state
is defined by multiplications of two density matrices which includes non-pure
states. We find the sufficient and necessary condition for separability of
two-qubits density matrices and show that under this condition the two-qubit
density matrices are strongly separable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08644</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08644</id><created>2015-03-30</created><updated>2015-09-21</updated><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>S&#xf6;der</keyname><forenames>Johan</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>On the Capacity of the Wiener Phase-Noise Channel: Bounds and Capacity
  Achieving Distributions</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, 2015</comments><doi>10.1109/TCOMM.2015.2465389</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the capacity of the additive white Gaussian noise (AWGN)
channel, affected by time-varying Wiener phase noise is investigated. Tight
upper and lower bounds on the capacity of this channel are developed. The upper
bound is obtained by using the duality approach, and considering a specific
distribution over the output of the channel. In order to lower-bound the
capacity, first a family of capacity-achieving input distributions is found by
solving a functional optimization of the channel mutual information. Then,
lower bounds on the capacity are obtained by drawing samples from the proposed
distributions through Monte-Carlo simulations. The proposed capacity-achieving
input distributions are circularly symmetric, non-Gaussian, and the input
amplitudes are correlated over time. The evaluated capacity bounds are tight
for a wide range of signal-to-noise-ratio (SNR) values, and thus they can be
used to quantify the capacity. Specifically, the bounds follow the well-known
AWGN capacity curve at low SNR, while at high SNR, they coincide with the
high-SNR capacity result available in the literature for the phase-noise
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08650</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08650</id><created>2015-03-30</created><updated>2016-01-15</updated><authors><author><keyname>Piironen</keyname><forenames>Juho</forenames></author><author><keyname>Vehtari</keyname><forenames>Aki</forenames></author></authors><title>Comparison of Bayesian predictive methods for model selection</title><categories>stat.ME cs.LG</categories><comments>Added one figure, one section (4.5) and material to the appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to compare several widely used Bayesian model
selection methods in practical model selection problems, highlight their
differences and give recommendations about the preferred approaches. We focus
on the variable subset selection for regression and classification and perform
several numerical experiments using both simulated and real world data. The
results show that the optimization of a utility estimate such as the
cross-validation (CV) score is liable to finding overfitted models due to
relatively high variance in the utility estimates when the data is scarce. This
can also lead to substantial selection induced bias and optimism in the
performance evaluation for the selected model. From a predictive viewpoint,
best results are obtained by accounting for model uncertainty by forming the
full encompassing model, preferably the Bayesian model averaging solution over
the candidate models. If the encompassing model is too complex, it can be
robustly simplified by the projection method, in which the information of the
full model is projected onto the submodels. This approach is substantially less
prone to overfitting than selection based on CV-score. Overall, the projection
method appears to outperform also the maximum a posteriori model and the
selection of the most probable variables. The study also demonstrates that the
model selection can greatly benefit from using cross-validation outside the
searching process both for guiding the model size selection and assessing the
predictive performance of the finally selected model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08659</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08659</id><created>2015-03-30</created><updated>2015-05-17</updated><authors><author><keyname>Held</keyname><forenames>Stephan</forenames></author><author><keyname>Spirkl</keyname><forenames>Sophie Theresa</forenames></author></authors><title>Binary Adder Circuits of Asymptotically Minimum Depth, Linear Size, and
  Fan-Out Two</title><categories>cs.AR cs.CC cs.DC</categories><acm-class>B.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing fast and small binary adder circuits.
Among widely-used adders, the Kogge-Stone adder is often considered the
fastest, because it computes the carry bits for two $n$-bit numbers (where $n$
is a power of two) with a depth of $2\log_2 n$ logic gates, size $4 n\log_2 n$,
and all fan-outs bounded by two. Fan-outs of more than two are avoided, because
they lead to the insertion of repeaters for repowering the signal and
additional depth in the physical implementation. However, the depth bound of
the Kogge-Stone adder is off by a factor of two from the lower bound of $\log_2
n$. This bound is achieved asymptotically in two separate constructions by
Brent and Krapchenko. Brent's construction gives neither a bound on the fan-out
nor the size, while Krapchenko's adder has linear size, but can have up to
linear fan-out. In this paper we introduce the first family of adders with an
asymptotically optimum depth of $\log_2 n + o(\log_2 n)$, linear size $\mathcal
{O}(n)$, and a fan-out bound of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08661</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08661</id><created>2015-03-30</created><updated>2015-09-10</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Li-Chun</forenames></author></authors><title>Optimal Cell Load and Throughput in Green Small Cell Networks with
  Generalized Cell Association</title><categories>cs.IT math.IT</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper thoroughly explored the fundamental interactions between cell
association, cell load and throughput in a green (energy-efficient) small cell
network in which all base stations form a homogeneous Poisson point process
(PPP) of intensity $\lambda_B$ and all users form another independent PPP of
intensity $\lambda_U$. Cell voidness, usually disregarded due to rarity in
cellular network modeling, is first theoretically analyzed under generalized
(channel-aware) cell association (GCA). We showed that the void cell
probability cannot be neglected any more since it is bounded above by
$\exp(-\lambda_U/\lambda_B)$ that is typically not small in a small cell
network. The accurate expression of the void cell probability for GCA was
characterized and it was used to derive the average cell and user throughputs.
We learned that cell association and cell load $\lambda_U/\lambda_B$
significantly affect these two throughputs. According to the average cell and
user throughputs, the green cell and user throughputs are defined respectively
to reflect whether the energy of a base station is efficiently used to transmit
information or not. In order to achieve satisfactory throughput with certain
level of greenness, cell load should be properly determined. We presented the
theoretical solutions of the optimal cell loads that maximize the green cell
and user throughputs, respectively, and verified their correctness by
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08663</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08663</id><created>2015-03-30</created><updated>2015-04-10</updated><authors><author><keyname>Li</keyname><forenames>Guanbin</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Visual Saliency Based on Multiscale Deep Features</title><categories>cs.CV</categories><comments>To appear in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual saliency is a fundamental problem in both cognitive and computational
sciences, including computer vision. In this CVPR 2015 paper, we discover that
a high-quality visual saliency model can be trained with multiscale features
extracted using a popular deep learning architecture, convolutional neural
networks (CNNs), which have had many successes in visual recognition tasks. For
learning such saliency models, we introduce a neural network architecture,
which has fully connected layers on top of CNNs responsible for extracting
features at three different scales. We then propose a refinement method to
enhance the spatial coherence of our saliency results. Finally, aggregating
multiple saliency maps computed for different levels of image segmentation can
further boost the performance, yielding saliency maps better than those
generated from a single segmentation. To promote further research and
evaluation of visual saliency models, we also construct a new large database of
4447 challenging images and their pixelwise saliency annotation. Experimental
results demonstrate that our proposed method is capable of achieving
state-of-the-art performance on all public benchmarks, improving the F-Measure
by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset
(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively
on these two datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08665</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08665</id><created>2015-03-30</created><updated>2015-06-04</updated><authors><author><keyname>Schneider</keyname><forenames>Sigurd</forenames></author><author><keyname>Smolka</keyname><forenames>Gert</forenames></author><author><keyname>Hack</keyname><forenames>Sebastian</forenames></author></authors><title>A Linear First-Order Functional Intermediate Language for Verified
  Compilers</title><categories>cs.PL</categories><comments>Addressed comments from reviewers (ITP 2015): (1) Added discussion of
  a paper in related work (2) Added definition of renamed-apart in appendix (3)
  Formulation changes in a coupe of places</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the linear first-order intermediate language IL for verified
compilers. IL is a functional language with calls to a nondeterministic
environment. We give IL terms a second, imperative semantic interpretation and
obtain a register transfer language. For the imperative interpretation we
establish a notion of live variables. Based on live variables, we formulate a
decidable property called coherence ensuring that the functional and the
imperative interpretation of a term coincide. We formulate a register
assignment algorithm for IL and prove its correctness. The algorithm translates
a functional IL program into an equivalent imperative IL program. Correctness
follows from the fact that the algorithm reaches a coherent program after
consistently renaming local variables. We prove that the maximal number of live
variables in the initial program bounds the number of different variables in
the final coherent program. The entire development is formalized in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08677</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08677</id><created>2015-03-30</created><updated>2015-10-01</updated><authors><author><keyname>Akata</keyname><forenames>Zeynep</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Label-Embedding for Image Classification</title><categories>cs.CV</categories><comments>IEEE TPAMI preprint</comments><doi>10.1109/TPAMI.2015.2487986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attributes act as intermediate representations that enable parameter sharing
between classes, a must when training data is scarce. We propose to view
attribute-based image classification as a label-embedding problem: each class
is embedded in the space of attribute vectors. We introduce a function that
measures the compatibility between an image and a label embedding. The
parameters of this function are learned on a training set of labeled samples to
ensure that, given an image, the correct classes rank higher than the incorrect
ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets
show that the proposed framework outperforms the standard Direct Attribute
Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a
built-in ability to leverage alternative sources of information instead of or
in addition to attributes, such as e.g. class hierarchies or textual
descriptions. Moreover, label embedding encompasses the whole range of learning
settings from zero-shot learning to regular learning with a large number of
labeled examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08682</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08682</id><created>2015-03-30</created><authors><author><keyname>Jaziri</keyname><forenames>Aymen</forenames></author><author><keyname>Nasri</keyname><forenames>Ridha</forenames></author><author><keyname>Chahed</keyname><forenames>Tijani</forenames></author></authors><title>A New Alternative for Traffic Hotspot Localization in Wireless Networks
  Using O&amp;M Metrics</title><categories>cs.NI</categories><comments>Submitted to EURASIP Journal on Wireless Communications and
  Networking (EURASIP JWCN)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been an increasing awareness to traffic
localization techniques driven by the problematic of hotspot offloading
solutions, the emergence of heterogeneous networks (HetNet) with small cells'
deployment and the green networks. The localization of traffic hotspots with a
high accuracy is indeed of great interest to know how the congested zones can
be offloaded, where small cells should be deployed and how they can be managed
for sleep mode concept. We propose, in this paper, a new hotspot localization
technique based on the direct exploitation of five Key Performance Indicators
(KPIs) extracted from the Operation and Maintenance (O&amp;M) database of the
network. These KPIs are the Timing Advance (TA), the angle of arrival (AoA),
the neighboring cell level, the load time and two mean throughputs: arithmetic
(AMT) and harmonic (HMT). The combined use of these KPIs, projected over a
coverage map, yields a promising localization precision and can be further
optimized by exploiting commercial data on potential hotspots. This solution
can be implemented in the network at an appreciable low cost when compared with
widely used probing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08687</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08687</id><created>2015-03-30</created><updated>2015-06-28</updated><authors><author><keyname>Kala</keyname><forenames>Srikant Manas</forenames></author><author><keyname>Reddy</keyname><forenames>M. Pavan Kumar</forenames></author><author><keyname>Tamma</keyname><forenames>Bheemarjuna Reddy</forenames></author></authors><title>Predicting Performance of Channel Assignments in Wireless Mesh Networks
  through Statistical Interference Estimation</title><categories>cs.NI</categories><journal-ref>CONECCT 2015, July 2015, 1 - 6</journal-ref><doi>10.1109/CONECCT.2015.7383864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Mesh Network (WMN) deployments are poised to reduce the reliance on
wired infrastructure especially with the advent of the multi-radio
multi-channel (MRMC) WMN architecture. But the benefits that MRMC WMNs offer
viz., augmented network capacity, uninterrupted connectivity and reduced
latency, are depreciated by the detrimental effect of prevalent interference.
Interference mitigation is thus a prime objective in WMN deployments. It is
often accomplished through prudent channel allocation (CA) schemes which
minimize the adverse impact of interference and enhance the network
performance. However, a multitude of CA schemes have been proposed in research
literature and absence of a CA performance prediction metric, which could aid
in the selection of an efficient CA scheme for a given WMN, is often felt. In
this work, we offer a fresh characterization of the interference endemic in
wireless networks. We then propose a reliable CA performance prediction metric,
which employs a statistical interference estimation approach. We carry out a
rigorous quantitative assessment of the proposed metric by validating its CA
performance predictions with experimental results, recorded from extensive
simulations run on an ns-3 802.11g environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08691</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08691</id><created>2015-03-30</created><authors><author><keyname>Neumann</keyname><forenames>David</forenames></author><author><keyname>Joham</keyname><forenames>Michael</forenames></author><author><keyname>Utschick</keyname><forenames>Wolfgang</forenames></author></authors><title>Channel Estimation in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce novel blind and semi-blind channel estimation methods for
cellular time-division duplexing systems with a large number of antennas at
each base station. The methods are based on the maximum a-posteriori principle
given a prior for the distribution of the channel vectors and the received
signals from the uplink training and data phases. Contrary to the
state-of-the-art massive MIMO channel estimators which either perform linear
estimation based on the pilot symbols or rely on a blind principle, the
proposed semi-blind method efficiently suppresses most of the interference
caused by pilot-contamination. The simulative analysis illustrates that the
semi-blind estimator outperforms state- of-the-art linear and non-linear
approaches to the massive MIMO channel estimation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08696</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08696</id><created>2015-03-30</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Graded quantization for multiple description coding of compressive
  measurements</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Communications, 2015</journal-ref><doi>10.1109/TCOMM.2015.2413405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is an emerging paradigm for acquisition of compressed
representations of a sparse signal. Its low complexity is appealing for
resource-constrained scenarios like sensor networks. However, such scenarios
are often coupled with unreliable communication channels and providing robust
transmission of the acquired data to a receiver is an issue. Multiple
description coding (MDC) effectively combats channel losses for systems without
feedback, thus raising the interest in developing MDC methods explicitly
designed for the CS framework, and exploiting its properties. We propose a
method called Graded Quantization (CS-GQ) that leverages the democratic
property of compressive measurements to effectively implement MDC, and we
provide methods to optimize its performance. A novel decoding algorithm based
on the alternating directions method of multipliers is derived to reconstruct
signals from a limited number of received descriptions. Simulations are
performed to assess the performance of CS-GQ against other methods in presence
of packet losses. The proposed method is successful at providing robust coding
of CS measurements and outperforms other schemes for the considered test
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08715</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08715</id><created>2015-03-30</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author></authors><title>Industrial Computing Systems: A Case Study of Fault Tolerance Analysis</title><categories>cs.SY cs.DC</categories><comments>6 figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V21(1):50-55, March 2015. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V21P110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault tolerance is a key factor of industrial computing systems design. But
in practical terms, these systems, like every commercial product, are under
great financial constraints and they have to remain in operational state as
long as possible due to their commercial attractiveness. This work provides an
analysis of the instantaneous failure rate of these systems at the end of their
life-time period. On the basis of this analysis, we determine the effect of a
critical increase in the system failure rate and the basic condition of its
existence. The next step determines the maintenance scheduling which can help
to avoid this effect and to extend the system life-time in fault-tolerant mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08723</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08723</id><created>2015-03-30</created><authors><author><keyname>Oster</keyname><forenames>Elad</forenames></author><author><keyname>Gilad</keyname><forenames>Erez</forenames></author><author><keyname>Feigel</keyname><forenames>Alexander</forenames></author></authors><title>Internet comments as a barometer of public opinion</title><categories>physics.soc-ph cs.SI</categories><doi>10.1209/0295-5075/111/28005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social susceptibility is defined and analyzed using data from CNN news
website. The current models of opinion dynamics, voting, and herding in closed
communities are extended, and the community's response to the injection of a
group with predetermined and permanent opinions is calculated. A method to
estimate the values of possible response in Internet communities that follow a
specific developing subject is developed. The level of social influence in a
community follows from the statistics of responses (&quot;like&quot; and &quot;dislike&quot; votes)
to the comments written by the members of the same community. Three real cases
of developing news stories are analyzed. We suggest that Internet comments may
predict the level of social response similar to a barometer that predicts the
intensity of a coming storm in still calm environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08726</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08726</id><created>2015-03-30</created><updated>2015-10-16</updated><authors><author><keyname>Lin</keyname><forenames>Chi-Heng</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Lee</keyname><forenames>Ji-Tang</forenames></author><author><keyname>Liao</keyname><forenames>Wanjiun</forenames></author></authors><title>Error-Resilient Multicasting for Multi-View 3D Videos in Wireless
  Networks</title><categories>cs.MM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.8352</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of naked-eye 3D mobile devices, mobile 3D video services
are becoming increasingly important for video service providers, such as
Youtube and Netflix, while multi-view 3D videos have the potential to inspire a
variety of innovative applications. However, enabling multi-view 3D video
services may overwhelm WiFi networks when every view of a video are
multicasted. In this paper, therefore, we propose to incorporate
depth-image-based rendering (DIBR), which allows each mobile client to
synthesize the desired view from nearby left and right views, in order to
effectively reduce the bandwidth consumption. Moreover, when each client
suffers from packet losses, retransmissions incur additional bandwidth
consumption and excess delay, which in turn undermines the quality of
experience in video applications. To address the above issue, we first discover
the merit of view protection via DIBR for multi-view video multicast using a
mathematical analysis and then design a new protocol, named Multi-View Group
Management Protocol (MVGMP), to support the dynamic join and leave of users and
the change of desired views. The simulation results demonstrate that our
protocol effectively reduces bandwidth consumption and increases the
probability for each client to successfully playback the desired views in a
multi-view 3D video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08744</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08744</id><created>2015-03-30</created><updated>2015-03-31</updated><authors><author><keyname>van Doorn</keyname><forenames>Floris</forenames></author></authors><title>Propositional Calculus in Coq</title><categories>math.LO cs.LO</categories><comments>11 pages, project for 2014 Proof Theory class at CMU. Added ancillary
  files (Coq source files) in this version</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  I formalize important theorems about classical propositional logic in the
proof assistant Coq. The main theorems I prove are (1) the soundness and
completeness of natural deduction calculus, (2) the equivalence between natural
deduction calculus, Hilbert systems and sequent calculus and (3) cut
elimination for sequent calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08758</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08758</id><created>2015-03-30</created><authors><author><keyname>Luo</keyname><forenames>Chunbo</forenames></author><author><keyname>Peoples</keyname><forenames>Cathryn</forenames></author><author><keyname>Parr</keyname><forenames>Gerard</forenames></author><author><keyname>McClean</keyname><forenames>Sally</forenames></author><author><keyname>Wang</keyname><forenames>Xinheng</forenames></author></authors><title>Hybrid Demodulate-Forward Relay Protocol for Two-Way Relay Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>12 pages. 12 figures, Accepted by IEEE Transactions on Wireless
  Communications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Two-Way Relay Channel (TWRC) plays an important role in relay networks, and
efficient relaying protocols are particularly important for this model.
{\color{black} However, existing protocols may not be able to realize the
potential of TWRC if the two independent fading channels are not carefully
handled}. In this paper, a Hybrid DeModulate-Forward (HDMF) protocol is
proposed to address such a problem. {\color{black} We first introduce the two
basic components of HDMF - direct and differential DMF, and then propose the
key decision criterion for HDMF based on the corresponding log-likelihood
ratios. We further enhance the protocol so that it can be applied independently
from the modulation schemes. Through extensive mathematical analysis,
theoretical performance of the proposed protocol is investigated. By comparing
with existing protocols, the proposed HDMF has lower error rate. A novel
scheduling scheme for the proposed protocol is introduced, which has lower
length than the benchmark method. The results also reveal the protocol's
potential to improve spectrum efficiency of relay channels with unbalanced
bilateral traffic.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08761</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08761</id><created>2015-03-30</created><authors><author><keyname>Rybakov</keyname><forenames>Vladimir</forenames></author></authors><title>Intransitive Linear Temporal Logic, Knowledge from Past, Decidability,
  Admissible Rules</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1406.2783</comments><msc-class>03B70, 03B44, 03B47, 03B42, 03B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our manuscript studies linear temporal (with UNTIL and NEXT) logic based at a
conception of intransitive time. non-transitive time. In particular, we
demonstrate how the notion of knowledge might be represented in such a
framework (here we consider logical operation NN and the operation UNTIL
(actually, the time overall) to be directed to past). The basic mathematical
problems we study are the fundamental ones for any logical system
  - decidability and decidability w.r.t. admissible rules. First, we consider
the logic with non-uniform non-transitivity, and describe how to solve the
decidability problem for this logic. Then we consider a modification of this
logic - linear temporal logic with uniform intransitivity and solve the problem
of admissibility for inference rules. A series of open problems is enumerated
in the concluding part of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08768</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08768</id><created>2015-03-30</created><updated>2015-11-23</updated><authors><author><keyname>Syta</keyname><forenames>Ewa</forenames></author><author><keyname>Tamas</keyname><forenames>Iulia</forenames></author><author><keyname>Visher</keyname><forenames>Dylan</forenames></author><author><keyname>Wolinsky</keyname><forenames>David Isaac</forenames></author><author><keyname>Gasser</keyname><forenames>Linus</forenames></author><author><keyname>Gailly</keyname><forenames>Nicolas</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Keeping Authorities &quot;Honest or Bust&quot; with Decentralized Witness
  Cosigning</title><categories>cs.CR</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical Internet authorities - such as time, name, certificate, and software
update services - are prime targets for hackers, criminals, and spy agencies
who might secretly use an authority's private keys to compromise many other
hosts. To protect both authorities and their users proactively we introduce
CoSi, a protocol enabling authorities to have their statements collectively
signed by a diverse, decentralized, scalable group of witnesses. Clients can
verify these witness cosignatures efficiently without extra communication,
protecting clients from secret misuse of the authority's private keys and
disincentivizing the malicious acquisition of these keys in the first place.
CoSi builds on existing cryptographic multisignature methods, scaling them to
support thousands of participants via signature aggregation over efficient
communication trees. A working prototype demonstrates CoSi in the context of
timestamping and logging authorities, enabling groups of over 8,000 distributed
witnesses to collectively sign authoritative statements in under two seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08771</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08771</id><created>2015-03-30</created><updated>2015-08-03</updated><authors><author><keyname>Zhang</keyname><forenames>Jinxue</forenames></author><author><keyname>Sun</keyname><forenames>Jingchao</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Yanchao</forenames></author></authors><title>Your Actions Tell Where You Are: Uncovering Twitter Users in a
  Metropolitan Area</title><categories>cs.SI</categories><comments>Accepted by IEEE Conference on Communications and Network Security
  (CNS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is an extremely popular social networking platform. Most Twitter
users do not disclose their locations due to privacy concerns. Although
inferring the location of an individual Twitter user has been extensively
studied, it is still missing to effectively find the majority of the users in a
specific geographical area without scanning the whole Twittersphere, and
obtaining these users will result in both positive and negative significance.
In this paper, we propose LocInfer, a novel and lightweight system to tackle
this problem. LocInfer explores the fact that user communications in Twitter
exhibit strong geographic locality, which we validate through large-scale
datasets. Based on the experiments from four representative metropolitan areas
in U.S., LocInfer can discover on average 86.6% of the users with 73.2%
accuracy in each area by only checking a small set of candidate users. We also
present a countermeasure to the users highly sensitive to location privacy and
show its efficacy by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08776</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08776</id><created>2015-03-30</created><updated>2015-08-20</updated><authors><author><keyname>Stalzer</keyname><forenames>Mark</forenames></author><author><keyname>Mentzel</keyname><forenames>Chris</forenames></author></authors><title>A Preliminary Review of Influential Works in Data-Driven Discovery</title><categories>cs.DL cs.GL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gordon and Betty Moore Foundation ran an Investigator Competition as part
of its Data-Driven Discovery Initiative in 2014. We received about 1,100
applications and each applicant had the opportunity to list up to five
influential works in the general field of &quot;Big Data&quot; for scientific discovery.
We collected nearly 5,000 references and 53 works were cited at least six
times. This paper contains our preliminary findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08778</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08778</id><created>2015-03-30</created><updated>2016-01-28</updated><authors><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>Covert Communication over Noisy Channels: A Resolvability Perspective</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures, accepted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the situation in which a transmitter attempts to communicate
reliably over a discrete memoryless channel while simultaneously ensuring
covertness (low probability of detection) with respect to a warden, who
observes the signals through another discrete memoryless channel. We develop a
coding scheme based on the principle of channel resolvability, which
generalizes and extends prior work in several directions. First, it shows that,
irrespective of the quality of the channels, it is possible to communicate on
the order of $\sqrt{n}$ reliable and covert bits over $n$ channel uses if the
transmitter and the receiver share on the order of $\sqrt{n}$ key bits; this
improves upon earlier results requiring on the order of $\sqrt{n}\log n$ key
bits. Second, it proves that, if the receiver's channel is &quot;better&quot; than the
warden's channel in a sense that we make precise, it is possible to communicate
on the order of $\sqrt{n}$ reliable and covert bits over $n$ channel uses
without a secret key; this generalizes earlier results established for binary
symmetric channels. We also identify the fundamental limits of covert and
secret communications in terms of the optimal asymptotic scaling of the message
size and key size, and we extend the analysis to Gaussian channels. The main
technical problem that we address is how to develop concentration inequalities
for &quot;low-weight&quot; sequences; the crux of our approach is to define suitably
modified typical sets that are amenable to concentration inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08782</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08782</id><created>2015-03-30</created><updated>2015-09-29</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author></authors><title>Robust Recovery of Positive Stream of Pulses</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering the delays and amplitudes of a
positive superposition of pulses. This problem is motivated by a variety of
applications, such as single-molecule microscopy. We show that for univariate
and bivariate stream of pulses, the recovery error of a tractable convex
optimization problem is proportional to the noise level. The recovery error
also depends on the localization properties of the pulse and on the density of
the delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08792</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08792</id><created>2015-03-30</created><authors><author><keyname>Kiefer</keyname><forenames>Sandra</forenames></author><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author><author><keyname>Selman</keyname><forenames>Erkal</forenames></author></authors><title>Graphs Identified by Logics with Counting</title><categories>cs.LO cs.DM math.CO math.LO</categories><comments>33 pages, 8 Figures</comments><msc-class>68Q19, 03C13, 05C75, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify graphs and, more generally, finite relational structures that are
identified by C2, that is, two-variable first-order logic with counting. Using
this classification, we show that it can be decided in almost linear time
whether a structure is identified by C2. Our classification implies that for
every graph identified by this logic, all vertex-colored versions of it are
also identified. A similar statement is true for finite relational structures.
  We provide constructions that solve the inversion problem for finite
structures in linear time. This problem has previously been shown to be
polynomial time solvable by Martin Otto. For graphs, we conclude that every
C2-equivalence class contains a graph whose orbits are exactly the classes of
the C2-partition of its vertex set and which has a single automorphism
witnessing this fact.
  For general k, we show that such statements are not true by providing
examples of graphs of size linear in k which are identified by C3 but for which
the orbit partition is strictly finer than the Ck-partition. We also provide
identified graphs which have vertex-colored versions that are not identified by
Ck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08796</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08796</id><created>2015-03-30</created><authors><author><keyname>Hoberg</keyname><forenames>Rebecca</forenames></author><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>A Logarithmic Additive Integrality Gap for Bin Packing</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For bin packing, the input consists of $n$ items with sizes $s_1,...,s_n \in
[0,1]$ which have to be assigned to a minimum number of bins of size 1.
Recently, the second author gave an LP-based polynomial time algorithm that
employed techniques from discrepancy theory to find a solution using at most
$OPT + O(\log OPT \cdot \log \log OPT)$ bins.
  In this paper, we present an approximation algorithm that has an additive gap
of only $O(\log OPT)$ bins, which matches certain combinatorial lower bounds.
Any further improvement would have to use more algebraic structure. Our
improvement is based on a combination of discrepancy theory techniques and a
novel 2-stage packing: first we pack items into containers; then we pack
containers into bins of size 1. Apart from being more effective, we believe our
algorithm is much cleaner than the one of Rothvoss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08804</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08804</id><created>2015-03-30</created><updated>2015-12-23</updated><authors><author><keyname>De Loera</keyname><forenames>Jes&#xfa;s A.</forenames></author><author><keyname>Petrovi&#x107;</keyname><forenames>Sonja</forenames></author><author><keyname>Stasi</keyname><forenames>Despina</forenames></author></authors><title>Random Sampling in Computational Algebra: Helly Numbers and Violator
  Spaces</title><categories>cs.DM math.AC math.AG math.CO</categories><comments>Minor edits, added two references; results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper transfers a randomized algorithm, originally used in geometric
optimization, to computational problems in commutative algebra. We show that
Clarkson's sampling algorithm can be applied to two problems in computational
algebra: solving large-scale polynomial systems and finding small generating
sets of graded ideals. The cornerstone of our work is showing that the theory
of violator spaces of G\&quot;artner et al.\ applies to polynomial ideal problems.
To show this, one utilizes a Helly-type result for algebraic varieties. The
resulting algorithms have expected runtime linear in the number of input
polynomials, making the ideas interesting for handling systems with very large
numbers of polynomials, but whose rank in the vector space of polynomials is
small (e.g., when the number of variables and degree is constant).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08809</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08809</id><created>2015-03-30</created><updated>2016-01-26</updated><authors><author><keyname>Briggs</keyname><forenames>J. P.</forenames></author><author><keyname>Pennycook</keyname><forenames>S. J.</forenames></author><author><keyname>Fergusson</keyname><forenames>J. R.</forenames></author><author><keyname>J&#xe4;ykk&#xe4;</keyname><forenames>J.</forenames></author><author><keyname>Shellard</keyname><forenames>E. P. S.</forenames></author></authors><title>Separable projection integrals for higher-order correlators of the
  cosmic microwave sky: Acceleration by factors exceeding 100</title><categories>cs.DC astro-ph.CO cs.PF</categories><comments>Accepted by Journal of Computational Physics</comments><doi>10.1016/j.jcp.2016.01.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a case study describing efforts to optimise and modernise &quot;Modal&quot;,
the simulation and analysis pipeline used by the Planck satellite experiment
for constraining general non-Gaussian models of the early universe via the
bispectrum (or three-point correlator) of the cosmic microwave background
radiation. We focus on one particular element of the code: the projection of
bispectra from the end of inflation to the spherical shell at decoupling, which
defines the CMB we observe today. This code involves a three-dimensional inner
product between two functions, one of which requires an integral, on a
non-rectangular domain containing a sparse grid. We show that by employing
separable methods this calculation can be reduced to a one-dimensional
summation plus two integrations, reducing the overall dimensionality from four
to three. The introduction of separable functions also solves the issue of the
non-rectangular sparse grid. This separable method can become unstable in
certain cases and so the slower non-separable integral must be calculated
instead. We present a discussion of the optimisation of both approaches. We
show significant speed-ups of ~100x, arising from a combination of algorithmic
improvements and architecture-aware optimisations targeted at improving thread
and vectorisation behaviour. The resulting MPI/OpenMP hybrid code is capable of
executing on clusters containing processors and/or coprocessors, with
strong-scaling efficiency of 98.6% on up to 16 nodes. We find that a single
coprocessor outperforms two processor sockets by a factor of 1.3x and that
running the same code across a combination of both microarchitectures improves
performance-per-node by a factor of 3.38x. By making bispectrum calculations
competitive with those for the power spectrum (or two-point correlator) we are
now able to consider joint analysis for cosmological science exploitation of
new data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08810</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08810</id><created>2015-03-30</created><authors><author><keyname>Bonato</keyname><forenames>Anthony</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>P&#xe9;rez-Gim&#xe9;nez</keyname><forenames>Xavier</forenames></author><author><keyname>Pra&#x142;at</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>A probabilistic version of the game of Zombies and Survivors on graphs</title><categories>cs.DM</categories><msc-class>(Primary) 05C57, (Secondary) 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a new probabilistic graph searching game played on graphs,
inspired by the familiar game of Cops and Robbers. In Zombies and Survivors, a
set of zombies attempts to eat a lone survivor loose on a given graph. The
zombies randomly choose their initial location, and during the course of the
game, move directly toward the survivor. At each round, they move to the
neighbouring vertex that minimizes the distance to the survivor; if there is
more than one such vertex, then they choose one uniformly at random. The
survivor attempts to escape from the zombies by moving to a neighbouring vertex
or staying on his current vertex. The zombies win if eventually one of them
eats the survivor by landing on their vertex; otherwise, the survivor wins. The
zombie number of a graph is the minimum number of zombies needed to play such
that the probability that they win is strictly greater than 1/2. We present
asymptotic results for the zombie numbers of several graph families, such as
cycles, hypercubes, incidence graphs of projective planes, and Cartesian and
toroidal grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08818</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08818</id><created>2015-03-29</created><authors><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author></authors><title>Founding Digital Currency on Imprecise Commodity</title><categories>cs.CY cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1503.08407</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current digital currency schemes provide instantaneous exchange on precise
commodity, in which &quot;precise&quot; means a buyer can possibly verify the function of
the commodity without error. However, imprecise commodities, e.g. statistical
data, with error existing are abundant in digital world. Existing digital
currency schemes do not offer a mechanism to help the buyer for payment
decision on precision of commodity, which may lead the buyer to a dilemma
between having to buy and being unconfident. In this paper, we design a
currency schemes IDCS for imprecise digital commodity. IDCS completes a trade
in three stages of handshake between a buyer and providers. We present an IDCS
prototype implementation that assigns weights on the trustworthy of the
providers, and calculates a confidence level for the buyer to decide the
quality of a imprecise commodity. In experiment, we characterize the
performance of IDCS prototype under varying impact factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08819</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08819</id><created>2015-03-30</created><authors><author><keyname>Mandal</keyname><forenames>Swagata</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Subhasis</forenames></author></authors><title>FPGA based High Speed Data Acquisition System for High Energy Physics
  Application</title><categories>physics.ins-det cs.AR hep-ex</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high energy physics experiments (HEP), high speed and fault resilient data
communication is needed between detectors/sensors and the host PC. Transient
faults can occur in the communication hardware due to various external effects
like presence of charged particles, noise in the environment or radiation
effects in HEP experiments and that leads to single/multiple bit error. In
order to keep the communication system functional in such a radiation
environment where direct intervention of human is not possible, a high speed
data acquisition (DAQ) architecture is necessary which supports error recovery.
This design presents an efficient implementation of field programmable gate
array (FPGA) based high speed DAQ system with optical communication link
supported by multi-bit error correcting model. The design has been implemented
on Xilinx Kintex-7 board and is tested for board to board communication as well
as for PC communication using PCI (Peripheral Component Interconnect express).
Data communication speed up to 4.8 Gbps has been achieved in board to board and
board to PC communication and estimation of resource utilization and critical
path delay are also measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08843</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08843</id><created>2015-03-30</created><authors><author><keyname>Sun</keyname><forenames>Peng</forenames></author><author><keyname>Min</keyname><forenames>James K.</forenames></author><author><keyname>Xiong</keyname><forenames>Guanglei</forenames></author></authors><title>Globally Tuned Cascade Pose Regression via Back Propagation with
  Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a successful pose estimation algorithm, called Cascade Pose
Regression (CPR), was proposed in the literature. Trained over Pose Index
Feature, CPR is a regressor ensemble that is similar to Boosting. In this paper
we show how CPR can be represented as a Neural Network. Specifically, we adopt
a Graph Transformer Network (GTN) representation and accordingly train CPR with
Back Propagation (BP) that permits globally tuning. In contrast, previous CPR
literature only took a layer wise training without any post fine tuning. We
empirically show that global training with BP outperforms layer-wise
(pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor,
which utilized sparse connection to learn local image feature representation.
We tested the proposed CPR-GTN on 2D face pose estimation problem as in
previous CPR literature. Besides, we also investigated the possibility of
extending CPR-GTN to 3D pose estimation by doing experiments using 3D Computed
Tomography dataset for heart segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08847</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08847</id><created>2015-03-30</created><updated>2015-07-15</updated><authors><author><keyname>Beigel</keyname><forenames>Richard</forenames></author><author><keyname>Gasarch</keyname><forenames>William</forenames></author></authors><title>On the Sizes of DPDAs, PDAs, LBAs</title><categories>cs.FL</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are languages A such that there is a Pushdown Automata (PDA) that
recognizes A which is much smaller than any Deterministic Pushdown Automata
(DPDA) that recognizes A. There are languages A such that there is a Linear
Bounded Automata (Linear Space Turing Machine, henceforth LBA) that recognizes
A which is much smaller than ny PDA that recognizes A. There are languages A
such that both A and compliment(A) are recognizable by a PDA, but the PDA for A
is much smaller than the PDA for compliment(A). There are languages A1, A2 such
that A1,A2,A1 INTERSECT A_2 are recognizable by a PDA, but the PDA for A1 and
A2 are much smaller than the PDA for A1 INTERSECT A2. We investigate these
phenomenon and show that, in all these cases, the size difference is captured
by a function whose Turing degree is on the second level of the arithmetic
hierarchy.
  Our theorems lead to infinitely-often results. For example: for infinitely
many $n$ there exists a language An recognized by a DPDA such that there is a
small PDA for An, but any DPDA for An is large. We look at cases where we can
get almost-all results, though with much smaller size differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08853</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08853</id><created>2015-03-30</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Tanner</keyname><forenames>James</forenames></author></authors><title>Reconciling saliency and object center-bias hypotheses in explaining
  free-viewing fixations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting where people look in natural scenes has attracted a lot of
interest in computer vision and computational neuroscience over the past two
decades. Two seemingly contrasting categories of cues have been proposed to
influence where people look: \textit{low-level image saliency} and
\textit{high-level semantic information}. Our first contribution is to take a
detailed look at these cues to confirm the hypothesis proposed by
Henderson~\cite{henderson1993eye} and Nuthmann \&amp;
Henderson~\cite{nuthmann2010object} that observers tend to look at the center
of objects. We analyzed fixation data for scene free-viewing over 17 observers
on 60 fully annotated images with various types of objects. Images contained
different types of scenes, such as natural scenes, line drawings, and 3D
rendered scenes. Our second contribution is to propose a simple combined model
of low-level saliency and object center-bias that outperforms each individual
component significantly over our data, as well as on the OSIE dataset by Xu et
al.~\cite{xu2014predicting}. The results reconcile saliency with object
center-bias hypotheses and highlight that both types of cues are important in
guiding fixations. Our work opens new directions to understand strategies that
humans use in observing scenes and objects, and demonstrates the construction
of combined models of low-level saliency and high-level object-based
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08855</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08855</id><created>2015-03-30</created><authors><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author><author><keyname>Ling</keyname><forenames>Qing</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Schizas</keyname><forenames>Ioannis D.</forenames></author><author><keyname>Zhu</keyname><forenames>Hao</forenames></author></authors><title>Decentralized learning for wireless communications and networking</title><categories>math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML</categories><comments>Contributed chapter to appear in Splitting Methods in Communication
  and Imaging, Science and Engineering, R. Glowinski, S. Osher, and W. Yin,
  Editors, New York, Springer, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter deals with decentralized learning algorithms for in-network
processing of graph-valued data. A generic learning problem is formulated and
recast into a separable form, which is iteratively minimized using the
alternating-direction method of multipliers (ADMM) so as to gain the desired
degree of parallelization. Without exchanging elements from the distributed
training sets and keeping inter-node communications at affordable levels, the
local (per-node) learners consent to the desired quantity inferred globally,
meaning the one obtained if the entire training data set were centrally
available. Impact of the decentralized learning framework to contemporary
wireless communications and networking tasks is illustrated through case
studies including target tracking using wireless sensor networks, unveiling
Internet traffic anomalies, power system state estimation, as well as spectrum
cartography for wireless cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08866</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08866</id><created>2015-03-30</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Mettler</keyname><forenames>Berenice</forenames></author><author><keyname>Kowalewski</keyname><forenames>Timonthy M.</forenames></author></authors><title>Towards Data-Driven Hierarchical Surgical Skill Analysis</title><categories>cs.HC</categories><comments>M2CAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper evaluates methods of hierarchical skill analysis developed in
aerospace to the problem of surgical skill assessment and modeling. The
analysis employs tool motion data of Fundamental of Laparoscopic Skills (FLS)
tasks collected from clinicians of various skill levels at three different
clinical teaching hospitals in the United States. Outcomes are evaluated based
on their ability to provide relevant information about the underlying processes
across the entire system hierarchy including control, guidance and planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08873</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08873</id><created>2015-03-30</created><authors><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author></authors><title>Fast Label Embeddings for Extremely Large Output Spaces</title><categories>cs.LG</categories><comments>Accepted as a workshop contribution at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern multiclass and multilabel problems are characterized by
increasingly large output spaces. For these problems, label embeddings have
been shown to be a useful primitive that can improve computational and
statistical efficiency. In this work we utilize a correspondence between rank
constrained estimation and low dimensional label embeddings that uncovers a
fast label embedding algorithm which works in both the multiclass and
multilabel settings. The result is a randomized algorithm for partial least
squares, whose running time is exponentially faster than naive algorithms. We
demonstrate our techniques on two large-scale public datasets, from the Large
Scale Hierarchical Text Challenge and the Open Directory Project, where we
obtain state of the art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08877</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08877</id><created>2015-03-30</created><authors><author><keyname>Isard</keyname><forenames>Michael</forenames></author><author><keyname>Abadi</keyname><forenames>Mart&#xed;n</forenames></author></authors><title>Falkirk Wheel: Rollback Recovery for Dataflow Systems</title><categories>cs.DC</categories><comments>DRAFT work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new model for rollback recovery in distributed dataflow systems.
We explain existing rollback schemes by assigning a logical time to each event
such as a message delivery. If some processors fail during an execution, the
system rolls back by selecting a set of logical times for each processor. The
effect of events at times within the set is retained or restored from saved
state, while the effect of other events is undone and re-executed. We show
that, by adopting different logical time &quot;domains&quot; at different processors, an
application can adopt appropriate checkpointing schemes for different parts of
its computation. We illustrate with an example of an application that combines
batch processing with low-latency streaming updates. We show rules, and an
algorithm, to determine a globally consistent state for rollback in a system
that uses multiple logical time domains. We also introduce selective rollback
at a processor, which can selectively preserve the effect of events at some
logical times and not others, independent of the original order of execution of
those events. Selective rollback permits new checkpointing policies that are
particularly well suited to iterative streaming algorithms. We report on an
implementation of our new framework in the context of the Naiad system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08880</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08880</id><created>2015-03-30</created><authors><author><keyname>Borenstein</keyname><forenames>David Bruce</forenames></author></authors><title>A composite constraints approach to declarative agent-based modeling</title><categories>cs.MA</categories><comments>12 pages, 4 figures</comments><acm-class>I.6.8; I.6.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent-based models (ABMs) are ubiquitous in research and industry. Currently,
simulating ABMs involves at least some imperative (step-by-step) computer
instructions. An alternative approach is declarative programming, in which a
set of requirements is described at a high level of abstraction. Here we
describe a fully declarative approach to the automated construction of
simulations for ABMs. In this framework, logic for ABM simulations is
encapsulated into predefined components. The user specifies a set of
requirements describing the desired functionality. Additionally, each component
has a set of consistency requirements. The framework iteratively seeks a
simulation design that satisfies both user and system requirements. This
approach allows the user to omit most details from the simulation
specification, simplifying simulation design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08889</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08889</id><created>2015-03-30</created><updated>2015-04-02</updated><authors><author><keyname>Cong</keyname><forenames>Yirui</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Interference Prediction in Mobile Ad Hoc Networks with a General
  Mobility Model</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures, accepted for publication in IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a mobile ad hoc network (MANET), effective prediction of time-varying
interferences can enable adaptive transmission designs and therefore improve
the communication performance. This paper investigates interference prediction
in MANETs with a finite number of nodes by proposing and using a general-order
linear model for node mobility. The proposed mobility model can well
approximate node dynamics of practical MANETs. In contrast to previous studies
on interference statistics, we are able through this model to give a best
estimate of the time-varying interference at any time rather than long-term
average effects. Specifically, we propose a compound Gaussian point process
functional as a general framework to obtain analytical results on the mean
value and moment-generating function of the interference prediction. With a
series form of this functional, we give the necessary and sufficient condition
for when the prediction is essentially equivalent to that from a Binomial Point
Process (BPP) network in the limit as time goes to infinity. These conditions
permit one to rigorously determine when the commonly used BPP approximations
are valid. Finally, our simulation results corroborate the effectiveness and
accuracy of the analytical results on interference prediction and also show the
advantages of our method in dealing with complex mobilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08895</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08895</id><created>2015-03-30</created><updated>2015-11-24</updated><authors><author><keyname>Sukhbaatar</keyname><forenames>Sainbayar</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>End-To-End Memory Networks</title><categories>cs.NE cs.CL</categories><comments>Accepted to NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a neural network with a recurrent attention model over a
possibly large external memory. The architecture is a form of Memory Network
(Weston et al., 2015) but unlike the model in that work, it is trained
end-to-end, and hence requires significantly less supervision during training,
making it more generally applicable in realistic settings. It can also be seen
as an extension of RNNsearch to the case where multiple computational steps
(hops) are performed per output symbol. The flexibility of the model allows us
to apply it to tasks as diverse as (synthetic) question answering and to
language modeling. For the former our approach is competitive with Memory
Networks, but with less supervision. For the latter, on the Penn TreeBank and
Text8 datasets our approach demonstrates comparable performance to RNNs and
LSTMs. In both cases we show that the key concept of multiple computational
hops yields improved results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08909</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08909</id><created>2015-03-31</created><updated>2015-04-13</updated><authors><author><keyname>Ng</keyname><forenames>Joe Yue-Hei</forenames></author><author><keyname>Hausknecht</keyname><forenames>Matthew</forenames></author><author><keyname>Vijayanarasimhan</keyname><forenames>Sudheendra</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Monga</keyname><forenames>Rajat</forenames></author><author><keyname>Toderici</keyname><forenames>George</forenames></author></authors><title>Beyond Short Snippets: Deep Networks for Video Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) have been extensively applied for image
recognition problems giving state-of-the-art results on recognition, detection,
segmentation and retrieval. In this work we propose and evaluate several deep
neural network architectures to combine image information across a video over
longer time periods than previously attempted. We propose two methods capable
of handling full length videos. The first method explores various convolutional
temporal feature pooling architectures, examining the various design choices
which need to be made when adapting a CNN for this task. The second proposed
method explicitly models the video as an ordered sequence of frames. For this
purpose we employ a recurrent neural network that uses Long Short-Term Memory
(LSTM) cells which are connected to the output of the underlying CNN. Our best
networks exhibit significant performance improvements over previously published
results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101
datasets with (88.6% vs. 88.0%) and without additional optical flow information
(82.6% vs. 72.8%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08913</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08913</id><created>2015-03-31</created><authors><author><keyname>Tithi</keyname><forenames>Tasnuva</forenames></author><author><keyname>Winstead</keyname><forenames>Chris</forenames></author><author><keyname>Sundararajan</keyname><forenames>Gopalakrishnan</forenames></author></authors><title>Decoding LDPC codes via Noisy Gradient Descent Bit-Flipping with
  Re-Decoding</title><categories>cs.IT math.IT</categories><comments>4 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the performance of the Noisy Gradient Descent Bit
Flipping (NGDBF) algorithm under re-decoding of failed frames. NGDBF is a
recent algorithm that uses a non-deterministic gradient descent search to
decode low-density parity check (LDPC) codes. The proposed re-decode procedure
obtains improved performance because the perturbations are independent at each
re-decoding phase, therefore increasing the likelihood of successful decoding.
We examine the benefits of re-decoding for an LDPC code from the IEEE 802.3an
standard, and find that only a small fraction of re-decoded frames are needed
to obtain significant performance benefits. When re-decoding is used, the NGDBF
performance is very close to a benchmark offset min-sum decoder for the 802.3an
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08925</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08925</id><created>2015-03-31</created><authors><author><keyname>Hamano</keyname><forenames>Masahiro</forenames></author></authors><title>Geometry of Interaction for MALL via Hughes-vanGlabbeek Proof-Nets</title><categories>cs.LO math.LO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents, for the first time, a Geometry of Interaction (GoI)
interpretation using Hughes-vanGlabbeek (HvG) proof-nets for multiplicative
additive linear logic (MALL). Our GoI captures dynamically HvG's geometric
correctness criterion--the toggling cycle condition--in terms of algebraic
operators. Our new ingredient is a scalar extension of the *-algebra in
Girard's *-ring of partial isometries over a boolean polynomial ring with
literals of eigenweights as indeterminates. In order to capture feedback
arising from cuts, we construct a finer grained execution formula. The
expansion of this execution formula is longer than that for collections of
slices for multiplicative GoI, hence is harder to prove termination. Our GoI
gives a dynamical, semantical account of boolean valuations (in particular,
pruning sub-proofs), conversion of weights (in particular, alpha-conversion),
and additive (co)contraction, peculiar to additive proof-theory. Termination of
our execution formula is shown to correspond to HvG's toggling criterion. The
slice-wise restriction of our execution formula (by collapsing the boolean
structure) yields the well known correspondence, explicit or implicit in
previous works on multiplicative GoI, between the convergence of execution
formulas and acyclicity of proof-nets. Feedback arising from the execution
formula by restricting to the boolean structure yields definability of
eigenweights among cuts from the rest of the eigenweights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08928</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08928</id><created>2015-03-31</created><authors><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Fan</keyname><forenames>Lisheng</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Near-Optimal Modulo-and-Forward Scheme for the Untrusted Relay Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies an untrusted relay channel, in which the destination sends
artificial noise simultaneously with the source sending a message to the relay,
in order to protect the source's confidential message. The traditional
amplify-and-forward (AF) scheme shows poor performance in this situation
because of the interference power dilemma: providing better security by using
stronger artificial noise will decrease the confidential message power from the
relay to the destination. To solve this problem, a modulo-and-forward (MF)
operation at the relay with nested lattice encoding at the source is proposed.
For this system with full channel state information at the transmitter (CSIT),
theoretical analysis shows that the proposed MF scheme approaches the secrecy
capacity within 1/2 bit for any channel realization, and hence achieves full
generalized security degrees of freedom (G-SDoF). In contrast, the AF scheme
can only achieve a small fraction of the G-SDoF. For this system without any
CSIT, the total outage event, defined as either connection outage or secrecy
outage, is introduced. Based on this total outage definition, analysis shows
that the proposed MF scheme achieves the full generalized secure diversity gain
(G-SDG) of order one. On the other hand, the AF scheme can only achieve a G-SDG
of 1/2 at most.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08934</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08934</id><created>2015-03-31</created><authors><author><keyname>Oshima</keyname><forenames>Takahiro</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Index ARQ Protocol for Reliable Contents Distribution over Broadcast
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we propose a broadcast ARQ protocol based on the
concept of index coding. In the proposed scenario, a server wishes to transmit
a finite sequence of packets to multiple receivers via a broadcast channel with
packet erasures until all of the receivers successfully receive all of the
packets. In the retransmission phase, the server produces a coded packet as a
retransmitted packet based on the side-information sent from the receivers via
feedback channels. A notable feature of the proposed protocol is that the
decoding process at the receiver side has low decoding complexity because only
a small number of addition operations are needed in order to recover an
intended packet. This feature may be preferable for reducing the power
consumption of receivers. The throughput performance of the proposed protocol
is close to that of the ideal FEC throughput performance when the erasure
probability is less than $0.1$. This implies that the proposed protocol
provides almost optimal throughput performance in such a regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08936</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08936</id><created>2015-03-31</created><authors><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames></author><author><keyname>van Gool</keyname><forenames>Samuel J.</forenames></author></authors><title>A model-theoretic characterization of monadic second order logic on
  infinite words</title><categories>math.LO cs.FL cs.LO</categories><comments>Preprint, submitted. 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monadic second order logic and linear temporal logic are two logical
formalisms that can be used to describe classes of infinite words, i.e.,
first-order models based on the natural numbers with order, successor, and
finitely many unary predicate symbols.
  Monadic second order logic over infinite words (S1S) can alternatively be
described as a first-order logic interpreted in $\mathcal{P}(\omega)$, the
power set Boolean algebra of the natural numbers, equipped with modal operators
for 'initial', 'next' and 'future' states. We prove that the first-order theory
of this structure is the model companion of a class of algebras corresponding
to the appropriate version of linear temporal logic (LTL).
  The proof makes crucial use of two classical, non-trivial results from the
literature, namely the completeness of LTL with respect to the natural numbers,
and the correspondence between S1S-formulas and B\&quot;uchi automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08937</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08937</id><created>2015-03-31</created><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Evaluation of Symmetric Mutual Information of the Simplified TDMR
  Channel Model</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, a simplified two-dimensional magnetic recording (TDMR)
channel model is proposed in order to capture the qualitative features of
writing and read-back processes of TDMR systems.The proposed channel model
incorporates the effects of both linear interference from adjacent bit-cells
and signal-dependent noise due to irregular grain boundaries between adjacent
bit-cells. The simplicity of the proposed model enables us to derive the closed
form of the conditional PDF representing the probabilistic nature of the
channel. The conditional PDF is Gaussian distributed and is parameterized by a
signal-dependent covariance matrix. Based on this conditional PDF, a Monte
Carlo method for approximating the symmetric mutual information of this channel
is developed. The symmetric mutual information is closely related to the areal
density limit for TDMR systems. The numerical results suggest that we may need
low-rate coding, e.g., 2/3 or 1/2, when the jitter-like noise becomes dominant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08944</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08944</id><created>2015-03-31</created><updated>2015-07-06</updated><authors><author><keyname>Zhou</keyname><forenames>Qiuju</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The Normalization of Occurrence and Co-occurrence Matrices in
  Bibliometrics using Cosine Similarities and Ochiai Coefficients</title><categories>cs.DL</categories><comments>accepted for publication in the Journal of the Association for
  Information Science and Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that Ochiai similarity of the co-occurrence matrix is equal to
cosine similarity in the underlying occurrence matrix. Neither the cosine nor
the Pearson correlation should be used for the normalization of co-occurrence
matrices because the similarity is then normalized twice, and therefore
over-estimated; the Ochiai coefficient can be used instead. Results are shown
using a small matrix (5 cases, 4 variables) for didactic reasons, and also
Ahlgren et al.'s (2003) co-occurrence matrix of 24 authors in library and
information sciences. The over-estimation is shown numerically and will be
illustrated using multidimensional scaling and cluster dendograms. If the
occurrence matrix is not available (such as in internet research or author
co-citation analysis) using Ochiai for the normalization is preferable to using
the cosine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08945</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08945</id><created>2015-03-31</created><authors><author><keyname>Hammouda</keyname><forenames>Marwan</forenames></author><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Peissig</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Performance Analysis of Energy-Detection-Based Massive SIMO</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, communications systems that are both energy efficient and reliable
are under investigation. In this paper, we concentrate on an
energy-detection-based transmission scheme where a communication scenario
between a transmitter with one antenna and a receiver with significantly many
antennas is considered. We assume that the receiver initially calculates the
average energy across all antennas, and then decodes the transmitted data by
exploiting the average energy level. Then, we calculate the average symbol
error probability by means of a maximum a-posteriori probability detector at
the receiver. Following that, we provide the optimal decision regions.
Furthermore, we develop an iterative algorithm that reaches the optimal
constellation diagram under a given average transmit power constraint. Through
numerical analysis, we explore the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08946</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08946</id><created>2015-03-31</created><updated>2015-05-09</updated><authors><author><keyname>Zhao</keyname><forenames>Weijie</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Rusu</keyname><forenames>Florin</forenames></author></authors><title>Workload-Driven Vertical Partitioning for Effective Query Processing
  over Raw Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional databases are not equipped with the adequate functionality to
handle the volume and variety of &quot;Big Data&quot;. Strict schema definition and data
loading are prerequisites even for the most primitive query session. Raw data
processing has been proposed as a schema-on-demand alternative that provides
instant access to the data. When loading is an option, it is driven exclusively
by the current-running query, resulting in sub-optimal performance across a
query workload. In this paper, we investigate the problem of workload-driven
raw data processing with partial loading. We model loading as fully-replicated
binary vertical partitioning. We provide a linear mixed integer programming
optimization formulation that we prove to be NP-hard. We design a two-stage
heuristic that comes within close range of the optimal solution in a fraction
of the time. We extend the optimization formulation and the heuristic to
pipelined raw data processing, scenario in which data access and extraction are
executed concurrently. We provide three case-studies over real data formats
that confirm the accuracy of the model when implemented in a state-of-the-art
pipelined operator for raw data processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08992</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08992</id><created>2015-03-31</created><authors><author><keyname>Zhang</keyname><forenames>Changwang</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Groppelli</keyname><forenames>Elisabetta</forenames></author><author><keyname>Pellegrino</keyname><forenames>Pierre</forenames></author><author><keyname>Williams</keyname><forenames>Ian</forenames></author><author><keyname>Borrow</keyname><forenames>Persephone</forenames></author><author><keyname>Chain</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Jolly</keyname><forenames>Clare</forenames></author></authors><title>Hybrid spreading mechanisms and T cell activation shape the dynamics of
  HIV-1 infection</title><categories>q-bio.PE cs.CE physics.bio-ph q-bio.CB</categories><journal-ref>PLOS Computational Biology. 2015 Apr 2;11(4):e1004179</journal-ref><doi>10.1371/journal.pcbi.1004179</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  HIV-1 can disseminate between susceptible cells by two mechanisms: cell-free
infection following fluid-phase diffusion of virions and by highly-efficient
direct cell-to-cell transmission at immune cell contacts. The contribution of
this hybrid spreading mechanism, which is also a characteristic of some
important computer worm outbreaks, to HIV-1 progression in vivo remains
unknown. Here we present a new mathematical model that explicitly incorporates
the ability of HIV-1 to use hybrid spreading mechanisms and evaluate the
consequences for HIV-1 pathogenenesis. The model captures the major phases of
the HIV-1 infection course of a cohort of treatment naive patients and also
accurately predicts the results of the Short Pulse Anti-Retroviral Therapy at
Seroconversion (SPARTAC) trial. Using this model we find that hybrid spreading
is critical to seed and establish infection, and that cell-to-cell spread and
increased CD4+ T cell activation are important for HIV-1 progression. Notably,
the model predicts that cell-to-cell spread becomes increasingly effective as
infection progresses and thus may present a considerable treatment barrier.
Deriving predictions of various treatments' influence on HIV-1 progression
highlights the importance of earlier intervention and suggests that treatments
effectively targeting cell-to-cell HIV-1 spread can delay progression to AIDS.
This study suggests that hybrid spreading is a fundamental feature of HIV
infection, and provides the mathematical framework incorporating this feature
with which to evaluate future therapeutic strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.08994</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.08994</id><created>2015-03-31</created><updated>2015-07-05</updated><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Robust Resource Allocation with Joint Carrier Aggregation for
  Multi-Carrier Cellular Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE. Part of this work has been uploaded to
  arXiv:1405.6448</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach for robust optimal resource
allocation with joint carrier aggregation to allocate multiple carriers
resources optimally among users with elastic and inelastic traffic in cellular
networks. We use utility proportional fairness allocation policy, where the
fairness among users is in utility percentage of the application running on the
user equipment (UE). Each UE is assigned an application utility function based
on the type of its application. Our objective is to allocate multiple carriers
resources optimally among users subscribing for mobile services. In addition,
each user is guaranteed a minimum quality of service (QoS) that varies based on
the user's application type. We present a robust algorithm that solves the
drawback in the algorithm presented in [1] by preventing the fluctuations in
the resource allocation process, in the case of scarce resources, and allocates
optimal rates for both high-traffic and low-traffic situations. Our distributed
resource allocation algorithm allocates an optimal rate to each user from all
carriers in its range while providing the minimum price for the allocated rate.
In addition, we analyze the convergence of the algorithm with different network
traffic densities and show that our algorithm provides traffic dependent
pricing for network providers. Finally, we present simulation results for the
performance of our resource allocation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09002</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09002</id><created>2015-03-31</created><authors><author><keyname>Sim</keyname><forenames>Min Soo</forenames></author><author><keyname>Park</keyname><forenames>Jeonghun</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Compressed Channel Feedback for Correlated Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is a promising approach for
cellular communication due to its energy efficiency and high achievable data
rate. These advantages, however, can be realized only when channel state
information (CSI) is available at the transmitter. Since there are many
antennas, CSI is too large to feed back without compression. To compress CSI,
prior work has applied compressive sensing (CS) techniques and the fact that
CSI can be sparsified. The adopted sparsifying bases fail, however, to reflect
the spatial correlation and channel conditions or to be feasible in practice.
In this paper, we propose a new sparsifying basis that reflects the long-term
characteristics of the channel, and needs no change as long as the spatial
correlation model does not change. We propose a new reconstruction algorithm
for CS, and also suggest dimensionality reduction as a compression method. To
feed back compressed CSI in practice, we propose a new codebook for the
compressed channel quantization assuming no other-cell interference. Numerical
results confirm that the proposed channel feedback mechanisms show better
performance in point-to-point (single-user) and point-to-multi-point
(multi-user) scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09006</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09006</id><created>2015-03-31</created><updated>2015-08-25</updated><authors><author><keyname>Aigner</keyname><forenames>Martin</forenames></author><author><keyname>Kirsch</keyname><forenames>Christoph M.</forenames></author><author><keyname>Lippautz</keyname><forenames>Michael</forenames></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames></author></authors><title>Fast, Multicore-Scalable, Low-Fragmentation Memory Allocation through
  Large Virtual Memory and Global Data Structures</title><categories>cs.PL</categories><acm-class>D.4.2; D.3.4</acm-class><doi>10.1145/2814270.2814294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that general-purpose memory allocation involving many threads
on many cores can be done with high performance, multicore scalability, and low
memory consumption. For this purpose, we have designed and implemented scalloc,
a concurrent allocator that generally performs and scales in our experiments
better than other allocators while using less memory, and is still competitive
otherwise. The main ideas behind the design of scalloc are: uniform treatment
of small and big objects through so-called virtual spans, efficiently and
effectively reclaiming free memory through fast and scalable global data
structures, and constant-time (modulo synchronization) allocation and
deallocation operations that trade off memory reuse and spatial locality
without being subject to false sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09016</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09016</id><created>2015-03-31</created><authors><author><keyname>Ivanyos</keyname><forenames>Gabor</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author></authors><title>On solving systems of diagonal polynomial equations over finite fields</title><categories>cs.CC quant-ph</categories><comments>A preliminary extended abstract of this paper will appear in
  Proceedings of FAW 2015 (Springer LNCS)</comments><msc-class>68Q25, 68W30, 68Q12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to solve a system of diagonal polynomial equations
over finite fields when the number of variables is greater than some fixed
polynomial of the number of equations whose degree depends only on the degree
of the polynomial equations. Our algorithm works in time polynomial in the
number of equations and the logarithm of the size of the field, whenever the
degree of the polynomial equations is constant. As a consequence we design
polynomial time quantum algorithms for two algebraic hidden structure problems:
for the hidden subgroup problem in certain semidirect product p-groups of
constant nilpotency class, and for the multi-dimensional univariate hidden
polynomial graph problem when the degree of the polynomials is constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09021</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09021</id><created>2015-03-31</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Roselli</keyname><forenames>Vincenzo</forenames></author></authors><title>Optimal Morphs of Convex Drawings</title><categories>cs.CG cs.DM math.CO</categories><comments>To appear in SoCG 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm to compute a morph between any two convex drawings of
the same plane graph. The morph preserves the convexity of the drawing at any
time instant and moves each vertex along a piecewise linear curve with linear
complexity. The linear bound is asymptotically optimal in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09022</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09022</id><created>2015-03-31</created><updated>2015-09-21</updated><authors><author><keyname>Read</keyname><forenames>Jesse</forenames></author><author><keyname>Hollm&#xe9;n</keyname><forenames>Jaakko</forenames></author></authors><title>Multi-label Classification using Labels as Hidden Nodes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Competitive methods for multi-label classification typically invest in
learning labels together. To do so in a beneficial way, analysis of label
dependence is often seen as a fundamental step, separate and prior to
constructing a classifier. Some methods invest up to hundreds of times more
computational effort in building dependency models, than training the final
classifier itself. We extend some recent discussion in the literature and
provide a deeper analysis, namely, developing the view that label dependence is
often introduced by an inadequate base classifier, rather than being inherent
to the data or underlying concept; showing how even an exhaustive analysis of
label dependence may not lead to an optimal classification structure. Viewing
labels as additional features (a transformation of the input), we create
neural-network inspired novel methods that remove the emphasis of a prior
dependency structure. Our methods have an important advantage particular to
multi-label data: they leverage labels to create effective units in middle
layers, rather than learning these units from scratch in an unsupervised
fashion with gradient-based methods. Results are promising. The methods we
propose perform competitively, and also have very important qualities of
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09025</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09025</id><created>2015-03-31</created><updated>2015-11-09</updated><authors><author><keyname>Arias</keyname><forenames>Marta</forenames></author><author><keyname>Balc&#xe1;zar</keyname><forenames>Jos&#xe9; L.</forenames></author><author><keyname>T&#xee;rn&#x103;uc&#x103;</keyname><forenames>Cristina</forenames></author></authors><title>Learning Definite Horn Formulas from Closure Queries</title><categories>cs.LG cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A definite Horn theory is a set of n-dimensional Boolean vectors whose
characteristic function is expressible as a definite Horn formula, that is, as
conjunction of definite Horn clauses. The class of definite Horn theories is
known to be learnable under different query learning settings, such as learning
from membership and equivalence queries or learning from entailment. We propose
yet a different type of query: the closure query. Closure queries are a natural
extension of membership queries and also a variant, appropriate in the context
of definite Horn formulas, of the so-called correction queries. We present an
algorithm that learns conjunctions of definite Horn clauses in polynomial time,
using closure and equivalence queries, and show how it relates to the canonical
Guigues-Duquenne basis for implicational systems. We also show how the
different query models mentioned relate to each other by either showing
full-fledged reductions by means of query simulation (where possible), or by
showing their connections in the context of particular algorithms that use them
for learning definite Horn formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09030</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09030</id><created>2015-03-31</created><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Cooley</keyname><forenames>Oliver</forenames></author><author><keyname>Kang</keyname><forenames>Mihyun</forenames></author><author><keyname>Skubch</keyname><forenames>Kathrin</forenames></author></authors><title>How does the core sit inside the mantle?</title><categories>math.CO cs.DM math.PR</categories><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-core, defined as the largest subgraph of minimum degree $k$, of the
random graph $G(n,p)$ has been studied extensively. In a landmark paper Pittel,
Wormald and Spencer [JCTB 67 (1996) 111--151] determined the threshold $d_k$
for the appearance of an extensive $k$-core. Here we derive a multi-type
Galton-Watson branching process that describes precisely how the $k$-core is
embedded into the random graph for any $k\geq3$ and any fixed average degree
$d=np&gt;d_k$. This generalises prior results on, e.g., the internal structure of
the $k$-core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09039</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09039</id><created>2015-03-31</created><updated>2015-06-04</updated><authors><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Operational Region of D2D Communications for Enhancing Cellular Network
  Performance</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important enabler towards the successful deployment of any new
element/feature to the cellular network is the investigation and
characterization of the operational conditions where its introduction will
enhance performance. Even though there has been significant research activity
on the potential of device-to-device (D2D) communications, there are currently
no clear indications of whether D2D communications are actually able to provide
benefits for a wide range of operational conditions, thus justifying their
introduction to the system. This paper attempts to fill this gap by taking a
stochastic geometry approach on characterizing the set (region) of operational
conditions for which D2D communications enhance performance in terms of average
user rate. For the practically interesting case of a heavy loaded network, the
operational region is provided in closed form as a function of a variety of
parameters such as maximum D2D link distances and user densities, reflecting a
wide range of operational conditions (points). It is shown that under the
appropriate deployment scheme, D2D communications can indeed be beneficial not
only for the usually considered regime of &quot;proximal communications&quot; but to a
wide range of operational conditions that include D2D link distances comparable
to the distance to the cellular access point and considerably large user
densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09052</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09052</id><created>2015-03-31</created><authors><author><keyname>Balegas</keyname><forenames>Valter</forenames></author><author><keyname>Serra</keyname><forenames>Diogo</forenames></author><author><keyname>Duarte</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Ferreira</keyname><forenames>Carla</forenames></author><author><keyname>Rodrigues</keyname><forenames>Rodrigo</forenames></author><author><keyname>Pregui&#xe7;a</keyname><forenames>Nuno</forenames></author><author><keyname>Shapiro</keyname><forenames>Marc</forenames></author><author><keyname>Najafzadeh</keyname><forenames>Mahsa</forenames></author></authors><title>Extending Eventually Consistent Cloud Databases for Enforcing Numeric
  Invariants</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geo-replicated databases often operate under the principle of eventual
consistency to offer high-availability with low latency on a simple key/value
store abstraction. Recently, some have adopted commutative data types to
provide seamless reconciliation for special purpose data types, such as
counters. Despite this, the inability to enforce numeric invariants across all
replicas still remains a key shortcoming of relying on the limited guarantees
of eventual consistency storage. We present a new replicated data type, called
bounded counter, which adds support for numeric invariants to eventually
consistent geo-replicated databases. We describe how this can be implemented on
top of existing cloud stores without modifying them, using Riak as an example.
Our approach adapts ideas from escrow transactions to devise a solution that is
decentralized, fault-tolerant and fast. Our evaluation shows much lower latency
and better scalability than the traditional approach of using strong
consistency to enforce numeric invariants, thus alleviating the tension between
consistency and availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09059</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09059</id><created>2015-03-31</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Blind Estimation of Effective Downlink Channel Gains in Massive MIMO</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the massive MIMO downlink with time-division duplex (TDD)
operation and conjugate beamforming transmission. To reliably decode the
desired signals, the users need to know the effective channel gain. In this
paper, we propose a blind channel estimation method which can be applied at the
users and which does not require any downlink pilots. We show that our proposed
scheme can substantially outperform the case where each user has only
statistical channel knowledge, and that the difference in performance is
particularly large in certain types of channel, most notably keyhole channels.
Compared to schemes that rely on downlink pilots, our proposed scheme yields
more accurate channel estimates for a wide range of signal-to-noise ratios and
avoid spending time-frequency resources on pilots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09060</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09060</id><created>2015-03-27</created><authors><author><keyname>Rojas</keyname><forenames>Raul</forenames></author></authors><title>A Tutorial Introduction to the Lambda Calculus</title><categories>cs.LO</categories><comments>4 figures</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a concise and painless introduction to the $\lambda$-calculus.
This formalism was developed by Alonzo Church as a tool for studying the
mathematical properties of effectively computable functions. The formalism
became popular and has provided a strong theoretical foundation for the family
of functional programming languages. This tutorial shows how to perform
arithmetical and logical computations using the $\lambda$-calculus and how to
define recursive functions, even though $\lambda$-calculus functions are
unnamed and thus cannot refer explicitly to themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09062</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09062</id><created>2015-03-31</created><updated>2015-04-02</updated><authors><author><keyname>Coppa</keyname><forenames>Emilio</forenames></author><author><keyname>Finocchi</keyname><forenames>Irene</forenames></author></authors><title>On data skewness, stragglers, and MapReduce progress indicators</title><categories>cs.DC cs.PF cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of predicting the performance of MapReduce
applications, designing accurate progress indicators that keep programmers
informed on the percentage of completed computation time during the execution
of a job. Through extensive experiments, we show that state-of-the-art progress
indicators (including the one provided by Hadoop) can be seriously harmed by
data skewness, load unbalancing, and straggling tasks. This is mainly due to
their implicit assumption that the running time depends linearly on the input
size. We thus design a novel profile-guided progress indicator, called
NearestFit, that operates without the linear hypothesis assumption and exploits
a careful combination of nearest neighbor regression and statistical curve
fitting techniques. Our theoretical progress model requires fine-grained
profile data, that can be very difficult to manage in practice. To overcome
this issue, we resort to computing accurate approximations for some of the
quantities used in our model through space- and time-efficient data streaming
algorithms. We implemented NearestFit on top of Hadoop 2.6.0. An extensive
empirical assessment over the Amazon EC2 platform on a variety of real-world
benchmarks shows that NearestFit is practical w.r.t. space and time overheads
and that its accuracy is generally very good, even in scenarios where
competitors incur non-negligible errors and wide prediction fluctuations.
Overall, NearestFit significantly improves the current state-of-art on progress
analysis for MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09066</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09066</id><created>2015-03-31</created><authors><author><keyname>Osman</keyname><forenames>Nardine</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author><author><keyname>Riggi</keyname><forenames>Valerio</forenames></author><author><keyname>Sierra</keyname><forenames>Carles</forenames></author></authors><title>MORE: Merged Opinions Reputation Model</title><categories>cs.MA</categories><comments>12th European Conference on Multi-Agent Systems (EUMAS 2014)</comments><journal-ref>Multi-Agent Systems Springer Lecture Notes in Computer Science,
  Vol. 8953, 2015</journal-ref><doi>10.1007/978-3-319-17130-2_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation is generally defined as the opinion of a group on an aspect of a
thing. This paper presents a reputation model that follows a probabilistic
modelling of opinions based on three main concepts: (1) the value of an opinion
decays with time, (2) the reputation of the opinion source impacts the
reliability of the opinion, and (3) the certainty of the opinion impacts its
weight with respect to other opinions. Furthermore, the model is flexible with
its opinion sources: it may use explicit opinions or implicit opinions that can
be extracted from agent behavior in domains where explicit opinions are sparse.
We illustrate the latter with an approach to extract opinions from behavioral
information in the sports domain, focusing on football in particular. One of
the uses of a reputation model is predicting behavior. We take up the challenge
of predicting the behavior of football teams in football matches, which we
argue is a very interesting yet difficult approach for evaluating the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09076</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09076</id><created>2015-03-31</created><updated>2015-10-06</updated><authors><author><keyname>Orsino</keyname><forenames>Antonino</forenames></author><author><keyname>Militano</keyname><forenames>Leonardo</forenames></author><author><keyname>Araniti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molinaro</keyname><forenames>Antonella</forenames></author><author><keyname>Iera</keyname><forenames>Antonio</forenames></author></authors><title>Efficient Data Uploading Supported by D2D Communications in LTE-A
  Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>We just realized that the submitted version is not compliant with the
  final version of the manuscript. In addition, there are also crucial error in
  the formulation of the analytical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reference scenario in this paper is a single cell in a Long Term
Evolution-Advanced (LTE-A) system, where multiple user equipments (UEs) aim at
uploading some data to a central server or to the Cloud. The traditional
uploading technique used in cellular systems, i.e., with separate links from
each UE to the eNodeB, is compared to innovative \textit{relay-based} schemes
that exploit Device-to-Device (D2D) communications between two (or more) UEs in
proximity to each other. Differences in the channel quality experienced by the
UEs offer an opportunity to develop D2D-based solutions, where \textit{(i)} the
UE with a poor direct link to the eNodeB will forward data to a nearby UE over
a high-quality D2D link; and \textit{(ii)} the receiving UE then uploads its
own generated data and the relayed data to the eNodeB over a good uplink
channel. A straightforward gain in the data uploading time can be obtained for
the first UE. To extend the benefits, also to the relaying UE, enhanced
D2D-based solutions are proposed that decrease the uploading time of this UE
based on the cooperative sharing of the resources allocated by the eNodeB to
the cooperating devices. Finally, preliminary results are also presented for a
multihop study case, where a chain of devices exploits D2D communications to
upload data to the eNodeB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09079</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09079</id><created>2015-03-29</created><authors><author><keyname>Montecinos</keyname><forenames>Gino I.</forenames></author></authors><title>Analytic solutions for the Burgers equation with source terms</title><categories>math.AP cs.NA</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analytic solutions for Burgers equations with source terms, possibly stiff,
represent an important element to assess numerical schemes. Here we present a
procedure, based on the characteristic technique to obtain analytic solutions
for these equations with smooth initial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09082</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09082</id><created>2015-03-31</created><updated>2016-01-14</updated><authors><author><keyname>Yu</keyname><forenames>Jian</forenames></author></authors><title>Generalized Categorization Axioms</title><categories>cs.LG</categories><comments>16 pages. Dimensionality reduction, density estimation, regression,
  clustering and classification are represented in a unified way, where
  unsupervised dimensionality reduction, density estimation, regression are
  considered as one category problem, clustering and classification are
  considered as multiple category problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorization axioms have been proposed to axiomatizing clustering results,
which offers a hint of bridging the difference between human recognition system
and machine learning through an intuitive observation: an object should be
assigned to its most similar category. However, categorization axioms cannot be
generalized into a general machine learning system as categorization axioms
become trivial when the number of categories becomes one. In order to
generalize categorization axioms into general cases, categorization input and
categorization output are reinterpreted by inner and outer category
representation. According to the categorization reinterpretation, two category
representation axioms are presented. Category representation axioms and
categorization axioms can be combined into a generalized categorization
axiomatic framework, which accurately delimit the theoretical categorization
constraints and overcome the shortcoming of categorization axioms. The proposed
axiomatic framework not only discuses categorization test issue but also
reinterprets many results in machine learning in a unified way, such as
dimensionality reduction,density estimation, regression, clustering and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09087</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09087</id><created>2015-03-31</created><authors><author><keyname>Disfani</keyname><forenames>Vahid. R</forenames></author><author><keyname>Miao</keyname><forenames>Zhixin</forenames></author><author><keyname>Fan</keyname><forenames>Lingling</forenames></author><author><keyname>Zeng</keyname><forenames>Bo</forenames></author></authors><title>Dual Decomposition-Based Privacy-Preserving Multi-Horizon
  Utility-Community Decision Making Paradigms</title><categories>cs.SY</categories><comments>8 pages, 15 figures, submitted to IEEE trans. Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two types of privacy-preserving decision making paradigms for
utility-community interactions for multi-horizon operation are examined in this
paper. In both designs, communities with renewable energy sources, distributed
generators, and energy storage systems minimize their costs with limited
information exchange with the utility. The utility makes decision based on the
information provided from the communities. Through an iterative process, all
parties achieve agreement. The authors' previous research results on
subgradient and lower-upper-bound switching (LUBS)-based distributed
optimization oriented multi-agent control strategies are examined and the
convergence analysis of both strategies are provided. The corresponding
decision making architectures, including information flow among agents and
learning (or iteration) procedure, are developed for multi-horizon decision
making scenarios. Numerical results illustrate the decision making procedures
and demonstrate their feasibility of practical implementation. The two decision
making architectures are compared for their implementation requirements as well
as performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09092</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09092</id><created>2015-03-31</created><updated>2015-08-27</updated><authors><author><keyname>Saptharishi</keyname><forenames>Ramprasad</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author><author><keyname>Volk</keyname><forenames>Ben Lee</forenames></author></authors><title>Efficiently decoding Reed-Muller codes from random errors</title><categories>cs.IT math.IT</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reed-Muller codes encode an $m$-variate polynomial of degree $r$ by
evaluating it on all points in $\{0,1\}^m$. We denote this code by $RM(m,r)$.
The minimal distance of $RM(m,r)$ is $2^{m-r}$ and so it cannot correct more
than half that number of errors in the worst case. For random errors one may
hope for a better result.
  In this work we give an efficient algorithm (in the block length $n=2^m$) for
decoding random errors in Reed-Muller codes far beyond the minimal distance.
Specifically, for low rate codes (of degree $r=o(\sqrt{m})$) we can correct a
random set of $(1/2-o(1))n$ errors with high probability. For high rate codes
(of degree $m-r$ for $r=o(\sqrt{m/\log m})$), we can correct roughly $m^{r/2}$
errors.
  More generally, for any integer $r$, our algorithm can correct any error
pattern in $RM(m,m-(2r+2))$ for which the same erasure pattern can be corrected
in $RM(m,m-(r+1))$. The results above are obtained by applying recent results
of Abbe, Shpilka and Wigderson (STOC, 2015), Kumar and Pfister (2015) and
Kudekar et al. (2015) regarding the ability of Reed-Muller codes to correct
random erasures.
  The algorithm is based on solving a carefully defined set of linear equations
and thus it is significantly different than other algorithms for decoding
Reed-Muller codes that are based on the recursive structure of the code. It can
be seen as a more explicit proof of a result of Abbe et al. that shows a
reduction from correcting erasures to correcting errors, and it also bares some
similarities with the famous Berlekamp-Welch algorithm for decoding
Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09097</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09097</id><created>2015-03-31</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author><author><keyname>Toneguzzo</keyname><forenames>Andrea</forenames></author></authors><title>Open Transactions on Shared Memory</title><categories>cs.PL</categories><doi>10.1007/978-3-319-19282-6_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory has arisen as a good way for solving many of the issues
of lock-based programming. However, most implementations admit isolated
transactions only, which are not adequate when we have to coordinate
communicating processes. To this end, in this paper we present OCTM, an
Haskell-like language with open transactions over shared transactional memory:
processes can join transactions at runtime just by accessing to shared
variables. Thus a transaction can co-operate with the environment through
shared variables, but if it is rolled-back, also all its effects on the
environment are retracted. For proving the expressive power of TCCS we give an
implementation of TCCS, a CCS-like calculus with open transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09105</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09105</id><created>2015-03-31</created><updated>2016-01-18</updated><authors><author><keyname>Karmakar</keyname><forenames>Prasenjit</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Two Timescale Stochastic Approximation with Controlled Markov noise and
  Off-policy temporal difference learning</title><categories>math.DS cs.AI stat.ML</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present for the first time an asymptotic convergence analysis of
two-timescale stochastic approximation driven by controlled Markov noise. In
particular, both the faster and slower recursions have non-additive Markov
noise components in addition to martingale difference noise. We analyze the
asymptotic behavior of our framework by relating it to limiting differential
inclusions in both time-scales that are defined in terms of the invariant
probability measures associated with the controlled Markov processes. Finally,
we show how to solve the off-policy convergence problem for temporal difference
learning with linear function approximation using our results and proving
stability of the iterates in this case. Moreover, in general, we emphasize the
fact that all the reinforcement learning scenarios where function approximation
of value function is deployed needs to consider Markov noise in the convergence
proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09112</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09112</id><created>2015-03-31</created><authors><author><keyname>Guo</keyname><forenames>Chuan</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>On the Combinatorics of Palindromes and Antipalindromes</title><categories>cs.FL</categories><comments>13 pages/ submitted to DLT 2015</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a number of results on the structure and enumeration of palindromes
and antipalindromes. In particular, we study conjugates of palindromes,
palindromic pairs, rich words, and the counterparts of these notions for
antipalindromes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09129</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09129</id><created>2015-03-31</created><authors><author><keyname>Gardner</keyname><forenames>Brian</forenames></author><author><keyname>Sporea</keyname><forenames>Ioana</forenames></author><author><keyname>Gr&#xfc;ning</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Encoding Spike Patterns in Multilayer Spiking Neural Networks</title><categories>cs.NE</categories><comments>31 pages, 14 figures</comments><doi>10.1162/NECO_a_00790</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information encoding in the nervous system is supported through the precise
spike-timings of neurons; however, an understanding of the underlying processes
by which such representations are formed in the first place remains unclear.
Here we examine how networks of spiking neurons can learn to encode for input
patterns using a fully temporal coding scheme. To this end, we introduce a
learning rule for spiking networks containing hidden neurons which optimizes
the likelihood of generating desired output spiking patterns. We show the
proposed learning rule allows for a large number of accurate input-output spike
pattern mappings to be learnt, which outperforms other existing learning rules
for spiking neural networks: both in the number of mappings that can be learnt
as well as the complexity of spike train encodings that can be utilised. The
learning rule is successful even in the presence of input noise, is
demonstrated to solve the linearly non-separable XOR computation and
generalizes well on an example dataset. We further present a biologically
plausible implementation of backpropagated learning in multilayer spiking
networks, and discuss the neural mechanisms that might underlie its function.
Our approach contributes both to a systematic understanding of how pattern
encodings might take place in the nervous system, and a learning rule that
displays strong technical capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09137</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09137</id><created>2015-03-31</created><updated>2015-04-28</updated><authors><author><keyname>Novacek</keyname><forenames>Vit</forenames></author></authors><title>Formalising Hypothesis Virtues in Knowledge Graphs: A General
  Theoretical Framework and its Validation in Literature-Based Discovery
  Experiments</title><categories>cs.AI</categories><comments>Pre-print of an article submitted to Artificial Intelligence Journal
  (after the manuscript has been refused by the editors of Journal of Web
  Semantics before the peer review process due to being out of scope for that
  journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to discovery informatics that uses so called
knowledge graphs as the essential representation structure. Knowledge graph is
an umbrella term that subsumes various approaches to tractable representation
of large volumes of loosely structured knowledge in a graph form. It has been
used primarily in the Web and Linked Open Data contexts, but is applicable to
any other area dealing with knowledge representation. In the perspective of our
approach motivated by the challenges of discovery informatics, knowledge graphs
correspond to hypotheses. We present a framework for formalising so called
hypothesis virtues within knowledge graphs. The framework is based on a classic
work in philosophy of science, and naturally progresses from mostly informative
foundational notions to actionable specifications of measures corresponding to
particular virtues. These measures can consequently be used to determine
refined sub-sets of knowledge graphs that have large relative potential for
making discoveries. We validate the proposed framework by experiments in
literature-based discovery. The experiments have demonstrated the utility of
our work and its superiority w.r.t. related approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09144</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09144</id><created>2015-03-31</created><authors><author><keyname>Lopes</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Cabarr&#xe3;o</keyname><forenames>Vera</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>Moniz</keyname><forenames>Helena</forenames></author><author><keyname>Trancoso</keyname><forenames>Isabel</forenames></author><author><keyname>Mata</keyname><forenames>Ana Isabel</forenames></author></authors><title>Towards Using Machine Translation Techniques to Induce Multilingual
  Lexica of Discourse Markers</title><categories>cs.CL</categories><comments>6 pages</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discourse markers are universal linguistic events subject to language
variation. Although an extensive literature has already reported language
specific traits of these events, little has been said on their cross-language
behavior and on building an inventory of multilingual lexica of discourse
markers. This work describes new methods and approaches for the description,
classification, and annotation of discourse markers in the specific domain of
the Europarl corpus. The study of discourse markers in the context of
translation is crucial due to the idiomatic nature of these structures.
Multilingual lexica together with the functional analysis of such structures
are useful tools for the hard task of translating discourse markers into
possible equivalents from one language to another. Using Daniel Marcu's
validated discourse markers for English, extracted from the Brown Corpus, our
purpose is to build multilingual lexica of discourse markers for other
languages, based on machine translation techniques. The major assumption in
this study is that the usage of a discourse marker is independent of the
language, i.e., the rhetorical function of a discourse marker in a sentence in
one language is equivalent to the rhetorical function of the same discourse
marker in another language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09156</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09156</id><created>2015-03-31</created><authors><author><keyname>Stegehuis</keyname><forenames>Clara</forenames></author><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>Predicting the long-term citation impact of recent publications</title><categories>cs.DL</categories><comments>17 pages, 17 figures</comments><doi>10.1016/j.joi.2015.06.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in citation analysis is the prediction of the long-term
citation impact of recent publications. We propose a model to predict a
probability distribution for the future number of citations of a publication.
Two predictors are used: The impact factor of the journal in which a
publication has appeared and the number of citations a publication has received
one year after its appearance. The proposed model is based on quantile
regression. We employ the model to predict the future number of citations of a
large set of publications in the field of physics. Our analysis shows that both
predictors (i.e., impact factor and early citations) contribute to the accurate
prediction of long-term citation impact. We also analytically study the
behavior of the quantile regression coefficients for high quantiles of the
distribution of citations. This is done by linking the quantile regression
approach to a quantile estimation technique from extreme value theory. Our work
provides insight into the influence of the impact factor and early citations on
the long-term citation impact of a publication, and it takes a step toward a
methodology that can be used to assess research institutions based on their
most recently published work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09163</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09163</id><created>2015-03-31</created><authors><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Kemper</keyname><forenames>Gregor</forenames></author></authors><title>Equivalence of Deterministic Top-Down Tree-to-String Transducers is
  Decidable</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that equivalence of deterministic top-down tree-to-string transducers
is decidable, thus solving a long standing open problem in formal language
theory. We also present efficient algorithms for subclasses: polynomial time
for total transducers with unary output alphabet (over a given top-down regular
domain language), and co-randomized polynomial time for linear transducers;
these results are obtained using techniques from multi-linear algebra.
  For our main result, we prove that equivalence can be certified by means of
inductive invariants using polynomial ideals. This allows us to construct two
semi-algorithms, one searching for a proof of equivalence, one for a witness of
non-equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09166</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09166</id><created>2015-03-31</created><authors><author><keyname>Rahman</keyname><forenames>Mehnaz</forenames></author><author><keyname>Choi</keyname><forenames>Gwan S.</forenames></author></authors><title>Fixed Point Realization of Iterative LR-Aided Soft MIMO Decoding
  Algorithm</title><categories>cs.IT math.IT</categories><comments>submitted to SPIJ (Signal Processing: An International
  Journal),(under review), 10 pages, 5 figures</comments><journal-ref>SPIJ (Signal Processing: An International Journal), vol. 9, issue
  2, pp. 14-24, May 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) systems have been widely acclaimed in
order to provide high data rates. Recently Lattice Reduction (LR) aided
detectors have been proposed to achieve near Maximum Likelihood (ML)
performance with low complexity. In this paper, we develop the fixed point
design of an iterative soft decision based LR-aided K-best decoder, which
reduces the complexity of existing sphere decoder. A simulation based
word-length optimization is presented for physical implementation of the K-best
decoder. Simulations show that the fixed point result of 16 bit precision can
keep bit error rate (BER) degradation within 0.3 dB for 8x8 MIMO systems with
different modulation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09168</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09168</id><created>2015-03-31</created><authors><author><keyname>Czyzowicz</keyname><forenames>Jurek</forenames><affiliation>DII</affiliation></author><author><keyname>Gasieniec</keyname><forenames>Leszek</forenames><affiliation>LIAFA, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>LIAFA, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames><affiliation>RA-CTI</affiliation></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames><affiliation>RA-CTI</affiliation></author><author><keyname>Uznanski</keyname><forenames>Przemyslaw</forenames></author></authors><title>On Convergence and Threshold Properties of Discrete Lotka-Volterra
  Population Protocols</title><categories>cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we focus on a natural class of population protocols whose
dynamics are modelled by the discrete version of Lotka-Volterra equations. In
such protocols, when an agent $a$ of type (species) $i$ interacts with an agent
$b$ of type (species) $j$ with $a$ as the initiator, then $b$'s type becomes
$i$ with probability $P\_{ij}$. In such an interaction, we think of $a$ as the
predator, $b$ as the prey, and the type of the prey is either converted to that
of the predator or stays as is. Such protocols capture the dynamics of some
opinion spreading models and generalize the well-known Rock-Paper-Scissors
discrete dynamics. We consider the pairwise interactions among agents that are
scheduled uniformly at random. We start by considering the convergence time and
show that any Lotka-Volterra-type protocol on an $n$-agent population converges
to some absorbing state in time polynomial in $n$, w.h.p., when any pair of
agents is allowed to interact. By contrast, when the interaction graph is a
star, even the Rock-Paper-Scissors protocol requires exponential time to
converge. We then study threshold effects exhibited by Lotka-Volterra-type
protocols with 3 and more species under interactions between any pair of
agents. We start by presenting a simple 4-type protocol in which the
probability difference of reaching the two possible absorbing states is
strongly amplified by the ratio of the initial populations of the two other
types, which are transient, but &quot;control&quot; convergence. We then prove that the
Rock-Paper-Scissors protocol reaches each of its three possible absorbing
states with almost equal probability, starting from any configuration
satisfying some sub-linear lower bound on the initial size of each species.
That is, Rock-Paper-Scissors is a realization of a &quot;coin-flip consensus&quot; in a
distributed system. Some of our techniques may be of independent value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09169</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09169</id><created>2015-03-31</created><authors><author><keyname>Mirbel</keyname><forenames>Isabelle</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Crescenzo</keyname><forenames>Pierre</forenames><affiliation>I3S</affiliation></author></authors><title>Improving Collaborations in Neuroscientist Community</title><categories>cs.CY</categories><comments>{\'e}galement rapport de recherche I3S/RR--2009-05--FR</comments><proxy>ccsd</proxy><journal-ref>International Journal of Web Portals, IGI Global, 2011, 1 (3),
  pp.33-49</journal-ref><doi>10.1109/WI-IAT.2009.351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our approach, called SATIS (Semantically AnnotaTed
Intentions for Services), relying on intentional process modeling and semantic
web technologies and models, to assist collaboration among the members of a
neurosciences community. The main expected result of this work is to derive and
share semantic web service specifications from a neuro-scientists point of view
in order to operationalise image analysis pipelines with web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09170</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09170</id><created>2015-03-31</created><authors><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Maximizing Energy Efficiency in Multiple Access Channels by Exploiting
  Packet Dropping and Transmitter Buffering</title><categories>cs.IT math.IT</categories><comments>in IEEE trans. Wireless communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of service (QoS) for a network is characterized in terms of various
parameters specifying packet delay and loss tolerance requirements for the
application. The unpredictable nature of the wireless channel demands for
application of certain mechanisms to meet the QoS requirements. Traditionally,
medium access control (MAC) and network layers perform these tasks. However,
these mechanisms do not take (fading) channel conditions into account. In this
paper, we investigate the problem using cross layer techniques where
information flow and joint optimization of higher and physical layer is
permitted. We propose a scheduling scheme to optimize the energy consumption of
a multiuser multi-access system such that QoS constraints in terms of packet
loss are fulfilled while the system is able to maximize the advantages emerging
from multiuser diversity. Specifically, this work focuses on modeling and
analyzing the effects of packet buffering capabilities of the transmitter on
the system energy for a packet loss tolerant application. We discuss low
complexity schemes which show comparable performance to the proposed scheme.
The numerical evaluation reveals useful insights about the coupling effects of
different QoS parameters on the system energy consumption and validates our
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.09178</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.09178</id><created>2015-03-31</created><updated>2015-09-03</updated><authors><author><keyname>Zhang</keyname><forenames>Shuowen</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Constant Envelope Precoding with Adaptive Receiver Constellation in
  Fading Channel</title><categories>cs.IT math.IT</categories><comments>This paper has been submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constant envelope (CE) precoding is an appealing transmission technique which
enables the use of highly efficient power amplifiers (PAs). For CE precoding in
a single-user multiple-input single-output (MISO) channel, a desired
constellation is feasible at the receiver if and only if it can be scaled to
lie in an annulus, whose boundaries are characterized by the instantaneous
channel realization. Therefore, if a fixed receiver constellation is used for
CE precoding in fading channel, where the annulus is time-varying, there is in
general a non-zero probability of encountering a channel that makes CE
precoding infeasible. To tackle this problem, this paper studies the adaptive
receiver constellation design for CE precoding in a single-user MISO
flat-fading channel with an arbitrary number of antennas at the transmitter. We
first investigate the fixed-rate adaptive receiver constellation design to
minimize the symbol error rate (SER). Specifically, an efficient algorithm is
proposed to find the optimal two-ring amplitude-and-phase shift keying (APSK)
constellation that is both feasible and of the maximum minimum Euclidean
distance (MED), for any given constellation size and instantaneous channel
realization. Numerical results show that by using the optimized fixed-rate
adaptive receiver constellation, our proposed scheme achieves significantly
improved SER performance than CE precoding with fixed receiver constellation.
Moreover, with the PA efficiency gain achieved by CE precoding, our proposed
scheme requires less transmitter power consumption to achieve a desired SER
level than conventional linear precoding scheme under the less-stringent
average per-antenna power constraint (PAPC). Furthermore, based on the family
of optimal fixed-rate adaptive two-ring APSK constellation sets, a
variable-rate CE transmission scheme is proposed and numerically examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00028</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00028</id><created>2015-03-31</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Shechtman</keyname><forenames>Eli</forenames></author><author><keyname>Agarwala</keyname><forenames>Aseem</forenames></author><author><keyname>Brandt</keyname><forenames>Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Real-World Font Recognition Using Deep Network and Domain Adaptation</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a challenging fine-grain classification problem: recognizing a
font style from an image of text. In this task, it is very easy to generate
lots of rendered font examples but very hard to obtain real-world labeled
images. This real-to-synthetic domain gap caused poor generalization to new
real data in previous methods (Chen et al. (2014)). In this paper, we refer to
Convolutional Neural Networks, and use an adaptation technique based on a
Stacked Convolutional Auto-Encoder that exploits unlabeled real-world images
combined with synthetic data. The proposed method achieves an accuracy of
higher than 80% (top-5) on a real-world dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00037</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00037</id><created>2015-03-31</created><authors><author><keyname>Horn</keyname><forenames>Alex</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>On partial order semantics for SAT/SMT-based symbolic encodings of weak
  memory concurrency</title><categories>cs.LO</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent systems are notoriously difficult to analyze, and technological
advances such as weak memory architectures greatly compound this problem. This
has renewed interest in partial order semantics as a theoretical foundation for
formal verification techniques. Among these, symbolic techniques have been
shown to be particularly effective at finding concurrency-related bugs because
they can leverage highly optimized decision procedures such as SAT/SMT solvers.
This paper gives new fundamental results on partial order semantics for
SAT/SMT-based symbolic encodings of weak memory concurrency. In particular, we
give the theoretical basis for a decision procedure that can handle a fragment
of concurrent programs endowed with least fixed point operators. In addition,
we show that a certain partial order semantics of relaxed sequential
consistency is equivalent to the conjunction of three extensively studied weak
memory axioms by Alglave et al. An important consequence of this equivalence is
an asymptotically smaller symbolic encoding for bounded model checking which
has only a quadratic number of partial order constraints compared to the
state-of-the-art cubic-size encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00039</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00039</id><created>2015-03-31</created><updated>2015-09-03</updated><authors><author><keyname>Soudjani</keyname><forenames>Sadegh Esmaeil Zadeh</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Quantitative Approximation of the Probability Distribution of a Markov
  Process by Formal Abstractions</title><categories>cs.LO</categories><comments>29 pages, Journal of Logical Methods in Computer Science</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:8) 2015</journal-ref><doi>10.2168/LMCS-11(3:8)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to formally abstract a Markov process evolving in
discrete time over a general state space as a finite-state Markov chain, with
the objective of precisely approximating its state probability distribution in
time, which allows for its approximate, faster computation by that of the
Markov chain. The approach is based on formal abstractions and employs an
arbitrary finite partition of the state space of the Markov process, and the
computation of average transition probabilities between partition sets. The
abstraction technique is formal, in that it comes with guarantees on the
introduced approximation that depend on the diameters of the partitions: as
such, they can be tuned at will. Further in the case of Markov processes with
unbounded state spaces, a procedure for precisely truncating the state space
within a compact set is provided, together with an error bound that depends on
the asymptotic properties of the transition kernel of the original process. The
overall abstraction algorithm, which practically hinges on piecewise constant
approximations of the density functions of the Markov process, is extended to
higher-order function approximations: these can lead to improved error bounds
and associated lower computational requirements. The approach is practically
tested to compute probabilistic invariance of the Markov process under study,
and is compared to a known alternative approach from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00041</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00041</id><created>2015-03-31</created><updated>2015-09-16</updated><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Optimality of Treating Interference as Noise: A Combinatorial
  Perspective</title><categories>cs.IT math.IT</categories><comments>A short version has been presented at IEEE International Symposium on
  Information Theory (ISIT 2015), Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For single-antenna Gaussian interference channels, we re-formulate the
problem of determining the Generalized Degrees of Freedom (GDoF) region
achievable by treating interference as Gaussian noise (TIN) derived in [3] from
a combinatorial perspective. We show that the TIN power control problem can be
cast into an assignment problem, such that the globally optimal power
allocation variables can be obtained by well-known polynomial time algorithms.
Furthermore, the expression of the TIN-Achievable GDoF region (TINA region) can
be substantially simplified with the aid of maximum weighted matchings. We also
provide conditions under which the TINA region is a convex polytope that relax
those in [3]. For these new conditions, together with a channel connectivity
(i.e., interference topology) condition, we show TIN optimality for a new class
of interference networks that is not included, nor includes, the class found in
[3].
  Building on the above insights, we consider the problem of joint link
scheduling and power control in wireless networks, which has been widely
studied as a basic physical layer mechanism for device-to-device (D2D)
communications. Inspired by the relaxed TIN channel strength condition as well
as the assignment-based power allocation, we propose a low-complexity
GDoF-based distributed link scheduling and power control mechanism (ITLinQ+)
that improves upon the ITLinQ scheme proposed in [4] and further improves over
the heuristic approach known as FlashLinQ. It is demonstrated by simulation
that ITLinQ+ provides significant average network throughput gains over both
ITLinQ and FlashLinQ, and yet still maintains the same level of implementation
complexity. More notably, the energy efficiency of the newly proposed ITLinQ+
is substantially larger than that of ITLinQ and FlashLinQ, which is desirable
for D2D networks formed by battery-powered devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00045</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00045</id><created>2015-03-31</created><authors><author><keyname>Shi</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy M.</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author></authors><title>Weakly Supervised Learning of Objects, Attributes and their Associations</title><categories>cs.CV</categories><comments>14 pages, Accepted to ECCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When humans describe images they tend to use combinations of nouns and
adjectives, corresponding to objects and their associated attributes
respectively. To generate such a description automatically, one needs to model
objects, attributes and their associations. Conventional methods require strong
annotation of object and attribute locations, making them less scalable. In
this paper, we model object-attribute associations from weakly labelled images,
such as those widely available on media sharing sites (e.g. Flickr), where only
image-level labels (either object or attributes) are given, without their
locations and associations. This is achieved by introducing a novel weakly
supervised non-parametric Bayesian model. Once learned, given a new image, our
model can describe the image, including objects, attributes and their
associations, as well as their locations and segmentation. Extensive
experiments on benchmark datasets demonstrate that our weakly supervised model
performs at par with strongly supervised models on tasks such as image
description and retrieval based on object-attribute associations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00052</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00052</id><created>2015-03-31</created><authors><author><keyname>Bax</keyname><forenames>Eric</forenames></author></authors><title>Improved Error Bounds Based on Worst Likely Assignments</title><categories>stat.ML cs.IT cs.LG math.IT math.PR</categories><comments>IJCNN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error bounds based on worst likely assignments use permutation tests to
validate classifiers. Worst likely assignments can produce effective bounds
even for data sets with 100 or fewer training examples. This paper introduces a
statistic for use in the permutation tests of worst likely assignments that
improves error bounds, especially for accurate classifiers, which are typically
the classifiers of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00057</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00057</id><created>2015-03-31</created><authors><author><keyname>Roald</keyname><forenames>Line</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Optimal Power Flow with Weighted Chance Constraints and General Policies
  for Generation Control</title><categories>math.OC cs.SY</categories><comments>7 pages, 3 figures</comments><report-no>LA-UR-15-22263</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing amount of electricity generated from renewable sources,
uncertainty in power system operation will grow. This has implications for
tools such as Optimal Power Flow (OPF), an optimization problem widely used in
power system operations and planning, which should be adjusted to account for
this uncertainty. One way to handle the uncertainty is to formulate a Chance
Constrained OPF (CC-OPF) which limits the probability of constraint violation
to a predefined value. However, existing CC-OPF formulations and solutions are
not immune to drawbacks. On one hand, they only consider affine policies for
generation control, which are not always realistic and may be sub-optimal. On
the other hand, the standard CC-OPF formulations do not distinguish between
large and small violations, although those might carry significantly different
risk. In this paper, we introduce the Weighted CC-OPF (WCC-OPF) that can handle
general control policies while preserving convexity and allowing for efficient
computation. The weighted chance constraints account for the size of violations
through a weighting function, which assigns a higher risk to a higher
overloads. We prove that the problem remains convex for any convex weighting
function, and for very general generation control policies. In a case study, we
compare the performance of the new WCC-OPF and the standard CC-OPF and
demonstrate that WCC-OPF effectively reduces the number of severe overloads.
Furthermore, we compare an affine generation control policy with a more general
policy, and show that the additional flexibility allow for a lower cost while
maintaining the same level of risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00060</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00060</id><created>2015-03-31</created><authors><author><keyname>Hamlet</keyname><forenames>Alan J.</forenames></author><author><keyname>Crane</keyname><forenames>Carl D.</forenames></author></authors><title>Joint Belief and Intent Prediction for Collision Avoidance in Autonomous
  Vehicles</title><categories>cs.RO</categories><comments>5 pages, Florida Conference on Recent Advances in Robotics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel method for allowing an autonomous ground vehicle
to predict the intent of other agents in an urban environment. This method,
termed the cognitive driving framework, models both the intent and the
potentially false beliefs of an obstacle vehicle. By modeling the relationships
between these variables as a dynamic Bayesian network, filtering can be
performed to calculate the intent of the obstacle vehicle as well as its belief
about the environment. This joint knowledge can be exploited to plan safer and
more efficient trajectories when navigating in an urban environment. Simulation
results are presented that demonstrate the ability of the proposed method to
calculate the intent of obstacle vehicles as an autonomous vehicle navigates a
road intersection such that preventative maneuvers can be taken to avoid
imminent collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00062</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00062</id><created>2015-03-31</created><updated>2015-04-08</updated><authors><author><keyname>Hong</keyname><forenames>Neil P. Chue</forenames></author><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Takeda</keyname><forenames>Kenji</forenames></author></authors><title>Top Tips to Make Your Research Irreproducible</title><categories>cs.CE cs.CY</categories><comments>2 pages, LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is an unfortunate convention of science that research should pretend to be
reproducible; our top tips will help you mitigate this fussy conventionality,
enabling you to enthusiastically showcase your irreproducible work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00064</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00064</id><created>2015-03-31</created><authors><author><keyname>Zou</keyname><forenames>James Y.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Kalai</keyname><forenames>Adam Tauman</forenames></author></authors><title>Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an unsupervised approach to efficiently discover the underlying
features in a data set via crowdsourcing. Our queries ask crowd members to
articulate a feature common to two out of three displayed examples. In addition
we also ask the crowd to provide binary labels to the remaining examples based
on the discovered features. The triples are chosen adaptively based on the
labels of the previously discovered features on the data set. In two natural
models of features, hierarchical and independent, we show that a simple
adaptive algorithm, using &quot;two-out-of-three&quot; similarity queries, recovers all
features with less labor than any nonadaptive algorithm. Experimental results
validate the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00065</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00065</id><created>2015-03-31</created><updated>2015-04-07</updated><authors><author><keyname>Koufogiannis</keyname><forenames>Fragkiskos</forenames></author><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Optimality of the Laplace Mechanism in Differential Privacy</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the highly interconnected realm of Internet of Things, exchange of
sensitive information raises severe privacy concerns. The Laplace mechanism --
adding Laplace-distributed artificial noise to sensitive data -- is one of the
widely used methods of providing privacy guarantees within the framework of
differential privacy. In this work, we present Lipschitz privacy, a slightly
tighter version of differential privacy. We prove that the Laplace mechanism is
optimal in the sense that it minimizes the mean-squared error for identity
queries which provide privacy with respect to the $\ell_{1}$-norm. In addition
to the $\ell_{1}$-norm which respects individuals' participation, we focus on
the use of the $\ell_{2}$-norm which provides privacy of high-dimensional data.
A variation of the Laplace mechanism is proven to have the optimal mean-squared
error from the identity query. Finally, the optimal mechanism for the scenario
in which individuals submit their high-dimensional sensitive data is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00082</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00082</id><created>2015-03-31</created><updated>2015-04-15</updated><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>A Unified Scheme for Two-Receiver Broadcast Channels with Receiver
  Message Side Information</title><categories>cs.IT math.IT</categories><comments>accepted and to be presented at the 2015 IEEE International Symposium
  on Information Theory (ISIT 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity regions of two-receiver broadcast
channels where each receiver (i) has both common and private-message requests,
and (ii) knows part of the private message requested by the other receiver as
side information. We first propose a transmission scheme and derive an inner
bound for the two-receiver memoryless broadcast channel. We next prove that
this inner bound is tight for the deterministic channel and the more capable
channel, thereby establishing their capacity regions. We show that this inner
bound is also tight for all classes of two-receiver broadcast channels whose
capacity regions were known prior to this work. Our proposed scheme is
consequently a unified capacity-achieving scheme for these classes of broadcast
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00083</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00083</id><created>2015-03-31</created><authors><author><keyname>van Rooyen</keyname><forenames>Brendan</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>A Theory of Feature Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature Learning aims to extract relevant information contained in data sets
in an automated fashion. It is driving force behind the current deep learning
trend, a set of methods that have had widespread empirical success. What is
lacking is a theoretical understanding of different feature learning schemes.
This work provides a theoretical framework for feature learning and then
characterizes when features can be learnt in an unsupervised fashion. We also
provide means to judge the quality of features via rate-distortion theory and
its generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00086</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00086</id><created>2015-03-31</created><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author></authors><title>A remark on weaken restricted isometry property in compressed sensing</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The restricted isometry property (RIP) has become well-known in the
compressed sensing community. Recently, a weaken version of RIP was proposed
for exact sparse recovery under weak moment assumptions. In this note, we prove
that the weaken RIP is also sufficient for \textsl{stable and robust} sparse
recovery by linking it with a recently introduced robust width property in
compressed sensing. Moreover, we show that it can be widely apply to other
compressed sensing instances as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00087</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00087</id><created>2015-03-31</created><authors><author><keyname>Saab</keyname><forenames>Rayan</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Quantization of compressive samples with stable and robust recovery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the quantization stage that is implicit in any
compressed sensing signal acquisition paradigm. We propose using Sigma-Delta
quantization and a subsequent reconstruction scheme based on convex
optimization. We prove that the reconstruction error due to quantization decays
polynomially in the number of measurements. Our results apply to arbitrary
signals, including compressible ones, and account for measurement noise.
Additionally, they hold for sub-Gaussian (including Gaussian and Bernoulli)
random compressed sensing measurements, as well as for both high bit-depth and
coarse quantizers, and they extend to 1-bit quantization. In the noise-free
case, when the signal is strictly sparse we prove that by optimizing the order
of the quantization scheme one can obtain root-exponential decay in the
reconstruction error due to quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00088</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00088</id><created>2015-03-31</created><updated>2015-06-03</updated><authors><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>State Estimation for the Individual and the Population in Mean Field
  Control with Application to Demand Dispatch</title><categories>cs.SY math.OC</categories><comments>Submitted to IEEE Trans. Auto. Control. Preliminary version submitted
  to the 54rd IEEE Conference on Decision and Control</comments><msc-class>93E20, 62M05, 93E10, 68W15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns state estimation problems in a mean field control
setting. In a finite population model, the goal is to estimate the joint
distribution of the population state and the state of a typical individual. The
observation equations are a noisy measurement of the population.
  The general results are applied to demand dispatch for regulation of the
power grid, based on randomized local control algorithms. In prior work by the
authors it has been shown that local control can be carefully designed so that
the aggregate of loads behaves as a controllable resource with accuracy
matching or exceeding traditional sources of frequency regulation. The
operational cost is nearly zero in many cases.
  The information exchange between grid and load is minimal, but it is assumed
in the overall control architecture that the aggregate power consumption of
loads is available to the grid operator. It is shown that the Kalman filter can
be constructed to reduce these communication requirements,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00091</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00091</id><created>2015-03-31</created><updated>2015-07-04</updated><authors><author><keyname>van Rooyen</keyname><forenames>Brendan</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>Learning in the Presence of Corruption</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In supervised learning one wishes to identify a pattern present in a joint
distribution $P$, of instances, label pairs, by providing a function $f$ from
instances to labels that has low risk $\mathbb{E}_{P}\ell(y,f(x))$. To do so,
the learner is given access to $n$ iid samples drawn from $P$. In many real
world problems clean samples are not available. Rather, the learner is given
access to samples from a corrupted distribution $\tilde{P}$ from which to
learn, while the goal of predicting the clean pattern remains. There are many
different types of corruption one can consider, and as of yet there is no
general means to compare the relative ease of learning under these different
corruption processes. In this paper we develop a general framework for tackling
such problems as well as introducing upper and lower bounds on the risk for
learning in the presence of corruption. Our ultimate goal is to be able to make
informed economic decisions in regards to the acquisition of data sets. For a
certain subclass of corruption processes (those that are
\emph{reconstructible}) we achieve this goal in a particular sense. Our lower
bounds are in terms of the coefficient of ergodicity, a simple to calculate
property of stochastic matrices. Our upper bounds proceed via a generalization
of the method of unbiased estimators appearing in recent work of Natarajan et
al and implicit in the earlier work of Kearns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00097</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00097</id><created>2015-03-31</created><authors><author><keyname>Yueh</keyname><forenames>Mei-Heng</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng David</forenames></author><author><keyname>Lin</keyname><forenames>Wen-Wei</forenames></author><author><keyname>Wu</keyname><forenames>Chin-Tien</forenames></author><author><keyname>Yau</keyname><forenames>Shing-Tung</forenames></author></authors><title>Conformal Surface Morphing with Applications on Facial Expressions</title><categories>cs.GR cs.CG</categories><comments>8 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphing is the process of changing one figure into another. Some numerical
methods of 3D surface morphing by deformable modeling and conformal mapping are
shown in this study. It is well known that there exists a unique Riemann
conformal mapping from a simply connected surface into a unit disk by the
Riemann mapping theorem. The dilation and relative orientations of the 3D
surfaces can be linked through the M\&quot;obius transformation due to the conformal
characteristic of the Riemann mapping. On the other hand, a 3D surface
deformable model can be built via various approaches such as mutual
parameterization from direct interpolation or surface matching using landmarks.
In this paper, we take the advantage of the unique representation of 3D
surfaces by the mean curvatures and the conformal factors associated with the
Riemann mapping. By registering the landmarks on the conformal parametric
domains, the correspondence of the mean curvatures and the conformal factors
for each surfaces can be obtained. As a result, we can construct the 3D
deformation field from the surface reconstruction algorithm proposed by Gu and
Yau. Furthermore, by composition of the M\&quot;obius transformation and the 3D
deformation field, the morphing sequence can be generated from the mean
curvatures and the conformal factors on a unified mesh structure by using the
cubic spline homotopy. Several numerical experiments of the face morphing are
presented to demonstrate the robustness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00110</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00110</id><created>2015-04-01</created><authors><author><keyname>Lowd</keyname><forenames>Daniel</forenames></author><author><keyname>Rooshenas</keyname><forenames>Amirmohammad</forenames></author></authors><title>The Libra Toolkit for Probabilistic Models</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Libra Toolkit is a collection of algorithms for learning and inference
with discrete probabilistic models, including Bayesian networks, Markov
networks, dependency networks, and sum-product networks. Compared to other
toolkits, Libra places a greater emphasis on learning the structure of
tractable models in which exact inference is efficient. It also includes a
variety of algorithms for learning graphical models in which inference is
potentially intractable, and for performing exact and approximate inference.
Libra is released under a 2-clause BSD license to encourage broad use in
academia and industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00126</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00126</id><created>2015-04-01</created><updated>2015-06-23</updated><authors><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Conjugate-Root Offset-QAM for Orthogonal Multicarrier Transmission</title><categories>cs.IT math.IT</categories><comments>4pages, revised version submitted to IEEE WCL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current implementations of OFDM/OQAM are restricted to band-limited symmetric
filters. To circumvent this, non-symmetric conjugate root (CR) filters are
proposed for OQAM modulation. The system is applied to Generalized Frequency
Division Multiplexing (GFDM) and a method for achieving transmit diversity with
OQAM modulation is presented. The proposal reduces implementation complexity
compared to existing works and provides a more regular phase space.
GFDM/CR-OQAM outperforms conventional GFDM in terms of symbol error rate in
fading multipath channels and provides a more localized spectrum compared to
conventional OQAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00134</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00134</id><created>2015-04-01</created><authors><author><keyname>Brian</keyname><forenames>Will</forenames></author><author><keyname>Mislove</keyname><forenames>Michael</forenames></author></authors><title>From Haar to Lebesgue via Domain Theory, Revised version</title><categories>math.FA cs.LO math.GR</categories><comments>This is a revised version of an earlier paper. The original claimed
  that all Haar measures on C are the same, which is not true. This version
  corrects that error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If ${\mathcal C}\simeq 2^{\mathbb N}$ denotes the Cantor set realized as the
infinite product of two-point groups, then a folklore result says the Cantor
map from ${\mathcal C}$ into $[0,1]$ sends Haar measure to Lebesgue measure on
the interval. In fact, ${\mathcal C}$ admits many distinct topological group
structures. In this note, we show that the Haar measures induced by these
distinct group structures are share this property. We prove this by showing
that Haar measure for any group structure is the same as Haar measure induced
by a related abelian group structure. Moreover, each abelian group structure on
${\mathcal C}$ supports a natural total order that determines a map onto the
unit interval that is monotone, and hence sends intervals in ${\mathcal C}$ to
subintervals of the unit interval. Using techniques from domain theory, we show
this implies this map sends Haar measure on ${\mathcal C}$ to Lebesgue measure
on the interval, and we then use this to contract a Borel isomorphism between
any two group structures on ${\mathcal C}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00136</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00136</id><created>2015-04-01</created><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author></authors><title>Knowledge reduction of dynamic covering decision information systems
  with immigration of more objects</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical situations, it is of interest to investigate computing
approximations of sets as an important step of knowledge reduction of dynamic
covering decision information systems. In this paper, we present incremental
approaches to computing the type-1 and type-2 characteristic matrices of
dynamic coverings whose cardinalities increase with immigration of more
objects. We also present the incremental algorithms of computing the second and
sixth lower and upper approximations of sets in dynamic covering approximation
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00143</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00143</id><created>2015-04-01</created><authors><author><keyname>Aistleitner</keyname><forenames>Christoph</forenames></author></authors><title>Fully explicit large deviation inequalities for empirical processes with
  applications to information-based complexity</title><categories>math.PR cs.CC cs.IT math.IT math.NA</categories><comments>10 pages</comments><msc-class>60F10, 62G30, 65D32, 65C05, 52C17, 11K38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we obtain fully explicit large deviation inequalities
for empirical processes indexed by a Vapnik--Chervonenkis class of sets (or
functions). Furthermore we illustrate the importance of such results for the
theory of information-based complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00150</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00150</id><created>2015-04-01</created><authors><author><keyname>Peng</keyname><forenames>Feifei</forenames></author><author><keyname>Chen</keyname><forenames>Haiming</forenames></author></authors><title>Discovering Restricted Regular Expressions with Interleaving</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering a concise schema from given XML documents is an important problem
in XML applications. In this paper, we focus on the problem of learning an
unordered schema from a given set of XML examples, which is actually a problem
of learning a restricted regular expression with interleaving using positive
example strings. Schemas with interleaving could present meaningful knowledge
that cannot be disclosed by previous inference techniques. Moreover, inference
of the minimal schema with interleaving is challenging. The problem of finding
a minimal schema with interleaving is shown to be NP-hard. Therefore, we
develop an approximation algorithm and a heuristic solution to tackle the
problem using techniques different from known inference algorithms. We do
experiments on real-world data sets to demonstrate the effectiveness of our
approaches. Our heuristic algorithm is shown to produce results that are very
close to optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00151</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00151</id><created>2015-04-01</created><authors><author><keyname>Yabe</keyname><forenames>Akihiro</forenames></author></authors><title>Bi-polynomial rank and determinantal complexity</title><categories>cs.CC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The permanent vs. determinant problem is one of the most important problems
in theoretical computer science, and is the main target of geometric complexity
theory proposed by Mulmuley and Sohoni. The current best lower bound for the
determinantal complexity of the d by d permanent polynomial is d^2/2, due to
Mignon and Ressayre in 2004. Inspired by their proof method, we introduce a
natural rank concept of polynomials, called the bi-polynomial rank. The
bi-polynomial rank is related to width of an arithmetic branching program. The
bi-polynomial rank of a homogeneous polynomial p of even degree 2k is defined
as the minimum n such that p can be written as a summation of n products of
polynomials of degree k. We prove that the bi-polynomial rank gives a lower
bound of the determinantal complexity. As a consequence, the above Mignon and
Ressayre bound is improved to (d-1)^2 + 1 over the field of reals. We show that
the computation of the bi-polynomial rank is formulated as a rank minimization
problem. Applying the concave minimization technique, we reduce the problem of
lower-bounding determinantal complexity to that of proving the positive
semidefiniteness of matrices, and this is a new approach for the permanent vs.
determinant problem. We propose a computational approach for giving a lower
bound of this rank minimization, via techniques of the concave minimization.
This also yields a new strategy to attack the permanent vs. determinant
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00154</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00154</id><created>2015-04-01</created><authors><author><keyname>Fan</keyname><forenames>Zhun</forenames></author><author><keyname>Li</keyname><forenames>Wenji</forenames></author><author><keyname>Cai</keyname><forenames>Xinye</forenames></author><author><keyname>Lin</keyname><forenames>Huibiao</forenames></author><author><keyname>Xie</keyname><forenames>Shuxiang</forenames></author><author><keyname>Goodman</keyname><forenames>Erik</forenames></author></authors><title>A New Repair Operator for Multi-objective Evolutionary Algorithm in
  Constrained Optimization Problems</title><categories>cs.NE</categories><comments>8 pages</comments><msc-class>68Q01</msc-class><acm-class>G.1.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we design a set of multi-objective constrained optimization
problems (MCOPs) and propose a new repair operator to address them. The
proposed repair operator is used to fix the solutions that violate the box
constraints. More specifically, it employs a reversed correction strategy that
can effectively avoid the population falling into local optimum. In addition,
we integrate the proposed repair operator into two classical multi-objective
evolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is
compared with other two kinds of commonly used repair operators on benchmark
problems CTPs and MCOPs. The experiment results demonstrate that our proposed
approach is very effective in terms of convergence and diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00156</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00156</id><created>2015-04-01</created><updated>2015-11-23</updated><authors><author><keyname>Wollstadt</keyname><forenames>Patricia</forenames></author><author><keyname>Meyer</keyname><forenames>Ulrich</forenames></author><author><keyname>Wibral</keyname><forenames>Michael</forenames></author></authors><title>A Graph Algorithmic Approach to Separate Direct from Indirect Neural
  Interactions</title><categories>cs.IT math.IT q-bio.NC</categories><comments>24 pages, 8 figures, published in PLOS One</comments><acm-class>F.2.2; G.2.2; G.4; H.1.1</acm-class><journal-ref>PLoS ONE 10(10): e0140530 (2015)</journal-ref><doi>10.1371/journal.pone.0140530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network graphs have become a popular tool to represent complex systems
composed of many interacting subunits; especially in neuroscience, network
graphs are increasingly used to represent and analyze functional interactions
between neural sources. Interactions are often reconstructed using pairwise
bivariate analyses, overlooking their multivariate nature: it is neglected that
investigating the effect of one source on a target necessitates to take all
other sources as potential nuisance variables into account; also combinations
of sources may act jointly on a given target. Bivariate analyses produce
networks that may contain spurious interactions, which reduce the
interpretability of the network and its graph metrics. A truly multivariate
reconstruction, however, is computationally intractable due to combinatorial
explosion in the number of potential interactions. Thus, we have to resort to
approximative methods to handle the intractability of multivariate interaction
reconstruction, and thereby enable the use of networks in neuroscience. Here,
we suggest such an approximative approach in the form of an algorithm that
extends fast bivariate interaction reconstruction by identifying potentially
spurious interactions post-hoc: the algorithm flags potentially spurious edges,
which may then be pruned from the network. This produces a statistically
conservative network approximation that is guaranteed to contain non-spurious
interactions only. We describe the algorithm and present a reference
implementation to test its performance. We discuss the algorithm in relation to
other approximative multivariate methods and highlight suitable application
scenarios. Our approach is a tractable and data-efficient way of reconstructing
approximative networks of multivariate interactions. It is preferable if
available data are limited or if fully multivariate approaches are
computationally infeasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00169</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00169</id><created>2015-04-01</created><updated>2015-04-28</updated><authors><author><keyname>Castillo-Ramirez</keyname><forenames>Alonso</forenames></author><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Universal Simulation of Automata Networks</title><categories>cs.FL cs.CC cs.DM math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a finite set and $n \geq 2$. This paper introduces the concept of
universal simulation in the context of semigroups of transformations of $A^n$,
also known as finite state-homogeneous automata networks. For $m \geq n$, a
transformation of $A^m$ is defined as $n$-universal of size $m$ if it may
simulate every transformation of $A^n$ by updating one coordinate (or register)
at a time. Using tools from memoryless computation, it is established that
there is no $n$-universal transformation of size $n$, but there is such a
transformation of size $n+2$. An $n$-universal transformation is defined as
complete if it may sequentially simulate every finite sequence of
transformations of $A^n$; in this case, minimal examples and bounds for the
size and time of simulation are determined. It is also shown that there is no
$n$-universal transformation that updates all the registers in parallel, but
that there exists a complete one that updates all but one register in parallel.
This illustrates the strengths and weaknesses of parallel models of
computation, such as cellular automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00182</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00182</id><created>2015-04-01</created><updated>2015-07-12</updated><authors><author><keyname>Pumpluen</keyname><forenames>Susanne</forenames></author><author><keyname>Steele</keyname><forenames>Andrew</forenames></author></authors><title>The nonassociative algebras used to build fast-decodable space-time
  block codes</title><categories>cs.IT math.IT</categories><comments>Final version, to appear in Advances in Mathematics of
  Communications. Contains updated contact details for second author</comments><msc-class>17A35, 94B05</msc-class><doi>10.3934/amc.2015.9.449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $K/F$ and $K/L$ be two cyclic Galois field extensions and
$D=(K/F,\sigma,c)$ a cyclic algebra. Given an invertible element $d\in D$, we
present three families of unital nonassociative algebras over $L\cap F$ defined
on the direct sum of $n$ copies of $D$. Two of these families appear either
explicitly or implicitly in the designs of fast-decodable space-time block
codes in papers by Srinath, Rajan, Markin, Oggier, and the authors. We present
conditions for the algebras to be division and propose a construction for fully
diverse fast decodable space-time block codes of rate-$m$ for $nm$ transmit and
$m$ receive antennas. We present a DMT-optimal rate-3 code for 6 transmit and 3
receive antennas which is fast-decodable, with ML-decoding complexity at most
$\mathcal{O}(M^{15})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00190</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00190</id><created>2015-04-01</created><authors><author><keyname>Pumpluen</keyname><forenames>Susanne</forenames></author></authors><title>A note on linear codes and nonassociative algebras obtained from
  skew-polynomial rings</title><categories>cs.IT math.IT math.RA</categories><comments>6 pages</comments><msc-class>94B05, 94B15, 16S36, 17A35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different approaches to construct linear codes using skew polynomials can be
unified by using the nonassociative algebras built from skew-polynomial rings
by Petit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00191</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00191</id><created>2015-04-01</created><authors><author><keyname>Roul</keyname><forenames>Rajendra Kumar</forenames></author><author><keyname>Asthana</keyname><forenames>Shubham Rohan</forenames></author><author><keyname>Sahay</keyname><forenames>Sanjay Kumar</forenames></author></authors><title>Automated Document Indexing via Intelligent Hierarchical Clustering: A
  Novel Approach</title><categories>cs.IR</categories><comments>6 Pages, 3 Figures. IEEE Xplore, ICHPCA-2014</comments><doi>10.1109/ICHPCA.2014.7045347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rising quantity of textual data available in electronic format, the
need to organize it become a highly challenging task. In the present paper, we
explore a document organization framework that exploits an intelligent
hierarchical clustering algorithm to generate an index over a set of documents.
The framework has been designed to be scalable and accurate even with large
corpora. The advantage of the proposed algorithm lies in the need for minimal
inputs, with much of the hierarchy attributes being decided in an automated
manner using statistical methods. The use of topic modeling in a pre-processing
stage ensures robustness to a range of variations in the input data. For
experimental work 20-Newsgroups dataset has been used. The F- measure of the
proposed approach has been compared with the traditional K-Means and K-Medoids
clustering algorithms. Test results demonstrate the applicability, efficiency
and effectiveness of our proposed approach. After extensive experimentation, we
conclude that the framework shows promise for further research and specialized
commercial applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00198</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00198</id><created>2015-04-01</created><authors><author><keyname>Gretz</keyname><forenames>Friedrich</forenames></author><author><keyname>Jansen</keyname><forenames>Nils</forenames></author><author><keyname>Kaminski</keyname><forenames>Benjamin Lucien</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Olmedo</keyname><forenames>Federico</forenames></author></authors><title>Conditioning in Probabilistic Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the semantic intricacies of conditioning, a main feature in
probabilistic programming. We provide a weakest (liberal) pre-condition (w(l)p)
semantics for the elementary probabilistic programming language pGCL extended
with conditioning. We prove that quantitative weakest (liberal) pre-conditions
coincide with conditional (liberal) expected rewards in Markov chains and show
that semantically conditioning is a truly conservative extension. We present
two program transformations which entirely eliminate conditioning from any
program and prove their correctness using the w(l)p-semantics. Finally, we show
how the w(l)p-semantics can be used to determine conditional probabilities in a
parametric anonymity protocol and show that an inductive w(l)p-semantics for
conditioning in non-deterministic probabilistic programs cannot exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00203</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00203</id><created>2015-04-01</created><authors><author><keyname>Steinwandt</keyname><forenames>Jens</forenames></author><author><keyname>Roemer</keyname><forenames>Florian</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author><author><keyname>Del Galdo</keyname><forenames>Giovanni</forenames></author></authors><title>Deterministic Cramer-Rao bound for strictly non-circular sources and
  analytical analysis of the achievable gains</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing, 13 pages, 4
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several high-resolution parameter estimation algorithms have been
developed to exploit the structure of strictly second-order (SO) non-circular
(NC) signals. They achieve a higher estimation accuracy and can resolve up to
twice as many signal sources compared to the traditional methods for arbitrary
signals. In this paper, as a benchmark for these NC methods, we derive the
closed-form deterministic R-D NC Cramer-Rao bound (NC CRB) for the
multi-dimensional parameter estimation of strictly non-circular (rectilinear)
signal sources. Assuming a separable centro-symmetric R-D array, we show that
in some special cases, the deterministic R-D NC CRB reduces to the existing
deterministic R-D CRB for arbitrary signals. This suggests that no gain from
strictly non-circular sources (NC gain) can be achieved in these cases. For
more general scenarios, finding an analytical expression of the NC gain for an
arbitrary number of sources is very challenging. Thus, in this paper, we
simplify the derived NC CRB and the existing CRB for the special case of two
closely-spaced strictly non-circular sources captured by a uniform linear array
(ULA). Subsequently, we use these simplified CRB expressions to analytically
compute the maximum achievable asymptotic NC gain for the considered two source
case. The resulting expression only depends on the various physical parameters
and we find the conditions that provide the largest NC gain for two sources.
Our analysis is supported by extensive simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00204</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00204</id><created>2015-04-01</created><authors><author><keyname>Horn</keyname><forenames>Alex</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>Faster linearizability checking via $P$-compositionality</title><categories>cs.DC</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability is a well-established consistency and correctness criterion
for concurrent data types. An important feature of linearizability is Herlihy
and Wing's locality principle, which says that a concurrent system is
linearizable if and only if all of its constituent parts (so-called objects)
are linearizable. This paper presents $P$-compositionality, which generalizes
the idea behind the locality principle to operations on the same concurrent
data type. We implement $P$-compositionality in a novel linearizability
checker. Our experiments with over nine implementations of concurrent sets,
including Intel's TBB library, show that our linearizability checker is one
order of magnitude faster and/or more space efficient than the state-of-the-art
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00215</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00215</id><created>2015-04-01</created><authors><author><keyname>Zhang</keyname><forenames>Zhi-Hua</forenames></author><author><keyname>Shu</keyname><forenames>Lan</forenames></author><author><keyname>Zheng</keyname><forenames>Jun</forenames></author></authors><title>Controlled Remote State Preparation via General Pure Three-Qubit State</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The protocols for controlled remote state preparation of a single qubit and a
general two-qubit state are presented in this paper. The general pure
three-qubit states are chosen as shared quantum channel, which are not LOCC
equivalent to the mostly used GHZ-state. It is the first time to introduce
general pure three-qubit states to complete remote state preparation. The
probability of successful preparation is presented. Moreover, in some special
cases, the successful probability could reach unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00221</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00221</id><created>2015-04-01</created><authors><author><keyname>Malinowski</keyname><forenames>M. J.</forenames></author><author><keyname>Matsinos</keyname><forenames>E.</forenames></author></authors><title>Comparative study of the two versions of the Microsoft Kinect$^{\rm TM}$
  sensor in regard to the analysis of human motion</title><categories>physics.med-ph cs.RO</categories><comments>18 pages, 1 table, 6 figures. arXiv admin note: substantial text
  overlap with arXiv:1412.2032</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper is part of a broader programme, exploring the possibility
of involving the Microsoft Kinect$^{\rm TM}$ sensor in the analysis of human
motion. In this study, the output obtained from the two available versions of
this sensor is critically examined. We demonstrate that the two outputs differ
in regard to the variation of the physical quantities involved in the modelling
of the human motion. As the original sensor has been found unsuitable for
applications requiring high precision, the observed differences in the output
of the two sensors call for the validation of the upgraded sensor on the basis
of a marker-based system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00222</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00222</id><created>2015-04-01</created><authors><author><keyname>Kumar</keyname><forenames>S.</forenames></author><author><keyname>Pivaro</keyname><forenames>G. F.</forenames></author><author><keyname>Fraidenraich</keyname><forenames>G.</forenames></author><author><keyname>Dias</keyname><forenames>C. F.</forenames></author></authors><title>On the Exact and Approximate Eigenvalue Distribution for Sum of Wishart
  Matrices</title><categories>cs.IT math.IT</categories><comments>18 pages, 8 figures, 1 table</comments><msc-class>94A15, 94A17, 15A18, 15B52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum of Wishart matrices has an important role in multiuser communication
employing multiantenna elements, such as multiple-input multiple-output (MIMO)
multiple access channel (MAC), MIMO Relay channel, and other multiuser channels
where the mathematical model is best described using random matrices. In this
paper, the distribution of linear combination of complex Wishart distributed
matrices has been studied. We present a new closed form expression for the
marginal distribution of the eigenvalues of a weighted sum of K complex central
Wishart matrices having covariance matrices proportional to the identity
matrix. The expression is general and allows for any set of linear
coefficients. As an application example, we have used the marginal distribution
expression to obtain the ergodic sum-rate capacity for the MIMO-MAC network,
and the cut-set upper bound for the MIMO-Relay case, both as closed form
expressions. We also present a very simple expression to approximate the sum of
Wishart matrices by one equivalent Wishart matrix. All of our results are
validated by means of Monte Carlo simulations. As expected, the agreement
between the exact eigenvalue distribution and simulations is perfect, whereas
for the approximate solution the difference is indistinguishable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00229</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00229</id><created>2015-04-01</created><authors><author><keyname>Dayan</keyname><forenames>Niv</forenames></author><author><keyname>Bouganim</keyname><forenames>Luc</forenames></author><author><keyname>Bonnet</keyname><forenames>Philippe</forenames></author></authors><title>Modelling and Managing SSD Write-amplification</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How stable is the performance of your flash-based Solid State Drives (SSDs)?
This question is central for database designers and administrators, cloud
service providers, and SSD constructors. The answer depends on
write-amplification, i.e., garbage collection overhead. More specifically, the
answer depends on how write-amplification evolves in time.
  How then can one model and manage write-amplification, especially when
application workloads change? This is the focus of this paper. Managing
write-amplification boils down to managing the surplus physical space, called
over-provisioned space. Modern SSDs essentially separate the physical space
into several partitions, based on the update frequency of the pages they
contain, and divide the over-provisioned space among the groups so as to
minimize write-amplification. We introduce Wolf, a block manager that allocates
over-provisioned space to SSD partitions using a near-optimal closed-form
expression, based on the sizes and update frequencies of groups of pages. Our
evaluation shows that Wolf is robust to workloads change, with an improvement
factor of 2 with respect to the state-of-the-art. We also show that Wolf
performs comparably and even slightly better than the state of the art with
stable workloads (over 20% improvement with a TPC-C workload).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00231</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00231</id><created>2015-04-01</created><authors><author><keyname>Petra</keyname><forenames>Stefania</forenames></author><author><keyname>Popa</keyname><forenames>Constantin</forenames></author></authors><title>Single Projection Kaczmarz Extended Algorithms</title><categories>math.NA cs.DS</categories><comments>14 pages</comments><msc-class>65F10, 65F20, 90C06, 90C25</msc-class><acm-class>G.1.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To find the least squares solution of a very large and inconsistent system of
equations, one can employ the extended Kaczmarz algorithm. This method
simultaneously removes the error term, such that a consistent system is
asymptotically obtained, and applies Kaczmarz iterations for the current
approximation of this system. For random corrections of the right hand side and
Kaczmarz updates selected at random, convergence to the least squares solution
has been shown. We consider the deterministic control strategies, and show
convergence to a least squares solution when row and column updates are chosen
according to the almost-cyclic or maximal-residual choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00233</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00233</id><created>2015-04-01</created><updated>2015-10-18</updated><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>Quantum Information Processing with Finite Resources - Mathematical
  Foundations</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>135 pages, partly based on arXiv:1203.2142, v3: minor fixes,
  published version, SpringerBriefs in Mathematical Physics (2016)</comments><doi>10.1007/978-3-319-21891-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the predominant challenges when engineering future quantum information
processors is that large quantum systems are notoriously hard to maintain and
control accurately. It is therefore of immediate practical relevance to
investigate quantum information processing with limited physical resources, for
example to ask: How well can we perform information processing tasks if we only
have access to a small quantum device? Can we beat fundamental limits imposed
on information processing with classical resources? This book will introduce
the reader to the mathematical framework required to answer such questions.
  A strong emphasis is given to information measures that are essential for the
study of devices of finite size, including R\'enyi entropies and smooth
entropies. The presentation is self-contained and includes rigorous and concise
proofs of the most important properties of these measures. The first chapters
will introduce the formalism of quantum mechanics, with particular emphasis on
norms and metrics for quantum states. This is necessary to explore quantum
generalizations of R\'enyi divergence and conditional entropy, information
measures that lie at the core of information theory. The smooth entropy
framework is discussed next and provides a natural means to lift many arguments
from information theory to the quantum setting. Finally selected applications
of the theory to statistics and cryptography are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00234</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00234</id><created>2015-04-01</created><authors><author><keyname>Senning</keyname><forenames>Christian</forenames></author><author><keyname>Karakonstantis</keyname><forenames>Georgios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Cross-layer Energy-Efficiency Optimization of Packet Based Wireless MIMO
  Communication Systems</title><categories>cs.NI</categories><comments>Accepted for publication in The Journal of Signal Processing Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy in today's short-range wireless communication is mostly spent on the
analog- and digital hardware rather than on radiated power. Hence, purely
information-theoretic considerations fail to achieve the lowest energy per
information bit and the optimization process must carefully consider the
overall transceiver. In this paper, we propose to perform cross-layer
optimization, based on an energy-aware rate adaptation scheme combined with a
physical layer that is able to properly adjust its processing effort to the
data rate and the channel conditions to minimize the energy consumption per
information bit. This energy proportional behavior is enabled by extending the
classical system modes with additional configuration parameters at the various
layers. Fine grained models of the power consumption of the hardware are
developed to provide awareness of the physical layer capabilities to the medium
access control layer. The joint application of the proposed energy-aware rate
adaptation and modifications to the physical layer of an IEEE 802.11n system,
improves energy-efficiency (averaged over many noise and channel realizations)
in all considered scenarios by up to 44%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00241</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00241</id><created>2015-04-01</created><updated>2015-09-05</updated><authors><author><keyname>Costa</keyname><forenames>Eduardo Chinelate</forenames></author><author><keyname>Vieira</keyname><forenames>Alex Borges</forenames></author><author><keyname>Wehmuth</keyname><forenames>Klaus</forenames></author><author><keyname>Ziviani</keyname><forenames>Artur</forenames></author><author><keyname>da Silva</keyname><forenames>Ana Paula Couto</forenames></author></authors><title>Time Centrality in Dynamic Complex Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an ever-increasing interest in investigating dynamics in
time-varying graphs (TVGs). Nevertheless, so far, the notion of centrality in
TVG scenarios usually refers to metrics that assess the relative importance of
nodes along the temporal evolution of the dynamic complex network. For some TVG
scenarios, however, more important than identifying the central nodes under a
given node centrality definition is identifying the key time instants for
taking certain actions. In this paper, we thus introduce and investigate the
notion of time centrality in TVGs. Analogously to node centrality, time
centrality evaluates the relative importance of time instants in dynamic
complex networks. In this context, we present two time centrality metrics
related to diffusion processes. We evaluate the two defined metrics using both
a real-world dataset representing an in-person contact dynamic network and a
synthetically generated randomized TVG. We validate the concept of time
centrality showing that diffusion starting at the best classified time instants
(i.e. the most central ones), according to our metrics, can perform a faster
and more efficient diffusion process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00247</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00247</id><created>2015-04-01</created><authors><author><keyname>Jebabli</keyname><forenames>Malek</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author><author><keyname>Cherifi</keyname><forenames>Chantal</forenames></author><author><keyname>Hamouda</keyname><forenames>Atef</forenames></author></authors><title>Overlapping Community Structure in Co-authorship Networks: a Case Study</title><categories>cs.SI physics.soc-ph</categories><comments>2014 7th International Conference on u- and e- Service, Science and
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure is one of the key properties of real-world complex
networks. It plays a crucial role in their behaviors and topology. While an
important work has been done on the issue of community detection, very little
attention has been devoted to the analysis of the community structure. In this
paper, we present an extensive investigation of the overlapping community
network deduced from a large-scale co-authorship network. The nodes of the
overlapping community network represent the functional communities of the
co-authorship network, and the links account for the fact that communities
share some nodes in the co-authorship network. The comparative evaluation of
the topological properties of these two networks shows that they share similar
topological properties. These results are very interesting. Indeed, the network
of communities seems to be a good representative of the original co-authorship
network. With its smaller size, it may be more practical in order to realize
various analyses that cannot be performed easily in large-scale real-world
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00249</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00249</id><created>2015-04-01</created><authors><author><keyname>Tayebinik</keyname><forenames>Maryam</forenames></author><author><keyname>Puteh</keyname><forenames>Marlia</forenames></author></authors><title>Sense of Community: How Important is this Quality in Blended Courses</title><categories>cs.CY</categories><comments>10 pages, Conference proceeding: Proceeding of the International
  Conference on Education and Management Innovation,2012,Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining online classes with the traditional classes foster the advantages
of both learning environments.The aim of this study was to examine the effects
of integrating face to face classes in fully online courses.Forty eight
undergraduate students studying at the e-learning center of a public university
in Iran were the subjects of this study.They were required to provide their
feedback on the inclusion of face to face component in their e-learning
classes.Data collected through open ended questions indicated that the most
dominant outcome of such a hybrid course on the students was the perception on
the sense of community.The findings suggest that students' high satisfaction on
blended learning courses was due to the fact that it promoted their sense of
community.This supports another conclusion of the study that face to face
classes and online classes are complementary and provide a balanced pedagogical
role for each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00253</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00253</id><created>2015-04-01</created><authors><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Tables of the existence of equiangular tight frames</title><categories>math.FA cs.IT math.CO math.IT</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A Grassmannian frame is a collection of unit vectors which are optimally
incoherent. The most accessible (and perhaps most beautiful) of Grassmannian
frames are equiangular tight frames (ETFs); indeed, there are infinite families
of known ETFs, whereas only finitely many non-ETF Grassmannian frames are known
to date. This paper surveys every known construction of ETFs and tabulates
existence for sufficiently small dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00280</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00280</id><created>2015-04-01</created><updated>2015-10-05</updated><authors><author><keyname>Tall</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Multilevel beamforming for high data rate communication in 5G networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Orange Labs Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large antenna arrays can be used to generate highly focused beams that
support very high data rates and reduced energy consumption. However, optimal
beam focusing requires large amount of feedback from the users in order to
choose the best beam, especially in Frequency Division Duplex (FDD) mode. This
paper develops a methodology for designing a multilevel codebook of beams in an
environment with low number of multipaths. The antenna design supporting the
focused beams is formulated as an optimization problem. A multilevel codebook
of beams is constructed according to the coverage requirements. An iterative
beam scheduling is proposed that searches through the codebook to select the
best beam for a given user. The methodology is applied to a mass event and to a
rural scenario, both analyzed using an event-based network simulator. Very
significant gains are obtained for both scenarios. It is shown that the more
dominant the Line of Sight (LoS) component, the higher the gain achieved by the
multilevel beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00284</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00284</id><created>2015-04-01</created><updated>2015-12-22</updated><authors><author><keyname>Calma</keyname><forenames>Adrian</forenames></author><author><keyname>Reitmaier</keyname><forenames>Tobias</forenames></author><author><keyname>Sick</keyname><forenames>Bernhard</forenames></author><author><keyname>Lukowicz</keyname><forenames>Paul</forenames></author><author><keyname>Embrechts</keyname><forenames>Mark</forenames></author></authors><title>A New Vision of Collaborative Active Learning</title><categories>cs.LG stat.ML</categories><comments>16 pages, 6 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning (AL) is a learning paradigm where an active learner has to
train a model (e.g., a classifier) which is in principal trained in a
supervised way, but in AL it has to be done by means of a data set with
initially unlabeled samples. To get labels for these samples, the active
learner has to ask an oracle (e.g., a human expert) for labels. The goal is to
maximize the performance of the model and to minimize the number of queries at
the same time. In this article, we first briefly discuss the state of the art
and own, preliminary work in the field of AL. Then, we propose the concept of
collaborative active learning (CAL). With CAL, we will overcome some of the
harsh limitations of current AL. In particular, we envision scenarios where an
expert may be wrong for various reasons, there might be several or even many
experts with different expertise, the experts may label not only samples but
also knowledge at a higher level such as rules, and we consider that the
labeling costs depend on many conditions. Moreover, in a CAL process human
experts will profit by improving their own knowledge, too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00304</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00304</id><created>2015-04-01</created><updated>2015-11-03</updated><authors><author><keyname>Whidden</keyname><forenames>Chris</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author></authors><title>Ricci-Ollivier Curvature of the Rooted Phylogenetic
  Subtree-Prune-Regraft Graph</title><categories>cs.DM cs.CE q-bio.PE</categories><comments>17 2-column pages, 6 figures, 2 tables. To appear in the Proceedings
  of the Thirteenth Workshop on Analytic Algorithmics and Combinatorics
  (ANALCO)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical phylogenetic inference methods use tree rearrangement operations
to perform either hill-climbing local search or Markov chain Monte Carlo across
tree topologies. The canonical class of such moves are the
subtree-prune-regraft (SPR) moves that remove a subtree and reattach it
somewhere else via the cut edge of the subtree. Phylogenetic trees and such
moves naturally form the vertices and edges of a graph, such that tree search
algorithms perform a (potentially stochastic) traversal of this SPR graph.
Despite the centrality of such graphs in phylogenetic inference, rather little
is known about their large-scale properties. In this paper we learn about the
rooted-tree version of the graph, known as the rSPR graph, by calculating the
Ricci-Ollivier curvature for pairs of vertices in the rSPR graph with respect
to two simple random walks on the rSPR graph. By proving theorems and direct
calculation with novel algorithms, we find a remarkable diversity of different
curvatures on the rSPR graph for pairs of vertices separated by the same
distance. We confirm using simulation that degree and curvature have the
expected impact on mean access time distributions, demonstrating relevance of
these curvature results to stochastic tree search. This indicates significant
structure of the rSPR graph beyond that which was previously understood in
terms of pairwise distances and vertex degrees; a greater understanding of
curvature could ultimately lead to improved strategies for tree search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00305</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00305</id><created>2015-04-01</created><authors><author><keyname>Ivanov</keyname><forenames>V. K.</forenames></author><author><keyname>Palyukh</keyname><forenames>B. V.</forenames></author></authors><title>Study the effectiveness of genetic algorithm for documentary subject
  search</title><categories>cs.IR</categories><comments>7 pages, in Russian</comments><journal-ref>OSTIS-2015 1 (2015) 471-476</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents results of experimental studies the effectiveness of
the genetic algorithm that was applied to effective queries creation and
relevant document selection. Studies were carried out to the comparative
analysis of the semantic relevance and quality ranking of the documents found
on the Internet in various ways. Analysis of the results shows that the
greatest effect of presented technology is achieved by finding new documents
for skilled users in the initial stages of the study of the topic.
Additionally, the number of unique and relevant results is significantly
increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00316</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00316</id><created>2015-04-01</created><authors><author><keyname>Filippidis</keyname><forenames>Christos</forenames></author><author><keyname>Cotronis</keyname><forenames>Yiannis</forenames></author><author><keyname>Markou</keyname><forenames>Christos</forenames></author></authors><title>Using IKAROS to provide Scalable I/O bandwidth</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present IKAROS as a utility that permit us to form scalable storage
platforms. IKAROS enable us to create ad-hoc nearby storage formations and use
a huge number of I/O nodes in order to increase the available bandwidth. We
measure the performance and scalability of IKAROS versus the IBMs General
Parallel File System (GPFS) under a variety of conditions. The measurements are
based on benchmark programs that allow us to vary block sizes and to measure
aggregate throughput rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00325</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00325</id><created>2015-04-01</created><updated>2015-04-03</updated><authors><author><keyname>Chen</keyname><forenames>Xinlei</forenames></author><author><keyname>Fang</keyname><forenames>Hao</forenames></author><author><keyname>Lin</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Vedantam</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Dollar</keyname><forenames>Piotr</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author></authors><title>Microsoft COCO Captions: Data Collection and Evaluation Server</title><categories>cs.CV cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1411.4952</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the Microsoft COCO Caption dataset and evaluation
server. When completed, the dataset will contain over one and a half million
captions describing over 330,000 images. For the training and validation
images, five independent human generated captions will be provided. To ensure
consistency in evaluation of automatic caption generation algorithms, an
evaluation server is used. The evaluation server receives candidate captions
and scores them using several popular metrics, including BLEU, METEOR, ROUGE
and CIDEr. Instructions for using the evaluation server are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00331</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00331</id><created>2015-04-01</created><authors><author><keyname>Carman</keyname><forenames>E. Preston</forenames><suffix>Jr.</suffix><affiliation>UC Riverside</affiliation></author><author><keyname>Westmann</keyname><forenames>Till</forenames><affiliation>Oracle Labs</affiliation></author><author><keyname>Borkar</keyname><forenames>Vinayak R.</forenames><affiliation>UC Irvine</affiliation></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames><affiliation>UC Irvine</affiliation></author><author><keyname>Tsotras</keyname><forenames>Vassilis J.</forenames><affiliation>UC Riverside</affiliation></author></authors><title>Apache VXQuery: A Scalable XQuery Implementation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide use of XML for document management and data exchange has created the
need to query large repositories of XML data. To efficiently query such large
data collections and take advantage of parallelism, we have implemented Apache
VXQuery, an open-source scalable XQuery processor. The system builds upon two
other open-source frameworks -- Hyracks, a parallel execution engine, and
Algebricks, a language agnostic compiler toolbox. Apache VXQuery extends these
two frameworks and provides an implementation of the XQuery specifics (data
model, data-model dependent functions and optimizations, and a parser). We
describe the architecture of Apache VXQuery, its integration with Hyracks and
Algebricks, and the XQuery optimization rules applied to the query plan to
improve path expression efficiency and to enable query parallelism. An
experimental evaluation using a real 500GB dataset with various selection,
aggregation and join XML queries shows that Apache VXQuery performs well both
in terms of scale-up and speed-up. Our experiments show that it is about 3x
faster than Saxon (an open-source and commercial XQuery processor) on a 4-core,
single node implementation, and around 2.5x faster than Apache MRQL (a
MapReduce-based parallel query processor) on an eight (4-core) node cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00337</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00337</id><created>2015-04-01</created><updated>2015-11-02</updated><authors><author><keyname>Guinea</keyname><forenames>Alejandro Sanchez</forenames></author></authors><title>Deterministic Polynomial Solution for NP based on a Mechanical Process
  of Understanding</title><categories>cs.CC</categories><comments>15 pages, 8 figures, exposition completely changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deterministic algorithm that avoids brute-force search in
solving 3SAT and provides efficient solution to any instance of this problem.
The algorithm solves a problem instance based on building an understanding on
how the context of each literal (viz., the rest of the instance and the problem
definition) makes the literal be related to a specific truth value. We show
that based on the proposed algorithm it is possible to solve efficiently the
problems in the class NP of decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00341</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00341</id><created>2015-04-01</created><updated>2015-05-10</updated><authors><author><keyname>Farina</keyname><forenames>Gabriele</forenames></author></authors><title>A linear time algorithm to compute the impact of all the articulation
  points</title><categories>cs.DS</categories><comments>4 pages, 3 figures. Accepted for YR-ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The articulation points of an undirected connected graphs are those vertices
whose removal increases the number of connected components of the graph, i.e.
the vertices whose removal disconnects the graph. However, not all the
articulation points are equal: the removal of some of them might end in a
single vertex disconnected from the graph, whilst in other cases the graph can
be split in several small pieces. In order to measure the effect of the removal
of an articulation point, in \cite{AFL12} has been proposed the impact, defined
as the number of vertices that get disconnected from the main (largest)
surviving connected component (CC). In this paper we present the first linear
time algorithm ($\mathcal{O}(m+n)$ for a graph with $n$ vertices and $m$ edges)
to compute the impact of all the articulation points of the graph, thus
improving from the $\mathcal{O}(a(m+n))\approx\mathcal{O}(nm+n^2)$ of the
na\&quot;ive algorithm, with $a$ being the number of articulation points of the
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00353</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00353</id><created>2015-04-01</created><authors><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Leroux</keyname><forenames>Camille</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Low-Latency Software Polar Decoders</title><categories>cs.IT cs.PF math.IT</categories><comments>16 pages, 8 figures, submitted to IEEE Trans. Signal Process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are a new class of capacity-achieving error-correcting codes with
low encoding and decoding complexity. Their low-complexity decoding algorithms
render them attractive for use in software-defined radio applications where
computational resources are limited. In this work, we present low-latency
software polar decoders that exploit modern processor capabilities. We show how
adapting the algorithm at various levels can lead to significant improvements
in latency and throughput, yielding polar decoders that are suitable for
high-performance software-defined radio applications on modern desktop
processors and embedded-platform processors. These proposed decoders have an
order of magnitude lower latency compared to state of the art decoders, while
maintaining comparable throughput. In addition, we present strategies and
results for implementing polar decoders on graphical processing units. Finally,
we show that the energy efficiency of the proposed decoders, running on desktop
a processor, is comparable to state of the art software polar decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00377</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00377</id><created>2015-04-01</created><authors><author><keyname>Zhang</keyname><forenames>Zhengwu</forenames></author><author><keyname>Pati</keyname><forenames>Debdeep</forenames></author><author><keyname>Srivastava</keyname><forenames>Anuj</forenames></author></authors><title>Bayesian Clustering of Shapes of Curves</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised clustering of curves according to their shapes is an important
problem with broad scientific applications. The existing model-based clustering
techniques either rely on simple probability models (e.g., Gaussian) that are
not generally valid for shape analysis or assume the number of clusters. We
develop an efficient Bayesian method to cluster curve data using an elastic
shape metric that is based on joint registration and comparison of shapes of
curves. The elastic-inner product matrix obtained from the data is modeled
using a Wishart distribution whose parameters are assigned carefully chosen
prior distributions to allow for automatic inference on the number of clusters.
Posterior is sampled through an efficient Markov chain Monte Carlo procedure
based on the Chinese restaurant process to infer (1) the posterior distribution
on the number of clusters, and (2) clustering configuration of shapes. This
method is demonstrated on a variety of synthetic data and real data examples on
protein structure analysis, cell shape analysis in microscopy images, and
clustering of shaped from MPEG7 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00386</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00386</id><created>2015-04-01</created><authors><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author><author><keyname>Marzen</keyname><forenames>Sarah</forenames></author></authors><title>Signatures of Infinity: Nonergodicity and Resource Scaling in
  Prediction, Complexity, and Learning</title><categories>cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML</categories><comments>8 pages, 1 figure; http://csc.ucdavis.edu/~cmg/compmech/pubs/soi.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple analysis of the structural complexity of
infinite-memory processes built from random samples of stationary, ergodic
finite-memory component processes. Such processes are familiar from the well
known multi-arm Bandit problem. We contrast our analysis with
computation-theoretic and statistical inference approaches to understanding
their complexity. The result is an alternative view of the relationship between
predictability, complexity, and learning that highlights the distinct ways in
which informational and correlational divergences arise in complex ergodic and
nonergodic processes. We draw out consequences for the resource divergences
that delineate the structural hierarchy of ergodic processes and for processes
that are themselves hierarchical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00390</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00390</id><created>2015-04-01</created><authors><author><keyname>Subramanian</keyname><forenames>Lavanya</forenames></author><author><keyname>Lee</keyname><forenames>Donghyuk</forenames></author><author><keyname>Seshadri</keyname><forenames>Vivek</forenames></author><author><keyname>Rastogi</keyname><forenames>Harsha</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>The Blacklisting Memory Scheduler: Balancing Performance, Fairness and
  Complexity</title><categories>cs.DC</categories><report-no>SAFARI Technical Report No. 2015-004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multicore system, applications running on different cores interfere at
main memory. This inter-application interference degrades overall system
performance and unfairly slows down applications. Prior works have developed
application-aware memory schedulers to tackle this problem. State-of-the-art
application-aware memory schedulers prioritize requests of applications that
are vulnerable to interference, by ranking individual applications based on
their memory access characteristics and enforcing a total rank order.
  In this paper, we observe that state-of-the-art application-aware memory
schedulers have two major shortcomings. First, such schedulers trade off
hardware complexity in order to achieve high performance or fairness, since
ranking applications with a total order leads to high hardware complexity.
Second, ranking can unfairly slow down applications that are at the bottom of
the ranking stack. To overcome these shortcomings, we propose the Blacklisting
Memory Scheduler (BLISS), which achieves high system performance and fairness
while incurring low hardware complexity, based on two observations. First, we
find that, to mitigate interference, it is sufficient to separate applications
into only two groups. Second, we show that this grouping can be efficiently
performed by simply counting the number of consecutive requests served from
each application.
  We evaluate BLISS across a wide variety of workloads/system configurations
and compare its performance and hardware complexity, with five state-of-the-art
memory schedulers. Our evaluations show that BLISS achieves 5% better system
performance and 25% better fairness than the best-performing previous scheduler
while greatly reducing critical path latency and hardware area cost of the
memory scheduler (by 79% and 43%, respectively), thereby achieving a good
trade-off between performance, fairness and hardware complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00416</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00416</id><created>2015-04-01</created><authors><author><keyname>Xuan</keyname><forenames>Junyu</forenames></author><author><keyname>Lu</keyname><forenames>Jie</forenames></author><author><keyname>Luo</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Guangquan</forenames></author></authors><title>Nonnegative Multi-level Network Factorization for Latent Factor Analysis</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two
optimized nonnegative matrices and has been widely used for unsupervised
learning tasks such as product recommendation based on a rating matrix.
However, although networks between nodes with the same nature exist, standard
NMF overlooks them, e.g., the social network between users. This problem leads
to comparatively low recommendation accuracy because these networks are also
reflections of the nature of the nodes, such as the preferences of users in a
social network. Also, social networks, as complex networks, have many different
structures. Each structure is a composition of links between nodes and reflects
the nature of nodes, so retaining the different network structures will lead to
differences in recommendation performance. To investigate the impact of these
network structures on the factorization, this paper proposes four multi-level
network factorization algorithms based on the standard NMF, which integrates
the vertical network (e.g., rating matrix) with the structures of horizontal
network (e.g., user social network). These algorithms are carefully designed
with corresponding convergence proofs to retain four desired network
structures. Experiments on synthetic data show that the proposed algorithms are
able to preserve the desired network structures as designed. Experiments on
real-world data show that considering the horizontal networks improves the
accuracy of document clustering and recommendation with standard NMF, and
various structures show their differences in performance on these two tasks.
These results can be directly used in document clustering and recommendation
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00427</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00427</id><created>2015-04-01</created><authors><author><keyname>Chan</keyname><forenames>T. -H. Hubert</forenames></author><author><keyname>Ning</keyname><forenames>Li</forenames></author></authors><title>Influence Maximization under The Non-progressive Linear Threshold Model</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the problem of influence maximization in information networks, the
objective is to choose a set of initially active nodes subject to some budget
constraints such that the expected number of active nodes over time is
maximized. The linear threshold model has been introduced to study the opinion
cascading behavior, for instance, the spread of products and innovations. In
this paper, we we extends the classic linear threshold model [18] to capture
the non-progressive be- havior. The information maximization problem under our
model is proved to be NP-Hard, even for the case when the underlying network
has no directed cycles. The first result of this paper is negative. In general,
the objective function of the extended linear threshold model is no longer
submodular, and hence the hill climbing approach that is commonly used in the
existing studies is not applicable. Next, as the main result of this paper, we
prove that if the underlying information network is directed acyclic, the
objective function is submodular (and monotone). Therefore, in directed acyclic
networks with a speci?ed budget we can achieve 1/2 -approximation on maximizing
the number of active nodes over a certain period of time by a deterministic
algorithm, and achieve the (1 - 1/e )-approximation by a randomized algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00429</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00429</id><created>2015-04-01</created><authors><author><keyname>Koufogiannis</keyname><forenames>Fragkiskos</forenames></author><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Gradual Release of Sensitive Data under Differential Privacy</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of releasing sensitive data under differential
privacy when the privacy level is subject to change over time. Existing work
assumes that privacy level is determined by the system designer as a fixed
value before sensitive data is released. For certain applications, however,
users may wish to relax the privacy level for subsequent releases of the same
data after either a re-evaluation of the privacy concerns or the need for
better accuracy. Specifically, given a database containing sensitive data, we
assume that a response $y_1$ that preserves $\epsilon_{1}$-differential privacy
has already been published. Then, the privacy level is relaxed to $\epsilon_2$,
with $\epsilon_2 &gt; \epsilon_1$, and we wish to publish a more accurate response
$y_2$ while the joint response $(y_1, y_2)$ preserves $\epsilon_2$-differential
privacy. How much accuracy is lost in the scenario of gradually releasing two
responses $y_1$ and $y_2$ compared to the scenario of releasing a single
response that is $\epsilon_{2}$-differentially private? Our results show that
there exists a composite mechanism that achieves \textit{no loss} in accuracy.
We consider the case in which the private data lies within $\mathbb{R}^{n}$
with an adjacency relation induced by the $\ell_{1}$-norm, and we focus on
mechanisms that approximate identity queries. We show that the same accuracy
can be achieved in the case of gradual release through a mechanism whose
outputs can be described by a \textit{lazy Markov stochastic process}. This
stochastic process has a closed form expression and can be efficiently sampled.
Our results are applicable beyond identity queries. To this end, we demonstrate
that our results can be applied in several cases, including Google's RAPPOR
project, trading of sensitive data, and controlled transmission of private data
in a social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00430</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00430</id><created>2015-04-01</created><authors><author><keyname>Peng</keyname><forenames>Hanyang</forenames></author><author><keyname>Fan</keyname><forenames>Yong</forenames></author></authors><title>Direct l_(2,p)-Norm Learning for Feature Selection</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel sparse learning based feature selection
method that directly optimizes a large margin linear classification model
sparsity with l_(2,p)-norm (0 &lt; p &lt; 1)subject to data-fitting constraints,
rather than using the sparsity as a regularization term. To solve the direct
sparsity optimization problem that is non-smooth and non-convex when 0&lt;p&lt;1, we
provide an efficient iterative algorithm with proved convergence by converting
it to a convex and smooth optimization problem at every iteration step. The
proposed algorithm has been evaluated based on publicly available datasets, and
extensive comparison experiments have demonstrated that our algorithm could
achieve feature selection performance competitive to state-of-the-art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00434</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00434</id><created>2015-04-01</created><updated>2015-04-07</updated><authors><author><keyname>Lakshminarayana</keyname><forenames>Subhash</forenames></author><author><keyname>Assaad</keyname><forenames>Mohamad</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Coordinated Multi-cell Beamforming for Massive MIMO: A Random Matrix
  Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of coordinated multi- cell downlink beamforming in
massive multiple input multiple output (MIMO) systems consisting of N cells, Nt
antennas per base station (BS) and K user terminals (UTs) per cell.
Specifically, we formulate a multi-cell beamforming algorithm for massive MIMO
systems which requires limited amount of information exchange between the BSs.
The design objective is to minimize the aggregate transmit power across all the
BSs subject to satisfying the user signal to interference noise ratio (SINR)
constraints. The algorithm requires the BSs to exchange parameters which can be
computed solely based on the channel statistics rather than the instantaneous
CSI. We make use of tools from random matrix theory to formulate the
decentralized algorithm. We also characterize a lower bound on the set of
target SINR values for which the decentralized multi-cell beamforming algorithm
is feasible. We further show that the performance of our algorithm
asymptotically matches the performance of the centralized algorithm with full
CSI sharing. While the original result focuses on minimizing the aggregate
transmit power across all the BSs, we formulate a heuristic extension of this
algorithm to incorporate a practical constraint in multi-cell systems, namely
the individual BS transmit power constraints. Finally, we investigate the
impact of imperfect CSI and pilot contamination effect on the performance of
the decentralized algorithm, and propose a heuristic extension of the algorithm
to accommodate these issues. Simulation results illustrate that our algorithm
closely satisfies the target SINR constraints and achieves minimum power in the
regime of massive MIMO systems. In addition, it also provides substantial power
savings as compared to zero-forcing beamforming when the number of antennas per
BS is of the same orders of magnitude as the number of UTs per cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00442</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00442</id><created>2015-04-01</created><updated>2015-11-09</updated><authors><author><keyname>Cui</keyname><forenames>Peng</forenames></author></authors><title>Refuting Unique Game Conjecture</title><categories>cs.CC</categories><comments>6 pages, short note. arXiv admin note: substantial text overlap with
  arXiv:1401.6520</comments><acm-class>F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, the author shows that the gap problem of some $k$-CSPs
with the support of its predicate the ground of a balanced pairwise independent
distribution can be solved by a modified version of Hast's Algorithm BiLin that
calls Charikar\&amp;Wirth's SDP algorithm for two rounds in polynomial time, when
$k$ is sufficiently large, the support of its predicate is combined by the
grounds of three biased homogeneous distributions and the three biases satisfy
certain conditions. To conclude, the author refutes Unique Game Conjecture,
assuming $P\ne NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00450</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00450</id><created>2015-04-02</created><authors><author><keyname>Xue</keyname><forenames>Yang</forenames></author></authors><title>Recent Development in Analog Computation - A Brief Overview</title><categories>cs.ET cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent development in analog computation is reviewed in this paper.
Analog computation was used in many applications where power and energy
efficiency is of paramount importance. It is shown that by using innovative
architecture and circuit design, analog computation systems can achieve much
higher energy efficiency than their digital counterparts, as they are able to
exploit the computational power inherent to the devices and physics. However,
these systems do suffer from some disadvantages, such as lower accuracy and
speed, and designers have come up with novel approaches to overcome them. The
paper provides an overview of analog computation systems, from basic components
such as memory and arithmetic elements, to architecture and system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00457</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00457</id><created>2015-04-02</created><authors><author><keyname>Wartner</keyname><forenames>Christian</forenames></author><author><keyname>Arnold</keyname><forenames>Patrick</forenames></author><author><keyname>Rahm</keyname><forenames>Erhard</forenames></author></authors><title>Semi-automatic identification of counterfeit offers in online shopping
  platforms</title><categories>cs.DB cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product counterfeiting is a serious problem causing the industry estimated
losses of billions of dollars every year. With the increasing spread of
e-commerce, the number of counterfeit products sold online increased
substantially. We propose the adoption of a semi-automatic workflow to identify
likely counterfeit offers in online platforms and to present these offers to a
domain expert for manual verification. The workflow includes steps to generate
search queries for relevant product offers, to match and cluster similar
product offers, and to assess the counterfeit suspiciousness based on different
criteria. The goal is to support the periodic identification of many
counterfeit offers with a limited amount of manual effort. We explain how the
proposed approach can be realized. We also present a preliminary evaluation of
its most important steps on a case study using the eBay platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00458</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00458</id><created>2015-04-02</created><updated>2015-12-15</updated><authors><author><keyname>Hisakado</keyname><forenames>Masato</forenames></author><author><keyname>Mori</keyname><forenames>Shintaro</forenames></author></authors><title>Information cascade on networks</title><categories>physics.soc-ph cs.SI</categories><comments>31 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1203.3274</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss a voting model by considering three different kinds
of networks: a random graph, the Barab\'{a}si-Albert(BA) model, and a fitness
model. A voting model represents the way in which public perceptions are
conveyed to voters. Our voting model is constructed by using two types of
voters--herders and independents--and two candidates. Independents conduct
voting based on their fundamental values; on the other hand, herders base their
voting on the number of previous votes. Hence, herders vote for the majority
candidates and obtain information relating to previous votes from their
networks. We discuss the difference between the phases on which the networks
depend. Two kinds of phase transitions, an information cascade transition and a
super-normal transition, were identified. The first of these is a transition
between a state in which most voters make the correct choices and a state in
which most of them are wrong. The second is a transition of convergence speed.
The information cascade transition prevails when herder effects are stronger
than the super-normal transition. In the BA and fitness models, the critical
point of the information cascade transition is the same as that of the random
network model. However, the critical point of the super-normal transition
disappears when these two models are used. In conclusion, the influence of
networks is shown to only affect the convergence speed and not the information
cascade transition. We are therefore able to conclude that the influence of
hubs on voters' perceptions is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00474</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00474</id><created>2015-04-02</created><updated>2015-04-26</updated><authors><author><keyname>Tang</keyname><forenames>Shaoting</forenames></author><author><keyname>Teng</keyname><forenames>Xian</forenames></author><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Yan</keyname><forenames>Shu</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author></authors><title>Identification of highly susceptible individuals in complex networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.physa.2015.03.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying highly susceptible individuals in spreading processes is of great
significance in controlling outbreaks. In this paper, we explore the
susceptibility of people in susceptible-infectious-recovered (SIR) and rumor
spreading dynamics. We first study the impact of community structure on
people's susceptibility. Despite that the community structure can reduce the
infected population given same infection rates, it will not deterministically
affect nodes' susceptibility. We find the susceptibility of individuals is
sensitive to the choice of spreading dynamics. For SIR spreading, since the
susceptibility is highly correlated to nodes' influence, the topological
indicator k-shell can better identify highly susceptible individuals,
outperforming degree, betweenness centrality and PageRank. In contrast, in
rumor spreading model, where nodes' susceptibility and influence have no clear
correlation, degree performs the best among considered topological measures.
Our finding highlights the significance of both topological features and
spreading mechanisms in identifying highly susceptible population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00481</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00481</id><created>2015-04-02</created><updated>2015-10-09</updated><authors><author><keyname>Kubjas</keyname><forenames>Ivo</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>Data Dissemination Problem in Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Notation clarification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we formulate and study a data dissemination problem, which can
be viewed as a generalization of the index coding problem and of the data
exchange problem to networks with an arbitrary topology. We define $r$-solvable
networks, in which data dissemination can be achieved in $r &gt; 0$ communications
rounds. We show that the optimum number of transmissions for any one-round
communications scheme is given by the minimum rank of a certain constrained
family of matrices. For a special case of this problem, called bipartite data
dissemination problem, we present lower and upper graph-theoretic bounds on the
optimum number of transmissions. For general $r$-solvable networks, we derive
an upper bound on the minimum number of transmissions in any scheme with $\geq
r$ rounds. We experimentally compare the obtained upper bound to a simple lower
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00495</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00495</id><created>2015-04-02</created><authors><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Muchnik</keyname><forenames>Lev</forenames></author><author><keyname>Tang</keyname><forenames>Shaoting</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Exploring the complex pattern of information spreading in online blog
  communities</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information spreading in online social communities has attracted tremendous
attention due to its utmost practical values in applications. Despite that
several individual-level diffusion data have been investigated, we still lack
the detailed understanding of the spreading pattern of information. Here, by
comparing information flows and social links in a blog community, we find that
the diffusion processes are induced by three different spreading mechanisms:
social spreading, self-promotion and broadcast. Although numerous previous
studies have employed epidemic spreading models to simulate information
diffusion, we observe that such models fail to reproduce the realistic
diffusion pattern. In respect to users behaviors, strikingly, we find that most
users would stick to one specific diffusion mechanism. Moreover, our
observations indicate that the social spreading is not only crucial for the
structure of diffusion trees, but also capable of inducing more subsequent
individuals to acquire the information. Our findings suggest new directions for
modeling of information diffusion in social systems and could inform design of
efficient propagation strategies based on users behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00502</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00502</id><created>2015-04-02</created><authors><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Tang</keyname><forenames>Shaoting</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author></authors><title>Detecting the Influence of Spreading in Social Networks with Excitable
  Sensor Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting spreading outbreaks in social networks with sensors is of great
significance in applications. Inspired by the formation mechanism of human's
physical sensations to external stimuli, we propose a new method to detect the
influence of spreading by constructing excitable sensor networks. Exploiting
the amplifying effect of excitable sensor networks, our method can better
detect small-scale spreading processes. At the same time, it can also
distinguish large-scale diffusion instances due to the self-inhibition effect
of excitable elements. Through simulations of diverse spreading dynamics on
typical real-world social networks (facebook, coauthor and email social
networks), we find that the excitable senor networks are capable of detecting
and ranking spreading processes in a much wider range of influence than other
commonly used sensor placement methods, such as random, targeted, acquaintance
and distance strategies. In addition, we validate the efficacy of our method
with diffusion data from a real-world online social system, Twitter. We find
that our method can detect more spreading topics in practice. Our approach
provides a new direction in spreading detection and should be useful for
designing effective detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00512</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00512</id><created>2015-04-02</created><authors><author><keyname>Arbach</keyname><forenames>Youssef</forenames></author><author><keyname>Karcher</keyname><forenames>David</forenames></author><author><keyname>Peters</keyname><forenames>Kirstin</forenames></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames></author></authors><title>Dynamic Causality in Event Structures (Technical Report)</title><categories>cs.LO</categories><comments>Proofs and additional information for the FORTE'15 paper 'Dynamic
  Causality in Event Structures'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1] we present an extension of Prime Event Structures by a mechanism to
express dynamicity in the causal relation. More precisely we add the
possibility that the occurrence of an event can add or remove causal
dependencies between events and analyse the expressive power of the resulting
Event Structures w.r.t. to some well-known Event Structures from the
literature. This technical report contains some additional information and the
missing proofs of [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00513</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00513</id><created>2015-04-02</created><authors><author><keyname>Ruchansky</keyname><forenames>Natali</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Garcia-Soriano</keyname><forenames>David</forenames></author><author><keyname>Gullo</keyname><forenames>Francesco</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author></authors><title>The Minimum Wiener Connector</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wiener index of a graph is the sum of all pairwise shortest-path
distances between its vertices. In this paper we study the novel problem of
finding a minimum Wiener connector: given a connected graph $G=(V,E)$ and a set
$Q\subseteq V$ of query vertices, find a subgraph of $G$ that connects all
query vertices and has minimum Wiener index.
  We show that The Minimum Wiener Connector admits a polynomial-time (albeit
impractical) exact algorithm for the special case where the number of query
vertices is bounded. We show that in general the problem is NP-hard, and has no
PTAS unless $\mathbf{P} = \mathbf{NP}$. Our main contribution is a
constant-factor approximation algorithm running in time
$\widetilde{O}(|Q||E|)$.
  A thorough experimentation on a large variety of real-world graphs confirms
that our method returns smaller and denser solutions than other methods, and
does so by adding to the query set $Q$ a small number of important vertices
(i.e., vertices with high centrality).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00522</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00522</id><created>2015-04-02</created><authors><author><keyname>Behzadian</keyname><forenames>Bahram</forenames></author><author><keyname>Agarwal</keyname><forenames>Pratik</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author><author><keyname>Tipaldi</keyname><forenames>Gian Diego</forenames></author></authors><title>Monte Carlo Localization in Hand-Drawn Maps</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot localization is a one of the most important problems in robotics. Most
of the existing approaches assume that the map of the environment is available
beforehand and focus on accurate metrical localization. In this paper, we
address the localization problem when the map of the environment is not present
beforehand, and the robot relies on a hand-drawn map from a non-expert user. We
addressed this problem by expressing the robot pose in the pixel coordinate and
simultaneously estimate a local deformation of the hand-drawn map. Experiments
show that we are able to localize the robot in the correct room with a
robustness up to 80%
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00523</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00523</id><created>2015-04-02</created><updated>2015-11-10</updated><authors><author><keyname>Ye</keyname><forenames>Haishan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>Fast Spectral Low Rank Matrix Approximation</title><categories>cs.NA</categories><comments>This paper has some error in proof</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  First, we extend the results of approximate matrix multiplication from the
Frobenius norm to the spectral norm. Second, We develop a class of fast
approximate generalized linear regression algorithms with respect to the
spectral norm. Finally, We give a fast approximate SVD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00527</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00527</id><created>2015-04-02</created><authors><author><keyname>Sergeraert</keyname><forenames>Francis</forenames></author></authors><title>Functional Programming is Free</title><categories>cs.PL</categories><comments>26 pages</comments><msc-class>14Q20, 55-04</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A paper has recently been published in SIAM-JC. This paper is faulty: 1) The
standard requirements about the definition of an algorithm are not respected,
2) The main point in the complexity study, namely the functional programming
component, is absent. The Editorial Board of the SIAM JC had been warned a
confirmed publication would be openly commented, it is the role of this text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00532</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00532</id><created>2015-04-02</created><updated>2015-12-30</updated><authors><author><keyname>Jin</keyname><forenames>Kyong Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Dongwook</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>A general framework for compressed sensing and parallel MRI using
  annihilating filter based low-rank Hankel matrix</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel MRI (pMRI) and compressed sensing MRI (CS-MRI) have been considered
as two distinct reconstruction problems. Inspired by recent k-space
interpolation methods, an annihilating filter based low-rank Hankel matrix
approach (ALOHA) is proposed as a general framework for sparsity-driven k-space
interpolation method which unifies pMRI and CS-MRI. Specifically, our framework
is based on the fundamental duality between the transform domain sparsity in
the primary space and the low-rankness of weighted Hankel matrix in the
reciprocal space, which converts pMRI and CS-MRI to a k-space interpolation
problem using structured matrix completion. Using theoretical results from the
latest compressed sensing literatures, we showed that the required sampling
rates for ALOHA may achieve the optimal rate. Experimental results with in vivo
data for single/multi-coil imaging as well as dynamic imaging confirmed that
the proposed method outperforms the state-of-the-art pMRI and CS-MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00542</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00542</id><created>2015-04-02</created><authors><author><keyname>Diertens</keyname><forenames>Bob</forenames></author></authors><title>Composition in the Function-Behaviour-Structure Framework</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.2489</comments><report-no>TCS1501</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce composition in the function-behaviour-structure framework for
design, as described by John Gero, in order to deal with complexity. We do this
by connecting the frameworks for the design of several models, in which one is
constrained by the others. The result is a framework for the design of an
object that supports modularity. This framework can easily be extended for the
design of an object with more than one layer of modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00545</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00545</id><created>2015-04-02</created><authors><author><keyname>Bingmann</keyname><forenames>Timo</forenames></author><author><keyname>Keh</keyname><forenames>Thomas</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>A Bulk-Parallel Priority Queue in External Memory with STXXL</title><categories>cs.DS</categories><comments>extended version of SEA'15 conference paper</comments><acm-class>E.1; D.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the design and an implementation of a bulk-parallel external
memory priority queue to take advantage of both shared-memory parallelism and
high external memory transfer speeds to parallel disks. To achieve higher
performance by decoupling item insertions and extractions, we offer two
parallelization interfaces: one using &quot;bulk&quot; sequences, the other by defining
&quot;limit&quot; items. In the design, we discuss how to parallelize insertions using
multiple heaps, and how to calculate a dynamic prediction sequence to prefetch
blocks and apply parallel multiway merge for extraction. Our experimental
results show that in the selected benchmarks the priority queue reaches 75% of
the full parallel I/O bandwidth of rotational disks and and 65% of SSDs, or the
speed of sorting in external memory when bounded by computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00548</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00548</id><created>2015-04-02</created><updated>2015-08-30</updated><authors><author><keyname>Hill</keyname><forenames>Felix</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Korhonen</keyname><forenames>Anna</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Learning to Understand Phrases by Embedding the Dictionary</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributional models that learn rich semantic word representations are a
success story of recent NLP research. However, developing models that learn
useful representations of phrases and sentences has proved far harder. We
propose using the definitions found in everyday dictionaries as a means of
bridging this gap between lexical and phrasal semantics. Neural language
embedding models can be effectively trained to map dictionary definitions
(phrases) to (lexical) representations of the words defined by those
definitions. We present two applications of these architectures: &quot;reverse
dictionaries&quot; that return the name of a concept given a definition or
description and general-knowledge crossword question answerers. On both tasks,
neural language embedding models trained on definitions from a handful of
freely-available lexical resources perform as well or better than existing
commercial systems that rely on significant task-specific engineering. The
results highlight the effectiveness of both neural embedding architectures and
definition-based training for developing models that understand phrases and
sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00549</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00549</id><created>2015-04-02</created><updated>2015-04-03</updated><authors><author><keyname>Yeon</keyname><forenames>Hanul</forenames></author><author><keyname>Har</keyname><forenames>Dongsoo</forenames></author></authors><title>Situation-Aware Integration and Transmission of Safety Information for
  Smart Railway Vehicles</title><categories>cs.NI</categories><comments>15 pages</comments><msc-class>94A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent trend of railway train development can be characterized in several
aspects : high speed, infortainment, intelligence in driving, and so on. In
particular, trend of high speed in driving is prominent and competition for
high speed amongst several techno-savvy countries is becoming severe. To
achieve high speed, engines or motors are distributed over multiple vehicles of
train to provide increased motive power, while a single engine or motor has
been mostly used for conventional trains. Increased speed and more complicated
powertrain system naturally incur much higher chance of massive accidents. From
this perspective, importance of proactive safety control before accident takes
place cannot be over-emphasized. To implement proactive safety control requires
situation-aware integration and transmission of safety information obtained
from IoT sensors. Types of critical IoT sensors depend on situational
conditions. Thus, integration and transmission of safety information should be
performed with IoT sensors providing the safety information proper for faced
situation. This brief paper is to devise a methodology how to operate IoT
sensor network enabling proactive safety control for railway vehicles and to
propose a queue management based medium access control scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00553</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00553</id><created>2015-04-02</created><updated>2016-02-27</updated><authors><author><keyname>Wang</keyname><forenames>Chien-Yi</forenames></author><author><keyname>Lim</keyname><forenames>Sung Hoon</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Information-Theoretic Caching: Sequential Coding for Computing</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Inf. Theory and presented in part at ISIT
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the paradigm of caching, partial data is delivered before the actual
requests of users are known. In this paper, this problem is modeled as a
canonical distributed source coding problem with side information, where the
side information represents the users' requests. For the single-user case, a
single-letter characterization of the optimal rate region is established, and
for several important special cases, closed-form solutions are given, including
the scenario of uniformly distributed user requests. In this case, it is shown
that the optimal caching strategy is closely related to total correlation and
Wyner's common information. Using the insight gained from the single-user case,
three two-user scenarios admitting single-letter characterization are
considered, which draw connections to existing source coding problems in the
literature: the Gray--Wyner system and distributed successive refinement.
Finally, the model studied by Maddah-Ali and Niesen is rephrased to make a
comparison with the considered information-theoretic model. Although the two
caching models have a similar behavior for the single-user case, it is shown
through a two-user example that the two caching models behave differently in
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00572</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00572</id><created>2015-04-02</created><authors><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author></authors><title>Efficient indexing of necklaces and irreducible polynomials over finite
  fields</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of indexing irreducible polynomials over finite fields,
and give the first efficient algorithm for this problem. Specifically, we show
the existence of poly(n, log q)-size circuits that compute a bijection between
{1, ... , |S|} and the set S of all irreducible, monic, univariate polynomials
of degree n over a finite field F_q. This has applications in pseudorandomness,
and answers an open question of Alon, Goldreich, H{\aa}stad and Peralta[AGHP].
  Our approach uses a connection between irreducible polynomials and necklaces
( equivalence classes of strings under cyclic rotation). Along the way, we give
the first efficient algorithm for indexing necklaces of a given length over a
given alphabet, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00576</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00576</id><created>2015-04-02</created><authors><author><keyname>Demidova</keyname><forenames>A. V.</forenames></author><author><keyname>Korolkova</keyname><forenames>A. V.</forenames></author><author><keyname>Kulyabov</keyname><forenames>D. S.</forenames></author><author><keyname>Sevastyanov</keyname><forenames>L. A.</forenames></author></authors><title>The Method of Constructing Models of Peer to Peer Protocols</title><categories>cs.NI</categories><comments>in Russian; in English</comments><journal-ref>6th International Congress on Ultra Modern Telecommunications and
  Control Systems and Workshops (ICUMT). 2014, IEEE, pp. 557-562</journal-ref><doi>10.1109/ICUMT.2014.7002162</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The models of peer to peer protocols are presented with the help of one-step
processes. On the basis of this presentation and the method of randomization of
one-step processes described method for constructing models of peer to peer
protocols. As specific implementations of proposed method the models of
FastTrack and Bittorrent protocols are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00580</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00580</id><created>2015-04-02</created><authors><author><keyname>Ostaszewski</keyname><forenames>Mateusz</forenames></author><author><keyname>Sadowski</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Gawron</keyname><forenames>Piotr</forenames></author></authors><title>Quantum image classification using principal component analysis</title><categories>quant-ph cs.CV cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel quantum algorithm for classification of images. The
algorithm is constructed using principal component analysis and von Neuman
quantum measurements. In order to apply the algorithm we present a new quantum
representation of grayscale images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00591</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00591</id><created>2015-04-02</created><updated>2015-04-12</updated><authors><author><keyname>Beard</keyname><forenames>Jonathan C.</forenames></author><author><keyname>Chamberlain</keyname><forenames>Roger D.</forenames></author></authors><title>Run Time Approximation of Non-blocking Service Rates for Streaming
  Systems</title><categories>cs.PF</categories><comments>technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream processing is a compute paradigm that promises safe and efficient
parallelism. Modern big-data problems are often well suited for stream
processing's throughput-oriented nature. Realization of efficient stream
processing requires monitoring and optimization of multiple communications
links. Most techniques to optimize these links use queueing network models or
network flow models, which require some idea of the actual execution rate of
each independent compute kernel within the system. What we want to know is how
fast can each kernel process data independent of other communicating kernels.
This is known as the &quot;service rate&quot; of the kernel within the queueing
literature. Current approaches to divining service rates are static. Modern
workloads, however, are often dynamic. Shared cloud systems also present
applications with highly dynamic execution environments (multiple users,
hardware migration, etc.). It is therefore desirable to continuously re-tune an
application during run time (online) in response to changing conditions. Our
approach enables online service rate monitoring under most conditions,
obviating the need for reliance on steady state predictions for what are
probably non-steady state phenomena. First, some of the difficulties associated
with online service rate determination are examined. Second, the algorithm to
approximate the online non-blocking service rate is described. Lastly, the
algorithm is implemented within the open source RaftLib framework for
validation using a simple microbenchmark as well as two full streaming
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00593</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00593</id><created>2015-04-02</created><authors><author><keyname>Olivetti</keyname><forenames>Emanuele</forenames></author><author><keyname>Nguyen</keyname><forenames>Thien Bao</forenames></author><author><keyname>Avesani</keyname><forenames>Paolo</forenames></author></authors><title>The Approximation of the Dissimilarity Projection</title><categories>stat.ML cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3D
pathways of axons within the white matter of the brain as a tractography. The
analysis of tractographies has drawn attention from the machine learning and
pattern recognition communities providing novel challenges such as finding an
appropriate representation space for the data. Many of the current learning
algorithms require the input to be from a vectorial space. This requirement
contrasts with the intrinsic nature of the tractography because its basic
elements, called streamlines or tracks, have different lengths and different
number of points and for this reason they cannot be directly represented in a
common vectorial space. In this work we propose the adoption of the
dissimilarity representation which is an Euclidean embedding technique defined
by selecting a set of streamlines called prototypes and then mapping any new
streamline to the vector of distances from prototypes. We investigate the
degree of approximation of this projection under different prototype selection
policies and prototype set sizes in order to characterise its use on
tractography data. Additionally we propose the use of a scalable approximation
of the most effective prototype selection policy that provides fast and
accurate dissimilarity approximations of complete tractographies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00596</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00596</id><created>2015-04-02</created><updated>2015-04-15</updated><authors><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author><author><keyname>Vu</keyname><forenames>Dominik K.</forenames></author></authors><title>Extremal properties of flood-filling games</title><categories>math.CO cs.DM</categories><comments>Discussion of a further open problem added to conclusions section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of determining the number of &quot;flooding operations&quot; required to
make a given coloured graph monochromatic in the one-player combinatorial game
Flood-It has been studied extensively from an algorithmic point of view, but
basic questions about the maximum number of moves that might be required in the
worst case remain unanswered. We begin a systematic investigation of such
questions, with the goal of determining, for a given graph, the maximum number
of moves that may be required, taken over all possible colourings. We give two
upper bounds on this quantity for arbitrary graphs, which we show to be tight
for particular classes of graphs, and determine this maximum number of moves
exactly when the underlying graph is a path, cycle, or a blow-up of a path or
cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00601</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00601</id><created>2015-04-02</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Oblivious Transfer Protocol with Verification</title><categories>cs.CR</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although random sequences can be used to generate probability events, they
come with the risk of cheating in an unsupervised situation. In such cases, the
oblivious transfer protocol may be used and this paper presents a variation to
the DH key-exchange to serve as this protocol. A method to verify the
correctness of the procedure, without revealing the random numbers used by the
two parties, is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00616</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00616</id><created>2015-04-02</created><authors><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Peternek</keyname><forenames>Fabian</forenames></author></authors><title>A Survey on Methods and Systems for Graph Compression</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an informal survey (meant to accompany another paper) on graph
compression methods. We focus on lossless methods, briefly list available
pproaches, and compare them where possible or give some indicators on their
compression ratios. We also mention some relevant results from the field of
lossy compression and algorithms specialized for the use on large graphs. ---
Note: The comparison is by no means complete. This document is a first draft
and will be updated and extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00619</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00619</id><created>2015-04-02</created><authors><author><keyname>Ambrosin</keyname><forenames>Moreno</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Dargahi</keyname><forenames>Tooska</forenames></author></authors><title>On the Feasibility of Attribute-Based Encryption on Smartphone Devices</title><categories>cs.CR</categories><comments>Accepted at the 1st International Workshop on IoT challenges in
  Mobile and Industrial Systems (IoT-Sys 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute-Based Encryption (ABE) is a powerful cryptographic tool that allows
fine-grained access control over data. Due to its features, ABE has been
adopted in several applications, such as encrypted storage or access control
systems. Recently, researchers argued about the non acceptable performance of
ABE when implemented on mobile devices. Indeed, the non feasibility of ABE on
mobile devices would hinder the deployment of novel protocols and
services--that could instead exploit the full potential of such devices.
However, we believe the conclusion of non usability was driven by a not-very
efficient implementation.
  In this paper, we want to shine a light on this concern by studying the
feasibility of applying ABE on smartphone devices. In particular, we
implemented AndrABEn, an ABE library for Android operating system. Our library
is written in the C language and implements two main ABE schemes:
Ciphertext-Policy Attribute-Based Encryption, and Key- Policy Attribute-Based
Encryption. We also run a thorough set of experimental evaluation for AndrABEn,
and compare it with the current state-of-the-art (considering the same
experimental setting). The results confirm the possibility to effectively use
ABE on smartphone devices, requiring an acceptable amount of resources in terms
of computations and energy consumption. Since the current state-of-the-art
claims the non feasibility of ABE on mobile devices, we believe that our study
(together with the AndrABEn library that we made available online) is a key
result that will pave the way for researchers and developers to design and
implement novel protocols and applications for mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00627</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00627</id><created>2015-04-02</created><updated>2015-05-15</updated><authors><author><keyname>Shepherd</keyname><forenames>F. Bruce</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>The Inapproximability of Maximum Single-Sink Unsplittable, Priority and
  Confluent Flow Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider single-sink network flow problems. An instance consists of a
capacitated graph (directed or undirected), a sink node $t$ and a set of
demands that we want to send to the sink. Here demand $i$ is located at a node
$s_i$ and requests an amount $d_i$ of flow capacity in order to route
successfully. Two standard objectives are to maximise (i) the number of demands
(cardinality) and (ii) the total demand (throughput) that can be routed subject
to the capacity constraints. Furthermore, we examine these maximisation
problems for three specialised types of network flow: unsplittable, confluent
and priority flows.
  In the {\em unsplittable flow} problem (UFP), we have edge capacities, and
the demand for $s_i$ must be routed on a single path. In the {\em confluent
flow} problem, we have node capacities, and the final flow must induce a tree.
Both of these problems have been studied extensively, primarily in the
single-sink setting. However, most of this work imposed the {\em no-bottleneck
assumption} (that the maximum demand $d_{max}$ is at most the minimum capacity
$u_{min}$). Given the no-bottleneck assumption (NBA), there is a factor
$4.43$-approximation algorithm due to Dinitz et al. for the unsplittable flow
problem. Under the stronger assumption of uniform capacities, there is a factor
$3$-approximation algorithm due to Chen et al. for the confluent flow problem.
However, unlike the UFP, we show that a constant factor approximation algorithm
cannot be obtained for the single-sink confluent flows even {\bf with} the NBA.
  Without NBA, we show that maximum cardinality single-sink UFP is hard to
approximate to within a factor $n^{.5-\epsilon}$ even when all demands lie in a
small interval $[1,1+\Delta]$ where $\Delta&gt;0$ (but has polynomial input size).
This is very sharp since when $\Delta=0$, this becomes a maximum flow problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00629</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00629</id><created>2015-04-02</created><updated>2015-04-03</updated><authors><author><keyname>Mukherjee</keyname><forenames>Manuj</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>The communication complexity of achieving SK capacity in a class of PIN
  models</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication complexity of achieving secret key (SK) capacity in the
multiterminal source model of Csisz$\'a$r and Narayan is the minimum rate of
public communication required to generate a maximal-rate SK. It is well known
that the minimum rate of communication for omniscience, denoted by
$R_{\text{CO}}$, is an upper bound on the communication complexity, denoted by
$R_{\text{SK}}$. A source model for which this upper bound is tight is called
$R_{\text{SK}}$-maximal. In this paper, we establish a sufficient condition for
$R_{\text{SK}}$-maximality within the class of pairwise independent network
(PIN) models defined on hypergraphs. This allows us to compute $R_{\text{SK}}$
exactly within the class of PIN models satisfying this condition. On the other
hand, we also provide a counterexample that shows that our condition does not
in general guarantee $R_{\text{SK}}$-maximality for sources beyond PIN models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00639</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00639</id><created>2015-04-02</created><authors><author><keyname>Liu</keyname><forenames>Qingzhi</forenames></author><author><keyname>Goli&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author></authors><title>Green Wireless Power Transfer Networks</title><categories>cs.NI</categories><comments>submitted for possible publication.
  http://www.es.ewi.tudelft.nl/reports/ES-2015-01.pdf</comments><report-no>TU Delft ES-2015-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wireless Power Transfer Network (WPTN) aims to support devices with
cable-less energy on-demand. Unfortunately, wireless power transfer
itself-especially through radio frequency radiation rectification-is fairly
inefficient due to decaying power with distance, antenna polarization, etc.
Consequently, idle charging needs to be minimized to reduce already large costs
of providing energy to the receivers and at the same time reduce the carbon
footprint of WPTNs. In turn, energy saving in a WPTN can be boosted by simply
switching off the energy transmitter when the received energy is too weak for
rectification. Therefore in this paper we propose, and experimentally evaluate,
two &quot;green&quot; protocols for the control plane of static charger/mobile receiver
WPTN aimed at optimizing the charger workflow to make WPTN green. Those
protocols are: 'beaconing', where receivers advertise their presence to WPTN,
and 'probing' exploiting the receiver feedback from WTPN on the level of
received energy. We demonstrate that both protocols reduce the unnecessary WTPN
uptime, however trading it for the reduced energy provision, compared to the
base case of 'WPTN charger always on'. For example, our system (in our
experiments) saves at most approx. 80% of energy and increases 5.5 times the
efficiency with only approx. 17% less energy possibly harvested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00641</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00641</id><created>2015-04-02</created><authors><author><keyname>Patel</keyname><forenames>Ankit B.</forenames></author><author><keyname>Nguyen</keyname><forenames>Tan</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>A Probabilistic Theory of Deep Learning</title><categories>stat.ML cs.CV cs.LG cs.NE</categories><comments>56 pages, 6 figures, 2 tables</comments><report-no>Rice University Electrical and Computer Engineering Dept. Technical
  Report No 2015-1</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A grand challenge in machine learning is the development of computational
algorithms that match or outperform humans in perceptual inference tasks that
are complicated by nuisance variation. For instance, visual object recognition
involves the unknown object position, orientation, and scale in object
recognition while speech recognition involves the unknown voice pronunciation,
pitch, and speed. Recently, a new breed of deep learning algorithms have
emerged for high-nuisance inference tasks that routinely yield pattern
recognition systems with near- or super-human capabilities. But a fundamental
question remains: Why do they work? Intuitions abound, but a coherent framework
for understanding, analyzing, and synthesizing deep learning architectures has
remained elusive. We answer this question by developing a new probabilistic
framework for deep learning based on the Deep Rendering Model: a generative
probabilistic model that explicitly captures latent nuisance variation. By
relaxing the generative model to a discriminative one, we can recover two of
the current leading deep learning systems, deep convolutional neural networks
and random decision forests, providing insights into their successes and
shortcomings, as well as a principled route to their improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00653</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00653</id><created>2015-04-02</created><updated>2016-01-18</updated><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author></authors><title>Scalable Constrained Clustering: A Generalized Spectral Method</title><categories>cs.SI</categories><comments>this paper is superseded by the article &quot;Scalable Constrained
  Clustering: A Generalized Spectral Method&quot; authored by M. Cucuring, I.
  Koutis, S. Chawla, G. Miller and R. Peng</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a principled spectral approach to the well-studied constrained
clustering problem. It reduces clustering to a generalized eigenvalue problem
on Laplacians. The method works in nearly-linear time and provides concrete
guarantees for the quality of the clusters, at least for the case of 2-way
partitioning. In practice this translates to a very fast implementation that
consistently outperforms existing spectral approaches. We support this claim
with experiments on various data sets: our approach recovers correct clusters
in examples where previous methods fail, and handles data sets with millions of
data points - two orders of magnitude larger than before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00657</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00657</id><created>2015-04-02</created><updated>2015-08-24</updated><authors><author><keyname>Fairchild</keyname><forenames>Geoffrey</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM, USA</affiliation><affiliation>The University of Iowa, Iowa City, IA, USA</affiliation></author><author><keyname>De Silva</keyname><forenames>Lalindra</forenames><affiliation>The University of Utah, Salt Lake City, UT, USA</affiliation></author><author><keyname>Del Valle</keyname><forenames>Sara Y.</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM, USA</affiliation></author><author><keyname>Segre</keyname><forenames>Alberto M.</forenames><affiliation>The University of Iowa, Iowa City, IA, USA</affiliation></author></authors><title>Eliciting Disease Data from Wikipedia Articles</title><categories>cs.IR cs.CL cs.SI q-bio.PE</categories><comments>9 pages, 3 figures, 4 tables, accepted to 2015 ICWSM Wikipedia
  workshop; v2 includes author formatting fixes and a few sentences removed to
  make it 8 pages (although arXiv renders it as 9); v3 uses embedded type 1
  fonts in the figures and title-cases the title (required by AAAI); v4 fixes
  typo in abstract</comments><report-no>LA-UR-15-22528</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional disease surveillance systems suffer from several disadvantages,
including reporting lags and antiquated technology, that have caused a movement
towards internet-based disease surveillance systems. Internet systems are
particularly attractive for disease outbreaks because they can provide data in
near real-time and can be verified by individuals around the globe. However,
most existing systems have focused on disease monitoring and do not provide a
data repository for policy makers or researchers. In order to fill this gap, we
analyzed Wikipedia article content.
  We demonstrate how a named-entity recognizer can be trained to tag case
counts, death counts, and hospitalization counts in the article narrative that
achieves an F1 score of 0.753. We also show, using the 2014 West African Ebola
virus disease epidemic article as a case study, that there are detailed time
series data that are consistently updated that closely align with ground truth
data.
  We argue that Wikipedia can be used to create the first community-driven
open-source emerging disease detection, monitoring, and repository system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00680</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00680</id><created>2015-04-02</created><authors><author><keyname>Cheng</keyname><forenames>Justin</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Antisocial Behavior in Online Discussion Communities</title><categories>cs.SI cs.CY stat.AP stat.ML</categories><comments>ICWSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User contributions in the form of posts, comments, and votes are essential to
the success of online communities. However, allowing user participation also
invites undesirable behavior such as trolling. In this paper, we characterize
antisocial behavior in three large online discussion communities by analyzing
users who were banned from these communities. We find that such users tend to
concentrate their efforts in a small number of threads, are more likely to post
irrelevantly, and are more successful at garnering responses from other users.
Studying the evolution of these users from the moment they join a community up
to when they get banned, we find that not only do they write worse than other
users over time, but they also become increasingly less tolerated by the
community. Further, we discover that antisocial behavior is exacerbated when
community feedback is overly harsh. Our analysis also reveals distinct groups
of users with different levels of antisocial behavior that can change over
time. We use these insights to identify antisocial users early on, a task of
high practical importance to community maintainers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00681</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00681</id><created>2015-04-02</created><updated>2015-04-05</updated><authors><author><keyname>Kindler</keyname><forenames>Guy</forenames></author><author><keyname>Kolla</keyname><forenames>Alexandra</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Approximation of non-boolean 2CSP</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a polynomial time $\Omega\left ( \frac 1R \log R \right)$
approximate algorithm for Max 2CSP-$R$, the problem where we are given a
collection of constraints, each involving two variables, where each variable
ranges over a set of size $R$, and we want to find an assignment to the
variables that maximizes the number of satisfied constraints. Assuming the
Unique Games Conjecture, this is the best possible approximation up to constant
factors.
  Previously, a $1/R$-approximate algorithm was known, based on linear
programming. Our algorithm is based on semidefinite programming (SDP) and on a
novel rounding technique. The SDP that we use has an almost-matching
integrality gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00686</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00686</id><created>2015-04-02</created><authors><author><keyname>Kwok</keyname><forenames>Tsz Chiu</forenames></author><author><keyname>Lau</keyname><forenames>Lap Chi</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author></authors><title>Improved Cheeger's Inequality and Analysis of Local Graph Partitioning
  using Vertex Expansion and Expansion Profile</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two generalizations of the Cheeger's inequality. The first
generalization relates the second eigenvalue to the edge expansion and the
vertex expansion of the graph G, $\lambda_2 = \Omega(\phi^V(G) \phi(G))$, where
$\phi^V(G)$ denotes the robust vertex expansion of G and $\phi(G)$ denotes the
edge expansion of G. The second generalization relates the second eigenvalue to
the edge expansion and the expansion profile of G, for all $k \ge 2$,
$\lambda_2 = \Omega(\phi_k(G) \phi(G) / k)$, where $\phi_k(G)$ denotes the
k-way expansion of G. These show that the spectral partitioning algorithm has
better performance guarantees when $\phi^V(G)$ is large (e.g. planted random
instances) or $\phi_k(G)$ is large (instances with few disjoint non-expanding
sets). Both bounds are tight up to a constant factor.
  Our approach is based on a method to analyze solutions of Laplacian systems,
and this allows us to extend the results to local graph partitioning
algorithms. In particular, we show that our approach can be used to analyze
personal pagerank vectors, and to give a local graph partitioning algorithm for
the small-set expansion problem with performance guarantees similar to the
generalizations of Cheeger's inequality. We also present a spectral approach to
prove similar results for the truncated random walk algorithm. These show that
local graph partitioning algorithms almost match the performance of the
spectral partitioning algorithm, with the additional advantages that they apply
to the small-set expansion problem and their running time could be sublinear.
Our techniques provide common approaches to analyze the spectral partitioning
algorithm and local graph partitioning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00693</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00693</id><created>2015-04-02</created><authors><author><keyname>Alomari</keyname><forenames>Zakaria</forenames></author><author><keyname>Halimi</keyname><forenames>Oualid El</forenames></author><author><keyname>Sivaprasad</keyname><forenames>Kaushik</forenames></author><author><keyname>Pandit</keyname><forenames>Chitrang</forenames></author></authors><title>Comparative Studies of Six Programming Languages</title><categories>cs.PL</categories><comments>arXiv admin note: text overlap with arXiv:1008.3434 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparison of programming languages is a common topic of discussion among
software engineers. Multiple programming languages are designed, specified, and
implemented every year in order to keep up with the changing programming
paradigms, hardware evolution, etc. In this paper we present a comparative
study between six programming languages: C++, PHP, C#, Java, Python, VB ; These
languages are compared under the characteristics of reusability, reliability,
portability, availability of compilers and tools, readability, efficiency,
familiarity and expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00695</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00695</id><created>2015-04-02</created><authors><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Lachish</keyname><forenames>Oded</forenames></author><author><keyname>Vasudev</keyname><forenames>Yadu</forenames></author></authors><title>Trading query complexity for sample-based testing and multi-testing
  scalability</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show here that every non-adaptive property testing algorithm making a
constant number of queries, over a fixed alphabet, can be converted to a
sample-based (as per [Goldreich and Ron, 2015]) testing algorithm whose average
number of queries is a fixed, smaller than $1$, power of $n$. Since the query
distribution of the sample-based algorithm is not dependent at all on the
property, or the original algorithm, this has many implications in scenarios
where there are many properties that need to be tested for concurrently, such
as testing (relatively large) unions of properties, or converting a
Merlin-Arthur Proximity proof (as per [Gur and Rothblum, 2013]) to a proper
testing algorithm.
  The proof method involves preparing the original testing algorithm for a
combinatorial analysis, which in turn involves a new result about the existence
of combinatorial structures (essentially generalized sunflowers) that allow the
sample-based tester to replace the original constant query complexity tester.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00702</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00702</id><created>2015-04-02</created><updated>2015-12-07</updated><authors><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>End-to-End Training of Deep Visuomotor Policies</title><categories>cs.LG cs.CV cs.RO</categories><comments>extended version including supporting material, minor formatting
  fixes, fixes to typos, and some technical clarifications</comments><report-no>BVLC Report Series #100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy search methods can allow robots to learn control policies for a wide
range of tasks, but practical applications of policy search often require
hand-engineered components for perception, state estimation, and low-level
control. In this paper, we aim to answer the following question: does training
the perception and control systems jointly end-to-end provide better
performance than training each component separately? To this end, we develop a
method that can be used to learn policies that map raw image observations
directly to torques at the robot's motors. The policies are represented by deep
convolutional neural networks (CNNs) with 92,000 parameters, and are trained
using a partially observed guided policy search method, which transforms policy
search into supervised learning, with supervision provided by a simple
trajectory-centric reinforcement learning method. We evaluate our method on a
range of real-world manipulation tasks that require close coordination between
vision and control, such as screwing a cap onto a bottle, and present simulated
comparisons to a range of prior policy search methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00703</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00703</id><created>2015-04-02</created><updated>2015-07-20</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Brown-Cohen</keyname><forenames>Jonah</forenames></author><author><keyname>Huq</keyname><forenames>Arefin</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Roy</keyname><forenames>Aurko</forenames></author><author><keyname>Weitz</keyname><forenames>Benjamin</forenames></author><author><keyname>Zink</keyname><forenames>Daniel</forenames></author></authors><title>The matching problem has no small symmetric SDP</title><categories>cs.CC</categories><comments>17 pages</comments><msc-class>68Q17, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yannakakis showed that the matching problem does not have a small symmetric
linear program. Rothvo{\ss} recently proved that any, not necessarily
symmetric, linear program also has exponential size. It is natural to ask
whether the matching problem can be expressed compactly in a framework such as
semidefinite programming (SDP) that is more powerful than linear programming
but still allows efficient optimization. We answer this question negatively for
symmetric SDPs: any symmetric SDP for the matching problem has exponential
size.
  We also show that an O(k)-round Lasserre SDP relaxation for the metric
traveling salesperson problem yields at least as good an approximation as any
symmetric SDP relaxation of size $n^k$.
  The key technical ingredient underlying both these results is an upper bound
on the degree needed to derive polynomial identities that hold over the space
of matchings or traveling salesperson tours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00704</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00704</id><created>2015-04-02</created><authors><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Grbovic</keyname><forenames>Mihajlo</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Mantrach</keyname><forenames>Amin</forenames></author></authors><title>Evolution of Conversations in the Age of Email Overload</title><categories>cs.SI</categories><comments>11 page, 24th International World Wide Web Conference</comments><acm-class>H.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Email is a ubiquitous communications tool in the workplace and plays an
important role in social interactions. Previous studies of email were largely
based on surveys and limited to relatively small populations of email users
within organizations. In this paper, we report results of a large-scale study
of more than 2 million users exchanging 16 billion emails over several months.
We quantitatively characterize the replying behavior in conversations within
pairs of users. In particular, we study the time it takes the user to reply to
a received message and the length of the reply sent. We consider a variety of
factors that affect the reply time and length, such as the stage of the
conversation, user demographics, and use of portable devices. In addition, we
study how increasing load affects emailing behavior. We find that as users
receive more email messages in a day, they reply to a smaller fraction of them,
using shorter replies. However, their responsiveness remains intact, and they
may even reply to emails faster. Finally, we predict the time to reply, length
of reply, and whether the reply ends a conversation. We demonstrate
considerable improvement over the baseline in all three prediction tasks,
showing the significant role that the factors that we uncover play, in
determining replying behavior. We rank these factors based on their predictive
power. Our findings have important implications for understanding human
behavior and designing better email management applications for tasks like
ranking unread emails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00717</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00717</id><created>2015-04-02</created><authors><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author></authors><title>Super-Resolution of Positive Sources: the Discrete Setup</title><categories>cs.IT math.IT math.NA math.OC</categories><comments>31 page, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In single-molecule microscopy it is necessary to locate with high precision
point sources from noisy observations of the spectrum of the signal at
frequencies capped by $f_c$, which is just about the frequency of natural
light. This paper rigorously establishes that this super-resolution problem can
be solved via linear programming in a stable manner. We prove that the quality
of the reconstruction crucially depends on the Rayleigh regularity of the
support of the signal; that is, on the maximum number of sources that can occur
within a square of side length about $1/f_c$. The theoretical performance
guarantee is complemented with a converse result showing that our simple convex
program convex is nearly optimal. Finally, numerical experiments illustrate our
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00719</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00719</id><created>2015-04-02</created><authors><author><keyname>Drumwright</keyname><forenames>Evan</forenames></author></authors><title>Rapidly computable viscous friction and no-slip rigid contact models</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents computationally efficient algorithms for modeling two
special cases of rigid contact---contact with only viscous friction and contact
without slip---that have particularly useful applications in robotic locomotion
and grasping. Modeling rigid contact with Coulomb friction generally exhibits
$O(n^3)$ expected time complexity in the number of contact points and
$2^{O(n)}$ worst-case complexity. The special cases we consider exhibit $O(m^3
+ m^2n)$ time complexity ($m$ is the number of independent coordinates in the
multi rigid body system) in the expected case and polynomial complexity in the
worst case; thus, asymptotic complexity is no longer driven by number of
contact points (which is conceivably limitless) but instead is more dependent
on the number of bodies in the system (which is often fixed). These special
cases also require considerably fewer constrained nonlinear optimization
variables thus yielding substantial improvements in running time. Finally,
these special cases also afford one other advantage: the nonlinear optimization
problems are numerically easier to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00724</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00724</id><created>2015-04-02</created><authors><author><keyname>Cavraro</keyname><forenames>Guido</forenames></author><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author></authors><title>Data-Driven Approach for Distribution Network Topology Detection</title><categories>cs.SY</categories><comments>5 Pages, Submitted to IEEE PES GM 2015, Denver, CO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a data-driven approach to detect the switching actions
and topology transitions in distribution networks. It is based on the real time
analysis of time-series voltages measurements. The analysis approach draws on
data from high-precision phasor measurement units ($\mu$PMUs or synchrophasors)
for distribution networks. The key fact is that time-series measurement data
taken from the distribution network has specific patterns representing state
transitions such as topology changes. The proposed algorithm is based on
comparison of actual voltage measurements with a library of signatures derived
from the possible topologies simulation. The IEEE 33-bus model is used for the
algorithm validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00736</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00736</id><created>2015-04-02</created><authors><author><keyname>Du</keyname><forenames>Liang</forenames></author><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author></authors><title>Unsupervised Feature Selection with Adaptive Structure Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of feature selection has raised considerable interests in the
past decade. Traditional unsupervised methods select the features which can
faithfully preserve the intrinsic structures of data, where the intrinsic
structures are estimated using all the input features of data. However, the
estimated intrinsic structures are unreliable/inaccurate when the redundant and
noisy features are not removed. Therefore, we face a dilemma here: one need the
true structures of data to identify the informative features, and one need the
informative features to accurately estimate the true structures of data. To
address this, we propose a unified learning framework which performs structure
learning and feature selection simultaneously. The structures are adaptively
learned from the results of feature selection, and the informative features are
reselected to preserve the refined structures of data. By leveraging the
interactions between these two essential tasks, we are able to capture accurate
structures and select more informative features. Experimental results on many
benchmark data sets demonstrate that the proposed method outperforms many state
of the art unsupervised feature selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00744</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00744</id><created>2015-04-03</created><updated>2015-04-07</updated><authors><author><keyname>Derakhshandeh</keyname><forenames>Zahra</forenames><affiliation>Arizona State University, USA</affiliation></author><author><keyname>Gmyr</keyname><forenames>Robert</forenames><affiliation>University of Paderborn, Germany</affiliation></author><author><keyname>Richa</keyname><forenames>Andrea W.</forenames><affiliation>Arizona State University, USA</affiliation></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames><affiliation>University of Paderborn, Germany</affiliation></author><author><keyname>Strothmann</keyname><forenames>Thim</forenames><affiliation>University of Paderborn, Germany</affiliation></author></authors><title>An Algorithmic Framework for Shape Formation Problems in Self-Organizing
  Particle Systems</title><categories>cs.ET</categories><comments>Corrected typos. Algorithms and results unchanged. arXiv admin note:
  text overlap with arXiv:1503.07991</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many proposals have already been made for realizing programmable matter,
ranging from shape-changing molecules, DNA tiles, and synthetic cells to
reconfigurable modular robotics. Envisioning systems of nano-sensors devices,
we are particularly interested in programmable matter consisting of systems of
simple computational elements, called particles, that can establish and release
bonds and can actively move in a self-organized way, and in shape formation
problems relevant for programmable matter in those self-organizing particle
systems (SOPS). In this paper, we present a general algorithmic framework for
shape formation problems in SOPS, and show direct applications of this
framework to the problems of having the particle system self-organize to form a
hexagonal or triangular shape. Our algorithms utilize only local control,
require only constant-size memory particles, and are asymptotically optimal
both in terms of the total number of movements needed to reach the desired
shape configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00747</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00747</id><created>2015-04-03</created><authors><author><keyname>Jiang</keyname><forenames>He</forenames></author><author><keyname>Chen</keyname><forenames>Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Shuwei</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Kong</keyname><forenames>Weiqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author></authors><title>Software for Wearable Devices: Challenges and Opportunities</title><categories>cs.HC cs.CY</categories><comments>6 pages, 1 figure, for Compsac 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Wearable devices are a new form of mobile computer system that provides
exclusive and user-personalized services. Wearable devices bring new issues and
challenges to computer science and technology. This paper summarizes the
development process and the categories of wearable devices. In addition, we
present new key issues arising in aspects of wearable devices, including
operating systems, database management system, network communication protocol,
application development platform, privacy and security, energy consumption,
human-computer interaction, software engineering, and big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00757</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00757</id><created>2015-04-03</created><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Learning Mixed Membership Mallows Models from Pairwise Comparisons</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel parameterized family of Mixed Membership Mallows Models
(M4) to account for variability in pairwise comparisons generated by a
heterogeneous population of noisy and inconsistent users. M4 models individual
preferences as a user-specific probabilistic mixture of shared latent Mallows
components. Our key algorithmic insight for estimation is to establish a
statistical connection between M4 and topic models by viewing pairwise
comparisons as words, and users as documents. This key insight leads us to
explore Mallows components with a separable structure and leverage recent
advances in separable topic discovery. While separability appears to be overly
restrictive, we nevertheless show that it is an inevitable outcome of a
relatively small number of latent Mallows components in a world of large number
of items. We then develop an algorithm based on robust extreme-point
identification of convex polygons to learn the reference rankings, and is
provably consistent with polynomial sample complexity guarantees. We
demonstrate that our new model is empirically competitive with the current
state-of-the-art approaches in predicting real-world preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00766</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00766</id><created>2015-04-03</created><updated>2015-07-23</updated><authors><author><keyname>Ito</keyname><forenames>Hiro</forenames><affiliation>UEC, Japan and CREST, JST, Japan</affiliation></author></authors><title>Every property is testable on a natural class of scale-free multigraphs</title><categories>cs.DS</categories><comments>13 pages, one figure. Difference from ver. 1: Definitions of HSF and
  SF become more general. Typos were fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a natural class of multigraphs called
hierarchical-scale-free (HSF) multigraphs, and consider constant-time
testability on the class. We show that a very wide subclass, specifically, that
in which the power-law exponent is greater than two, of HSF is hyperfinite.
Based on this result, an algorithm for a deterministic partitioning oracle can
be constructed. We conclude by showing that every property is constant-time
testable on the above subclass of HSF. This algorithm utilizes findings by
Newman and Sohler of STOC'11. However, their algorithm is based on the
bounded-degree model, while it is known that actual scale-free networks usually
include hubs, which have a very large degree. HSF is based on scale-free
properties and includes such hubs. This is the first universal result of
constant-time testability on the general graph model, and it has the potential
to be applicable on a very wide range of scale-free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00770</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00770</id><created>2015-04-03</created><authors><author><keyname>Zhao</keyname><forenames>Mingxiong</forenames></author><author><keyname>Feng</keyname><forenames>Suili</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Fu</keyname><forenames>Hao</forenames></author></authors><title>Joint Power Splitting and Secure Beamforming Design in the
  Wireless-powered Untrusted Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to GlobeCom2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we maximize the secrecy rate of the wireless-powered untrusted
relay network by jointly designing power splitting (PS) ratio and relay
beamforming with the proposed global optimal algorithm (GOA) and local optimal
algorithm (LOA). Different from the literature, artificial noise (AN) sent by
the destination not only degrades the channel condition of the eavesdropper to
improve the secrecy rate, but also becomes a new source of energy powering the
untrusted relay based on PS. Hence, it is of high economic benefits and
efficiency to take advantage of AN compared with the literature. Simulation
results show that LOA can achieve satisfactory secrecy rate performance
compared with that of GOA, but with less computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00774</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00774</id><created>2015-04-03</created><updated>2015-07-23</updated><authors><author><keyname>Ito</keyname><forenames>Hiro</forenames><affiliation>UEC, Japan and CREST, JST, Japan</affiliation></author><author><keyname>Ueda</keyname><forenames>Takahiro</forenames><affiliation>Komatsu Ltd., Japan</affiliation></author></authors><title>How to solve the cake-cutting problem in sublinear time</title><categories>cs.DS</categories><comments>15 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show algorithms for solving the cake-cutting problem in
sublinear-time. More specifically, we preassign (simple) fair portions to o(n)
players in o(n)-time, and minimize the damage to the rest of the players. All
currently known algorithms require Omega(n)-time, even when assigning a portion
to just one player, and it is nontrivial to revise these algorithms to run in
$o(n)$-time since many of the remaining players, who have not been asked any
queries, may not be satisfied with the remaining cake. To challenge this
problem, we begin by providing a framework for solving the cake-cutting problem
in sublinear-time. Generally speaking, solving a problem in sublinear-time
requires the use of approximations. However, in our framework, we introduce the
concept of &quot;eps n-victims,&quot; which means that eps n players (victims) may not
get fair portions, where 0&lt; eps =&lt; 1 is an arbitrary constant. In our
framework, an algorithm consists of the following two parts: In the first
(Preassigning) part, it distributes fair portions to r &lt; n players in
o(n)-time. In the second (Completion) part, it distributes fair portions to the
remaining n-r players except for the eps n victims in poly}(n)-time. There are
two variations on the r players in the first part. Specifically, whether they
can or cannot be designated. We will then present algorithms in this framework.
In particular, an O(r/eps)-time algorithm for r =&lt; eps n/127 undesignated
players with eps n-victims, and an O~(r^2/eps)-time algorithm for r =&lt; eps
e^{{sqrt{ln{n}}}/{7}} designated players and eps =&lt; 1/e with eps n-victims are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00781</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00781</id><created>2015-04-03</created><authors><author><keyname>C</keyname><forenames>Dharmani Bhaveshkumar</forenames></author></authors><title>The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth
  Selection in Univariate and Multivariate Kernel Density Estimations</title><categories>cs.LG stat.CO stat.ME stat.ML</categories><comments>30 pages</comments><msc-class>11Kxx</msc-class><acm-class>I.5.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article derives a novel Gram-Charlier A (GCA) Series based Extended
Rule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation
(KDE). There are existing various bandwidth selection rules achieving
minimization of the Asymptotic Mean Integrated Square Error (AMISE) between the
estimated probability density function (PDF) and the actual PDF. The rules
differ in a way to estimate the integration of the squared second order
derivative of an unknown PDF $(f(\cdot))$, identified as the roughness
$R(f''(\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\cdot))$
with an assumption that the density being estimated is Gaussian. Intuitively,
better estimation of $R(f''(\cdot))$ and consequently better bandwidth
selection rules can be derived, if the unknown PDF is approximated through an
infinite series expansion based on a more generalized density assumption. As a
demonstration and verification to this concept, the ExROT derived in the
article uses an extended assumption that the density being estimated is near
Gaussian. This helps use of the GCA expansion as an approximation to the
unknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for
multivariate KDE. The required multivariate AMISE criteria is re-derived using
elementary calculus of several variables, instead of Tensor calculus. The
derivation uses the Kronecker product and the vector differential operator to
achieve the AMISE expression in vector notations. There is also derived ExROT
for kernel based density derivative estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00785</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00785</id><created>2015-04-03</created><authors><author><keyname>de Assuncao</keyname><forenames>Marcos Dias</forenames></author></authors><title>Enhanced Red-Black-Tree Data Structure for Facilitating the Scheduling
  of Reservations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details a data structure for managing and scheduling requests for
computing resources of clusters and virtualised infrastructure such as private
clouds. The data structure uses a red-black tree whose nodes represent the
start times and/or completion times of requests. The tree is enhanced by a
double-linked list that facilitates the iteration of nodes once the start time
of a request is determined by using the tree. We describe the data structure
main features, provide an example of use, and discuss experiments that
demonstrate that the average complexity of two operations are often below 10%
of their respective theoretical worst cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00788</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00788</id><created>2015-04-03</created><authors><author><keyname>Nasir</keyname><forenames>Muhammad Anis Uddin</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Garc&#xed;a-Soriano</keyname><forenames>David</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Serafini</keyname><forenames>Marco</forenames></author></authors><title>The Power of Both Choices: Practical Load Balancing for Distributed
  Stream Processing Engines</title><categories>cs.DC</categories><comments>31st IEEE International Conference on Data Engineering (ICDE), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of load balancing in distributed stream processing
engines, which is exacerbated in the presence of skew. We introduce Partial Key
Grouping (PKG), a new stream partitioning scheme that adapts the classical
&quot;power of two choices&quot; to a distributed streaming setting by leveraging two
novel techniques: key splitting and local load estimation. In so doing, it
achieves better load balancing than key grouping while being more scalable than
shuffle grouping. We test PKG on several large datasets, both real-world and
synthetic. Compared to standard hashing, PKG reduces the load imbalance by up
to several orders of magnitude, and often achieves nearly-perfect load balance.
This result translates into an improvement of up to 60% in throughput and up to
45% in latency when deployed on a real Storm cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00800</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00800</id><created>2015-04-03</created><updated>2015-11-28</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Rating alternatives from pairwise comparisons by solving tropical
  optimization problems</title><categories>math.OC cs.SY</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1503.04003</comments><msc-class>65K10 (Primary), 15A80, 41A50, 90B50, 91B08 (Secondary)</msc-class><journal-ref>12th Intern. Conf. on Fuzzy Systems and Knowledge Discovery (FSKD
  2015), pp. 162-167. ISBN 978-1-4673-7681-5</journal-ref><doi>10.1109/FSKD.2015.7381933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider problems of rating alternatives based on their pairwise
comparison under various assumptions, including constraints on the final scores
of alternatives. The problems are formulated in the framework of tropical
mathematics to approximate pairwise comparison matrices by reciprocal matrices
of unit rank, and written in a common form for both multiplicative and additive
comparison scales. To solve the unconstrained and constrained approximation
problems, we apply recent results in tropical optimization, which provide new
complete direct solutions given in a compact vector form. These solutions
extend known results and involve less computational effort. As an illustration,
numerical examples of rating alternatives are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00802</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00802</id><created>2015-04-03</created><authors><author><keyname>Gordienko</keyname><forenames>Yuri</forenames></author><author><keyname>Stirenko</keyname><forenames>Serhii</forenames></author><author><keyname>Gatsenko</keyname><forenames>Olexandr</forenames></author><author><keyname>Bekenov</keyname><forenames>Lev</forenames></author></authors><title>Science Gateway for Distributed Multiscale Course Management in
  e-Science and e-Learning - Use Case for Study and Investigation of
  Functionalized Nanomaterials</title><categories>cs.CY cond-mat.mtrl-sci</categories><comments>6 pages, 4 figures, 2 tables. 38th International Convention on
  Information and Communication Technology, Electronics and Microelectronics
  (MIPRO 2015); Distributed Computing, Visualization and Biomedical Engineering
  (DC VIS) May 25-29, 2015 (Opatija, Croatia)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current tendency of human learning and teaching is targeted to
development and integration of digital technologies (like cloud solutions,
mobile technology, learning analytics, big data, augmented reality, natural
interaction technologies, etc.). Our Science Gateway
(http://scigate.imp.kiev.ua) in collaboration with High Performance Computing
Center (http://hpcc.kpi.ua) is aimed on the close cooperation among the main
actors in learning and researching world (teachers, students, scientists,
supporting personnel, volunteers, etc.) with industry and academia to propose
the new frameworks and interoperability requirements for the building blocks of
a digital ecosystem for learning (including informal learning) that develops
and integrates the current and new tools and systems. It is the portal for
management of distributed courses (workflows), tools, resources, and users,
which is constructed on the basis of the Liferay framework and gUSE/WS-PGRADE
technology. It is based on development of multi-level approach (as to
methods/algorithms) for effective study and research through flexible selection
and combination of unified modules (&quot;gaming&quot; with modules as with LEGO-bricks).
It allows us to provide the flexible and adjustable framework with direct
involvement in real-world and scientific use cases motivated by the educational
aims of students and real scientific aims in labs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00806</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00806</id><created>2015-04-03</created><authors><author><keyname>Gordienko</keyname><forenames>Nikita</forenames></author><author><keyname>Lodygensky</keyname><forenames>Oleg</forenames></author><author><keyname>Fedak</keyname><forenames>Gilles</forenames></author><author><keyname>Gordienko</keyname><forenames>Yuri</forenames></author></authors><title>Synergy of Volunteer Measurements and Volunteer Computing for Effective
  Data Collecting, Processing, Simulating and Analyzing on a Worldwide Scale</title><categories>cs.CY cs.DC physics.ins-det</categories><comments>6 pages, 8 figures, 1 table. 38th International Convention on
  Information and Communication Technology, Electronics and Microelectronics
  (MIPRO 2015); Distributed Computing, Visualization and Biomedical Engineering
  (DC VIS) May 25-29, 2015 (Opatija, Croatia)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concerns the hype idea of &quot;Citizen Science&quot; and the related
paradigm shift: to go from the passive &quot;volunteer computing&quot; to other volunteer
actions like &quot;volunteer measurements&quot; under guidance of scientists. They can be
carried out by ordinary people with standard computing gadgets (smartphone,
tablet, etc.) and the various standard sensors in them. Here the special
attention is paid to the system of volunteer scientific measurements to study
air showers caused by cosmic rays. The technical implementation is based on
integration of data about registered night flashes (by radiometric software) in
shielded camera chip, synchronized time and GPS-data in ordinary gadgets: to
identify night &quot;air showers&quot; of elementary particles; to analyze the frequency
and to map the distribution of &quot;air showers&quot; in the densely populated cities.
The project currently includes the students of the National Technical
University of Ukraine &quot;KPI&quot;, which are compactly located in Kyiv city and
contribute their volunteer measurements. The technology would be very effective
for other applications also, especially if it will be automated (e.g., on the
basis of XtremWeb or/and BOINC technologies for distributed computing) and used
in some small area with many volunteers, e.g. in local communities
(Corporative/Community Crowd Computing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00822</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00822</id><created>2015-04-03</created><authors><author><keyname>Leverrier</keyname><forenames>Anthony</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>Quantum Expander Codes</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>2015 IEEE 56th Annual Symposium on Foundations of Computer Science
  (FOCS), pp. 810-824</journal-ref><doi>10.1109/FOCS.2015.55</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient decoding algorithm for constant rate quantum
hypergraph-product LDPC codes which provably corrects adversarial errors of
weight $\Omega(\sqrt{n})$ for codes of length $n$. The algorithm runs in time
linear in the number of qubits, which makes its performance the strongest to
date for linear-time decoding of quantum codes. The algorithm relies on
expanding properties, not of the quantum code's factor graph directly, but of
the factor graph of the original classical code it is constructed from.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00825</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00825</id><created>2015-04-03</created><authors><author><keyname>Mukhanov</keyname><forenames>Lev</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Dimitrios S.</forenames></author><author><keyname>de Supinski</keyname><forenames>Bronis R.</forenames></author></authors><title>ALEA: Fine-grain Energy Profiling with Basic Block Sampling</title><categories>cs.PF</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is an essential requirement for all contemporary computing
systems. We thus need tools to measure the energy consumption of computing
systems and to understand how workloads affect it. Significant recent research
effort has targeted direct power measurements on production computing systems
using on-board sensors or external instruments. These direct methods have in
turn guided studies of software techniques to reduce energy consumption via
workload allocation and scaling. Unfortunately, direct energy measurements are
hampered by the low power sampling frequency of power sensors. The coarse
granularity of power sensing limits our understanding of how power is allocated
in systems and our ability to optimize energy efficiency via workload
allocation.
  We present ALEA, a tool to measure power and energy consumption at the
granularity of basic blocks, using a probabilistic approach. ALEA provides
fine-grained energy profiling via statistical sampling, which overcomes the
limitations of power sensing instruments. Compared to state-of-the-art energy
measurement tools, ALEA provides finer granularity without sacrificing
accuracy. ALEA achieves low overhead energy measurements with mean error rates
between 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both
Intel and ARM platforms. The sampling method caps execution time overhead at
approximately 1%. ALEA is thus suitable for online energy monitoring and
optimization. Finally, ALEA is a user-space tool with a portable,
machine-independent sampling method. We demonstrate two use cases of ALEA,
where we reduce the energy consumption of a k-means computational kernel by 37%
and an ocean modelling code by 33%, compared to high-performance execution
baselines, by varying the power optimization strategy between basic blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00832</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00832</id><created>2015-04-03</created><authors><author><keyname>Ma</keyname><forenames>Chaofan</forenames></author><author><keyname>Liang</keyname><forenames>Wei</forenames></author><author><keyname>Zheng</keyname><forenames>Meng</forenames></author></authors><title>Set Covering-based Approximation Algorithm for Delay Constrained Relay
  Node Placement in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>11 pages, 12 figures</comments><msc-class>Communication networks</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Delay Constrained Relay Node Placement (DCRNP) problem in Wireless Sensor
Networks (WSNs) aims to deploy minimum relay nodes such that for each sensor
node there is a path connecting this sensor node to the sink without violating
delay constraint. As WSNs are gradually employed in time-critical applications,
the importance of the DCRNP problem becomes noticeable. For the NP-hard nature
of DCRNP problem, an approximation algorithm-Set-Covering-based Relay Node
Placement (SCA) is proposed to solve the DCRNP problem in this paper. The
proposed SCA algorithm deploys relay nodes iteratively from sink to the given
sensor nodes in hops, i.e., in the $k$th iteration SCA deploys relay nodes at
the locations that are $k$ hops apart from the sink. Specifically, in each
iteration, SCA first finds the candidate deployment locations located within 1
hop to the relay nodes and sensor nodes, which have already been connected to
the sink. Then, a subset of these candidate deployment locations, which can
guarantee the existence of paths connecting unconnected sensor nodes to the
sink within delay constraint, is selected to deploy relay nodes based on the
set covering method. As the iteration of SCA algorithm, the sensor nodes are
gradually connected to the sink with satisfying delay constraint.
  The elaborated analysis of the approximation ratio of SCA algorithm is given
out, and we also prove that the SCA is a polynomial time algorithm through
rigorous time complexity analysis. To evaluate the performance of the proposed
SCA algorithm, extensive simulations are implemented, and the simulation
results show that the SCA algorithm can significantly save the deployed relay
nodes comparing to the existing algorithms, i.e., at most 31.48% deployed relay
nodes can be saved due to SCA algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00834</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00834</id><created>2015-04-03</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>The complexity of computation in bit streams</title><categories>cs.CC cs.DS</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the complexity of online computation in the cell probe model. We
consider a class of problems where we are first given a fixed pattern or vector
$F$ of $n$ symbols and then one symbol arrives at a time in a stream. After
each symbol has arrived we must output some function of $F$ and the $n$-length
suffix of the arriving stream. Cell probe bounds of $\Omega(\delta\lg{n}/w)$
have previously been shown for both convolution and Hamming distance in this
setting, where $\delta$ is the size of a symbol in bits and
$w\in\Omega(\lg{n})$ is the cell size in bits. However, when $\delta$ is a
constant, as it is in many natural situations, these previous results no longer
give us non-trivial bounds.
  We introduce a new lop-sided information transfer proof technique which
enables us to prove meaningful lower bounds even for constant size input
alphabets. We use our new framework to prove an amortised cell probe lower
bound of $\Omega(\lg^2 n/(w\cdot \lg \lg n))$ time per arriving bit for an
online version of a well studied problem known as pattern matching with address
errors. This is the first non-trivial cell probe lower bound for any online
problem on bit streams that still holds when the cell sizes are large. We also
show the same bound for online convolution conditioned on a new combinatorial
conjecture related to Toeplitz matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00847</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00847</id><created>2015-04-03</created><authors><author><keyname>Hachem</keyname><forenames>Walid</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris</forenames></author><author><keyname>Pastur</keyname><forenames>Leonid</forenames></author></authors><title>The Shannon's mutual information of a multiple antenna time and
  frequency dependent channel: an ergodic operator approach</title><categories>cs.IT math-ph math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random non-centered multiple antenna radio transmission channel.
Assume that the deterministic part of the channel is itself frequency
selective, and that the random multipath part is represented by an ergodic
stationary vector process. In the Hilbert space $l^2({\mathbb Z})$, one can
associate to this channel a random ergodic self-adjoint operator having a
so-called Integrated Density of States (IDS). Shannon's mutual information per
receive antenna of this channel coincides then with the integral of a $\log$
function with respect to the IDS. In this paper, it is shown that when the
numbers of antennas at the transmitter and at the receiver tend to infinity at
the same rate, the mutual information per receive antenna tends to a quantity
that can be identified and, in fact, is closely related to that obtained within
the random matrix approach. This result can be obtained by analyzing the
behavior of the Stieltjes transform of the IDS in the regime of the large
numbers of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00854</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00854</id><created>2015-04-03</created><authors><author><keyname>Powers</keyname><forenames>David M. W.</forenames></author></authors><title>Evaluation Evaluation a Monte Carlo study</title><categories>cs.AI cs.CL stat.ML</categories><comments>5 pages, 14 Equations, 2 Figures, 1 Table, as submitted to European
  Conference on Artificial Intelligence (shorter version published with 2
  pages, 4 Equations, 0 Figures, 1 Table)</comments><journal-ref>ECAI 2008, pp.843-844</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade there has been increasing concern about the biases
embodied in traditional evaluation methods for Natural Language
Processing/Learning, particularly methods borrowed from Information Retrieval.
Without knowledge of the Bias and Prevalence of the contingency being tested,
or equivalently the expectation due to chance, the simple conditional
probabilities Recall, Precision and Accuracy are not meaningful as evaluation
measures, either individually or in combinations such as F-factor. The
existence of bias in NLP measures leads to the 'improvement' of systems by
increasing their bias, such as the practice of improving tagging and parsing
scores by using most common value (e.g. water is always a Noun) rather than the
attempting to discover the correct one. The measures Cohen Kappa and Powers
Informedness are discussed as unbiased alternative to Recall and related to the
psychologically significant measure DeltaP. In this paper we will analyze both
biased and unbiased measures theoretically, characterizing the precise
relationship between all these measures as well as evaluating the evaluation
measures themselves empirically using a Monte Carlo simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00874</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00874</id><created>2015-04-03</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Steering state statistics with output feedback</title><categories>math.OC cs.SY</categories><comments>10 pages, 2 figures</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a linear stochastic system whose initial state is a random vector
with a specified Gaussian distribution. Such a distribution may represent a
collection of particles abiding by the specified system dynamics. In recent
publications, we have shown that, provided the system is controllable, it is
always possible to steer the state covariance to any specified terminal
Gaussian distribution using state feedback. The purpose of the present work is
to show that, in the case where only partial state observation is available, a
necessary and sufficient condition for being able to steer the system to a
specified terminal Gaussian distribution for the state vector is that the
terminal state covariance be greater (in the positive-definite sense) than the
error covariance of a corresponding Kalman filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00905</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00905</id><created>2015-04-03</created><updated>2015-05-30</updated><authors><author><keyname>Lopez</keyname><forenames>Jose A.</forenames></author><author><keyname>Camps</keyname><forenames>Octavia</forenames></author><author><keyname>Sznaier</keyname><forenames>Mario</forenames></author></authors><title>Robust Anomaly Detection Using Semidefinite Programming</title><categories>math.OC cs.CV cs.LG cs.SY</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach, based on polynomial optimization and the
method of moments, to the problem of anomaly detection. The proposed technique
only requires information about the statistical moments of the normal-state
distribution of the features of interest and compares favorably with existing
approaches (such as Parzen windows and 1-class SVM). In addition, it provides a
succinct description of the normal state. Thus, it leads to a substantial
simplification of the the anomaly detection problem when working with higher
dimensional datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00907</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00907</id><created>2015-04-03</created><authors><author><keyname>Edwards</keyname><forenames>Essex</forenames></author><author><keyname>Bridson</keyname><forenames>Robert</forenames></author></authors><title>The Discretely-Discontinuous Galerkin Coarse Grid for Domain
  Decomposition</title><categories>cs.NA math.NA</categories><comments>19 pages, 5 figures</comments><msc-class>65N55</msc-class><acm-class>G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algebraic method for constructing a highly effective coarse
grid correction to accelerate domain decomposition. The coarse problem is
constructed from the original matrix and a small set of input vectors that span
a low-degree polynomial space, but no further knowledge of meshes or continuous
functionals is used. We construct a coarse basis by partitioning the problem
into subdomains and using the restriction of each input vector to each
subdomain as its own basis function. This basis resembles a Discontinuous
Galerkin basis on subdomain-sized elements. Constructing the coarse problem by
Galerkin projection, we prove a high-order convergent error bound for the
coarse solutions. Used in a two-level symmetric multiplicative overlapping
Schwarz preconditioner, the resulting conjugate gradient solver shows optimal
scaling. Convergence requires a constant number of iterations, independent of
fine problem size, on a range of scalar and vector-valued second-order and
fourth-order PDEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00923</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00923</id><created>2015-04-03</created><authors><author><keyname>Richardson</keyname><forenames>Fred</forenames></author><author><keyname>Reynolds</keyname><forenames>Douglas</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author></authors><title>A Unified Deep Neural Network for Speaker and Language Recognition</title><categories>cs.CL cs.CV cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learned feature representations and sub-phoneme posteriors from Deep Neural
Networks (DNNs) have been used separately to produce significant performance
gains for speaker and language recognition tasks. In this work we show how
these gains are possible using a single DNN for both speaker and language
recognition. The unified DNN approach is shown to yield substantial performance
improvements on the the 2013 Domain Adaptation Challenge speaker recognition
task (55% reduction in EER for the out-of-domain condition) and on the NIST
2011 Language Recognition Evaluation (48% reduction in EER for the 30s test
condition).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00931</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00931</id><created>2015-04-01</created><authors><author><keyname>Reid</keyname><forenames>Greg</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Wolkowicz</keyname><forenames>Henry</forenames></author><author><keyname>Wu</keyname><forenames>Wenyuan</forenames></author></authors><title>Facial Reduction and SDP Methods for Systems of Polynomial Equations</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real radical ideal of a system of polynomials with finitely many complex
roots is generated by a system of real polynomials having only real roots and
free of multiplicities. It is a central object in computational real algebraic
geometry and important as a preconditioner for numerical solvers. Lasserre and
co-workers have shown that the real radical ideal of real polynomial systems
with finitely many real solutions can be determined by a combination of
semi-definite programming (SDP) and geometric involution techniques. A
conjectured extension of such methods to positive dimensional polynomial
systems has been given recently by Ma, Wang and Zhi.
  We show that regularity in the form of the Slater constraint qualification
(strict feasibility) fails for the resulting SDP feasibility problems. Facial
reduction is then a popular technique whereby SDP problems that fail strict
feasibility can be regularized by projecting onto a face of the convex cone of
semi-definite problems.
  In this paper we introduce a framework for combining facial reduction with
such SDP methods for analyzing $0$ and positive dimensional real ideals of real
polynomial systems. The SDP methods are implemented in MATLAB and our geometric
involutive form is implemented in Maple. We use two approaches to find a
feasible moment matrix. We use an interior point method within the CVX package
for MATLAB and also the Douglas-Rachford (DR) projection-reflection method.
  Illustrative examples show the advantages of the DR approach for some
problems over standard interior point methods. We also see the advantage of
facial reduction both in regularizing the problem and also in reducing the
dimension of the moment matrices. Problems requiring more than one facial
reduction are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00932</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00932</id><created>2015-04-03</created><authors><author><keyname>de Almeida</keyname><forenames>Lirio Onofre Baptista</forenames></author><author><keyname>Matias</keyname><forenames>Paulo</forenames></author><author><keyname>Guariento</keyname><forenames>Rafael Tuma</forenames></author></authors><title>An embedded system for real-time feedback neuroscience experiments</title><categories>q-bio.QM cs.OH</categories><comments>17 pages, 11 figures, IV Brazilian Symposium on Computing Systems
  Engineering</comments><doi>10.13140/RG.2.1.4077.7769</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A complete data acquisition and signal output control system for synchronous
stimuli generation, geared towards in vivo neuroscience experiments, was
developed using the Terasic DE2i-150 board. All emotions and thoughts are an
emergent property of the chemical and electrical activity of neurons. Most of
these cells are regarded as excitable cells (spiking neurons), which produce
temporally localized electric patterns (spikes). Researchers usually consider
that only the instant of occurrence (timestamp) of these spikes encodes
information. Registering neural activity evoked by stimuli demands timing
determinism and data storage capabilities that cannot be met without dedicated
hardware and a hard real-time operational system (RTOS). Indeed, research in
neuroscience usually requires dedicated electronic instrumentation for studies
in neural coding, brain machine interfaces and closed loop in vivo or in vitro
experiments. We developed a complete embedded system solution consisting of a
hardware/software co-design with the Intel Atom processor running a free RTOS
and a FPGA communicating via a PCIe-to-Avalon bridge. Our system is capable of
registering input event timestamps with 1{\mu}s precision and digitally
generating stimuli output in hard real-time. The whole system is controlled by
a Linux-based Graphical User Interface (GUI). Collected results are
simultaneously saved in a local file and broadcasted wirelessly to mobile
device web-browsers in an user-friendly graphic format, enhanced by HTML5
technology. The developed system is low-cost and highly configurable, enabling
various neuroscience experimental setups, while the commercial off-the-shelf
systems have low availability and are less flexible to adapt to specific
experimental configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00934</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00934</id><created>2015-04-03</created><updated>2015-09-15</updated><authors><author><keyname>Marshoud</keyname><forenames>Hanaa</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author></authors><title>Non-Orthogonal Multiple Access for Visible Light Communications</title><categories>cs.IT cs.ET math.IT</categories><comments>Published in IEEE Photonics Technology Letters. 4 pages, 5 figures</comments><journal-ref>IEEE Photonics Technology Letters, vol. 28, no. 1, pp. 51-54, Jan.
  2016</journal-ref><doi>10.1109/LPT.2015.2479600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main limitation of visible light communication (VLC) is the narrow
modulation bandwidth, which reduces the achievable data rates. In this paper,
we apply the non-orthogonal multiple access (NOMA) scheme to enhance the
achievable throughput in high-rate VLC downlink networks. We first propose a
novel gain ratio power allocation (GRPA) strategy that takes into account the
users' channel conditions to ensure efficient and fair power allocation. Our
results indicate that GRPA significantly enhances system performance compared
to the static power allocation. We also study the effect of tuning the
transmission angles of the light emitting diodes (LEDs) and the field of views
(FOVs) of the receivers, and demonstrate that these parameters can offer new
degrees of freedom to boost NOMA performance. Simulation results reveal that
NOMA is a promising multiple access scheme for the downlink of VLC networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00941</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00941</id><created>2015-04-03</created><updated>2015-04-07</updated><authors><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author><author><keyname>Hinton</keyname><forenames>Geoffrey E.</forenames></author></authors><title>A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning long term dependencies in recurrent networks is difficult due to
vanishing and exploding gradients. To overcome this difficulty, researchers
have developed sophisticated optimization techniques and network architectures.
In this paper, we propose a simpler solution that use recurrent neural networks
composed of rectified linear units. Key to our solution is the use of the
identity matrix or its scaled version to initialize the recurrent weight
matrix. We find that our solution is comparable to LSTM on our four benchmarks:
two toy problems involving long-range temporal structures, a large language
modeling problem and a benchmark speech recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00942</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00942</id><created>2015-04-03</created><authors><author><keyname>Tomanek</keyname><forenames>Martin</forenames></author><author><keyname>Klima</keyname><forenames>Tomas</forenames></author></authors><title>Penetration Testing in Agile Software Development Projects</title><categories>cs.SE</categories><journal-ref>International Journal on Cryptography and Information Security
  03/2015; 5(1):1-7</journal-ref><doi>10.5121/ijcis.2015.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile development methods are commonly used to iteratively develop the
information systems and they can easily handle ever-changing business
requirements. Scrum is one of the most popular agile software development
frameworks. The popularity is caused by the simplified process framework and
its focus on teamwork. The objective of Scrum is to deliver working software
and demonstrate it to the customer faster and more frequent during the software
development project. However the security requirements for the developing
information systems have often a low priority. This requirements prioritization
issue results in the situations where the solution meets all the business
requirements but it is vulnerable to potential security threats.
  The major benefit of the Scrum framework is the iterative development
approach and the opportunity to automate penetration tests. Therefore the
security vulnerabilities can be discovered and solved more often which will
positively contribute to the overall information system protection against
potential hackers. In this research paper the authors propose how the agile
software development framework Scrum can be enriched by considering the
penetration tests and related security requirements during the software
development lifecycle. Authors apply in this paper the knowledge and expertise
from their previous work focused on development of the new information system
penetration tests methodology PETA with focus on using COBIT 4.1 as the
framework for management of these tests, and on previous work focused on
tailoring the project management framework PRINCE2 with Scrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00943</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00943</id><created>2015-04-03</created><authors><author><keyname>Adlam</keyname><forenames>Emily</forenames></author><author><keyname>Kent</keyname><forenames>Adrian</forenames></author></authors><title>Deterministic Relativistic Quantum Bit Commitment</title><categories>quant-ph cs.CR</categories><journal-ref>Int. J. Quantum Inform. 13, 1550029 (2015)</journal-ref><doi>10.1142/S021974991550029X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe new unconditionally secure bit commitment schemes whose security
is based on Minkowski causality and the monogamy of quantum entanglement. We
first describe an ideal scheme that is purely deterministic, in the sense that
neither party needs to generate any secret randomness at any stage. We also
describe a variant that allows the committer to proceed deterministically,
requires only local randomness generation from the receiver, and allows the
commitment to be verified in the neighbourhood of the unveiling point. We show
that these schemes still offer near-perfect security in the presence of losses
and errors, which can be made perfect if the committer uses an extra single
random secret bit. We discuss scenarios where these advantages are significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00944</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00944</id><created>2015-04-03</created><authors><author><keyname>Adlam</keyname><forenames>Emily</forenames></author><author><keyname>Kent</keyname><forenames>Adrian</forenames></author></authors><title>Device-Independent Relativistic Quantum Bit Commitment</title><categories>quant-ph cs.CR</categories><journal-ref>Phys. Rev. A 92, 022315 (2015)</journal-ref><doi>10.1103/PhysRevA.92.022315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the possibility of device-independent relativistic quantum bit
commitment. We note the potential threat of {\it location attacks}, in which
the behaviour of untrusted devices used in relativistic quantum cryptography
depends on their space-time location. We describe relativistic quantum bit
commitment schemes that are immune to these attacks, and show that these
schemes offer device-independent security against hypothetical post-quantum
adversaries subject only to the no-signalling principle. We compare a
relativistic classical bit commitment scheme with similar features, and note
some possible advantages of the quantum schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00948</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00948</id><created>2015-04-03</created><updated>2015-07-31</updated><authors><author><keyname>Li</keyname><forenames>Liangyue</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author></authors><title>The Child is Father of the Man: Foresee the Success at the Early Stage</title><categories>cs.LG</categories><comments>Correct some typos in our KDD paper</comments><doi>10.1145/2783258.2783340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the dynamic mechanisms that drive the high-impact scientific
work (e.g., research papers, patents) is a long-debated research topic and has
many important implications, ranging from personal career development and
recruitment search, to the jurisdiction of research resources. Recent advances
in characterizing and modeling scientific success have made it possible to
forecast the long-term impact of scientific work, where data mining techniques,
supervised learning in particular, play an essential role. Despite much
progress, several key algorithmic challenges in relation to predicting
long-term scientific impact have largely remained open. In this paper, we
propose a joint predictive model to forecast the long-term scientific impact at
the early stage, which simultaneously addresses a number of these open
challenges, including the scholarly feature design, the non-linearity, the
domain-heterogeneity and dynamics. In particular, we formulate it as a
regularized optimization problem and propose effective and scalable algorithms
to solve it. We perform extensive empirical evaluations on large, real
scholarly data sets to validate the effectiveness and the efficiency of our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00953</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00953</id><created>2015-04-03</created><authors><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Outage Analysis of Full-Duplex Architectures in Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in Proc. IEEE VTC 2015 Spring, Glasgow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of full-duplex (FD) radio in wireless communications is a
potential approach for achieving higher spectral efficiency. A possible
application is its employment in the next generation of cellular networks.
However, the performance of large-scale FD multiuser networks is an area mostly
unexplored. Most of the related work focuses on the performance analysis of
small-scale networks or on loop interference cancellation schemes. In this
paper, we derive the outage probability performance of large-scale FD cellular
networks in the context of two architectures: two-node and three-node. We show
how the performance is affected with respect to the model's parameters and
provide a comparison between the two architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00954</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00954</id><created>2015-04-03</created><updated>2015-09-22</updated><authors><author><keyname>Eden</keyname><forenames>Talya</forenames></author><author><keyname>Levi</keyname><forenames>Amit</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Approximately Counting Triangles in Sublinear Time</title><categories>cs.DS</categories><comments>To appear in the 56th Annual IEEE Symposium on Foundations of
  Computer Science (FOCS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the number of triangles in a graph.
This problem has been extensively studied in both theory and practice, but all
existing algorithms read the entire graph. In this work we design a {\em
sublinear-time\/} algorithm for approximating the number of triangles in a
graph, where the algorithm is given query access to the graph. The allowed
queries are degree queries, vertex-pair queries and neighbor queries.
  We show that for any given approximation parameter $0&lt;\epsilon&lt;1$, the
algorithm provides an estimate $\widehat{t}$ such that with high constant
probability, $(1-\epsilon)\cdot t&lt; \widehat{t}&lt;(1+\epsilon)\cdot t$, where $t$
is the number of triangles in the graph $G$. The expected query complexity of
the algorithm is $\!\left(\frac{n}{t^{1/3}} + \min\left\{m,
\frac{m^{3/2}}{t}\right\}\right)\cdot {\rm poly}(\log n, 1/\epsilon)$, where
$n$ is the number of vertices in the graph and $m$ is the number of edges, and
the expected running time is $\!\left(\frac{n}{t^{1/3}} +
\frac{m^{3/2}}{t}\right)\cdot {\rm poly}(\log n, 1/\epsilon)$. We also prove
that $\Omega\!\left(\frac{n}{t^{1/3}} + \min\left\{m,
\frac{m^{3/2}}{t}\right\}\right)$ queries are necessary, thus establishing that
the query complexity of this algorithm is optimal up to polylogarithmic factors
in $n$ (and the dependence on $1/\epsilon$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00976</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00976</id><created>2015-04-03</created><updated>2015-06-03</updated><authors><author><keyname>Parekh</keyname><forenames>Ankit</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Convex Denoising using Non-Convex Tight Frame Regularization</title><categories>cs.CV math.OC</categories><comments>5 pages, 6 figures</comments><journal-ref>IEEE Signal Processing Letters, 22(10):1786-1790, Oct. 2015</journal-ref><doi>10.1109/LSP.2015.2432095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of signal denoising using a sparse
tight-frame analysis prior. The L1 norm has been extensively used as a
regularizer to promote sparsity; however, it tends to under-estimate non-zero
values of the underlying signal. To more accurately estimate non-zero values,
we propose the use of a non-convex regularizer, chosen so as to ensure
convexity of the objective function. The convexity of the objective function is
ensured by constraining the parameter of the non-convex penalty. We use ADMM to
obtain a solution and show how to guarantee that ADMM converges to the global
optimum of the objective function. We illustrate the proposed method for 1D and
2D signal denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00977</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00977</id><created>2015-04-03</created><authors><author><keyname>Dymchenko</keyname><forenames>Sergii</forenames></author><author><keyname>Mykhailova</keyname><forenames>Mariia</forenames></author></authors><title>Declaratively solving Google Code Jam problems with Picat</title><categories>cs.PL</categories><doi>10.1007/978-3-319-19686-2_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present several examples of solving algorithmic problems
from the Google Code Jam programming contest with Picat programming language
using declarative techniques: constraint logic programming and tabled logic
programming. In some cases the use of Picat simplifies the implementation
compared to conventional imperative programming languages, while in others it
allows to directly convert the problem statement into an efficiently solvable
declarative problem specification without inventing an imperative algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00981</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00981</id><created>2015-04-04</created><updated>2015-11-30</updated><authors><author><keyname>Ai</keyname><forenames>Wu</forenames></author><author><keyname>Chen</keyname><forenames>Weisheng</forenames></author></authors><title>ELM-Based Distributed Cooperative Learning Over Networks</title><categories>cs.LG math.OC</categories><comments>This paper has been withdrawn by the authors due to the incorrect
  proof of Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates distributed cooperative learning algorithms for data
processing in a network setting. Specifically, the extreme learning machine
(ELM) is introduced to train a set of data distributed across several
components, and each component runs a program on a subset of the entire data.
In this scheme, there is no requirement for a fusion center in the network due
to e.g., practical limitations, security, or privacy reasons. We first
reformulate the centralized ELM training problem into a separable form among
nodes with consensus constraints. Then, we solve the equivalent problem using
distributed optimization tools. A new distributed cooperative learning
algorithm based on ELM, called DC-ELM, is proposed. The architecture of this
algorithm differs from that of some existing parallel/distributed ELMs based on
MapReduce or cloud computing. We also present an online version of the proposed
algorithm that can learn data sequentially in a one-by-one or chunk-by-chunk
mode. The novel algorithm is well suited for potential applications such as
artificial intelligence, computational biology, finance, wireless sensor
networks, and so on, involving datasets that are often extremely large,
high-dimensional and located on distributed data sources. We show simulation
results on both synthetic and real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00983</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00983</id><created>2015-04-04</created><updated>2015-08-04</updated><authors><author><keyname>Sun</keyname><forenames>Chen</forenames></author><author><keyname>Shetty</keyname><forenames>Sanketh</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author><author><keyname>Nevatia</keyname><forenames>Ram</forenames></author></authors><title>Temporal Localization of Fine-Grained Actions in Videos by Domain
  Transfer from Web Images</title><categories>cs.CV cs.MM</categories><comments>Camera ready version for ACM Multimedia 2015</comments><acm-class>I.2.10</acm-class><doi>10.1145/2733373.2806226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of fine-grained action localization from temporally
untrimmed web videos. We assume that only weak video-level annotations are
available for training. The goal is to use these weak labels to identify
temporal segments corresponding to the actions, and learn models that
generalize to unconstrained web videos. We find that web images queried by
action names serve as well-localized highlights for many actions, but are
noisily labeled. To solve this problem, we propose a simple yet effective
method that takes weak video labels and noisy image labels as input, and
generates localized action frames as output. This is achieved by cross-domain
transfer between video frames and web images, using pre-trained deep
convolutional neural networks. We then use the localized action frames to train
action recognition models with long short-term memory networks. We collect a
fine-grained sports action data set FGA-240 of more than 130,000 YouTube
videos. It has 240 fine-grained actions under 85 sports activities. Convincing
results are shown on the FGA-240 data set, as well as the THUMOS 2014
localization data set with untrimmed training videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.00984</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.00984</id><created>2015-04-04</created><authors><author><keyname>Ghorbani</keyname><forenames>Behrooz</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Sparse regression with highly correlated predictors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a linear regression $y=X\beta+u$ where
$X\in\mathbb{\mathbb{{R}}}^{n\times p}$, $p\gg n,$ and $\beta$ is $s$-sparse.
Motivated by examples in financial and economic data, we consider the situation
where $X$ has highly correlated and clustered columns. To perform sparse
recovery in this setting, we introduce the \emph{clustering removal algorithm}
(CRA), that seeks to decrease the correlation in $X$ by removing the cluster
structure without changing the parameter vector $\beta$. We show that as long
as certain assumptions hold about $X$, the decorrelated matrix will satisfy the
restricted isometry property (RIP) with high probability. We also provide
examples of the empirical performance of CRA and compare it with other sparse
recovery techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01000</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01000</id><created>2015-04-04</created><authors><author><keyname>Bhattacharya</keyname><forenames>Avik</forenames></author><author><keyname>Muhuri</keyname><forenames>Arnab</forenames></author><author><keyname>De</keyname><forenames>Shaunak</forenames></author><author><keyname>Manickam</keyname><forenames>Surendar</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>Modifying the Yamaguchi Four-Component Decomposition Scattering Powers
  Using a Stochastic Distance</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE J-STARS (IEEE Journal of Selected
  Topics in Applied Earth Observations and Remote Sensing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based decompositions have gained considerable attention after the
initial work of Freeman and Durden. This decomposition which assumes the target
to be reflection symmetric was later relaxed in the Yamaguchi et al.
decomposition with the addition of the helix parameter. Since then many
decomposition have been proposed where either the scattering model was modified
to fit the data or the coherency matrix representing the second order
statistics of the full polarimetric data is rotated to fit the scattering
model. In this paper we propose to modify the Yamaguchi four-component
decomposition (Y4O) scattering powers using the concept of statistical
information theory for matrices. In order to achieve this modification we
propose a method to estimate the polarization orientation angle (OA) from
full-polarimetric SAR images using the Hellinger distance. In this method, the
OA is estimated by maximizing the Hellinger distance between the un-rotated and
the rotated $T_{33}$ and the $T_{22}$ components of the coherency matrix
$\mathbf{[T]}$. Then, the powers of the Yamaguchi four-component model-based
decomposition (Y4O) are modified using the maximum relative stochastic distance
between the $T_{33}$ and the $T_{22}$ components of the coherency matrix at the
estimated OA. The results show that the overall double-bounce powers over
rotated urban areas have significantly improved with the reduction of volume
powers. The percentage of pixels with negative powers have also decreased from
the Y4O decomposition. The proposed method is both qualitatively and
quantitatively compared with the results obtained from the Y4O and the Y4R
decompositions for a Radarsat-2 C-band San-Francisco dataset and an UAVSAR
L-band Hayward dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01002</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01002</id><created>2015-04-04</created><updated>2015-06-02</updated><authors><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Passive Loop Interference Suppression in Large-Scale Full-Duplex
  Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in Proc. IEEE SPAWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loop interference (LI) in wireless communications, is a notion resulting from
the full-duplex (FD) operation. In a large-scale network, FD also increases the
multiuser interference due to the large number of active wireless links that
exist. Hence, in order to realize the FD potentials, this interference needs to
be restricted. This paper presents a stochastic geometry model of FD cellular
networks where the users and base stations employ directional antennas. Based
on previous experimental results, we model the passive suppression of the LI at
each FD terminal as a function of the angle between the two antennas and show
the significant gains that can be achieved by this method. Together with the
reduction of multiuser interference resulting from antenna directionality, our
model demonstrates that FD can potentially be implemented in large-scale
directional networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01004</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01004</id><created>2015-04-04</created><updated>2015-11-18</updated><authors><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author><author><keyname>Guo</keyname><forenames>Chonghui</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Luis</forenames></author></authors><title>Managing Multi-Granular Linguistic Distribution Assessments in
  Large-Scale Multi-Attribute Group Decision Making</title><categories>cs.AI</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linguistic large-scale group decision making (LGDM) problems are more and
more common nowadays. In such problems a large group of decision makers are
involved in the decision process and elicit linguistic information that are
usually assessed in different linguistic scales with diverse granularity
because of decision makers' distinct knowledge and background. To keep maximum
information in initial stages of the linguistic LGDM problems, the use of
multi-granular linguistic distribution assessments seems a suitable choice,
however to manage such multigranular linguistic distribution assessments, it is
necessary the development of a new linguistic computational approach. In this
paper it is proposed a novel computational model based on the use of extended
linguistic hierarchies, which not only can be used to operate with
multi-granular linguistic distribution assessments, but also can provide
interpretable linguistic results to decision makers. Based on this new
linguistic computational model, an approach to linguistic large-scale
multi-attribute group decision making is proposed and applied to a talent
selection process in universities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01013</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01013</id><created>2015-04-04</created><updated>2015-04-22</updated><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van dan</forenames></author></authors><title>Efficient piecewise training of deep structured models for semantic
  segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in semantic image segmentation have mostly been achieved by
training deep convolutional neural networks (CNNs) for the task. We show how to
improve semantic segmentation through the use of contextual information, by
combining the strengths of deep CNNs to learn powerful feature representations,
with Conditional Random Fields (CRFs) which can capture contextual relation
modeling. Unlike previous work, our formulation of &quot;deep CRFs&quot; learns both
unary {\em and} pairwise terms using multi-scale fully convolutional neural
networks (FCNNs) in an end-to-end fashion, which enables us to model complex
spatial relations between image regions. A naive method for training such an
approach would rely on direct likelihood maximization of the CRF, but this
would require expensive inference at each stochastic gradient decent iteration,
rendering the approach computationally unviable. We propose a novel method for
efficient joint training of the deep structured model based on piecewise
training. This approximate training method avoids repeated inference, and so is
computationally tractable. We also demonstrate that it yields results that are
competitive with the state-of-art in semantic segmentation for the PASCAL VOC
2012 dataset. In particular, we achieve an intersection-over-union score of
$70.7$ on its test set, which outperforms state-of-the-art results that make
use of the same size training set, thus demonstrating the value of our deep,
multi-scale approach to modelling contextual relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01014</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01014</id><created>2015-04-04</created><updated>2015-11-24</updated><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Lewis</keyname><forenames>Megan E.</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Discrete uncertainty principles and sparse signal processing</title><categories>cs.IT math.FA math.IT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We develop new discrete uncertainty principles in terms of numerical
sparsity, which is a continuous proxy for the 0-norm. Unlike traditional
sparsity, the continuity of numerical sparsity naturally accommodates functions
which are nearly sparse. After studying these principles and the functions that
achieve exact or near equality in them, we identify certain consequences in a
number of sparse signal processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01018</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01018</id><created>2015-04-04</created><authors><author><keyname>Wu</keyname><forenames>Zhihao</forenames></author><author><keyname>Lin</keyname><forenames>Youfang</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author></authors><title>A parameter free similarity index based on clustering ability for link
  prediction in complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:0905.3558, arXiv:1010.0725 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction in complex network based on solely topological information is
a challenging problem. In this paper, we propose a novel similarity index,
which is efficient and parameter free, based on clustering ability. Here
clustering ability is defined as average clustering coefficient of nodes with
the same degree. The motivation of our idea is that common-neighbors are able
to contribute to the likelihood of forming a link because they own some ability
of clustering their neighbors together, and then clustering ability defined
here is a measure for this capacity. Experimental numerical simulations on both
real-world networks and modeled networks demonstrated the high accuracy and
high efficiency of the new similarity index compared with three well-known
common-neighbor based similarity indices: CN, AA and RA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01019</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01019</id><created>2015-04-04</created><updated>2015-11-18</updated><authors><author><keyname>Ganesan</keyname><forenames>Karthik</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author><author><keyname>Rabaey</keyname><forenames>Jan</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>On the Total-Power Capacity of Regular-LDPC Codes with Iterative
  Message-Passing Decoders</title><categories>cs.IT math.IT</categories><comments>21 pages, 6 figures. To appear in JSAC Recent Advances In Capacity
  Approaching Codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recently derived fundamental limits on total (transmit +
decoding) power for coded communication with VLSI decoders, this paper
investigates the scaling behavior of the minimum total power needed to
communicate over AWGN channels as the target bit-error-probability tends to
zero. We focus on regular-LDPC codes and iterative message-passing decoders. We
analyze scaling behavior under two VLSI complexity models of decoding. One
model abstracts power consumed in processing elements (&quot;node model&quot;), and
another abstracts power consumed in wires which connect the processing elements
(&quot;wire model&quot;). We prove that a coding strategy using regular-LDPC codes with
Gallager-B decoding achieves order-optimal scaling of total power under the
node model. However, we also prove that regular-LDPC codes and iterative
message-passing decoders cannot meet existing fundamental limits on total power
under the wire model. Further, if the transmit energy-per-bit is bounded, total
power grows at a rate that is worse than uncoded transmission. Complementing
our theoretical results, we develop detailed physical models of decoding
implementations using post-layout circuit simulations. Our theoretical and
numerical results show that approaching fundamental limits on total power
requires increasing the complexity of both the code design and the
corresponding decoding algorithm as communication distance is increased or
error-probability is lowered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01020</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01020</id><created>2015-04-04</created><authors><author><keyname>Ray</keyname><forenames>Arupratan</forenames><affiliation>Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India</affiliation></author><author><keyname>Mandal</keyname><forenames>Debmalya</forenames><affiliation>Harvard School of Engineering and Applied Sciences, Cambridge, MA</affiliation></author><author><keyname>Narahari</keyname><forenames>Y.</forenames><affiliation>Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India</affiliation></author></authors><title>Profit Maximizing Prior-free Multi-unit Procurement Auctions with
  Capacitated Sellers</title><categories>cs.GT</categories><comments>16 pages, short version of the paper to be published in the
  Proceedings of International Conference on Autonomous Agents and Multiagent
  Systems 2015</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive bounds for profit maximizing prior-free procurement
auctions where a buyer wishes to procure multiple units of a homogeneous item
from n sellers who are strategic about their per unit valuation. The buyer
earns the profit by reselling these units in an external consumer market. The
paper looks at three scenarios of increasing complexity. First, we look at unit
capacity sellers where per unit valuation is private information of each seller
and the revenue curve is concave. For this setting, we define two benchmarks.
We show that no randomized prior free auction can be constant competitive
against any of these two benchmarks. However, for a lightly constrained
benchmark we design a prior-free auction PEPA (Profit Extracting Procurement
Auction) which is 4-competitive and we show this bound is tight. Second, we
study a setting where the sellers have non-unit capacities that are common
knowledge and derive similar results. In particular, we propose a prior free
auction PEPAC (Profit Extracting Procurement Auction with Capacity) which is
truthful for any concave revenue curve. Third, we obtain results in the
inherently harder bi-dimensional case where per unit valuation as well as
capacities are private information of the sellers. We show that PEPAC is
truthful and constant competitive for the specific case of linear revenue
curves. We believe that this paper represents the first set of results on
single dimensional and bi-dimensional profit maximizing prior-free multi-unit
procurement auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01023</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01023</id><created>2015-04-04</created><authors><author><keyname>Bana&#x15b;</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kru&#x17c;el</keyname><forenames>Filip</forenames></author><author><keyname>Biela&#x144;ski</keyname><forenames>Jan</forenames></author></authors><title>Finite element numerical integration for first order approximations on
  multi-core architectures</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents investigations on the implementation and performance of
the finite element numerical integration algorithm for first order
approximations and three processor architectures, popular in scientific
computing, classical CPU, Intel Xeon Phi and NVIDIA Kepler GPU. A unifying
programming model and portable OpenCL implementation is considered for all
architectures. Variations of the algorithm due to different problems solved and
different element types are investigated and several optimizations aimed at
proper optimization and mapping of the algorithm to computer architectures are
demonstrated. Performance models of execution are developed for different
processors and tested in practical experiments. The results show the varying
levels of performance for different architectures, but indicate that the
algorithm can be effectively ported to all of them. The general conclusion is
that the finite element numerical integration can achieve sufficient
performance on different multi- and many-core architectures and should not
become a performance bottleneck for finite element simulation codes. Specific
observations lead to practical advises on how to optimize the kernels and what
performance can be expected for the tested architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01025</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01025</id><created>2015-04-04</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Feng</keyname><forenames>Liangbing</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Preprint Extending Touch-less Interaction on Vision Based Wearable
  Device</title><categories>cs.HC cs.CV cs.GR</categories><comments>This is the preprint version of our paper on IEEE Virtual Reality
  Conference 2015</comments><acm-class>H.1.2; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on IEEE Virtual Reality Conference
2015. A touch-less interaction technology on vision based wearable device is
designed and evaluated. Users interact with the application with dynamic
hands/feet gestures in front of the camera. Several proof-of-concept prototypes
with eleven dynamic gestures are developed based on the touch-less interaction.
At last, a comparing user study evaluation is proposed to demonstrate the
usability of the touch-less approach, as well as the impact on user's emotion,
running on a wearable framework or Google Glass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01027</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01027</id><created>2015-04-04</created><authors><author><keyname>Santos</keyname><forenames>Marco</forenames></author></authors><title>Mapeamento Sistematico</title><categories>cs.DL</categories><comments>in Portuguese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A systematic mapping is a way to identify, evaluate and interpret all
relevant research available to a matter of particular research. One of the
reasons for conducting systematic reviews is that it summarizes the existing
evidence regarding treatment or technology [Kitchenham, 2004].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01030</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01030</id><created>2015-04-04</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Esteve</keyname><forenames>Chantal</forenames></author><author><keyname>Chirivella</keyname><forenames>Javier</forenames></author><author><keyname>Gagliardo</keyname><forenames>Pablo</forenames></author></authors><title>Preprint A Game Based Assistive Tool for Rehabilitation of Dysphonic
  Patients</title><categories>cs.HC cs.MM</categories><comments>This is the preprint version of our paper on 3rd International
  Workshop on Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual
  Reality 2015 (VR2015)</comments><acm-class>I.3.7; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 3rd International Workshop on
Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015
(VR2015). An assistive training tool for rehabilitation of dysphonic patients
is designed and developed according to the practical clinical needs. The
assistive tool employs a space flight game as the attractive logic part, and
microphone arrays as input device, which is getting rid of ambient noise by
setting a specific orientation. The therapist can guide the patient to play the
game as well as the voice training simultaneously side by side, while not
interfere the patient voice. The voice information can be recorded and
extracted for evaluating the long-time rehabilitation progress. This paper
outlines a design science approach for the development of an initial useful
software prototype of such a tool, considering 'Intuitive', 'Entertainment',
'Incentive' as main design factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01033</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01033</id><created>2015-04-04</created><updated>2015-11-17</updated><authors><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Watch and Learn: Optimizing from Revealed Preferences Feedback</title><categories>cs.DS cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Stackelberg game is played between a leader and a follower. The leader
first chooses an action, then the follower plays his best response. The goal of
the leader is to pick the action that will maximize his payoff given the
follower's best response. In this paper we present an approach to solving for
the leader's optimal strategy in certain Stackelberg games where the follower's
utility function (and thus the subsequent best response of the follower) is
unknown.
  Stackelberg games capture, for example, the following interaction between a
producer and a consumer. The producer chooses the prices of the goods he
produces, and then a consumer chooses to buy a utility maximizing bundle of
goods. The goal of the seller here is to set prices to maximize his
profit---his revenue, minus the production cost of the purchased bundle. It is
quite natural that the seller in this example should not know the buyer's
utility function. However, he does have access to revealed preference
feedback---he can set prices, and then observe the purchased bundle and his own
profit. We give algorithms for efficiently solving, in terms of both
computational and query complexity, a broad class of Stackelberg games in which
the follower's utility function is unknown, using only &quot;revealed preference&quot;
access to it. This class includes in particular the profit maximization
problem, as well as the optimal tolling problem in nonatomic congestion games,
when the latency functions are unknown. Surprisingly, we are able to solve
these problems even though the optimization problems are non-convex in the
leader's actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01039</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01039</id><created>2015-04-04</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Bade</keyname><forenames>David</forenames></author><author><keyname>Buluc</keyname><forenames>Ayd&#x131;n</forenames></author><author><keyname>Gilbert</keyname><forenames>John</forenames></author><author><keyname>Mattson</keyname><forenames>Timothy</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Graphs, Matrices, and the GraphBLAS: Seven Good Reasons</title><categories>cs.DC</categories><comments>10 pages; International Conference on Computational Science workshop
  on the Applications of Matrix Computational Methods in the Analysis of Modern
  Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of graphs has become increasingly important to a wide range of
applications. Graph analysis presents a number of unique challenges in the
areas of (1) software complexity, (2) data complexity, (3) security, (4)
mathematical complexity, (5) theoretical analysis, (6) serial performance, and
(7) parallel performance. Implementing graph algorithms using matrix-based
approaches provides a number of promising solutions to these challenges. The
GraphBLAS standard (istc- bigdata.org/GraphBlas) is being developed to bring
the potential of matrix based graph algorithms to the broadest possible
audience. The GraphBLAS mathematically defines a core set of matrix-based graph
operations that can be used to implement a wide class of graph algorithms in a
wide range of programming environments. This paper provides an introduction to
the GraphBLAS and describes how the GraphBLAS can be used to address many of
the challenges associated with analysis of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01042</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01042</id><created>2015-04-04</created><updated>2015-08-13</updated><authors><author><keyname>Shen</keyname><forenames>Wenlong</forenames></author><author><keyname>Yin</keyname><forenames>Bo</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>A Distributed Secure Outsourcing Scheme for Solving Linear Algebraic
  Equations in Ad Hoc Clouds</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging ad hoc clouds form a new cloud computing paradigm by leveraging
untapped local computation and storage resources. An important application
application over ad hoc clouds is outsourcing computationally intensive
problems to nearby cloud agents to solve in a distributed manner. A risk with
ad hoc clouds is however the potential cyber attacks, with the security and
privacy in distributed outsourcing a significant challenging issue. In this
paper, we consider distributed secure outsourcing of linear algebraic equations
(LAE), one of the most frequently used mathematical tools, in ad hoc clouds.
The outsourcing client assigns each agent a subproblem : all involved agents
then apply a consensus based algorithm to obtain the correct solution in a
distributed and iterative manner. We identify a number of security risks in
this process, and propose a secure outsourcing scheme which can not only
preserve privacy to shield the original LAE parameters and the final solution
from the computing agents, but also detect misbehavior based on mutual
verifications in a real-time manner. We rigorously prove that the proposed
scheme converges to the correct solution of the LAE exponentially fast, has low
computation complexity at each agent, and is robust against the identified
security attacks. Extensive numerical results are presented to demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01044</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01044</id><created>2015-04-04</created><updated>2015-05-03</updated><authors><author><keyname>Wang</keyname><forenames>Heng</forenames></author><author><keyname>Abraham</keyname><forenames>Zubin</forenames></author></authors><title>Concept Drift Detection for Streaming Data</title><categories>stat.ML cs.LG</categories><comments>9 pages, accepted in the International Joint Conference of Neural
  Networks 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common statistical prediction models often require and assume stationarity in
the data. However, in many practical applications, changes in the relationship
of the response and predictor variables are regularly observed over time,
resulting in the deterioration of the predictive performance of these models.
This paper presents Linear Four Rates (LFR), a framework for detecting these
concept drifts and subsequently identifying the data points that belong to the
new concept (for relearning the model). Unlike conventional concept drift
detection approaches, LFR can be applied to both batch and stream data; is not
limited by the distribution properties of the response variable (e.g., datasets
with imbalanced labels); is independent of the underlying statistical-model;
and uses user-specified parameters that are intuitively comprehensible. The
performance of LFR is compared to benchmark approaches using both simulated and
commonly used public datasets that span the gamut of concept drift types. The
results show LFR significantly outperforms benchmark approaches in terms of
recall, accuracy and delay in detection of concept drifts across datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01046</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01046</id><created>2015-04-04</created><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Clustering Consistent Sparse Subspace Clustering</title><categories>stat.ML cs.LG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering is the problem of clustering data points into a union of
low-dimensional linear or affine subspaces. It is the mathematical abstraction
of many important problems in computer vision, image processing and has been
drawing avid attention in machine learning and statistics recently. In
particular, a line of recent work (Elhamifar and Vidal, 2013; Soltanolkotabi et
al., 2012; Wang and Xu, 2013; Soltanolkotabi et al., 2014) provided strong
theoretical guarantee for the seminal algorithm: Sparse Subspace Clustering
(SSC) (Elhamifar and Vidal, 2013) under various settings, and to some extent,
justified its state-of-the-art performance in applications such as motion
segmentation and face clustering. The focus of these work has been getting
milder conditions under which SSC obeys &quot;self-expressiveness property&quot;, which
ensures that no two points from different subspaces can be clustered together.
Such guarantee however is not sufficient for the clustering to be correct,
thanks to the notorious &quot;graph connectivity problem&quot; (Nasihatkon and Hartley,
2011). In this paper, we show that this issue can be resolved by a very simple
post-processing procedure under only a mild &quot;general position&quot; assumption. In
addition, we show that the approach is robust to arbitrary bounded perturbation
of the data whenever the &quot;general position&quot; assumption holds with a margin.
These results provide the first exact clustering guarantee of SSC for subspaces
of dimension greater than 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01048</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01048</id><created>2015-04-04</created><updated>2015-12-19</updated><authors><author><keyname>Binnig</keyname><forenames>Carsten</forenames></author><author><keyname>Crotty</keyname><forenames>Andrew</forenames></author><author><keyname>Galakatos</keyname><forenames>Alex</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Zamanian</keyname><forenames>Erfan</forenames></author></authors><title>The End of Slow Networks: It's Time for a Redesign</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation high-performance RDMA-capable networks will require a
fundamental rethinking of the design and architecture of modern distributed
DBMSs. These systems are commonly designed and optimized under the assumption
that the network is the bottleneck: the network is slow and &quot;thin&quot;, and thus
needs to be avoided as much as possible. Yet this assumption no longer holds
true. With InfiniBand FDR 4x, the bandwidth available to transfer data across
network is in the same ballpark as the bandwidth of one memory channel, and it
increases even further with the most recent EDR standard. Moreover, with the
increasing advances of RDMA, the latency improves similarly fast. In this
paper, we first argue that the &quot;old&quot; distributed database design is not capable
of taking full advantage of the network. Second, we propose architectural
redesigns for OLTP, OLAP and advanced analytical frameworks to take better
advantage of the improved bandwidth, latency and RDMA capabilities. Finally,
for each of the workload categories, we show that remarkable performance
improvements can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01049</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01049</id><created>2015-04-04</created><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Su</keyname><forenames>Tianyun</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author></authors><title>3D visual analysis of seabed on smartphone</title><categories>cs.HC cs.GR</categories><comments>2015 IEEE Pacific Visualization Symposium (PacificVis)</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We create a 'virtual-seabed' platform to realize the 3D visual analysis of
seabed on smartphone. The 3D seabed platform is based on a 'section-drilling'
model, implementing visualization and analysis of the integrated data of seabed
on the 3D browser on smartphone. Some 3D visual analysis functions are
developed. This work presents a thorough and interesting way of presenting
seabed data on smartphone, which raises many application possibilities. This
platform is another practical proof based on our WebVRGIS platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01050</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01050</id><created>2015-04-04</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>An Online Approach to Dynamic Channel Access and Transmission Scheduling</title><categories>cs.LG cs.SY</categories><comments>10 pages, to appear in MobiHoc 2015</comments><acm-class>F.1.2; C.2.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making judicious channel access and transmission scheduling decisions is
essential for improving performance as well as energy and spectral efficiency
in multichannel wireless systems. This problem has been a subject of extensive
study in the past decade, and the resulting dynamic and opportunistic channel
access schemes can bring potentially significant improvement over traditional
schemes. However, a common and severe limitation of these dynamic schemes is
that they almost always require some form of a priori knowledge of the channel
statistics. A natural remedy is a learning framework, which has also been
extensively studied in the same context, but a typical learning algorithm in
this literature seeks only the best static policy, with performance measured by
weak regret, rather than learning a good dynamic channel access policy. There
is thus a clear disconnect between what an optimal channel access policy can
achieve with known channel statistics that actively exploits temporal, spatial
and spectral diversity, and what a typical existing learning algorithm aims
for, which is the static use of a single channel devoid of diversity gain. In
this paper we bridge this gap by designing learning algorithms that track known
optimal or sub-optimal dynamic channel access and transmission scheduling
policies, thereby yielding performance measured by a form of strong regret, the
accumulated difference between the reward returned by an optimal solution when
a priori information is available and that by our online algorithm. We do so in
the context of two specific algorithms that appeared in [1] and [2],
respectively, the former for a multiuser single-channel setting and the latter
for a single-user multichannel setting. In both cases we show that our
algorithms achieve sub-linear regret uniform in time and outperforms the
standard weak-regret learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01051</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01051</id><created>2015-04-04</created><authors><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Zhang</keyname><forenames>Baoyun</forenames></author><author><keyname>Wang</keyname><forenames>Weixi</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author><author><keyname>Hu</keyname><forenames>Jinxing</forenames></author></authors><title>WebVRGIS Based City Bigdata 3D Visualization and Analysis</title><categories>cs.HC</categories><comments>2015 IEEE Pacific Visualization Symposium (PacificVis)</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows the WEBVRGIS platform overlying multiple types of data about
Shenzhen over a 3d globe. The amount of information that can be visualized with
this platform is overwhelming, and the GIS-based navigational scheme allows to
have great flexibility to access the different available data sources. For
example,visualising historical and forecasted passenger volume at stations
could be very helpful when overlaid with other social data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01052</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01052</id><created>2015-04-04</created><authors><author><keyname>Ehrensperger</keyname><forenames>Gregor</forenames></author><author><keyname>Ostermann</keyname><forenames>Alexander</forenames></author><author><keyname>Schwitzer</keyname><forenames>Felix</forenames></author></authors><title>Fast algorithms for morphological operations using run-length encoded
  binary images</title><categories>cs.CV cs.GR cs.IT math.IT</categories><comments>17 pages, 2 figures. Submitted to Elsevier (Pattern Recognition). For
  the associated source code, see
  https://numerical-analysis.uibk.ac.at/g.ehrensperger</comments><msc-class>94A08 (Primary) 65D18, 65D19, 68U10, 94A12 (Secondary)</msc-class><acm-class>I.4.3; I.5; I.4.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents innovative algorithms to efficiently compute erosions and
dilations of run-length encoded (RLE) binary images with arbitrary shaped
structuring elements. An RLE image is given by a set of runs, where a run is a
horizontal concatenation of foreground pixels. The proposed algorithms extract
the skeleton of the structuring element and build distance tables of the input
image, which are storing the distance to the next background pixel on the left
and right hand sides. This information is then used to speed up the
calculations of the erosion and dilation operator by enabling the use of
techniques which allow to skip the analysis of certain pixels whenever a hit or
miss occurs. Additionally the input image gets trimmed during the preprocessing
steps on the base of two primitive criteria. Experimental results show the
advantages over other algorithms. The source code of our algorithms is
available in C++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01057</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01057</id><created>2015-04-04</created><updated>2015-08-13</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Hu</keyname><forenames>Jinxing</forenames></author><author><keyname>Yin</keyname><forenames>Ling</forenames></author><author><keyname>Zhang</keyname><forenames>Baoyun</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author></authors><title>Preprint Virtual Geographic Environment Based Coach Passenger Flow
  Forecasting</title><categories>cs.SI physics.soc-ph</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in section 3</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 2015 IEEE Computational
Intelligence and Virtual Environments for Measurement Systems and Applications
(CIVEMSA). There are lacks of integrated analysis and visual display of
multiple real-time dynamic traffic information. This research proposed a deep
research and application examples on this basis which is conducted in virtual
geographic environment. Currently, there are many kinds of traffic passenger
flow forecasting models, and the common models include regression forecasting
model and time series prediction model. The coach passenger flow shows strong
regularity and stability without longterm change trend, so this research adopts
regression forecasting model to forecast the coach passenger flow
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01070</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01070</id><created>2015-04-04</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author></authors><title>Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via
  Eigenvector and Semidefinite Programming Synchronization</title><categories>cs.LG cs.SI math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classic problem of establishing a statistical ranking of a
set of n items given a set of inconsistent and incomplete pairwise comparisons
between such items. Instantiations of this problem occur in numerous
applications in data analysis (e.g., ranking teams in sports data), computer
vision, and machine learning. We formulate the above problem of ranking with
incomplete noisy information as an instance of the group synchronization
problem over the group SO(2) of planar rotations, whose usefulness has been
demonstrated in numerous applications in recent years. Its least squares
solution can be approximated by either a spectral or a semidefinite programming
(SDP) relaxation, followed by a rounding procedure. We perform extensive
numerical simulations on both synthetic and real-world data sets, showing that
our proposed method compares favorably to other algorithms from the recent
literature. Existing theoretical guarantees on the group synchronization
problem imply lower bounds on the largest amount of noise permissible in the
ranking data while still achieving exact recovery. We propose a similar
synchronization-based algorithm for the rank-aggregation problem, which
integrates in a globally consistent ranking pairwise comparisons given by
different rating systems on the same set of items. We also discuss the problem
of semi-supervised ranking when there is available information on the ground
truth rank of a subset of players, and propose an algorithm based on SDP which
recovers the ranks of the remaining players. Finally, synchronization-based
ranking, combined with a spectral technique for the densest subgraph problem,
allows one to extract locally-consistent partial rankings, in other words, to
identify the rank of a small subset of players whose pairwise comparisons are
less noisy than the rest of the data, which other methods are not able to
identify.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01072</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01072</id><created>2015-04-04</created><authors><author><keyname>Kokalj-Filipovic</keyname><forenames>Silvija</forenames></author><author><keyname>Greenstein</keyname><forenames>Larry</forenames></author></authors><title>EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by
  Noise and Interference</title><categories>cs.LG</categories><comments>CISS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for estimating channel parameters from RSSI measurements
and the lost packet count, which can work in the presence of losses due to both
interference and signal attenuation below the noise floor. This is especially
important in the wireless networks, such as vehicular, where propagation model
changes with the density of nodes. The method is based on Stochastic
Expectation Maximization, where the received data is modeled as a mixture of
distributions (no/low interference and strong interference), incomplete
(censored) due to packet losses. The PDFs in the mixture are Gamma, according
to the commonly accepted model for wireless signal and interference power. This
approach leverages the loss count as additional information, hence
outperforming maximum likelihood estimation, which does not use this
information (ML-), for a small number of received RSSI samples. Hence, it
allows inexpensive on-line channel estimation from ad-hoc collected data. The
method also outperforms ML- on uncensored data mixtures, as ML- assumes that
samples are from a single-mode PDF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01076</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01076</id><created>2015-04-04</created><authors><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Nearly-optimal bounds for sparse recovery in generic norms, with
  applications to $k$-median sketching</title><categories>cs.DS cs.CG cs.IT math.IT</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of trade-offs between sparsity and the number of
measurements in sparse recovery schemes for generic norms. Specifically, for a
norm $\|\cdot\|$, sparsity parameter $k$, approximation factor $K&gt;0$, and
probability of failure $P&gt;0$, we ask: what is the minimal value of $m$ so that
there is a distribution over $m \times n$ matrices $A$ with the property that
for any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in
the given norm with probability at least $1-P$? We give a partial answer to
this problem, by showing that for norms that admit efficient linear sketches,
the optimal number of measurements $m$ is closely related to the doubling
dimension of the metric induced by the norm $\|\cdot\|$ on the set of all
$k$-sparse vectors. By applying our result to specific norms, we cast known
measurement bounds in our general framework (for the $\ell_p$ norms, $p \in
[1,2]$) as well as provide new, measurement-efficient schemes (for the
Earth-Mover Distance norm). The latter result directly implies more succinct
linear sketches for the well-studied planar $k$-median clustering problem.
Finally, our lower bound for the doubling dimension of the EMD norm enables us
to address the open question of [Frahling-Sohler, STOC'05] about the space
complexity of clustering problems in the dynamic streaming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01085</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01085</id><created>2015-04-05</created><updated>2015-04-09</updated><authors><author><keyname>Gao</keyname><forenames>Bing</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>Stable Signal Recovery from Phaseless Measurements</title><categories>math.FA cs.IT math.IT</categories><doi>10.1007/s00041-015-9434-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study the stability of the $\ell_1$ minimization
for the compressive phase retrieval and to extend the instance-optimality in
compressed sensing to the real phase retrieval setting. We first show that the
$m={\mathcal O}(k\log(N/k))$ measurements is enough to guarantee the $\ell_1$
minimization to recover $k$-sparse signals stably provided the measurement
matrix $A$ satisfies the strong RIP property. We second investigate the
phaseless instance-optimality with presenting a null space property of the
measurement matrix $A$ under which there exists a decoder $\Delta$ so that the
phaseless instance-optimality holds. We use the result to study the phaseless
instance-optimality for the $\ell_1$ norm. The results build a parallel for
compressive phase retrieval with the classical compressive sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01090</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01090</id><created>2015-04-05</created><authors><author><keyname>Zahedi</keyname><forenames>Adel</forenames></author><author><keyname>&#xd8;stergaard</keyname><forenames>Jan</forenames></author><author><keyname>Jensen</keyname><forenames>S&#xf8;ren Holdt</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author><author><keyname>Bech</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>On the Covariance Matrix Distortion Constraint for the Gaussian
  Wyner-Ziv Problem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first present an explicit R(D) for the rate-distortion function (RDF) of
the vector Gaussian remote Wyner-Ziv problem with covariance matrix distortion
constraints. To prove the lower bound, we use a particular variant of joint
matrix diagonalization to establish a notion of the minimum of two symmetric
positive-definite matrices. We then show that from the resulting RDF, it is
possible to derive RDFs with different distortion constraints. Specifically, we
rederive the RDF for the vector Gaussian remote Wyner-Ziv problem with the
mean-squared error distortion constraint, and a rate-mutual information
function. This is done by minimizing R(D) subject to appropriate constraints on
the distortion matrix D. The key idea to solve the resulting minimization
problems is to lower-bound them with simpler optimization problems and show
that they lead to identical solutions. We thus illustrate the generality of the
covariance matrix distortion constraint in the Wyner-Ziv setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01092</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01092</id><created>2015-04-05</created><authors><author><keyname>Suchenek</keyname><forenames>Marek A.</forenames></author></authors><title>Technical Notes on Complexity of the Satisfiability Problem</title><categories>cs.CC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes contain, among others, a proof that the average running time of
an easy solution to the satisfiability problem for propositional calculus is,
under some reasonable assumptions, linear (with constant 2) in the size of the
input. Moreover, some suggestions are made about criteria for tractability of
complex algorithms. In particular, it is argued that the distribution of
probability on the whole input space of an algorithm constitutes an
non-negligible factor in estimating whether the algorithm is tractable or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01093</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01093</id><created>2015-04-05</created><authors><author><keyname>Cohen</keyname><forenames>Ilan Reuven</forenames></author><author><keyname>Eden</keyname><forenames>Alon</forenames></author><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Je&#x17c;</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Pricing Online Decisions: Beyond Auctions</title><categories>cs.GT cs.DS</categories><comments>Appeared in the ACM-SIAM Symposium on Discrete Algorithms (SODA),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dynamic pricing schemes in online settings where selfish agents
generate online events. Previous work on online mechanisms has dealt almost
entirely with the goal of maximizing social welfare or revenue in an auction
settings. This paper deals with quite general settings and minimizing social
costs. We show that appropriately computed posted prices allow one to achieve
essentially the same performance as the best online algorithm. This holds in a
wide variety of settings. Unlike online algorithms that learn about the event,
and then make enforceable decisions, prices are posted without knowing the
future events or even the current event, and are thus inherently dominant
strategy incentive compatible.
  In particular we show that one can give efficient posted price mechanisms for
metrical task systems, some instances of the $k$-server problem, and metrical
matching problems. We give both deterministic and randomized algorithms. Such
posted price mechanisms decrease the social cost dramatically over selfish
behavior where no decision incurs a charge. One alluring application of this is
reducing the social cost of free parking exponentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01099</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01099</id><created>2015-04-05</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Asad</forenames></author><author><keyname>Khan</keyname><forenames>Amir Ali</forenames></author><author><keyname>Mirza</keyname><forenames>Fauzan</forenames></author></authors><title>CRT and Fixed Patterns in Combinatorial Sequences</title><categories>cs.CR</categories><comments>New results on finite fields theory of combinatorial sequences and
  their CRT based analysis. arXiv admin note: substantial text overlap with
  arXiv:1503.00943</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new context of Chinese Remainder Theorem (CRT) based analysis
of combinatorial sequence generators has been presented. CRT is exploited to
establish fixed patterns in LFSR sequences and underlying cyclic structures of
finite fields. New methodology of direct computations of DFT spectral points in
higher finite fields from known DFT spectra points of smaller constituent
fields is also introduced. Novel approach of CRT based structural analysis of
LFSR based combinatorial sequence is given both in time and frequency domain.
The proposed approach is demonstrated on some examples of combiner generators
and is scalable to general configuration of combiner generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01100</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01100</id><created>2015-04-05</created><updated>2015-05-25</updated><authors><author><keyname>Wang</keyname><forenames>Taotao</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author></authors><title>Joint Multiple Symbol Differential Detection and Channel Decoding for
  Noncoherent UWB Impulse Radio by Belief Propagation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a belief propagation (BP) message passing algorithm based
joint multiple symbol differential detection (MSDD) and channel decoding scheme
for noncoherent differential ultra-wideband impulse radio (UWB-IR) systems.
MSDD is an effective means to improve the performance of noncoherent
differential UWB-IR systems. To optimize the overall detection and decoding
performance, in this paper, we propose a novel soft-in soft-out (SISO) MSDD
scheme and its integration with SISO channel decoding for noncoherent
differential UWB-IR. we first propose a new auto-correlation receiver (AcR)
architecture to sample the received UWB-IR signal. The proposed AcR can exploit
the dependencies (imposed by the differential modulation) among data symbols
throughout the whole packet. The signal probabilistic model has a hidden Markov
chain structure. We use a factor graph to represent this hidden Markov chain.
Then, we apply BP message passing algorithm on the factor graph to develop a
SISO MSDD scheme, which has better performance than the previous MSDD scheme
and is easy to be integrated with SISO channel decoding to form a joint MSDD
and channel decoding scheme. Simulation results indicate the performance
advantages of our MSDD scheme and joint MSDD and channel decoding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01101</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01101</id><created>2015-04-05</created><updated>2015-04-16</updated><authors><author><keyname>Mishra</keyname><forenames>Manoj</forenames></author><author><keyname>Sharma</keyname><forenames>Tanmay</forenames></author><author><keyname>Dey</keyname><forenames>Bikash K.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Private Data Transfer over a Broadcast Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>To be presented at IEEE International Symposium on Information Theory
  (ISIT 2015), Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following private data transfer problem: Alice has a database of
files. Bob and Cathy want to access a file each from this database (which may
or may not be the same file), but each of them wants to ensure that their
choices of file do not get revealed even if Alice colludes with the other user.
Alice, on the other hand, wants to make sure that each of Bob and Cathy does
not learn any more information from the database than the files they demand
(the identities of which will be unknown to her). Moreover, they should not
learn any information about the other files even if they collude.
  It turns out that it is impossible to accomplish this if Alice, Bob, and
Cathy have access only to private randomness and noiseless communication links.
We consider this problem when a binary erasure broadcast channel with
independent erasures is available from Alice to Bob and Cathy in addition to a
noiseless public discussion channel. We study the
file-length-per-broadcast-channel-use rate in the honest-but-curious model. We
focus on the case when the database consists of two files, and obtain the
optimal rate. We then extend to the case of larger databases, and give upper
and lower bounds on the optimal rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01106</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01106</id><created>2015-04-05</created><updated>2015-06-02</updated><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Xu</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Discriminative Neural Sentence Modeling by Tree-Based Convolution</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a tree-based convolutional neural network (TBCNN) for
discriminative sentence modeling. Our models leverage either constituency trees
or dependency trees of sentences. The tree-based convolution process extracts
sentences' structural features, and these features are aggregated by max
pooling. Such architecture allows short propagation paths between the output
layer and underlying feature detectors, which enables effective structural
feature learning and extraction. We evaluate our models on two tasks: sentiment
analysis and question classification. In both experiments, TBCNN outperforms
previous state-of-the-art results, including existing neural networks and
dedicated feature/rule engineering. We also make efforts to visualize the
tree-based convolution process, shedding light on how our models work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01112</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01112</id><created>2015-04-05</created><authors><author><keyname>Bahaddad</keyname><forenames>Adel A.</forenames></author><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>Adoption Factors for e-Malls in the SME Sector in Saudi Arabia</title><categories>cs.CY</categories><comments>22 pages, International Journal of Computer Science and Information
  Technologies, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The small and medium-sized enterprise (SME) sector represents one of the
fundamental pillars in the trade field. It contributes significantly to raising
the economies of countries by providing significant numbers of job
opportunities, which are beneficial to directly supporting national economies.
One of the most important obstacles facing this sector in the information
technology era is the lack of online trading channels with consumers, which
require more financial support than the their capabilities. Therefore, e-Malls
might be one of the best low-cost solutions to overcome this obstacle. Also,
they provide electronic platforms that include most SME requirements for sales
via electronic channels as well as offer essential technical support. According
to a report published in 2013 by the Saudi Arabian Monetary Agency (SAMA), the
percentage of SMEs is equivalent to 90% of the total number of companies in
Saudi Arabia, which is numbered at 848,500. The e-Mall is a modern idea in
Saudi Arabia that requires the use of the Disunion Of Innovation (DOI) approach
to diffuse e-Malls through determining companies requirements and difficulties.
Therefore, this paper focuses on the factors that help SMEs to adopt e-Malls. A
quantitative questionnaire was conducted on 108 companies in Saudi Arabia to
find what obstacles and requirements they face to adopt an e-Mall and focus on
the factors affecting the implementation of this system, which are divided into
organizational, technical and cultural factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01117</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01117</id><created>2015-04-05</created><authors><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author><author><keyname>Syed</keyname><forenames>Umar</forenames></author></authors><title>An $\tilde{O}(\frac{1}{\sqrt{T}})$-error online algorithm for retrieving
  heavily perturbated statistical databases in the low-dimensional querying
  mode</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first $\tilde{O}(\frac{1}{\sqrt{T}})$-error online algorithm for
reconstructing noisy statistical databases, where $T$ is the number of (online)
sample queries received. The algorithm, which requires only $O(\log T)$ memory,
aims to learn a hidden database-vector $w^{*} \in \mathbb{R}^{D}$ in order to
accurately answer a stream of queries regarding the hidden database, which
arrive in an online fashion from some unknown distribution $\mathcal{D}$. We
assume the distribution $\mathcal{D}$ is defined on the neighborhood of a
low-dimensional manifold. The presented algorithm runs in $O(dD)$-time per
query, where $d$ is the dimensionality of the query-space. Contrary to the
classical setting, there is no separate training set that is used by the
algorithm to learn the database --- the stream on which the algorithm will be
evaluated must also be used to learn the database-vector. The algorithm only
has access to a binary oracle $\mathcal{O}$ that answers whether a particular
linear function of the database-vector plus random noise is larger than a
threshold, which is specified by the algorithm. We note that we allow for a
significant $O(D)$ amount of noise to be added while other works focused on the
low noise $o(\sqrt{D})$-setting. For a stream of $T$ queries our algorithm
achieves an average error $\tilde{O}(\frac{1}{\sqrt{T}})$ by filtering out
random noise, adapting threshold values given to the oracle based on its
previous answers and, as a consequence, recovering with high precision a
projection of a database-vector $w^{*}$ onto the manifold defining the
query-space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01118</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01118</id><created>2015-04-05</created><authors><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author></authors><title>Learning how to rank from heavily perturbed statistics - digraph
  clustering approach</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking is one of the most fundamental problems in machine learning with
applications in many branches of computer science such as: information
retrieval systems, recommendation systems, machine translation and
computational biology. Ranking objects based on possibly conflicting
preferences is a central problem in voting research and social choice theory.
In this paper we present a new simple combinatorial ranking algorithm adapted
to the preference-based setting. We apply this new algorithm to the well-known
scenario where the edges of the preference tournament are determined by the
majority-voting model. It outperforms existing methods when it cannot be
assumed that there exists global ranking of good enough quality and applies
combinatorial techniques that havent been used in the ranking context before.
Performed experiments show the superiority of the new algorithm over existing
methods, also over these that were designed to handle heavily perturbed
statistics. By combining our techniques with those presented in \cite{mohri},
we obtain a purely combinatorial algorithm that answers correctly most of the
queries in the heterogeneous scenario, where the preference tournament is only
locally of good quality but is not necessarily pseudotransitive. As a byproduct
of our methods, we obtain the algorithm solving clustering problem for the
directed planted partition model. To the best of our knowledge, it is the first
purely combinatorial algorithm tackling this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01119</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01119</id><created>2015-04-05</created><authors><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Jebara</keyname><forenames>Tony</forenames></author></authors><title>Coloring tournaments with forbidden substructures</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coloring graphs is an important algorithmic problem in combinatorics with
many applications in computer science. In this paper we study coloring
tournaments. A chromatic number of a random tournament is of order
$\Omega(\frac{n}{\log(n)})$. The question arises whether the chromatic number
can be proven to be smaller for more structured nontrivial classes of
tournaments. We analyze the class of tournaments defined by a forbidden
subtournament $H$. This paper gives a first quasi-polynomial algorithm running
in time $e^{O(\log(n)^{2})}$ that constructs colorings of $H$-free tournaments
using only $O(n^{1-\epsilon(H)}\log(n))$ colors, where $\epsilon(H) \geq
2^{-2^{50|H|^{2}+1}}$ for many forbidden tournaments $H$. To the best of our
knowledge all previously known related results required at least
sub-exponential time and relied on the regularity lemma. Since we do not use
the regularity lemma, we obtain the first known lower bounds on $\epsilon(H)$
that can be given by a closed-form expression. As a corollary, we give a
constructive proof of the celebrated open Erd\H{o}s-Hajnal conjecture with
explicitly given lower bounds on the EH coefficients for all classes of prime
tournaments for which the conjecture is known. Such a constractive proof was
not known before. Thus we significantly reduce the gap between best lower and
upper bounds on the EH coefficients from the conjecture for all known prime
tournaments that satisfy it. We also briefly explain how our methods may be
used for coloring $H$-free tournaments under the following conditions: $H$ is
any tournament with $\leq 5$ vertices or: $H$ is any but one tournament of six
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01123</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01123</id><created>2015-04-05</created><updated>2015-08-29</updated><authors><author><keyname>Wang</keyname><forenames>Sinong</forenames></author><author><keyname>Li</keyname><forenames>Wenxin</forenames></author><author><keyname>Tian</keyname><forenames>Xiaohua</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Coded Caching with Heterogenous Cache Sizes</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the coded caching scheme under heterogenous cache sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01124</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01124</id><created>2015-04-05</created><updated>2015-12-01</updated><authors><author><keyname>C.</keyname><forenames>Amit Kumar K.</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author></authors><title>Discriminative and Efficient Label Propagation on Complementary Graphs
  for Multi-Object Tracking</title><categories>cs.CV</categories><comments>15 pages, 6 figures, submitted to PAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of detections, detected at each time instant independently, we
investigate how to associate them across time. This is done by propagating
labels on a set of graphs, each graph capturing how either the spatio-temporal
or the appearance cues promote the assignment of identical or distinct labels
to a pair of detections. The graph construction is motivated by a locally
linear embedding of the detection features. Interestingly, the neighborhood of
a node in appearance graph is defined to include all the nodes for which the
appearance feature is available (even if they are temporally distant). This
gives our framework the uncommon ability to exploit the appearance features
that are available only sporadically. Once the graphs have been defined,
multi-object tracking is formulated as the problem of finding a label
assignment that is consistent with the constraints captured each graph, which
results into a difference of convex (DC) program. We propose to decompose the
global objective function into node-wise sub-problems. This not only allows a
computationally efficient solution, but also supports an incremental and
scalable construction of the graph, thereby making the framework applicable to
large graphs and practical tracking scenarios. Moreover, it opens the
possibility of parallel implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01130</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01130</id><created>2015-04-05</created><updated>2015-07-15</updated><authors><author><keyname>Bruna</keyname><forenames>Maria</forenames></author><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Proving the Herman-Protocol Conjecture</title><categories>cs.DS cs.CC cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herman's self-stabilisation algorithm, introduced 25 years ago, is a
well-studied synchronous randomised protocol for enabling a ring of $N$
processes collectively holding any odd number of tokens to reach a stable state
in which a single token remains. Determining the worst-case expected time to
stabilisation is the central outstanding open problem about this protocol. It
is known that there is a constant $h$ such that any initial configuration has
expected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan
established a lower bound of $4/27 \approx 0.148$ for $h$, achieved with three
equally-spaced tokens, and conjectured this to be the optimal value of $h$. A
series of papers over the last decade gradually reduced the upper bound on $h$,
with the present record (achieved last year) standing at approximately $0.156$.
In this paper, we prove McIver and Morgan's conjecture and establish that $h =
4/27$ is indeed optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01139</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01139</id><created>2015-04-05</created><authors><author><keyname>Lakshminarayanan</keyname><forenames>Ramkumar</forenames></author><author><keyname>Ramalingam</keyname><forenames>Rajasekar</forenames></author><author><keyname>Shaik</keyname><forenames>Shimaz Khan</forenames></author></authors><title>Challenges in transforming, engaging and improving m-learning in Higher
  Educational Institutions: Oman perspective</title><categories>cs.CY</categories><comments>The Third International Conference of Educational Technology 24-26
  March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the student community is growing up with mobile devices and it has
becomes an integral part of their life. Devices such as smartphones, tablets,
and e-book readers connect users to access information and enabling instant
communication with others. The enormous growth and affordability of mobile
devices influenced their learning practices. Mobile technologies are playing a
significant role in students' academic activities. The factors like
convenience, flexibility, engagement, interactivity and easy-to-use enable
mobile learning more attractive to students. With these trends in mind, it is
important for the educators to inherit the mobile technologies in effective
teaching and learning. Our study explores the challenges that exist in
implementing the m-learning technologies in the teaching and learning practices
of higher educational institutions of Oman. Our study also addressed various
issue like adoption of technology, transition to new technology and issues
related to engaging students. Based on the outcomes of the study, a framework
has been formulated to address all the challenges that are identified for the
successful implementation of m-learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01140</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01140</id><created>2015-04-05</created><authors><author><keyname>Aminjavaheri</keyname><forenames>Amir</forenames></author><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Frequency Spreading Equalization in Multicarrier Massive MIMO</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE ICC 2015 - Workshop on 5G &amp; Beyond - Enabling
  Technologies and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application of filter bank multicarrier (FBMC) as an effective method for
signaling over massive MIMO channels has been recently proposed. This paper
further expands the application of FBMC to massive MIMO by applying frequency
spreading equalization (FSE) to these channels. FSE allows us to achieve a more
accurate equalization. Hence, higher number of bits per symbol can be
transmitted and the bandwidth of each subcarrier can be widened. Widening the
bandwidth of each subcarrier leads to (i) higher bandwidth efficiency; (ii)
lower complexity; (iii) lower sensitivity to carrier frequency offset (CFO);
(iv) reduced peak-to-average power ratio (PAPR); and (iv) reduced latency. All
these appealing advantages have a direct impact on the digital as well as
analog circuitry that is needed for the system implementation. In this paper,
we develop the mathematical formulation of the minimum mean square error (MMSE)
FSE for massive MIMO systems. This analysis guides us to decide on the number
of subcarriers that will be sufficient for practical channel models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01142</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01142</id><created>2015-04-05</created><authors><author><keyname>Nguyen</keyname><forenames>Nam-phuong</forenames></author><author><keyname>Mirarab</keyname><forenames>Siavash</forenames></author><author><keyname>Kumar</keyname><forenames>Keerthana</forenames></author><author><keyname>Warnow</keyname><forenames>Tandy</forenames></author></authors><title>Ultra-large alignments using Phylogeny-aware Profiles</title><categories>q-bio.GN cs.CE cs.LG</categories><comments>Online supplemental materials and data are available at
  http://www.cs.utexas.edu/users/phylo/software/upp/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many biological questions, including the estimation of deep evolutionary
histories and the detection of remote homology between protein sequences, rely
upon multiple sequence alignments (MSAs) and phylogenetic trees of large
datasets. However, accurate large-scale multiple sequence alignment is very
difficult, especially when the dataset contains fragmentary sequences. We
present UPP, an MSA method that uses a new machine learning technique - the
Ensemble of Hidden Markov Models - that we propose here. UPP produces highly
accurate alignments for both nucleotide and amino acid sequences, even on
ultra-large datasets or datasets containing fragmentary sequences. UPP is
available at https://github.com/smirarab/sepp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01145</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01145</id><created>2015-04-05</created><updated>2015-12-29</updated><authors><author><keyname>Babin</keyname><forenames>Mikhail A.</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author></authors><title>Dualization in Lattices Given by Ordered Sets of Irreducibles</title><categories>cs.LO cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dualization of a monotone Boolean function on a finite lattice can be
represented by transforming the set of its minimal 1 to the set of its maximal
0 values. In this paper we consider finite lattices given by ordered sets of
their meet and join irreducibles (i.e., as a concept lattice of a formal
context). We show that in this case dualization is equivalent to the
enumeration of so-called minimal hypotheses. In contrast to usual dualization
setting, where a lattice is given by the ordered set of its elements,
dualization in this case is shown to be impossible in output polynomial time
unless P = NP. However, if the lattice is distributive, dualization is shown to
be possible in subexponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01151</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01151</id><created>2015-04-05</created><authors><author><keyname>Cerruti</keyname><forenames>Giulio</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Gouaillier</keyname><forenames>David</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Sakka</keyname><forenames>Sophie</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Design method for an anthropomorphic hand able to gesture and grasp</title><categories>cs.RO</categories><comments>IEEE International Conference on Robotics and Automation, May 2015,
  Seattle, United States. IEEE, 2015, Proceeding IEEE International Conference
  on Robotics and Automation</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a numerical method to conceive and design the kinematic
model of an anthropomorphic robotic hand used for gesturing and grasping. In
literature, there are few numerical methods for the finger placement of
human-inspired robotic hands. In particular, there are no numerical methods,
for the thumb placement, that aim to improve the hand dexterity and grasping
capabilities by keeping the hand design close to the human one. While existing
models are usually the result of successive parameter adjustments, the proposed
method determines the fingers placements by mean of empirical tests. Moreover,
a surgery test and the workspace analysis of the whole hand are used to find
the best thumb position and orientation according to the hand kinematics and
structure. The result is validated through simulation where it is checked that
the hand looks well balanced and that it meets our constraints and needs. The
presented method provides a numerical tool which allows the easy computation of
finger and thumb geometries and base placements for a human-like dexterous
robotic hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01158</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01158</id><created>2015-04-05</created><authors><author><keyname>Malek-Mohammadi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Iterative Concave Rank Approximation for Recovering Low-Rank Matrices</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. on Signal Processing, vol. 62, no. 20</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new algorithm for recovery of low-rank matrices
from compressed linear measurements. The underlying idea of this algorithm is
to closely approximate the rank function with a smooth function of singular
values, and then minimize the resulting approximation subject to the linear
constraints. The accuracy of the approximation is controlled via a scaling
parameter $\delta$, where a smaller $\delta$ corresponds to a more accurate
fitting. The consequent optimization problem for any finite $\delta$ is
nonconvex. Therefore, in order to decrease the risk of ending up in local
minima, a series of optimizations is performed, starting with optimizing a
rough approximation (a large $\delta$) and followed by successively optimizing
finer approximations of the rank with smaller $\delta$'s. To solve the
optimization problem for any $\delta &gt; 0$, it is converted to a new program in
which the cost is a function of two auxiliary positive semidefinete variables.
The paper shows that this new program is concave and applies a
majorize-minimize technique to solve it which, in turn, leads to a few convex
optimization iterations. This optimization scheme is also equivalent to a
reweighted Nuclear Norm Minimization (NNM), where weighting update depends on
the used approximating function. For any $\delta &gt; 0$, we derive a necessary
and sufficient condition for the exact recovery which are weaker than those
corresponding to NNM. On the numerical side, the proposed algorithm is compared
to NNM and a reweighted NNM in solving affine rank minimization and matrix
completion problems showing its considerable and consistent superiority in
terms of success rate, especially, when the number of measurements decreases
toward the lower-bound for the unique representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01161</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01161</id><created>2015-04-05</created><authors><author><keyname>Jarecka</keyname><forenames>Dorota</forenames></author><author><keyname>Arabas</keyname><forenames>Sylwester</forenames></author><author><keyname>Del Vento</keyname><forenames>Davide</forenames></author></authors><title>Python bindings for libcloudph++</title><categories>physics.comp-ph cs.MS physics.ao-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note introduces the Python bindings for libcloudph++. The
libcloudph++ is a C++ library of algorithms for representing atmospheric cloud
microphysics in numerical models. The bindings expose the complete
functionality of the library to the Python users. The bindings are implemented
using the Boost.Python C++ library and use NumPy arrays. This note includes
listings with Python scripts exemplifying the use of selected library
components. An example solution for using the Python bindings to access
libcloudph++ from Fortran is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01163</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01163</id><created>2015-04-05</created><updated>2015-09-08</updated><authors><author><keyname>Pizzi</keyname><forenames>Giovanni</forenames></author><author><keyname>Cepellotti</keyname><forenames>Andrea</forenames></author><author><keyname>Sabatini</keyname><forenames>Riccardo</forenames></author><author><keyname>Marzari</keyname><forenames>Nicola</forenames></author><author><keyname>Kozinsky</keyname><forenames>Boris</forenames></author></authors><title>AiiDA: Automated Interactive Infrastructure and Database for
  Computational Science</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.SE</categories><comments>30 pages, 7 figures</comments><journal-ref>G. Pizzi, A. Cepellotti, R. Sabatini, N. Marzari, and B. Kozinsky,
  Comp. Mat. Sci 111, 218-230 (2016)</journal-ref><doi>10.1016/j.commatsci.2015.09.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational science has seen in the last decades a spectacular rise in the
scope, breadth, and depth of its efforts. Notwithstanding this prevalence and
impact, it is often still performed using the renaissance model of individual
artisans gathered in a workshop, under the guidance of an established
practitioner. Great benefits could follow instead from adopting concepts and
tools coming from computer science to manage, preserve, and share these
computational efforts. We illustrate here our paradigm sustaining such vision,
based around the four pillars of Automation, Data, Environment, and Sharing. We
then discuss its implementation in the open-source AiiDA platform
(http://www.aiida.net), that has been tuned first to the demands of
computational materials science. AiiDA's design is based on directed acyclic
graphs to track the provenance of data and calculations, and ensure
preservation and searchability. Remote computational resources are managed
transparently, and automation is coupled with data storage to ensure
reproducibility. Last, complex sequences of calculations can be encoded into
scientific workflows. We believe that AiiDA's design and its sharing
capabilities will encourage the creation of social ecosystems to disseminate
codes, data, and scientific workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01166</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01166</id><created>2015-04-05</created><authors><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>An extension of the Ky Fan inequality</title><categories>cs.IT math.IT math.PR</categories><comments>12 pages, 7 figures</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to analyze the weighted KyFan inequality proposed in
[11]. A number of numerical simulations involving the exponential weighted
function is given. We show that in several cases and types of examples one can
imply an improvement of the standard KyFan inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01167</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01167</id><created>2015-04-05</created><authors><author><keyname>Sezener</keyname><forenames>Can Eren</forenames></author><author><keyname>Oztop</keyname><forenames>Erhan</forenames></author></authors><title>Heuristic algorithms for obtaining Polynomial Threshold Functions with
  low densities</title><categories>cs.CC cs.NE</categories><comments>This paper will appear in the 13th Cologne-Twente Workshop on Graphs
  &amp; Combinatorial Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present several heuristic algorithms, including a Genetic
Algorithm (GA), for obtaining polynomial threshold function (PTF)
representations of Boolean functions (BFs) with small number of monomials. We
compare these among each other and against the algorithm of Oztop via
computational experiments. The results indicate that our heuristic algorithms
find more parsimonious representations compared to the those of non-heuristic
and GA-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01169</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01169</id><created>2015-04-05</created><authors><author><keyname>Pourkamali-Anaraki</keyname><forenames>Farhad</forenames></author><author><keyname>Becker</keyname><forenames>Stephen</forenames></author><author><keyname>Hughes</keyname><forenames>Shannon M.</forenames></author></authors><title>Efficient Dictionary Learning via Very Sparse Random Projections</title><categories>stat.ML cs.LG</categories><comments>5 pages, 2 figures, accepted in Sampling Theory and Applications
  (SampTA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing signal processing tasks on compressive measurements of data has
received great attention in recent years. In this paper, we extend previous
work on compressive dictionary learning by showing that more general random
projections may be used, including sparse ones. More precisely, we examine
compressive K-means clustering as a special case of compressive dictionary
learning and give theoretical guarantees for its performance for a very general
class of random projections. We then propose a memory and computation efficient
dictionary learning algorithm, specifically designed for analyzing large
volumes of high-dimensional data, which learns the dictionary from very sparse
random projections. Experimental results demonstrate that our approach allows
for reduction of computational complexity and memory/data access, with
controllable loss in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01173</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01173</id><created>2015-04-05</created><authors><author><keyname>Choi</keyname><forenames>Arthur</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Dual Decomposition from the Perspective of Relax, Compensate and then
  Recover</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relax, Compensate and then Recover (RCR) is a paradigm for approximate
inference in probabilistic graphical models that has previously provided
theoretical and practical insights on iterative belief propagation and some of
its generalizations. In this paper, we characterize the technique of dual
decomposition in the terms of RCR, viewing it as a specific way to compensate
for relaxed equivalence constraints. Among other insights gathered from this
perspective, we propose novel heuristics for recovering relaxed equivalence
constraints with the goal of incrementally tightening dual decomposition
approximations, all the way to reaching exact solutions. We also show
empirically that recovering equivalence constraints can sometimes tighten the
corresponding approximation (and obtaining exact results), without increasing
much the complexity of inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01175</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01175</id><created>2015-04-05</created><authors><author><keyname>Semaev</keyname><forenames>Igor</forenames></author></authors><title>New algorithm for the discrete logarithm problem on elliptic curves</title><categories>cs.CR cs.CC math.AC math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new algorithms for computing discrete logarithms on elliptic curves defined
over finite fields is suggested. It is based on a new method to find zeroes of
summation polynomials. In binary elliptic curves one is to solve a cubic system
of Boolean equations. Under a first fall degree assumption the regularity
degree of the system is at most $4$. Extensive experimental data which supports
the assumption is provided. An heuristic analysis suggests a new asymptotical
complexity bound $2^{c\sqrt{n\ln n}}, c\approx 1.69$ for computing discrete
logarithms on an elliptic curve over a field of size $2^n$. For several binary
elliptic curves recommended by FIPS the new method performs better than
Pollard's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01182</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01182</id><created>2015-04-05</created><authors><author><keyname>Kalita</keyname><forenames>Nayan Jyoti</forenames></author><author><keyname>Islam</keyname><forenames>Baharul</forenames></author></authors><title>Bengali to Assamese Statistical Machine Translation using Moses (Corpus
  Based)</title><categories>cs.CL</categories><comments>6 pages, International Conference on Cognitive Computing and
  Information Processing (CCIP-15), 3-4 March 2015, Noida (India)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine dialect interpretation assumes a real part in encouraging man-machine
correspondence and in addition men-men correspondence in Natural Language
Processing (NLP). Machine Translation (MT) alludes to utilizing machine to
change one dialect to an alternate. Statistical Machine Translation is a type
of MT consisting of Language Model (LM), Translation Model (TM) and decoder. In
this paper, Bengali to Assamese Statistical Machine Translation Model has been
created by utilizing Moses. Other translation tools like IRSTLM for Language
Model and GIZA-PP-V1.0.7 for Translation model are utilized within this
framework which is accessible in Linux situations. The purpose of the LM is to
encourage fluent output and the purpose of TM is to encourage similarity
between input and output, the decoder increases the probability of translated
text in target language. A parallel corpus of 17100 sentences in Bengali and
Assamese has been utilized for preparing within this framework. Measurable MT
procedures have not so far been generally investigated for Indian dialects. It
might be intriguing to discover to what degree these models can help the
immense continuous MT deliberations in the nation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01183</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01183</id><created>2015-04-05</created><authors><author><keyname>Jha</keyname><forenames>Monica</forenames></author></authors><title>Document Clustering using K-Medoids</title><categories>cs.IR</categories><comments>5 pages</comments><journal-ref>International Journal on Advanced Computer Theory and Engineering
  (IJACTE), ISSN (Print): 2319-2526, Volume-4, Issue-1, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People are always in search of matters for which they are prone to use
internet, but again it has huge assemblage of data due to which it becomes
difficult for the reader to get the most accurate data. To make it easier for
people to gather accurate data, similar information has to be clustered at one
place. There are many algorithms used for clustering of relevant information in
one platform. In this paper, K-Medoids clustering algorithm has been employed
for formation of clusters which is further used for document summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01185</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01185</id><created>2015-04-05</created><authors><author><keyname>Zhang</keyname><forenames>Linyuan</forenames></author><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author></authors><title>Byzantine Attack and Defense in Cognitive Radio Networks: A Survey</title><categories>cs.NI cs.CR cs.IT math.IT</categories><comments>Accepted by IEEE Communications Surveys and Tutoirals</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Byzantine attack in cooperative spectrum sensing (CSS), also known as the
spectrum sensing data falsification (SSDF) attack in the literature, is one of
the key adversaries to the success of cognitive radio networks (CRNs). In the
past couple of years, the research on the Byzantine attack and defense
strategies has gained worldwide increasing attention. In this paper, we provide
a comprehensive survey and tutorial on the recent advances in the Byzantine
attack and defense for CSS in CRNs. Specifically, we first briefly present the
preliminaries of CSS for general readers, including signal detection
techniques, hypothesis testing, and data fusion. Second, we analyze the spear
and shield relation between Byzantine attack and defense from three aspects:
the vulnerability of CSS to attack, the obstacles in CSS to defense, and the
games between attack and defense. Then, we propose a taxonomy of the existing
Byzantine attack behaviors and elaborate on the corresponding attack
parameters, which determine where, who, how, and when to launch attacks. Next,
from the perspectives of homogeneous or heterogeneous scenarios, we classify
the existing defense algorithms, and provide an in-depth tutorial on the
state-of-the-art Byzantine defense schemes, commonly known as robust or secure
CSS in the literature. Furthermore, we highlight the unsolved research
challenges and depict the future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01191</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01191</id><created>2015-04-05</created><authors><author><keyname>Wu</keyname><forenames>Jinbiao</forenames></author><author><keyname>Peng</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Zaiming</forenames></author></authors><title>On the BMAP_1, BMAP_2/PH/g, c retrial queueing system</title><categories>math.PR cs.PF</categories><comments>28 pages with 3 figures</comments><msc-class>60K25</msc-class><acm-class>G.3.9</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we analyze a retrial queueing system with Batch Markovian
Arrival Processes and two types of customers. The rate of individual repeated
attempts from the orbit is modulated according to a Markov Modulated Poisson
Process. Using the theory of multi-dimensional asymptotically quasi-Toeplitz
Markov chain, we obtain the stability condition and the algorithm for
calculating the stationary state distribution of the system. Main performance
measures are presented. Furthermore, we investigate some optimization problems.
The algorithm for determining the optimal number of guard servers and total
servers is elaborated. Finally, this queueing system is applied to the cellular
wireless network. Numerical results to illustrate the optimization problems and
the impact of retrial on performance measures are provided. We find that the
performance measures are mainly affected by the two types of customers'
arrivals and service patterns, but the retrial rate plays a less crucial role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01192</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01192</id><created>2015-04-05</created><authors><author><keyname>Gu</keyname><forenames>Wenjun</forenames></author><author><keyname>Kashani</keyname><forenames>Mohammadreza A.</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>Multipath Reflections Analysis on Indoor Visible Light Positioning
  System</title><categories>physics.optics cs.NI</categories><comments>Submitted to IEEE Globecom 2015, 7 Pages, 13 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) has become a promising research topic in
recent years, and finds its wide applications in indoor environments.
Particularly, for location based services (LBS), visible light also provides a
practical solution for indoor positioning. Multipath-induced dispersion is one
of the major concerns for complex indoor environments. It affects not only the
communication performance but also the positioning accuracy. In this paper, we
investigate the impact of multipath reflections on the positioning accuracy of
indoor VLC positioning systems. Combined Deterministic and Modified Monte Carlo
(CDMMC) approach is applied to estimate the channel impulse response
considering multipath reflections. Since the received signal strength (RSS)
information is used for the positioning algorithm, the power distribution from
one transmitter in a typical room configuration is first calculated. Then, the
positioning accuracy in terms of root mean square error is obtained and
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01207</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01207</id><created>2015-04-06</created><authors><author><keyname>Safavi</keyname><forenames>Sam</forenames></author><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author></authors><title>Localization and tracking in mobile networks: Virtual convex hulls and
  beyond</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the problem of tracking the locations of an
arbitrary number of agents moving in a bounded region. Assuming that each agent
knows its motion precisely, and also its distances and angles to the nodes in
its communication radius, we provide a \emph{geometric approach} to continually
update the distances and angles even when the agents move out of range. Based
on this approach, we provide a simple \emph{linear} update to track the
locations of an arbitrary number of mobile agents when they follow some
convexity in their deployment and motion, given at least $m+1$ agents whose
locations are always known (hereinafter referred to as anchors) in
${\mathbb{R}}^{m}$. More precisely, this linear update requires the agents to
intermittently move inside the convex hull of the anchors.
  Since the agents are mobile, they may move inside and outside of the convex
hull formed by the anchors and the above convexity in the deployment is not
necessarily satisfied. To deal with such issues, we introduce the notion of a
\emph{virtual convex hull} with the help of the aforementioned geometric
approach. Based on the virtual hulls, we provide another algorithm where no
agent is required to lie in any convex hull at any given time. Each agent only
keeps track of a virtual convex hull, which may not physically exist, of other
nodes (agents and/or anchors), and updates its location with respect to its
neighbors in the virtual hull. We show that the corresponding localization
algorithm can be abstracted as a Linear Time-Varying (LTV) system that
asymptotically tracks the true locations of the agents. Finally, we show that
exactly one anchor suffices to track the locations of an arbitrary number of
mobile agents in $\mathbb{R}^m,m\geq 1$, using the approach described in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01218</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01218</id><created>2015-04-06</created><authors><author><keyname>Karim</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author></authors><title>Instantly Decodable Network Coding for Real-Time Scalable Video
  Broadcast over Wireless Networks</title><categories>cs.IT math.IT</categories><doi>10.1186/s13634-015-0299-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a real-time scalable video broadcast over wireless
networks in instantly decodable network coded (IDNC) systems. Such real-time
scalable video has a hard deadline and imposes a decoding order on the video
layers.We first derive the upper bound on the probability that the individual
completion times of all receivers meet the deadline. Using this probability, we
design two prioritized IDNC algorithms, namely the expanding window IDNC
(EW-IDNC) algorithm and the non-overlapping window IDNC (NOW-IDNC) algorithm.
These algorithms provide a high level of protection to the most important video
layer before considering additional video layers in coding decisions. Moreover,
in these algorithms, we select an appropriate packet combination over a given
number of video layers so that these video layers are decoded by the maximum
number of receivers before the deadline. We formulate this packet selection
problem as a two-stage maximal clique selection problem over an IDNC graph.
Simulation results over a real scalable video stream show that our proposed
EW-IDNC and NOW-IDNC algorithms improve the received video quality compared to
the existing IDNC algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01220</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01220</id><created>2015-04-06</created><authors><author><keyname>Liu</keyname><forenames>Si</forenames></author><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Liu</keyname><forenames>Luoqi</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Xu</keyname><forenames>Changsheng</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Cao</keyname><forenames>Xiaochun</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Matching-CNN Meets KNN: Quasi-Parametric Human Parsing</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both parametric and non-parametric approaches have demonstrated encouraging
performances in the human parsing task, namely segmenting a human image into
several semantic regions (e.g., hat, bag, left arm, face). In this work, we aim
to develop a new solution with the advantages of both methodologies, namely
supervision from annotated data and the flexibility to use newly annotated
(possibly uncommon) images, and present a quasi-parametric human parsing model.
Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the
parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict
the matching confidence and displacements of the best matched region in the
testing image for a particular semantic region in one KNN image. Given a
testing image, we first retrieve its KNN images from the
annotated/manually-parsed human image corpus. Then each semantic region in each
KNN image is matched with confidence to the testing image using M-CNN, and the
matched regions from all KNN images are further fused, followed by a superpixel
smoothing procedure to obtain the ultimate human parsing result. The M-CNN
differs from the classic CNN in that the tailored cross image matching filters
are introduced to characterize the matching between the testing image and the
semantic region of a KNN image. The cross image matching filters are defined at
different convolutional layers, each aiming to capture a particular range of
displacements. Comprehensive evaluations over a large dataset with 7,700
annotated human images well demonstrate the significant performance gain from
the quasi-parametric model over the state-of-the-arts, for the human parsing
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01252</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01252</id><created>2015-04-06</created><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Pascal</keyname><forenames>Frederic</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Convergence and Fluctuations of Regularized Tyler Estimators</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the behavior of regularized Tyler estimators (RTEs) of
scatter matrices. The key advantages of these estimators are twofold. First,
they guarantee by construction a good conditioning of the estimate and second,
being a derivative of robust Tyler estimators, they inherit their robustness
properties, notably their resilience to the presence of outliers. Nevertheless,
one major problem that poses the use of RTEs in practice is represented by the
question of setting the regularization parameter $\rho$. While a high value of
$\rho$ is likely to push all the eigenvalues away from zero, it comes at the
cost of a larger bias with respect to the population covariance matrix. A deep
understanding of the statistics of RTEs is essential to come up with
appropriate choices for the regularization parameter. This is not an easy task
and might be out of reach, unless one considers asymptotic regimes wherein the
number of observations $n$ and/or their size $N$ increase together. First
asymptotic results have recently been obtained under the assumption that $N$
and $n$ are large and commensurable. Interestingly, no results concerning the
regime of $n$ going to infinity with $N$ fixed exist, even though the
investigation of this assumption has usually predated the analysis of the most
difficult $N$ and $n$ large case. This motivates our work. In particular, we
prove in the present paper that the RTEs converge to a deterministic matrix
when $n\to\infty$ with $N$ fixed, which is expressed as a function of the
theoretical covariance matrix. We also derive the fluctuations of the RTEs
around this deterministic matrix and establish that these fluctuations converge
in distribution to a multivariate Gaussian distribution with zero mean and a
covariance depending on the population covariance and the parameter $\rho$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01255</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01255</id><created>2015-04-06</created><updated>2015-11-01</updated><authors><author><keyname>Johnson</keyname><forenames>Rie</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Semi-supervised Convolutional Neural Networks for Text Categorization
  via Region Embedding</title><categories>stat.ML cs.CL cs.LG</categories><comments>v1 has a different title, and the results there are obsolete. The
  current version is to appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new semi-supervised framework with convolutional neural
networks (CNNs) for text categorization. Unlike the previous approaches that
rely on word embeddings, our method learns embeddings of small text regions
from unlabeled data for integration into a supervised CNN. The proposed scheme
for embedding learning is based on the idea of two-view semi-supervised
learning, which is intended to be useful for the task of interest even though
the training is done on unlabeled data. Our models achieve better results than
previous approaches on sentiment classification and topic classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01257</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01257</id><created>2015-04-06</created><authors><author><keyname>N</keyname><forenames>Lakshmi H</forenames></author><author><keyname>Mohanty</keyname><forenames>Hrushikesha</forenames></author></authors><title>Usages of Composition Search Tree in Web Service Composition</title><categories>cs.SE cs.DC</categories><comments>11 Pages ISSN : 0973-8215 IK International Publishing House Pvt.
  Ltd., New Delhi, India</comments><journal-ref>International Journal of Information Processing, 9(1), 28-37, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of web services within an organization and on the
Web demands for efficient search and composition mechanisms to find services
satisfying user requirements. Often consumers may be unaware of exact service
names that is fixed by service providers. Rather consumers being well aware of
their requirements would like to search a service based on their commitments
(inputs) and expectations (outputs). Based on this concept we have explored the
feasibility of I/O based web service search and composition in our previous
work. The classical definition of service composition, i.e., one-to-one and
onto mapping between input and output sets of composing services, is extended
to give rise to three types of service match: Exact, Super and Partial match.
Based on matches of all three types, different kinds of compositions are
defined: Exact, Super and Collaborative Composition. Process of composition,
being a match between inputs and outputs of services, is hastened by making use
of information on service dependency that is made available in repository as an
one time preprocessed information obtained from services populating the
registry. Adopting three schemes for matching for a desired service outputs,
the possibility of having different kinds of compositions is demonstrated in
form of a Composition Search Tree. As an extension to our previous work, in
this paper, we propose the utility of Composition Search Tree for finding
compositions of interest like leanest and the shortest depth compositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01258</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01258</id><created>2015-04-06</created><authors><author><keyname>Pakrooh</keyname><forenames>Pooria</forenames></author><author><keyname>Scharf</keyname><forenames>Louis L.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author></authors><title>Modal Analysis Using Sparse and Co-prime Arrays</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let a measurement consist of a linear combination of damped complex
exponential modes, plus noise. The problem is to estimate the parameters of
these modes, as in line spectrum estimation, vibration analysis, speech
processing, system identification, and direction of arrival estimation. Our
results differ from standard results of modal analysis to the extent that we
consider sparse and co-prime samplings in space, or equivalently sparse and
co-prime samplings in time. Our main result is a characterization of the
orthogonal subspace. This is the subspace that is orthogonal to the signal
subspace spanned by the columns of the generalized Vandermonde matrix of modes
in sparse or co-prime arrays. This characterization is derived in a form that
allows us to adapt modern methods of linear prediction and approximate least
squares, such as iterative quadratic maximum likelihood (IQML), for estimating
mode parameters. Several numerical examples are presented to demonstrate the
validity of the proposed modal estimation methods, and to compare the fidelity
of modal estimation with sparse and co-prime arrays, versus SNR. Our
calculations of Cram\'{e}r-Rao bounds allow us to analyze the loss in
performance sustained by sparse and co-prime arrays that are compressions of
uniform linear arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01274</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01274</id><created>2015-04-06</created><authors><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author><author><keyname>Li</keyname><forenames>Shuxing</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>The Weight Hierarchy of Some Reducible Cyclic Codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized Hamming weights (GHWs) of linear codes are fundamental
parameters, the knowledge of which is of great interest in many applications.
However, to determine the GHWs of linear codes is difficult in general. In this
paper, we study the GHWs for a family of reducible cyclic codes and obtain the
complete weight hierarchy in several cases. This is achieved by extending the
idea of \cite{YLFL} into higher dimension and by employing some interesting
combinatorial arguments. It shall be noted that these cyclic codes may have
arbitrary number of nonzeroes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01287</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01287</id><created>2015-04-06</created><authors><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Hancock</keyname><forenames>Braden</forenames></author><author><keyname>Kaiser</keyname><forenames>Benjamin</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Michaleas</keyname><forenames>Pete</forenames></author><author><keyname>Varia</keyname><forenames>Mayank</forenames></author><author><keyname>Yerukhimovich</keyname><forenames>Arkady</forenames></author></authors><title>Computing on Masked Data to improve the Security of Big Data</title><categories>cs.CR</categories><comments>6 pages, Accepted to IEEE HST Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizations that make use of large quantities of information require the
ability to store and process data from central locations so that the product
can be shared or distributed across a heterogeneous group of users. However,
recent events underscore the need for improving the security of data stored in
such untrusted servers or databases. Advances in cryptographic techniques and
database technologies provide the necessary security functionality but rely on
a computational model in which the cloud is used solely for storage and
retrieval. Much of big data computation and analytics make use of signal
processing fundamentals for computation. As the trend of moving data storage
and computation to the cloud increases, homeland security missions should
understand the impact of security on key signal processing kernels such as
correlation or thresholding. In this article, we propose a tool called
Computing on Masked Data (CMD), which combines advances in database
technologies and cryptographic tools to provide a low overhead mechanism to
offload certain mathematical operations securely to the cloud. This article
describes the design and development of the CMD tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01294</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01294</id><created>2015-04-06</created><authors><author><keyname>Asamov</keyname><forenames>Tsvetan</forenames></author><author><keyname>Ben-Israel</keyname><forenames>Adi</forenames></author></authors><title>An $\ell_1$-Method for Clustering High-Dimensional Data</title><categories>math.ST cs.LG math.OC stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, the clustering problem is NP-hard, and global optimality cannot
be established for non-trivial instances. For high-dimensional data,
distance-based methods for clustering or classification face an additional
difficulty, the unreliability of distances in very high-dimensional spaces. We
propose a distance-based iterative method for clustering data in very
high-dimensional space, using the $\ell_1$-metric that is less sensitive to
high dimensionality than the Euclidean distance. For $K$ clusters in
$\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by
probabilities, and an iteration reduces to finding $Kn$ weighted medians of
points on a line. The complexity of the algorithm is linear in the dimension of
the data space, and its performance was observed to improve significantly as
the dimension increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01304</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01304</id><created>2015-04-06</created><authors><author><keyname>Ye</keyname><forenames>Yuzhen</forenames></author><author><keyname>Tang</keyname><forenames>Haixu</forenames></author></authors><title>Utilizing de Bruijn graph of metagenome assembly for metatranscriptome
  analysis</title><categories>q-bio.GN cs.CE cs.DS</categories><comments>8 pages, 4 figures, accepted in RECOMB-Seq 2015, under consideration
  in Bioinformatics (a special issue for RECOMB-Seq/CBB)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metagenomics research has accelerated the studies of microbial organisms,
providing insights into the composition and potential functionality of various
microbial communities. Metatranscriptomics (studies of the transcripts from a
mixture of microbial species) and other meta-omics approaches hold even greater
promise for providing additional insights into functional and regulatory
characteristics of the microbial communities. Current metatranscriptomics
projects are often carried out without matched metagenomic datasets (of the
same microbial communities). For the projects that produce both
metatranscriptomic and metagenomic datasets, their analyses are often not
integrated. Metagenome assemblies are far from perfect, partially explaining
why metagenome assemblies are not used for the analysis of metatranscriptomic
datasets. Here we report a reads mapping algorithm for mapping of short reads
onto a de Bruijn graph of assemblies. A hash table of junction k-mers (k-mers
spanning branching structures in the de Bruijn graph) is used to facilitate
fast mapping of reads to the graph. We developed an application of this mapping
algorithm: a reference based approach to metatranscriptome assembly using
graphs of metagenome assembly as the reference. Our results show that this new
approach (called TAG) helps to assemble substantially more transcripts that
otherwise would have been missed or truncated because of the fragmented nature
of the reference metagenome. TAG was implemented in C++ and has been tested
extensively on the linux platform. It is available for download as open source
at http://omics.informatics.indiana.edu/TAG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01310</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01310</id><created>2015-04-06</created><updated>2015-06-15</updated><authors><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Hall</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author></authors><title>Reproducibility as a Technical Specification</title><categories>cs.CE</categories><comments>Submitted to the 18th IEEE International Conference on Computational
  Science and Engineering (CSE 2015); 6 pages, LaTeX. arXiv admin note:
  substantial text overlap with arXiv:1502.02448</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reproducibility of computationally-derived scientific discoveries should be a
certainty. As the product of several person-years' worth of effort, results --
whether disseminated through academic journals, conferences or exploited
through commercial ventures -- should at some level be expected to be
repeatable by other researchers. While this stance may appear to be obvious and
trivial, a variety of factors often stand in the way of making it commonplace.
Whilst there has been detailed cross-disciplinary discussions of the various
social, cultural and ideological drivers and (potential) solutions, one factor
which has had less focus is the concept of reproducibility as a technical
challenge. Specifically, that the definition of an unambiguous and measurable
standard of reproducibility would offer a significant benefit to the wider
computational science community.
  In this paper, we propose a high-level technical specification for a service
for reproducibility, presenting cyberinfrastructure and associated workflow for
a service which would enable such a specification to be verified and validated.
In addition to addressing a pressing need for the scientific community, we
further speculate on the potential contribution to the wider software
development community of services which automate de novo compilation and
testing of code from source. We illustrate our proposed specification and
workflow by using the BioModelAnalyzer tool as a running example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01311</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01311</id><created>2015-04-06</created><authors><author><keyname>Fox-Epstein</keyname><forenames>Eli</forenames></author><author><keyname>Levin</keyname><forenames>Roie</forenames></author><author><keyname>Meierfrankenfeld</keyname><forenames>David</forenames></author></authors><title>PTAS for MAP Assignment on Pairwise Markov Random Fields in Planar
  Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a PTAS for computing the maximum a posteriori assignment on
Pairwise Markov Random Fields with non-negative weights in planar graphs. This
algorithm is practical and not far behind state-of-the-art techniques in image
processing. MAP on Pairwise Markov Random Fields with (possibly) negative
weights cannot be approximated unless P = NP, even on planar graphs. We also
show via reduction that this yields a PTAS for one scoring function of
Correlation Clustering in planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01316</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01316</id><created>2015-04-06</created><authors><author><keyname>Lima</keyname><forenames>Antonio</forenames></author><author><keyname>Pejovic</keyname><forenames>Veljko</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta</forenames></author></authors><title>Progmosis: Evaluating Risky Individual Behavior During Epidemics Using
  Mobile Network Data</title><categories>cs.SI physics.soc-ph</categories><comments>D4D Senegal - NetMob 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility to analyze, quantify and forecast epidemic outbreaks is
fundamental when devising effective disease containment strategies. Policy
makers are faced with the intricate task of drafting realistically
implementable policies that strike a balance between risk management and cost.
Two major techniques policy makers have at their disposal are: epidemic
modeling and contact tracing. Models are used to forecast the evolution of the
epidemic both globally and regionally, while contact tracing is used to
reconstruct the chain of people who have been potentially infected, so that
they can be tested, isolated and treated immediately. However, both techniques
might provide limited information, especially during an already advanced crisis
when the need for action is urgent.
  In this paper we propose an alternative approach that goes beyond epidemic
modeling and contact tracing, and leverages behavioral data generated by mobile
carrier networks to evaluate contagion risk on a per-user basis. The individual
risk represents the loss incurred by not isolating or treating a specific
person, both in terms of how likely it is for this person to spread the disease
as well as how many secondary infections it will cause. To this aim, we develop
a model, named Progmosis, which quantifies this risk based on movement and
regional aggregated statistics about infection rates. We develop and release an
open-source tool that calculates this risk based on cellular network events. We
simulate a realistic epidemic scenarios, based on an Ebola virus outbreak; we
find that gradually restricting the mobility of a subset of individuals reduces
the number of infected people after 30 days by 24%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01320</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01320</id><created>2015-04-06</created><authors><author><keyname>Ranjha</keyname><forenames>Bilal A.</forenames></author><author><keyname>Kashani</keyname><forenames>Mohammadreza A.</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author><author><keyname>Deng</keyname><forenames>Peng</forenames></author></authors><title>Robust Timing Synchronization for AC-OFDM Based Optical Wireless
  Communications</title><categories>cs.IT math.IT physics.optics</categories><comments>Accepted for publication in IEEE ICNS 2015, 10 Pages, 7 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communications (VLC) have recently attracted a growing interest
and can be a potential solution to realize indoor wireless communication with
high bandwidth capacity for RF-restricted environments such as airplanes and
hospitals. Optical based orthogonal frequency division multiplexing (OFDM)
systems have been proposed in the literature to combat multipath distortion and
intersymbol interference (ISI) caused by multipath signal propagation. In this
paper, we present a robust timing synchronization scheme suitable for
asymmetrically clipped (AC) OFDM based optical intensity modulated direct
detection (IM/DD) wireless systems. Our proposed method works perfectly for
ACO-OFDM, Pulse amplitude modulated discrete multitone (PAM-DMT) and discrete
Hartley transform (DHT) based optical OFDM systems. In contrast to existing
OFDM timing synchronization methods which are either not suitable for AC OFDM
techniques due to unipolar nature of output signal or perform poorly, our
proposed method is suitable for AC OFDM schemes and outperforms all other
available techniques. Both numerical and experimental results confirm the
accuracy of the proposed method. Our technique is also computationally
efficient as it requires very few computations as compared to conventional
methods in order to achieve good accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01329</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01329</id><created>2015-04-06</created><authors><author><keyname>Grout</keyname><forenames>R. W.</forenames></author><author><keyname>Kolla</keyname><forenames>H.</forenames></author><author><keyname>Minion</keyname><forenames>M. L.</forenames></author><author><keyname>Bell</keyname><forenames>J. B.</forenames></author></authors><title>Achieving algorithmic resilience for temporal integration through
  spectral deferred corrections</title><categories>cs.CE cs.MS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Spectral deferred corrections (SDC) is an iterative approach for constructing
higher- order accurate numerical approximations of ordinary differential
equations. SDC starts with an initial approximation of the solution defined at
a set of Gaussian or spectral collocation nodes over a time interval and uses
an iterative application of lower-order time discretizations applied to a
correction equation to improve the solution at these nodes. Each deferred
correction sweep increases the formal order of accuracy of the method up to the
limit inherent in the accuracy defined by the collocation points. In this
paper, we demonstrate that SDC is well suited to recovering from soft
(transient) hardware faults in the data. A strategy where extra correction
iterations are used to recover from soft errors and provide algorithmic
resilience is proposed. Specifically, in this approach the iteration is
continued until the residual (a measure of the error in the approximation) is
small relative to the residual on the first correction iteration and changes
slowly between successive iterations. We demonstrate the effectiveness of this
strategy for both canonical test problems and a comprehen- sive situation
involving a mature scientific application code that solves the reacting
Navier-Stokes equations for combustion research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01339</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01339</id><created>2015-04-06</created><authors><author><keyname>Kothari</keyname><forenames>Robin</forenames></author><author><keyname>Racicot-Desloges</keyname><forenames>David</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author></authors><title>Separating decision tree complexity from subcube partition complexity</title><categories>cs.CC</categories><comments>16 pages, 1 figure</comments><report-no>MIT-CTP #4663</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subcube partition model of computation is at least as powerful as
decision trees but no separation between these models was known. We show that
there exists a function whose deterministic subcube partition complexity is
asymptotically smaller than its randomized decision tree complexity, resolving
an open problem of Friedgut, Kahn, and Wigderson (2002). Our lower bound is
based on the information-theoretic techniques first introduced to lower bound
the randomized decision tree complexity of the recursive majority function.
  We also show that the public-coin partition bound, the best known lower bound
method for randomized decision tree complexity subsuming other general
techniques such as block sensitivity, approximate degree, randomized
certificate complexity, and the classical adversary bound, also lower bounds
randomized subcube partition complexity. This shows that all these lower bound
techniques cannot prove optimal lower bounds for randomized decision tree
complexity, which answers an open question of Jain and Klauck (2010) and Jain,
Lee, and Vishnoi (2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01344</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01344</id><created>2015-04-06</created><authors><author><keyname>Maclaurin</keyname><forenames>Dougal</forenames></author><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Early Stopping is Nonparametric Variational Inference</title><categories>stat.ML cs.LG</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that unconverged stochastic gradient descent can be interpreted as a
procedure that samples from a nonparametric variational approximate posterior
distribution. This distribution is implicitly defined as the transformation of
an initial distribution by a sequence of optimization updates. By tracking the
change in entropy over this sequence of transformations during optimization, we
form a scalable, unbiased estimate of the variational lower bound on the log
marginal likelihood. We can use this bound to optimize hyperparameters instead
of using cross-validation. This Bayesian interpretation of SGD suggests
improved, overfitting-resistant optimization procedures, and gives a
theoretical foundation for popular tricks such as early stopping and
ensembling. We investigate the properties of this marginal likelihood estimator
on neural network models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01352</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01352</id><created>2015-04-06</created><authors><author><keyname>Reddy</keyname><forenames>Sai Praneeth</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Vaya</keyname><forenames>Shailesh</forenames></author></authors><title>Multi-Broadcasting under the SINR Model</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the multi-broadcast problem in multi-hop wireless networks under the
SINR model deployed in the 2D Euclidean plane. In multi-broadcast, there are
$k$ initial rumours, potentially belonging to different nodes, that must be
forwarded to all $n$ nodes of the network. Furthermore, in each round a node
can only transmit a small message that could contain at most one initial rumor
and $O(\log n)$ control bits. In order to be successfully delivered to a node,
transmissions must satisfy the (Signal-to-Inference-and-Noise-Ratio) SINR
condition and have sufficiently strong signal at the receiver. We present
deterministic algorithms for multi-broadcast for different settings that
reflect the different types of knowledge about the topology of the network
available to the nodes: (i) the whole network topology (ii) their own
coordinates and coordinates of their neighbors (iii) only their own
coordinates, and (iv) only their own ids and the ids of their neighbors. For
the former two settings, we present solutions that are scalable with respect to
the diameter of the network and the polylogarithm of the network size, i.e.,
$\log^c n$ for some constant $c&gt; 0$, while the solutions for the latter two
have round complexity that is superlinear in the number of nodes. The last
result is of special significance, as it is the first result for the SINR model
that does not require nodes to know their coordinates in the plane (a very
specialized type of knowledge), but intricately exploits the understanding that
nodes are implanted in the 2D Euclidean plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01355</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01355</id><created>2015-04-01</created><authors><author><keyname>Dyshko</keyname><forenames>Serhii</forenames></author></authors><title>MacWilliams Extension Theorem for MDS additive codes</title><categories>cs.IT math.CO math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MacWilliams Extension Theorem states that each linear isometry of a
linear code extends to a monomial map. Unlike the linear codes, in general,
additive codes do not have the extension property. In this paper, an analogue
of the extension theorem for additive codes in the case of additive MDS codes
is proved. More precisely, it is shown that for almost all additive MDS codes
their additive isometries extend to isometries of the ambient space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01356</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01356</id><created>2015-04-06</created><updated>2015-06-24</updated><authors><author><keyname>D'Andreagiovanni</keyname><forenames>Fabio</forenames></author><author><keyname>Nardin</keyname><forenames>Antonella</forenames></author></authors><title>Towards the fast and robust optimal design of Wireless Body Area
  Networks</title><categories>math.OC cs.NI</categories><comments>Authors' manuscript version of the paper that was published in
  Applied Soft Computing</comments><doi>10.1016/j.asoc.2015.04.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless body area networks are wireless sensor networks whose adoption has
recently emerged and spread in important healthcare applications, such as the
remote monitoring of health conditions of patients. A major issue associated
with the deployment of such networks is represented by energy consumption: in
general, the batteries of the sensors cannot be easily replaced and recharged,
so containing the usage of energy by a rational design of the network and of
the routing is crucial. Another issue is represented by traffic uncertainty:
body sensors may produce data at a variable rate that is not exactly known in
advance, for example because the generation of data is event-driven. Neglecting
traffic uncertainty may lead to wrong design and routing decisions, which may
compromise the functionality of the network and have very bad effects on the
health of the patients. In order to address these issues, in this work we
propose the first robust optimization model for jointly optimizing the topology
and the routing in body area networks under traffic uncertainty. Since the
problem may result challenging even for a state-of-the-art optimization solver,
we propose an original optimization algorithm that exploits suitable linear
relaxations to guide a randomized fixing of the variables, supported by an
exact large variable neighborhood search. Experiments on realistic instances
indicate that our algorithm performs better than a state-of-the-art solver,
fast producing solutions associated with improved optimality gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01358</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01358</id><created>2015-04-06</created><authors><author><keyname>Washbourne</keyname><forenames>Logan</forenames></author></authors><title>A Survey of P2P Network Security</title><categories>cs.CR</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a review of peer-to-peer network security. Popular for
sharing of multimedia files, these networks carry risks and vulnerabilities
relating to data integrity, spyware, adware, and unwanted files. Further
attacks include those of forgery, pollution, repudiation, membership and
Eclipse attacks, neighbor selection attacks, Sybil, DoS, and omission attacks.
We review some protection mechanisms that have been devised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01359</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01359</id><created>2015-03-18</created><authors><author><keyname>Markin</keyname><forenames>Nadya</forenames></author><author><keyname>Thomas</keyname><forenames>Eldho K.</forenames></author><author><keyname>Oggier</keyname><forenames>Frederique</forenames></author></authors><title>On Group Violations of Inequalities in five Subgroups</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider ten linear rank inequalities, which always hold for ranks of
vector subspaces, and look at them as group inequalities. We prove that groups
of order pq, for p,q two distinct primes, always satisfy these ten group
inequalities. We give partial results for groups of order $p^2q$, and find that
the symmetric group $S_4$ is the smallest group that yield violations, for two
among the ten group inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01365</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01365</id><created>2015-04-06</created><authors><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Yu</keyname><forenames>Hsiang-Fu</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Dual Coordinate Descent (SDCD) has become one of the most
efficient ways to solve the family of $\ell_2$-regularized empirical risk
minimization problems, including linear SVM, logistic regression, and many
others. The vanilla implementation of DCD is quite slow; however, by
maintaining primal variables while updating dual variables, the time complexity
of SDCD can be significantly reduced. Such a strategy forms the core algorithm
in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD
algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD
algorithms have been proposed, however, they fail to achieve good speedup in
the shared memory multi-core setting. In this paper, we propose a family of
asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread
repeatedly selects a random dual variable and conducts coordinate updates using
the primal variables that are stored in the shared memory. We analyze the
convergence properties when different locking/atomic mechanisms are applied.
For implementation with atomic operations, we show linear convergence under
mild conditions. For implementation without any atomic operations or locking,
we present the first {\it backward error analysis} for ASDCD under the
multi-core environment, showing that the converged solution is the exact
solution for a primal problem with perturbed regularizer. Experimental results
show that our methods are much faster than previous parallel coordinate descent
solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01368</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01368</id><created>2015-04-06</created><updated>2015-10-09</updated><authors><author><keyname>Kleineberg</keyname><forenames>Kaj-Kolja</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author></authors><title>Is bigger always better? How local online social networks can outperform
  global ones</title><categories>physics.soc-ph cs.SI</categories><comments>New version with additional material, mainly comparison with
  empirical data and estimation of most probable parameters. See videos at
  https://www.youtube.com/watch?v=z3dP3PD7ueA and
  https://www.youtube.com/watch?v=XkZTxnJd-eI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overwhelming success of online social networks, the key actors in the
cosmos of the Web 2.0, has reshaped human interactions on a worldwide scale. To
help understand the fundamental mechanisms which determine the fate of online
social networks at the system level, we describe the digital world as a complex
ecosystem of interacting networks. In this paper, we discuss the impact of
heterogeneity in network fitnesses induced by competition between an
international network, such as Facebook, and local services.To this end, we
construct a 1:1000 scale model of the digital world, consisting of the 80
countries with the most Internet users. We show how inter-country social ties
induce increased fitness of the international network. Under certain
conditions, this leads to the extinction of local networks; whereas under
different conditions, local networks can persist and even dominate the
international network completely. These findings provide new insights into the
possibilities for preserving digital diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01369</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01369</id><created>2015-04-06</created><updated>2015-07-15</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Information Recovery from Pairwise Measurements: A Shannon-Theoretic
  Approach</title><categories>cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH</categories><comments>This work has been presented in part in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with jointly recovering $n$ node-variables $\{ x_i \}
_{1\leq i\leq n}$ from a collection of pairwise difference measurements.
Imagine we acquire a few observations taking the form of $x_i-x_j$; the
observation pattern is represented by a measurement graph $\mathcal{G}$ with an
edge set $\mathcal{E}$ such that $x_i-x_j$ is observed if and only if
$(i,j)\in\mathcal{E}$. To account for noisy measurements in a general manner,
we model the data acquisition process by a set of channels with given
input/output transition measures. Employing information-theoretic tools applied
to channel decoding problems, we develop a unified framework to characterize
the fundamental recovery criterion, which accommodates general graph
structures, alphabet sizes, and channel transition measures. In particular, our
results isolate a family of minimum channel divergence measures to characterize
the degree of measurement corruption, which together with the minimum cut size
of $\mathcal{G}$ dictates the feasibility of exact information recovery. For
various homogeneous graphs, the recovery condition depends almost only on the
edge sparsity irrespective of other graphical metrics. We apply our general
theory to three concrete applications, including the stochastic block model,
the outlier model, and the haplotype assembly problem. Our theory leads to
order-wise tight recovery conditions for all these scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01375</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01375</id><created>2015-04-04</created><updated>2015-07-29</updated><authors><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Hu</keyname><forenames>Jinxing</forenames></author><author><keyname>Zhang</keyname><forenames>Baoyun</forenames></author><author><keyname>Yin</keyname><forenames>Ling</forenames></author><author><keyname>Zhong</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Weixi</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author></authors><title>Preprint Traffic Management and Forecasting System Based on 3D GIS</title><categories>cs.OH</categories><comments>This is the preprint version of our paper on 2015 15th IEEE/ACM
  International Symposium on Cluster, Cloud and Grid Computing (CCGrid). arXiv
  admin note: text overlap with arXiv:1504.01057. arXiv admin note: substantial
  text overlap with arXiv:1504.01057</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 2015 15th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing (CCGrid). This paper takes
Shenzhen Futian comprehensive transportation junction as the case, and makes
use of continuous multiple real-time dynamic traffic information to carry out
monitoring and analysis on spatial and temporal distribution of passenger flow
under different means of transportation and service capacity of junction from
multi-dimensional space-time perspectives such as different period and special
period. Virtual reality geographic information system is employed to present
the forecasting result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01379</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01379</id><created>2015-04-06</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Baoyun</forenames></author><author><keyname>Wang</keyname><forenames>Weixi</forenames></author><author><keyname>Feng</keyname><forenames>Shengzhong</forenames></author><author><keyname>Hu</keyname><forenames>Jinxing</forenames></author></authors><title>Preprint Big City 3D Visual Analysis</title><categories>cs.GR cs.HC</categories><comments>This is the preprint version of our paper on EUROGRAPHICS 2015</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on EUROGRAPHICS 2015. A big city
visual analysis platform based on Web Virtual Reality Geographical Information
System (WEBVRGIS) is presented. Extensive model editing functions and spatial
analysis functions are available, including terrain analysis, spatial analysis,
sunlight analysis, traffic analysis, population analysis and community
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01380</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01380</id><created>2015-04-06</created><updated>2015-11-14</updated><authors><author><keyname>Alhubail</keyname><forenames>Maitham Makki</forenames></author><author><keyname>Wang</keyname><forenames>Qiqi</forenames></author></authors><title>The swept rule for breaking the latency barrier in time advancing PDEs</title><categories>cs.CE cs.MS</categories><comments>30 pages</comments><journal-ref>Journal of Computational Physics (2016), pp. 110-121</journal-ref><doi>10.1016/j.jcp.2015.11.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates the swept rule of space-time domain decomposition,
an idea to break the latency barrier via communicating less often when
explicitly solving time-dependent PDEs. The swept rule decomposes space and
time among computing nodes in ways that exploit the domains of influence and
the domain of dependency, making it possible to communicate once per many
timesteps without redundant computation. The article presents simple
theoretical analysis to the performance of the swept rule which then was shown
to be accurate by conducting numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01381</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01381</id><created>2015-04-06</created><updated>2015-05-08</updated><authors><author><keyname>Cho</keyname><forenames>Hyungmin</forenames></author><author><keyname>Cher</keyname><forenames>Chen-Yong</forenames></author><author><keyname>Shepherd</keyname><forenames>Thomas</forenames></author><author><keyname>Mitra</keyname><forenames>Subhasish</forenames></author></authors><title>Understanding Soft Errors in Uncore Components</title><categories>cs.OH cs.DC</categories><comments>to be published in Proceedings of the 52nd Annual Design Automation
  Conference</comments><acm-class>B.8.1</acm-class><doi>10.1145/2744769.2744923</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effects of soft errors in processor cores have been widely studied.
However, little has been published about soft errors in uncore components, such
as memory subsystem and I/O controllers, of a System-on-a-Chip (SoC). In this
work, we study how soft errors in uncore components affect system-level
behaviors. We have created a new mixed-mode simulation platform that combines
simulators at two different levels of abstraction, and achieves 20,000x speedup
over RTL-only simulation. Using this platform, we present the first study of
the system-level impact of soft errors inside various uncore components of a
large-scale, multi-core SoC using the industrial-grade, open-source OpenSPARC
T2 SoC design. Our results show that soft errors in uncore components can
significantly impact system-level reliability. We also demonstrate that uncore
soft errors can create major challenges for traditional system-level checkpoint
recovery techniques. To overcome such recovery challenges, we present a new
replay recovery technique for uncore components belonging to the memory
subsystem. For the L2 cache controller and the DRAM controller components of
OpenSPARC T2, our new technique reduces the probability that an application run
fails to produce correct results due to soft errors by more than 100x with
3.32% and 6.09% chip-level area and power impact, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01383</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01383</id><created>2015-04-06</created><authors><author><keyname>Niculae</keyname><forenames>Vlad</forenames></author><author><keyname>Suen</keyname><forenames>Caroline</forenames></author><author><keyname>Zhang</keyname><forenames>Justine</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting
  Patterns</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>To appear in the Proceedings of WWW 2015. 11pp, 10 fig. Interactive
  visualization, data, and other info available at
  http://snap.stanford.edu/quotus/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the extremely large pool of events and stories available, media outlets
need to focus on a subset of issues and aspects to convey to their audience.
Outlets are often accused of exhibiting a systematic bias in this selection
process, with different outlets portraying different versions of reality.
However, in the absence of objective measures and empirical evidence, the
direction and extent of systematicity remains widely disputed.
  In this paper we propose a framework based on quoting patterns for
quantifying and characterizing the degree to which media outlets exhibit
systematic bias. We apply this framework to a massive dataset of news articles
spanning the six years of Obama's presidency and all of his speeches, and
reveal that a systematic pattern does indeed emerge from the outlet's quoting
behavior. Moreover, we show that this pattern can be successfully exploited in
an unsupervised prediction setting, to determine which new quotes an outlet
will select to broadcast. By encoding bias patterns in a low-rank space we
provide an analysis of the structure of political media coverage. This reveals
a latent media bias space that aligns surprisingly well with political ideology
and outlet type. A linguistic analysis exposes striking differences across
these latent dimensions, showing how the different types of media outlets
portray different realities even when reporting on the same events. For
example, outlets mapped to the mainstream conservative side of the latent space
focus on quotes that portray a presidential persona disproportionately
characterized by negativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01407</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01407</id><created>2015-03-18</created><authors><author><keyname>Viznyuk</keyname><forenames>Sergei</forenames></author></authors><title>Shannon's entropy revisited</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I consider the effect of a finite sample size on the entropy of a sample of
independent events. I propose formula for entropy which satisfies Shannon's
axioms, and which reduces to Shannon's entropy when sample size is infinite. I
discuss the physical meaning of the difference between two formulas, including
some practical implications, such as maximum achievable channel utilization,
and minimum achievable communication protocol overhead, for a given message
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01420</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01420</id><created>2015-04-06</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil</forenames></author><author><keyname>Devanuj</keyname></author><author><keyname>Srivastava</keyname><forenames>Akhilesh</forenames></author><author><keyname>Rao</keyname><forenames>P. V. S.</forenames></author></authors><title>Knowledge driven Offline to Online Script Conversion</title><categories>cs.CV</categories><comments>4 pages, 5 figures, KBCS 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of offline to online script conversion is a challenging and an
ill-posed problem. The interest in offline to online conversion exists because
there are a plethora of robust algorithms in online script literature which can
not be used on offline scripts. In this paper, we propose a method, based on
heuristics, to extract online script information from offline bitmap image. We
show the performance of the proposed method on a real sample signature offline
image, whose online information is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01424</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01424</id><created>2015-04-06</created><authors><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Endo</keyname><forenames>Yutaka</forenames></author><author><keyname>Hirayama</keyname><forenames>Ryuji</forenames></author><author><keyname>Hiyama</keyname><forenames>Daisuke</forenames></author><author><keyname>Hasegawa</keyname><forenames>Satoki</forenames></author><author><keyname>Nagahama</keyname><forenames>Yuki</forenames></author><author><keyname>Sano</keyname><forenames>Marie</forenames></author><author><keyname>Oikawa</keyname><forenames>Minoru</forenames></author><author><keyname>Sugie</keyname><forenames>Takashige</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyoshi</forenames></author></authors><title>Improvement of the image quality of random phase--free holography using
  an iterative method</title><categories>physics.optics cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our proposed method of random phase-free holography using virtual convergence
light can obtain large reconstructed images exceeding the size of the hologram,
without the assistance of random phase. The reconstructed images have
low-speckle noise in the amplitude and phase-only holograms (kinoforms);
however, in low-resolution holograms, we obtain a degraded image quality
compared to the original image. We propose an iterative random phase-free
method with virtual convergence light to address this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01427</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01427</id><created>2015-04-06</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Saurabh</forenames></author><author><keyname>Sahana</keyname><forenames>K.</forenames></author><author><keyname>Sathyanarayana</keyname></author><author><keyname>Srivastava</keyname><forenames>Akhilesh</forenames></author><author><keyname>Rao</keyname><forenames>P. V. S.</forenames></author></authors><title>A Metric to Classify Style of Spoken Speech</title><categories>cs.CL</categories><comments>5 pages; OCOCOSDA 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to classify spoken speech based on the style of speaking is an
important problem. With the advent of BPO's in recent times, specifically those
that cater to a population other than the local population, it has become
necessary for BPO's to identify people with certain style of speaking
(American, British etc). Today BPO's employ accent analysts to identify people
having the required style of speaking. This process while involving human bias,
it is becoming increasingly infeasible because of the high attrition rate in
the BPO industry. In this paper, we propose a new metric, which robustly and
accurately helps classify spoken speech based on the style of speaking. The
role of the proposed metric is substantiated by using it to classify real
speech data collected from over seventy different people working in a BPO. We
compare the performance of the metric against human experts who independently
carried out the classification process. Experimental results show that the
performance of the system using the novel metric performs better than two
different human expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01431</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01431</id><created>2015-04-06</created><updated>2015-11-05</updated><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>If the Current Clique Algorithms are Optimal, so is Valiant's Parser</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CFG recognition problem is: given a context-free grammar $\mathcal{G}$
and a string $w$ of length $n$, decide if $w$ can be obtained from
$\mathcal{G}$. This is the most basic parsing question and is a core computer
science problem. Valiant's parser from 1975 solves the problem in
$O(n^{\omega})$ time, where $\omega&lt;2.373$ is the matrix multiplication
exponent. Dozens of parsing algorithms have been proposed over the years, yet
Valiant's upper bound remains unbeaten. The best combinatorial algorithms have
mildly subcubic $O(n^3/\log^3{n})$ complexity.
  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for
CFG parsing, and that very efficient and practical algorithms might be hard or
even impossible to obtain. Lee showed that any algorithm for a more general
parsing problem with running time $O(|\mathcal{G}|\cdot n^{3-\varepsilon})$ can
be converted into a surprising subcubic algorithm for Boolean Matrix
Multiplication. Unfortunately, Lee's hardness result required that the grammar
size be $|\mathcal{G}|=\Omega(n^6)$. Nothing was known for the more relevant
case of constant size grammars.
  In this work, we prove that any improvement on Valiant's algorithm, even for
constant size grammars, either in terms of runtime or by avoiding the
inefficiencies of fast matrix multiplication, would imply a breakthrough
algorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if
there are $k$ that form a clique.
  Besides classifying the complexity of a fundamental problem, our reduction
has led us to similar lower bounds for more modern and well-studied cubic time
problems for which faster algorithms are highly desirable in practice: RNA
Folding, a central problem in computational biology, and Dyck Language Edit
Distance, answering an open question of Saha (FOCS'14).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01433</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01433</id><created>2015-04-06</created><authors><author><keyname>Hurtado</keyname><forenames>Joan</forenames></author></authors><title>Automated System for Improving RSS Feeds Data Quality</title><categories>cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the majority of RSS feeds provide incomplete information about
their news items. The lack of information leads to engagement loss in users. We
present a new automated system for improving the RSS feeds' data quality. RSS
feeds provide a list of the latest news items ordered by date. Therefore, it
makes it easy for a web crawler to precisely locate the item and extract its
raw content. Then it identifies where the main content is located and extracts:
main text corpus, relevant keywords, bigrams, best image and predicts the
category of the item. The output of the system is an enhanced RSS feed. The
proposed system showed an average item data quality improvement from 39.98% to
95.62%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01438</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01438</id><created>2015-04-06</created><updated>2016-02-02</updated><authors><author><keyname>Basar</keyname><forenames>Tamer</forenames></author><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Convergence Time of Quantized Metropolis Consensus Over Time-Varying
  Networks</title><categories>cs.SY cs.DC cs.MA math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantized consensus problem on undirected time-varying
connected graphs with n nodes, and devise a protocol with fast convergence time
to the set of consensus points. Specifically, we show that when the edges of
each network in a sequence of connected time-varying networks are activated
based on Poisson processes with Metropolis rates, the expected convergence time
to the set of consensus points is at most O(n^2 log^2 n), where each node
performs a constant number of updates per unit time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01441</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01441</id><created>2015-04-06</created><updated>2015-05-04</updated><authors><author><keyname>Gallo</keyname><forenames>Orazio</forenames><affiliation>NVIDIA</affiliation></author><author><keyname>Troccoli</keyname><forenames>Alejandro</forenames><affiliation>NVIDIA</affiliation></author><author><keyname>Hu</keyname><forenames>Jun</forenames><affiliation>NVIDIA</affiliation><affiliation>Duke University</affiliation></author><author><keyname>Pulli</keyname><forenames>Kari</forenames><affiliation>NVIDIA</affiliation><affiliation>Light</affiliation></author><author><keyname>Kautz</keyname><forenames>Jan</forenames><affiliation>NVIDIA</affiliation></author></authors><title>Locally Non-rigid Registration for Mobile HDR Photography</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration for stack-based HDR photography is challenging. If not
properly accounted for, camera motion and scene changes result in artifacts in
the composite image. Unfortunately, existing methods to address this problem
are either accurate, but too slow for mobile devices, or fast, but prone to
failing. We propose a method that fills this void: our approach is extremely
fast---under 700ms on a commercial tablet for a pair of 5MP images---and
prevents the artifacts that arise from insufficient registration quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01442</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01442</id><created>2015-04-06</created><updated>2015-06-25</updated><authors><author><keyname>Barbosa</keyname><forenames>Hugo</forenames></author><author><keyname>Neto</keyname><forenames>Fernando Buarque de Lima</forenames></author><author><keyname>Evsukoff</keyname><forenames>Alexandre</forenames></author><author><keyname>Menezes</keyname><forenames>Ronaldo</forenames></author></authors><title>The Effect of Recency to Human Mobility</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, we have seen scientists attempt to model and explain human
dynamics and, in particular, human movement. Many aspects of our complex life
are affected by human movements such as disease spread and epidemics modeling,
city planning, wireless network development, and disaster relief, to name a
few. Given the myriad of applications it is clear that a complete understanding
of how people move in space can lead to huge benefits to our society. In most
of the recent works, scientists have focused on the idea that people movements
are biased towards frequently-visited locations. According to them, human
movement is based on an exploration/exploitation dichotomy in which individuals
choose new locations (exploration) or return to frequently-visited locations
(exploitation). In this work, we focus on the concept of recency. We propose a
model in which exploitation in human movement also considers recently-visited
locations and not solely frequently-visited locations. We test our hypothesis
against different empirical data of human mobility and show that our proposed
model is able to better explain the human trajectories in these datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01446</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01446</id><created>2015-04-06</created><authors><author><keyname>Denchev</keyname><forenames>Vasil S.</forenames></author><author><keyname>Ding</keyname><forenames>Nan</forenames></author><author><keyname>Matsushima</keyname><forenames>Shin</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author><author><keyname>Neven</keyname><forenames>Hartmut</forenames></author></authors><title>Totally Corrective Boosting with Cardinality Penalization</title><categories>cs.LG quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a totally corrective boosting algorithm with explicit cardinality
regularization. The resulting combinatorial optimization problems are not known
to be efficiently solvable with existing classical methods, but emerging
quantum optimization technology gives hope for achieving sparser models in
practice. In order to demonstrate the utility of our algorithm, we use a
distributed classical heuristic optimizer as a stand-in for quantum hardware.
Even though this evaluation methodology incurs large time and resource costs on
classical computing machinery, it allows us to gauge the potential gains in
generalization performance and sparsity of the resulting boosted ensembles. Our
experimental results on public data sets commonly used for benchmarking of
boosting algorithms decidedly demonstrate the existence of such advantages. If
actual quantum optimization were to be used with this algorithm in the future,
we would expect equivalent or superior results at much smaller time and energy
costs during training. Moreover, studying cardinality-penalized boosting also
sheds light on why unregularized boosting algorithms with early stopping often
yield better results than their counterparts with explicit convex
regularization: Early stopping performs suboptimal cardinality regularization.
The results that we present here indicate it is beneficial to explicitly solve
the combinatorial problem still left open at early termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01452</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01452</id><created>2015-04-06</created><authors><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Sinong</forenames></author><author><keyname>Ding</keyname><forenames>Lianghui</forenames></author><author><keyname>Yang</keyname><forenames>Feng</forenames></author><author><keyname>Zhang</keyname><forenames>Wenjun</forenames></author></authors><title>The Performance Analysis of Coded Cache in Wireless Fading Channel</title><categories>cs.NI cs.IT math.IT</categories><comments>submitted GC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of data volume and the accompanying congestion problems over
the wireless networks have been critical issues to content providers. A novel
technique, termed as coded cache, is proposed to relieve the burden. Through
creating coded-multicasting opportunities, the coded-cache scheme can provide
extra performance gain over the conventional push technique that simply
pre-stores contents at local caches during the network idle period. But
existing works on the coded caching scheme assumed the availability of an
error-free shared channel accessible by each user. This paper considers the
more realistic scenario where each user may experience different link quality.
In this case, the system performance would be restricted by the user with the
worst channel condition. And the corresponding resource allocation schemes
aimed at breaking this obstacles are developed. Specifically, we employ the
coded caching scheme in time division and frequency division transmission mode
and formulate the sub-optimal problems. Power and bandwidth are allocated
respectively to maximum the system throughput. The simulation results show that
the throughput of the technique in wireless scenario will be limited and would
decrease as the number of users becomes sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01456</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01456</id><created>2015-04-06</created><authors><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Chen</keyname><forenames>Jiaxuan</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Local Measurement and Reconstruction for Noisy Graph Signals</title><categories>cs.IT math.IT</categories><comments>24 pages, 6 figures, 2 tables, journal manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging field of signal processing on graph plays a more and more
important role in processing signals and information related to networks.
Existing works have shown that under certain conditions a smooth graph signal
can be uniquely reconstructed from its decimation, i.e., data associated with a
subset of vertices. However, in some potential applications (e.g., sensor
networks with clustering structure), the obtained data may be a combination of
signals associated with several vertices, rather than the decimation. In this
paper, we propose a new concept of local measurement, which is a generalization
of decimation. Using the local measurements, a local-set-based method named
iterative local measurement reconstruction (ILMR) is proposed to reconstruct
bandlimited graph signals. It is proved that ILMR can reconstruct the original
signal perfectly under certain conditions. The performance of ILMR against
noise is theoretically analyzed. The optimal choice of local weights and a
greedy algorithm of local set partition are given in the sense of minimizing
the expected reconstruction error. Compared with decimation, the proposed local
measurement sampling and reconstruction scheme is more robust in noise existing
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01459</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01459</id><created>2015-04-06</created><authors><author><keyname>Suchenek</keyname><forenames>Marek A.</forenames></author></authors><title>A Complete Worst-Case Analysis of Heapsort with Experimental
  Verification of Its Results, A manuscript (MS)</title><categories>cs.DS</categories><comments>115 pages 41 figures</comments><msc-class>68W40, 11A63</msc-class><acm-class>E.1; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rigorous proof is presented that the number of comparisons of keys
performed in the worst case by ${\tt Heapsort}$ on any array of size $N \geq 2$
is equal to: $ 2 (N-1)\, ( \, \lg \frac{N-1}{2} +\varepsilon \, ) - 2s_2(N) -
e_2(N) + \min (\lfloor \lg (N-1) \rfloor, 2) + 6 + c, $ where $ \varepsilon $,
given by: $\varepsilon = 1 + \lceil \lg \, (N-1) \rceil - \lg \, (N-1) -
2^{\lceil \lg \, (N-1) \rceil - \lg \, (N-1)} ,$ is a function of $ N $ with
the minimum value 0 and and the supremum value $\delta = 1 - \lg e + \lg \lg e
\approx 0.0860713320559342$, $s_2(N)$ is the sum of all digits of the binary
representation of $N$, $e_2(N)$ is the exponent of $2$ in the prime
factorization of $N$, and $ c $ is a binary function on the set of integers
defined by: $c = 1$, if $N \leq 2 ^{\lceil \lg N \rceil} - 4$, and $c = 0$,
otherwise. An algorithm that generates worst-case input arrays of any size $ N
\geq 2 $ for ${\tt Heapsort}$ is offered. The algorithm has been implemented in
Java, runs in $O( N \log N )$ time, and allows for precise experimental
verification of the above formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01467</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01467</id><created>2015-04-06</created><authors><author><keyname>Fujikawa</keyname><forenames>Kazuo</forenames></author><author><keyname>Ge</keyname><forenames>Mo-Lin</forenames></author><author><keyname>Liu</keyname><forenames>Yu-Long</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Uncertainty principle, Shannon-Nyquist sampling and beyond</title><categories>quant-ph cs.IT math.IT</categories><comments>27 pages. Journal of Physical Society of Japan (in press)</comments><doi>10.7566/JPSJ.84.064801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Donoho and Stark have shown that a precise deterministic recovery of missing
information contained in a time interval shorter than the time-frequency
uncertainty limit is possible. We analyze this signal recovery mechanism from a
physics point of view and show that the well-known Shannon-Nyquist sampling
theorem, which is fundamental in signal processing, also uses essentially the
same mechanism. The uncertainty relation in the context of information theory,
which is based on Fourier analysis, provides a criterion to distinguish
Shannon-Nyquist sampling from compressed sensing. A new signal recovery
formula, which is analogous to Donoho-Stark formula, is given using the idea of
Shannon-Nyquist sampling; in this formulation, the smearing of information
below the uncertainty limit as well as the recovery of information with
specified bandwidth take place. We also discuss the recovery of states from the
domain below the uncertainty limit of coordinate and momentum in quantum
mechanics and show that in principle the state recovery works by assuming ideal
measurement procedures. The recovery of the lost information in the
sub-uncertainty domain means that the loss of information in such a small
domain is not fatal, which is in accord with our common understanding of the
uncertainty principle, although its precise recovery is something we are not
used to in quantum mechanics. The uncertainty principle provides a universal
sampling criterion covering both the classical Shannon-Nyquist sampling theorem
and the quantum mechanical measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01472</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01472</id><created>2015-04-07</created><authors><author><keyname>Mathew</keyname><forenames>K. Ashik</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author></authors><title>New Lower Bounds for the Shannon Capacity of Odd Cycles</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shannon capacity of a graph $G$ is defined as $c(G)=\sup_{d\geq
1}(\alpha(G^d))^{\frac{1}{d}},$ where $\alpha(G)$ is the independence number of
$G$. The Shannon capacity of the cycle $C_5$ on $5$ vertices was determined by
Lov\'{a}sz in 1979, but the Shannon capacity of a cycle $C_p$ for general odd
$p$ remains one of the most notorious open problems in information theory. By
prescribing stabilizers for the independent sets in $C_p^d$ and using
stochastic search methods, we show that $\alpha(C_7^5)\geq 350$,
$\alpha(C_{11}^4)\geq 748$, $\alpha(C_{13}^4)\geq 1534$ and
$\alpha(C_{15}^3)\geq 381$. This leads to improved lower bounds on the Shannon
capacity of $C_7$ and $C_{15}$: $c(C_7)\geq 350^{\frac{1}{5}}&gt; 3.2271$ and
$c(C_{15})\geq 381^{\frac{1}{3}}&gt; 7.2495$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01476</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01476</id><created>2015-04-07</created><authors><author><keyname>L.</keyname><forenames>Lajish V.</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Mobile Phone Based Vehicle License Plate Recognition for Road Policing</title><categories>cs.CV</categories><comments>7 pages; PReMI Experiential Workshop, Delhi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identity of a vehicle is done through the vehicle license plate by traffic
police in general. Au- tomatic vehicle license plate recognition has several
applications in intelligent traffic management systems. The security situation
across the globe and particularly in India demands a need to equip the traffic
police with a system that enables them to get instant details of a vehicle. The
system should be easy to use, should be mobile, and work 24 x 7. In this paper,
we describe a mobile phone based, client-server architected, license plate
recognition system. While we use the state of the art image processing and
pattern recognition algorithms tuned for Indian conditions to automatically
recognize non-uniform license plates, the main contribution is in creating an
end to end usable solution. The client application runs on a mobile device and
a server application, with access to vehicle information database, is hosted
centrally. The solution enables capture of license plate image captured by the
phone camera and passes to the server; on the server the license plate number
is recognized; the data associated with the number plate is then sent back to
the mobile device, instantaneously. We describe the end to end system
architecture in detail. A working prototype of the proposed system has been
implemented in the lab environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01479</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01479</id><created>2015-04-07</created><authors><author><keyname>Saramaki</keyname><forenames>Jari</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author></authors><title>From seconds to months: multi-scale dynamics of mobile telephone calls</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Eur. Phys. J. B (2015) 88: 164</journal-ref><doi>10.1140/epjb/e2015-60106-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data on electronic records of social interactions allow approaching human
behaviour and sociality from a quantitative point of view with unforeseen
statistical power. Mobile telephone Call Detail Records (CDRs), automatically
collected by telecom operators for billing purposes, have proven especially
fruitful for understanding one-to-one communication patterns as well as the
dynamics of social networks that are reflected in such patterns. We present an
overview of empirical results on the multi-scale dynamics of social dynamics
and networks inferred from mobile telephone calls. We begin with the shortest
timescales and fastest dynamics, such as burstiness of call sequences between
individuals, and &quot;zoom out&quot; towards longer temporal and larger structural
scales, from temporal motifs formed by correlated calls between multiple
individuals to long-term dynamics of social groups. We conclude this overview
with a future outlook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01482</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01482</id><created>2015-04-07</created><authors><author><keyname>Chan</keyname><forenames>William</forenames></author><author><keyname>Lane</keyname><forenames>Ian</forenames></author></authors><title>Deep Recurrent Neural Networks for Acoustic Modelling</title><categories>cs.LG cs.CL cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel deep Recurrent Neural Network (RNN) model for acoustic
modelling in Automatic Speech Recognition (ASR). We term our contribution as a
TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with
Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory
(BLSTM), and a final DNN. The first DNN acts as a feature processor to our
model, the BLSTM then generates a context from the sequence acoustic signal,
and the final DNN takes the context and models the posterior probabilities of
the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)
eval92 task or more than 8% relative improvement over the baseline DNN models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01483</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01483</id><created>2015-04-07</created><authors><author><keyname>Chan</keyname><forenames>William</forenames></author><author><keyname>Ke</keyname><forenames>Nan Rosemary</forenames></author><author><keyname>Lane</keyname><forenames>Ian</forenames></author></authors><title>Transferring Knowledge from a RNN to a DNN</title><categories>cs.LG cs.CL cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art
results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent
Neural Network (RNN) models have been shown to outperform DNNs counterparts.
However, state-of-the-art DNN and RNN models tend to be impractical to deploy
on embedded systems with limited computational capacity. Traditionally, the
approach for embedded platforms is to either train a small DNN directly, or to
train a small DNN that learns the output distribution of a large DNN. In this
paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We
use the RNN model to generate soft alignments and minimize the Kullback-Leibler
divergence against the small DNN. The small DNN trained on the soft RNN
alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task
compared to a baseline 4.54 WER or more than 13% relative improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01488</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01488</id><created>2015-04-07</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>VL</keyname><forenames>Lajish</forenames></author></authors><title>On-line Handwritten Devanagari Character Recognition using Fuzzy
  Directional Features</title><categories>cs.CV</categories><comments>6 pages; 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new feature set for use in the recognition of on-line
handwritten Devanagari script based on Fuzzy Directional Features. Experiments
are conducted for the automatic recognition of isolated handwritten character
primitives (sub-character units). Initially we describe the proposed feature
set, called the Fuzzy Directional Features (FDF) and then show how these
features can be effectively utilized for writer independent character
recognition. Experimental results show that FDF set perform well for writer
independent data set at stroke level recognition. The main contribution of this
paper is the introduction of a novel feature set and establish experimentally
its ability in recognition of handwritten Devanagari script.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01492</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01492</id><created>2015-04-07</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Efficient SDP Inference for Fully-connected CRFs Based on Low-rank
  Decomposition</title><categories>cs.CV cs.LG stat.ML</categories><comments>15 pages. A conference version of this work appears in Proc. IEEE
  Conference on Computer Vision and Pattern Recognition, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Random Fields (CRF) have been widely used in a variety of
computer vision tasks. Conventional CRFs typically define edges on neighboring
image pixels, resulting in a sparse graph such that efficient inference can be
performed. However, these CRFs fail to model long-range contextual
relationships. Fully-connected CRFs have thus been proposed. While there are
efficient approximate inference methods for such CRFs, usually they are
sensitive to initialization and make strong assumptions. In this work, we
develop an efficient, yet general algorithm for inference on fully-connected
CRFs. The algorithm is based on a scalable SDP algorithm and the low- rank
approximation of the similarity/kernel matrix. The core of the proposed
algorithm is a tailored quasi-Newton method that takes advantage of the
low-rank matrix approximation when solving the specialized SDP dual problem.
Experiments demonstrate that our method can be applied on fully-connected CRFs
that cannot be solved previously, such as pixel-level image co-segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01496</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01496</id><created>2015-04-07</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Voice based self help System: User Experience Vs Accuracy</title><categories>cs.CL</categories><comments>5 pages; 1 figure</comments><doi>10.1007/978-90-481-3658-2_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, self help systems are being increasingly deployed by service
based industries because they are capable of delivering better customer service
and increasingly the switch is to voice based self help systems because they
provide a natural interface for a human to interact with a machine. A speech
based self help system ideally needs a speech recognition engine to convert
spoken speech to text and in addition a language processing engine to take care
of any misrecognitions by the speech recognition engine. Any off-the-shelf
speech recognition engine is generally a combination of acoustic processing and
speech grammar. While this is the norm, we believe that ideally a speech
recognition application should have in addition to a speech recognition engine
a separate language processing engine to give the system better performance. In
this paper, we discuss ways in which the speech recognition engine and the
language processing engine can be combined to give a better user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01497</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01497</id><created>2015-04-07</created><updated>2015-07-10</updated><authors><author><keyname>Efentakis</keyname><forenames>Alexandros</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author></authors><title>ReHub. Extending Hub Labels for Reverse k-Nearest Neighbor Queries on
  Large-Scale networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quite recently, the algorithmic community has focused on solving multiple
shortest-path query problems beyond simple vertex-to-vertex queries, especially
in the context of road networks. Unfortunately, this research cannot be
generalized for large-scale graphs, e.g., social or collaboration networks, or
to efficiently answer Reverse k-Nearest Neighbor (RkNN) queries, which are of
practical relevance to a wide range of applications. To remedy this, we propose
ReHub, a novel main-memory algorithm that extends the Hub Labeling technique to
efficiently answer RkNN queries on large-scale networks. Our experimentation
will show that ReHub is the best overall solution for this type of queries,
requiring only minimal preprocessing and providing very fast query times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01502</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01502</id><created>2015-04-07</created><authors><author><keyname>Lindeberg</keyname><forenames>Tony</forenames></author></authors><title>Separable time-causal and time-recursive spatio-temporal receptive
  fields</title><categories>cs.CV q-bio.NC</categories><comments>12 pages, 2 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1404.2037</comments><journal-ref>Proc SSVM 2015: Scale-Space and Variational Methods for Computer
  Vision, Springer LNCS vol 9087, pages 90-102, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improved model and theory for time-causal and time-recursive
spatio-temporal receptive fields, obtained by a combination of Gaussian
receptive fields over the spatial domain and first-order integrators or
equivalently truncated exponential filters coupled in cascade over the temporal
domain. Compared to previous spatio-temporal scale-space formulations in terms
of non-enhancement of local extrema or scale invariance, these receptive fields
are based on different scale-space axiomatics over time by ensuring
non-creation of new local extrema or zero-crossings with increasing temporal
scale. Specifically, extensions are presented about parameterizing the
intermediate temporal scale levels, analysing the resulting temporal dynamics
and transferring the theory to a discrete implementation in terms of recursive
filters over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01504</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01504</id><created>2015-04-07</created><authors><author><keyname>Chang</keyname><forenames>Chii</forenames></author><author><keyname>Srirama</keyname><forenames>Satish</forenames></author><author><keyname>Ling</keyname><forenames>Sea</forenames></author></authors><title>Service Discovery and Trust in Mobile Social Network in Proximity</title><categories>cs.SI cs.NI</categories><comments>40 pages, 14 figures, 3 tables</comments><acm-class>H.4.m</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Service-oriented Mobile Social Network in Proximity (MSNP) lets participants
establish new social interactions with strangers in public proximity using
heterogeneous platforms and devices. Such characteristic faces challenges in
discovery latency and trustworthiness. In a public service-oriented MSNP
environment, which consists of a large number of participants, a content
requester who searches for a particular service provided by other MSNP
participants will need to retrieve and process a large number of Service
Description Metadata (SDM) files, associated semantic metadata files and
identifying the trustworthiness of the content providers. Performing such tasks
on a resource constraint mobile device can be time consuming, and the overall
discovery performance will be affected and will result in high latency. This
paper analyses the service discovery models of MSNP and presents corresponding
solutions to improve the service discovery performance of MSNP. We firstly
present and analyse the basic service discovery models of service-oriented
MSNP. To follow up, we apply a context-aware user preference prediction scheme
to enhance the speed of the semantic service discovery process. Later, we
address the trustworthiness issue in MSNP and propose a scheme to reduce the
latency of the trustworthy service discovery for MSNP. The proposed scheme has
been tested and evaluated on MSNP application prototype operating on real
mobile devices and MSNP simulation environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01515</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01515</id><created>2015-04-07</created><updated>2015-10-14</updated><authors><author><keyname>Giampouras</keyname><forenames>Paris</forenames></author><author><keyname>Themelis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Rontogiannis</keyname><forenames>Athanasios</forenames></author><author><keyname>Koutroumbas</keyname><forenames>Konstantinos</forenames></author></authors><title>Simultaneously sparse and low-rank abundance matrix estimation for
  hyperspectral image unmixing</title><categories>cs.CV math.OC stat.ML</categories><comments>30 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a plethora of applications dealing with inverse problems, e.g. in image
processing, social networks, compressive sensing, biological data processing
etc., the signal of interest is known to be structured in several ways at the
same time. This premise has recently guided the research to the innovative and
meaningful idea of imposing multiple constraints on the parameters involved in
the problem under study. For instance, when dealing with problems whose
parameters form sparse and low-rank matrices, the adoption of suitably combined
constraints imposing sparsity and low-rankness, is expected to yield
substantially enhanced estimation results. In this paper, we address the
spectral unmixing problem in hyperspectral images. Specifically, two novel
unmixing algorithms are introduced, in an attempt to exploit both spatial
correlation and sparse representation of pixels lying in homogeneous regions of
hyperspectral images. To this end, a novel convex mixed penalty term is first
defined consisting of the sum of the weighted $\ell_1$ and the weighted nuclear
norm of the abundance matrix corresponding to a small area of the image
determined by a sliding square window. This penalty term is then used to
regularize a conventional quadratic cost function and impose simultaneously
sparsity and row-rankness on the abundance matrix. The resulting regularized
cost function is minimized by a) an incremental proximal sparse and low-rank
unmixing algorithm and b) an algorithm based on the alternating minimization
method of multipliers (ADMM). The effectiveness of the proposed algorithms is
illustrated in experiments conducted both on simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01526</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01526</id><created>2015-04-07</created><authors><author><keyname>Hossain</keyname><forenames>M. M. Aftab</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Cavdar</keyname><forenames>Cicek</forenames></author></authors><title>Energy saving market for mobile operators</title><categories>cs.NI</categories><comments>6 pages, 2 figures, to be published in ICC 2015 workshop on Next
  Generation Green ICT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring seamless coverage accounts for the lion's share of the energy
consumed in a mobile network. Overlapping coverage of three to five mobile
network operators (MNOs) results in enormous amount of energy waste which is
avoidable. The traffic demands of the mobile networks vary significantly
throughout the day. As the offered load for all networks are not same at a
given time and the differences in energy consumption at different loads are
significant, multi-MNO capacity/coverage sharing can dramatically reduce energy
consumption of mobile networks and provide the MNOs a cost effective means to
cope with the exponential growth of traffic. In this paper, we propose an
energy saving market for a multi-MNO network scenario. As the competing MNOs
are not comfortable with information sharing, we propose a double auction
clearinghouse market mechanism where MNOs sell and buy capacity in order to
minimize energy consumption. In our setting, each MNO proposes its bids and
asks simultaneously for buying and selling multi-unit capacities respectively
to an independent auctioneer, i.e., clearinghouse and ends up either as a buyer
or as a seller in each round. We show that the mechanism allows the MNOs to
save significant percentage of energy cost throughout a wide range of network
load. Different than other energy saving features such as cell sleep or antenna
muting which can not be enabled at heavy traffic load, dynamic capacity sharing
allows MNOs to handle traffic bursts with energy saving opportunity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01535</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01535</id><created>2015-04-07</created><updated>2015-07-01</updated><authors><author><keyname>van der Hoorn</keyname><forenames>Pim</forenames></author><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author></authors><title>Phase transitions for scaling of structural correlations in directed
  networks</title><categories>physics.soc-ph cs.SI math.PR</categories><msc-class>62H20, 05C80</msc-class><journal-ref>Phys. Rev. E 92, 022803 (2015)</journal-ref><doi>10.1103/PhysRevE.92.022803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of degree-degree dependencies in complex networks, and their impact
on processes on networks requires null models, i.e. models that generate
uncorrelated scale-free networks. Most models to date however show structural
negative dependencies, caused by finite size effects. We analyze the behavior
of these structural negative degree-degree dependencies, using rank based
correlation measures, in the directed Erased Configuration Model. We obtain
expressions for the scaling as a function of the exponents of the
distributions. Moreover, we show that this scaling undergoes a phase
transition, where one region exhibits scaling related to the natural cut-off of
the network while another region has scaling similar to the structural cut-off
for uncorrelated networks. By establishing the speed of convergence of these
structural dependencies we are able to asses statistical significance of
degree-degree dependencies on finite complex networks when compared to networks
generated by the directed Erased Configuration Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01552</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01552</id><created>2015-04-07</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Hybrid Scheduling/Signal-Level Coordination in the Downlink of
  Multi-Cloud Radio-Access Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of resource allocation in cloud-radio access networks, recent
studies assume either signal-level or scheduling-level coordination. This
paper, instead, considers a hybrid level of coordination for the scheduling
problem in the downlink of a multi-cloud radio-access network, as a means to
benefit from both scheduling policies. Consider a multi-cloud radio access
network, where each cloud is connected to several base-stations (BSs) via high
capacity links, and therefore allows joint signal processing between them.
Across the multiple clouds, however, only scheduling-level coordination is
permitted, as it requires a lower level of backhaul communication. The frame
structure of every BS is composed of various time/frequency blocks, called
power-zones (PZs), and kept at fixed power level. The paper addresses the
problem of maximizing a network-wide utility by associating users to clouds and
scheduling them to the PZs, under the practical constraints that each user is
scheduled, at most, to a single cloud, but possibly to many BSs within the
cloud, and can be served by one or more distinct PZs within the BSs' frame. The
paper solves the problem using graph theory techniques by constructing the
conflict graph. The scheduling problem is, then, shown to be equivalent to a
maximum-weight independent set problem in the constructed graph, in which each
vertex symbolizes an association of cloud, user, BS and PZ, with a weight
representing the utility of that association. Simulation results suggest that
the proposed hybrid scheduling strategy provides appreciable gain as compared
to the scheduling-level coordinated networks, with a negligible degradation to
signal-level coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01557</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01557</id><created>2015-04-07</created><updated>2015-04-10</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Meunier</keyname><forenames>No&#xe9;mie</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Weak Subgame Perfect Equilibria and their Application to Quantitative
  Reachability</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study $n$-player turn-based games played on a finite directed graph. For
each play, the players have to pay a cost that they want to minimize. Instead
of the well-known notion of Nash equilibrium (NE), we focus on the notion of
subgame perfect equilibrium (SPE), a refinement of NE well-suited in the
framework of games played on graphs. We also study natural variants of SPE,
named weak (resp. very weak) SPE, where players who deviate cannot use the full
class of strategies but only a subclass with a finite number of (resp. a
unique) deviation step(s).
  Our results are threefold. Firstly, we characterize in the form of a Folk
theorem the set of all plays that are the outcome of a weak SPE. Secondly, for
the class of quantitative reachability games, we prove the existence of a
finite-memory SPE and provide an algorithm for computing it (only existence was
known with no information regarding the memory). Moreover, we show that the
existence of a constrained SPE, i.e. an SPE such that each player pays a cost
less than a given constant, can be decided. The proofs rely on our Folk theorem
for weak SPEs (which coincide with SPEs in the case of quantitative
reachability games) and on the decidability of MSO logic on infinite words.
Finally with similar techniques, we provide a second general class of games for
which the existence of a (constrained) weak SPE is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01561</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01561</id><created>2015-04-07</created><authors><author><keyname>Wu</keyname><forenames>Zuxuan</forenames></author><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Jiang</keyname><forenames>Yu-Gang</forenames></author><author><keyname>Ye</keyname><forenames>Hao</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author></authors><title>Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for
  Video Classification</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying videos according to content semantics is an important problem
with a wide range of applications. In this paper, we propose a hybrid deep
learning framework for video classification, which is able to model static
spatial information, short-term motion, as well as long-term temporal clues in
the videos. Specifically, the spatial and the short-term motion features are
extracted separately by two Convolutional Neural Networks (CNN). These two
types of CNN-based features are then combined in a regularized feature fusion
network for classification, which is able to learn and utilize feature
relationships for improved performance. In addition, Long Short Term Memory
(LSTM) networks are applied on top of the two features to further model
longer-term temporal clues. The main contribution of this work is the hybrid
learning framework that can model several important aspects of the video data.
We also show that (1) combining the spatial and the short-term motion features
in the regularized fusion network is better than direct classification and
fusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is
highly complementary to the traditional classification strategy without
considering the temporal frame orders. Extensive experiments are conducted on
two popular and challenging benchmarks, the UCF-101 Human Actions and the
Columbia Consumer Videos (CCV). On both benchmarks, our framework achieves
to-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ on
the CCV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01563</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01563</id><created>2015-04-07</created><authors><author><keyname>Tygel</keyname><forenames>Alan Freihof</forenames></author><author><keyname>Attard</keyname><forenames>Judie</forenames></author><author><keyname>Orlandi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Campos</keyname><forenames>Maria Luiza Machado</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>&quot;How much?&quot; Is Not Enough - An Analysis of Open Budget Initiatives</title><categories>cs.CY cs.DB</categories><comments>10 pages. Submited as a full paper to WebSci2015 conference on 20th
  March 2015</comments><acm-class>J.1; E.m</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A worldwide movement towards the publication of Open Government Data is
taking place, and budget data is one of the key elements pushing this trend.
Its importance is mostly related to transparency, but publishing budget data,
combined with other actions, can also improve democratic participation, allow
comparative analysis of governments and boost data-driven business. However,
the lack of standards and common evaluation criteria still hinders the
development of appropriate tools and the materialization of the appointed
benefits. In this paper, we present a model to analyse government initiatives
to publish budget data. We identify the main features of these initiatives with
a double objective: (i) to drive a structured analysis, relating some
dimensions to their possible impacts, and (ii) to derive characterization
attributes to compare initiatives based on each dimension. We define use
perspectives and analyse some initiatives using this model. We conclude that,
in order to favour use perspectives, special attention must be given to user
feedback, semantics standards and linking possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01575</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01575</id><created>2015-04-07</created><updated>2015-11-02</updated><authors><author><keyname>Berglund</keyname><forenames>Mathias</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Honkala</keyname><forenames>Mikko</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Leo</forenames></author><author><keyname>Vetek</keyname><forenames>Akos</forenames></author><author><keyname>Karhunen</keyname><forenames>Juha</forenames></author></authors><title>Bidirectional Recurrent Neural Networks as Generative Models -
  Reconstructing Gaps in Time Series</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional recurrent neural networks (RNN) are trained to predict both in
the positive and negative time directions simultaneously. They have not been
used commonly in unsupervised tasks, because a probabilistic interpretation of
the model has been difficult. Recently, two different frameworks, GSN and NADE,
provide a connection between reconstruction and probabilistic modeling, which
makes the interpretation possible. As far as we know, neither GSN or NADE have
been studied in the context of time series before. As an example of an
unsupervised task, we study the problem of filling in gaps in high-dimensional
time series with complex dynamics. Although unidirectional RNNs have recently
been trained successfully to model such time series, inference in the negative
time direction is non-trivial. We propose two probabilistic interpretations of
bidirectional RNNs that can be used to reconstruct missing gaps efficiently.
Our experiments on text data show that both proposed methods are much more
accurate than unidirectional reconstructions, although a bit less accurate than
a computationally complex bidirectional Bayesian inference on the
unidirectional RNN. We also provide results on music data for which the
Bayesian inference is computationally infeasible, demonstrating the scalability
of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01584</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01584</id><created>2015-04-07</created><updated>2015-07-09</updated><authors><author><keyname>Bury</keyname><forenames>Marc</forenames></author><author><keyname>Schwiegelshohn</keyname><forenames>Chris</forenames></author></authors><title>Random Projections for k-Means: Maintaining Coresets Beyond Merge &amp;
  Reduce</title><categories>cs.DS</categories><comments>This paper has been withdrawn due to an error in Theorem 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new construction for a small space summary satisfying the coreset
guarantee of a data set with respect to the $k$-means objective function. The
number of points required in an offline construction is in $\tilde{O}(k
\epsilon^{-2}\min(d,k\epsilon^{-2}))$ which is minimal among all available
constructions.
  Aside from two constructions with exponential dependence on the dimension,
all known coresets are maintained in data streams via the merge and reduce
framework, which incurs are large space dependency on $\log n$. Instead, our
construction crucially relies on Johnson-Lindenstrauss type embeddings which
combined with results from online algorithms give us a new technique for
efficiently maintaining coresets in data streams without relying on merge and
reduce. The final number of points stored by our algorithm in a data stream is
in $\tilde{O}(k^2 \epsilon^{-2} \log^2 n \min(d,k\epsilon^{-2}))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01612</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01612</id><created>2015-04-07</created><updated>2015-07-29</updated><authors><author><keyname>Kelbert</keyname><forenames>Mark</forenames></author><author><keyname>Mozgunov</keyname><forenames>Pavel</forenames></author></authors><title>Asymptotic behaviour of weighted differential entropies in a Bayesian
  problem</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>33 pages</comments><msc-class>94A17(Primary), 62B10, 62C10 (Secondary)</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Bayesian problem of estimating of probability of success in a
series of conditionally independent trials with binary outcomes. We study the
asymptotic behaviour of differential entropy for posterior probability density
function conditional on $x$ successes after $n$ conditionally independent
trials, when $n \to \infty$. It is shown that after an appropriate
normalization in cases $x \sim n$ and $x$ $\sim n^\beta$ ($0&lt;\beta&lt;1$) limiting
distribution is Gaussian and the differential entropy of standardized RV
converges to differential entropy of standard Gaussian random variable. When
$x$ or $n-x$ is a constant the limiting distribution in not Gaussian, but still
the asymptotic of differential entropy can be found explicitly.
  Then suppose that one is interested to know whether the coin is fair or not
and for large $n$ is interested in the true frequency. To do so the concept of
weighted differential entropy introduced in \cite{Belis1968} is used when the
frequency $\gamma$ is necessary to emphasize. It was found that the weight in
suggested form does not change the asymptotic form of Shannon, Renyi, Tsallis
and Fisher entropies, but change the constants. The main term in weighted
Fisher Information is changed by some constant which depend on distance between
the true frequency and the value we want to emphasize.
  In third part we derived the weighted versions of Rao-Cram\'er, Bhattacharyya
and Kullback inequalities. This result is applied to the Bayesian problem
described above. The asymptotic forms of these inequalities are obtained for a
particular class of weight functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01614</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01614</id><created>2015-04-07</created><authors><author><keyname>Kittichokechai</keyname><forenames>Kittipong</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Secret key-based Authentication with a Privacy Constraint</title><categories>cs.CR cs.IT math.IT</categories><comments>8 pages, 2 figures, to be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider problems of authentication using secret key generation under a
privacy constraint on the enrolled source data. An adversary who has access to
the stored description and correlated side information tries to deceive the
authentication as well as learn about the source. We characterize the optimal
tradeoff between the compression rate of the stored description, the leakage
rate of the source data, and the exponent of the adversary's maximum false
acceptance probability. The related problem of secret key generation with a
privacy constraint is also studied where the optimal tradeoff between the
compression rate, leakage rate, and secret key rate is characterized. It
reveals a connection between the optimal secret key rate and security of the
authentication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01617</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01617</id><created>2015-04-07</created><authors><author><keyname>Ahmed</keyname><forenames>Karam</forenames></author><author><keyname>Abuelenin</keyname><forenames>Sherif</forenames></author><author><keyname>Soliman</keyname><forenames>Heba</forenames></author><author><keyname>Al-Barbary</keyname><forenames>Khairy</forenames></author></authors><title>Low Complexity V-BLAST MIMO-OFDM Detector by Successive Iterations
  Reduction</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, 2 tables. The final publication is available at
  www.aece.ro</comments><journal-ref>Advances in Electrical and Computer Engineering, vol. 15, no. 1,
  pp. 77-82, 2015</journal-ref><doi>10.4316/AECE.2015.01011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  V-BLAST detection method suffers large computational complexity due to its
successive detection of symbols. In this paper, we propose a modified V-BLAST
algorithm to decrease the computational complexity by reducing the number of
detection iterations required in MIMO communication systems. We begin by
showing the existence of a maximum number of iterations, beyond which, no
significant improvement is obtained. We establish a criterion for the number of
maximum effective iterations. We propose a modified algorithm that uses the
measured SNR to dynamically set the number of iterations to achieve an
acceptable bit-error rate. Then, we replace the feedback algorithm with an
approximate linear function to reduce the complexity. Simulations show that
significant reduction in computational complexity is achieved compared to the
ordinary V-BLAST, while maintaining a good BER performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01623</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01623</id><created>2015-04-07</created><updated>2015-04-27</updated><authors><author><keyname>Bouchard</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Dieudonn&#xe9;</keyname><forenames>Yoann</forenames></author><author><keyname>Ducourthial</keyname><forenames>Bertrand</forenames></author></authors><title>Byzantine Gathering in Networks</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates an open problem introduced in [14]. Two or more
mobile agents start from different nodes of a network and have to accomplish
the task of gathering which consists in getting all together at the same node
at the same time. An adversary chooses the initial nodes of the agents and
assigns a different positive integer (called label) to each of them. Initially,
each agent knows its label but does not know the labels of the other agents or
their positions relative to its own. Agents move in synchronous rounds and can
communicate with each other only when located at the same node. Up to f of the
agents are Byzantine. A Byzantine agent can choose an arbitrary port when it
moves, can convey arbitrary information to other agents and can change its
label in every round, in particular by forging the label of another agent or by
creating a completely new one.
  What is the minimum number M of good agents that guarantees deterministic
gathering of all of them, with termination?
  We provide exact answers to this open problem by considering the case when
the agents initially know the size of the network and the case when they do
not. In the former case, we prove M=f+1 while in the latter, we prove M=f+2.
More precisely, for networks of known size, we design a deterministic algorithm
gathering all good agents in any network provided that the number of good
agents is at least f+1. For networks of unknown size, we also design a
deterministic algorithm ensuring the gathering of all good agents in any
network but provided that the number of good agents is at least f+2. Both of
our algorithms are optimal in terms of required number of good agents, as each
of them perfectly matches the respective lower bound on M shown in [14], which
is of f+1 when the size of the network is known and of f+2 when it is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01628</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01628</id><created>2015-04-07</created><updated>2015-10-13</updated><authors><author><keyname>Arts</keyname><forenames>Martijn</forenames></author><author><keyname>Bollig</keyname><forenames>Andreas</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>Quickest Eigenvalue-Based Spectrum Sensing using Random Matrix Theory</title><categories>cs.IT cs.NI math.IT</categories><comments>updated copyright information; corrected error in definition of the
  non-centrality matrix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the potential of quickest detection based on the eigenvalues
of the sample covariance matrix for spectrum sensing applications. A simple
phase shift keying (PSK) model with additive white Gaussian noise (AWGN), with
$1$ primary user (PU) and $K$ secondary users (SUs) is considered. Under both
detection hypotheses $\mathcal{H}_0$ (noise only) and $\mathcal{H}_1$ (signal +
noise) the eigenvalues of the sample covariance matrix follow Wishart
distributions. For the case of $K = 2$ SUs, we derive an analytical formulation
of the probability density function (PDF) of the maximum-minimum eigenvalue
(MME) detector under $\mathcal{H}_1$. Utilizing results from the literature
under $\mathcal{H}_0$, we investigate two detection schemes. First, we
calculate the receiver operator characteristic (ROC) for MME block detector
based on analytical results. Second, we introduce two eigenvalue-based quickest
detection algorithms: a cumulative sum (CUSUM) algorithm, when the
signal-to-noise ratio (SNR) of the PU signal is known and an algorithm using
the generalized likelihood ratio, in case the SNR is unknown. Bounds on the
mean time to false-alarm $\tau_\text{fa}$ and the mean time to detection
$\tau_\text{d}$ are given for the CUSUM algorithm. Numerical simulations
illustrate the potential advantages of the quickest detection approach over the
block detection scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01637</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01637</id><created>2015-04-07</created><authors><author><keyname>Sanli</keyname><forenames>Ceyda</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Local Variation of Collective Attention in Hashtag Spike Trains</title><categories>cs.SI cs.CY</categories><comments>5 pages, 3 figures. Technical Report of the International AAAI
  Conference on Weblogs and Social Media (ICWSM-15) Workshop 3: Modeling and
  Mining Temporal Interactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a methodology quantifying temporal patterns of
nonlinear hashtag time series. Our approach is based on an analogy between
neuron spikes and hashtag diffusion. We adopt the local variation, originally
developed to analyze local time delays in neuron spike trains. We show that the
local variation successfully characterizes nonlinear features of hashtag spike
trains such as burstiness and regularity. We apply this understanding in an
extreme social event and are able to observe temporal evaluation of online
collective attention of Twitter users to that event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01639</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01639</id><created>2015-04-07</created><updated>2015-07-08</updated><authors><author><keyname>Bola&#xf1;os</keyname><forenames>Marc</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author></authors><title>Ego-Object Discovery</title><categories>cs.CV cs.AI</categories><comments>9 pages, 13 figures, Submitted to: Image and Vision Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifelogging devices are spreading faster everyday. This growth can represent
great benefits to develop methods for extraction of meaningful information
about the user wearing the device and his/her environment. In this paper, we
propose a semi-supervised strategy for easily discovering objects relevant to
the person wearing a first-person camera. Given an egocentric video/images
sequence acquired by the camera, our algorithm uses both the appearance
extracted by means of a convolutional neural network and an object refill
methodology that allows to discover objects even in case of small amount of
object appearance in the collection of images. An SVM filtering strategy is
applied to deal with the great part of the False Positive object candidates
found by most of the state of the art object detectors. We validate our method
on a new egocentric dataset of 4912 daily images acquired by 4 persons as well
as on both PASCAL 2012 and MSRC datasets. We obtain for all of them results
that largely outperform the state of the art approach. We make public both the
EDUB dataset and the algorithm code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01647</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01647</id><created>2015-04-07</created><authors><author><keyname>Vieira</keyname><forenames>Fabio R. J.</forenames></author><author><keyname>de Rezende</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>Scheduling wireless links by graph multicoloring in the physical
  interference model</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling wireless links for simultaneous activation in such a way that all
transmissions are successfully decoded at the receivers and moreover network
capacity is maximized is a computationally hard problem. Usually it is tackled
by heuristics whose output is a sequence of time slots in which every link
appears in exactly one time slot. Such approaches can be interpreted as the
coloring of a graph's vertices so that every vertex gets exactly one color.
Here we introduce a new approach that can be viewed as assigning multiple
colors to each vertex, so that, in the resulting schedule, every link may
appear more than once (though the same number of times for all links). We
report on extensive computational experiments, under the physical interference
model, revealing substantial gains for a variety of randomly generated
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01649</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01649</id><created>2015-04-07</created><authors><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>The List-Decoding Size of Fourier-Sparse Boolean Functions</title><categories>cs.DS cs.CC</categories><comments>16 pages, CCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function defined on the Boolean hypercube is $k$-Fourier-sparse if it has
at most $k$ nonzero Fourier coefficients. For a function $f: \mathbb{F}_2^n
\rightarrow \mathbb{R}$ and parameters $k$ and $d$, we prove a strong upper
bound on the number of $k$-Fourier-sparse Boolean functions that disagree with
$f$ on at most $d$ inputs. Our bound implies that the number of uniform and
independent random samples needed for learning the class of $k$-Fourier-sparse
Boolean functions on $n$ variables exactly is at most $O(n \cdot k \log k)$.
  As an application, we prove an upper bound on the query complexity of testing
Booleanity of Fourier-sparse functions. Our bound is tight up to a logarithmic
factor and quadratically improves on a result due to Gur and Tamuz (Chicago J.
Theor. Comput. Sci., 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01650</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01650</id><created>2015-04-07</created><authors><author><keyname>Bialas</keyname><forenames>Piotr</forenames></author><author><keyname>Strzelecki</keyname><forenames>Adam</forenames></author></authors><title>Benchmarking the cost of thread divergence in CUDA</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All modern processors include a set of vector instructions. While this gives
a tremendous boost to the performance, it requires a vectorized code that can
take advantage of such instructions. As an ideal vectorization is hard to
achieve in practice, one has to decide when different instructions may be
applied to different elements of the vector operand. This is especially
important in implicit vectorization as in NVIDIA CUDA Single Instruction
Multiple Threads (SIMT) model, where the vectorization details are hidden from
the programmer. In order to assess the costs incurred by incompletely
vectorized code, we have developed a micro-benchmark that measures the
characteristics of the CUDA thread divergence model on different architectures
focusing on the loops performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01656</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01656</id><created>2015-04-07</created><authors><author><keyname>Lauria</keyname><forenames>Massimo</forenames></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Jakob</forenames></author></authors><title>Tight Size-Degree Bounds for Sums-of-Squares Proofs</title><categories>cs.CC</categories><acm-class>F.2.3; F.1.3; I.2.3; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We exhibit families of $4$-CNF formulas over $n$ variables that have
sums-of-squares (SOS) proofs of unsatisfiability of degree (a.k.a. rank) $d$
but require SOS proofs of size $n^{\Omega(d)}$ for values of $d = d(n)$ from
constant all the way up to $n^{\delta}$ for some universal constant$\delta$.
This shows that the $n^{O(d)}$ running time obtained by using the Lasserre
semidefinite programming relaxations to find degree-$d$ SOS proofs is optimal
up to constant factors in the exponent. We establish this result by combining
$\mathsf{NP}$-reductions expressible as low-degree SOS derivations with the
idea of relativizing CNF formulas in [Kraj\'i\v{c}ek '04] and [Dantchev and
Riis'03], and then applying a restriction argument as in [Atserias, M\&quot;uller,
and Oliva '13] and [Atserias, Lauria, and Nordstr\&quot;om '14]. This yields a
generic method of amplifying SOS degree lower bounds to size lower bounds, and
also generalizes the approach in [ALN14] to obtain size lower bounds for the
proof systems resolution, polynomial calculus, and Sherali-Adams from lower
bounds on width, degree, and rank, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01661</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01661</id><created>2015-03-16</created><authors><author><keyname>Khmou</keyname><forenames>Youssef</forenames></author><author><keyname>Safi</keyname><forenames>Said</forenames></author></authors><title>An original Propagator for large array</title><categories>cs.IT math.IT stat.AP stat.CO</categories><comments>Fourteen pages and four figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we demonstrate that when the ratio $n$ of the number of
antenna elements $N$ to the number $P$ of radiating sources is superior or
equal to $2$, then it is possible to choose a propagator from a set of
$n(n+1)/2-1$ operators to compute the Angles of Arrival (AoA) of the narrowband
incoming waves. This new non eigenbased approach is efficient when the Signal
to Noise Ratio (SNR) is moderate, and gives multitude of possibilities, that
are dependent of the random data, to construct the complex sets whose columns
are orthogonal to the signal subspace generated by the radiating sources.
Elementary examples are given for $n=3$, $n=4$ and $n=6$. The simulation
results are presented to illustrate the performance of the proposed
computational methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01662</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01662</id><created>2015-03-12</created><authors><author><keyname>Xenaki</keyname><forenames>Angeliki</forenames></author><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author></authors><title>Grid-free compressive beamforming</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The direction-of-arrival (DOA) estimation problem involves the localization
of a few sources from a limited number of observations on an array of sensors,
thus it can be formulated as a sparse signal reconstruction problem and solved
efficiently with compressive sensing (CS) to achieve high-resolution imaging.
On a discrete angular grid, the CS reconstruction degrades due to basis
mismatch when the DOAs do not coincide with the angular directions on the grid.
To overcome this limitation, a continuous formulation of the DOA problem is
employed and an optimization procedure is introduced, which promotes sparsity
on a continuous optimization variable. The DOA estimation problem with
infinitely many unknowns, i.e., source locations and amplitudes, is solved over
a few optimization variables with semidefinite programming. The grid-free CS
reconstruction provides high-resolution imaging even with non-uniform arrays,
single-snapshot data and under noisy conditions as demonstrated on experimental
towed array data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01665</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01665</id><created>2015-04-06</created><authors><author><keyname>Chakravarti</keyname><forenames>Mohnish</forenames></author><author><keyname>Chakravarti</keyname><forenames>Arati</forenames></author></authors><title>Does Gaming Help Improve Cognitive Skills?</title><categories>cs.CY</categories><comments>10 pages, 4 figures</comments><journal-ref>Journal of Emerging Investigators, January 2015 (5), 4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nationally representative study of video game play among adolescents in the
United States showed that 97% of adolescents aged 12 to 17 years play computer,
web, and portable (or console) video games (Lenhart et al., 2008). We
hypothesized that if people play games as a regular exercise regime, gaming
will correlate with an improvement in their cognitive skills. For this
experiment, a few games that tested the logical reasoning and critical analysis
skills under a given time constraint were coded in Python using Pygame and were
played by a group of 7th grade students. In order to test whether there is a
relationship between gaming and test performance, we divided the students into
two groups and gave them tests before and after the experimentation period in
order to measure their improvement. One group played the games while the other
did not. In the group of students that played the games, an average improvement
of 62.19% was seen (p &lt; 0.0001). The group that did not play the games only
improved their performance by an average of 18.51% (p = 0.0882).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01666</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01666</id><created>2015-04-07</created><authors><author><keyname>Dayan</keyname><forenames>Niv</forenames></author><author><keyname>Bonnet</keyname><forenames>Philippe</forenames></author></authors><title>Garbage Collection Techniques for Flash-Resident Page-Mapping FTLs</title><categories>cs.DB cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storage devices based on flash memory have replaced hard disk drives (HDDs)
due to their superior performance, increasing density, and lower power
consumption. Unfortunately, flash memory is subject to challenging
idiosyncrasies like erase-before-write and limited block lifetime. These
constraints are handled by a flash translation layer (FTL), which performs
out-of-place updates, wear-leveling and garbage-collection behind the scene,
while offering the application a virtualization of the physical address space.
  A class of relevant FTLs employ a flash-resident page-associative mapping
table from logical to physical addresses, with a smaller RAM-resident cache for
frequently mapped entries. In this paper, we address the problem of performing
garbage-collection under such FTLs. We observe two problems. Firstly,
maintaining the metadata needed to perform garbage-collection under these
schemes is problematic, because at write-time we do not necessarily know the
physical address of the before-image. Secondly, the size of this metadata must
remain small, because it makes RAM unavailable for caching frequently accessed
entries. We propose two complementary techniques, called Lazy Gecko and
Logarithmic Gecko, which address these issues. Lazy Gecko works well when RAM
is plentiful enough to store the GC metadata. Logarithmic Gecko works well when
RAM isn't plentiful and efficiently stores the GC metadata in flash. Thus,
these techniques are applicable to a wide range of flash devices with varying
amounts of embedded RAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01683</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01683</id><created>2015-04-07</created><updated>2015-07-06</updated><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Cao</keyname><forenames>Kai</forenames></author><author><keyname>He</keyname><forenames>Yifan</forenames></author><author><keyname>Grishman</keyname><forenames>Ralph</forenames></author></authors><title>Jointly Embedding Relations and Mentions for Knowledge Population</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes a joint embedding model for predicting relations
between a pair of entities in the scenario of relation inference. It differs
from most stand-alone approaches which separately operate on either knowledge
bases or free texts. The proposed model simultaneously learns low-dimensional
vector representations for both triplets in knowledge repositories and the
mentions of relations in free texts, so that we can leverage the evidence both
resources to make more accurate predictions. We use NELL to evaluate the
performance of our approach, compared with cutting-edge methods. Results of
extensive experiments show that our model achieves significant improvement on
relation extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01684</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01684</id><created>2015-04-07</created><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author><author><keyname>Grishman</keyname><forenames>Ralph</forenames></author></authors><title>Large Margin Nearest Neighbor Embedding for Knowledge Representation</title><categories>cs.AI cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1503.08155</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional way of storing facts in triplets ({\it head\_entity, relation,
tail\_entity}), abbreviated as ({\it h, r, t}), makes the knowledge intuitively
displayed and easily acquired by mankind, but hardly computed or even reasoned
by AI machines. Inspired by the success in applying {\it Distributed
Representations} to AI-related fields, recent studies expect to represent each
entity and relation with a unique low-dimensional embedding, which is different
from the symbolic and atomic framework of displaying knowledge in triplets. In
this way, the knowledge computing and reasoning can be essentially facilitated
by means of a simple {\it vector calculation}, i.e. ${\bf h} + {\bf r} \approx
{\bf t}$. We thus contribute an effective model to learn better embeddings
satisfying the formula by pulling the positive tail entities ${\bf t^{+}}$ to
get together and close to {\bf h} + {\bf r} ({\it Nearest Neighbor}), and
simultaneously pushing the negatives ${\bf t^{-}}$ away from the positives
${\bf t^{+}}$ via keeping a {\it Large Margin}. We also design a corresponding
learning algorithm to efficiently find the optimal solution based on {\it
Stochastic Gradient Descent} in iterative fashion. Quantitative experiments
illustrate that our approach can achieve the state-of-the-art performance,
compared with several latest methods on some benchmark datasets for two
classical applications, i.e. {\it Link prediction} and {\it Triplet
classification}. Moreover, we analyze the parameter complexities among all the
evaluated models, and analytical results indicate that our model needs fewer
computational resources on outperforming the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01690</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01690</id><created>2015-04-07</created><updated>2015-04-07</updated><authors><author><keyname>Nazer</keyname><forenames>Bobak</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck</forenames></author><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Expanding the Compute-and-Forward Framework: Unequal Powers, Signal
  Levels, and Multiple Linear Combinations</title><categories>cs.IT math.IT</categories><comments>47 pages, 10 figures, Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compute-and-forward framework permits each receiver in a Gaussian network
to directly decode a linear combination of the transmitted messages. The
resulting linear combinations can then be employed as an end-to-end
communication strategy for relaying, interference alignment, and other
applications. Recent efforts have demonstrated the advantages of employing
unequal powers at the transmitters and decoding more than one linear
combination at each receiver. However, neither of these techniques fit
naturally within the original formulation of compute-and-forward. This paper
proposes an expanded compute-and-forward framework that incorporates both of
these possibilities and permits an intuitive interpretation in terms of signal
levels. Within this framework, recent achievability and optimality results are
unified and generalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01693</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01693</id><created>2015-04-07</created><authors><author><keyname>Holland</keyname><forenames>Benjamin</forenames></author><author><keyname>Deering</keyname><forenames>Tom</forenames></author><author><keyname>Kothari</keyname><forenames>Suresh</forenames></author><author><keyname>Mathews</keyname><forenames>Jon</forenames></author><author><keyname>Ranade</keyname><forenames>Nikhil</forenames></author></authors><title>Security Toolbox for Detecting Novel and Sophisticated Android Malware</title><categories>cs.CR cs.HC</categories><comments>4 pages, 1 listing, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a demo of our Security Toolbox to detect novel malware in
Android apps. This Toolbox is developed through our recent research project
funded by the DARPA Automated Program Analysis for Cybersecurity (APAC)
project. The adversarial challenge (&quot;Red&quot;) teams in the DARPA APAC program are
tasked with designing sophisticated malware to test the bounds of malware
detection technology being developed by the research and development (&quot;Blue&quot;)
teams. Our research group, a Blue team in the DARPA APAC program, proposed a
&quot;human-in-the-loop program analysis&quot; approach to detect malware given the
source or Java bytecode for an Android app. Our malware detection apparatus
consists of two components: a general-purpose program analysis platform called
Atlas, and a Security Toolbox built on the Atlas platform. This paper describes
the major design goals, the Toolbox components to achieve the goals, and the
workflow for auditing Android apps. The accompanying video
(http://youtu.be/WhcoAX3HiNU) illustrates features of the Toolbox through a
live audit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01697</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01697</id><created>2015-04-07</created><authors><author><keyname>Yang</keyname><forenames>Jiyan</forenames></author><author><keyname>Gittens</keyname><forenames>Alex</forenames></author></authors><title>Tensor machines for learning target-specific polynomial features</title><categories>cs.LG stat.ML</categories><comments>19 pages, 4 color figures, 2 tables. Submitted to ECML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have demonstrated that using random feature maps can
significantly decrease the training and testing times of kernel-based
algorithms without significantly lowering their accuracy. Regrettably, because
random features are target-agnostic, typically thousands of such features are
necessary to achieve acceptable accuracies. In this work, we consider the
problem of learning a small number of explicit polynomial features. Our
approach, named Tensor Machines, finds a parsimonious set of features by
optimizing over the hypothesis class introduced by Kar and Karnick for random
feature maps in a target-specific manner. Exploiting a natural connection
between polynomials and tensors, we provide bounds on the generalization error
of Tensor Machines. Empirically, Tensor Machines behave favorably on several
real-world datasets compared to other state-of-the-art techniques for learning
polynomial features, and deliver significantly more parsimonious models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01705</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01705</id><created>2015-04-06</created><authors><author><keyname>G.</keyname><forenames>Deepa K.</forenames></author><author><keyname>Ambat</keyname><forenames>Sooraj K.</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Fusion of Sparse Reconstruction Algorithms for Multiple Measurement
  Vectors</title><categories>stat.ME cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recovery of sparse signals that share a common support from
multiple measurement vectors. The performance of several algorithms developed
for this task depends on parameters like dimension of the sparse signal,
dimension of measurement vector, sparsity level, measurement noise. We propose
a fusion framework, where several multiple measurement vector reconstruction
algorithms participate and the final signal estimate is obtained by combining
the signal estimates of the participating algorithms. We present the conditions
for achieving a better reconstruction performance than the participating
algorithms. Numerical simulations demonstrate that the proposed fusion
algorithm often performs better than the participating algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01708</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01708</id><created>2015-04-07</created><updated>2015-05-04</updated><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Reactive Synthesis Without Regret</title><categories>cs.GT cs.FL cs.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-player zero-sum games of infinite duration and their quantitative
versions are used in verification to model the interaction between a controller
(Eve) and its environment (Adam). The question usually addressed is that of the
existence (and computability) of a strategy for Eve that can maximize her
payoff against any strategy of Adam. In this work, we are interested in
strategies of Eve that minimize her regret, i.e. strategies that minimize the
difference between her actual payoff and the payoff she could have achieved if
she had known the strategy of Adam in advance. We give algorithms to compute
the strategies of Eve that ensure minimal regret against an adversary whose
choice of strategy is (i) unrestricted, (ii) limited to positional strategies,
or (iii) limited to word strategies. We also establish relations between the
latter version and other problems studied in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01709</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01709</id><created>2015-04-07</created><authors><author><keyname>Mazowiecki</keyname><forenames>Filip</forenames></author><author><keyname>Riveros</keyname><forenames>Cristian</forenames></author></authors><title>On the expressibility of copyless cost register automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cost register automata (CRA) were proposed by Alur et all as an alternative
model for weighted automata. In hope of finding decidable subclasses of CRA,
they proposed to restrict their model with the copyless restriction but nothing
is really know about the structure or properties of this new computational
model called copyless CRA.
  In this paper we study the properties and expressiveness of copyless CRA. We
propose a normal form for copyless CRA and we study the properties of a special
group of registers (called stable registers). Furthermore, we find that
copyless CRA do not have good closure properties since we show that they are
not closed under reverse operation. Finally, we propose a subclass of copyless
CRA and we show that this subclass is closed under regular-lookahead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01716</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01716</id><created>2015-04-07</created><updated>2015-04-16</updated><authors><author><keyname>Huval</keyname><forenames>Brody</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Tandon</keyname><forenames>Sameep</forenames></author><author><keyname>Kiske</keyname><forenames>Jeff</forenames></author><author><keyname>Song</keyname><forenames>Will</forenames></author><author><keyname>Pazhayampallil</keyname><forenames>Joel</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Rajpurkar</keyname><forenames>Pranav</forenames></author><author><keyname>Migimatsu</keyname><forenames>Toki</forenames></author><author><keyname>Cheng-Yue</keyname><forenames>Royce</forenames></author><author><keyname>Mujica</keyname><forenames>Fernando</forenames></author><author><keyname>Coates</keyname><forenames>Adam</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>An Empirical Evaluation of Deep Learning on Highway Driving</title><categories>cs.RO cs.CV</categories><comments>Added a video for lane detection</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous groups have applied a variety of deep learning techniques to
computer vision problems in highway perception scenarios. In this paper, we
presented a number of empirical evaluations of recent deep learning advances.
Computer vision, combined with deep learning, has the potential to bring about
a relatively inexpensive, robust solution to autonomous driving. To prepare
deep learning for industry uptake and practical applications, neural networks
will require large data sets that represent all possible driving environments
and scenarios. We collect a large data set of highway data and apply deep
learning and computer vision algorithms to problems such as car and lane
detection. We show how existing convolutional neural networks (CNNs) can be
used to perform lane and vehicle detection while running at frame rates
required for a real-time system. Our results lend credence to the hypothesis
that deep learning holds promise for autonomous driving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01718</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01718</id><created>2015-04-07</created><authors><author><keyname>Matias</keyname><forenames>Paulo</forenames></author><author><keyname>Guariento</keyname><forenames>Rafael Tuma</forenames></author><author><keyname>de Almeida</keyname><forenames>Lirio Onofre Baptista</forenames></author><author><keyname>Slaets</keyname><forenames>Jan Frans Willem</forenames></author></authors><title>Modular Acquisition and Stimulation System for Timestamp-Driven
  Neuroscience Experiments</title><categories>q-bio.QM cs.AR</categories><comments>Preprint submitted to ARC 2015. Extended: 16 pages, 10 figures. The
  final publication is available at link.springer.com</comments><journal-ref>Lecture Notes in Computer Science Volume 9040, 2015, pp 339-348</journal-ref><doi>10.1007/978-3-319-16214-0_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dedicated systems are fundamental for neuroscience experimental protocols
that require timing determinism and synchronous stimuli generation. We
developed a data acquisition and stimuli generator system for neuroscience
research, optimized for recording timestamps from up to 6 spiking neurons and
entirely specified in a high-level Hardware Description Language (HDL). Despite
the logic complexity penalty of synthesizing from such a language, it was
possible to implement our design in a low-cost small reconfigurable device.
Under a modular framework, we explored two different memory arbitration schemes
for our system, evaluating both their logic element usage and resilience to
input activity bursts. One of them was designed with a decoupled and latency
insensitive approach, allowing for easier code reuse, while the other adopted a
centralized scheme, constructed specifically for our application. The usage of
a high-level HDL allowed straightforward and stepwise code modifications to
transform one architecture into the other. The achieved modularity is very
useful for rapidly prototyping novel electronic instrumentation systems
tailored to scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01747</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01747</id><created>2015-04-07</created><authors><author><keyname>Vilaipornsawai</keyname><forenames>Usa</forenames></author><author><keyname>Nikopour</keyname><forenames>Hosein</forenames></author><author><keyname>Bayesteh</keyname><forenames>Alireza</forenames></author><author><keyname>Ma</keyname><forenames>Jianglie</forenames></author></authors><title>SCMA for Open-Loop Joint Transmission CoMP</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Code Multiple Access (SCMA), a non-orthogonal multiple access scheme,
has been introduced as a key 5G technology to improve spectral efficiency. In
this work, we propose SCMA to enable open-loop coordinated multipoint (CoMP)
joint transmission (JT). The scheme combines CoMP techniques with multi-user
SCMA (MU-SCMA) in downlink. This scheme provides open-loop user multiplexing
and JT in power and code domains, with robustness to mobility and low overhead
of channel state information (CSI) acquisition. The combined scheme is called
MU-SCMA-CoMP, in which SCMA layers and transmit power of multiple transmit
points (TPs) are shared among multiple users while a user may receive multiple
SCMA layers from multiple TPs within a CoMP cluster. The benefits of the
proposed scheme includes: i) drastic overhead reduction of CSI acquisition, ii)
significant increase in throughput and coverage, and iii) robustness to channel
aging. Various algorithms of MU-SCMA-CoMP are presented, including the
detection strategy, power sharing optimization, and scheduling. System level
evaluation shows that the proposed schemes provide significant throughput and
coverage gains over OFDMA for both pedestrian and vehicular users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01753</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01753</id><created>2015-04-07</created><authors><author><keyname>Chen</keyname><forenames>Xida</forenames></author><author><keyname>Sutphen</keyname><forenames>Steve</forenames></author><author><keyname>Macoun</keyname><forenames>Paul</forenames></author><author><keyname>Yang</keyname><forenames>Yee-Hong</forenames></author></authors><title>Design and Implementation of a 3D Undersea Camera System</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the design and development of an undersea camera
system. The goal of our system is to provide a 3D model of the undersea habitat
in a long-term continuous manner. The most important feature of our system is
the use of multiple cameras and multiple projectors, which is able to provide
accurate 3D models with an accuracy of a millimeter. By introducing projectors
in our system, we can use many different structured light methods for different
tasks. There are two main advantages comparing our system with using ROVs or
AUVs. First, our system can provide continuous monitoring of the undersea
habitat. Second, our system has a low hardware cost. Comparing to existing
deployed camera systems, the advantage of our system is that it can provide
accurate 3D models and provides opportunities for future development of
innovative algorithms for undersea research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01760</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01760</id><created>2015-04-07</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>User Effort and Network Structure Mediate Access to Information in
  Networks</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>The 9TH International AAAI Conference on Web and Social Media</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals' access to information in a social network depends on its
distributed and where in the network individuals position themselves. However,
individuals have limited capacity to manage their social connections and
process information. In this work, we study how this limited capacity and
network structure interact to affect the diversity of information social media
users receive. Previous studies of the role of networks in information access
were limited in their ability to measure the diversity of information. We
address this problem by learning the topics of interest to social media users
by observing messages they share online with their followers. We present a
probabilistic model that incorporates human cognitive constraints in a
generative model of information sharing. We then use the topics learned by the
model to measure the diversity of information users receive from their social
media contacts. We confirm that users in structurally diverse network
positions, which bridge otherwise disconnected regions of the follower graph,
are exposed to more diverse information. In addition, we identify user effort
as an important variable that mediates access to diverse information in social
media. Users who invest more effort into their activity on the site not only
place themselves in more structurally diverse positions within the network than
the less engaged users, but they also receive more diverse information when
located in similar network positions. These findings indicate that the
relationship between network structure and access to information in networks is
more nuanced than previously thought.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01771</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01771</id><created>2015-04-07</created><authors><author><keyname>Gushchin</keyname><forenames>Andrey</forenames></author><author><keyname>Walid</keyname><forenames>Anwar</forenames></author><author><keyname>Tang</keyname><forenames>Ao</forenames></author></authors><title>Scalable Routing in SDN-enabled Networks with Consolidated Middleboxes</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Middleboxes are special network devices that perform various functions such
as enabling security and efficiency. SDN-based routing approaches in networks
with middleboxes need to address resource constraints, such as memory in the
switches and processing power of middleboxes, and traversal constraint where a
flow must visit the required middleboxes in a specific order. In this work we
propose a solution based on MultiPoint-To-Point Trees (MPTPT) for routing
traffic in SDN-enabled networks with consolidated middleboxes. We show both
theoretically and via simulations that our solution significantly reduces the
number of routing rules in the switches, while guaranteeing optimum throughput
and meeting processing requirements. Additionally, the underlying algorithm has
low complexity making it suitable in dynamic network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01777</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01777</id><created>2015-04-07</created><updated>2015-04-28</updated><authors><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Hong</keyname><forenames>Xia</forenames></author><author><keyname>Mishra</keyname><forenames>Bamdev</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Heterogeneous Tensor Decomposition for Clustering via Manifold
  Optimization</title><categories>cs.CV</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensors or multiarray data are generalizations of matrices. Tensor clustering
has become a very important research topic due to the intrinsically rich
structures in real-world multiarray datasets. Subspace clustering based on
vectorizing multiarray data has been extensively researched. However,
vectorization of tensorial data does not exploit complete structure
information. In this paper, we propose a subspace clustering algorithm without
adopting any vectorization process. Our approach is based on a novel
heterogeneous Tucker decomposition model. In contrast to existing techniques,
we propose a new clustering algorithm that alternates between different modes
of the proposed heterogeneous tensor model. All but the last mode have
closed-form updates. Updating the last mode reduces to optimizing over the
so-called multinomial manifold, for which we investigate second order
Riemannian geometry and propose a trust-region algorithm. Numerical experiments
show that our proposed algorithm compete effectively with state-of-the-art
clustering algorithms that are based on tensor factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01780</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01780</id><created>2015-04-07</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Raz</keyname><forenames>Ran</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>Welfare Maximization with Limited Interaction</title><categories>cs.GT</categories><acm-class>C.2.4; F.2.3; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the study of welfare maximization in unit-demand (matching)
markets, in a distributed information model where agent's valuations are
unknown to the central planner, and therefore communication is required to
determine an efficient allocation. Dobzinski, Nisan and Oren (STOC'14) showed
that if the market size is $n$, then $r$ rounds of interaction (with
logarithmic bandwidth) suffice to obtain an $n^{1/(r+1)}$-approximation to the
optimal social welfare. In particular, this implies that such markets converge
to a stable state (constant approximation) in time logarithmic in the market
size.
  We obtain the first multi-round lower bound for this setup. We show that even
if the allowable per-round bandwidth of each agent is $n^{\epsilon(r)}$, the
approximation ratio of any $r$-round (randomized) protocol is no better than
$\Omega(n^{1/5^{r+1}})$, implying an $\Omega(\log \log n)$ lower bound on the
rate of convergence of the market to equilibrium.
  Our construction and technique may be of interest to round-communication
tradeoffs in the more general setting of combinatorial auctions, for which the
only known lower bound is for simultaneous ($r=1$) protocols [DNO14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01781</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01781</id><created>2015-04-07</created><authors><author><keyname>Mai</keyname><forenames>Tiep</forenames></author><author><keyname>Ajwani</keyname><forenames>Deepak</forenames></author><author><keyname>Sala</keyname><forenames>Alessandra</forenames></author></authors><title>Profiling user activities with minimal traffic traces</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding user behavior is essential to personalize and enrich a user's
online experience. While there are significant benefits to be accrued from the
pursuit of personalized services based on a fine-grained behavioral analysis,
care must be taken to address user privacy concerns. In this paper, we consider
the use of web traces with truncated URLs - each URL is trimmed to only contain
the web domain - for this purpose. While such truncation removes the
fine-grained sensitive information, it also strips the data of many features
that are crucial to the profiling of user activity. We show how to overcome the
severe handicap of lack of crucial features for the purpose of filtering out
the URLs representing a user activity from the noisy network traffic trace
(including advertisement, spam, analytics, webscripts) with high accuracy. This
activity profiling with truncated URLs enables the network operators to provide
personalized services while mitigating privacy concerns by storing and sharing
only truncated traffic traces.
  In order to offset the accuracy loss due to truncation, our statistical
methodology leverages specialized features extracted from a group of
consecutive URLs that represent a micro user action like web click, chat reply,
etc., which we call bursts. These bursts, in turn, are detected by a novel
algorithm which is based on our observed characteristics of the inter-arrival
time of HTTP records. We present an extensive experimental evaluation on a real
dataset of mobile web traces, consisting of more than 130 million records,
representing the browsing activities of 10,000 users over a period of 30 days.
Our results show that the proposed methodology achieves around 90% accuracy in
segregating URLs representing user activities from non-representative URLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01782</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01782</id><created>2015-04-07</created><updated>2015-11-25</updated><authors><author><keyname>Kiani</keyname><forenames>Abbas</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Profit Maximization for Geographical Dispersed Green Data Centers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at maximizing the profit associated with running
geographically dispersed green data centers, which offer multiple classes of
service. To this end, we formulate an optimization framework which relies on
the accuracy of the G/D/1 queue in characterizing the workload distribution,
and taps on the merits of the workload decomposition into green and brown
workload served by green and brown energy resources. Moreover, we take into
account of not only the Service Level Agreements (SLAs) between the data
centers and clients but also different deregulated electricity markets of data
centers located at different regions. We prove the convexity of our
optimization problem and the performance of the proposed workload distribution
strategy is evaluated via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01783</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01783</id><created>2015-04-07</created><authors><author><keyname>Bento</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Derbinsky</keyname><forenames>Nate</forenames></author><author><keyname>Mathy</keyname><forenames>Charles</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan S.</forenames></author></authors><title>Proximal operators for multi-agent path planning</title><categories>cs.RO cs.AI math.OC</categories><comments>See movie at http://youtu.be/gRnsjd_ocxs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of planning collision-free paths for multiple agents
using optimization methods known as proximal algorithms. Recently this approach
was explored in Bento et al. 2013, which demonstrated its ease of
parallelization and decentralization, the speed with which the algorithms
generate good quality solutions, and its ability to incorporate different
proximal operators, each ensuring that paths satisfy a desired property.
Unfortunately, the operators derived only apply to paths in 2D and require that
any intermediate waypoints we might want agents to follow be preassigned to
specific agents, limiting their range of applicability. In this paper we
resolve these limitations. We introduce new operators to deal with agents
moving in arbitrary dimensions that are faster to compute than their 2D
predecessors and we introduce landmarks, space-time positions that are
automatically assigned to the set of agents under different optimality
criteria. Finally, we report the performance of the new operators in several
numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01786</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01786</id><created>2015-04-07</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Erban</keyname><forenames>Radek</forenames></author></authors><title>ADM-CLE approach for detecting slow variables in continuous time Markov
  chains and dynamic data</title><categories>cs.CE math.NA physics.chem-ph q-bio.QM</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for detecting intrinsic slow variables in high-dimensional
stochastic chemical reaction networks is developed and analyzed. It combines
anisotropic diffusion maps (ADM) with approximations based on the chemical
Langevin equation (CLE). The resulting approach, called ADM-CLE, has the
potential of being more efficient than the ADM method for a large class of
chemical reaction systems, because it replaces the computationally most
expensive step of ADM (running local short bursts of simulations) by using an
approximation based on the CLE. The ADM-CLE approach can be used to estimate
the stationary distribution of the detected slow variable, without any a-priori
knowledge of it. If the conditional distribution of the fast variables can be
obtained analytically, then the resulting ADM-CLE approach does not make any
use of Monte Carlo simulations to estimate the distributions of both slow and
fast variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01789</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01789</id><created>2015-04-07</created><authors><author><keyname>Areces</keyname><forenames>Carlos</forenames></author><author><keyname>Campercholi</keyname><forenames>Miguel</forenames></author><author><keyname>Penazzi</keyname><forenames>Daniel</forenames></author><author><keyname>Terraf</keyname><forenames>Pedro S&#xe1;nchez</forenames></author></authors><title>The Lattice of Congruences of a Finite Linear Frame</title><categories>math.LO cs.LO</categories><comments>28 pages, 11 figures</comments><msc-class>03B45 (Primary), 06B10, 06E25, 03B70 (Secondary)</msc-class><acm-class>F.4.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbf{F}=\left\langle F,R\right\rangle $ be a finite Kripke frame. A
congruence of $\mathbf{F}$ is a bisimulation of $\mathbf{F}$ that is also an
equivalence relation on F. The set of all congruences of $\mathbf{F}$ is a
lattice under the inclusion ordering. In this article we investigate this
lattice in the case that $\mathbf{F}$ is a finite linear frame. We give
concrete descriptions of the join and meet of two congruences with a nontrivial
upper bound. Through these descriptions we show that for every nontrivial
congruence $\rho$, the interval $[\mathrm{Id_{F},\rho]}$ embeds into the
lattice of divisors of a suitable positive integer. We also prove that any two
congruences with a nontrivial upper bound permute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01799</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01799</id><created>2015-04-07</created><updated>2015-04-14</updated><authors><author><keyname>Hamza</keyname><forenames>A. Ben</forenames></author></authors><title>Spectral Graph Theoretic Analysis of Tsallis Entropy-based Dissimilarity
  Measure</title><categories>cs.IT math.IT</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a nonextensive quantum information theoretic
measure which may be defined between any arbitrary number of density matrices,
and we analyze its fundamental properties in the spectral graph-theoretic
framework. Unlike other entropic measures, the proposed quantum divergence is
symmetric, matrix-convex, theoretically upper-bounded, and has the advantage of
being generalizable to any arbitrary number of density matrices, with a
possibility of assigning weights to these densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01800</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01800</id><created>2015-04-07</created><updated>2015-04-14</updated><authors><author><keyname>Khader</keyname><forenames>Mohammed</forenames></author><author><keyname>Hamza</keyname><forenames>A. Ben</forenames></author></authors><title>A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor
  Images</title><categories>cs.CV</categories><comments>19 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonrigid registration approach for diffusion tensor images using
a multicomponent information-theoretic measure. Explicit orientation
optimization is enabled by incorporating tensor reorientation, which is
necessary for wrapping diffusion tensor images. Experimental results on
diffusion tensor images indicate the feasibility of the proposed approach and a
much better performance compared to the affine registration method based on
mutual information in terms of registration accuracy in the presence of
geometric distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01802</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01802</id><created>2015-04-07</created><authors><author><keyname>Hashemi</keyname><forenames>Morteza</forenames></author><author><keyname>Cassuto</keyname><forenames>Yuval</forenames></author><author><keyname>Trachtenberg</keyname><forenames>Ari</forenames></author></authors><title>Fountain Codes with Nonuniform Selection Distributions through Feedback</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One key requirement for fountain (rateless) coding schemes is to achieve a
high intermediate symbol recovery rate. Recent coding schemes have incorporated
the use of a feedback channel to improve intermediate performance of
traditional rateless codes; however, these codes with feedback are designed
based on uniformly at random selection of input symbols. In this paper, on the
other hand, we develop feedback-based fountain codes with dynamically-adjusted
nonuniform symbol selection distributions, and show that this characteristic
can enhance the intermediate decoding rate. We provide an analysis of our
codes, including bounds on computational complexity and failure probability for
a maximum likelihood decoder; the latter are tighter than bounds known for
classical rateless codes. Through numerical simulations, we also show that
feedback information paired with a nonuniform selection distribution can highly
improve the symbol recovery rate, and that the amount of feedback sent can be
tuned to the specific transmission properties of a given feedback channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01806</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01806</id><created>2015-04-07</created><authors><author><keyname>Wang</keyname><forenames>Boyue</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Kernelized Low Rank Representation on Grassmann Manifolds</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank representation (LRR) has recently attracted great interest due to
its pleasing efficacy in exploring low-dimensional subspace structures embedded
in data. One of its successful applications is subspace clustering which means
data are clustered according to the subspaces they belong to. In this paper, at
a higher level, we intend to cluster subspaces into classes of subspaces. This
is naturally described as a clustering problem on Grassmann manifold. The
novelty of this paper is to generalize LRR on Euclidean space onto an LRR model
on Grassmann manifold in a uniform kernelized framework. The new methods have
many applications in computer vision tasks. Several clustering experiments are
conducted on handwritten digit images, dynamic textures, human face clips and
traffic scene sequences. The experimental results show that the proposed
methods outperform a number of state-of-the-art subspace clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01807</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01807</id><created>2015-04-07</created><authors><author><keyname>Wang</keyname><forenames>Boyue</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision algorithms employ subspace models to represent data. The
Low-rank representation (LRR) has been successfully applied in subspace
clustering for which data are clustered according to their subspace structures.
The possibility of extending LRR on Grassmann manifold is explored in this
paper. Rather than directly embedding Grassmann manifold into a symmetric
matrix space, an extrinsic view is taken by building the self-representation of
LRR over the tangent space of each Grassmannian point. A new algorithm for
solving the proposed Grassmannian LRR model is designed and implemented.
Several clustering experiments are conducted on handwritten digits dataset,
dynamic texture video clips and YouTube celebrity face video data. The
experimental results show our method outperforms a number of existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01809</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01809</id><created>2015-04-07</created><authors><author><keyname>Liu</keyname><forenames>Lanchao</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Multi-Block ADMM for Big Data Optimization in Modern Communication
  Networks</title><categories>cs.NA cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review the parallel and distributed optimization algorithms
based on the alternating direction method of multipliers (ADMM) for solving
&quot;big data&quot; optimization problems in modern communication networks. We first
introduce the canonical formulation of the large-scale optimization problem.
Next, we describe the general form of ADMM and then focus on several direct
extensions and sophisticated modifications of ADMM from $2$-block to $N$-block
settings to deal with the optimization problem. The iterative schemes and
convergence properties of each extension/modification are given, and the
implementation on large-scale computing facilities is also illustrated.
Finally, we numerate several applications in communication networks, such as
the security constrained optimal power flow problem in smart grid networks and
mobile data offloading problem in software defined networks (SDNs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01826</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01826</id><created>2015-04-08</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Dynamic Power Control for Delay-Aware Device-to-Device Communications</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.07966</comments><journal-ref>IEEE Journal of Selected Areas in Communications, vol. 33, no. 1,
  pp. 14 - 27, Jan. 2015</journal-ref><doi>10.1109/JSAC.2014.2369614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the dynamic power control for delay-aware D2D
communications. The stochastic optimization problem is formulated as an
infinite horizon average cost Markov decision process. To deal with the curse
of dimensionality, we utilize the interference filtering property of the
CSMA-like MAC protocol and derive a closed-form approximate priority function
and the associated error bound using perturbation analysis. Based on the
closed-form approximate priority function, we propose a low-complexity power
control algorithm solving the per-stage optimization problem. The proposed
solution is further shown to be asymptotically optimal for a sufficiently large
carrier sensing distance. Finally, the proposed power control scheme is
compared with various baselines through simulations, and it is shown that
significant performance gain can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01828</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01828</id><created>2015-04-08</created><authors><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Menzel</keyname><forenames>Michael</forenames></author><author><keyname>Nepal</keyname><forenames>Surya</forenames></author><author><keyname>Strazdins</keyname><forenames>Peter</forenames></author><author><keyname>Wang</keyname><forenames>Lizhe</forenames></author></authors><title>A Cloud Infrastructure Service Recommendation System for Optimizing
  Real-time QoS Provisioning Constraints</title><categories>cs.DC</categories><comments>IEEE Systems Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proliferation of cloud computing has revolutionized hosting and delivery of
Internet-based application services. However, with the constant launch of new
cloud services and capabilities almost every month by both big (e.g., Amazon
Web Service, Microsoft Azure) and small companies (e.g. Rackspace, Ninefold),
decision makers (e.g. application developers, CIOs) are likely to be
overwhelmed by choices available. The decision making problem is further
complicated due to heterogeneous service configurations and application
provisioning Quality of Service (QoS) constraints. To address this hard
challenge, in our previous work we developed a semi-automated, extensible, and
ontology-based approach to infrastructure service discovery and selection based
on only design time constraints (e.g., renting cost, datacentre location,
service feature, etc.). In this paper, we extend our approach to include the
real-time (run-time) QoS (endto- end message latency, end-to-end message
throughput) in the decision making process. Hosting of next generation
applications in domain of on-line interactive gaming, large scale sensor
analytics, and real-time mobile applications on cloud services necessitates
optimization of such real-time QoS constraints for meeting Service Level
Agreements (SLAs). To this end, we present a real-time QoS aware multi-criteria
decision making technique that builds over well known Analytics Hierarchy
Process (AHP) method. The proposed technique is applicable to selecting
Infrastructure as a Service (IaaS) cloud offers, and it allows users to define
multiple design-time and real-time QoS constraints or requirements. These
requirements are then matched against our knowledge base to compute possible
best fit combinations of cloud services at IaaS layer. We conducted extensive
experiments to prove the feasibility of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01836</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01836</id><created>2015-04-08</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Gr&#xf8;nlund</keyname><forenames>Allan</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author></authors><title>New Unconditional Hardness Results for Dynamic and Online Problems</title><categories>cs.DS cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a resurgence of interest in lower bounds whose truth rests on
the conjectured hardness of well known computational problems. These
conditional lower bounds have become important and popular due to the painfully
slow progress on proving strong unconditional lower bounds. Nevertheless, the
long term goal is to replace these conditional bounds with unconditional ones.
In this paper we make progress in this direction by studying the cell probe
complexity of two conjectured to be hard problems of particular importance:
matrix-vector multiplication and a version of dynamic set disjointness known as
Patrascu's Multiphase Problem. We give improved unconditional lower bounds for
these problems as well as introducing new proof techniques of independent
interest. These include a technique capable of proving strong threshold lower
bounds of the following form: If we insist on having a very fast query time,
then the update time has to be slow enough to compute a lookup table with the
answer to every possible query. This is the first time a lower bound of this
type has been proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01840</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01840</id><created>2015-04-08</created><authors><author><keyname>Tkachenko</keyname><forenames>Yegor</forenames></author></authors><title>Autonomous CRM Control via CLV Approximation with Deep Reinforcement
  Learning in Discrete and Continuous Action Space</title><categories>cs.LG</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper outlines a framework for autonomous control of a CRM (customer
relationship management) system. First, it explores how a modified version of
the widely accepted Recency-Frequency-Monetary Value system of metrics can be
used to define the state space of clients or donors. Second, it describes a
procedure to determine the optimal direct marketing action in discrete and
continuous action space for the given individual, based on his position in the
state space. The procedure involves the use of model-free Q-learning to train a
deep neural network that relates a client's position in the state space to
rewards associated with possible marketing actions. The estimated value
function over the client state space can be interpreted as customer lifetime
value, and thus allows for a quick plug-in estimation of CLV for a given
client. Experimental results are presented, based on KDD Cup 1998 mailing
dataset of donation solicitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01842</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01842</id><created>2015-04-08</created><updated>2015-04-16</updated><authors><author><keyname>Gunadi</keyname><forenames>Hendra</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames></author></authors><title>Formal Certification of Android Bytecode</title><categories>cs.PL</categories><comments>12 pages content, 51 pages total including Appendices, double-column
  IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android is an operating system that has been used in a majority of mobile
devices. Each application in Android runs in an instance of the Dalvik virtual
machine, which is a register-based virtual machine (VM). Most applications for
Android are developed using Java, compiled to Java bytecode and then translated
to DEX bytecode using the dx tool in the Android SDK. In this work, we aim to
develop a type-based method for certifying non-interference properties of DEX
bytecode, following a methodology that has been developed for Java bytecode
certification by Barthe et al. To this end, we develop a formal operational
semantics of the Dalvik VM, a type system for DEX bytecode, and prove the
soundness of the type system with respect to a notion of non-interference. We
then study the translation process from Java bytecode to DEX bytecode, as
implemented in the dx tool in the Android SDK. We show that an abstracted
version of the translation from Java bytecode to DEX bytecode preserves the
non-interference property. More precisely, we show that if the Java bytecode is
typable in Barthe et al's type system (which guarantees non-interference) then
its translation is typable in our type system. This result opens up the
possibility to leverage existing bytecode verifiers for Java to certify
non-interference properties of Android bytecode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01855</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01855</id><created>2015-04-08</created><authors><author><keyname>Balaji</keyname><forenames>R. D.</forenames></author><author><keyname>Veeramani</keyname><forenames>V.</forenames></author><author><keyname>Balaji</keyname><forenames>Malathi</forenames></author></authors><title>Math Marvel with M-Learning</title><categories>cs.CY</categories><comments>7 pages, ROMIST 2015, Oman</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Math is the backbone of any field. Still its a night mare for many. Recent
survey proves that many students become dropouts from their higher education
due to math courses. ICT is an enchanted word in the contemporary educational
environment. It made the learning process more entertaining and almost made the
knowledge loss negligible. Adopting the ICT in math courses are still in the
infant level. Hence its a challenge placed in front of the IT and academic
professionals teaching math to make a suitable ICT tools for math courses to
make the learning an amusing experience. In this paper we have highlighted
three main concepts which make the math classes in a fascinating way. The first
method is introducing revolutionary hybrid ebooks which make the reading with
both audio and video facilities. The second method is facilitating the flip
class room so that student may have anywhere-anytime learning experience.
Finally the recent trends in m-learning are using apps for math courses. We
have highlighted the improvement showed by the students based on the survey
conducted with two groups of students with m-learning tools and without
m-learning tools. Even though there are good improvements showed by the
students, we the researchers feel that few more improvements are required in
these methodologies. The suggestions for the same are made in the
recommendations and conclusion section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01861</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01861</id><created>2015-04-08</created><authors><author><keyname>Balaji</keyname><forenames>R. D.</forenames></author><author><keyname>Al-Mahri</keyname><forenames>Fatma</forenames></author><author><keyname>Al-Fatnaasi</keyname><forenames>Tarek</forenames></author></authors><title>Social Impact of MOOC's in Oman Higher Education</title><categories>cs.CY</categories><comments>ICOET 2015, Oman</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word E transformed everything is this world, as well as the whole globe
itself. To a great extend this helps for eco friendly green world. In
educational field, electronic medium has played a major role. It influenced and
changed almost every component of it to electronic medium like e-book, online
courses, etc. Throughout the world, leading universities are offering online
courses voluntarily. Generally we refer to it as Massive Online Open Courses
(MOOCs). There are many debates going on related to success and consequences of
MOOCs. Many are highlighting that these courses are self-paced, economical, and
provide quality training to all irrespective of geographical constraints. But
many other academic people go against these points and keep listing many other
disadvantages of MOOCs. This paper explores the basics of MOOCs at the initial
section. Following section will deal with advantages and disadvantages of MOOCs
in general. We the researchers collected the details about the awareness of
MOOCs among teachers and students in a higher education institution in Oman. We
have also collected the details about MOOCs implementation and usage within
Oman educational society. Based on the collected information, we have evaluated
and presented the findings about MOOCs impact in Oman higher education. We have
felt that doing appropriate improvements in MOOCs may become an imperative
medium in Oman educational institutions. The suggestions are listed in the
discussion and recommendation section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01873</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01873</id><created>2015-04-08</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Wang</keyname><forenames>Shanshan</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Z.</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>Location, location, location: Border effects in interference limited ad
  hoc networks</title><categories>cs.NI cs.IT math.IT</categories><comments>8 pages, 5 figures, conference proceedings of SPASWIN'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks are fundamentally limited by the intensity of the received
signals and by their inherent interference. It is shown here that in finite ad
hoc networks where node placement is modelled according to a Poisson point
process and no carrier sensing is employed for medium access, the SINR received
by nodes located at the border of the network deployment/operation region is on
average greater than the rest. This is primarily due to the uneven interference
landscape of such networks which is particularly kind to border nodes giving
rise to all sorts of performance inhomogeneities and access unfairness. Using
tools from stochastic geometry we quantify these spatial variations and provide
closed form communication-theoretic results showing why the receiver's location
is so important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01877</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01877</id><created>2015-04-08</created><updated>2015-05-04</updated><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy R.</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author></authors><title>Social media in scholarly communication</title><categories>cs.DL</categories><comments>Guest Editorial to the special issue &quot;Social Media Metrics in
  Scholarly Communication: Exploring Tweets, Blogs, Likes and other Altmetrics&quot;
  in Aslib Journal of Information Management 67(3)</comments><journal-ref>Aslib Journal of Information Management 67(3) (2015)</journal-ref><doi>10.1108/AJIM-03-2015-0047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media metrics - commonly coined as &quot;altmetrics&quot; - have been heralded
as great democratizers of science, providing broader and timelier indicators of
impact than citations. These metrics come from a range of sources, including
Twitter, blogs, social reference managers, post-publication peer review, and
other social media platforms. Social media metrics have begun to be used as
indicators of scientific impact, yet the theoretical foundation, empirical
validity, and extent of use of platforms underlying these metrics lack thorough
treatment in the literature. This editorial provides an overview of terminology
and definitions of altmetrics and summarizes current research regarding social
media use in academia, social media metrics as well as data reliability and
validity. The papers of the special issue are introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01879</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01879</id><created>2015-04-08</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Nguyen</keyname><forenames>Camly</forenames></author></authors><title>Multihop connectivity of ad hoc networks with randomly oriented
  directional antennas</title><categories>cs.NI cs.IT math.IT</categories><comments>4 pages, 3 figures, Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directional antennas and beamforming can significantly improve point-to-point
wireless links when perfectly aligned. In this letter we investigate the
extreme opposite where antenna orientations and positions are chosen at random
in the presence of Rayleigh fading. We show that while the 1-hop network
connectivity is deteriorated, the multihop routes improve, especially in the
dense regime. We derive closed form expressions for the expectation of the
$1$-hop and $2$-hop degree which are verified through computer simulations. We
conclude that node density does not greatly affect the number of hops required
between stations whilst simple random beamforming schemes do, thus returning
substantial network performance benefits due to the existence of shorter
multi-hop paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01883</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01883</id><created>2015-04-08</created><authors><author><keyname>Naik</keyname><forenames>Narmada</forenames></author><author><keyname>Rathna</keyname><forenames>G. N</forenames></author></authors><title>Robust real time face recognition and tracking on gpu using fusion of
  rgb and depth image</title><categories>cs.CV</categories><doi>10.5121/csit.2015.50601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a real-time face recognition system using kinect sensor.
The algorithm is implemented on GPU using opencl and significant speed
improvements are observed. We use kinect depth image to increase the robustness
and reduce computational cost of conventional LBP based face recognition. The
main objective of this paper was to perform robust, high speed fusion based
face recognition and tracking. The algorithm is mainly composed of three steps.
First step is to detect all faces in the video using viola jones algorithm. The
second step is online database generation using a tracking window on the face.
A modified LBP feature vector is calculated using fusion information from depth
and greyscale image on GPU. This feature vector is used to train a svm
classifier. Third step involves recognition of multiple faces based on our
modified feature vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01887</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01887</id><created>2015-04-08</created><authors><author><keyname>Tavassoli</keyname><forenames>Babak</forenames></author></authors><title>Design and Evaluation of Distributed Networked Control for a
  Dual-Machine Power System</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oscillations between swing modes of electric machines is an important
limitation in achieving a high level of transient performance and reliability
in power grids. Based on the new advances in measurement and transmission of
wide-area information, this work proposes a distributed networked control
scheme by considering the communication delays. The results are applied to
reduce the inter-area swing oscillations in a power grid. In comparison with
the previous works, we provide a more realistic modeling of the resulting
networked control system with data sampling and delays. The exactness of the
proposed modeling allows for precise evaluation and comparison between the
distributed and decentralized schema. A symmetric a dual machine power system
is highly oscillatory and we focus on this case to evaluate the ability of the
proposed control design in dampening of the oscillations. The design can be
done either based on optimization of a quadratic cost function or a disturbance
attenuation level
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01891</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01891</id><created>2015-04-08</created><updated>2015-09-11</updated><authors><author><keyname>Meimaris</keyname><forenames>Marios</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Viglas</keyname><forenames>Stratis</forenames></author><author><keyname>Stavrakas</keyname><forenames>Yannis</forenames></author><author><keyname>Pateritsas</keyname><forenames>Christos</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>A Query Language for Multi-version Data Web Archives</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Data Web refers to the vast and rapidly increasing quantity of
scientific, corporate, government and crowd-sourced data published in the form
of Linked Open Data, which encourages the uniform representation of
heterogeneous data items on the web and the creation of links between them. The
growing availability of open linked datasets has brought forth significant new
challenges regarding their proper preservation and the management of evolving
information within them. In this paper, we focus on the evolution and
preservation challenges related to publishing and preserving evolving linked
data across time. We discuss the main problems regarding their proper modelling
and querying and provide a conceptual model and a query language for modelling
and retrieving evolving data along with changes affecting them. We present in
details the syntax of the query language and demonstrate its functionality over
a real-world use case of evolving linked dataset from the biological domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01920</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01920</id><created>2015-04-08</created><authors><author><keyname>Ye</keyname><forenames>Hao</forenames></author><author><keyname>Wu</keyname><forenames>Zuxuan</forenames></author><author><keyname>Zhao</keyname><forenames>Rui-Wei</forenames></author><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Jiang</keyname><forenames>Yu-Gang</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author></authors><title>Evaluating Two-Stream CNN for Video Classification</title><categories>cs.CV</categories><comments>ACM ICMR'15</comments><doi>10.1145/2671188.2749406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Videos contain very rich semantic information. Traditional hand-crafted
features are known to be inadequate in analyzing complex video semantics.
Inspired by the huge success of the deep learning methods in analyzing image,
audio and text data, significant efforts are recently being devoted to the
design of deep nets for video analytics. Among the many practical needs,
classifying videos (or video clips) based on their major semantic categories
(e.g., &quot;skiing&quot;) is useful in many applications. In this paper, we conduct an
in-depth study to investigate important implementation options that may affect
the performance of deep nets on video classification. Our evaluations are
conducted on top of a recent two-stream convolutional neural network (CNN)
pipeline, which uses both static frames and motion optical flows, and has
demonstrated competitive performance against the state-of-the-art methods. In
order to gain insights and to arrive at a practical guideline, many important
options are studied, including network architectures, model fusion, learning
parameters and the final prediction methods. Based on the evaluations, very
competitive results are attained on two popular video classification
benchmarks. We hope that the discussions and conclusions from this work can
help researchers in related fields to quickly set up a good basis for further
investigations along this very promising direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01927</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01927</id><created>2015-04-08</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Harmer</keyname><forenames>Russ</forenames></author></authors><title>Proceedings Tenth International Workshop on Developments in
  Computational Models</title><categories>cs.LO cs.FL cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 179, 2015</journal-ref><doi>10.4204/EPTCS.179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the Tenth International Workshop
on Developments in Computational Models (DCM) held in Vienna, Austria on 13th
July 2014, as part of the Vienna Summer of Logic.
  Several new models of computation have emerged in the last years, and many
developments of traditional computational models have been proposed with the
aim of taking into account the new demands of computer systems users and the
new capabilities of computation engines. A new computational model, or a new
feature in a traditional one, usually is reflected in a new family of
programming languages, and new paradigms of software development.
  The aim of this workshop is to bring together researchers who are currently
developing new computational models or new features for traditional
computational models, in order to foster their interaction, to provide a forum
for presenting new ideas and work in progress, and to enable newcomers to learn
about current activities in this area. Topics of interest include all abstract
models of computation and their applications to the development of programming
languages and systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01928</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01928</id><created>2015-04-08</created><authors><author><keyname>Pakulin</keyname><forenames>Nikolay</forenames><affiliation>Institute for System Programming Russian Academy of Sciences</affiliation></author><author><keyname>Petrenko</keyname><forenames>Alexander K.</forenames><affiliation>Institute for System Programming Russian Academy of Sciences</affiliation></author><author><keyname>Schlingloff</keyname><forenames>Bernd-Holger</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin, Institut f&#xfc;r Informatik</affiliation></author></authors><title>Proceedings Tenth Workshop on Model Based Testing</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015</journal-ref><doi>10.4204/EPTCS.180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workshop is devoted to model-based testing of both software and hardware.
Model-based testing uses models describing the required behavior of the system
under consideration to guide such efforts as test selection and test results
evaluation. Testing validates the real system behavior against models and
checks that the implementation conforms to them, but is capable also to find
errors in the models themselves.
  The intent of this workshop is to bring together researchers and users of
model-based testing techniques and tools to discuss the state of the art in
theory, applications, tools, and industrialization of model-based testing and
related domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01934</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01934</id><created>2015-04-08</created><authors><author><keyname>Thakur</keyname><forenames>Gaurav Singh</forenames></author><author><keyname>Gupta</keyname><forenames>Anubhav</forenames></author><author><keyname>Gupta</keyname><forenames>Sangita</forenames></author></authors><title>Data Mining for Prediction of Human Performance Capability in the
  Software-Industry</title><categories>cs.LG</categories><comments>Data Mining for Prediction of Human Performance Capability in the
  Software-Industry, International Journal of Data-Mining and Knowledge
  Management Process (IJDKP) - March 2015 Issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recruitment of new personnel is one of the most essential business
processes which affect the quality of human capital within any company. It is
highly essential for the companies to ensure the recruitment of right talent to
maintain a competitive edge over the others in the market. However IT companies
often face a problem while recruiting new people for their ongoing projects due
to lack of a proper framework that defines a criteria for the selection
process. In this paper we aim to develop a framework that would allow any
project manager to take the right decision for selecting new talent by
correlating performance parameters with the other domain-specific attributes of
the candidates. Also, another important motivation behind this project is to
check the validity of the selection procedure often followed by various big
companies in both public and private sectors which focus only on academic
scores, GPA/grades of students from colleges and other academic backgrounds. We
test if such a decision will produce optimal results in the industry or is
there a need for change that offers a more holistic approach to recruitment of
new talent in the software companies. The scope of this work extends beyond the
IT domain and a similar procedure can be adopted to develop a recruitment
framework in other fields as well. Data-mining techniques provide useful
information from the historical projects depending on which the hiring-manager
can make decisions for recruiting high-quality workforce. This study aims to
bridge this hiatus by developing a data-mining framework based on an
ensemble-learning technique to refocus on the criteria for personnel selection.
The results from this research clearly demonstrated that there is a need to
refocus on the selection-criteria for quality objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01942</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01942</id><created>2015-04-08</created><authors><author><keyname>Leal-Taix&#xe9;</keyname><forenames>Laura</forenames></author><author><keyname>Milan</keyname><forenames>Anton</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author><author><keyname>Roth</keyname><forenames>Stefan</forenames></author><author><keyname>Schindler</keyname><forenames>Konrad</forenames></author></authors><title>MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, the computer vision community has developed centralized
benchmarks for the performance evaluation of a variety of tasks, including
generic object and pedestrian detection, 3D reconstruction, optical flow,
single-object short-term tracking, and stereo estimation. Despite potential
pitfalls of such benchmarks, they have proved to be extremely helpful to
advance the state of the art in the respective area. Interestingly, there has
been rather limited work on the standardization of quantitative benchmarks for
multiple target tracking. One of the few exceptions is the well-known PETS
dataset, targeted primarily at surveillance applications. Despite being widely
used, it is often applied inconsistently, for example involving using different
subsets of the available data, different ways of training the models, or
differing evaluation scripts. This paper describes our work toward a novel
multiple object tracking benchmark aimed to address such issues. We discuss the
challenges of creating such a framework, collecting existing and new data,
gathering state-of-the-art methods to be tested on the datasets, and finally
creating a unified evaluation system. With MOTChallenge we aim to pave the way
toward a unified evaluation framework for a more meaningful quantification of
multi-target tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01949</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01949</id><created>2015-04-08</created><authors><author><keyname>Dross</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Montassier</keyname><forenames>Mickael</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>A lower bound on the order of the largest induced forest in planar
  graphs with high girth</title><categories>cs.DM math.CO</categories><comments>12 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1409.1348</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give here new upper bounds on the size of a smallest feedback vertex set
in planar graphs with high girth. In particular, we prove that a planar graph
with girth $g$ and size $m$ has a feedback vertex set of size at most
$\frac{4m}{3g}$, improving the trivial bound of $\frac{2m}{g}$. We also prove
that every $2$-connected graph with maximum degree $3$ and order $n$ has a
feedback vertex set of size at most $\frac{n+2}{3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01954</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01954</id><created>2015-04-08</created><authors><author><keyname>Ali</keyname><forenames>Heider K.</forenames></author><author><keyname>Whitehead</keyname><forenames>Anthony</forenames></author></authors><title>Image Subset Selection Using Gabor Filters and Neural Networks</title><categories>cs.CV</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automatic method for the selection of subsets of images, both modern and
historic, out of a set of landmark large images collected from the Internet is
presented in this paper. This selection depends on the extraction of dominant
features using Gabor filtering. Features are selected carefully from a
preliminary image set and fed into a neural network as a training data. The
method collects a large set of raw landmark images containing modern and
historic landmark images and non-landmark images. The method then processes
these images to classify them as landmark and non-landmark images. The
classification performance highly depends on the number of candidate features
of the landmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01957</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01957</id><created>2015-04-08</created><authors><author><keyname>Tsang</keyname><forenames>Tony</forenames></author></authors><title>Video Contents Prior Storing Server for Optical Access Network</title><categories>cs.NI cs.MM</categories><comments>10 pages. in March 2015, Volume 7. Number 1 International Journal of
  Computer Networks &amp; Communications (IJCNC) March 2015, Volume 7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important multimedia applications is Internet protocol TV
(IPTV) for next-generation networks. IPTV provides triple-play services that
require high-speed access networks with the functions of multicasting and
quality of service (QoS) guarantees. Among optical access networks, Ethernet
passive optical networks (EPONs) are regarded as among the best solutions to
meet higher bandwidth demands. In this paper, we propose a new architecture for
multicasting live IPTV traffic in optical access network. The proposed
mechanism involves assigning a unique logical link identifier to each IPTV
channel. To manage multicasting, a prior storing server in the optical line
terminal (OLT) and in each optical network unit (ONU) is constructed. In this
work, we propose a partial prior storing strategy that considers the changes in
the popularity of the video content segments over time and the access patterns
of the users to compute the utility of the objects in the prior storage. We
also propose to partition the prior storage to avoid the eviction of the
popular objects (those not accessed frequently) by the unpopular ones which are
accessed with higher frequency. The popularity distribution and ageing of
popularity are measured from two online datasets and use the parameters in
simulations. Simulation results show that our proposed architecture can improve
the system performance and QoS parameters in terms of packet delay, jitter and
packet loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01974</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01974</id><created>2015-04-08</created><updated>2015-07-13</updated><authors><author><keyname>Maitra</keyname><forenames>Arpita</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author><author><keyname>Pal</keyname><forenames>Asim K.</forenames></author></authors><title>Secure Computation Excluding Embedded XOR with Rational Players: a
  Unified Approach in Classical and Quantum Paradigms</title><categories>cs.CR quant-ph</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A seminal result of Cleve (STOC 1986) showed that fairness, in general, is
impossible to achieve in case of two-party computation if one of them is
malicious. Ben-Or et al. and Chaum et al. (STOC 1988) showed that absolute
correctness can be achieved in case of multiparty computation when one third
players are faulty. However, they analyzed the problem in broadcasting channel
model. In non-simultaneous channel model, Gordon et al. (STOC 2008) observed
that there exist some functions for which fairness can be achieved even though
one of the two parties is malicious. One of the functions considered by Gordon
et al. is exactly the millionaires' problem (Yao, FOCS 1982) or, equivalently,
the `greater than' function. The problem deals with two millionaires, Alice and
Bob, who are interested in finding who amongst them is richer, without
revealing their actual wealth to each other. We, for the first time, study this
problem in presence of rational players. In particular, we show that Gordon's
protocol no longer remains fair when the players are rational. Next, we design
a protocol with rational players, that not only achieves fairness, but also
achieves correctness and strict Nash equilibrium for natural utilities. Gordon
et al. (JACM, 2011) showed that any function over polynomial-size domains which
does not contain an &quot;embedded XOR&quot; can be converted into the greater than
function. Thus, the proposed protocol is applicable for any function except
embedded XOR.
  We, also for the first time, provide a solution to the quantum version of
millionaires' problem with rational players, that achieves fairness,
correctness and strict Nash equilibrium. Both our classical and quantum
protocols follow a unified approach; both use a rational third party and get
rid of the requirement of the online dealer of Groce et al. (EUROCRYPT 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01982</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01982</id><created>2015-04-08</created><authors><author><keyname>Fernandez-Bes</keyname><forenames>Jesus</forenames></author><author><keyname>Arenas-Garc&#xed;a</keyname><forenames>Jer&#xf3;nimo</forenames></author><author><keyname>Silva</keyname><forenames>Magno T. M.</forenames></author><author><keyname>Azpicueta-Ruiz</keyname><forenames>Luis A.</forenames></author></authors><title>Decoupled Adapt-then-Combine diffusion networks with adaptive combiners</title><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a novel diffusion strategy for adaptive networks
called Decoupled Adapt-then-Combine, which keeps a fully local estimate of the
solution for the adaptation step. Our strategy, which is specially convenient
for heterogeneous networks, is compared with the standard Adapt-then-Combine
scheme and theoretically analyzed using energy conservation arguments. Such
comparison shows the need of implementing adaptive combiners for both schemes
to obtain a good performance in case of heterogeneous networks. Therefore, we
propose two adaptive rules to learn the combination coefficients that are
useful for our diffusion strategy. Several experiments simulating both
stationary estimation and tracking problems show that our method outperforms
state-of-the-art techniques, becoming a competitive approach in different
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01987</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01987</id><created>2015-04-08</created><authors><author><keyname>Raamkumar</keyname><forenames>Aravind Sesagiri</forenames></author><author><keyname>Thangavelu</keyname><forenames>Muthu Kumaar</forenames></author><author><keyname>Khoo</keyname><forenames>Sudarsan Kaleeswaran amd Christopher S. G.</forenames></author></authors><title>Designing a Linked Data Migrational Framework for Singapore Government
  Datasets</title><categories>cs.CY cs.DL</categories><comments>23 pages, 5 figures</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subject area of this report is Linked Data and its application to the
Government domain. Linked Data is an alternative method of data representation
that aims to interlink data from varied sources through relationships.
Governments around the world have started publishing their data in this format
to assist citizens in making better use of public services. This report
provides an eight step migrational framework for converting Singapore
Government data from legacy systems to Linked Data format. The framework
formulation is based on a study of the Singapore data ecosystem with help from
Infocomm Development Authority (iDA) of Singapore. Each step in the migrational
framework has been constructed with objectives, recommendations, best practices
and issues with entry and exit points. This work builds on the existing Linked
Data literature, implementations in other countries and cookbooks provided by
Linked Data researchers. iDA can use this report to gain an understanding of
the effort and work involved in the implementation of Linked Data system on top
of their legacy systems. The framework can be evaluated by building a Proof of
Concept (POC) application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01989</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01989</id><created>2015-04-08</created><authors><author><keyname>Hwang</keyname><forenames>Jyh-Jing</forenames></author><author><keyname>Liu</keyname><forenames>Tyng-Luh</forenames></author></authors><title>Pixel-wise Deep Learning for Contour Detection</title><categories>cs.CV cs.LG cs.NE</categories><comments>2 pages. arXiv admin note: substantial text overlap with
  arXiv:1412.6857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of contour detection via per-pixel classifications of
edge point. To facilitate the process, the proposed approach leverages with
DenseNet, an efficient implementation of multiscale convolutional neural
networks (CNNs), to extract an informative feature vector for each pixel and
uses an SVM classifier to accomplish contour detection. In the experiment of
contour detection, we look into the effectiveness of combining per-pixel
features from different CNN layers and verify their performance on BSDS500.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.01995</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.01995</id><created>2015-04-08</created><updated>2015-09-28</updated><authors><author><keyname>Aggarwal</keyname><forenames>Divesh</forenames></author><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>Stephens-Davidowitz</keyname><forenames>Noah</forenames></author></authors><title>Solving the Closest Vector Problem in $2^n$ Time--- The Discrete
  Gaussian Strikes Again!</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a $2^{n+o(n)}$-time and space randomized algorithm for solving the
exact Closest Vector Problem (CVP) on $n$-dimensional Euclidean lattices. This
improves on the previous fastest algorithm, the deterministic
$\widetilde{O}(4^{n})$-time and $\widetilde{O}(2^{n})$-space algorithm of
Micciancio and Voulgaris.
  We achieve our main result in three steps. First, we show how to modify the
sampling algorithm from [ADRS15] to solve the problem of discrete Gaussian
sampling over lattice shifts, $L- t$, with very low parameters. While the
actual algorithm is a natural generalization of [ADRS15], the analysis uses
substantial new ideas. This yields a $2^{n+o(n)}$-time algorithm for
approximate CVP for any approximation factor $\gamma = 1+2^{-o(n/\log n)}$.
Second, we show that the approximate closest vectors to a target vector $t$ can
be grouped into &quot;lower-dimensional clusters,&quot; and we use this to obtain a
recursive reduction from exact CVP to a variant of approximate CVP that
&quot;behaves well with these clusters.&quot; Third, we show that our discrete Gaussian
sampling algorithm can be used to solve this variant of approximate CVP.
  The analysis depends crucially on some new properties of the discrete
Gaussian distribution and approximate closest vectors, which might be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02001</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02001</id><created>2015-04-08</created><authors><author><keyname>van der Walt</keyname><forenames>Paul</forenames></author></authors><title>Constraining application behaviour by generating languages</title><categories>cs.SE</categories><comments>8 pages, 8th European Lisp Symposium</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Writing a platform for reactive applications which enforces operational
constraints is difficult, and has been approached in various ways. In this
experience report, we detail an approach using an embedded DSL which can be
used to specify the structure and permissions of a program in a given
application domain. Once the developer has specified which components an
application will consist of, and which permissions each one needs, the
specification itself evaluates to a new, tailored, language. The final
implementation of the application is then written in this specialised
environment where precisely the API calls associated with the permissions which
have been granted, are made available.
  Our prototype platform targets the domain of mobile computing, and is
implemented using Racket. It demonstrates resource access control (e.g.,
camera, address book, etc.) and tries to prevent leaking of private data.
Racket is shown to be an extremely effective platform for designing new
programming languages and their run-time libraries. We demonstrate that this
approach allows reuse of an inter-component communication layer, is convenient
for the application developer because it provides high-level building blocks to
structure the application, and provides increased control to the platform
owner, preventing certain classes of errors by the developer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02010</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02010</id><created>2015-04-08</created><authors><author><keyname>Sahai</keyname><forenames>Tuhin</forenames></author><author><keyname>Mathew</keyname><forenames>George</forenames></author><author><keyname>Surana</keyname><forenames>Amit</forenames></author></authors><title>A Chaotic Dynamical System that Paints</title><categories>nlin.CD cs.LG</categories><comments>Four movies have been uploaded in the ancillary folder. They display
  the evolution of the dynamical system based reconstruction of paintings and
  pictures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa or
Monet's Water Lilies? Moreover, can this dynamical system be chaotic in the
sense that although the trajectories are sensitive to initial conditions, the
same painting is created every time? Setting aside the creative aspect of
painting a picture, in this work, we develop a novel algorithm to reproduce
paintings and photographs. Combining ideas from ergodic theory and control
theory, we construct a chaotic dynamical system with predetermined statistical
properties. If one makes the spatial distribution of colors in the picture the
target distribution, akin to a human, the algorithm first captures large scale
features and then goes on to refine small scale features. Beyond reproducing
paintings, this approach is expected to have a wide variety of applications
such as uncertainty quantification, sampling for efficient inference in
scalable machine learning for big data, and developing effective strategies for
search and rescue. In particular, our preliminary studies demonstrate that this
algorithm provides significant acceleration and higher accuracy than competing
methods for Markov Chain Monte Carlo (MCMC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02017</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02017</id><created>2015-04-08</created><authors><author><keyname>Mwangi</keyname><forenames>Karanja Evanson</forenames></author><author><keyname>Thuku</keyname><forenames>Lawrence Xavier</forenames></author><author><keyname>Kangethe</keyname><forenames>John Patrick</forenames></author></authors><title>Software Development Industry In East Africa: Knowledge Management
  Perspective And Value Proposition</title><categories>cs.SE cs.CY</categories><comments>16 pages and five figures,Africa Casebook - Synergies in African
  Business and Management Practices,ISBN 978-9966-1570-0-3. from 1st African
  International Business and Management Conference (AIBUMA) on Knowledge and
  Innovation Leadership for Competitiveness Nairobi, Kenya 25-27th August 2010</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Increased usage of the internet has contributed immensely to the growth of
software development practice in East Africa. This paper investigates the
existence of formal KM (Knowledge Management) initiatives in the Software
industry such as creation of virtual communities (Communities of practice and
communities of interest); expert localization and establishment of knowledge
taxonomies in these communities; the knowledge transfer and sharing processes;
incubation and Mentorship; collaborative software development and their role in
creating entrepreneurship initiatives and providing a building block towards
the knowledge economies. We propose a hybrid framework for use in KM initiative
focusing on Software Development in East Africa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02018</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02018</id><created>2015-04-08</created><authors><author><keyname>Islam</keyname><forenames>Md. Rafiqul</forenames></author><author><keyname>Habib</keyname><forenames>Md. Ahsan</forenames></author></authors><title>A Data Mining Approach to Predict Prospective Business Sectors for
  Lending in Retail Banking Using Decision Tree</title><categories>cs.CY</categories><comments>This paper contains 10 pages, 7 figures, 1 table and 1 algorithm. See
  more from here, http://airccse.org/journal/ijdkp/papers/5215ijdkp02.pdf and
  journal information see here http://airccse.org/journal/ijdkp/ijdkp.html
  http://www.airccse.org http://www.airccj.org</comments><acm-class>H.2.8; H.3.3</acm-class><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP), vol. 5(2), pp. 13-22, 2015</journal-ref><doi>10.5121/ijdkp.2015.5202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A potential objective of every financial organization is to retain existing
customers and attain new prospective customers for long-term. The economic
behaviour of customer and the nature of the organization are controlled by a
prescribed form called Know Your Customer (KYC) in manual banking. Depositor
customers in some sectors (business of Jewellery/Gold, Arms, Money exchanger
etc) are with high risk; whereas in some sectors (Transport Operators,
Auto-delear, religious) are with medium risk; and in remaining sectors (Retail,
Corporate, Service, Farmer etc) belongs to low risk. Presently, credit risk for
counterparty can be broadly categorized under quantitative and qualitative
factors. Although there are many existing systems on customer retention as well
as customer attrition systems in bank, these rigorous methods suffers clear and
defined approach to disburse loan in business sector. In the paper, we have
used records of business customers of a retail commercial bank in the city
including rural and urban area of (Tangail city) Bangladesh to analyse the
major transactional determinants of customers and predicting of a model for
prospective sectors in retail bank. To achieve this, data mining approach is
adopted for analysing the challenging issues, where pruned decision tree
classification technique has been used to develop the model and finally tested
its performance with Weka result. Moreover, this paper attempts to build up a
model to predict prospective business sectors in retail banking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02026</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02026</id><created>2015-04-08</created><authors><author><keyname>Thuku</keyname><forenames>Lawrence Xavier Waweru</forenames></author><author><keyname>Mwangi</keyname><forenames>Karanja Evanson</forenames></author></authors><title>Towards Efficient Service Delivery: The Role Of Workflow Systems In
  Public Sector In Kenya</title><categories>cs.CY</categories><comments>Strathmore University ICT conference 19 pages, the 11th Strathmore
  University ICT Conference , 3rd -4th September 2010, Strathmore University
  Nairobi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current challenges in Electronic Government initiatives include performance
management, effective and efficient means of sharing information between
different stakeholders e.g. government departments in order to improve the
quality of service delivery to the citizens. To address these challenges, this
paper addresses an application domain of workflow systems in public
institutions and proposes a framework that can be effectively used in
implementing workflow systems in public institutions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02027</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02027</id><created>2015-02-05</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>The Neutrosophic Entropy and its Five Components</title><categories>cs.AI</categories><journal-ref>Neutrosophic Sets and Systems, Vol.7, 2015,pp. 40-46</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two variants of penta-valued representation for
neutrosophic entropy. The first is an extension of Kaufmann's formula and the
second is an extension of Kosko's formula.
  Based on the primary three-valued information represented by the degree of
truth, degree of falsity and degree of neutrality there are built some
penta-valued representations that better highlights some specific features of
neutrosophic entropy. Thus, we highlight five features of neutrosophic
uncertainty such as ambiguity, ignorance, contradiction, neutrality and
saturation. These five features are supplemented until a seven partition of
unity by adding two features of neutrosophic certainty such as truth and
falsity.
  The paper also presents the particular forms of neutrosophic entropy obtained
in the case of bifuzzy representations, intuitionistic fuzzy representations,
paraconsistent fuzzy representations and finally the case of fuzzy
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02035</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02035</id><created>2015-04-08</created><authors><author><keyname>Garg</keyname><forenames>Mohit</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author></authors><title>Set Membership with a Few Bit Probes</title><categories>cs.DS</categories><comments>19 pages, expanded version of 'Set membership with a few bit probes.
  SODA 2015: 776-784' (with additional results)</comments><msc-class>68P05, 68P20, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the bit-probe complexity of the set membership problem, where a
set S of size at most n from a universe of size m is to be represented as a
short bit vector in order to answer membership queries of the form &quot;Is x in S?&quot;
by adaptively probing the bit vector at t places. Let s(m,n,t) be the minimum
number of bits of storage needed for such a scheme. Several recent works
investigate s(m,n,t) for various ranges of the parameter; we obtain
improvements over some of the bounds shown by Buhrman, Miltersen,
Radhakrishnan, and Srinivasan (2002) and Alon and Feige (2009).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02044</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02044</id><created>2015-04-08</created><updated>2015-11-17</updated><authors><author><keyname>Harvey</keyname><forenames>Nicholas</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>An Algorithmic Proof of the Lovasz Local Lemma via Resampling Oracles</title><categories>cs.DS math.CO</categories><comments>47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lovasz Local Lemma is a seminal result in probabilistic combinatorics. It
gives a sufficient condition on a probability space and a collection of events
for the existence of an outcome that simultaneously avoids all of those events.
Finding such an outcome by an efficient algorithm has been an active research
topic for decades. Breakthrough work of Moser and Tardos (2009) presented an
efficient algorithm for a general setting primarily characterized by a product
structure on the probability space.
  In this work we present an efficient algorithm for a much more general
setting. Our main assumption is that there exist certain functions, called
resampling oracles, that can be invoked to address the undesired occurrence of
the events. We show that, in all scenarios to which the original Lovasz Local
Lemma applies, there exist resampling oracles, although they are not
necessarily efficient. Nevertheless, for essentially all known applications of
the Lovasz Local Lemma and its generalizations, we have designed efficient
resampling oracles. As applications of these techniques, we present new results
for packings of Latin transversals, rainbow matchings and rainbow spanning
trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02052</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02052</id><created>2015-04-08</created><authors><author><keyname>Georgiadis</keyname><forenames>Leonidas</forenames></author><author><keyname>Iosifidis</keyname><forenames>George</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Exchange of Services in Networks: Competition, Cooperation, and Fairness</title><categories>cs.GT</categories><comments>to appear in ACM Sigmetrics 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchange of services and resources in, or over, networks is attracting
nowadays renewed interest. However, despite the broad applicability and the
extensive study of such models, e.g., in the context of P2P networks, many
fundamental questions regarding their properties and efficiency remain
unanswered. We consider such a service exchange model and analyze the users'
interactions under three different approaches. First, we study a centrally
designed service allocation policy that yields the fair total service each user
should receive based on the service it others to the others. Accordingly, we
consider a competitive market where each user determines selfishly its
allocation policy so as to maximize the service it receives in return, and a
coalitional game model where users are allowed to coordinate their policies. We
prove that there is a unique equilibrium exchange allocation for both game
theoretic formulations, which also coincides with the central fair service
allocation. Furthermore, we characterize its properties in terms of the
coalitions that emerge and the equilibrium allocations, and analyze its
dependency on the underlying network graph. That servicing policy is the
natural reference point to the various mechanisms that are currently proposed
to incentivize user participation and improve the efficiency of such networked
service (or, resource) exchange markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02059</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02059</id><created>2015-04-08</created><authors><author><keyname>Alrefaie</keyname><forenames>Hayat</forenames></author><author><keyname>Ramsay</keyname><forenames>Allan</forenames></author></authors><title>Supporting Language Learners with the Meanings Of Closed Class Items</title><categories>cs.AI cs.CL</categories><comments>12 pages include references, 5 figures and AIAPP 2015 conference in
  Geneva</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of language learning involves the mastery of countless tasks:
making the constituent sounds of the language being learned, learning the
grammatical patterns, and acquiring the requisite vocabulary for reception and
production. While a plethora of computational tools exist to facilitate the
first and second of these tasks, a number of challenges arise with respect to
enabling the third. This paper describes a tool that has been designed to
support language learners with the challenge of understanding the use of
closed-class lexical items. The process of learning the Arabic for office is
(mktb) is relatively simple and should be possible by means of simple
repetition of the word. However, it is much more difficult to learn and
correctly use the Arabic equivalent of the word on. The current paper describes
a mechanism for the delivery of diagnostic information regarding specific
lexical examples, with the aim of clearly demonstrating why a particular
translation of a given closed-class item may be appropriate in certain
situations but not others, thereby helping learners to understand and use the
term correctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02063</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02063</id><created>2015-04-08</created><authors><author><keyname>Pananjady</keyname><forenames>Ashwin</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author></authors><title>Compressing Sparse Sequences under Local Decodability Constraints</title><categories>cs.IT cs.DS math.IT</categories><comments>8 pages, 1 figure. First five pages to appear in 2015 International
  Symposium on Information Theory. This version contains supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variable-length source coding problem subject to local
decodability constraints. In particular, we investigate the blocklength scaling
behavior attainable by encodings of $r$-sparse binary sequences, under the
constraint that any source bit can be correctly decoded upon probing at most
$d$ codeword bits. We consider both adaptive and non-adaptive access models,
and derive upper and lower bounds that often coincide up to constant factors.
Notably, such a characterization for the fixed-blocklength analog of our
problem remains unknown, despite considerable research over the last three
decades. Connections to communication complexity are also briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02072</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02072</id><created>2015-04-08</created><updated>2015-12-05</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author></authors><title>Intractability of Optimal Multi-Robot Path Planning on Planar Graphs</title><categories>cs.RO cs.CC</categories><comments>Updated draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of optimally solving multi-robot path
planning problems on planar graphs. For four common time- and distance-based
objectives, we show that the associated path optimization problems for multiple
robots are all NP-complete, even when the underlying graph is planar.
Establishing the computational intractability of optimal multi-robot path
planning problems on planar graphs has important practical implications. In
particular, our result suggests the preferred approach toward solving such
problems, when the number of robots is large, is to augment the planar
environment to reduce the sharing of paths among robots traveling in opposite
directions on those paths. Indeed, such efficiency boosting structures, such as
highways and elevated intersections, are ubiquitous in robotics and
transportation applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02081</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02081</id><created>2015-04-08</created><updated>2015-11-16</updated><authors><author><keyname>Ni</keyname><forenames>Weiheng</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author></authors><title>Hybrid Block Diagonalization for Massive Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 12 figures, double column, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a massive multiple-input multiple-output (MIMO) system, restricting the
number of RF chains to far less than the number of antenna elements can
significantly reduce the implementation cost compared to the full complexity RF
chain configuration. In this paper, we consider the downlink communication of a
massive multiuser MIMO (MU-MIMO) system and propose a low-complexity hybrid
block diagonalization (Hy-BD) scheme to approach the capacity performance of
the traditional BD processing method. We aim to harvest the large array gain
through the phase-only RF precoding and combining and then digital BD
processing is performed on the equivalent baseband channel. The proposed Hy-BD
scheme is examined in both the large Rayleigh fading channels and millimeter
wave (mmWave) channels. A performance analysis is further conducted for
single-path channels and large number of transmit and receive antennas.
Finally, simulation results demonstrate that our Hy-BD scheme, with a lower
implementation and computational complexity, achieves a capacity performance
that is close to (sometimes even higher than) that of the traditional
high-dimensional BD processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02089</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02089</id><created>2015-04-08</created><updated>2016-01-27</updated><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author></authors><title>The Computational Power of Optimization in Online Learning</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental problem of prediction with expert advice where
the experts are &quot;optimizable&quot;: there is a black-box optimization oracle that
can be used to compute, in constant time, the leading expert in retrospect at
any point in time. In this setting, we give a novel online algorithm that
attains vanishing regret with respect to $N$ experts in total
$\widetilde{O}(\sqrt{N})$ computation time. We also give a lower bound showing
that this running time cannot be improved (up to log factors) in the oracle
model, thereby exhibiting a quadratic speedup as compared to the standard,
oracle-free setting where the required time for vanishing regret is
$\widetilde{\Theta}(N)$. These results demonstrate an exponential gap between
the power of optimization in online learning and its power in statistical
learning: in the latter, an optimization oracle---i.e., an efficient empirical
risk minimizer---allows to learn a finite hypothesis class of size $N$ in time
$O(\log{N})$. We also study the implications of our results to learning in
repeated zero-sum games, in a setting where the players have access to oracles
that compute, in constant time, their best-response to any mixed strategy of
their opponent. We show that the runtime required for approximating the minimax
value of the game in this setting is $\widetilde{\Theta}(\sqrt{N})$, yielding
again a quadratic improvement upon the oracle-free setting, where
$\widetilde{\Theta}(N)$ is known to be tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02092</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02092</id><created>2015-04-08</created><authors><author><keyname>Balaji</keyname><forenames>R. D.</forenames></author><author><keyname>Lakshminarayan</keyname><forenames>Ramkumar</forenames></author><author><keyname>Balaji</keyname><forenames>Er. Malathi</forenames></author></authors><title>Revolutionary Hybrid E-Books for Enhanced Higher Learning</title><categories>cs.CY</categories><comments>8 pages, ICOET 2015, Oman. arXiv admin note: substantial text overlap
  with arXiv:1504.01855</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Books are the best friends of human beings and make them a rational animal.
In this E-world our traditional books are losing their values. Recent surveys
are proving that the younger generations are not much interested in visiting
libraries and reading books due to their addiction towards the electronic
gadgets. Many publishers are now changed their strategy by publishing and
promoting ebooks. Academicians are also badly affected by this trend and they
are forced to motivate the students to improve their reading habits for their
better performance in the institutions and to be responsible citizens of a
country.
  There are many difficulties faced by the ebook readers and it discourages the
people to read an ebook for long time like a normal printed book. The main
objective of this paper is to introduce a hybrid Ebook which is capable of
having audio and video files also in it. This makes the people to read the
e-book with modern electronic formats. This multimedia facility makes the
people to read or listen or watch the ebooks for long time. We have quoted the
survey results which prove that the students prefer to read hybrid books than
normal ebooks or the printed handouts in College of Applied Sciences, Oman.
Still we feel these books should have few more facilities, so that people who
like traditional books will also adopt these ebooks, without losing the
satisfaction of reading printed books. These things are listed in the
recommendations and conclusion section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02093</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02093</id><created>2015-04-08</created><authors><author><keyname>Dafalla</keyname><forenames>Z. I.</forenames></author><author><keyname>Balaji</keyname><forenames>R. D.</forenames></author></authors><title>Enhancing the Understanding of Computer Networking Courses through
  Software Tools</title><categories>cs.CY</categories><comments>4 pages, ICOET 2015, Oman. arXiv admin note: substantial text overlap
  with arXiv:1504.01855</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer networking is an important specialization in Information and
Communication Technologies. However imparting the right knowledge to students
can be a challenging task due to the fact that there is not enough time to
deliver lengthy labs during normal lecture hours. Augmenting the use of
physical machines with software tools help the students to learn beyond the
limited lab sessions within the environment of higher Institutions of learning
throughout the world. The Institutions focus mostly on effective use of
available resources i.e. a lab may have few lab sessions scheduled within a day
for different courses. Hence a particular lab session must begin and end on
time. A slow student who did not complete his/her lab exercise must vacate the
lab because another class is about to commence. Hence using free software tools
such as OPNET IT guru, Packet trace and NS2 will help the student to learn
beyond the College time. A Student will gain an insight into Computer
networking field by repeatedly doing the same labs at the comfort of his laptop
or PC at home, hence increasing deeper understanding into the subject. The main
objective of this paper is to explore the software tools which will enhance the
networks practical skills among higher education students at College of Applied
Sciences Salalah. The paper also discusses the methodologies of implementing
and executing lab sessions after the normal college hours. The limitations and
suggestions for the improvements are also specified at the end of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02115</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02115</id><created>2015-04-08</created><authors><author><keyname>Gangan</keyname><forenames>Subodh</forenames></author></authors><title>A Review of Man-in-the-Middle Attacks</title><categories>cs.CR</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey of man-in-the-middle (MIM) attacks in
communication networks and methods of protection against them. In real time
communication, the attack can in many situations be discovered by the use of
timing information. The most common attacks occur due to Address Resolution
Protocol (ARP) cache poisoning, DNS spoofing, session hijacking, and SSL
hijacking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02125</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02125</id><created>2015-04-08</created><authors><author><keyname>Ruelens</keyname><forenames>Frederik</forenames></author><author><keyname>Claessens</keyname><forenames>Bert</forenames></author><author><keyname>Vandael</keyname><forenames>Stijn</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author><author><keyname>Babuska</keyname><forenames>Robert</forenames></author><author><keyname>Belmans</keyname><forenames>Ronnie</forenames></author></authors><title>Residential Demand Response Applications Using Batch Reinforcement
  Learning</title><categories>cs.SY cs.LG</categories><comments>Submitted to Trans. on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by recent advances in batch Reinforcement Learning (RL), this paper
contributes to the application of batch RL to demand response. In contrast to
conventional model-based approaches, batch RL techniques do not require a
system identification step, which makes them more suitable for a large-scale
implementation. This paper extends fitted Q-iteration, a standard batch RL
technique, to the situation where a forecast of the exogenous data is provided.
In general, batch RL techniques do not rely on expert knowledge on the system
dynamics or the solution. However, if some expert knowledge is provided, it can
be incorporated by using our novel policy adjustment method. Finally, we tackle
the challenge of finding an open-loop schedule required to participate in the
day-ahead market. We propose a model-free Monte-Carlo estimator method that
uses a metric to construct artificial trajectories and we illustrate this
method by finding the day-ahead schedule of a heat-pump thermostat. Our
experiments show that batch RL techniques provide a valuable alternative to
model-based controllers and that they can be used to construct both closed-loop
and open-loop policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02128</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02128</id><created>2015-04-08</created><updated>2016-01-25</updated><authors><author><keyname>Paikan</keyname><forenames>Ali</forenames></author><author><keyname>Domenichelli</keyname><forenames>Daniele</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Communication channel prioritization in a publish-subscribe architecture</title><categories>cs.SE</categories><comments>In 8th Workshop on Software Engineering and Architectures for
  Realtime Interactive Systems (SEARIS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-Time communication are important in all those distributed applications
where timing constraints on data proccessing and task executation play a
fundamental role. Standards-base software engineering does not yet specify how
real-time properties should be integrated into a publish/subscribe middleware.
This article describes an approach for integration of priority quality of
service in a publish/subscribe middleware. The approach simply leverages the
operating system functionalities to provide a framework where specific
communication channels can be prioritized at run-time. The quality of service
is implemented in YARP (Yet Another Robot Platform) framework and the primarily
results of performance tests are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02134</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02134</id><created>2015-04-08</created><authors><author><keyname>Thomas</keyname><forenames>Christopher</forenames></author><author><keyname>Jennings</keyname><forenames>Brandon</forenames></author></authors><title>Hand Posture's Effect on Touch Screen Text Input Behaviors: A Touch Area
  Based Study</title><categories>cs.HC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices with touch keyboards have become ubiquitous, but text entry on
these devices remains slow and errorprone. Understanding touch patterns during
text entry could be useful in designing robust error-correction algorithms for
soft keyboards. In this paper, we present an analysis of text input behaviors
on a soft QWERTY keyboard in three different text entry postures: index finger
only, one thumb, and two thumb. Our work expands on the work of [1] by
considering the entire surface area of digit contact with the smartphone
keyboard, rather than interpreting each touch as a single point. To do this, we
captured touch areas for every key in a lab study with 8 participants and
calculated offsets, error rates, and size measurements. We then repeated the
original experiment described in [1] and showed that significant differences
exist when basing offset calculations on touch area compared to touch points
for two postures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02141</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02141</id><created>2015-04-08</created><updated>2015-09-21</updated><authors><author><keyname>Khan</keyname><forenames>Shehroz S.</forenames></author><author><keyname>Karg</keyname><forenames>Michelle E.</forenames></author><author><keyname>Kulic</keyname><forenames>Dana</forenames></author><author><keyname>Hoey</keyname><forenames>Jesse</forenames></author></authors><title>Detecting falls with X-Factor HMMs when the training data for falls is
  not available</title><categories>cs.LG cs.AI</categories><comments>9 pages, 4 figures, 5 tables, under review in IEEE JBHI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification of falls while performing normal activities of daily living
(ADL) is important to ensure personal safety and well-being. However, falling
is a short term activity that occurs infrequently. This poses a challenge to
traditional classification algorithms, because there may be very little
training data for falls (or none at all). This paper proposes an approach for
the identification of falls using a wearable device in the absence of training
data for falls but with plentiful data for normal ADL. We propose three
`X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls
using &quot;inflated&quot; output covariances (observation models). To estimate the
inflated covariances, we propose a novel cross validation method to remove
&quot;outliers&quot; from the normal ADL that serve as proxies for the unseen falls and
allow learning the XHMMs using only normal activities. We tested the proposed
XHMM approaches on two activity recognition datasets and show high detection
rates for falls in the absence of fall-specific training data. We show that the
traditional method of choosing a threshold based on maximum of negative of
log-likelihood to identify unseen falls is ill-posed for this problem. We also
show that supervised classification methods perform poorly when very limited
fall data are available during the training phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02146</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02146</id><created>2015-04-08</created><updated>2015-04-24</updated><authors><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>Kletenik</keyname><forenames>Devorah</forenames></author><author><keyname>Lin</keyname><forenames>Patrick</forenames></author></authors><title>Discrete Stochastic Submodular Maximization: Adaptive vs. Non-Adaptive
  vs. Offline</title><categories>cs.DS cs.CC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of stochastic monotone submodular function
maximization, subject to constraints. We give results on adaptivity gaps, and
on the gap between the optimal offline and online solutions. We present a
procedure that transforms a decision tree (adaptive algorithm) into a
non-adaptive chain. We prove that this chain achieves at least ${\tau}$ times
the utility of the decision tree, over a product distribution and binary state
space, where ${\tau} = \min_{i,j} \Pr[x_i=j]$. This proves an adaptivity gap of
$1/{\tau}$ (which is $2$ in the case of a uniform distribution) for the problem
of stochastic monotone submodular maximization subject to state-independent
constraints. For a cardinality constraint, we prove that a simple adaptive
greedy algorithm achieves an approximation factor of $(1-1/e^{\tau})$ with
respect to the optimal offline solution; previously, it has been proven that
the algorithm achieves an approximation factor of $(1-1/e)$ with respect to the
optimal adaptive online solution. Finally, we show that there exists a
non-adaptive solution for the stochastic max coverage problem that is within a
factor $(1-1/e)$ of the optimal adaptive solution and within a factor of
${\tau}(1-1/e)$ of the optimal offline solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02147</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02147</id><created>2015-04-08</created><authors><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Barabin</keyname><forenames>Kawika</forenames></author><author><keyname>Sayre</keyname><forenames>Kent</forenames></author></authors><title>Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent approaches to distributed model fitting rely heavily on consensus
ADMM, where each node solves small sub-problems using only local data. We
propose iterative methods that solve {\em global} sub-problems over an entire
distributed dataset. This is possible using transpose reduction strategies that
allow a single node to solve least-squares over massive datasets without
putting all the data in one place. This results in simple iterative methods
that avoid the expensive inner loops required for consensus methods. To
demonstrate the efficiency of this approach, we fit linear classifiers and
sparse linear models to datasets over 5 Tb in size using a distributed
implementation with over 7000 cores in far less time than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02148</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02148</id><created>2015-04-08</created><authors><author><keyname>Bol</keyname><forenames>Peter K.</forenames></author><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Wang</keyname><forenames>Hongsu</forenames></author></authors><title>Mining and discovering biographical information in Difangzhi with a
  language-model-based approach</title><categories>cs.CL cs.CY cs.DL</categories><comments>6 pages, 4 figures, 1 table, 2015 International Conference on Digital
  Humanities. in Proceedings of the 2015 International Conference on Digital
  Humanities (DH 2015). July 2015</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results of expanding the contents of the China Biographical
Database by text mining historical local gazetteers, difangzhi. The goal of the
database is to see how people are connected together, through kinship, social
connections, and the places and offices in which they served. The gazetteers
are the single most important collection of names and offices covering the Song
through Qing periods. Although we begin with local officials we shall
eventually include lists of local examination candidates, people from the
locality who served in government, and notable local figures with biographies.
The more data we collect the more connections emerge. The value of doing
systematic text mining work is that we can identify relevant connections that
are either directly informative or can become useful without deep historical
research. Academia Sinica is developing a name database for officials in the
central governments of the Ming and Qing dynasties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02150</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02150</id><created>2015-04-08</created><authors><author><keyname>Huang</keyname><forenames>Wei-Jie</forenames></author><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author></authors><title>Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual
  Entailment in NTCIR RITE Evaluation Tasks</title><categories>cs.CL cs.AI cs.DL</categories><comments>20 pages, 1 figure, 26 tables, Journal article in Soft Computing
  (Spinger). Soft Computing, online. Springer, Germany, 2015</comments><acm-class>I.2.7</acm-class><doi>10.1007/s00500-015-1629-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We computed linguistic information at the lexical, syntactic, and semantic
levels for Recognizing Inference in Text (RITE) tasks for both traditional and
simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,
named-entity recognition, and near synonym recognition were employed, and
features like counts of common words, statement lengths, negation words, and
antonyms were considered to judge the entailment relationships of two
statements, while we explored both heuristics-based functions and
machine-learning approaches. The reported systems showed robustness by
simultaneously achieving second positions in the binary-classification subtasks
for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted
more experiments with the test data of NTCIR-9 RITE, with good results. We also
extended our work to search for better configurations of our classifiers and
investigated contributions of individual features. This extended work showed
interesting results and should encourage further discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02151</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02151</id><created>2015-04-08</created><updated>2015-12-19</updated><authors><author><keyname>Woods</keyname><forenames>Brad</forenames></author><author><keyname>Punnen</keyname><forenames>Abraham</forenames></author><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author></authors><title>A Linear Time Algorithm for the $3$-neighbour Traveling Salesman Problem
  on Halin graphs and extensions</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quadratic Travelling Salesman Problem (QTSP) is to find a least cost
Hamilton cycle in an edge-weighted graph, where costs are defined on all pairs
of edges contained in the Hamilton cycle. This is a more general version than
the commonly studied QTSP which only considers pairs of adjacent edges. We
define a restricted version of QTSP, the $k$-neighbour TSP (TSP($k$)), and give
a linear time algorithm to solve TSP($k$) on a Halin graph for $k\leq 3$. This
algorithm can be extended to solve TSP($k$) on any fully reducible class of
graphs for any fixed $k$ in polynomial time. This result generalizes
corresponding results for the standard TSP. TSP($k$) can be used to model
various machine scheduling problems as well as an optimal routing problem for
unmanned aerial vehicles (UAVs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02162</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02162</id><created>2015-04-08</created><updated>2015-06-18</updated><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Silva</keyname><forenames>Filipi N.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Concentric network symmetry grasps authors' styles in word adjacency
  networks</title><categories>cs.CL</categories><comments>Accepted for publication in Europhys. Lett. (EPL). The supplementary
  information is available from
  https://dl.dropboxusercontent.com/u/2740286/symmetry.pdf</comments><journal-ref>Europhys. Lett. 110 68001 (2015)</journal-ref><doi>10.1209/0295-5075/110/68001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several characteristics of written texts have been inferred from statistical
analysis derived from networked models. Even though many network measurements
have been adapted to study textual properties at several levels of complexity,
some textual aspects have been disregarded. In this paper, we study the
symmetry of word adjacency networks, a well-known representation of text as a
graph. A statistical analysis of the symmetry distribution performed in several
novels showed that most of the words do not display symmetric patterns of
connectivity. More specifically, the merged symmetry displayed a distribution
similar to the ubiquitous power-law distribution. Our experiments also revealed
that the studied metrics do not correlate with other traditional network
measurements, such as the degree or betweenness centrality. The effectiveness
of the symmetry measurements was verified in the authorship attribution task.
Interestingly, we found that specific authors prefer particular types of
symmetric motifs. As a consequence, the authorship of books could be accurately
identified in 82.5% of the cases, in a dataset comprising books written by 8
authors. Because the proposed measurements for text analysis are complementary
to the traditional approach, they can be used to improve the characterization
of text networks, which might be useful for related applications, such as those
relying on the identification of topical words and information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02164</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02164</id><created>2015-04-08</created><updated>2015-04-09</updated><authors><author><keyname>Li</keyname><forenames>Xiangru</forenames></author><author><keyname>Lu</keyname><forenames>Yu</forenames></author><author><keyname>Comte</keyname><forenames>Georges</forenames></author><author><keyname>Luo</keyname><forenames>Ali</forenames></author><author><keyname>Zhao</keyname><forenames>Yongheng</forenames></author><author><keyname>Wang</keyname><forenames>Yongjun</forenames></author></authors><title>Linearly Supporting Feature Extraction For Automated Estimation Of
  Stellar Atmospheric Parameters</title><categories>astro-ph.SR astro-ph.IM cs.CV</categories><comments>21 pages, 7 figures, 8 tables, The Astrophysical Journal Supplement
  Series (accepted for publication)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We describe a scheme to extract linearly supporting (LSU) features from
stellar spectra to automatically estimate the atmospheric parameters $T_{eff}$,
log$~g$, and [Fe/H]. &quot;Linearly supporting&quot; means that the atmospheric
parameters can be accurately estimated from the extracted features through a
linear model. The successive steps of the process are as follow: first,
decompose the spectrum using a wavelet packet (WP) and represent it by the
derived decomposition coefficients; second, detect representative spectral
features from the decomposition coefficients using the proposed method Least
Absolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate the
atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detected
features using a linear regression method. One prominent characteristic of this
scheme is its ability to evaluate quantitatively the contribution of each
detected feature to the atmospheric parameter estimate and also to trace back
the physical significance of that feature. This work also shows that the
usefulness of a component depends on both wavelength and frequency. The
proposed scheme has been evaluated on both real spectra from the Sloan Digital
Sky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODF
models. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62
features for log$~g$, and 68 features for [Fe/H]. Test consistencies between
our estimates and those provided by the Spectroscopic Sarameter Pipeline of
SDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$
(83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. For
the synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$
(32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02168</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02168</id><created>2015-04-08</created><authors><author><keyname>Thapa</keyname><forenames>Chandra</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>A New Index Coding Scheme Exploiting Interlinked Cycles</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2015 IEEE International Symposium on
  Information Theory (ISIT 2015), Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the index coding problem in the unicast message setting, i.e., where
each message is requested by one unique receiver. This problem can be modeled
by a directed graph. We propose a new scheme called interlinked cycle cover,
which exploits interlinked cycles in the directed graph, for designing index
codes. This new scheme generalizes the existing clique cover and cycle cover
schemes. We prove that for a class of infinitely many digraphs with messages of
any length, interlinked cycle cover provides an optimal index code.
Furthermore, the index code is linear with linear time encoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02174</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02174</id><created>2015-04-08</created><updated>2015-12-23</updated><authors><author><keyname>Boxer</keyname><forenames>Laurence</forenames></author><author><keyname>Staecker</keyname><forenames>P. Christopher</forenames></author></authors><title>Connectivity Preserving Multivalued Functions in Digital Topology</title><categories>cs.CV math.GN</categories><comments>small changes</comments><acm-class>I.4.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study connectivity preserving multivalued functions between digital
images. This notion generalizes that of continuous multivalued functions
studied mostly in the setting of the digital plane $Z^2$. We show that
connectivity preserving multivalued functions, like continuous multivalued
functions, are appropriate models for digital morpholological operations.
Connectivity preservation, unlike continuity, is preserved by compositions, and
generalizes easily to higher dimensions and arbitrary adjacency relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02202</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02202</id><created>2015-04-09</created><authors><author><keyname>Endoo</keyname><forenames>Pisutpong</forenames></author></authors><title>Fackbook Implementation in Developing English Writing for Thai Students</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objectives of this research were to study FB implementation and attitudes
in developing English writing skills of Thai students studying in the first
year students program in EIC academic year 1/2014 at RMUTI, Surin Campus. The
Purposive sampling was designed for data collecting. The instruments for this
research were questionnaires and in-depth questions. The data analysis was
analyzed by the Descriptive statistics to find out the value of the frequency
and percentage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02205</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02205</id><created>2015-04-09</created><updated>2015-12-04</updated><authors><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>Zhan</keyname><forenames>Shulin</forenames></author><author><keyname>Shao</keyname><forenames>Chenrong</forenames></author><author><keyname>Wang</keyname><forenames>Junwei</forenames></author><author><keyname>John</keyname><forenames>Lizy K.</forenames></author><author><keyname>Xu</keyname><forenames>Jiangtao</forenames></author><author><keyname>Lu</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author></authors><title>BigDataBench-MT: A Benchmark Tool for Generating Realistic Mixed Data
  Center Workloads</title><categories>cs.DC</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long-running service workloads (e.g. web search engine) and short-term data
analysis workloads (e.g. Hadoop MapReduce jobs) co-locate in today's data
centers. Developing realistic benchmarks to reflect such practical scenario of
mixed workload is a key problem to produce trustworthy results when evaluating
and comparing data center systems. This requires using actual workloads as well
as guaranteeing their submissions to follow patterns hidden in real-world
traces. However, existing benchmarks either generate actual workloads based on
probability models, or replay real-world workload traces using basic I/O
operations. To fill this gap, we propose a benchmark tool that is a first step
towards generating a mix of actual service and data analysis workloads on the
basis of real workload traces. Our tool includes a combiner that enables the
replaying of actual workloads according to the workload traces, and a
multi-tenant generator that flexibly scales the workloads up and down according
to users' requirements. Based on this, our demo illustrates the workload
customization and generation process using a visual interface. The proposed
tool, called BigDataBench-MT, is a multi-tenant version of our comprehensive
benchmark suite BigDataBench and it is publicly available from
http://prof.ict.ac.cn/BigDataBench/multi-tenancyversion/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02206</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02206</id><created>2015-04-09</created><updated>2016-02-12</updated><authors><author><keyname>Li</keyname><forenames>Fang</forenames></author><author><keyname>Osher</keyname><forenames>Stanley</forenames></author><author><keyname>Qin</keyname><forenames>Jing</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author></authors><title>A Multiphase Image Segmentation Based on Fuzzy Membership Functions and
  L1-norm Fidelity</title><categories>math.OC cs.CV</categories><comments>28 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a variational multiphase image segmentation model
based on fuzzy membership functions and L1-norm fidelity. Then we apply the
alternating direction method of multipliers to solve an equivalent problem. All
the subproblems can be solved efficiently. Specifically, we propose a fast
method to calculate the fuzzy median. Experimental results and comparisons show
that the L1-norm based method is more robust to outliers such as impulse noise
and keeps better contrast than its L2-norm counterpart. Theoretically, we prove
the existence of the minimizer and analyze the convergence of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02209</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02209</id><created>2015-04-09</created><updated>2015-04-18</updated><authors><author><keyname>Zhang</keyname><forenames>Di</forenames></author><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Yu</keyname><forenames>Keping</forenames></author><author><keyname>Sato</keyname><forenames>Takuro</forenames></author></authors><title>Energy Efficient Policy for Cloud Radio Access Network</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy Efficiency (EE) is a big issue in 5th Generation Wireless
Communications (5G) on condition that the number of access User Equipments
(UEs) are exploding and more antennas should be equipped in one Base Station
(BS). In EE studies, prior literatures focus on the energy consumption of
single separated BS coverage area or through scheduling mechanism or network
coding method. But some other elements are ignored in those literatures, such
as the energy consumption of machine room, circuit, etc. In this paper, to be
more closer to the reality, based on the Cloud Radio Access Network (C-RAN), we
modify its traditional structure for easier layout of sleeping mechanism in the
real world, study the EE issue within a comprehensive view while taking more
elements into consideration. We modified the traditional C-RAN structure with
the purpose of much easily adopting the sleeping mechanism with on-off
selection method. Afterwards, the EE issue is modeled into a mathematical
optimizing problem and its solution is given by a tractable method. The
analysis of sum capacity in one cluster of this modified structure is addressed
first. Then based on the analysis, the EE issue is studied with a comprehensive
view while taking more elements into consideration. In the next step, we
convert it into an optimization problem and give its solution with the sleeping
techniques. Comparing with prior works, this proposal is of better performance
for the merit of comprehensive vision and easier layout characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02212</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02212</id><created>2015-04-09</created><authors><author><keyname>Zhang</keyname><forenames>Di</forenames></author><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Sato</keyname><forenames>Takuro</forenames></author></authors><title>Towards SE and EE in 5G with NOMA and Massive MIMO Technologies</title><categories>cs.IT math.IT</categories><comments>in IEICE General Conference, Mar. 13, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Orthogonal Multiple Access (NOMA) has been proposed to enhance the
Spectrum Efficiency (SE) and cell-edge capacity. This paper considers the
massive Multi-Input Multi-Output (MIMO) with Non-Orthogonal Multiple Access
(NOMA) encoding. The close-form expression of capacity of the massive MIMO with
NOMA is given here. Apart from the previous Successive Interference
Cancellation (SIC) method, the Power Hard Limiter (PHD) is introduced here for
better reality implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02214</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02214</id><created>2015-04-09</created><authors><author><keyname>Plonka</keyname><forenames>Gerlind</forenames></author><author><keyname>Wannenwetsch</keyname><forenames>Katrin</forenames></author></authors><title>A deterministic sparse FFT algorithm for vectors with small support</title><categories>math.NA cs.NA</categories><comments>16 pages</comments><msc-class>65T50, 42A38</msc-class><doi>10.1007/s11075-015-0028-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the special case where a discrete signal ${\bf x}$
of length N is known to vanish outside a support interval of length $m &lt; N$. If
the support length $m$ of ${\bf x}$ or a good bound of it is a-priori known we
derive a sublinear deterministic algorithm to compute ${\bf x}$ from its
discrete Fourier transform. In case of exact Fourier measurements we require
only ${\cal O}(m \log m)$ arithmetical operations. For noisy measurements, we
propose a stable ${\cal O}(m \log N)$ algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02218</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02218</id><created>2015-04-09</created><authors><author><keyname>Nusrat</keyname><forenames>Sabrina</forenames></author><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author></authors><title>Evaluating Cartogram Effectiveness</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cartograms are maps in which areas of geographic regions (countries, states)
appear in proportion to some variable of interest (population, income).
Cartograms are popular visualizations for geo-referenced data that have been
used for over a century and that make it possible to gain insight into patterns
and trends in the world around us. Despite the popularity of cartograms and the
large number of cartogram types, there are few studies evaluating the
effectiveness of cartograms in conveying information. Based on a recent task
taxonomy for cartograms, we evaluate four major different types of cartograms:
contiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically,
we evaluate the effectiveness of these cartograms by quantitative performance
analysis, as well as by subjective preferences. We analyze the results of our
study in the context of some prevailing assumptions in the literature of
cartography and cognitive science. Finally, we make recommendations for the use
of different types of cartograms for different tasks and settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02222</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02222</id><created>2015-04-09</created><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Mike</forenames></author></authors><title>Binary words with the fewest unbordered conjugates</title><categories>cs.FL math.CO</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any primitive binary word contains at least two unbordered conjugates. We
characterize binary words that have exactly two unbordered conjugates and show
that they can be expressed as a product of two palindromes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02235</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02235</id><created>2015-04-09</created><authors><author><keyname>Gowri</keyname><forenames>R.</forenames></author><author><keyname>Rathipriya</keyname><forenames>R.</forenames></author></authors><title>Extraction of Protein Sequence Motif Information using PSO K-Means</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The main objective of the paper is to find the motif information.The
functionalities of the proteins are ideally found from their motif information
which is extracted using various techniques like clustering with k-means,
hybrid k-means, self-organising maps, etc., in the literature. In this work
protein sequence information is extracted using optimised k-means algorithm.
The particle swarm optimisation technique is one of the frequently used
optimisation method. In the current work the PSO k-means is used for motif
information extraction. This paper also deals with the comparison between the
motif information obtained from clusters and biclustersusing PSO k-means
algorithm. The motif information acquired is based on the structure homogeneity
of the protein sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02242</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02242</id><created>2015-04-09</created><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Achievable Rates for the Fading Half-Duplex Single Relay Selection
  Network Using Buffer-Aided Relaying</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2015.2421912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the half-duplex single relay selection network, comprised of a source, $M$
half-duplex relays, and a destination, only one relay is active at any given
time, i.e., only one relay receives or transmits, and the other relays are
inactive, i.e., they do not receive nor transmit. The capacity of this network,
when all links are affected by independent slow time-continuous fading and
additive white Gaussian noise (AWGN), is still unknown, and only achievable
average rates have been reported in the literature so far. In this paper, we
present new achievable average rates for this network which are larger than the
best known average rates. These new average rates are achieved with a
buffer-aided relaying protocol. Since the developed buffer-aided protocol
introduces unbounded delay, we also devise a buffer-aided protocol which limits
the delay at the expense of a decrease in rate. Moreover, we discuss the
practical implementation of the proposed buffer-aided relaying protocols and
show that they do not require more resources for channel state information
acquisition than the existing relay selection protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02247</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02247</id><created>2015-04-09</created><authors><author><keyname>Melnikov</keyname><forenames>Alexey A.</forenames></author><author><keyname>Makmal</keyname><forenames>Adi</forenames></author><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Projective simulation with generalization</title><categories>cs.AI cs.LG stat.ML</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to generalize is an important feature of any intelligent agent.
Not only because it may allow the agent to cope with large amounts of data, but
also because in some environments, an agent with no generalization ability is
simply doomed to fail. In this work we outline several criteria for
generalization, and present a dynamic and autonomous machinery that enables
projective simulation agents to meaningfully generalize. Projective simulation,
a novel, physical, approach to artificial intelligence, was recently shown to
perform well, in comparison with standard models, on both simple reinforcement
learning problems, as well as on more complicated canonical tasks, such as the
&quot;grid world&quot; and the &quot;mountain car problem&quot;. Both the basic projective
simulation model and the presented generalization machinery are based on very
simple principles. This simplicity allows us to provide a full analytical
analysis of the agent's performance and to illustrate the benefit the agent
gains by generalizing. Specifically, we show how such an ability allows the
agent to learn in rather extreme environments, in which learning is otherwise
impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02255</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02255</id><created>2015-04-09</created><authors><author><keyname>Buzmakov</keyname><forenames>Aleksey</forenames></author><author><keyname>Egho</keyname><forenames>Elias</forenames></author><author><keyname>Jay</keyname><forenames>Nicolas</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames></author><author><keyname>Ra&#xef;ssi</keyname><forenames>Chedy</forenames></author></authors><title>On mining complex sequential data by means of FCA and pattern structures</title><categories>cs.AI cs.DB</categories><comments>An accepted publication in International Journal of General Systems.
  The paper is created in the wake of the conference on Concept Lattice and
  their Applications (CLA'2013). 27 pages, 9 figures, 3 tables</comments><acm-class>H.2.8; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays data sets are available in very complex and heterogeneous ways.
Mining of such data collections is essential to support many real-world
applications ranging from healthcare to marketing. In this work, we focus on
the analysis of &quot;complex&quot; sequential data by means of interesting sequential
patterns. We approach the problem using the elegant mathematical framework of
Formal Concept Analysis (FCA) and its extension based on &quot;pattern structures&quot;.
Pattern structures are used for mining complex data (such as sequences or
graphs) and are based on a subsumption operation, which in our case is defined
with respect to the partial order on sequences. We show how pattern structures
along with projections (i.e., a data reduction of sequential structures), are
able to enumerate more meaningful patterns and increase the computing
efficiency of the approach. Finally, we show the applicability of the presented
method for discovering and analyzing interesting patient patterns from a French
healthcare data set on cancer. The quantitative and qualitative results (with
annotations and analysis from a physician) are reported in this use case which
is the main motivation for this work.
  Keywords: data mining; formal concept analysis; pattern structures;
projections; sequences; sequential data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02261</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02261</id><created>2015-04-09</created><authors><author><keyname>Swan</keyname><forenames>A.</forenames></author><author><keyname>Gargouri</keyname><forenames>Y.</forenames></author><author><keyname>Hunt</keyname><forenames>M.</forenames></author><author><keyname>Harnad</keyname><forenames>S.</forenames></author></authors><title>Open Access Policy: Numbers, Analysis, Effectiveness</title><categories>cs.DL</categories><comments>49 pages, 21 figures, 15 tables. Pasteur4OA Work Package 3 report:
  Open Access policies 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The PASTEUR4OA project analyses what makes an Open Access (OA) policy
effective. The total number of institutional or funder OA policies worldwide is
now 663 (March 2015), over half of them mandatory. ROARMAP, the policy
registry, has been rebuilt to record more policy detail and provide more
extensive search functionality. Deposit rates were measured for articles in
institutions' repositories and compared to the total number of WoS-indexed
articles published from those institutions. Average deposit rate was over four
times as high for institutions with a mandatory policy. Six positive
correlations were found between deposit rates and (1) Must-Deposit; (2)
Cannot-Waive-Deposit; (3) Deposit-Linked-to-Research-Evaluation; (4)
Cannot-Waive-Rights-Retention; (5) Must-Make-Deposit-OA (after allowable
embargo) and (6) Can-Waive-OA. For deposit latency, there is a positive
correlation between earlier deposit and (7) Must-Deposit-Immediately as well as
with (4) Cannot-Waive-Rights-Retention and with mandate age. There are not yet
enough OA policies to test whether still further policy conditions would
contribute to mandate effectiveness but the present findings already suggest
that it would be useful for current and future OA policies to adopt the seven
positive conditions so as to accelerate and maximise the growth of OA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02264</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02264</id><created>2015-04-09</created><authors><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>Model Coupling between the Weather Research and Forecasting Model and
  the DPRI Large Eddy Simulator for Urban Flows on GPU-accelerated Multicore
  Systems</title><categories>cs.DC</categories><comments>This work was conducted during a research visit at the Disaster
  Prevention Research Institute of Kyoto University, supported by an EPSRC
  Overseas Travel Grant, EP/L026201/1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we present a novel approach to model coupling for
shared-memory multicore systems hosting OpenCL-compliant accelerators, which we
call The Glasgow Model Coupling Framework (GMCF). We discuss the implementation
of a prototype of GMCF and its application to coupling the Weather Research and
Forecasting Model and an OpenCL-accelerated version of the Large Eddy Simulator
for Urban Flows (LES) developed at DPRI.
  The first stage of this work concerned the OpenCL port of the LES. The
methodology used for the OpenCL port is a combination of automated analysis and
code generation and rule-based manual parallelization. For the evaluation, the
non-OpenCL LES code was compiled using gfortran, fort and pgfortran}, in each
case with auto-parallelization and auto-vectorization. The OpenCL-accelerated
version of the LES achieves a 7 times speed-up on a NVIDIA GeForce GTX 480
GPGPU, compared to the fastest possible compilation of the original code
running on a 12-core Intel Xeon E5-2640.
  In the second stage of this work, we built the Glasgow Model Coupling
Framework and successfully used it to couple an OpenMP-parallelized WRF
instance with an OpenCL LES instance which runs the LES code on the GPGPI. The
system requires only very minimal changes to the original code. The report
discusses the rationale, aims, approach and implementation details of this
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02268</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02268</id><created>2015-04-09</created><updated>2015-09-22</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Sayan</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>Space- and Time-Efficient Algorithm for Maintaining Dense Subgraphs on
  One-Pass Dynamic Streams</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in STOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While in many graph mining applications it is crucial to handle a stream of
updates efficiently in terms of {\em both} time and space, not much was known
about achieving such type of algorithm. In this paper we study this issue for a
problem which lies at the core of many graph mining applications called {\em
densest subgraph problem}. We develop an algorithm that achieves time- and
space-efficiency for this problem simultaneously. It is one of the first of its
kind for graph problems to the best of our knowledge.
  In a graph $G = (V, E)$, the &quot;density&quot; of a subgraph induced by a subset of
nodes $S \subseteq V$ is defined as $|E(S)|/|S|$, where $E(S)$ is the set of
edges in $E$ with both endpoints in $S$. In the densest subgraph problem, the
goal is to find a subset of nodes that maximizes the density of the
corresponding induced subgraph. For any $\epsilon&gt;0$, we present a dynamic
algorithm that, with high probability, maintains a $(4+\epsilon)$-approximation
to the densest subgraph problem under a sequence of edge insertions and
deletions in a graph with $n$ nodes. It uses $\tilde O(n)$ space, and has an
amortized update time of $\tilde O(1)$ and a query time of $\tilde O(1)$. Here,
$\tilde O$ hides a $O(\poly\log_{1+\epsilon} n)$ term. The approximation ratio
can be improved to $(2+\epsilon)$ at the cost of increasing the query time to
$\tilde O(n)$. It can be extended to a $(2+\epsilon)$-approximation
sublinear-time algorithm and a distributed-streaming algorithm. Our algorithm
is the first streaming algorithm that can maintain the densest subgraph in {\em
one pass}. The previously best algorithm in this setting required $O(\log n)$
passes [Bahmani, Kumar and Vassilvitskii, VLDB'12]. The space required by our
algorithm is tight up to a polylogarithmic factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02281</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02281</id><created>2015-04-09</created><authors><author><keyname>Ansari</keyname><forenames>Ahlam</forenames></author><author><keyname>Sayyed</keyname><forenames>Mohd Amin</forenames></author><author><keyname>Ratlamwala</keyname><forenames>Khatija</forenames></author><author><keyname>Shaikh</keyname><forenames>Parvin</forenames></author></authors><title>An Optimized Hybrid Approach for Path Finding</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Path finding algorithm addresses problem of finding shortest path from source
to destination avoiding obstacles. There exist various search algorithms namely
A*, Dijkstra's and ant colony optimization. Unlike most path finding algorithms
which require destination co-ordinates to compute path, the proposed algorithm
comprises of a new method which finds path using backtracking without requiring
destination co-ordinates. Moreover, in existing path finding algorithm, the
number of iterations required to find path is large. Hence, to overcome this,
an algorithm is proposed which reduces number of iterations required to
traverse the path. The proposed algorithm is hybrid of backtracking and a new
technique(modified 8- neighbor approach). The proposed algorithm can become
essential part in location based, network, gaming applications. grid traversal,
navigation, gaming applications, mobile robot and Artificial Intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02288</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02288</id><created>2015-04-09</created><authors><author><keyname>Follner</keyname><forenames>Andreas</forenames></author><author><keyname>Bodden</keyname><forenames>Eric</forenames></author></authors><title>ROPocop - Dynamic Mitigation of Code-Reuse Attacks</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control-flow attacks, usually achieved by exploiting a buffer-overflow
vulnerability, have been a serious threat to system security for over fifteen
years. Researchers have answered the threat with various mitigation techniques,
but nevertheless, new exploits that successfully bypass these technologies
still appear on a regular basis.
  In this paper, we propose ROPocop, a novel approach for detecting and
preventing the execution of injected code and for mitigating code-reuse attacks
such as return-oriented programming (RoP). ROPocop uses dynamic binary
instrumentation, requiring neither access to source code nor debug symbols or
changes to the operating system. It mitigates attacks by both monitoring the
program counter at potentially dangerous points and by detecting suspicious
program flows.
  We have implemented ROPocop for Windows x86 using PIN, a dynamic program
instrumentation framework from Intel. Benchmarks using the SPEC CPU2006 suite
show an average overhead of 2.4x, which is comparable to similar approaches,
which give weaker guarantees. Real-world applications show only an initially
noticeable input lag and no stutter. In our evaluation our tool successfully
detected all 11 of the latest real-world code-reuse exploits, with no false
alarms. Therefore, despite the overhead, it is a viable, temporary solution to
secure critical systems against exploits if a vendor patch is not yet
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02290</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02290</id><created>2015-04-09</created><authors><author><keyname>Narojczyk</keyname><forenames>Jakub</forenames></author><author><keyname>Wojciechowski</keyname><forenames>Krzysztof W.</forenames></author></authors><title>Computer simulation of Poisson's ratio of soft polydisperse discs at
  zero temperature</title><categories>physics.comp-ph cond-mat.soft cs.CE</categories><comments>8th International Conference on Intermolecular and Magnetic
  Interactions in Matter, Naleczow, POLAND, SEP 08-10, 2005</comments><journal-ref>Materials Science-Poland Volume: 24 Issue: 4 pp.921-927 Published:
  2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple algorithm is proposed for studies of structural and elastic
properties in the presence of structural disorder at zero temperature. The
algorithm is used to determine the properties of the polydisperse soft disc
system. It is shown that the Poisson's ratio of the system essentially depends
on the size polydispersity parameter - larger polydispersity implies larger
Poisson's ratio. In the presence of any size polidispersity the Poisson's ratio
increases also when the interactions between the particles tend to the hard
potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02293</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02293</id><created>2015-04-09</created><authors><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Karachontzitis</keyname><forenames>Sotiris</forenames></author><author><keyname>Berberidis</keyname><forenames>Kostas</forenames></author></authors><title>Spatial Domain Simultaneous Information and Power Transfer for MIMO
  Channels</title><categories>cs.IT math.IT</categories><comments>14 pages, 5 figures, Accepted for publication in IEEE Trans. on
  Wireless Communications</comments><doi>10.1109/TWC.2015.2416721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we theoretically investigate a new technique for simultaneous
information and power transfer (SWIPT) in multiple-input multiple-output (MIMO)
point-to-point with radio frequency energy harvesting capabilities. The
proposed technique exploits the spatial decomposition of the MIMO channel and
uses the eigenchannels either to convey information or to transfer energy. In
order to generalize our study, we consider channel estimation error in the
decomposition process and the interference between the eigenchannels. An
optimization problem that minimizes the total transmitted power subject to
maximum power per eigenchannel, information and energy constraints is
formulated as a mixed-integer nonlinear program and solved to optimality using
mixed-integer second-order cone programming. A near-optimal mixed-integer
linear programming solution is also developed with robust computational
performance. A polynomial complexity algorithm is further proposed for the
optimal solution of the problem when no maximum power per eigenchannel
constraints are imposed. In addition, a low polynomial complexity algorithm is
developed for the power allocation problem with a given eigenchannel
assignment, as well as a low-complexity heuristic for solving the eigenchannel
assignment problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02297</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02297</id><created>2015-04-09</created><updated>2015-11-05</updated><authors><author><keyname>Buckley</keyname><forenames>Mitchell</forenames></author></authors><title>Formalizing parity complexes</title><categories>math.CT cs.LO</categories><comments>Second version following revision for publication, 27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalise, in Coq, the opening sections of Parity Complexes [Street1991]
up to and including the all important excision of extremals algorithm. Parity
complexes describe the essential combinatorial structure exhibited by
simplexes, cubes and globes, that enable the construction of free
$\omega$-categories on such objects. The excision of extremals is a recursive
algorithm that presents every cell in such a category as a (unique) composite
of atomic cells. This is the sense in which the $\omega$-category is (freely)
generated from its atoms. Due to the complicated multi-dimensional nature of
this work, the detail of definitions and proofs can be hard to follow and
verify. Indeed, some corrections were required some years following the
original publication~\cite{Street1994}. Our formalisation verifies that all
cases of each result operate as stated. In particular, we indicate which
portions of the theory can be proved directly from definitions, and which
require more subtle and complex arguments. By identifying results that require
the most complicated proofs, we are able to investigate where this theory might
benefit from further study and which results need to be considered most
carefully in future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02298</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02298</id><created>2015-04-09</created><updated>2015-04-11</updated><authors><author><keyname>Dokuchaev</keyname><forenames>Nikolai</forenames></author></authors><title>A closed equation in time domain for band-limited extensions of
  one-sided sequences</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1208.3278</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggests a method of optimal band-limited extension of one-sided real
sequences (i.e., discrete time processes in deterministic setting) by
band-limited sequences.
  We derived a closed linear equation for the band-limited extension connecting
the past observations of the underlying process with the future values of the
band-limited process. This allows to skip calculation of the past values for
the band-limited approximating process; respectively, solution of the
extrapolation problem for the band-limited process is not required. The
equation is in the time domain. We established solvability and uniqueness of
this equation and investigated its regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02300</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02300</id><created>2015-04-09</created><authors><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Fairness for Non-Orthogonal Multiple Access in 5G Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 Figures, accepted in IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2015.2417119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In non-orthogonal multiple access (NOMA) downlink, multiple data flows are
superimposed in the power domain and user decoding is based on successive
interference cancellation. NOMA's performance highly depends on the power split
among the data flows and the associated power allocation (PA) problem. In this
letter, we study NOMA from a fairness standpoint and we investigate PA
techniques that ensure fairness for the downlink users under i) instantaneous
channel state information (CSI) at the transmitter, and ii) average CSI.
Although the formulated problems are non-convex, we have developed
low-complexity polynomial algorithms that yield the optimal solution in both
cases considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02305</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02305</id><created>2015-04-09</created><authors><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author><author><keyname>Han</keyname><forenames>Seungyeop</forenames></author></authors><title>Exploring Cyberbullying and Other Toxic Behavior in Team Competition
  Online Games</title><categories>cs.CY cs.HC cs.MM cs.SI physics.soc-ph</categories><comments>CHI'15</comments><acm-class>J.4; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore cyberbullying and other toxic behavior in team
competition online games. Using a dataset of over 10 million player reports on
1.46 million toxic players along with corresponding crowdsourced decisions, we
test several hypotheses drawn from theories explaining toxic behavior. Besides
providing large-scale, empirical based understanding of toxic behavior, our
work can be used as a basis for building systems to detect, prevent, and
counter-act toxic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02306</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02306</id><created>2015-04-09</created><updated>2016-02-15</updated><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author></authors><title>Optimal induced universal graphs and adjacency labeling for trees</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared at FOCS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists a graph $G$ with $O(n)$ nodes, where any forest of
$n$ nodes is a node-induced subgraph of $G$. Furthermore, for constant
arboricity $k$, the result implies the existence of a graph with $O(n^k)$ nodes
that contains all $n$-node graphs as node-induced subgraphs, matching a
$\Omega(n^k)$ lower bound. The lower bound and previously best upper bounds
were presented in Alstrup and Rauhe (FOCS'02). Our upper bounds are obtained
through a $\log_2 n +O(1)$ labeling scheme for adjacency queries in forests.
  We hereby solve an open problem being raised repeatedly over decades, e.g. in
Kannan, Naor, Rudich (STOC 1988), Chung (J. of Graph Theory 1990), Fraigniaud
and Korman (SODA 2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02307</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02307</id><created>2015-04-09</created><authors><author><keyname>Pitarokoilis</keyname><forenames>Antonios</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Optimal Detection in Training Assisted SIMO Systems with Phase Noise
  Impairments</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, IEEE ICC 2015, London, UK, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of optimal maximum likelihood detection in a
single user single-input multiple-output (SIMO) channel with phase noise at the
receiver is considered. The optimal detection rules under training are derived
for two operation modes, namely when the phase increments are fully correlated
among the $M$ receiver antennas (synchronous operation) and when they are
independent (non-synchronous operation). The phase noise increments are
parameterized by a very general distribution, which includes the Wiener phase
noise model as a special case. It is proven that phase noise creates a
symbol-error-rate (SER) floor for both operation modes. In the synchronous
operation this error floor is independent of $M$, while it goes to zero
exponentially with $M$ in the non-synchronous operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02317</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02317</id><created>2015-04-09</created><authors><author><keyname>Pu</keyname><forenames>Ye</forenames></author><author><keyname>Zeilinger</keyname><forenames>Melanie N.</forenames></author><author><keyname>Jones</keyname><forenames>Colin N.</forenames></author></authors><title>Quantization Design for Distributed Optimization</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of solving a distributed optimization problem using a
distributed computing platform, where the communication in the network is
limited: each node can only communicate with its neighbours and the channel has
a limited data-rate. A common technique to address the latter limitation is to
apply quantization to the exchanged information. We propose two distributed
optimization algorithms with an iteratively refining quantization design based
on the inexact proximal gradient method and its accelerated variant. We show
that if the parameters of the quantizers, i.e. the number of bits and the
initial quantization intervals, satisfy certain conditions, then the
quantization error is bounded by a linearly decreasing function and the
convergence of the distributed algorithms is guaranteed. Furthermore, we prove
that after imposing the quantization scheme, the distributed algorithms still
exhibit a linear convergence rate, and show complexity upper-bounds on the
number of iterations to achieve a given accuracy. Finally, we demonstrate the
performance of the proposed algorithms and the theoretical findings for solving
a distributed optimal control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02324</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02324</id><created>2015-04-09</created><authors><author><keyname>Velieva</keyname><forenames>T. R.</forenames></author><author><keyname>Korolkova</keyname><forenames>A. V.</forenames></author><author><keyname>Kulyabov</keyname><forenames>D. S.</forenames></author></authors><title>Designing Installations for Verification of the Model of Active Queue
  Management Discipline RED in the GNS3</title><categories>cs.NI cs.PF</categories><comments>in Russian; in English</comments><journal-ref>6th ICUMT, IEEE, 2014, pp. 570--577</journal-ref><doi>10.1109/ICUMT.2014.7002164</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The problem of RED-module mathematical model results verification, based on
GNS3 experimental stand, is discussed in this article. The experimental stand
consists of virtual Cisco router, traffic generator D-ITG and traffic receiver.
The process of construction of such stand is presented. Also, the interaction
between experimental stand and a computer of investigation in order to obtain
and analyze data from stand is revised. A stochastic model of the traffic
management RED type module was built. Verification of the model was carried out
on the NS-2 basis. However, we would like to conduct verification on a real
router. As a result was the task of designing an experimental stand. It was
decided to verify the clean RED algorithm based on Cisco router. For the
construction of the stand software package GNS3 (Graphical Network Simulator)
was chosen. Thus, the purpose of the study is to build on the GNS3 basis a
virtual stand consisting of a Cisco router, a traffic generator and a receiver.
A traffic generator D-ITG (Distributed Internet Traffic Generator) is used as.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02333</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02333</id><created>2015-04-09</created><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author></authors><title>Lower bounds on $q$-wise independence tails and applications to
  min-entropy condensers</title><categories>cs.CR math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel and sharp lower bounds for higher load moments in the
classical problem of mapping $M$ balls into $N$ bins by $q$-universal hashing,
specialized to the case when $M=N$. As a corollary we prove a tight counterpart
for the result about min-entropy condensers due to Dodis, Pietrzak and Wichs
(CRYPTO'14), which has found important applications in key derivation. It
states that condensing $k$ bits of min-entropy into a $k$-bit string
$\epsilon$-close to almost full min-entropy (precisely $
k-\log\log(1/\epsilon)$ bits of entropy) can be achieved by the use of
$q$-independent hashing with $q= \log(1/\epsilon)$. We prove that when given a
source of min-entropy $k$ and aiming at entropy loss $\ell = \log\log
(1/\epsilon) - 3$, the independence level $q=(1-o(1))\log(1/\epsilon)$ is
necessary (for small values of $\epsilon$), which almost matches the positive
result. Besides these asymptotic bounds, we provide clear hard bounds in terms
of Bell numbers and some numerical examples. Our technique is based on an
explicit representation of the load moments in terms of Stirling numbers, some
asymptotic estimates on Stirling numbers and a tricky application of the
Paley-Zygmund inequality. \keywords{ min-entropy condensers, key derivation,
balls and bins hashing, anti-concentration inequalities }
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02335</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02335</id><created>2015-04-09</created><authors><author><keyname>Thapen</keyname><forenames>Nicholas</forenames></author><author><keyname>Simmie</keyname><forenames>Donal</forenames></author><author><keyname>Hankin</keyname><forenames>Chris</forenames></author></authors><title>The Early Bird Catches The Term: Combining Twitter and News Data For
  Event Detection and Situational Awareness</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter updates now represent an enormous stream of information originating
from a wide variety of formal and informal sources, much of which is relevant
to real-world events. In this paper we adapt existing bio-surveillance
algorithms to detect localised spikes in Twitter activity corresponding to real
events with a high level of confidence. We then develop a methodology to
automatically summarise these events, both by providing the tweets which fully
describe the event and by linking to highly relevant news articles. We apply
our methods to outbreaks of illness and events strongly affecting sentiment. In
both case studies we are able to detect events verifiable by third party
sources and produce high quality summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02338</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02338</id><created>2015-04-09</created><updated>2015-06-05</updated><authors><author><keyname>Tuia</keyname><forenames>Devis</forenames></author><author><keyname>Camps-Valls</keyname><forenames>Gustau</forenames></author></authors><title>Kernel Manifold Alignment</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a kernel method for manifold alignment (KEMA) and domain
adaptation that can match an arbitrary number of data sources without needing
corresponding pairs, just few labeled examples in all domains. KEMA has
interesting properties: 1) it generalizes other manifold alignment methods, 2)
it can align manifolds of very different complexities, performing a sort of
manifold unfolding plus alignment, 3) it can define a domain-specific metric to
cope with multimodal specificities, 4) it can align data spaces of different
dimensionality, 5) it is robust to strong nonlinear feature deformations, and
6) it is closed-form invertible which allows transfer across-domains and data
synthesis. We also present a reduced-rank version for computational efficiency
and discuss the generalization performance of KEMA under Rademacher principles
of stability. KEMA exhibits very good performance over competing methods in
synthetic examples, visual object recognition and recognition of facial
expressions tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02340</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02340</id><created>2015-04-09</created><authors><author><keyname>Choi</keyname><forenames>Wongun</forenames></author></authors><title>Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the two key aspects of multiple target tracking
problem: 1) designing an accurate affinity measure to associate detections and
2) implementing an efficient and accurate (near) online multiple target
tracking algorithm. As the first contribution, we introduce a novel Aggregated
Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a
pair of temporally distant detections using long term interest point
trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust
affinity measure for estimating the likelihood of matching detections
regardless of the application scenarios. As another contribution, we present a
Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is
formulated as a data-association between targets and detections in a temporal
window, that is performed repeatedly at every frame. While being efficient,
NOMT achieves robustness via integrating multiple cues including ALFD metric,
target dynamics, appearance similarity, and long term trajectory regularization
into the model. Our ablative analysis verifies the superiority of the ALFD
metric over the other conventional affinity metrics. We run a comprehensive
experimental evaluation on two challenging tracking datasets, KITTI and MOT
datasets. The NOMT method combined with ALFD metric achieves the best accuracy
in both datasets with significant margins (about 10% higher MOTA) over the
state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02346</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02346</id><created>2015-04-09</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Optimal User Association for Massive MIMO Empowered Ultra-Dense Wireless
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>to be presented in IEEE ICC 2015 - Workshop on Advanced PHY and MAC
  Techniques for Super Dense Wireless Networks (ICC'15 - Workshops 13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra network densification and Massive MIMO are considered major 5G enablers
since they promise huge capacity gains by exploiting proximity, spectral and
spatial reuse benefits. Both approaches rely on increasing the number of access
elements per user, either through deploying more access nodes over an area or
increasing the number of antenna elements per access node. At the
network-level, optimal user-association for a densely and randomly deployed
network of Massive MIMO empowered access nodes must account for both channel
and load conditions. In this paper we formulate this complex problem, report
its computationally intractability and reformulate it to a plausible form,
amenable to acquire a global optimal solution with reasonable complexity. We
apply the proposed optimization model to typical ultra-dense outdoor small-cell
setups and demonstrate: (i) the significant impact of optimal user-association
to the achieved rate levels compared to a baseline strategy, and (ii) the
optimality of alternative network access element deployment strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02347</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02347</id><created>2015-04-09</created><updated>2015-04-18</updated><authors><author><keyname>Karabina</keyname><forenames>Koray</forenames></author></authors><title>Point Decomposition Problem in Binary Elliptic Curves</title><categories>cs.CR math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the point decomposition problem (PDP) in binary elliptic curves.
It is known that PDP in an elliptic curve group can be reduced to solving a
particular system of multivariate non-linear system of equations derived from
the so called Semaev summation polynomials. We modify the underlying system of
equations by introducing some auxiliary variables. We argue that the trade-off
between lowering the degree of Semaev polynomials and increasing the number of
variables is worth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02351</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02351</id><created>2015-04-09</created><authors><author><keyname>Hu</keyname><forenames>Guosheng</forenames></author><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Yi</keyname><forenames>Dong</forenames></author><author><keyname>Kittler</keyname><forenames>Josef</forenames></author><author><keyname>Christmas</keyname><forenames>William</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author></authors><title>When Face Recognition Meets with Deep Learning: an Evaluation of
  Convolutional Neural Networks for Face Recognition</title><categories>cs.CV cs.LG cs.NE</categories><comments>7 pages, 4 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning, in particular Convolutional Neural Network (CNN), has achieved
promising results in face recognition recently. However, it remains an open
question: why CNNs work well and how to design a 'good' architecture. The
existing works tend to focus on reporting CNN architectures that work well for
face recognition rather than investigate the reason. In this work, we conduct
an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a
common ground to make our work easily reproducible. Specifically, we use public
database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing
CNNs trained on private databases. We propose three CNN architectures which are
the first reported architectures trained using LFW data. This paper
quantitatively compares the architectures of CNNs and evaluate the effect of
different implementation choices. We identify several useful properties of
CNN-FRS. For instance, the dimensionality of the learned features can be
significantly reduced without adverse effect on face recognition accuracy. In
addition, traditional metric learning method exploiting CNN-learned features is
evaluated. Experiments show two crucial factors to good CNN-FRS performance are
the fusion of multiple CNNs and metric learning. To make our work reproducible,
source code and models will be made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02356</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02356</id><created>2015-04-09</created><authors><author><keyname>Mohedano</keyname><forenames>Eva</forenames></author><author><keyname>Salvador</keyname><forenames>Amaia</forenames></author><author><keyname>Porta</keyname><forenames>Sergi</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author><author><keyname>Healy</keyname><forenames>Graham</forenames></author><author><keyname>McGuinness</keyname><forenames>Kevin</forenames></author><author><keyname>O'Connor</keyname><forenames>Noel</forenames></author><author><keyname>Smeaton</keyname><forenames>Alan F.</forenames></author></authors><title>Exploring EEG for Object Detection and Retrieval</title><categories>cs.HC cs.CV cs.IR</categories><comments>This preprint is the full version of a short paper accepted in the
  ACM International Conference on Multimedia Retrieval (ICMR) 2015 (Shanghai,
  China)</comments><acm-class>H.1.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the potential for using Brain Computer Interfaces (BCI)
as a relevance feedback mechanism in content-based image retrieval. We
investigate if it is possible to capture useful EEG signals to detect if
relevant objects are present in a dataset of realistic and complex images. We
perform several experiments using a rapid serial visual presentation (RSVP) of
images at different rates (5Hz and 10Hz) on 8 users with different degrees of
familiarization with BCI and the dataset. We then use the feedback from the BCI
and mouse-based interfaces to retrieve localized objects in a subset of TRECVid
images. We show that it is indeed possible to detect such objects in complex
images and, also, that users with previous knowledge on the dataset or
experience with the RSVP outperform others. When the users have limited time to
annotate the images (100 seconds in our experiments) both interfaces are
comparable in performance. Comparing our best users in a retrieval task, we
found that EEG-based relevance feedback outperforms mouse-based feedback. The
realistic and complex image dataset differentiates our work from previous
studies on EEG for image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02357</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02357</id><created>2015-04-09</created><authors><author><keyname>Britz</keyname><forenames>Thomas</forenames></author><author><keyname>Shiromoto</keyname><forenames>Keisuke</forenames></author></authors><title>On the covering dimension of a linear code</title><categories>cs.IT math.IT</categories><msc-class>05B35, 06A07, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The critical exponent of a matroid is one of the important parameters in
matroid theory and is related to the Rota and Crapo's Critical Problem. This
paper introduces the covering dimension of a linear code over a finite field,
which is analogous to the critical exponent of a representable matroid. An
upper bound on the covering dimension is conjectured and nearly proven,
improving a classical bound for the critical exponent. Finally, a construction
is given of linear codes that attain equality in the covering dimension bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02358</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02358</id><created>2015-04-09</created><authors><author><keyname>Bernava</keyname><forenames>Carlo</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Maggiorini</keyname><forenames>Dario</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author><author><keyname>Ripamonti</keyname><forenames>Laura</forenames></author></authors><title>RDF annotation of Second Life objects: Knowledge Representation meets
  Social Virtual reality</title><categories>cs.AI cs.HC</categories><comments>The final publication is available at link.springer.com</comments><acm-class>H.5.1; I.2.4</acm-class><journal-ref>Computational and Mathematical Organization Theory (2014) Vol. 20,
  pages 20-35. ISSN 1381-298X</journal-ref><doi>10.1007/s10588-012-9148-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have designed and implemented an application running inside Second Life
that supports user annotation of graphical objects and graphical visualization
of concept ontologies, thus providing a formal, machine-accessible description
of objects. As a result, we offer a platform that combines the graphical
knowledge representation that is expected from a MUVE artifact with the
semantic structure given by the Resource Framework Description (RDF)
representation of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02360</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02360</id><created>2015-04-09</created><authors><author><keyname>Leng</keyname><forenames>Shiyang</forenames></author></authors><title>Multi-Objective Power Allocation for Energy Efficient Wireless
  Information and Power Transfer Systems</title><categories>cs.IT math.IT</categories><comments>Master thesis,Institute for Digital Communications, Germany,
  http://www.idc.lnt.de/en/forschung/energy-harvesting/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous wireless information and power transfer (SWIPT) provides a
promising solution for enabling perpetual wireless networks. As energy
efficiency (EE) is an im- portant evaluation of system performance, this thesis
studies energy-efficient resource allocation algorithm designs in SWIPT
systems. We first investigate the trade-off between the EE for information
transmission, the EE for power transfer, and the total transmit power in a
basic SWIPT system with separated receivers. A multi-objective optimization
problem is formulated under the constraint of maximum transmit power. We
propose an algorithm which achieves flexible resource allocation for energy
efficiencies maxi- mization and transmit power minimization. The trade-off
region of the system design objectives is shown in simulation results. Further,
we consider secure communication in a SWIPT system with power splitting
receivers. Artificial noise is injected to the com- munication channel to
combat the eavesdropping capability of potential eavesdroppers. A
power-efficient resource allocation algorithm is developed when multiple
legitimate information receivers and multi-antenna potential eavesdroppers
co-exist in the system. Simulation results demonstrate a significant
performance gain by the proposed optimal algorithm compared to suboptimal
baseline schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02362</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02362</id><created>2015-04-09</created><authors><author><keyname>Ivanov</keyname><forenames>V. K.</forenames></author><author><keyname>Palyukh</keyname><forenames>B. V.</forenames></author><author><keyname>Sotnikov</keyname><forenames>A. N.</forenames></author></authors><title>Approaches to the Intelligent Subject Search</title><categories>cs.IR</categories><comments>8 pages</comments><journal-ref>FedCSIS'2014 3 (2014) 13-20</journal-ref><doi>10.15439/2014F70 10.15439/978-83-60810-57-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents main results of the pilot study of approaches to the
subject information search based on automated semantic processing of mass
scientific and technical data. The authors focus on technology of building and
qualification of search queries with the following filtering and ranking of
search data. Software architecture, specific features of subject search and
research results application are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02363</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02363</id><created>2015-04-09</created><authors><author><keyname>Zhang</keyname><forenames>Ke</forenames></author><author><keyname>Pelechrinis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Lappas</keyname><forenames>Theodoros</forenames></author></authors><title>Analyzing and Modeling Special Offer Campaigns in Location-based Social
  Networks</title><categories>cs.SI</categories><comments>in The 9th International AAAI Conference on Web and Social Media
  (ICWSM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of mobile handheld devices in combination with the
technological advancements in mobile computing has led to a number of
innovative services that make use of the location information available on such
devices. Traditional yellow pages websites have now moved to mobile platforms,
giving the opportunity to local businesses and potential, near-by, customers to
connect. These platforms can offer an affordable advertisement channel to local
businesses. One of the mechanisms offered by location-based social networks
(LBSNs) allows businesses to provide special offers to their customers that
connect through the platform. We collect a large time-series dataset from
approximately 14 million venues on Foursquare and analyze the performance of
such campaigns using randomization techniques and (non-parametric) hypothesis
testing with statistical bootstrapping. Our main finding indicates that this
type of promotions are not as effective as anecdote success stories might
suggest. Finally, we design classifiers by extracting three different types of
features that are able to provide an educated decision on whether a special
offer campaign for a local business will succeed or not both in short and long
term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02366</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02366</id><created>2015-04-09</created><authors><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author><author><keyname>Grosan</keyname><forenames>Crina</forenames></author></authors><title>A Collection of Challenging Optimization Problems in Science,
  Engineering and Economics</title><categories>cs.NA cs.MS cs.NE math.AG math.NA math.OC physics.comp-ph</categories><comments>Accepted as an invited contribution to the special session on
  Evolutionary Computation for Nonlinear Equation Systems at the 2015 IEEE
  Congress on Evolutionary Computation (at Sendai International Center, Sendai,
  Japan, from 25th to 28th May, 2015.)</comments><report-no>ADP-15-9/T911</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Function optimization and finding simultaneous solutions of a system of
nonlinear equations (SNE) are two closely related and important optimization
problems. However, unlike in the case of function optimization in which one is
required to find the global minimum and sometimes local minima, a database of
challenging SNEs where one is required to find stationary points (extrama and
saddle points) is not readily available. In this article, we initiate building
such a database of important SNE (which also includes related function
optimization problems), arising from Science, Engineering and Economics. After
providing a short review of the most commonly used mathematical and
computational approaches to find solutions of such systems, we provide a
preliminary list of challenging problems by writing the Mathematical
formulation down, briefly explaning the origin and importance of the problem
and giving a short account on the currently known results, for each of the
problems. We anticipate that this database will not only help benchmarking
novel numerical methods for solving SNEs and function optimization problems but
also will help advancing the corresponding research areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02367</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02367</id><created>2015-04-09</created><updated>2015-04-30</updated><authors><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author><author><keyname>Wang</keyname><forenames>Jiasong</forenames></author></authors><title>Periodic power spectrum with applications in detection of latent
  periodicities in DNA sequences</title><categories>cs.DM cs.CE q-bio.QM</categories><msc-class>65T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent periodic elements in genomes play important roles in genomic
functions. Many complex periodic elements in genomes are difficult to be
detected by commonly used digital signal processing (DSP). We present a novel
method to compute the periodic power spectrum of a DNA sequence based on the
nucleotide distributions on periodic positions of the sequence. The method
directly calculates full periodic spectrum of a DNA sequence rather than
frequency spectrum by Fourier transform. The magnitude of the periodic power
spectrum reflects the strength of the periodicity signals, thus, the algorithm
can capture all the latent periodicities in DNA sequences. We apply this method
on detection of latent periodicities in different genome elements, including
exons and microsatellite DNA sequences. The results show that the method
minimizes the impact of spectral leakage, captures a much broader latent
periodicities in genomes, and outperforms the conventional Fourier transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02374</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02374</id><created>2015-04-09</created><authors><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharm</forenames></author></authors><title>Uplink Performance of Conventional and Massive MIMO Cellular Systems
  with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) networks, where the base
stations (BSs) are equipped with large number of antennas and serve a number of
users simultaneously, are very promising, but suffer from pilot contamination.
Despite its importance, delayed channel state information (CSI) due to user
mobility, being another degrading factor, lacks investigation in the
literature. Hence, we consider an uplink model, where each BS applies
zero-forcing decoder, accounting for both effects, but with the focal point on
the relative users' movement with regard to the BS antennas. In this setting,
analytical closed-form expressions for the sum-rate with finite number of BS
antennas, and the asymptotic limits with infinite number of BS antennas
epitomize the main contributions. In particular, the probability density
function of the signal-to-interference-plus-noise ratio and the ergodic
sum-rate are derived for any finite number of antennas. Insights of the impact
of the arising Doppler shift due to user mobility into the low signal-to-noise
ratio regime as well as the outage probability are obtained. Moreover,
asymptotic analysis performance results in terms of infinitely increasing
number of antennas, power, and both numbers of antennas and users (while their
ratio is fixed) are provided. The numerical results demonstrate the performance
loss in various Doppler shifts. An interesting observation is that massive MIMO
is favorable even in time-varying channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02382</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02382</id><created>2015-04-09</created><updated>2015-04-12</updated><authors><author><keyname>Basiri</keyname><forenames>Shahab</forenames></author><author><keyname>Ollila</keyname><forenames>Esa</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Robust, scalable and fast bootstrap method for analyzing large scale
  data</title><categories>stat.ME cs.IR cs.IT math.IT stat.CO stat.ML</categories><comments>This paper is submitted for publication in IEEE Transactions On
  Signal Processing, 8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of performing statistical inference for
large scale data sets i.e., Big Data. The volume and dimensionality of the data
may be so high that it cannot be processed or stored in a single computing
node. We propose a scalable, statistically robust and computationally efficient
bootstrap method, compatible with distributed processing and storage systems.
Bootstrap resamples are constructed with smaller number of distinct data points
on multiple disjoint subsets of data, similarly to the bag of little bootstrap
method (BLB) [1]. Then significant savings in computation is achieved by
avoiding the re-computation of the estimator for each bootstrap sample.
Instead, a computationally efficient fixed-point estimation equation is
analytically solved via a smart approximation following the Fast and Robust
Bootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use
of highly robust statistical methods in analyzing large scale data sets. The
favorable statistical properties of the method are established analytically.
Numerical examples demonstrate scalability, low complexity and robust
statistical performance of the method in analyzing large data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02395</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02395</id><created>2015-04-09</created><updated>2015-04-29</updated><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames></author><author><keyname>Yuan</keyname><forenames>Xiao</forenames></author></authors><title>Bridging the gap between general probabilistic theories and the
  device-independent framework for nonlocality and contextuality</title><categories>quant-ph cs.CR cs.GT cs.LO math-ph math.MP</categories><comments>61 pages, no figures, few typos corrected, several references added,
  presentation improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing quantum correlations in terms of information-theoretic
principles is a popular chapter of quantum foundations. Traditionally, the
principles used for this scope have been expressed in terms of conditional
probability distributions, specifying the probability that a black box produces
a certain output upon receiving a certain input. This framework is known as
&quot;device-independent&quot;. Another major chapter of quantum foundations is the
information-theoretic characterization of quantum theory as a whole, with its
sets of states and measurements, and with its allowed dynamics. The different
frameworks adopted for this scope are known under the umbrella term &quot;general
probabilistic theories&quot;. With only a few exceptions, the two programmes on
characterizing quantum correlations and characterizing quantum theory have so
far proceeded on separate tracks, each one developing its own methods and its
own agenda. However, both programmes share the same basic goal: a new and
better understanding of quantum mechanics in information-theoretic terms.
Considering such a common goal, it is quite striking that the connections
between the two programmes are still largely undeveloped. This paper aims at
bridging the gap, by presenting a &quot;Rosetta stone&quot; for the two frameworks and by
illustrating how the two programmes can benefit each other. As a case study, we
focus on two device-independent features known as Local Orthogonality (LO) and
Consistent Exclusivity (CE) and reconstruct them from a different set of
principles about pure orthogonal measurements, that is, measurements that
cannot be further refined and that identify states without error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02398</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02398</id><created>2015-04-09</created><authors><author><keyname>G&#xe1;lvez-L&#xf3;pez</keyname><forenames>Dorian</forenames></author><author><keyname>Salas</keyname><forenames>Marta</forenames></author><author><keyname>Tard&#xf3;s</keyname><forenames>Juan D.</forenames></author><author><keyname>Montiel</keyname><forenames>J. M. M.</forenames></author></authors><title>Real-time Monocular Object SLAM</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a real-time object-based SLAM system that leverages the largest
object database to date. Our approach comprises two main components: 1) a
monocular SLAM algorithm that exploits object rigidity constraints to improve
the map and find its real scale, and 2) a novel object recognition algorithm
based on bags of binary words, which provides live detections with a database
of 500 3D objects. The two components work together and benefit each other: the
SLAM algorithm accumulates information from the observations of the objects,
anchors object features to especial map landmarks and sets constrains on the
optimization. At the same time, objects partially or fully located within the
map are used as a prior to guide the recognition algorithm, achieving higher
recall. We evaluate our proposal on five real environments showing improvements
on the accuracy of the map and efficiency with respect to other
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02406</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02406</id><created>2015-04-09</created><authors><author><keyname>Temerinac-Ott</keyname><forenames>Maja</forenames></author><author><keyname>Naik</keyname><forenames>Armaghan W.</forenames></author><author><keyname>Murphy</keyname><forenames>Robert F.</forenames></author></authors><title>Deciding when to stop: Efficient stopping of active learning guided
  drug-target prediction</title><categories>q-bio.QM cs.LG stat.ML</categories><comments>This paper was selected for oral presentation at RECOMB 2015 and an
  abstract is published in the conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning has shown to reduce the number of experiments needed to
obtain high-confidence drug-target predictions. However, in order to actually
save experiments using active learning, it is crucial to have a method to
evaluate the quality of the current prediction and decide when to stop the
experimentation process. Only by applying reliable stoping criteria to active
learning, time and costs in the experimental process can be actually saved. We
compute active learning traces on simulated drug-target matrices in order to
learn a regression model for the accuracy of the active learner. By analyzing
the performance of the regression model on simulated data, we design stopping
criteria for previously unseen experimental matrices. We demonstrate on four
previously characterized drug effect data sets that applying the stopping
criteria can result in upto 40% savings of the total experiments for highly
accurate predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02408</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02408</id><created>2015-04-09</created><authors><author><keyname>Alkkiom&#xe4;ki</keyname><forenames>Ville</forenames></author><author><keyname>Smolander</keyname><forenames>Kari</forenames></author></authors><title>Observations of service identification from two enterprises</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.6, No.2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-oriented computing has created new requirements for information
systems development processes and methods. The adoption of service-oriented
development requires service identification methods matching the challenge in
enterprises. A wide variety of service identification methods (SIM) have been
proposed, but less attention has been paid to the actual requirements of the
methods. This paper provides an ethnographical look at challenges in service
identification based on data from 14 service identification sessions, providing
insight into the practice of service identification. The findings identified
two types of service identification sessions and the results can be used for
selecting the appropriate SIM based on the type of the session.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02411</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02411</id><created>2015-04-09</created><updated>2015-06-08</updated><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>Can Almost Everybody be Almost Happy? PCP for PPAD and the
  Inapproximability of Nash</title><categories>cs.CC cs.GT</categories><comments>Revision 2 derandomizes the main reduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conjecture that PPAD has a PCP-like complete problem, seeking a near
equilibrium in which all but very few players have very little incentive to
deviate. We show that, if one assumes that this problem requires exponential
time, several open problems in this area are settled. The most important
implication, proved via a &quot;birthday repetition&quot; reduction, is that the n^O(log
n) approximation scheme of [LMM03] for the Nash equilibrium of two-player games
is essentially optimum. Two other open problems in the area are resolved once
one assumes this conjecture, establishing that certain approximate equilibria
are PPAD-complete: Finding a relative approximation of two-player Nash
equilibria (without the well-supported restriction of [Das13]), and an
approximate competitive equilibrium with equal incomes [Bud11] with small
clearing error and near-optimal Gini coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02412</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02412</id><created>2015-04-09</created><updated>2015-04-10</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Phase Transitions in Spectral Community Detection of Large Noisy
  Networks</title><categories>cs.SI physics.data-an physics.soc-ph stat.ML</categories><comments>conference paper at IEEE ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the sensitivity of the spectral clustering based
community detection algorithm subject to a Erdos-Renyi type random noise model.
We prove phase transitions in community detectability as a function of the
external edge connection probability and the noisy edge presence probability
under a general network model where two arbitrarily connected communities are
interconnected by random external edges. Specifically, the community detection
performance transitions from almost perfect detectability to low detectability
as the inter-community edge connection probability exceeds some critical value.
We derive upper and lower bounds on the critical value and show that the bounds
are identical when the two communities have the same size. The phase transition
results are validated using network simulations. Using the derived expressions
for the phase transition threshold we propose a method for estimating this
threshold from observed data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02420</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02420</id><created>2015-04-09</created><authors><author><keyname>Cohen</keyname><forenames>D.</forenames></author><author><keyname>Crampton</keyname><forenames>J.</forenames></author><author><keyname>Gagarin</keyname><forenames>A.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author></authors><title>Algorithms for the workflow satisfiability problem engineered for
  counting constraints</title><categories>cs.DS cs.CR</categories><doi>10.1007/s10878-015-9877-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workflow satisfiability problem (WSP) asks whether there exists an
assignment of authorized users to the steps in a workflow specification that
satisfies the constraints in the specification. The problem is NP-hard in
general, but several subclasses of the problem are known to be fixed-parameter
tractable (FPT) when parameterized by the number of steps in the specification.
In this paper, we consider the WSP with user-independent counting constraints,
a large class of constraints for which the WSP is known to be FPT. We describe
an efficient implementation of an FPT algorithm for solving this subclass of
the WSP and an experimental evaluation of this algorithm. The algorithm
iteratively generates all equivalence classes of possible partial solutions
until, whenever possible, it finds a complete solution to the problem. We also
provide a reduction from a WSP instance to a pseudo-Boolean SAT instance. We
apply this reduction to the instances used in our experiments and solve the
resulting PB SAT problems using SAT4J, a PB SAT solver. We compare the
performance of our algorithm with that of SAT4J and discuss which of the two
approaches would be more effective in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02424</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02424</id><created>2015-04-09</created><updated>2015-07-30</updated><authors><author><keyname>Diamantoulakis</keyname><forenames>Panagiotis D.</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Big Data Analytics for Dynamic Energy Management in Smart Grids</title><categories>cs.DB</categories><comments>Published in ELSEVIER Big Data Research</comments><journal-ref>Big Data Research, vol. 2, no. 3, pp. 94-101, Sep. 2015</journal-ref><doi>10.1016/j.bdr.2015.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart electricity grid enables a two-way flow of power and data between
suppliers and consumers in order to facilitate the power flow optimization in
terms of economic efficiency, reliability and sustainability. This
infrastructure permits the consumers and the micro-energy producers to take a
more active role in the electricity market and the dynamic energy management
(DEM). The most important challenge in a smart grid (SG) is how to take
advantage of the users' participation in order to reduce the cost of power.
However, effective DEM depends critically on load and renewable production
forecasting. This calls for intelligent methods and solutions for the real-time
exploitation of the large volumes of data generated by a vast amount of smart
meters. Hence, robust data analytics, high performance computing, efficient
data network management, and cloud computing techniques are critical towards
the optimized operation of SGs. This research aims to highlight the big data
issues and challenges faced by the DEM employed in SG networks. It also
provides a brief description of the most commonly used data processing methods
in the literature, and proposes a promising direction for future research in
the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02437</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02437</id><created>2015-04-09</created><authors><author><keyname>Guo</keyname><forenames>Ruiqi</forenames></author><author><keyname>Zou</keyname><forenames>Chuhang</forenames></author><author><keyname>Hoiem</keyname><forenames>Derek</forenames></author></authors><title>Predicting Complete 3D Models of Indoor Scenes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major goal of vision is to infer physical models of objects, surfaces,
and their layout from sensors. In this paper, we aim to interpret indoor scenes
from one RGBD image. Our representation encodes the layout of walls, which must
conform to a Manhattan structure but is otherwise flexible, and the layout and
extent of objects, modeled with CAD-like 3D shapes. We represent both the
visible and occluded portions of the scene, producing a $complete$ 3D parse.
Such a scene interpretation is useful for robotics and visual reasoning, but
difficult to produce due to the well-known challenge of segmentation, the high
degree of occlusion, and the diversity of objects in indoor scene. We take a
data-driven approach, generating sets of potential object regions, matching to
regions in training images, and transferring and aligning associated 3D models
while encouraging fit to observations and overall consistency. We demonstrate
encouraging results on the NYU v2 dataset and highlight a variety of
interesting directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02440</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02440</id><created>2015-04-09</created><authors><author><keyname>Espada</keyname><forenames>Ana Rosario</forenames><affiliation>University of M&#xe1;laga</affiliation></author><author><keyname>Gallardo</keyname><forenames>Mar&#xed;a del Mar</forenames><affiliation>University of M&#xe1;laga</affiliation></author><author><keyname>Salmer&#xf3;n</keyname><forenames>Alberto</forenames><affiliation>University of M&#xe1;laga</affiliation></author><author><keyname>Merino</keyname><forenames>Pedro</forenames><affiliation>University of M&#xe1;laga</affiliation></author></authors><title>Using Model Checking to Generate Test Cases for Android Applications</title><categories>cs.SE</categories><comments>In Proceedings MBT 2015, arXiv:1504.01928</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015, pp. 7-21</journal-ref><doi>10.4204/EPTCS.180.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior of mobile devices is highly non deterministic and barely
predictable due to the interaction of the user with its applications. In
consequence, analyzing the correctness of applications running on a smartphone
involves dealing with the complexity of its environment. In this paper, we
propose the use of model-based testing to describe the potential behaviors of
users interacting with mobile applications. These behaviors are modeled by
composing specially-designed state machines. These composed state machines can
be exhaustively explored using a model checking tool to automatically generate
all possible user interactions. Each generated trace model checker can be
interpreted as a test case to drive a runtime analysis of actual applications.
We have implemented a tool that follows the proposed methodology to analyze
Android devices using the model checker Spin as the exhaustive generator of
test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02441</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02441</id><created>2015-04-09</created><authors><author><keyname>Gerhold</keyname><forenames>Marcus</forenames><affiliation>University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>Stoelinga</keyname><forenames>Mari&#xeb;lle</forenames><affiliation>University of Twente, Enschede, The Netherlands</affiliation></author></authors><title>Ioco Theory for Probabilistic Automata</title><categories>cs.LO</categories><comments>In Proceedings MBT 2015, arXiv:1504.01928</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015, pp. 23-40</journal-ref><doi>10.4204/EPTCS.180.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based testing (MBT) is a well-known technology, which allows for
automatic test case generation, execution and evaluation. To test
non-functional properties, a number of test MBT frameworks have been developed
to test systems with real-time, continuous behaviour, symbolic data and
quantitative system aspects. Notably, a lot of these frameworks are based on
Tretmans' classical input/output conformance (ioco) framework. However, a
model-based test theory handling probabilistic behaviour does not exist yet.
Probability plays a role in many different systems: unreliable communication
channels, randomized algorithms and communication protocols, service level
agreements pinning down up-time percentages, etc. Therefore, a probabilistic
test theory is of great practical importance. We present the ingredients for a
probabilistic variant of ioco and define the {\pi}oco relation, show that it
conservatively extends ioco and define the concepts of test case, execution and
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02442</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02442</id><created>2015-04-09</created><authors><author><keyname>Jorgensen</keyname><forenames>Paul C.</forenames><affiliation>Grand Valley State University</affiliation></author></authors><title>A Visual Formalism for Interacting Systems</title><categories>cs.SE cs.MA</categories><comments>In Proceedings MBT 2015, arXiv:1504.01928</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015, pp. 41-55</journal-ref><doi>10.4204/EPTCS.180.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interacting systems are increasingly common. Many examples pervade our
everyday lives: automobiles, aircraft, defense systems, telephone switching
systems, financial systems, national governments, and so on. Closer to computer
science, embedded systems and Systems of Systems are further examples of
interacting systems. Common to all of these is that some &quot;whole&quot; is made up of
constituent parts, and these parts interact with each other. By design, these
interactions are intentional, but it is the unintended interactions that are
problematic. The Systems of Systems literature uses the terms &quot;constituent
systems&quot; and &quot;constituents&quot; to refer to systems that interact with each other.
That practice is followed here. This paper presents a visual formalism, Swim
Lane Event-Driven Petri Nets, that is proposed as a basis for Model-Based
Testing (MBT) of interacting systems. In the absence of available tools, this
model can only support the offline form of Model-Based Testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02443</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02443</id><created>2015-04-09</created><authors><author><keyname>Lackner</keyname><forenames>Hartmut</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author><author><keyname>Schmidt</keyname><forenames>Martin</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author></authors><title>Potential Errors and Test Assessment in Software Product Line
  Engineering</title><categories>cs.SE</categories><comments>In Proceedings MBT 2015, arXiv:1504.01928</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015, pp. 57-72</journal-ref><doi>10.4204/EPTCS.180.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software product lines (SPL) are a method for the development of variant-rich
software systems. Compared to non-variable systems, testing SPLs is extensive
due to an increasingly amount of possible products. Different approaches exist
for testing SPLs, but there is less research for assessing the quality of these
tests by means of error detection capability. Such test assessment is based on
error injection into correct version of the system under test. However to our
knowledge, potential errors in SPL engineering have never been systematically
identified before. This article presents an overview over existing paradigms
for specifying software product lines and the errors that can occur during the
respective specification processes. For assessment of test quality, we leverage
mutation testing techniques to SPL engineering and implement the identified
errors as mutation operators. This allows us to run existing tests against
defective products for the purpose of test assessment. From the results, we
draw conclusions about the error-proneness of the surveyed SPL design paradigms
and how quality of SPL tests can be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02444</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02444</id><created>2015-04-09</created><authors><author><keyname>Kushik</keyname><forenames>Natalia</forenames><affiliation>Tomsk State University</affiliation></author><author><keyname>Yevtushenko</keyname><forenames>Nina</forenames><affiliation>Tomsk State University</affiliation></author></authors><title>Adaptive Homing is in P</title><categories>cs.FL cs.CC</categories><comments>In Proceedings MBT 2015, arXiv:1504.01928</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 180, 2015, pp. 73-78</journal-ref><doi>10.4204/EPTCS.180.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homing preset and adaptive experiments with Finite State Machines (FSMs) are
widely used when a non-initialized discrete event system is given for testing
and thus, has to be set to the known state at the first step. The length of a
shortest homing sequence is known to be exponential with respect to the number
of states for a complete observable nondeterministic FSM while the problem of
checking the existence of such sequence (Homing problem) is PSPACE-complete. In
order to decrease the complexity of related problems, one can consider adaptive
experiments when a next input to be applied to a system under experiment
depends on the output responses to the previous inputs. In this paper, we study
the problem of the existence of an adaptive homing experiment for complete
observable nondeterministic machines. We show that if such experiment exists
then it can be constructed with the use of a polynomial-time algorithm with
respect to the number of FSM states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02448</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02448</id><created>2015-04-09</created><authors><author><keyname>Rensink</keyname><forenames>Arend</forenames></author><author><keyname>Zambon</keyname><forenames>Eduardo</forenames></author></authors><title>Proceedings Graphs as Models</title><categories>cs.SE cs.DS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015</journal-ref><doi>10.4204/EPTCS.181</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the (first) Graphs as Models (GaM)
2015 workshop, held on 10-11 April 2015 in London, U.K., as a satellite
workshop of ETAPS 2015, the European Joint Conferences on Theory and Practice
of Software. This new workshop combines the strengths of two pre-existing
workshop series: GT-VMT (Graph Transformation and Visual Modelling Techniques)
and GRAPHITE (Graph Inspection and Traversal Engineering).
  Graphs are used as models in all areas of computer science: examples are
state space graphs, control flow graphs, syntax graphs, UML-type models of all
kinds, network layouts, social networks, dependency graphs, and so forth. Used
to model a particular phenomenon or process, graphs are then typically analysed
to find out properties of the modelled subject, or transformed to construct
other types of models.
  The workshop aimed at attracting and stimulating research on the techniques
for graph analysis, inspection and transformation, on a general level rather
than in any specific domain. In total, we received 15 submissions covering
several different areas. Of these 15 submissions, nine were eventually accepted
and appear in this volume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02462</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02462</id><created>2015-04-08</created><updated>2015-04-21</updated><authors><author><keyname>Paul</keyname><forenames>Arnab</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>A Group Theoretic Perspective on Unsupervised Deep Learning</title><categories>cs.LG cs.NE stat.ML</categories><comments>2-page version of arXiv:1412.6621 prepared for presentation at ICLR
  2015 workshop as required by ICLR PC). This version has some minor formatting
  changes as required by the conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why does Deep Learning work? What representations does it capture? How do
higher-order representations emerge? We study these questions from the
perspective of group theory, thereby opening a new approach towards a theory of
Deep learning.
  One factor behind the recent resurgence of the subject is a key algorithmic
step called {\em pretraining}: first search for a good generative model for the
input samples, and repeat the process one layer at a time. We show deeper
implications of this simple principle, by establishing a connection with the
interplay of orbits and stabilizers of group actions. Although the neural
networks themselves may not form groups, we show the existence of {\em shadow}
groups whose elements serve as close approximations.
  Over the shadow groups, the pre-training step, originally introduced as a
mechanism to better initialize a network, becomes equivalent to a search for
features with minimal orbits. Intuitively, these features are in a way the {\em
simplest}. Which explains why a deep learning network learns simple features
first. Next, we show how the same principle, when repeated in the deeper
layers, can capture higher order representations, and why representation
complexity increases as the layers get deeper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02463</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02463</id><created>2015-04-09</created><updated>2015-06-04</updated><authors><author><keyname>Small</keyname><forenames>Christopher</forenames></author><author><keyname>Becker</keyname><forenames>Richard</forenames></author><author><keyname>C&#xe1;ceres</keyname><forenames>Ram&#xf3;n</forenames></author><author><keyname>Urbanek</keyname><forenames>Simon</forenames></author></authors><title>Earthquakes, Hurricanes, and Mobile Communication Patterns in the New
  York Metro Area: Collective Behavior during Extreme Events</title><categories>cs.SI cs.CY</categories><comments>24 pages, 10 figures, added 1 sentence to abstract, corrected font
  issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use wireless voice-call and text-message volumes to quantify
spatiotemporal communication patterns in the New York Metro area before,
during, and after the Virginia earthquake and Hurricane Irene in 2011. The
earthquake produces an instantaneous and pervasive increase in volume and a
~90-minute temporal disruption to both call and text volume patterns, but call
volume anomalies are much larger. The magnitude of call volume anomaly
diminishes with distance from earthquake epicenter, with multiple clusters of
high response in Manhattan. The hurricane produces a two-day, spatially varying
disruption to normal call and text volume patterns. In most coastal areas, call
volumes dropped anomalously in the afternoon before the hurricane's arrival,
but text volumes showed a much less consistent pattern. These spatial patterns
suggest partial, but not full, compliance with evacuation orders for low-lying
areas. By helping us understand how people behave in actual emergencies,
wireless data patterns may assist network operators and emergency planners who
want to provide the best possible services to the community. We have been
careful to preserve privacy throughout this work by using only anonymous and
aggregate data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02485</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02485</id><created>2015-04-09</created><authors><author><keyname>Peng</keyname><forenames>Xingchao</forenames></author><author><keyname>Sun</keyname><forenames>Baochen</forenames></author><author><keyname>Ali</keyname><forenames>Karim</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>What Do Deep CNNs Learn About Objects?</title><categories>cs.CV</categories><comments>2 pages workshop paper. arXiv admin note: substantial text overlap
  with arXiv:1412.7122</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks learn extremely powerful image
representations, yet most of that power is hidden in the millions of deep-layer
parameters. What exactly do these parameters represent? Recent work has started
to analyse CNN representations, finding that, e.g., they are invariant to some
2D transformations Fischer et al. (2014), but are confused by particular types
of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how
invariant are CNNs to object-class variations caused by 3D shape, pose, and
photorealism?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02490</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02490</id><created>2015-04-09</created><authors><author><keyname>Jaech</keyname><forenames>Aaron</forenames></author><author><keyname>Ostendorf</keyname><forenames>Mari</forenames></author></authors><title>Leveraging Twitter for Low-Resource Conversational Speech Language
  Modeling</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applications involving conversational speech, data sparsity is a limiting
factor in building a better language model. We propose a simple,
language-independent method to quickly harvest large amounts of data from
Twitter to supplement a smaller training set that is more closely matched to
the domain. The techniques lead to a significant reduction in perplexity on
four low-resource languages even though the presence on Twitter of these
languages is relatively small. We also find that the Twitter text is more
useful for learning word classes than the in-domain text and that use of these
word classes leads to further reductions in perplexity. Additionally, we
introduce a method of using social and textual information to prioritize the
download queue during the Twitter crawling. This maximizes the amount of useful
data that can be collected, impacting both perplexity and vocabulary coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02491</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02491</id><created>2015-04-09</created><authors><author><keyname>Shabtai</keyname><forenames>Revital Hollander</forenames></author><author><keyname>Roditty</keyname><forenames>Yehuda</forenames></author></authors><title>Line-Broadcasting in Complete k-Trees</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A line-broadcasting model in a connected graph $G=(V,E)$, $|V|=n$, is a model
in which one vertex, called the {\it originator} of the broadcast holds a
message that has to be transmitted to all vertices of the graph through
placement of a series of calls over the graph. In this model, an informed
vertex can transmit a message through a path of any length in a single time
unit, as long as two transmissions do not use the same edge at the same time.
Farley \cite{f} has shown that the process is completed within at most $\lceil
\log_{2}n \rceil$ time units from any originator in a tree (and thus in any
connected undirected graph). and that the cost of broadcasting one message from
any vertex is at most $(n-1) \lceil \log_{2}n \rceil$.
  In this paper, we present lower and upper bounds for the cost to broadcast
one message in a complete $k-$tree, from any vertex using the line-broadcasting
model. We prove that if $B(u)$ is the minimum cost to broadcast in a graph
$G=(V,E)$ from a vertex $u \in V$ using the line-broadcasting model, then
$(1+o(1))n \le B(u) \le (2+o(1))n$, where $u$ is any vertex in a complete
$k$-tree. Furthermore, for certain conditions, $B(u) \le (2-o(1))n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02502</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02502</id><created>2015-04-09</created><authors><author><keyname>Pfifer</keyname><forenames>Harald</forenames></author><author><keyname>Seiler</keyname><forenames>Peter</forenames></author></authors><title>An Overview of Integral Quadratic Constraints for Delayed Nonlinear and
  Parameter-Varying Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general framework is presented for analyzing the stability and performance
of nonlinear and linear parameter varying (LPV) time delayed systems. First,
the input/output behavior of the time delay operator is bounded in the
frequency domain by integral quadratic constraints (IQCs). A constant delay is
a linear, time-invariant system and this leads to a simple, intuitive
interpretation for these frequency domain constraints. This simple
interpretation is used to derive new IQCs for both constant and varying delays.
Second, the performance of nonlinear and LPV delayed systems is bounded using
dissipation inequalities that incorporate IQCs. This step makes use of recent
results that show, under mild technical conditions, that an IQC has an
equivalent representation as a finite-horizon time-domain constraint. Numerical
examples are provided to demonstrate the effectiveness of the method for both
class of systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02504</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02504</id><created>2015-04-09</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>Using Spectral Radius Ratio for Node Degree to Analyze the Evolution of
  Scale Free Networks and Small World Networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 8 figures, Second International Conference on Computer
  Science and Information Technology, (COSIT-2015), Geneva, Switzerland, March
  21-22, 2015</comments><doi>10.5121/csit.2015.50603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show the evaluation of the spectral radius for node degree
as the basis to analyze the variation in the node degrees during the evolution
of scale-free networks and small-world networks. Spectral radius is the
principal eigenvalue of the adjacency matrix of a network graph and spectral
radius ratio for node degree is the ratio of the spectral radius and the
average node degree. We observe a very high positive correlation between the
spectral radius ratio for node degree and the coefficient of variation of node
degree (ratio of the standard deviation of node degree and average node
degree). We show how the spectral radius ratio for node degree can be used as
the basis to tune the operating parameters of the evolution models for
scale-free networks and small-world networks as well as evaluate the impact of
the number of links added per node introduced during the evolution of a
scale-free network and evaluate the impact of the probability of rewiring
during the evolution of a small-world network from a regular network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02511</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02511</id><created>2015-04-09</created><authors><author><keyname>Ch&#xe1;vez-Angeles</keyname><forenames>Manuel G.</forenames></author><author><keyname>S&#xe1;nchez-Medina</keyname><forenames>Patricia S.</forenames></author></authors><title>Application of the war of attrition game to the analysis of intellectual
  property disputes</title><categories>cs.CY q-fin.GN</categories><journal-ref>International Journal of Game Theory and Technology (IJGTT),
  Vol.3, No.1, March 2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In many developing countries intellectual property infringement and the
commerce of pirate goods is an entrepreneurial activity. Digital piracy is very
often the only media for having access to music, cinema, books and software. At
the same time, bio-prospecting and infringement of indigenous knowledge rights
by international consortiums is usual in places with high biodiversity. In
these arenas transnational actors interact with local communities. Accusations
of piracy often go both ways. This article analyzes the case of southeast
Mexico. Using a war of attrition game theory model it explains different
situations of intellectual property rights piracy and protection. It analyzes
different levels of interaction and institutional settings from the global to
the very local. The article proposes free IP zones as a solution of IP
disputes. The formation of technological local clusters through Free
Intellectual Property Zones (FIPZ) would allow firms to copy and share de facto
public domain content for developing new products inside the FIPZ. Enforcement
of intellectual property could be pursuit outside of the FIPZ. FIPZ are
envisioned as a new type of a sui generis intellectual property regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02514</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02514</id><created>2015-04-09</created><authors><author><keyname>Leung</keyname><forenames>Samantha</forenames></author><author><keyname>Lui</keyname><forenames>Edward</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Stronger Impossibility Results for Strategy-Proof Voting with i.i.d.
  Beliefs</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.5827</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic Gibbard-Satterthwaite theorem says that every strategy-proof
voting rule with at least three possible candidates must be dictatorial. In
\cite{McL11}, McLennan showed that a similar impossibility result holds even if
we consider a weaker notion of strategy-proofness where voters believe that the
other voters' preferences are i.i.d.~(independent and identically distributed):
If an anonymous voting rule (with at least three candidates) is strategy-proof
w.r.t.~all i.i.d.~beliefs and is also Pareto efficient, then the voting rule
must be a random dictatorship. In this paper, we strengthen McLennan's result
by relaxing Pareto efficiency to $\epsilon$-Pareto efficiency where Pareto
efficiency can be violated with probability $\epsilon$, and we further relax
$\epsilon$-Pareto efficiency to a very weak notion of efficiency which we call
$\epsilon$-super-weak unanimity. We then show the following: If an anonymous
voting rule (with at least three candidates) is strategy-proof w.r.t.~all
i.i.d.~beliefs and also satisfies $\epsilon$-super-weak unanimity, then the
voting rule must be $O(\epsilon)$-close to random dictatorship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02517</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02517</id><created>2015-04-09</created><updated>2015-04-13</updated><authors><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Kumar</keyname><forenames>Uday</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Survey of Operating Systems for the IoT Environment</title><categories>cs.OS</categories><comments>5 pages, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a comprehensive survey of the various operating systems
available for the Internet of Things environment. At first the paper introduces
the various aspects of the operating systems designed for the IoT environment
where resource constraint poses a huge problem for the operation of the general
OS designed for the various computing devices. The latter part of the paper
describes the various OS available for the resource constraint IoT environment
along with the various platforms each OS supports, the software development
kits available for the development of applications in the respective OS along
with the various protocols implemented in these OS for the purpose of
communication and networking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02518</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02518</id><created>2015-04-09</created><updated>2015-04-15</updated><authors><author><keyname>Goroshin</keyname><forenames>Ross</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Unsupervised Feature Learning from Temporal Data</title><categories>cs.CV cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1412.6056</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art classification and detection algorithms rely on
supervised training. In this work we study unsupervised feature learning in the
context of temporally coherent video data. We focus on feature learning from
unlabeled video data, using the assumption that adjacent video frames contain
semantically similar information. This assumption is exploited to train a
convolutional pooling auto-encoder regularized by slowness and sparsity. We
establish a connection between slow feature learning to metric learning and
show that the trained encoder can be used to define a more temporally and
semantically coherent metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02523</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02523</id><created>2015-04-09</created><updated>2015-04-18</updated><authors><author><keyname>Alu&#xe7;</keyname><forenames>G&#xfc;ne&#x15f;</forenames></author><author><keyname>&#xd6;zsu</keyname><forenames>M. Tamer</forenames></author><author><keyname>Daudjee</keyname><forenames>Khuzaima</forenames></author></authors><title>Clustering RDF Databases Using Tunable-LSH</title><categories>cs.DB</categories><comments>Fixed typos, updated related work section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Resource Description Framework (RDF) is a W3C standard for representing
graph-structured data, and SPARQL is the standard query language for RDF.
Recent advances in Information Extraction, Linked Data Management and the
Semantic Web have led to a rapid increase in both the volume and the variety of
RDF data that are publicly available. As businesses start to capitalize on RDF
data, RDF data management systems are being exposed to workloads that are far
more diverse and dynamic than what they were designed to handle. Consequently,
there is a growing need for developing workload-adaptive and self-tuning RDF
data management systems. To realize this vision, we introduce a fast and
efficient method for dynamically clustering records in an RDF data management
system. Specifically, we assume nothing about the workload upfront, but as
SPARQL queries are executed, we keep track of records that are co-accessed by
the queries in the workload and physically cluster them. To decide dynamically
(hence, in constant-time) where a record needs to be placed in the storage
system, we develop a new locality-sensitive hashing (LSH) scheme, Tunable-LSH.
Using Tunable-LSH, records that are co-accessed across similar sets of queries
can be hashed to the same or nearby physical pages in the storage system. What
sets Tunable-LSH apart from existing LSH schemes is that it can auto-tune to
achieve the aforementioned clustering objective with high accuracy even when
the workloads change. Experimental evaluation of Tunable-LSH in our prototype
RDF data management system, chameleon-db, as well as in a standalone hashtable
shows significant end-to-end improvements over existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02526</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02526</id><created>2015-04-09</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Rabani</keyname><forenames>Yuval</forenames></author><author><keyname>Schulman</keyname><forenames>Leonard J.</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Learning Arbitrary Statistical Mixtures of Discrete Distributions</title><categories>cs.LG cs.DS</categories><comments>23 pages. Preliminary version in the Proceeding of the 47th ACM
  Symposium on the Theory of Computing (STOC15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning from unlabeled samples very general
statistical mixture models on large finite sets. Specifically, the model to be
learned, $\vartheta$, is a probability distribution over probability
distributions $p$, where each such $p$ is a probability distribution over $[n]
= \{1,2,\dots,n\}$. When we sample from $\vartheta$, we do not observe $p$
directly, but only indirectly and in very noisy fashion, by sampling from $[n]$
repeatedly, independently $K$ times from the distribution $p$. The problem is
to infer $\vartheta$ to high accuracy in transportation (earthmover) distance.
  We give the first efficient algorithms for learning this mixture model
without making any restricting assumptions on the structure of the distribution
$\vartheta$. We bound the quality of the solution as a function of the size of
the samples $K$ and the number of samples used. Our model and results have
applications to a variety of unsupervised learning scenarios, including
learning topic models and collaborative filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02530</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02530</id><created>2015-04-09</created><authors><author><keyname>Moharrer</keyname><forenames>A.</forenames></author><author><keyname>Wei</keyname><forenames>S.</forenames></author><author><keyname>Amariucai</keyname><forenames>G. T.</forenames></author><author><keyname>Deng</keyname><forenames>J.</forenames></author></authors><title>Classifying Unrooted Gaussian Trees under Privacy Constraints</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures, part of this work is submitted to IEEE Globecom
  2015, San Diego, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, our objective is to find out how topological and algebraic
properties of unrooted Gaussian tree models determine their security
robustness, which is measured by our proposed max-min information (MaMI)
metric. Such metric quantifies the amount of common randomness extractable
through public discussion between two legitimate nodes under an eavesdropper
attack. We show some general topological properties that the desired max-min
solutions shall satisfy. Under such properties, we develop conditions under
which comparable trees are put together to form partially ordered sets
(posets). Each poset contains the most favorable structure as the poset leader,
and the least favorable structure. Then, we compute the Tutte-like polynomial
for each tree in a poset in order to assign a polynomial to any tree in a
poset. Moreover, we propose a novel method, based on restricted integer
partitions, to effectively enumerate all poset leaders. The results not only
help us understand the security strength of different Gaussian trees, which is
critical when we evaluate the information leakage issues for various jointly
Gaussian distributed measurements in networks, but also provide us both an
algebraic and a topological perspective in grasping some fundamental properties
of such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02531</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02531</id><created>2015-04-09</created><updated>2015-05-17</updated><authors><author><keyname>Gao</keyname><forenames>Zhimin</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhou</keyname><forenames>Luping</forenames></author><author><keyname>Zhang</keyname><forenames>Jianjia</forenames></author></authors><title>HEp-2 Cell Image Classification with Deep Convolutional Neural Networks</title><categories>cs.CV</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitate
the diagnosis of many autoimmune diseases. This paper presents an automatic
framework for this classification task, by utilizing the deep convolutional
neural networks (CNNs) which have recently attracted intensive attention in
visual recognition. This paper elaborates the important components of this
framework, discusses multiple key factors that impact the efficiency of
training a deep CNN, and systematically compares this framework with the
well-established image classification models in the literature. Experiments on
benchmark datasets show that i) the proposed framework can effectively
outperform existing models by properly applying data augmentation; ii) our
CNN-based framework demonstrates excellent adaptability across different
datasets, which is highly desirable for classification under varying laboratory
settings. Our system is ranked high in the cell image classification
competition hosted by ICPR 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02532</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02532</id><created>2015-04-09</created><updated>2015-09-15</updated><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Optimal Design of Switched Networks of Positive Linear Systems via
  Geometric Programming</title><categories>math.OC cs.SI</categories><comments>Accepted for publications in IEEE Transactions on Control of Network
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an optimization framework to design a network of
positive linear systems whose structure switches according to a Markov process.
The optimization framework herein proposed allows the network designer to
optimize the coupling elements of a directed network, as well as the dynamics
of the nodes in order to maximize the stabilization rate of the network and/or
the disturbance rejection against an exogenous input. The cost of implementing
a particular network is modeled using posynomial cost functions, which allow
for a wide variety of modeling options. In this context, we show that the
cost-optimal network design can be efficiently found using geometric
programming in polynomial time. We illustrate our results with a practical
problem in network epidemiology, namely, the cost-optimal stabilization of the
spread of a disease over a time-varying contact network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02536</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02536</id><created>2015-04-09</created><updated>2015-05-06</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Equivocations, Exponents and Second-Order Coding Rates under Various
  R\'enyi Information Measures</title><categories>cs.IT cs.CR math.IT</categories><comments>40 pages, 7 figures; To be presented at the 2015 International
  Symposium on Information Theory; Submitted to the IEEE Transactions on
  Information Theory; v2: fixed typos and added some clarifications to the
  proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we evaluate the asymptotics of equivocations, their exponents
as well as their second-order coding rates under various R\'enyi information
measures. Specifically, we consider the effect of applying a hash function on a
source and we quantify the level of non-uniformity and dependence of the
compressed source from another correlated source when the number of copies of
the sources is large. Unlike previous works that use Shannon information
measures to quantify randomness, information or uniformity, in this paper, we
define our security measures in terms of a more general class of information
measures---the R\'enyi information measures and their Gallager-type
counterparts. A special case of these R\'enyi information measure is the class
of Shannon information measures. We prove tight asymptotic results for the
security measures and their exponential rates of decay. We also prove bounds on
the second-order asymptotics and show that these bounds match when the
magnitudes of the second-order coding rates are large. We do so by establishing
new classes non-asymptotic bounds on the equivocation and evaluating these
bounds using various probabilistic limit theorems asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02547</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02547</id><created>2015-04-10</created><updated>2015-04-14</updated><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Dolev</keyname><forenames>Danny</forenames></author></authors><title>Byzantine Agreement with Optimal Early Stopping, Optimal Resilience and
  Polynomial Complexity</title><categories>cs.DC</categories><comments>full version of STOC 2015 abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first protocol that solves Byzantine agreement with optimal
early stopping ($\min\{f+2,t+1\}$ rounds) and optimal resilience ($n&gt;3t$) using
polynomial message size and computation.
  All previous approaches obtained sub-optimal results and used resolve rules
that looked only at the immediate children in the EIG (\emph{Exponential
Information Gathering}) tree. At the heart of our solution are new resolve
rules that look at multiple layers of the EIG tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02549</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02549</id><created>2015-04-10</created><authors><author><keyname>Machicao</keyname><forenames>Jeaneth</forenames></author><author><keyname>Baetens</keyname><forenames>Jan M.</forenames></author><author><keyname>Marco</keyname><forenames>Anderson G.</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir M.</forenames></author></authors><title>A dynamical systems approach to the discrimination of the modes of
  operation of cryptographic systems</title><categories>cs.CR math.DS</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidence of signatures associated with cryptographic modes of operation is
established. Motivated by some analogies between cryptographic and dynamical
systems, in particular with chaos theory, we propose an algorithm based on
Lyapunov exponents of discrete dynamical systems to estimate the divergence
among ciphertexts as the encryption algorithm is applied iteratively. The
results allow to distinguish among six modes of operation, namely ECB, CBC,
OFB, CFB, CTR and PCBC using DES, IDEA, TEA and XTEA block ciphers of 64 bits,
as well as AES, RC6, Twofish, Seed, Serpent and Camellia block ciphers of 128
bits. Furthermore, the proposed methodology enables a classification of modes
of operation of cryptographic systems according to their strength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02555</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02555</id><created>2015-04-10</created><authors><author><keyname>Saini</keyname><forenames>Prerna</forenames></author><author><keyname>Bansal</keyname><forenames>Ankit</forenames></author><author><keyname>Sharma</keyname><forenames>Abhishek</forenames></author></authors><title>Time Critical Multitasking for Multicore Microcontroller using XMOS Kit</title><categories>cs.DC</categories><comments>18 pages, 18 figure, 9 tables,</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents the research work on multicore microcontrollers using
parallel, and time critical programming for the embedded systems. Due to the
high complexity and limitations, it is very hard to work on the application
development phase on such architectures. The experimental results mentioned in
the paper are based on xCORE multicore microcontroller form XMOS. The paper
also imitates multi-tasking and parallel programming for the same platform. The
tasks assigned to multiple cores are executed simultaneously, which saves the
time and energy. The relative study for multicore processor and multicore
controller concludes that micro architecture based controller having multiple
cores illustrates better performance in time critical multi-tasking
environment. The research work mentioned here not only illustrates the
functionality of multicore microcontroller, but also express the novel
technique of programming, profiling and optimization on such platforms in real
time environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02564</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02564</id><created>2015-04-10</created><authors><author><keyname>Bhattacharya</keyname><forenames>Anup</forenames></author><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Faster Algorithms for the Constrained k-means Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical center based clustering problems such as
$k$-means/median/center assume that the optimal clusters satisfy the locality
property that the points in the same cluster are close to each other. A number
of clustering problems arise in machine learning where the optimal clusters do
not follow such a locality property. Consider a variant of the $k$-means
problem that may be regarded as a general version of such problems. Here, the
optimal clusters $O_1, ..., O_k$ are an arbitrary partition of the dataset and
the goal is to output $k$-centers $c_1, ..., c_k$ such that the objective
function $\sum_{i=1}^{k} \sum_{x \in O_{i}} ||x - c_{i}||^2$ is minimized. It
is not difficult to argue that any algorithm (without knowing the optimal
clusters) that outputs a single set of $k$ centers, will not behave well as far
as optimizing the above objective function is concerned. However, this does not
rule out the existence of algorithms that output a list of such $k$ centers
such that at least one of these $k$ centers behaves well. Given an error
parameter $\varepsilon &gt; 0$, let $\ell$ denote the size of the smallest list of
$k$-centers such that at least one of the $k$-centers gives a $(1+\varepsilon)$
approximation w.r.t. the objective function above. In this paper, we show an
upper bound on $\ell$ by giving a randomized algorithm that outputs a list of
$2^{\tilde{O}(k/\varepsilon)}$ $k$-centers. We also give a closely matching
lower bound of $2^{\tilde{\Omega}(k/\sqrt{\varepsilon})}$. Moreover, our
algorithm runs in time $O \left(n d \cdot 2^{\tilde{O}(k/\varepsilon)}
\right)$. This is a significant improvement over the previous result of Ding
and Xu who gave an algorithm with running time $O \left(n d \cdot (\log{n})^{k}
\cdot 2^{poly(k/\varepsilon)} \right)$ and output a list of size $O
\left((\log{n})^k \cdot 2^{poly(k/\varepsilon)} \right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02576</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02576</id><created>2015-04-10</created><authors><author><keyname>Bauer</keyname><forenames>Johann</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Highly-cited papers in Library and Information Science (LIS): Authors,
  institutions, and network structures</title><categories>cs.DL</categories><comments>accepted for publication in the Journal of the Association for
  Information Science and Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a follow-up to the highly-cited authors list published by Thomson Reuters
in June 2014, we analyze the top-1% most frequently cited papers published
between 2002 and 2012 included in the Web of Science (WoS) subject category
&quot;Information Science &amp; Library Science.&quot; 798 authors contributed to 305 top-1%
publications; these authors were employed at 275 institutions. The authors at
Harvard University contributed the largest number of papers, when the addresses
are whole-number counted. However, Leiden University leads the ranking, if
fractional counting is used.
  Twenty-three of the 798 authors were also listed as most highly-cited authors
by Thomson Reuters in June 2014 (http://highlycited.com/). Twelve of these 23
authors were involved in publishing four or more of the 305 papers under study.
Analysis of co-authorship relations among the 798 highly-cited scientists shows
that co-authorships are based on common interests in a specific topic. Three
topics were important between 2002 and 2012: (1) collection and exploitation of
information in clinical practices, (2) the use of internet in public
communication and commerce, and (3) scientometrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02577</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02577</id><created>2015-04-10</created><updated>2015-04-13</updated><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Ma</keyname><forenames>Cong</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Jing</keyname><forenames>Yu</forenames></author><author><keyname>Li</keyname><forenames>Juanzi</forenames></author></authors><title>Panther: Fast Top-k Similarity Search in Large Networks</title><categories>cs.SI</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating similarity between vertices is a fundamental issue in network
analysis across various domains, such as social networks and biological
networks. Methods based on common neighbors and structural contexts have
received much attention. However, both categories of methods are difficult to
scale up to handle large networks (with billions of nodes). In this paper, we
propose a sampling method that provably and accurately estimates the similarity
between vertices. The algorithm is based on a novel idea of random path, and an
extended method is also presented, to enhance the structural similarity when
two vertices are completely disconnected. We provide theoretical proofs for the
error-bound and confidence of the proposed algorithm. We perform extensive
empirical study and show that our algorithm can obtain top-k similar vertices
for any vertex in a network approximately 300x faster than state-of-the-art
methods. We also use identity resolution and structural hole spanner finding,
two important applications in social networks, to evaluate the accuracy of the
estimated similarities. Our experimental results demonstrate that the proposed
algorithm achieves clearly better performance than several alternative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02578</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02578</id><created>2015-04-10</created><authors><author><keyname>Terei</keyname><forenames>David</forenames></author><author><keyname>Levy</keyname><forenames>Amit</forenames></author></authors><title>Blade: A Data Center Garbage Collector</title><categories>cs.DC</categories><comments>14 pages, 9 figures</comments><acm-class>D.1.3; D.3.3; D.3.4; D.4.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of high-performance distributed systems are written in
garbage collected languages. This removes a large class of harmful bugs from
these systems. However, it also introduces high tail-latency do to garbage
collection pause times. We address this problem through a new technique of
garbage collection avoidance which we call Blade. Blade is an API between the
collector and application developer that allows developers to leverage existing
failure recovery mechanisms in distributed systems to coordinate collection and
bound the latency impact. We describe Blade and implement it for the Go
programming language. We also investigate two different systems that utilize
Blade, a HTTP load-balancer and the Raft consensus algorithm. For the
load-balancer, we eliminate any latency introduced by the garbage collector,
for Raft, we bound the latency impact to a single network round-trip, (48
{\mu}s in our setup). In both cases, latency at the tail using Blade is up to
three orders of magnitude better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02583</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02583</id><created>2015-04-10</created><authors><author><keyname>Bruhn</keyname><forenames>Henning</forenames></author><author><keyname>Joos</keyname><forenames>Felix</forenames></author></authors><title>A stronger bound for the strong chromatic index</title><categories>math.CO cs.DM math.PR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove $\chi_s'(G)\leq 1.93 \Delta(G)^2$ for graphs of sufficiently large
maximum degree where $\chi_s'(G)$ is the strong chromatic index of $G$. This
improves an old bound of Molloy and Reed. As a by-product, we present a
Talagrand-type inequality where it is allowed to exclude unlikely bad outcomes
that would otherwise render the inequality unusable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02587</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02587</id><created>2015-04-10</created><authors><author><keyname>Valmari</keyname><forenames>Antti</forenames></author></authors><title>Stop It, and Be Stubborn!</title><categories>cs.LO</categories><msc-class>68Q60, 68Q85, 68N30</msc-class><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system is AG EF terminating, if and only if from every reachable state, a
terminal state is reachable. This publication argues that it is beneficial for
both catching non-progress errors and stubborn set state space reduction to try
to make verification models AG EF terminating. An incorrect mutual exclusion
algorithm is used as an example. The error does not manifest itself, unless the
first action of the customers is modelled differently from other actions. An
appropriate method is to add an alternative first action that models the
customer stopping for good. This method typically makes the model AG EF
terminating. If the model is AG EF terminating, then the basic strong stubborn
set method preserves safety and some progress properties without any additional
condition for solving the ignoring problem. Furthermore, whether the model is
AG EF terminating can be checked efficiently from the reduced state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02590</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02590</id><created>2015-04-10</created><authors><author><keyname>Ismkhan</keyname><forenames>Hassan</forenames></author><author><keyname>Zamanifar</keyname><forenames>Kamran</forenames></author></authors><title>Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic
  Algorithm, Using Symmetric Travelling Salesman Problem</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1209.5339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Travelling Salesman Problem (TSP) is one of the most famous optimization
problems. The Genetic Algorithm (GA) is one of metaheuristics that have been
applied to TSP. The Crossover and mutation operators are two important elements
of GA. There are many TSP solver crossover operators. In this paper, we state
implementation of some recent TSP solver crossovers at first and then we use
each of them in GA to solve some Symmetric TSP (STSP) instances and finally
compare their effects on speed and accuracy of presented GA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02597</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02597</id><created>2015-04-10</created><authors><author><keyname>Valmari</keyname><forenames>Antti</forenames></author></authors><title>A State Space Tool for Models Expressed In C++ (tool paper)</title><categories>cs.SE</categories><msc-class>68Q60, 68Q85, 68N30</msc-class><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This publication introduces A State Space Exploration Tool that is based on
representing the model under verification as a piece of C++ code that obeys
certain conventions. Its name is ASSET. Model checking takes place by compiling
the model and the tool together, and executing the result. This approach
facilitates very fast execution of the transitions of the model. On the other
hand, the use of stubborn sets and symmetries requires that either the modeller
or a preprocessor tool analyses the model at a syntactic level and expresses
stubborn set obligation rules and the symmetry mapping as suitable C++
functions. The tool supports the detection of illegal deadlocks, safety errors,
and may progress errors. It also partially supports the detection of must
progress errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02602</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02602</id><created>2015-04-10</created><updated>2016-02-17</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Algebraic solution of a tropical optimization problem via matrix
  sparsification with application to scheduling</title><categories>math.OC cs.SY</categories><comments>31 pages, 3 figures, an application example added</comments><msc-class>65K10 (Primary), 15A80, 65F50, 90B35, 90C48 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimization problem is considered in the framework of tropical algebra to
minimize a nonlinear objective function defined on vectors over an idempotent
semifield, and calculated using multiplicative conjugate transposition. We find
the minimum of the function, and give a partial solution, which explicitly
represents a subset of solution vectors. We characterize all solutions by a
system of simultaneous equation and inequality, and show that the solution set
is closed under vector addition and scalar multiplication. A matrix
sparsification technique is proposed to extend the partial solution, and then
to obtain a complete solution described as a family of subsets. We offer a
backtracking procedure that generates all members of the family, and derive an
explicit representation for the complete solution. The results obtained are
illustrated with illuminating examples and graphical representations. We apply
the results to solve a real-world problem drawn from just-in-time scheduling,
and give a numerical example of the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02603</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02603</id><created>2015-04-10</created><authors><author><keyname>Kahl</keyname><forenames>Wolfram</forenames><affiliation>McMaster University</affiliation></author></authors><title>A Simple Parallel Implementation of Interaction Nets in Haskell</title><categories>cs.PL</categories><comments>In Proceedings DCM 2014, arXiv:1504.01927</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 179, 2015, pp. 33-47</journal-ref><doi>10.4204/EPTCS.179.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their &quot;inherent parallelism&quot;, interaction nets have since their
introduction been considered as an attractive implementation mechanism for
functional programming. We show that a simple highly-concurrent implementation
in Haskell can achieve promising speed-ups on multiple cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02605</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02605</id><created>2015-04-10</created><authors><author><keyname>Fischer</keyname><forenames>Johannes</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>K&#xf6;ppl</keyname><forenames>Dominik</forenames></author></authors><title>Lempel Ziv Computation In Small Space (LZ-CISS)</title><categories>cs.DS</categories><comments>Full Version of CPM 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For both the Lempel Ziv 77- and 78-factorization we propose algorithms
generating the respective factorization using $(1+\epsilon) n \lg n + O(n)$
bits (for any positive constant $\epsilon \le 1$) working space (including the
space for the output) for any text of size \$n\$ over an integer alphabet in
$O(n / \epsilon^{2})$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02608</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02608</id><created>2015-04-10</created><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Asymptotic Estimates in Information Theory with Non-Vanishing Error
  Probabilities</title><categories>cs.IT math.IT</categories><comments>Further comments welcome</comments><journal-ref>Foundations and Trends in Communications and Information Theory,
  Vol. 11, Nos. 1-2, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This monograph presents a unified treatment of single- and multi-user
problems in Shannon's information theory where we depart from the requirement
that the error probability decays asymptotically in the blocklength. Instead,
the error probabilities for various problems are bounded above by a
non-vanishing constant and the spotlight is shone on achievable coding rates as
functions of the growing blocklengths. This represents the study of asymptotic
estimates with non-vanishing error probabilities.
  In Part I, after reviewing the fundamentals of information theory, we discuss
Strassen's seminal result for binary hypothesis testing where the type-I error
probability is non-vanishing and the rate of decay of the type-II error
probability with growing number of independent observations is characterized.
In Part II, we use this basic hypothesis testing result to develop second- and
sometimes, even third-order asymptotic expansions for point-to-point
communication. Finally in Part III, we consider network information theory
problems for which the second-order asymptotics are known. These problems
include some classes of channels with random state, the multiple-encoder
distributed lossless source coding (Slepian-Wolf) problem and special cases of
the Gaussian interference and multiple-access channels. Finally, we discuss
avenues for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02609</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02609</id><created>2015-04-10</created><authors><author><keyname>Zheng</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Yang</keyname><forenames>Baohua</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Uhlig</keyname><forenames>Steve</forenames></author></authors><title>LazyCtrl: Scalable Network Control for Cloud Data Centers</title><categories>cs.NI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of software defined networking enables flexible, reliable and
feature-rich control planes for data center networks. However, the tight
coupling of centralized control and complete visibility leads to a wide range
of issues among which scalability has risen to prominence. To address this, we
present LazyCtrl, a novel hybrid control plane design for data center networks
where network control is carried out by distributed control mechanisms inside
independent groups of switches while complemented with a global controller. Our
design is motivated by the observation that data center traffic is usually
highly skewed and thus edge switches can be grouped according to traffic
locality. LazyCtrl aims at bringing laziness to the global controller by
dynamically devolving most of the control tasks to independent switch groups to
process frequent intra-group events near datapaths while handling rare
inter-group or other specified events by the controller. We implement LazyCtrl
and build a prototype based on Open vSwich and Floodlight. Trace-driven
experiments on our prototype show that an effective switch grouping is easy to
maintain in multi-tenant clouds and the central controller can be significantly
shielded by staying lazy, with its workload reduced by up to 82%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02610</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02610</id><created>2015-04-10</created><authors><author><keyname>Wijs</keyname><forenames>Anton</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Confluence Detection for Transformations of Labelled Transition Systems</title><categories>cs.LO cs.DS</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><acm-class>G.2.2</acm-class><journal-ref>EPTCS 181, 2015, pp. 1-15</journal-ref><doi>10.4204/EPTCS.181.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of complex component software systems can be made more
manageable by first creating an abstract model and then incrementally adding
details. Model transformation is an approach to add such details in a
controlled way. In order for model transformation systems to be useful, it is
crucial that they are confluent, i.e. that when applied on a given model, they
will always produce a unique output model, independent of the order in which
rules of the system are applied on the input. In this work, we consider
Labelled Transition Systems (LTSs) to reason about the semantics of models, and
LTS transformation systems to reason about model transformations. In related
work, the problem of confluence detection has been investigated for general
graph structures. We observe, however, that confluence can be detected more
efficiently in special cases where the graphs have particular structural
properties. In this paper, we present a number of observations to detect
confluence of LTS transformation systems, and propose both a new confluence
detection algorithm and a conflict resolution algorithm based on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02611</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02611</id><created>2015-04-10</created><authors><author><keyname>Heu&#xdf;ner</keyname><forenames>Alexander</forenames></author><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author><author><keyname>Corrodi</keyname><forenames>Claudio</forenames></author><author><keyname>Morandi</keyname><forenames>Benjamin</forenames></author></authors><title>Towards Practical Graph-Based Verification for an Object-Oriented
  Concurrency Model</title><categories>cs.SE cs.DC cs.LO cs.PL</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 32-47</journal-ref><doi>10.4204/EPTCS.181.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To harness the power of multi-core and distributed platforms, and to make the
development of concurrent software more accessible to software engineers,
different object-oriented concurrency models such as SCOOP have been proposed.
Despite the practical importance of analysing SCOOP programs, there are
currently no general verification approaches that operate directly on program
code without additional annotations. One reason for this is the multitude of
partially conflicting semantic formalisations for SCOOP (either in theory or
by-implementation). Here, we propose a simple graph transformation system (GTS)
based run-time semantics for SCOOP that grasps the most common features of all
known semantics of the language. This run-time model is implemented in the
state-of-the-art GTS tool GROOVE, which allows us to simulate, analyse, and
verify a subset of SCOOP programs with respect to deadlocks and other
behavioural properties. Besides proposing the first approach to verify SCOOP
programs by automatic translation to GTS, we also highlight our experiences of
applying GTS (and especially GROOVE) for specifying semantics in the form of a
run-time model, which should be transferable to GTS models for other concurrent
languages and libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02612</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02612</id><created>2015-04-10</created><authors><author><keyname>Vallet</keyname><forenames>Jason</forenames><affiliation>LaBRI, Univ. Bordeaux, France</affiliation></author><author><keyname>Kirchner</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>Inria, Bordeaux, France</affiliation></author><author><keyname>Pinaud</keyname><forenames>Bruno</forenames><affiliation>LaBRI, Univ. Bordeaux, France</affiliation></author><author><keyname>Melan&#xe7;on</keyname><forenames>Guy</forenames><affiliation>LaBRI, Univ. Bordeaux, France</affiliation></author></authors><title>A Visual Analytics Approach to Compare Propagation Models in Social
  Networks</title><categories>cs.SI cs.LO</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 65-79</journal-ref><doi>10.4204/EPTCS.181.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous propagation models describing social influence in social networks
can be found in the literature. This makes the choice of an appropriate model
in a given situation difficult. Selecting the most relevant model requires the
ability to objectively compare them. This comparison can only be made at the
cost of describing models based on a common formalism and yet independent from
them. We propose to use graph rewriting to formally describe propagation
mechanisms as local transformation rules applied according to a strategy. This
approach makes sense when it is supported by a visual analytics framework
dedicated to graph rewriting. The paper first presents our methodology to
describe some propagation models as a graph rewriting problem. Then, we
illustrate how our visual analytics framework allows to interactively
manipulate models, and underline their differences based on measures computed
on simulation traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02613</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02613</id><created>2015-04-10</created><authors><author><keyname>Hoch</keyname><forenames>Nicklas</forenames><affiliation>Volkswagen AG</affiliation></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames><affiliation>University of Pisa</affiliation></author><author><keyname>Sammartino</keyname><forenames>Matteo</forenames><affiliation>Radboud University</affiliation></author></authors><title>Dynamic Programming on Nominal Graphs</title><categories>cs.LO cs.SE</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 80-96</journal-ref><doi>10.4204/EPTCS.181.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many optimization problems can be naturally represented as (hyper) graphs,
where vertices correspond to variables and edges to tasks, whose cost depends
on the values of the adjacent variables. Capitalizing on the structure of the
graph, suitable dynamic programming strategies can select certain orders of
evaluation of the variables which guarantee to reach both an optimal solution
and a minimal size of the tables computed in the optimization process. In this
paper we introduce a simple algebraic specification with parallel composition
and restriction whose terms up to structural axioms are the graphs mentioned
above. In addition, free (unrestricted) vertices are labelled with variables,
and the specification includes operations of name permutation with finite
support. We show a correspondence between the well-known tree decompositions of
graphs and our terms. If an axiom of scope extension is dropped, several
(hierarchical) terms actually correspond to the same graph. A suitable
graphical structure can be found, corresponding to every hierarchical term.
Evaluating such a graphical structure in some target algebra yields a dynamic
programming strategy. If the target algebra satisfies the scope extension
axiom, then the result does not depend on the particular structure, but only on
the original graph. We apply our approach to the parking optimization problem
developed in the ASCENS e-mobility case study, in collaboration with
Volkswagen. Dynamic programming evaluations are particularly interesting for
autonomic systems, where actual behavior often consists of propagating local
knowledge to obtain global knowledge and getting it back for local decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02614</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02614</id><created>2015-04-10</created><authors><author><keyname>Kulcs&#xe1;r</keyname><forenames>G&#xe9;za</forenames><affiliation>Technische Universit&#xe4;t Darmstadt Real-Time Systems Lab</affiliation></author><author><keyname>Deckwerth</keyname><forenames>Frederik</forenames><affiliation>Technische Universit&#xe4;t Darmstadt Real-Time Systems Lab</affiliation></author><author><keyname>Lochau</keyname><forenames>Malte</forenames><affiliation>Technische Universit&#xe4;t Darmstadt Real-Time Systems Lab</affiliation></author><author><keyname>Varr&#xf3;</keyname><forenames>Gergely</forenames><affiliation>Technische Universit&#xe4;t Darmstadt Real-Time Systems Lab</affiliation></author><author><keyname>Sch&#xfc;rr</keyname><forenames>Andy</forenames><affiliation>Technische Universit&#xe4;t Darmstadt Real-Time Systems Lab</affiliation></author></authors><title>Improved Conflict Detection for Graph Transformation with Attributes</title><categories>cs.SE cs.LO</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 97-112</journal-ref><doi>10.4204/EPTCS.181.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In graph transformation, a conflict describes a situation where two
alternative transformations cannot be arbitrarily serialized. When enriching
graphs with attributes, existing conflict detection techniques typically report
a conflict whenever at least one of two transformations manipulates a shared
attribute. In this paper, we propose an improved, less conservative condition
for static conflict detection of graph transformation with attributes by
explicitly taking the semantics of the attribute operations into account. The
proposed technique is based on symbolic graphs, which extend the traditional
notion of graphs by logic formulas used for attribute handling. The approach is
proven complete, i.e., any potential conflict is guaranteed to be detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02615</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02615</id><created>2015-04-10</created><authors><author><keyname>Radwan</keyname><forenames>Marwan</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Heckel</keyname><forenames>Reiko</forenames><affiliation>University of Leicester</affiliation></author></authors><title>Detecting and Refactoring Operational Smells within the Domain Name
  System</title><categories>cs.NI cs.CR</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 113-128</journal-ref><doi>10.4204/EPTCS.181.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Domain Name System (DNS) is one of the most important components of the
Internet infrastructure. DNS relies on a delegation-based architecture, where
resolution of names to their IP addresses requires resolving the names of the
servers responsible for those names. The recursive structures of the inter
dependencies that exist between name servers associated with each zone are
called dependency graphs. System administrators' operational decisions have far
reaching effects on the DNSs qualities. They need to be soundly made to create
a balance between the availability, security and resilience of the system. We
utilize dependency graphs to identify, detect and catalogue operational bad
smells. Our method deals with smells on a high-level of abstraction using a
consistent taxonomy and reusable vocabulary, defined by a DNS Operational
Model. The method will be used to build a diagnostic advisory tool that will
detect configuration changes that might decrease the robustness or security
posture of domain names before they become into production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02616</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02616</id><created>2015-04-10</created><authors><author><keyname>Moreau</keyname><forenames>Luc</forenames><affiliation>University of Southampton</affiliation></author></authors><title>Aggregation by Provenance Types: A Technique for Summarising Provenance
  Graphs</title><categories>cs.DB cs.DS</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 129-144</journal-ref><doi>10.4204/EPTCS.181.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As users become confronted with a deluge of provenance data, dedicated
techniques are required to make sense of this kind of information. We present
Aggregation by Provenance Types, a provenance graph analysis that is capable of
generating provenance graph summaries. It proceeds by converting provenance
paths up to some length k to attributes, referred to as provenance types, and
by grouping nodes that have the same provenance types. The summary also
includes numeric values representing the frequency of nodes and edges in the
original graph. A quantitative evaluation and a complexity analysis show that
this technique is tractable; with small values of k, it can produce useful
summaries and can help detect outliers. We illustrate how the generated
summaries can further be used for conformance checking and visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02621</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02621</id><created>2015-04-10</created><authors><author><keyname>Bak</keyname><forenames>Christopher</forenames></author><author><keyname>Faulkner</keyname><forenames>Glyn</forenames></author><author><keyname>Plump</keyname><forenames>Detlef</forenames></author><author><keyname>Runciman</keyname><forenames>Colin</forenames></author></authors><title>A Reference Interpreter for the Graph Programming Language GP 2</title><categories>cs.PL</categories><comments>In Proceedings GaM 2015, arXiv:1504.02448</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 181, 2015, pp. 48-64</journal-ref><doi>10.4204/EPTCS.181.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GP 2 is an experimental programming language for computing by graph
transformation. An initial interpreter for GP 2, written in the functional
language Haskell, provides a concise and simply structured reference
implementation. Despite its simplicity, the performance of the interpreter is
sufficient for the comparative investigation of a range of test programs. It
also provides a platform for the development of more sophisticated
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02622</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02622</id><created>2015-04-10</created><authors><author><keyname>Czarnecki</keyname><forenames>Wojciech Marian</forenames></author><author><keyname>J&#xf3;zefowicz</keyname><forenames>Rafa&#x142;</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Maximum Entropy Linear Manifold for Learning Discriminative
  Low-dimensional Representation</title><categories>cs.LG</categories><comments>submitted to ECMLPKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning is currently a very hot topic in modern machine
learning, mostly due to the great success of the deep learning methods. In
particular low-dimensional representation which discriminates classes can not
only enhance the classification procedure, but also make it faster, while
contrary to the high-dimensional embeddings can be efficiently used for visual
based exploratory data analysis.
  In this paper we propose Maximum Entropy Linear Manifold (MELM), a
multidimensional generalization of Multithreshold Entropy Linear Classifier
model which is able to find a low-dimensional linear data projection maximizing
discriminativeness of projected classes. As a result we obtain a linear
embedding which can be used for classification, class aware dimensionality
reduction and data visualization. MELM provides highly discriminative 2D
projections of the data which can be used as a method for constructing robust
classifiers.
  We provide both empirical evaluation as well as some interesting theoretical
properties of our objective function such us scale and affine transformation
invariance, connections with PCA and bounding of the expected balanced accuracy
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02627</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02627</id><created>2015-04-10</created><authors><author><keyname>Klein</keyname><forenames>Felix</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>What are Strategies in Delay Games? Borel Determinacy for Games with
  Lookahead</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate determinacy of delay games with Borel winning conditions,
infinite-duration two-player games in which one player may delay her moves to
obtain a lookahead on her opponent's moves.
  First, we prove determinacy of such games with respect to a fixed evolution
of the lookahead. However, strategies in such games may depend on information
about the evolution. Thus, we introduce different notions of universal
strategies for both players, which are evolution-independent, and determine the
exact amount of information a universal strategy needs about the history of a
play and the evolution of the lookahead to be winning. In particular, we show
that delay games with Borel winning conditions are determined with respect to
universal strategies. Finally, we consider decidability problems, e.g., &quot;Does a
player have a universal winning strategy for delay games with a given winning
condition?&quot;, for omega-regular and omega-context-free winning conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02644</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02644</id><created>2015-04-10</created><updated>2015-09-10</updated><authors><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author></authors><title>OneMax in Black-Box Models with Several Restrictions</title><categories>cs.NE cs.DS</categories><comments>This is the full version of a paper accepted to GECCO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Black-box complexity studies lower bounds for the efficiency of
general-purpose black-box optimization algorithms such as evolutionary
algorithms and other search heuristics. Different models exist, each one being
designed to analyze a different aspect of typical heuristics such as the memory
size or the variation operators in use. While most of the previous works focus
on one particular such aspect, we consider in this work how the combination of
several algorithmic restrictions influence the black-box complexity. Our
testbed are so-called OneMax functions, a classical set of test functions that
is intimately related to classic coin-weighing problems and to the board game
Mastermind.
  We analyze in particular the combined memory-restricted ranking-based
black-box complexity of OneMax for different memory sizes. While its isolated
memory-restricted as well as its ranking-based black-box complexity for bit
strings of length $n$ is only of order $n/\log n$, the combined model does not
allow for algorithms being faster than linear in $n$, as can be seen by
standard information-theoretic considerations. We show that this linear bound
is indeed asymptotically tight. Similar results are obtained for other memory-
and offspring-sizes. Our results also apply to the (Monte Carlo) complexity of
OneMax in the recently introduced elitist model, in which only the best-so-far
solution can be kept in the memory. Finally, we also provide improved lower
bounds for the complexity of OneMax in the regarded models.
  Our result enlivens the quest for natural evolutionary algorithms optimizing
OneMax in $o(n \log n)$ iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02648</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02648</id><created>2015-04-10</created><updated>2015-12-07</updated><authors><author><keyname>Lindeberg</keyname><forenames>Tony</forenames></author></authors><title>Time-causal and time-recursive spatio-temporal receptive fields</title><categories>cs.CV q-bio.NC</categories><comments>39 pages, 12 figures, 5 tables in Journal of Mathematical Imaging and
  Vision, published online Dec 2015</comments><doi>10.1007/s10851-015-0613-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improved model and theory for time-causal and time-recursive
spatio-temporal receptive fields, based on a combination of Gaussian receptive
fields over the spatial domain and first-order integrators or equivalently
truncated exponential filters coupled in cascade over the temporal domain.
  Compared to previous spatio-temporal scale-space formulations in terms of
non-enhancement of local extrema or scale invariance, these receptive fields
are based on different scale-space axiomatics over time by ensuring
non-creation of new local extrema or zero-crossings with increasing temporal
scale. Specifically, extensions are presented about (i) parameterizing the
intermediate temporal scale levels, (ii) analysing the resulting temporal
dynamics, (iii) transferring the theory to a discrete implementation, (iv)
computing scale-normalized spatio-temporal derivative expressions for
spatio-temporal feature detection and (v) computational modelling of receptive
fields in the lateral geniculate nucleus (LGN) and the primary visual cortex
(V1) in biological vision.
  We show that by distributing the intermediate temporal scale levels according
to a logarithmic distribution, we obtain much faster temporal response
properties (shorter temporal delays) compared to a uniform distribution.
Specifically, these kernels converge very rapidly to a limit kernel possessing
true self-similar scale-invariant properties over temporal scales, thereby
allowing for true scale invariance over variations in the temporal scale,
although the underlying temporal scale-space representation is based on a
discretized temporal scale parameter.
  We show how scale-normalized temporal derivatives can be defined for these
time-causal scale-space kernels and how the composed theory can be used for
computing basic types of scale-normalized spatio-temporal derivative
expressions in a computationally efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02651</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02651</id><created>2015-04-10</created><updated>2015-07-17</updated><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Reachability analysis of first-order definable pushdown systems</title><categories>cs.FL cs.LO</categories><comments>to appear in CSL'15</comments><acm-class>F.1.1; F.2.2; F.3.1; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study pushdown systems where control states, stack alphabet, and
transition relation, instead of being finite, are first-order definable in a
fixed countably-infinite structure. We show that the reachability analysis can
be addressed with the well-known saturation technique for the wide class of
oligomorphic structures. Moreover, for the more restrictive homogeneous
structures, we are able to give concrete complexity upper bounds. We show ample
applicability of our technique by presenting several concrete examples of
homogeneous structures, subsuming, with optimal complexity, known results from
the literature. We show that infinitely many such examples of homogeneous
structures can be obtained with the classical wreath product construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02656</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02656</id><created>2015-04-10</created><authors><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author><author><keyname>Ruggiero</keyname><forenames>Carlos Antonio</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>A complex network approach to cloud computing</title><categories>physics.soc-ph cs.DC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has become an important means to speed up computing. One
problem influencing heavily the performance of such systems is the choice of
nodes as servers responsible for executing the users' tasks. In this article we
report how complex networks can be used to model such a problem. More
specifically, we investigate the performance of the processing respectively to
cloud systems underlain by Erdos-Renyi and Barabasi-Albert topology containing
two servers. Cloud networks involving two communities not necessarily of the
same size are also considered in our analysis. The performance of each
configuration is quantified in terms of two indices: the cost of communication
between the user and the nearest server, and the balance of the distribution of
tasks between the two servers. Regarding the latter index, the ER topology
provides better performance than the BA case for smaller average degrees and
opposite behavior for larger average degrees. With respect to the cost, smaller
values are found in the BA topology irrespective of the average degree. In
addition, we also verified that it is easier to find good servers in the ER
than in BA. Surprisingly, balance and cost are not too much affected by the
presence of communities. However, for a well-defined community network, we
found that it is important to assign each server to a different community so as
to achieve better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02671</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02671</id><created>2015-04-10</created><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author></authors><title>Longest Common Extensions in Sublinear Space</title><categories>cs.DS</categories><comments>An extended abstract of this paper has been accepted to CPM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The longest common extension problem (LCE problem) is to construct a data
structure for an input string $T$ of length $n$ that supports LCE$(i,j)$
queries. Such a query returns the length of the longest common prefix of the
suffixes starting at positions $i$ and $j$ in $T$. This classic problem has a
well-known solution that uses $O(n)$ space and $O(1)$ query time. In this paper
we show that for any trade-off parameter $1 \leq \tau \leq n$, the problem can
be solved in $O(\frac{n}{\tau})$ space and $O(\tau)$ query time. This
significantly improves the previously best known time-space trade-offs, and
almost matches the best known time-space product lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02687</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02687</id><created>2015-04-10</created><authors><author><keyname>Moura</keyname><forenames>Daniel C.</forenames></author></authors><title>3D Density Histograms for Criteria-driven Edge Bundling</title><categories>cs.GR</categories><comments>Submitted to a conference (under review)</comments><msc-class>68U05</msc-class><acm-class>I.3.3; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a graph bundling algorithm that agglomerates edges taking
into account both spatial proximity as well as user-defined criteria in order
to reveal patterns that were not perceivable with previous bundling techniques.
Each edge belongs to a group that may either be an input of the problem or
found by clustering one or more edge properties such as origin, destination,
orientation, length or domain-specific properties. Bundling is driven by a
stack of density maps, with each map capturing both the edge density of a given
group as well as interactions with edges from other groups. Density maps are
efficiently calculated by smoothing 2D histograms of edge occurrence using
repeated averaging filters based on integral images.
  A CPU implementation of the algorithm is tested on several graphs, and
different grouping criteria are used to illustrate how the proposed technique
can render different visualizations of the same data. Bundling performance is
much higher than on previous approaches, being particularly noticeable on large
graphs, with millions of edges being bundled in seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02692</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02692</id><created>2015-04-10</created><authors><author><keyname>Chen</keyname><forenames>Liang-Ting</forenames></author><author><keyname>Urbat</keyname><forenames>Henning</forenames></author></authors><title>A Fibrational Approach to Automata Theory</title><categories>cs.FL math.CT</categories><journal-ref>CALCO (2015) 50-65</journal-ref><doi>10.4230/LIPIcs.CALCO.2015.50</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For predual categories C and D we establish isomorphisms between opfibrations
representing local varieties of languages in C, local pseudovarieties of
D-monoids, and finitely generated profinite D-monoids. The global sections of
these opfibrations are shown to correspond to varieties of languages in C,
pseudovarieties of D-monoids, and profinite equational theories of D-monoids,
respectively. As an application, we obtain a new proof of Eilenberg's variety
theorem along with several related results, covering varieties of languages and
their coalgebraic modifications, Straubing's C-varieties, fully invariant local
varieties, etc., within a single framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02694</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02694</id><created>2015-04-10</created><updated>2015-06-16</updated><authors><author><keyname>Adamek</keyname><forenames>Jiri</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Urbat</keyname><forenames>Henning</forenames></author></authors><title>Syntactic Monoids in a Category</title><categories>cs.LO cs.FL math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic monoid of a language is generalized to the level of a symmetric
monoidal closed category D. This allows for a uniform treatment of several
notions of syntactic algebras known in the literature, including the syntactic
monoids of Rabin and Scott (D = sets), the syntactic semirings of Polak (D =
semilattices), and the syntactic associative algebras of Reutenauer (D = vector
spaces). Assuming that D is an entropic variety of algebras, we prove that the
syntactic D-monoid of a language L can be constructed as a quotient of a free
D-monoid modulo the syntactic congruence of L, and that it is isomorphic to the
transition D-monoid of the minimal automaton for L in D. Furthermore, in case
the variety D is locally finite, we characterize the regular languages as
precisely the languages with finite syntactic D-monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02696</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02696</id><created>2015-04-10</created><authors><author><keyname>Yan</keyname><forenames>Xinyan</forenames></author><author><keyname>Indelman</keyname><forenames>Vadim</forenames></author><author><keyname>Boots</keyname><forenames>Byron</forenames></author></authors><title>Incremental Sparse GP Regression for Continuous-time Trajectory
  Estimation &amp; Mapping</title><categories>cs.RO</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on simultaneous trajectory estimation and mapping (STEAM) for
mobile robots has found success by representing the trajectory as a Gaussian
process. Gaussian processes can represent a continuous-time trajectory,
elegantly handle asynchronous and sparse measurements, and allow the robot to
query the trajectory to recover its estimated position at any time of interest.
A major drawback of this approach is that STEAM is formulated as a batch
estimation problem. In this paper we provide the critical extensions necessary
to transform the existing batch algorithm into an extremely efficient
incremental algorithm. In particular, we are able to vastly speed up the
solution time through efficient variable reordering and incremental sparse
updates, which we believe will greatly increase the practicality of Gaussian
process methods for robot mapping and localization. Finally, we demonstrate the
approach and its advantages on both synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02712</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02712</id><created>2015-04-10</created><authors><author><keyname>C</keyname><forenames>Dharmani Bhaveshkumar</forenames></author></authors><title>Gradient of Probability Density Functions based Contrasts for Blind
  Source Separation (BSS)</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>47 pages</comments><msc-class>94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article derives some novel independence measures and contrast functions
for Blind Source Separation (BSS) application. For the $k^{th}$ order
differentiable multivariate functions with equal hyper-volumes (region bounded
by hyper-surfaces) and with a constraint of bounded support for $k&gt;1$, it
proves that equality of any $k^{th}$ order derivatives implies equality of the
functions. The difference between product of marginal Probability Density
Functions (PDFs) and joint PDF of a random vector is defined as Function
Difference (FD) of a random vector. Assuming the PDFs are $k^{th}$ order
differentiable, the results on generalized functions are applied to the
independence condition. This brings new sets of independence measures and BSS
contrasts based on the $L^p$-Norm, $ p \geq 1$ of - FD, gradient of FD (GFD)
and Hessian of FD (HFD). Instead of a conventional two stage indirect
estimation method for joint PDF based BSS contrast estimation, a single stage
direct estimation of the contrasts is desired. The article targets both the
efficient estimation of the proposed contrasts and extension of the potential
theory for an information field. The potential theory has a concept of
reference potential and it is used to derive closed form expression for the
relative analysis of potential field. Analogous to it, there are introduced
concepts of Reference Information Potential (RIP) and Cross Reference
Information Potential (CRIP) based on the potential due to kernel functions
placed at selected sample points as basis in kernel methods. The quantities are
used to derive closed form expressions for information field analysis using
least squares. The expressions are used to estimate $L^2$-Norm of FD and
$L^2$-Norm of GFD based contrasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02716</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02716</id><created>2015-04-10</created><updated>2015-10-13</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Zamdzhiev</keyname><forenames>Vladimir</forenames></author></authors><title>Equational reasoning with context-free families of string diagrams</title><categories>cs.LO cs.FL math.CT</categories><comments>International Conference on Graph Transformation, ICGT 2015. The
  final publication is available at Springer via
  http://dx.doi.org/10.1007/978-3-319-21145-9_9</comments><doi>10.1007/978-3-319-21145-9_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String diagrams provide an intuitive language for expressing networks of
interacting processes graphically. A discrete representation of string
diagrams, called string graphs, allows for mechanised equational reasoning by
double-pushout rewriting. However, one often wishes to express not just single
equations, but entire families of equations between diagrams of arbitrary size.
To do this we define a class of context-free grammars, called B-ESG grammars,
that are suitable for defining entire families of string graphs, and crucially,
of string graph rewrite rules. We show that the language-membership and
match-enumeration problems are decidable for these grammars, and hence that
there is an algorithm for rewriting string graphs according to B-ESG rewrite
patterns. We also show that it is possible to reason at the level of grammars
by providing a simple method for transforming a grammar by string graph
rewriting, and showing admissibility of the induced B-ESG rewrite pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02719</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02719</id><created>2015-04-10</created><authors><author><keyname>Cho</keyname><forenames>Hyunghoon</forenames></author><author><keyname>Berger</keyname><forenames>Bonnie</forenames></author><author><keyname>Peng</keyname><forenames>Jian</forenames></author></authors><title>Diffusion Component Analysis: Unraveling Functional Topology in
  Biological Networks</title><categories>q-bio.MN cs.LG cs.SI stat.ML</categories><comments>RECOMB 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex biological systems have been successfully modeled by biochemical and
genetic interaction networks, typically gathered from high-throughput (HTP)
data. These networks can be used to infer functional relationships between
genes or proteins. Using the intuition that the topological role of a gene in a
network relates to its biological function, local or diffusion based
&quot;guilt-by-association&quot; and graph-theoretic methods have had success in
inferring gene functions. Here we seek to improve function prediction by
integrating diffusion-based methods with a novel dimensionality reduction
technique to overcome the incomplete and noisy nature of network data. In this
paper, we introduce diffusion component analysis (DCA), a framework that plugs
in a diffusion model and learns a low-dimensional vector representation of each
node to encode the topological properties of a network. As a proof of concept,
we demonstrate DCA's substantial improvement over state-of-the-art
diffusion-based approaches in predicting protein function from molecular
interaction networks. Moreover, our DCA framework can integrate multiple
networks from heterogeneous sources, consisting of genomic information,
biochemical experiments and other resources, to even further improve function
prediction. Yet another layer of performance gain is achieved by integrating
the DCA framework with support vector machines that take our node vector
representations as features. Overall, our DCA framework provides a novel
representation of nodes in a network that can be used as a plug-in architecture
to other machine learning algorithms to decipher topological properties of and
obtain novel insights into interactomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02730</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02730</id><created>2015-04-10</created><updated>2015-04-24</updated><authors><author><keyname>Heunen</keyname><forenames>Chris</forenames></author><author><keyname>Lindenhovius</keyname><forenames>Bert</forenames></author></authors><title>Domains of commutative C*-subalgebras</title><categories>math.OA cs.LO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operator algebras provide uniform semantics for deterministic, reversible,
probabilistic, and quantum computing, where intermediate results of partial
computations are given by commutative subalgebras. We study this setting using
domain theory, and show that a given operator algebra is scattered if and only
if its associated partial order is, equivalently: continuous (a domain),
algebraic, atomistic, quasi-continuous, or quasi-algebraic. In that case,
conversely, we prove that the Lawson topology, modelling information
approximation, allows one to associate an operator algebra to the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02744</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02744</id><created>2015-04-10</created><authors><author><keyname>Hadzieva</keyname><forenames>Elena</forenames></author><author><keyname>Shuminoska</keyname><forenames>Marija</forenames></author></authors><title>Real-time Tool for Affine Transformations of Two Dimensional IFS
  Fractals</title><categories>cs.GR</categories><comments>8 pages, 2 figures</comments><msc-class>28A80, 65D17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a novel tool for interactive, real-time transformations
of two dimensional IFS fractals. We assign barycentric coordinates (relative to
an arbitrary affine basis of $\mathbb{R}^2$) to the points that constitute the
image of a fractal. The tool uses some of the nice properties of the
barycentric coordinates, enabling any affine transformation of the basis, done
by click-and-drag, to be immediately followed by the same affine transformation
of the IFS fractal attractor. In order to have a better control over the
fractal, as affine basis we use a kind of minimal simplex that contains the
attractor. We give theoretical grounds of the tool and then the software
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02756</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02756</id><created>2015-04-10</created><updated>2015-05-15</updated><authors><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author><author><keyname>Sadeghian</keyname><forenames>Hamid</forenames></author></authors><title>Discrimination and characterization of Parkinsonian rest tremors by
  analyzing long-term correlations and multifractal signatures</title><categories>physics.med-ph cs.CV physics.data-an</categories><comments>10 pages, 41 references</comments><doi>10.1109/TBME.2016.2515760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze 48 signals of rest tremor velocity related to 12
distinct subjects affected by Parkinson's disease. The subjects belong to two
different groups, formed by four and eight subjects with, respectively, high-
and low-amplitude rest tremors. Each subject is tested in four settings, given
by combining the use of deep brain stimulation and L-DOPA medication. We
develop two main feature-based representations of such signals, which are
obtained by considering (i) the long-term correlations and multifractal
properties, and (ii) the power spectra. The feature-based representations are
initially utilized for the purpose of characterizing the subjects under
different settings. In agreement with previous studies, we show that deep brain
stimulation does not significantly characterize neither of the two groups,
regardless of the adopted representation. On the other hand, the medication
effect yields statistically significant differences in both high- and
low-amplitude tremor groups. We successively test several different instances
of the two feature-based representations of the signals in the setting of
supervised classification and (nonlinear) feature transformation. We consider
three different classification problems, involving the recognition of (i) the
presence of medication, (ii) the use of deep brain stimulation, and (iii) the
membership to the high- and low-amplitude tremor groups. Classification results
show that the use of medication can be discriminated with higher accuracy,
considering many of the feature-based representations. Notably, we show that
the best results are obtained with a parsimonious, two-dimensional
representation encoding the long-term correlations and multifractal character
of the signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02762</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02762</id><created>2015-04-10</created><updated>2015-12-10</updated><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Delouille</keyname><forenames>Veronique</forenames></author><author><keyname>Li</keyname><forenames>Jimmy J.</forenames></author><author><keyname>De Visscher</keyname><forenames>Ruben</forenames></author><author><keyname>Watson</keyname><forenames>Fraser</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Image patch analysis of sunspots and active regions. II. Clustering via
  matrix factorization</title><categories>astro-ph.SR cs.CV</categories><comments>Accepted for publication in the Journal of Space Weather and Space
  Climate (SWSC). 33 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separating active regions that are quiet from potentially eruptive ones is a
key issue in Space Weather applications. Traditional classification schemes
such as Mount Wilson and McIntosh have been effective in relating an active
region large scale magnetic configuration to its ability to produce eruptive
events. However, their qualitative nature prevents systematic studies of an
active region's evolution for example. We introduce a new clustering of active
regions that is based on the local geometry observed in Line of Sight
magnetogram and continuum images. We use a reduced-dimension representation of
an active region that is obtained by factoring the corresponding data matrix
comprised of local image patches. Two factorizations can be compared via the
definition of appropriate metrics on the resulting factors. The distances
obtained from these metrics are then used to cluster the active regions. We
find that these metrics result in natural clusterings of active regions. The
clusterings are related to large scale descriptors of an active region such as
its size, its local magnetic field distribution, and its complexity as measured
by the Mount Wilson classification scheme. We also find that including data
focused on the neutral line of an active region can result in an increased
correspondence between our clustering results and other active region
descriptors such as the Mount Wilson classifications and the $R$ value. We
provide some recommendations for which metrics, matrix factorization
techniques, and regions of interest to use to study active regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02763</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02763</id><created>2015-04-10</created><updated>2016-01-27</updated><authors><author><keyname>Condessa</keyname><forenames>Filipe</forenames></author><author><keyname>Kovacevic</keyname><forenames>Jelena</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jose</forenames></author></authors><title>Performance measures for classification systems with rejection</title><categories>cs.CV cs.LG</categories><msc-class>68-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifiers with rejection are essential in real-world applications where
misclassifications and their effects are critical. However, if no problem
specific cost function is defined, there are no established measures to assess
the performance of such classifiers. We introduce a set of desired properties
for performance measures for classifiers with rejection, based on which we
propose a set of three performance measures for the evaluation of the
performance of classifiers with rejection that satisfy the desired properties.
The nonrejected accuracy measures the ability of the classifier to accurately
classify nonrejected samples; the classification quality measures the correct
decision making of the classifier with rejector; and the rejection quality
measures the ability to concentrate all misclassified samples onto the set of
rejected samples. From the measures, we derive the concept of relative
optimality that allows us to connect the measures to a family of cost functions
that take into account the trade-off between rejection and misclassification.
We illustrate the use of the proposed performance measures on classifiers with
rejection applied to synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02764</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02764</id><created>2015-04-10</created><authors><author><keyname>Mottaghi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Xiang</keyname><forenames>Yu</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that object detection, 3D pose estimation, and sub-category
recognition are highly correlated tasks, they are usually addressed
independently from each other because of the huge space of parameters. To
jointly model all of these tasks, we propose a coarse-to-fine hierarchical
representation, where each level of the hierarchy represents objects at a
different level of granularity. The hierarchical representation prevents
performance loss, which is often caused by the increase in the number of
parameters (as we consider more tasks to model), and the joint modelling
enables resolving ambiguities that exist in independent modelling of these
tasks. We augment PASCAL3D+ dataset with annotations for these tasks and show
that our hierarchical model is effective in joint modelling of object
detection, 3D pose estimation, and sub-category recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02789</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02789</id><created>2015-04-10</created><updated>2015-09-19</updated><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Koppula</keyname><forenames>Hema S.</forenames></author><author><keyname>Raghavan</keyname><forenames>Bharad</forenames></author><author><keyname>Soh</keyname><forenames>Shane</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Car that Knows Before You Do: Anticipating Maneuvers via Learning
  Temporal Driving Models</title><categories>cs.CV</categories><comments>ICCV 2015, http://brain4cars.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced Driver Assistance Systems (ADAS) have made driving safer over the
last decade. They prepare vehicles for unsafe road conditions and alert drivers
if they perform a dangerous maneuver. However, many accidents are unavoidable
because by the time drivers are alerted, it is already too late. Anticipating
maneuvers beforehand can alert drivers before they perform the maneuver and
also give ADAS more time to avoid or prepare for the danger.
  In this work we anticipate driving maneuvers a few seconds before they occur.
For this purpose we equip a car with cameras and a computing device to capture
the driving context from both inside and outside of the car. We propose an
Autoregressive Input-Output HMM to model the contextual information alongwith
the maneuvers. We evaluate our approach on a diverse data set with 1180 miles
of natural freeway and city driving and show that we can anticipate maneuvers
3.5 seconds before they occur with over 80\% F1-score in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02790</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02790</id><created>2015-04-09</created><authors><author><keyname>Dano-Hinosolango</keyname><forenames>Maria Angeles</forenames></author></authors><title>Sharpening Skills in Using Presentation Tools: Students' Experiences</title><categories>cs.CY</categories><comments>It has 10 pages and 4 tables to illustrate the results of the data
  gathered</comments><journal-ref>This has been published in International Journal on Integrating
  Technology in Education (IJITE) Vol 4, No 1, 2015 with the ISSN 2320-1886
  (Online); 2320-3935 (Print)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making use of Information and Communication Technology resources is deemed
necessary for students. With this, they need to demonstrate their competence in
using various technologies such as Prezi and PowerPoint to communicate
effectively and efficiently. Hence, their experiences in using these
presentation tools is important to assist them in their needs. In this study,
all the fourth year Bachelor of Science in Technology Communication Management
students who have used the PowerPoint and Prezi were the respondents. Survey
instruments were given to determine the experiences of students of these
technologies based on their familiarity, skills, and effectiveness in
delivering the reports and presentations. It was found out that students were
generally good in using these tools. On the other hand, following the basic
rules to use these presentation tools effectively has to be reinforced in the
classroom to enhance and enrich their learning in sharpening their skills on
presentation tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02796</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02796</id><created>2015-04-10</created><authors><author><keyname>Phan</keyname><forenames>Quoc-Sang</forenames></author></authors><title>Model Counting Modulo Theories</title><categories>cs.CR cs.LO</categories><comments>PhD thesis (2015); Queen Mary University of London
  (http://theory.eecs.qmul.ac.uk/)</comments><acm-class>H.1.1; D.4.6; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis is concerned with the quantitative assessment of security in
software. More specifically, it tackles the problem of efficient computation of
channel capacity, the maximum amount of confidential information leaked by
software, measured in Shannon entropy or R\'{e}nyi's min-entropy.
  Most approaches to computing channel capacity are either efficient and return
only (possibly very loose) upper bounds, or alternatively are inefficient but
precise; few target realistic programs. In this thesis, we present a novel
approach to the problem by reducing it to a model counting problem on
first-order logic, which we name Model Counting Modulo Theories or #SMT for
brevity.
  For quantitative security, our contribution is twofold. First, on the
theoretical side we establish the connections between measuring confidentiality
leaks and fundamental verification algorithms like Symbolic Execution, SMT
solvers and DPLL. Second, exploiting these connections, we develop novel
#SMT-based techniques to compute channel capacity, which achieve both accuracy
and efficiency. These techniques are scalable to real-world programs, and
illustrative case studies include C programs from Linux kernel, a Java program
from a European project and anonymity protocols.
  For formal verification, our contribution is also twofold. First, we
introduce and study a new research problem, namely #SMT, which has other
potential applications beyond computing channel capacity, such as returning
multiple-counterexamples for Bounded Model Checking or automated test
generation. Second, we propose an alternative approach for Bounded Model
Checking using classical Symbolic Execution, which can be parallelised to
leverage modern multi-core and distributed architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02799</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02799</id><created>2015-04-02</created><updated>2015-05-13</updated><authors><author><keyname>Menz</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>Justin</forenames></author><author><keyname>Xie</keyname><forenames>Jiyang</forenames></author></authors><title>Discrete All-Pay Bidding Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an all-pay auction, only one bidder wins but all bidders must pay the
auctioneer. All-pay bidding games arise from attaching a similar bidding
structure to traditional combinatorial games to determine which player moves
next. In contrast to the established theory of single-pay bidding games,
optimal play involves choosing bids from some probability distribution that
will guarantee a minimum probability of winning. In this manner, all-pay
bidding games wed the underlying concepts of economic and combinatorial games.
We present several results on the structures of optimal strategies in these
games. We then give a fast algorithm for computing such strategies for a large
class of all-pay bidding games. The methods presented provide a framework for
further development of the theory of all-pay bidding games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02817</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02817</id><created>2015-04-10</created><authors><author><keyname>Guerrini</keyname><forenames>Stefano</forenames></author><author><keyname>Martini</keyname><forenames>Simone</forenames></author><author><keyname>Masini</keyname><forenames>Andrea</forenames></author></authors><title>Towards A Theory Of Quantum Computability</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a definition of quantum computable functions as mappings between
superpositions of natural numbers to probability distributions of natural
numbers. Each function is obtained as a limit of an infinite computation of a
quantum Turing machine. The class of quantum computable functions is
recursively enumerable, thus opening the door to a quantum computability theory
which may follow some of the classical developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02824</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02824</id><created>2015-04-10</created><updated>2015-06-04</updated><authors><author><keyname>Shen</keyname><forenames>Yelong</forenames></author><author><keyname>Jin</keyname><forenames>Ruoming</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>A Deep Embedding Model for Co-occurrence Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Co-occurrence Data is a common and important information source in many
areas, such as the word co-occurrence in the sentences, friends co-occurrence
in social networks and products co-occurrence in commercial transaction data,
etc, which contains rich correlation and clustering information about the
items. In this paper, we study co-occurrence data using a general energy-based
probabilistic model, and we analyze three different categories of energy-based
model, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture
different levels of dependency in the co-occurrence data. We also discuss how
several typical existing models are related to these three types of energy
models, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix
Factorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted
Boltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model
(DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner.
Furthermore, motivated by the observation that the partition function in the
energy model is intractable and the fact that the major objective of modeling
the co-occurrence data is to predict using the conditional probability, we
apply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence,
the developed model and its learning method naturally avoid the above
difficulties and can be easily used to compute the conditional probability in
prediction. Interestingly, our method is equivalent to learning a special
structured deep neural network using back-propagation and a special sampling
strategy, which makes it scalable on large-scale datasets. Finally, in the
experiments, we show that the DEM can achieve comparable or better results than
state-of-the-art methods on datasets across several application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02830</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02830</id><created>2015-04-11</created><updated>2015-05-19</updated><authors><author><keyname>Nguyen</keyname><forenames>Kien Trung</forenames></author></authors><title>The inverse $p$-maxian problem on trees with variable edge lengths</title><categories>math.OC cs.DS</categories><comments>9 pages</comments><msc-class>90B10, 90B80, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We concern the problem of modifying the edge lengths of a tree in minimum
total cost so that the prespecified $p$ vertices become the $p$-maxian with
respect to the new edge lengths. This problem is called the inverse $p$-maxian
problem on trees. \textbf{Gassner} proposed efficient combinatorial alogrithm
to solve the the inverse 1-maxian problem on trees in 2008. For the problem
with $p \geq 2$, we claim that the problem can be reduced to finitely many
inverse $2$-maxian problem. We then develop algorithms to solve the inverse
$2$-maxian problem for various objective functions. The problem under
$l_1$-norm can be formulated as a linear program and thus can be solved in
polynomial time. Particularly, if the underlying tree is a star, then the
problem can be solved in linear time. We also devised $O(n\log n)$ algorithms
to solve the problems under Chebyshev norm and bottleneck Hamming distance,
where $n$ is the number of vertices of the tree. Finally, the problem under
weighted sum Hamming distance is $NP$-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02833</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02833</id><created>2015-04-11</created><updated>2015-04-25</updated><authors><author><keyname>B&#xfc;rger</keyname><forenames>Jens</forenames></author><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>Hierarchical Composition of Memristive Networks for Real-Time Computing</title><categories>cs.ET cond-mat.dis-nn cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in materials science have led to physical instantiations of
self-assembled networks of memristive devices and demonstrations of their
computational capability through reservoir computing. Reservoir computing is an
approach that takes advantage of collective system dynamics for real-time
computing. A dynamical system, called a reservoir, is excited with a
time-varying signal and observations of its states are used to reconstruct a
desired output signal. However, such a monolithic assembly limits the
computational power due to signal interdependency and the resulting correlated
readouts. Here, we introduce an approach that hierarchically composes a set of
interconnected memristive networks into a larger reservoir. We use signal
amplification and restoration to reduce reservoir state correlation, which
improves the feature extraction from the input signals. Using the same number
of output signals, such a hierarchical composition of heterogeneous small
networks outperforms monolithic memristive networks by at least 20% on waveform
generation tasks. On the NARMA-10 task, we reduce the error by up to a factor
of 2 compared to homogeneous reservoirs with sigmoidal neurons, whereas single
memristive networks are unable to produce the correct result. Hierarchical
composition is key for solving more complex tasks with such novel nano-scale
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02840</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02840</id><created>2015-04-11</created><authors><author><keyname>Tafti</keyname><forenames>Ahmad Pahlavan</forenames></author><author><keyname>Hassannia</keyname><forenames>Hamid</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>siftservice.com - Turning a Computer Vision algorithm into a World Wide
  Web Service</title><categories>cs.CV</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image features detection and description is a longstanding topic in computer
vision and pattern recognition areas. The Scale Invariant Feature Transform
(SIFT) is probably the most popular and widely demanded feature descriptor
which facilitates a variety of computer vision applications such as image
registration, object tracking, image forgery detection, and 3D surface
reconstruction. This work introduces a Software as a Service (SaaS) based
implementation of the SIFT algorithm which is freely available at
http://siftservice.com for any academic, educational and research purposes. The
service provides application-to-application interaction and aims Rapid
Application Development (RAD) and also fast prototyping for computer vision
students and researchers all around the world. An Internet connection is all
they need!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02842</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02842</id><created>2015-04-11</created><authors><author><keyname>Liu</keyname><forenames>Feng</forenames></author></authors><title>Definition and Research of Internet Neurology</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More and more scientific research shows that there is a close correlation
between the Internet and brain science. This paper presents the idea of
establishing the Internet neurology, which means to make a cross-contrast
between the two in terms of physiology and psychology, so that a complete
infrastructure system of the Internet is established, predicting the
development trend of the Internet in the future as well as the brain structure
and operation mechanism, and providing theoretical support for the generation
principle of intelligence, cognition and emotion. It also proposes the
viewpoint that the Internet can be divided into Internet neurophysiology,
Internet neuropsychology, Brain Internet physiology, Brain Internet psychology
and the Internet in cognitive science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02843</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02843</id><created>2015-04-11</created><authors><author><keyname>Risuleo</keyname><forenames>Riccardo Sven</forenames></author><author><keyname>Molinari</keyname><forenames>Marco</forenames></author><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>A benchmark for data-based office modeling: challenges related to CO$_2$
  dynamics</title><categories>cs.SY</categories><comments>6 pages, submitted to IFAC SysId 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a benchmark consisting of a set of synthetic
measurements relative to an office environment simulated with the software
IDA-ICE. The simulated environment reproduces a laboratory at the KTH-EES Smart
Building, equipped with a building management system. The data set contains
records collected over a period of several days. The signals to CO$_2$
concentration, mechanical ventilation airflows, air infiltrations and
occupancy. Information on door and window opening is also available. This
benchmark is intended for testing data-based modeling techniques. The ultimate
goal is the development of models to improve the forecast and control of
environmental variables. Among the numerous challenges related to this
framework, we point out the problem of occupancy estimation using information
on CO$_2$ concentration. This can be seen as a blind identification problem.
For benchmarking purposes, we present two different identification approaches:
a baseline overparametrization method and a kernel-based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02856</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02856</id><created>2015-04-11</created><authors><author><keyname>Dash</keyname><forenames>Arabinda</forenames></author><author><keyname>Sathua</keyname><forenames>Sujaya Kumar</forenames></author></authors><title>High Density Noise Removal by Cascading Algorithms</title><categories>cs.CV</categories><comments>6 pages, 6 figures</comments><doi>10.1109/ACCT.2015.100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An advanced non-linear cascading filter algorithm for the removal of high
density salt and pepper noise from the digital images is proposed. The proposed
method consists of two stages. The first stage Decision base Median Filter
(DMF) acts as the preliminary noise removal algorithm. The second stage is
either Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) or
Modified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which is
used to remove the remaining noise and enhance the image quality. The DMF
algorithm performs well at low noise density but it fails to remove the noise
at medium and high level. The MDBPTGMF and MDUTMF have excellent performance at
low, medium and high noise density but these reduce the image quality and blur
the image at high noise level. So the basic idea behind this paper is to
combine the advantages of the filters used in both the stages to remove the
Salt and Pepper noise and enhance the image quality at all the noise density
level. The proposed method is tested against different gray scale images and it
gives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) and
Image Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), Decision
Base Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision Base
Unsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial Trimmed
Global Mean Filter (DBPTGMF).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02861</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02861</id><created>2015-04-11</created><authors><author><keyname>Hartmanns</keyname><forenames>Arnd</forenames></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author></authors><title>Explicit Model Checking of Very Large MDP using Partitioning and
  Secondary Storage</title><categories>cs.LO</categories><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The applicability of model checking is hindered by the state space explosion
problem in combination with limited amounts of main memory. To extend its
reach, the large available capacities of secondary storage such as hard disks
can be exploited. Due to the specific performance characteristics of secondary
storage technologies, specialised algorithms are required. In this paper, we
present a technique to use secondary storage for probabilistic model checking
of Markov decision processes. It combines state space exploration based on
partitioning with a block-iterative variant of value iteration over the same
partitions for the analysis of probabilistic reachability and expected-reward
properties. A sparse matrix-like representation is used to store partitions on
secondary storage in a compact format. All file accesses are sequential, and
compression can be used without affecting runtime. The technique has been
implemented within the Modest Toolset. We evaluate its performance on several
benchmark models of up to 3.5 billion states. In the analysis of time-bounded
properties on real-time models, our method neutralises the state space
explosion induced by the time bound in its entirety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02863</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02863</id><created>2015-04-11</created><authors><author><keyname>Zhang</keyname><forenames>Xucong</forenames></author><author><keyname>Sugano</keyname><forenames>Yusuke</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author></authors><title>Appearance-Based Gaze Estimation in the Wild</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Appearance-based gaze estimation is believed to work well in real-world
settings, but existing datasets have been collected under controlled laboratory
conditions and methods have been not evaluated across multiple datasets. In
this work we study appearance-based gaze estimation in the wild. We present the
MPIIGaze dataset that contains 213,659 images we collected from 15 participants
during natural everyday laptop use over more than three months. Our dataset is
significantly more variable than existing ones with respect to appearance and
illumination. We also present a method for in-the-wild appearance-based gaze
estimation using multimodal convolutional neural networks that significantly
outperforms state-of-the art methods in the most challenging cross-dataset
evaluation. We present an extensive evaluation of several state-of-the-art
image-based gaze estimation algorithms on three current datasets, including our
own. This evaluation provides clear insights and allows us to identify key
research challenges of gaze estimation in the wild.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02866</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02866</id><created>2015-04-11</created><authors><author><keyname>Bhuyan</keyname><forenames>Bhaskar</forenames></author><author><keyname>Sarma</keyname><forenames>Nityananda</forenames></author></authors><title>A Delay Aware Routing Protocol for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 Pages, 8 Figures, journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 11,
  Issue 6, No 2, November 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) consist of sensor nodes which can be deployed
for various operations such as agriculture and environmental sensing, wild life
monitoring, health care, military surveillance, industrial control, home
automation, security etc. Quality of Service (QoS) is an important issue in
wireless sensor networks (WSNs) and providing QoS support in WSNs is an
emerging area of research. Due to resource constraints nature of sensor
networks like processing power, memory, bandwidth, energy etc. providing QoS
support in WSNs is a challenging task. Delay is an important QoS parameter for
forwarding data in a time constraint WSNs environment. In this paper we propose
a delay aware routing protocol for transmission of time critical event
information to the Sink of WSNs. The performance of the proposed protocol is
evaluated by NS2 simulations under different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02870</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02870</id><created>2015-04-11</created><authors><author><keyname>Okumura</keyname><forenames>Shota</forenames></author><author><keyname>Suzuki</keyname><forenames>Yoshiki</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Quick sensitivity analysis for incremental data modification and its
  application to leave-one-out CV in linear classification problems</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel sensitivity analysis framework for large scale
classification problems that can be used when a small number of instances are
incrementally added or removed. For quickly updating the classifier in such a
situation, incremental learning algorithms have been intensively studied in the
literature. Although they are much more efficient than solving the optimization
problem from scratch, their computational complexity yet depends on the entire
training set size. It means that, if the original training set is large,
completely solving an incremental learning problem might be still rather
expensive. To circumvent this computational issue, we propose a novel framework
that allows us to make an inference about the updated classifier without
actually re-optimizing it. Specifically, the proposed framework can quickly
provide a lower and an upper bounds of a quantity on the unknown updated
classifier. The main advantage of the proposed framework is that the
computational cost of computing these bounds depends only on the number of
updated instances. This property is quite advantageous in a typical sensitivity
analysis task where only a small number of instances are updated. In this paper
we demonstrate that the proposed framework is applicable to various practical
sensitivity analysis tasks, and the bounds provided by the framework are often
sufficiently tight for making desired inferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02872</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02872</id><created>2015-04-11</created><authors><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Ljung</keyname><forenames>Lennart</forenames></author></authors><title>Regularized system identification using orthonormal basis functions</title><categories>cs.SY</categories><comments>6 pages, final submission of an contribution for European Control
  Conference 2015, uploaded on March 20, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of existing results on regularized system identification focus on
regularized impulse response estimation. Since the impulse response model is a
special case of orthonormal basis functions, it is interesting to consider if
it is possible to tackle the regularized system identification using more
compact orthonormal basis functions. In this paper, we explore two
possibilities. First, we construct reproducing kernel Hilbert space of impulse
responses by orthonormal basis functions and then use the induced reproducing
kernel for the regularized impulse response estimation. Second, we extend the
regularization method from impulse response estimation to the more general
orthonormal basis functions estimation. For both cases, the poles of the basis
functions are treated as hyperparameters and estimated by empirical Bayes
method. Then we further show that the former is a special case of the latter,
and more specifically, the former is equivalent to ridge regression of the
coefficients of the orthonormal basis functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02875</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02875</id><created>2015-04-11</created><updated>2016-01-31</updated><authors><author><keyname>Adaricheva</keyname><forenames>Kira</forenames></author><author><keyname>Nation</keyname><forenames>J. B.</forenames></author></authors><title>Discovery of the $D$-basis in binary tables based on hypergraph
  dualization</title><categories>cs.DB cs.DM</categories><comments>14 pages, one table and one figure</comments><doi>10.1016/j.tcs.2015.11.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovery of (strong) association rules, or implications, is an important
task in data management, and it finds application in artificial intelligence,
data mining and the semantic web. We introduce a novel approach for the
discovery of a specific set of implications, called the $D$-basis, that
provides a representation for a reduced binary table, based on the structure of
its Galois lattice. At the core of the method are the $D$-relation defined in
the lattice theory framework, and the hypergraph dualization algorithm that
allows us to effectively produce the set of transversals for a given Sperner
hypergraph. The latter algorithm, first developed by specialists from Rutgers
Center for Operations Research, has already found numerous applications in
solving optimization problems in data base theory, artificial intelligence and
game theory. One application of the method is for analysis of gene expression
data related to a particular phenotypic variable, and some initial testing is
done for the data provided by the University of Hawaii Cancer Center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02876</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02876</id><created>2015-04-11</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub</forenames></author><author><keyname>Soca&#x142;a</keyname><forenames>Arkadiusz</forenames></author></authors><title>The Hardness of Subgraph Isomorphism</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subgraph Isomorphism is a very basic graph problem, where given two graphs
$G$ and $H$ one is to check whether $G$ is a subgraph of $H$. Despite its
simple definition, the Subgraph Isomorphism problem turns out to be very broad,
as it generalizes problems such as Clique, $r$-Coloring, Hamiltonicity, Set
Packing and Bandwidth. However, for all of the mentioned problems
$2^{\mathcal{O}(n)}$ time algorithms exist, so a natural and frequently asked
question in the past was whether there exists a $2^{\mathcal{O}(n)}$ time
algorithm for Subgraph Isomorphism. In the monograph of Fomin and Kratsch
[Springer'10] this question is highlighted as an open problem, among few
others.
  Our main result is a reduction from 3-SAT, producing a subexponential number
of sublinear instances of the Subgraph Isomorphism problem. In particular, our
reduction implies a $2^{\Omega(n \sqrt{\log n})}$ lower bound for Subgraph
Isomorphism under the Exponential Time Hypothesis. This shows that there exist
classes of graphs that are strictly harder to embed than cliques or Hamiltonian
cycles.
  The core of our reduction consists of two steps. First, we preprocess and
pack variables and clauses of a 3-SAT formula into groups of logarithmic size.
However, the grouping is not arbitrary, since as a result we obtain only a
limited interaction between the groups. In the second step, we overcome the
technical hardness of encoding evaluations as permutations by a simple, yet
fruitful scheme of guessing the sizes of preimages of an arbitrary mapping,
reducing the case of arbitrary mapping to bijections. In fact, when applying
this step to a recent independent result of Fomin et al.[arXiv:1502.05447
(2015)], who showed hardness of Graph Homomorphism, we can transfer their
hardness result to Subgraph Isomorphism, implying a nearly tight lower bound of
$2^{\Omega(n \log n / \log \log n)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02878</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02878</id><created>2015-04-11</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author></authors><title>Data Science and Ebola</title><categories>cs.AI cs.CY</categories><comments>Inaugural lecture Leiden University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Science---Today, everybody and everything produces data. People produce
large amounts of data in social networks and in commercial transactions.
Medical, corporate, and government databases continue to grow. Sensors continue
to get cheaper and are increasingly connected, creating an Internet of Things,
and generating even more data. In every discipline, large, diverse, and rich
data sets are emerging, from astrophysics, to the life sciences, to the
behavioral sciences, to finance and commerce, to the humanities and to the
arts. In every discipline people want to organize, analyze, optimize and
understand their data to answer questions and to deepen insights. The science
that is transforming this ocean of data into a sea of knowledge is called data
science. This lecture will discuss how data science has changed the way in
which one of the most visible challenges to public health is handled, the 2014
Ebola outbreak in West Africa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02882</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02882</id><created>2015-04-11</created><authors><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Shi</keyname><forenames>Yong</forenames></author></authors><title>Quantitative Analysis of Whether Machine Intelligence Can Surpass Human
  Intelligence</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whether the machine intelligence can surpass the human intelligence is a
controversial issue. On the basis of traditional IQ, this article presents the
Universal IQ test method suitable for both the machine intelligence and the
human intelligence. With the method, machine and human intelligences were
divided into 4 major categories and 15 subcategories. A total of 50 search
engines across the world and 150 persons at different ages were subject to the
relevant test. And then, the Universal IQ ranking list of 2014 for the test
objects was obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02886</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02886</id><created>2015-04-11</created><authors><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>Fundamental Relations Between Reactive and Proactive Relay-Selection
  Strategies</title><categories>cs.IT math.IT</categories><comments>4 pages, to appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two major relay-selection strategies widely applied in cooperative
decode-and-forward (DF) relaying networks, namely, reactive relay selection
(RRS) and proactive relay selection (PRS), are generally looked upon as
independent and studied separately. In this paper, RRS and PRS are proven to be
equivalent with respect to the end-to-end outage probability from the first
principle, i.e. their respective relay-selection criteria. On the other hand,
RRS is shown to be superior to PRS with respect to the end-to-end symbol error
rate. Afterwards, a case study of a general DF relaying system, subject to
co-channel interferences and additive white Gaussian noise at both the relaying
nodes and the destination, is performed to explicitly illustrate the
aforementioned outage equivalence. These fundamental relations provide
intuitive yet insightful performance benchmarks for comparing various
applications of these two relay-selection strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02899</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02899</id><created>2015-04-11</created><updated>2015-06-24</updated><authors><author><keyname>Ba&#x161;i&#x107;</keyname><forenames>Bojan</forenames><affiliation>Department of Mathematics and Informatics, University of Novi Sad</affiliation></author></authors><title>On absorption in semigroups and $n$-ary semigroups</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:15) 2015</journal-ref><doi>10.2168/LMCS-11(2:15)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of absorption was developed a few years ago by Barto and Kozik and
immediately found many applications, particularly in topics related to the
constraint satisfaction problem. We investigate the behavior of absorption in
semigroups and n-ary semigroups (that is, algebras with one n-ary associative
operation). In the case of semigroups, we give a simple necessary and
sufficient condition for a semigroup to be absorbed by its subsemigroup. We
then proceed to n-ary semigroups, where we conjecture an analogue of this
necessary and sufficient condition, and prove that the conjectured condition is
indeed necessary and sufficient for B to absorb A (where A is an n-ary
semigroup and B is its n-ary subsemigroup) in the following three cases: when A
is commutative, when |A-B|=1 and when A is an idempotent ternary semigroup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02902</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02902</id><created>2015-04-11</created><authors><author><keyname>Kalmanovich</keyname><forenames>Alexander</forenames></author><author><keyname>Chechik</keyname><forenames>Gal</forenames></author></authors><title>Gradual Training Method for Denoising Auto Encoders</title><categories>cs.LG cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1412.6257</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stacked denoising auto encoders (DAEs) are well known to learn useful deep
representations, which can be used to improve supervised training by
initializing a deep network. We investigate a training scheme of a deep DAE,
where DAE layers are gradually added and keep adapting as additional layers are
added. We show that in the regime of mid-sized datasets, this gradual training
provides a small but consistent improvement over stacked training in both
reconstruction quality and classification error over stacked training on MNIST
and CIFAR datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02914</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02914</id><created>2015-04-11</created><authors><author><keyname>Neal</keyname><forenames>Radford M.</forenames></author></authors><title>Representing numeric data in 32 bits while preserving 64-bit precision</title><categories>stat.CO cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data files often consist of numbers having only a few significant decimal
digits, whose information content would allow storage in only 32 bits. However,
we may require that arithmetic operations involving these numbers be done with
64-bit floating-point precision, which precludes simply representing the data
as 32-bit floating-point values. Decimal floating point gives a compact and
exact representation, but requires conversion with a slow division operation
before it can be used. Here, I show that interesting subsets of 64-bit
floating-point values can be compactly and exactly represented by the 32 bits
consisting of the sign, exponent, and high-order part of the mantissa, with the
lower-order 32 bits of the mantissa filled in by table lookup, indexed by bits
from the part of the mantissa retained, and possibly from the exponent. For
example, decimal data with 4 or fewer digits to the left of the decimal point
and 2 or fewer digits to the right of the decimal point can be represented in
this way using the lower-order 5 bits of the retained part of the mantissa as
the index. Data consisting of 6 decimal digits with the decimal point in any of
the 7 positions before or after one of the digits can also be represented this
way, and decoded using 19 bits from the mantissa and exponent as the index.
Encoding with such a scheme is a simple copy of half the 64-bit value, followed
if necessary by verification that the value can be represented, by checking
that it decodes correctly. Decoding requires only extraction of index bits and
a table lookup. Lookup in a small table will usually reference cache; even with
larger tables, decoding is still faster than conversion from decimal floating
point with a division operation. I discuss how such schemes perform on recent
computer systems, and how they might be used to automatically compress large
arrays in interpretive languages such as R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02921</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02921</id><created>2015-04-11</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Antenna Array Signal Processing for Quaternion-Valued Wireless
  Communication Systems</title><categories>cs.SY</categories><comments>3 pages, 5 figures, published in Proc. of the IEEE Benjamin Franklin
  Symposium on Microwave and Antenna Sub-systems (BenMAS), Philadelphia, US,
  September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quaternion-valued wireless communication systems have been studied in the
past. Although progress has been made in this promising area, a crucial missing
link is lack of effective and efficient quaternion-valued signal processing
algorithms for channel equalisation and beamforming. With most recent
developments in quaternion-valued signal processing, in this work, we fill the
gap to solve the problem and further derive the quaternion-valued Wiener
solution for block-based calculation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02923</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02923</id><created>2015-04-11</created><authors><author><keyname>Woodworth</keyname><forenames>Joseph</forenames></author><author><keyname>Chartrand</keyname><forenames>Rick</forenames></author></authors><title>Compressed Sensing Recovery via Nonconvex Shrinkage Penalties</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\ell^0$ minimization of compressed sensing is often relaxed to $\ell^1$,
which yields easy computation using the shrinkage mapping known as soft
thresholding, and can be shown to recover the original solution under certain
hypotheses. Recent work has derived a general class of shrinkages and
associated nonconvex penalties that better approximate the original $\ell^0$
penalty and empirically can recover the original solution from fewer
measurements. We specifically examine p-shrinkage and firm thresholding. In
this work, we prove that given data and a measurement matrix from a broad class
of matrices, one can choose parameters for these classes of shrinkages to
guarantee exact recovery of the sparsest solution. We further prove convergence
of the algorithm iterative p-shrinkage (IPS) for solving one such relaxed
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02926</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02926</id><created>2015-04-11</created><updated>2015-12-06</updated><authors><author><keyname>Ghosh</keyname><forenames>Arnob</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Strategic Interaction Among Different Entities in Internet of Things</title><categories>cs.GT cs.NI cs.SI</categories><comments>Submitted in IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the economics of internet of things (IoT). An economic model
of IoT consists of end users, advertisers and three different kinds of
providers--IoT service provider (IoTSP), Wireless service provider (WSP) and
cloud service provider (CSP). We investigate three different kinds of
interactions-- i) push model, ii) pull model, and iii) hybrid model. We model
different kinds of interaction among the providers as a combination of
sequential and parallel non-cooperative games. We characterize the equilibrium
pricing strategy and payoff of providers and corresponding demands of end users
in each such setting. We quantify the impact of advertising revenue on the
equilibrium pricing and demands, and compare the payoffs and demands for
different interaction models. Our analysis reveals that the demand of
end-users, the payoffs of the providers are the non decreasing functions of the
advertisement revenue in each of the interaction models. For sufficiently high
advertisement revenue, the IoTSP will offer its service free of cost in each of
the interaction models. However, the payoffs of the providers, and the
advertisers and the demand of end-users vary across different interaction
models. Our analysis shows that the demand of end-users, the payoff of the WSP
and the expected payoff of the advertisers are the highest in the pull (push,
respv.) model in the low (high, respv.) advertisement revenue regime. The
payoff of the IoTSP is always higher in the pull model irrespective of the
advertisement revenue. The payoff of the CSP is the highest in the hybrid model
in the low advertisement revenue regime. However, in the high advertisement
revenue regime the payoff of the CSP in the hybrid model or in the push model
can be higher depending on the equilibrium chosen in the push model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02930</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02930</id><created>2015-04-11</created><authors><author><keyname>Cai</keyname><forenames>Mingjie</forenames></author></authors><title>Knowledge reduction of dynamic covering decision information systems
  with varying attribute values</title><categories>cs.IT cs.AI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge reduction of dynamic covering information systems involves with the
time in practical situations. In this paper, we provide incremental approaches
to computing the type-1 and type-2 characteristic matrices of dynamic coverings
because of varying attribute values. Then we present incremental algorithms of
constructing the second and sixth approximations of sets by using
characteristic matrices. We employ experimental results to illustrate that the
incremental approaches are e?ective to calculate approximations of sets in
dynamic covering information systems. Finally, we perform knowledge reduction
of dynamic covering information systems with the incremental approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02931</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02931</id><created>2015-04-11</created><authors><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Xing</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Zheng</keyname><forenames>Nanning</forenames></author><author><keyname>Pr&#xed;ncipe</keyname><forenames>Jos&#xe9; C.</forenames></author></authors><title>Generalized Correntropy for Robust Adaptive Filtering</title><categories>stat.ML cs.IT math.IT</categories><comments>34 pages, 9 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a robust nonlinear similarity measure in kernel space, correntropy has
received increasing attention in domains of machine learning and signal
processing. In particular, the maximum correntropy criterion (MCC) has recently
been successfully applied in robust regression and filtering. The default
kernel function in correntropy is the Gaussian kernel, which is, of course, not
always the best choice. In this work, we propose a generalized correntropy that
adopts the generalized Gaussian density (GGD) function as the kernel (not
necessarily a Mercer kernel), and present some important properties. We further
propose the generalized maximum correntropy criterion (GMCC), and apply it to
adaptive filtering. An adaptive algorithm, called the GMCC algorithm, is
derived, and the mean square convergence performance is studied. We show that
the proposed algorithm is very stable and can achieve zero probability of
divergence (POD). Simulation results confirm the theoretical expectations and
demonstrate the desirable performance of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02944</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02944</id><created>2015-04-12</created><authors><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>On the Efficiency of Far-Field Wireless Power Transfer</title><categories>cs.IT math.IT</categories><comments>13 pages, to appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2417497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Far-field wireless power transfer (WPT) is a promising technique to resolve
the painstaking power-charging problem inherent in various wireless terminals.
This paper investigates the power transfer efficiency of the WPT segment in
future communication systems in support of simultaneous power and data
transfer, by means of analytically computing the time-average output direct
current (DC) power at user equipments (UEs). In order to investigate the effect
of channel variety among UEs on the average output DC power, different policies
for the scheduling of the power transfer among the users are implemented and
compared in two scenarios: homogeneous, whereby users are symmetric and
experience similar path loss, and heterogeneous, whereby users are asymmetric
and exhibit different path losses. Specifically, if opportunistic scheduling is
performed among $N$ symmetric/asymmetric UEs, the power scaling laws are
attained by using extreme value theory, and reveal that the gain in power
transfer efficiency is $\ln{N}$ if UEs are symmetric whereas the gain is $N$ if
UEs are asymmetric, compared with that of conventional round-robin scheduling.
Thus, the channel variety among UEs inherent to the wireless environment can be
exploited by opportunistic scheduling to significantly improve the power
transfer efficiency when designing future wireless communication systems in
support of simultaneous power and data transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02945</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02945</id><created>2015-04-12</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Deep Transform: Cocktail Party Source Separation via Complex Convolution
  in a Deep Neural Network</title><categories>cs.SD cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional deep neural networks (DNN) are state of the art in many
engineering problems but have not yet addressed the issue of how to deal with
complex spectrograms. Here, we use circular statistics to provide a convenient
probabilistic estimate of spectrogram phase in a complex convolutional DNN. In
a typical cocktail party source separation scenario, we trained a convolutional
DNN to re-synthesize the complex spectrograms of two source speech signals
given a complex spectrogram of the monaural mixture - a discriminative deep
transform (DT). We then used this complex convolutional DT to obtain
probabilistic estimates of the magnitude and phase components of the source
spectrograms. Our separation results are on a par with equivalent binary-mask
based non-complex separation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02947</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02947</id><created>2015-04-12</created><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Looking at Mean-Payoff through Foggy Windows</title><categories>cs.LO cs.FL cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-payoff games (MPGs) are infinite duration two-player zero-sum games
played on weighted graphs. Under the hypothesis of perfect information, they
admit memoryless optimal strategies for both players and can be solved in
NP-intersect-coNP. MPGs are suitable quantitative models for open reactive
systems. However, in this context the assumption of perfect information is not
always realistic. For the partial-observation case, the problem that asks if
the first player has an observation-based winning strategy that enforces a
given threshold on the mean-payoff, is undecidable. In this paper, we study the
window mean-payoff objectives that were introduced recently as an alternative
to the classical mean-payoff objectives. We show that, in sharp contrast to the
classical mean-payoff objectives, some of the window mean-payoff objectives are
decidable in games with partial-observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02949</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02949</id><created>2015-04-12</created><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author><author><keyname>Capriotti</keyname><forenames>Paolo</forenames></author><author><keyname>Spadotti</keyname><forenames>R&#xe9;gis</forenames></author></authors><title>Non-wellfounded trees in Homotopy Type Theory</title><categories>cs.LO math.CT</categories><comments>14 pages, to be published in proceedings of TLCA 2015; ancillary
  files contain Agda files with formalized proofs</comments><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a conjecture about the constructibility of coinductive types - in
the principled form of indexed M-types - in Homotopy Type Theory. The
conjecture says that in the presence of inductive types, coinductive types are
derivable. Indeed, in this work, we construct coinductive types in a subsystem
of Homotopy Type Theory; this subsystem is given by Intensional Martin-L\&quot;of
type theory with natural numbers and Voevodsky's Univalence Axiom. Our results
are mechanized in the computer proof assistant Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02954</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02954</id><created>2015-04-12</created><updated>2015-04-17</updated><authors><author><keyname>Park</keyname><forenames>Sangkyu</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Bahk</keyname><forenames>Saewoong</forenames></author></authors><title>Large-scale Antenna Operation in Heterogeneous Cloud Radio Access
  Networks: A Partial Centralization Approach</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Magazine June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To satisfy the ever-increasing capacity demand and quality of service (QoS)
requirements of users, 5G cellular systems will take the form of heterogeneous
networks (HetNets) that consist of macro cells and small cells. To build and
operate such systems, mobile operators have given significant attention to
cloud radio access networks (C-RANs) due to their beneficial features of
performance optimization and cost effectiveness. Along with the architectural
enhancement of C-RAN, large-scale antennas (a.k.a. massive MIMO) at cell sites
contribute greatly to increased network capacity either with higher spectral
efficiency or through permitting many users at once. In this article, we
discuss the challenging issues of C-RAN based HetNets (H-CRAN), especially with
respect to large-scale antenna operation. We provide an overview of existing
C-RAN architectures in terms of large-scale antenna operation and promote a
partially centralized approach. This approach reduces, remarkably, fronthaul
overheads in CRANs with large-scale antennas. We also provide some insights
into its potential and applicability in the fronthaul bandwidthlimited H-CRAN
with large-scale antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02957</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02957</id><created>2015-04-12</created><authors><author><keyname>Hassen</keyname><forenames>Fadoua</forenames></author><author><keyname>Touzi</keyname><forenames>Amel Grissa</forenames></author></authors><title>Intelligent Implementation Processor Design for Oracle Distributed
  Databases System</title><categories>cs.DB</categories><journal-ref>International Conference on Control, Engineering &amp; Information
  Technology (CEIT 2014), pp. 278-296, Tunisie, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the increasing need for modeling and implementing Distributed
Databases (DDB), distributed database management systems are still quite far
from helping the designer to directly implement its BDD. Indeed, the
fundamental principle of implementation of a DDB is to make the database appear
as a centralized database, providing series of transparencies, something that
is not provided directly by the current DDBMS. We focus in this work on Oracle
DBMS which, despite its market dominance, offers only a few logical mechanisms
to implement distribution. To remedy this problem, we propose a new
architecture of DDBMS Oracle. The idea is based on extending it by an
intelligent layer that provides: 1) creation of different types of
fragmentation through a GUI for defining different sites geographically
dispersed 2) allocation and replication of DB. The system must automatically
generate SQL scripts for each site of the original configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02967</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02967</id><created>2015-04-12</created><updated>2015-08-21</updated><authors><author><keyname>Ito</keyname><forenames>Kosuke</forenames></author><author><keyname>Kumagai</keyname><forenames>Wataru</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Asymptotic Compatibility between LOCC Conversion and Recovery</title><categories>quant-ph cs.IT math.IT</categories><comments>16 pages, 6 figures</comments><journal-ref>Phys. Rev. A 92, 052308 (2015)</journal-ref><doi>10.1103/PhysRevA.92.052308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, entanglement concentration was explicitly shown to be irreversible.
However, it is still not clear what kind of states can be reversibly converted
in the asymptotic setting by LOCC when neither the initial nor the target state
is maximally entangled. We derive the necessary and sufficient condition for
the reversibility of LOCC conversions between two bipartite pure entangled
states in the asymptotic setting. In addition, we show that conversion can be
achieved perfectly with only local unitary operation under such condition
except for special cases. Interestingly, our result implies that an error-free
reversible conversion is asymptotically possible even between states whose
copies can never be locally unitarily equivalent with any finite numbers of
copies, although such a conversion is impossible in the finite setting. In
fact, we show such an example. Moreover, we establish how to overcome the
irreversibility of LOCC conversion in two ways. As for the first method, we
evaluate how many copies of the initial state is to be lost to overcome the
irreversibility of LOCC conversion. The second method is to add a supplementary
state appropriately, which also works for LU conversion unlike the first
method. Especially, for the qubit system, any non-maximally pure entangled
state can be a universal resource for the asymptotic reversibility when copies
of the state is sufficiently many. More interestingly, our analysis implies
that far-from-maximally entangled states can be better than nearly maximally
entangled states as this type of resource. This fact brings new insight to the
resource theory of state conversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02972</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02972</id><created>2015-04-12</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author></authors><title>Computing trading strategies based on financial sentiment data using
  evolutionary optimization</title><categories>q-fin.PM cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply evolutionary optimization techniques to compute
optimal rule-based trading strategies based on financial sentiment data. The
sentiment data was extracted from the social media service StockTwits to
accommodate the level of bullishness or bearishness of the online trading
community towards certain stocks. Numerical results for all stocks from the Dow
Jones Industrial Average (DJIA) index are presented and a comparison to
classical risk-return portfolio selection is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02975</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02975</id><created>2015-04-12</created><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author></authors><title>Classification with Extreme Learning Machine and Ensemble Algorithms
  Over Randomly Partitioned Data</title><categories>cs.LG</categories><comments>In Turkish, SIU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this age of Big Data, machine learning based data mining methods are
extensively used to inspect large scale data sets. Deriving applicable
predictive modeling from these type of data sets is a challenging obstacle
because of their high complexity. Opportunity with high data availability
levels, automated classification of data sets has become a critical and
complicated function. In this paper, the power of applying MapReduce based
Distributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build
reliable predictive bag of classification models. Thus, (i) dataset ensembles
are build; (ii) ELM algorithm is used to build weak classification models; and
(iii) build a strong classification model from a set of weak classification
models. This training model is applied to the publicly available knowledge
discovery and data mining datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02978</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02978</id><created>2015-04-12</created><authors><author><keyname>Huang</keyname><forenames>Kechao</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author></authors><title>Performance Analysis of Block Markov Superposition Transmission of Short
  Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Journal on Selected Areas in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the asymptotic and finite-length performance of
block Markov superposition transmission~(BMST) of short codes, which can be
viewed as a new class of spatially coupled~(SC) codes with the generator
matrices of short codes~(referred to as {\em basic codes}) coupled. A modified
extrinsic information transfer~(EXIT) chart analysis that takes into account
the relation between mutual information~(MI) and bit-error-rate~(BER) is
presented to study the convergence behavior of BMST codes. Using the modified
EXIT chart analysis, we investigate the impact of various parameters on BMST
code performance, thereby providing theoretical guidance for designing and
implementing practical BMST codes suitable for sliding window decoding. Then,
we present a performance comparison of BMST codes and SC low-density
parity-check (SC-LDPC) codes on the basis of equal decoding latency. Also
presented is a comparison of computational complexity. Simulation results show
that, under the equal decoding latency constraint, BMST codes using the
repetition code as the basic code can outperform $(3,6)$-regular SC-LDPC codes
in the waterfall region but have a higher computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02980</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02980</id><created>2015-04-12</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Chen</keyname><forenames>Hsiao-Hwa</forenames></author></authors><title>Multi-Antenna Relay Aided Wireless Physical Layer Security</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 figures, IEEE Communications Magazine, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With growing popularity of mobile Internet, providing secure wireless
services has become a critical issue. Physical layer security (PHY-security)
has been recognized as an effective means to enhance wireless security by
exploiting wireless medium characteristics, e.g., fading, noise, and
interference. A particularly interesting PHY-security technology is cooperative
relay due to the fact that it helps to provide distributed diversity and
shorten access distance. This article offers a tutorial on various
multi-antenna relaying technologies to improve security at physical layer. The
state of the art research results on multi-antenna relay aided PHY-security as
well as some secrecy performance optimization schemes are presented. In
particular, we focus on large-scale MIMO (LS-MIMO) relaying technology, which
is effective to tackle various challenging issues for implementing wireless
PHY-security, such as short-distance interception without eavesdropper channel
state information (CSI) and with imperfect legitimate CSI. Moreover, the future
directions are identified for further enhancement of secrecy performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.02990</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.02990</id><created>2015-04-12</created><authors><author><keyname>Liu</keyname><forenames>Haijing</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author></authors><title>Low-Complexity Downlink User Selection for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 27 figures, Accepted to publish on IEEE Systems Journal --
  Special Issue on 5G Wireless Systems with Massive MIMO, Apr. 2015</comments><doi>10.1109/JSYST.2015.2422475</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a pair of low-complexity user selection schemes with
zero-forcing precoding for multiuser massive MIMO downlink systems, in which
the base station is equipped with a large-scale antenna array. First, we derive
approximations of the ergodic sum rates of the systems invoking the
conventional random user selection (RUS) and the location-dependant user
selection (LUS). Then, the optimal number of simultaneously served user
equipments (UEs), $K^*$, is investigated to maximize the sum rate
approximations. Upon exploiting $K^*$, we develop two user selection schemes,
namely $K^*$-RUS and $K^*$-LUS, where $K^*$ UEs are selected either randomly or
based on their locations. Both of the proposed schemes are independent of the
instantaneous channel state information of small-scale fading, therefore
enjoying the same extremely-low computational complexity as that of the
conventional RUS scheme. Moreover, both of our proposed schemes achieve
significant sum rate improvement over the conventional RUS. In addition, it is
worth noting that like the conventional RUS, the $K^*$-RUS achieves good
fairness among UEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03004</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03004</id><created>2015-04-12</created><updated>2015-04-13</updated><authors><author><keyname>Coras</keyname><forenames>Florin</forenames></author><author><keyname>Domingo-Pascual</keyname><forenames>Jordi</forenames></author><author><keyname>Cabellos-Aparicio</keyname><forenames>Albert</forenames></author></authors><title>On the Scalability of LISP Mappings Caches</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Locator/ID Separation Protocol (LISP) limits the growth of the
Default-Free Zone routing tables by creating a highly aggregatable and
quasi-static Internet core. However, LISP pushes the forwarding state to edge
routers whose timely operation relies on caching of location to identity
bindings. In this paper we develop an analytical model to study the asymptotic
scalability of the LISP cache. Under the assumptions that (i) long-term
popularity can be modeled as a constant Generalized Zipf distribution and (ii)
temporal locality is predominantly determined by long-term popularity, we find
that the scalability of the LISP cache is O(1) with respect to the amount of
prefixes (Internet growth) and users (growth of the LISP site). We validate the
model and discuss the accuracy of our assumptions using several one-day-long
packet traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03013</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03013</id><created>2015-04-12</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author><author><keyname>Falkovich</keyname><forenames>Evgenia</forenames></author></authors><title>Cellular Automata are Generic</title><categories>cs.LO</categories><comments>In Proceedings DCM 2014, arXiv:1504.01927</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 179, 2015, pp. 17-32</journal-ref><doi>10.4204/EPTCS.179.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any algorithm (in the sense of Gurevich's abstract-state-machine
axiomatization of classical algorithms) operating over any arbitrary unordered
domain can be simulated by a dynamic cellular automaton, that is, by a
pattern-directed cellular automaton with unconstrained topology and with the
power to create new cells. The advantage is that the latter is closer to
physical reality. The overhead of our simulation is quadratic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03014</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03014</id><created>2015-04-12</created><authors><author><keyname>Atlee</keyname><forenames>Joanne M.</forenames><affiliation>University of Waterloo, Canada</affiliation></author><author><keyname>Gnesi</keyname><forenames>Stefania</forenames><affiliation>CNR-ISTI, Italy</affiliation></author></authors><title>Proceedings 6th Workshop on Formal Methods and Analysis in SPL
  Engineering</title><categories>cs.SE cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 182, 2015</journal-ref><doi>10.4204/EPTCS.182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workshop aims at reviewing the state of the art and the state of the
practice in which formal methods and analysis approaches are currently applied
in SPLE. This leads to a discussion of a research agenda for the extension of
existing formal approaches and the development of new formal techniques for
dealing with the particular needs of SPLE. To achieve the above objectives, the
workshop is intended as a highly interactive event fostering discussion and
initiating collaborations between the participants from both communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03016</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03016</id><created>2015-04-12</created><authors><author><keyname>Angjelichinoski</keyname><forenames>Marko</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Liu</keyname><forenames>Hongpeng</forenames></author><author><keyname>Loh</keyname><forenames>Poh Chiang</forenames></author><author><keyname>Blaabjerg</keyname><forenames>Frede</forenames></author></authors><title>Power Talk: How to Modulate Data over a DC Micro Grid Bus using Power
  Electronics</title><categories>cs.IT math.IT</categories><comments>IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel communication strategy for DC Micro Grids (MGs), termed
power talk, in which the devices communicate by modulating the power levels in
the DC bus. The information is transmitted by varying the parameters that the
MG units use to control the level of the common bus voltage, while it is
received by processing the bus measurements that units perform. This
communication is challenged by the fact that the voltage level is subject to
random disturbances, as the state of the MG changes with random load
variations. We develop a corresponding communication model and address the
random voltage fluctuations by using coding strategies that transform the MG
into some well-known communication channels. The performance analysis shows
that it is possible to mitigate the random voltage level variations and
communicate reliably over the MG bus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03021</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03021</id><created>2015-04-12</created><authors><author><keyname>Narojczyk</keyname><forenames>J. W.</forenames></author><author><keyname>Wojciechowski</keyname><forenames>K. W.</forenames></author></authors><title>Elastic properties of the degenerate f.c.c. crystal of polydisperse soft
  dimers at zero temperature</title><categories>physics.comp-ph cond-mat.soft cs.CE</categories><comments>Conference: International Workshop on Functional and Nanostructured
  Materials (FNMA)/International Conference on Intermolecular and Magnetic
  Interactions in Matter (IMIM) Location: L'Aquila, ITALY Date: SEP 27-30,2009</comments><journal-ref>Journal of Non Crystalline Solids. 356 pp. 2026-2032 (2010)</journal-ref><doi>10.1016/j.jnoncrysol.2010.05.080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elastic properties of soft, three-dimensional dimers, interacting through
site-site n-inverse-power potential, are determined by computer simulations at
zero temperature. The degenerate crystal of dimers exhibiting (Gaussian) size
distribution of atomic diameters - i.e. size polydispersity - is studied at the
molecular number density $1/\sqrt{2}$; the distance between centers of atoms
forming dimers is considered as a length unit. It is shown that, at the fixed
number density of the dimers, increasing polydispersity causes, typically, an
increase of pressure, elastic constants and Poisson's ratio; the latter is
positive in most direction. A direction is found, however, in which the size
polydispersity causes substantial decrease of Poisson's ratio, down to negative
values for large $n$. Thus, the system is partially auxetic for large
polydispersity and large n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03024</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03024</id><created>2015-04-12</created><updated>2015-04-22</updated><authors><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Taub&#xf6;ck</keyname><forenames>Georg</forenames></author></authors><title>Almost Lossless Analog Compression without Phase Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an information-theoretic framework for phase retrieval.
Specifically, we consider the problem of recovering an unknown n-dimensional
vector x up to an overall sign factor from m=Rn phaseless measurements with
compression rate R and derive a general achievability bound for R.
Surprisingly, it turns out that this bound on the compression rate is the same
as the one for almost lossless analog compression obtained by Wu and Verd\'u
(2010): Phaseless linear measurements are as good as linear measurements with
full phase information in the sense that ignoring the sign of m measurements
only leaves us with an ambiguity with respect to an overall sign factor of x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03026</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03026</id><created>2015-04-12</created><updated>2015-06-14</updated><authors><author><keyname>Schulman</keyname><forenames>Leonard J.</forenames></author><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author></authors><title>Analysis of a Classical Matrix Preconditioning Algorithm</title><categories>cs.DS cs.NA math.NA</categories><comments>The previous version (1) (see also STOC'15) handled UB (&quot;unique
  balance&quot;) input matrices. In this version (2) we extend the work to handle
  all input matrices</comments><msc-class>65F08</msc-class><acm-class>F.2.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a classical iterative algorithm for balancing matrices in the
$L_\infty$ norm via a scaling transformation. This algorithm, which goes back
to Osborne and Parlett \&amp; Reinsch in the 1960s, is implemented as a standard
preconditioner in many numerical linear algebra packages. Surprisingly, despite
its widespread use over several decades, no bounds were known on its rate of
convergence. In this paper we prove that, for any irreducible $n\times n$ (real
or complex) input matrix~$A$, a natural variant of the algorithm converges in
$O(n^3\log(n\rho/\varepsilon))$ elementary balancing operations, where $\rho$
measures the initial imbalance of~$A$ and $\varepsilon$ is the target imbalance
of the output matrix. (The imbalance of~$A$ is $\max_i
|\log(a_i^{\text{out}}/a_i^{\text{in}})|$, where
$a_i^{\text{out}},a_i^{\text{in}}$ are the maximum entries in magnitude in the
$i$th row and column respectively.) This bound is tight up to the $\log n$
factor. A balancing operation scales the $i$th row and column so that their
maximum entries are equal, and requires $O(m/n)$ arithmetic operations on
average, where $m$ is the number of non-zero elements in~$A$. Thus the running
time of the iterative algorithm is $\tilde{O}(n^2m)$. This is the first time
bound of any kind on any variant of the Osborne-Parlett-Reinsch algorithm. We
also prove a conjecture of Chen that characterizes those matrices for which the
limit of the balancing process is independent of the order in which balancing
operations are performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03033</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03033</id><created>2015-04-12</created><updated>2015-10-30</updated><authors><author><keyname>Diaz</keyname><forenames>Rafael</forenames></author><author><keyname>Vargas</keyname><forenames>Angelica</forenames></author></authors><title>On the stability of the PWP method</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PWP method was introduced by Diaz in 2009 as a technique for measuring
indirect influences in complex networks. It depends on a matrix D, provided by
the user, called the matrix of direct influences, and on a positive real
parameter which is part of the method itself. We study changes in the method's
predictions as D and the parameter vary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03048</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03048</id><created>2015-04-12</created><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zheng-An</forenames></author><author><keyname>Zhao</keyname><forenames>Chang-An</forenames></author></authors><title>The weight distributions of two classes of p ary cyclic codes with few
  weights</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><msc-class>11T71 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes have attracted a lot of research interest for decades as they
have efficient encoding and decoding algorithms.
  In this paper, for an odd prime $p$, the weight distributions of two classes
of $p$-ary cyclic codes are completely determined. We show that both codes have
at most five nonzero weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03068</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03068</id><created>2015-04-13</created><updated>2015-04-22</updated><authors><author><keyname>Kamal</keyname><forenames>Ahmad</forenames></author></authors><title>Review Mining for Feature Based Opinion Summarization and Visualization</title><categories>cs.IR cs.CL</categories><comments>6 pages, 5 figures, 2 tables</comments><journal-ref>International Journal of Computer Applications, 119(17), 2015, pp.
  6-13</journal-ref><doi>10.5120/21157-4183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application and usage of opinion mining, especially for business
intelligence, product recommendation, targeted marketing etc. have fascinated
many research attentions around the globe. Various research efforts attempted
to mine opinions from customer reviews at different levels of granularity,
including word-, sentence-, and document-level. However, development of a fully
automatic opinion mining and sentiment analysis system is still elusive. Though
the development of opinion mining and sentiment analysis systems are getting
momentum, most of them attempt to perform document-level sentiment analysis,
classifying a review document as positive, negative, or neutral. Such
document-level opinion mining approaches fail to provide insight about users
sentiment on individual features of a product or service. Therefore, it seems
to be a great help for both customers and manufacturers, if the reviews could
be processed at a finer-grained level and presented in a summarized form
through some visual means, highlighting individual features of a product and
users sentiment expressed over them. In this paper, the design of a unified
opinion mining and sentiment analysis framework is presented at the
intersection of both machine learning and natural language processing
approaches. Also, design of a novel feature-level review summarization scheme
is proposed to visualize mined features, opinions and their polarity values in
a comprehendible way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03071</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03071</id><created>2015-04-13</created><updated>2015-09-18</updated><authors><author><keyname>Sung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Jin</keyname><forenames>Seok Hyun</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Robobarista: Object Part based Transfer of Manipulation Trajectories
  from Crowd-sourcing in 3D Pointclouds</title><categories>cs.RO cs.AI cs.LG</categories><comments>In International Symposium on Robotics Research (ISRR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a large variety of objects and appliances in human environments,
such as stoves, coffee dispensers, juice extractors, and so on. It is
challenging for a roboticist to program a robot for each of these object types
and for each of their instantiations. In this work, we present a novel approach
to manipulation planning based on the idea that many household objects share
similarly-operated object parts. We formulate the manipulation planning as a
structured prediction problem and design a deep learning model that can handle
large noise in the manipulation demonstrations and learns features from three
different modalities: point-clouds, language and trajectory. In order to
collect a large number of manipulation demonstrations for different objects, we
developed a new crowd-sourcing platform called Robobarista. We test our model
on our dataset consisting of 116 objects with 249 parts along with 250 language
instructions, for which there are 1225 crowd-sourced manipulation
demonstrations. We further show that our robot can even manipulate objects it
has never seen before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03076</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03076</id><created>2015-04-13</created><authors><author><keyname>Guo</keyname><forenames>Xueying</forenames></author><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>A High Reliability Asymptotic Approach for Packet Inter-Delivery Time
  Optimization in Cyber-Physical Systems</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><journal-ref>ACM Mobihoc 2015</journal-ref><doi>10.1145/2746285.2746305.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cyber-physical systems such as automobiles, measurement data from sensor
nodes should be delivered to other consumer nodes such as actuators in a
regular fashion. But, in practical systems over unreliable media such as
wireless, it is a significant challenge to guarantee small enough
inter-delivery times for different clients with heterogeneous channel
conditions and inter-delivery requirements. In this paper, we design scheduling
policies aiming at satisfying the inter-delivery requirements of such clients.
We formulate the problem as a risk-sensitive Markov Decision Process (MDP).
Although the resulting problem involves an infinite state space, we first prove
that there is an equivalent MDP involving only a finite number of states. Then
we prove the existence of a stationary optimal policy and establish an
algorithm to compute it in a finite number of steps.
  However, the bane of this and many similar problems is the resulting
complexity, and, in an attempt to make fundamental progress, we further propose
a new high reliability asymptotic approach. In essence, this approach considers
the scenario when the channel failure probabilities for different clients are
of the same order, and asymptotically approach zero. We thus proceed to
determine the asymptotically optimal policy: in a two-client scenario, we show
that the asymptotically optimal policy is a &quot;modified least time-to-go&quot; policy,
which is intuitively appealing and easily implementable; in the general
multi-client scenario, we are led to an SN policy, and we develop an algorithm
of low computational complexity to obtain it. Simulation results show that the
resulting policies perform well even in the pre-asymptotic regime with moderate
failure probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03077</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03077</id><created>2015-04-13</created><authors><author><keyname>Liu</keyname><forenames>Beiyi</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author></authors><title>Iterative-Promoting Variable Step-size Least Mean Square Algorithm For
  Adaptive Sparse Channel Estimation</title><categories>cs.SY</categories><comments>6 pages, 10 figures, submitted for APCC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Least mean square (LMS) type adaptive algorithms have attracted much
attention due to their low computational complexity. In the scenarios of sparse
channel estimation, zero-attracting LMS (ZA-LMS), reweighted ZA-LMS (RZA-LMS)
and reweighted -norm LMS (RL1-LMS) have been proposed to exploit channel
sparsity. However, these proposed algorithms may hard to make tradeoff between
convergence speed and estimation performance with only one step-size. To solve
this problem, we propose three sparse iterative-promoting variable step-size
LMS (IP-VSS-LMS) algorithms with sparse constraints, i.e. ZA, RZA and RL1.
These proposed algorithms are termed as ZA-IPVSS-LMS, RZA-IPVSS-LMS and
RL1-IPVSS-LMS respectively. Simulation results are provided to confirm
effectiveness of the proposed sparse channel estimation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03080</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03080</id><created>2015-04-13</created><updated>2015-04-24</updated><authors><author><keyname>McPherson</keyname><forenames>Andrew</forenames></author><author><keyname>Roth</keyname><forenames>Andrew</forenames></author><author><keyname>Ha</keyname><forenames>Gavin</forenames></author><author><keyname>Shah</keyname><forenames>Sohrab P.</forenames></author><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author><author><keyname>Sahinalp</keyname><forenames>S. Cenk</forenames></author></authors><title>Joint Inference of Genome Structure and Content in Heterogeneous Tumour
  Samples</title><categories>q-bio.GN cs.CE</categories><comments>Presented at RECOMB 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a genomically unstable cancer, a single tumour biopsy will often contain
a mixture of competing tumour clones. These tumour clones frequently differ
with respect to their genomic content (copy number of each gene) and structure
(order of genes on each chromosome). Modern bulk genome sequencing mixes the
signals of tumour clones and contaminating normal cells, complicating inference
of genomic content and structure. We propose a method to unmix tumour and
contaminating normal signals and jointly predict genomic structure and content
of each tumour clone. We use genome graphs to represent tumour clones, and
model the likelihood of the observed reads given clones and mixing proportions.
Our use of haplotype blocks allows us to accurately measure allele specific
read counts, and infer allele specific copy number for each clone. The proposed
method is a heuristic local search based on applying incremental, locally
optimal modifications of the genome graphs. Using simulated data, we show that
our method predicts copy counts and gene adjacencies with reasonable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03083</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03083</id><created>2015-04-13</created><updated>2015-04-28</updated><authors><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Srivastava</keyname><forenames>Rupesh</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Joint Learning of Distributed Representations for Images and Texts</title><categories>cs.CV</categories><comments>This is a previous tech report of a part of the work of
  arXiv:1411.4952. In order to avoid confusion, we'd like to withdraw this
  report from arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report provides extra details of the deep multimodal
similarity model (DMSM) which was proposed in (Fang et al. 2015,
arXiv:1411.4952). The model is trained via maximizing global semantic
similarity between images and their captions in natural language using the
public Microsoft COCO database, which consists of a large set of images and
their corresponding captions. The learned representations attempt to capture
the combination of various visual concepts and cues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03095</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03095</id><created>2015-04-13</created><authors><author><keyname>Leng</keyname><forenames>Junyuan</forenames></author><author><keyname>Zhou</keyname><forenames>Yadong</forenames></author><author><keyname>Zhang</keyname><forenames>Junjie</forenames></author><author><keyname>Hu</keyname><forenames>Chengchen</forenames></author></authors><title>An Inference Attack Model for Flow Table Capacity and Usage: Exploiting
  the Vulnerability of Flow Table Overflow in Software-Defined Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the most competitive solution for next-generation network,
software-defined network (SDN) and its dominant implementation OpenFlow, are
attracting more and more interests. But besides convenience and flexibility,
SDN/OpenFlow also introduces new kinds of limitations and security issues. Of
these limitations, the most obvious and maybe the most neglected one, is the
flow table capacity of SDN/OpenFlow switches.
  In this paper, we proposed a novel inference attack targeting at SDN/OpenFlow
network, which is motivated by the limited flow table capacities of
SDN/OpenFlow switches and the following measurable network performance decrease
resulting from frequent interactions between data plane and control plane when
the flow table is full. To our best knowledge, this is the first proposed
inference attack model of this kind for SDN/OpenFlow. We also implemented an
inference attack framework according to our model and examined its efficiency
and accuracy. The simulation results demonstrate that our framework can infer
the network parameters(flow table capacity and flow table usage) with an
accuracy of 80% or higher. These findings give us a deeper understanding of
SDN/OpenFlow limitations and serve as guidelines to future improvements of
SDN/OpenFlow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03097</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03097</id><created>2015-04-13</created><authors><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Weiss</keyname><forenames>Gregor</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Sampling promotes community structure in social and information networks</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>15 pages, 6 figures, 5 tables</comments><journal-ref>Physica A 432, 206-215 (2015)</journal-ref><doi>10.1016/j.physa.2015.03.048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any network studied in the literature is inevitably just a sampled
representative of its real-world analogue. Additionally, network sampling is
lately often applied to large networks to allow for their faster and more
efficient analysis. Nevertheless, the changes in network structure introduced
by sampling are still far from understood. In this paper, we study the presence
of characteristic groups of nodes in sampled social and information networks.
We consider different network sampling techniques including random node and
link selection, network exploration and expansion. We first observe that the
structure of social networks reveals densely linked groups like communities,
while the structure of information networks is better described by modules of
structurally equivalent nodes. However, despite these notable differences, the
structure of sampled networks exhibits stronger characterization by
community-like groups than the original networks, irrespective of their type
and consistently across various sampling techniques. Hence, rich community
structure commonly observed in social and information networks is to some
extent merely an artifact of sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03101</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03101</id><created>2015-04-13</created><updated>2015-04-17</updated><authors><author><keyname>Ciliberto</keyname><forenames>Carlo</forenames></author><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Convex Learning of Multiple Tasks and their Structure</title><categories>cs.LG</categories><comments>26 pages, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing the amount of human supervision is a key problem in machine learning
and a natural approach is that of exploiting the relations (structure) among
different tasks. This is the idea at the core of multi-task learning. In this
context a fundamental question is how to incorporate the tasks structure in the
learning problem.We tackle this question by studying a general computational
framework that allows to encode a-priori knowledge of the tasks structure in
the form of a convex penalty; in this setting a variety of previously proposed
methods can be recovered as special cases, including linear and non-linear
approaches. Within this framework, we show that tasks and their structure can
be efficiently learned considering a convex optimization problem that can be
approached by means of block coordinate methods such as alternating
minimization and for which we prove convergence to the global minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03106</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03106</id><created>2015-04-13</created><authors><author><keyname>Ciliberto</keyname><forenames>Carlo</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Villa</keyname><forenames>Silvia</forenames></author></authors><title>Learning Multiple Visual Tasks while Discovering their Structure</title><categories>cs.LG cs.CV</categories><comments>19 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning is a natural approach for computer vision applications
that require the simultaneous solution of several distinct but related
problems, e.g. object detection, classification, tracking of multiple agents,
or denoising, to name a few. The key idea is that exploring task relatedness
(structure) can lead to improved performances.
  In this paper, we propose and study a novel sparse, non-parametric approach
exploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valued
functions. We develop a suitable regularization framework which can be
formulated as a convex optimization problem, and is provably solvable using an
alternating minimization approach. Empirical tests show that the proposed
method compares favorably to state of the art techniques and further allows to
recover interpretable structures, a problem of interest in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03109</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03109</id><created>2015-04-13</created><authors><author><keyname>Arapoglou</keyname><forenames>Pantelis-Daniel</forenames></author><author><keyname>Ginesi</keyname><forenames>Alberto</forenames></author><author><keyname>Cioni</keyname><forenames>Stefano</forenames></author><author><keyname>Erl</keyname><forenames>Stefan</forenames></author><author><keyname>Clazzer</keyname><forenames>Federico</forenames></author><author><keyname>Andrenacci</keyname><forenames>Stefano</forenames></author><author><keyname>Vanelli-Coralli</keyname><forenames>Alessandro</forenames></author></authors><title>DVB-S2x Enabled Precoding for High Throughput Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user Multiple-Input Multiple-Output (MU-MIMO) has allowed recent
releases of terrestrial LTE standards to achieve significant improvements in
terms of offered system capacity. The publications of the DVB-S2x standard and
particularly of its novel superframe structure is a key enabler for applying
similar interference management techniques -such as precoding- to multibeam
High Throughput Satellite (HTS) systems. This paper presents results resulting
from European Space Agency (ESA) funded R&amp;D activities concerning the practical
issues that arise when precoding is applied over an aggressive frequency re-use
HTS network. In addressing these issues, the paper also proposes pragmatic
solutions that have been developed in order to overcome these limitations.
Through the application of a comprehensive system simulator, it is demonstrated
that important capacity gains (beyond 40%) are to be expected from applying
precoding even after introducing a number of significant practical impairments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03115</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03115</id><created>2015-04-13</created><authors><author><keyname>Persson</keyname><forenames>Rasmus</forenames></author></authors><title>Bibliometric author evaluation through linear regression on the coauthor
  network</title><categories>cs.DL</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rising trend of coauthored academic works obscures the credit assignment
that is the basis for decisions of funding and career advancements. In this
paper, a simple model based on the assumption of an unvarying &quot;author ability&quot;
is introduced. With this assumption, the weight of author contributions to a
body of coauthored work can be statistically estimated. The method is tested on
a set of some more than five-hundred authors in a coauthor network from the
CiteSeerX database. The ranking obtained agrees fairly well with that given by
total fractional citation counts for an author, but noticeable differences
exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03128</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03128</id><created>2015-04-13</created><authors><author><keyname>Jacob</keyname><forenames>Florian</forenames></author><author><keyname>Haeb-Umbach</keyname><forenames>Reinhold</forenames></author></authors><title>Absolute Geometry Calibration of Distributed Microphone Arrays in an
  Audio-Visual Sensor Network</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint audio-visual speaker tracking requires that the locations of
microphones and cameras are known and that they are given in a common
coordinate system. Sensor self-localization algorithms, however, are usually
separately developed for either the acoustic or the visual modality and return
their positions in a modality specific coordinate system, often with an unknown
rotation, scaling and translation between the two. In this paper we propose two
techniques to determine the positions of acoustic sensors in a common
coordinate system, based on audio-visual correlates, i.e., events that are
localized by both, microphones and cameras separately. The first approach maps
the output of an acoustic self-calibration algorithm by estimating rotation,
scale and translation to the visual coordinate system, while the second solves
a joint system of equations with acoustic and visual directions of arrival as
input. The evaluation of the two strategies reveals that joint calibration
outperforms the mapping approach and achieves an overall calibration error of
0.20m even in reverberant environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03149</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03149</id><created>2015-04-13</created><updated>2015-04-19</updated><authors><author><keyname>Sarma</keyname><forenames>Siddhartha</forenames></author><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Secure Transmission in Amplify-and-Forward Diamond Networks with a
  Single Eavesdropper</title><categories>cs.IT math.IT</categories><comments>12pt font, 18 pages, 1 figure, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unicast communication over a network of $M$-parallel relays in the presence
of an eavesdropper is considered. The relay nodes, operating under individual
power constraints, amplify and forward the signals received at their inputs.
The problem of the maximum secrecy rate achievable with AF relaying is
addressed. Previous work on this problem provides iterative algorithms based on
semidefinite relaxation. However, those algorithms result in suboptimal
performance without any performance and convergence guarantees. We address this
problem for three specific network models, with real-valued channel gains. We
propose a novel transformation that leads to convex optimization problems. Our
analysis leads to (i)a polynomial-time algorithm to compute the optimal secure
AF rate for two of the models and (ii) a closed-form expression for the optimal
secure rate for the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03151</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03151</id><created>2015-04-13</created><authors><author><keyname>Qin</keyname><forenames>Yutong</forenames></author><author><keyname>Lin</keyname><forenames>Jianbiao</forenames></author><author><keyname>Huang</keyname><forenames>Xiang</forenames></author></authors><title>Massively Parallel Ray Tracing Algorithm Using GPU</title><categories>cs.GR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Ray tracing is a technique for generating an image by tracing the path of
light through pixels in an image plane and simulating the effects of
high-quality global illumination at a heavy computational cost. Because of the
high computation complexity, it can't reach the requirement of real-time
rendering. The emergence of many-core architectures, makes it possible to
reduce significantly the running time of ray tracing algorithm by employing the
powerful ability of floating point computation. In this paper, a new GPU
implementation and optimization of the ray tracing to accelerate the rendering
process is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03154</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03154</id><created>2015-04-13</created><updated>2015-04-14</updated><authors><author><keyname>Pasquale</keyname><forenames>Giulia</forenames></author><author><keyname>Ciliberto</keyname><forenames>Carlo</forenames></author><author><keyname>Odone</keyname><forenames>Francesca</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How
  Many Objects can iCub Learn?</title><categories>cs.RO cs.CV cs.LG</categories><comments>18 pages, 9 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to visually recognize objects is a fundamental skill for robotics
systems. Indeed, a large variety of tasks involving manipulation, navigation or
interaction with other agents, deeply depends on the accurate understanding of
the visual scene. Yet, at the time being, robots are lacking good visual
perceptual systems, which often become the main bottleneck preventing the use
of autonomous agents for real-world applications.
  Lately in computer vision, systems that learn suitable visual representations
and based on multi-layer deep convolutional networks are showing remarkable
performance in tasks such as large-scale visual recognition and image
retrieval. To this regard, it is natural to ask whether such remarkable
performance would generalize also to the robotic setting.
  In this paper we investigate such possibility, while taking further steps in
developing a computational vision system to be embedded on a robotic platform,
the iCub humanoid robot. In particular, we release a new dataset ({\sc
iCubWorld28}) that we use as a benchmark to address the question: {\it how many
objects can iCub recognize?} Our study is developed in a learning framework
which reflects the typical visual experience of a humanoid robot like the iCub.
Experiments shed interesting insights on the strength and weaknesses of current
computer vision approaches applied in real robotic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03161</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03161</id><created>2015-02-11</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Random intersection graphs and their applications in security, wireless
  communication, and social networks</title><categories>cs.DM cs.CR cs.SI math.CO physics.soc-ph</categories><comments>This is an invited paper in Information Theory and Applications
  Workshop (ITA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random intersection graphs have received much interest and been used in
diverse applications. They are naturally induced in modeling secure sensor
networks under random key predistribution schemes, as well as in modeling the
topologies of social networks including common-interest networks, collaboration
networks, and actor networks. Simply put, a random intersection graph is
constructed by assigning each node a set of items in some random manner and
then putting an edge between any two nodes that share a certain number of
items.
  Broadly speaking, our work is about analyzing random intersection graphs, and
models generated by composing it with other random graph models including
random geometric graphs and Erd\H{o}s-R\'enyi graphs. These compositional
models are introduced to capture the characteristics of various complex natural
or man-made networks more accurately than the existing models in the
literature. For random intersection graphs and their compositions with other
random graphs, we study properties such as ($k$-)connectivity,
($k$-)robustness, and containment of perfect matchings and Hamilton cycles. Our
results are typically given in the form of asymptotically exact probabilities
or zero-one laws specifying critical scalings, and provide key insights into
the design and analysis of various real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03170</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03170</id><created>2015-04-09</created><authors><author><keyname>Lawrencenko</keyname><forenames>Serge</forenames></author><author><keyname>Duborkina</keyname><forenames>Irina A.</forenames></author></authors><title>Search algorithms for efficient logistics chains</title><categories>cs.DS</categories><comments>10 pages, 1 figure, in Russian</comments><msc-class>90C27 (Primary), 90B06, 90B10, 05C85, 90B18, 68R10, 94C15
  (Secondary)</msc-class><journal-ref>Service in Russia and Abroad, Vol. 9 (2015), No. 2 (58), 37-48</journal-ref><doi>10.12737/11889</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logistics networks arise whenever there is a transfer of material substance
or objects (such as checked baggage on international flights) as well as
energy, information, or finance through links (channels). A general concept of
logistics network is suggested and motivated for modeling a service of any kind
supplied through links between the nodes of the network. The efficiency of a
single link is defined to be the ratio of the volume of useful service at the
output node to the volume of expended service at the input node of the link
(for a specific period of time). Similarly, the efficiency of a chain is the
ratio of the volume of service at the output to the volume of service at the
input of the chain. The overall efficiency of the chain is calculated as the
product of the efficiencies of its links; the more efficiency of the chain, the
less are the losses in the chain. This paper introduces the notion of
inadequacy of service in such a way that the overall inadequacy of a chain is
equal to the sum of the inadequacies of its links. So the efficiencies are
being multiplied, whereas the inadequacies are being added. Thus, the
antagonistic pair (efficiency, inadequacy) appears to be analogous to the pair
(reliability, entropy) in communication theory. Various possible
interpretations of the proposed logistic model are presented: energy, material,
information and financial networks. Four algorithms are provided for logistics
chain search: two algorithms for finding the most effective chain from a
specified origin to a specified destination, and two algorithms for finding the
guaranteed minimum level of service between any pair of unspecified nodes in a
given network. An example is shown as to how one of the algorithms finds the
most efficient energy chain from the electrical substation to a specified user
in a concrete energy network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03182</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03182</id><created>2015-04-13</created><updated>2015-04-20</updated><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>P&#xe4;&#xe4;kk&#xf6;nen</keyname><forenames>Joonas</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>A Low-Complexity Message Recovery Method for Compute-and-Forward
  Relaying</title><categories>cs.IT math.IT</categories><comments>5 figures, 5 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Compute-and-Forward relaying strategy achieves high computation rates by
decoding linear combinations of transmitted messages at intermediate relays.
However, if the involved relays independently choose which combinations of the
messages to decode, there is no guarantee that the overall system of linear
equations is solvable at the destination. In this article it is shown that, for
a Gaussian fading channel model with two transmitters and two relays, always
choosing the combination that maximizes the computation rate often leads to a
case where the original messages cannot be recovered. It is further shown that
by limiting the relays to select from carefully designed sets of equations, a
solvable system can be guaranteed while maintaining high computation rates. The
proposed method has a constant computational complexity and requires no
information exchange between the relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03184</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03184</id><created>2015-04-13</created><authors><author><keyname>Clingman</keyname><forenames>T.</forenames></author><author><keyname>Murugan</keyname><forenames>Jeff</forenames></author><author><keyname>Shock</keyname><forenames>Jonathan P.</forenames></author></authors><title>Probability Density Functions from the Fisher Information Metric</title><categories>cs.IT math.DG math.IT math.ST physics.data-an stat.TH</categories><comments>16 pages, no figures</comments><report-no>QGaSLAB-15-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a general relation between the spatially disjoint product of
probability density functions and the sum of their Fisher information metric
tensors. We then utilise this result to give a method for constructing the
probability density functions for an arbitrary Riemannian Fisher information
metric tensor. We note further that this construction is extremely
unconstrained, depending only on certain continuity properties of the
probability density functions and a select symmetry of their domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03195</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03195</id><created>2015-04-13</created><updated>2015-06-26</updated><authors><author><keyname>Malek-Mohammadi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author></authors><title>Upper Bounds on the Error of Sparse Vector and Low-Rank Matrix Recovery</title><categories>cs.IT math.IT</categories><comments>Submitted to Elsevier Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a solution $\widetilde{\mathbf{x}}$ to an underdetermined linear
system $\mathbf{b} = \mathbf{A} \mathbf{x}$ is given. $\widetilde{\mathbf{x}}$
is approximately sparse meaning that it has a few large components compared to
other small entries. However, the total number of nonzero components of
$\widetilde{\mathbf{x}}$ is large enough to violate any condition for the
uniqueness of the sparsest solution. On the other hand, if only the dominant
components are considered, then it will satisfy the uniqueness conditions. One
intuitively expects that $\widetilde{\mathbf{x}}$ should not be far from the
true sparse solution $\mathbf{x}_0$. We show that this intuition is the case by
providing an upper bound on $\| \widetilde{\mathbf{x}} - \mathbf{x}_0\|$ which
is a function of the magnitudes of small components of $\widetilde{\mathbf{x}}$
but independent from $\mathbf{x}_0$. This result is extended to the case that
$\mathbf{b}$ is perturbed by noise. Additionally, we generalize the upper
bounds to the low-rank matrix recovery problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03212</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03212</id><created>2015-04-13</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author></authors><title>Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th
  Rule in Discrete Settings</title><categories>cs.NE</categories><comments>This is the full version of a paper that is to appear at GECCO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While evolutionary algorithms are known to be very successful for a broad
range of applications, the algorithm designer is often left with many
algorithmic choices, for example, the size of the population, the mutation
rates, and the crossover rates of the algorithm. These parameters are known to
have a crucial influence on the optimization time, and thus need to be chosen
carefully, a task that often requires substantial efforts. Moreover, the
optimal parameters can change during the optimization process. It is therefore
of great interest to design mechanisms that dynamically choose best-possible
parameters. An example for such an update mechanism is the one-fifth success
rule for step-size adaption in evolutionary strategies. While in continuous
domains this principle is well understood also from a mathematical point of
view, no comparable theory is available for problems in discrete domains.
  In this work we show that the one-fifth success rule can be effective also in
discrete settings. We regard the $(1+(\lambda,\lambda))$~GA proposed in
[Doerr/Doerr/Ebel: From black-box complexity to designing new genetic
algorithms, TCS 2015]. We prove that if its population size is chosen according
to the one-fifth success rule then the expected optimization time on
\textsc{OneMax} is linear. This is better than what \emph{any} static
population size $\lambda$ can achieve and is asymptotically optimal also among
all adaptive parameter choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03213</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03213</id><created>2015-04-13</created><authors><author><keyname>Di Francesco</keyname><forenames>Paolo</forenames></author><author><keyname>Malandrino</keyname><forenames>Francesco</forenames></author><author><keyname>Forde</keyname><forenames>Tim K.</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author></authors><title>A Sharing- and Competition-Aware Framework for Cellular Network
  Evolution Planning</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile network operators are facing the difficult task of significantly
increasing capacity to meet projected demand while keeping CAPEX and OPEX down.
We argue that infrastructure sharing is a key consideration in operators'
planning of the evolution of their networks, and that such planning can be
viewed as a stage in the cognitive cycle. In this paper, we present a framework
to model this planning process while taking into account both the ability to
share resources and the constraints imposed by competition regulation (the
latter quantified using the Herfindahl index). Using real-world demand and
deployment data, we find that the ability to share infrastructure essentially
moves capacity from rural, sparsely populated areas (where some of the current
infrastructure can be decommissioned) to urban ones (where most of the
next-generation base stations would be deployed), with significant increases in
resource efficiency. Tight competition regulation somewhat limits the ability
to share but does not entirely jeopardize those gains, while having the
secondary effect of encouraging the wider deployment of next-generation
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03217</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03217</id><created>2015-04-13</created><authors><author><keyname>Gonz&#xe1;lez</keyname><forenames>Pedro Henrique</forenames></author><author><keyname>Simonetti</keyname><forenames>Luidi</forenames></author><author><keyname>Michelon</keyname><forenames>Philippe</forenames></author><author><keyname>Martinhon</keyname><forenames>Carlos</forenames></author><author><keyname>Santos</keyname><forenames>Edcarllos</forenames></author></authors><title>A Variable Fixing Heuristic with Local Branching for the Fixed Charge
  Uncapacitated Network Design Problem with User-optimal Flow</title><categories>cs.DS</categories><comments>32 pages, 10 figures, submitted to Computers and Operations Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an iterated local search for the fixed-charge
uncapacitated network design problem with user-optimal flow (FCNDP-UOF), which
concerns routing multiple commodities from its origin to its destination by
signing a network through selecting arcs, with an objective of minimizing the
sum of the fixed costs of the selected arcs plus the sum of variable costs
associated to the flows on each arc. Besides that, since the FCNDP-UOF is a
bi-level problem, each commodity has to be transported through a shortest path,
concerning the edges length, in the built network. The proposed algorithm
generate a initial solution using a variable fixing heuristic. Then a local
branching strategy is applied to improve the quality of the solution. At last,
an efficient perturbation strategy is presented to perform cycle-based moves to
explore different parts of the solution space. Computational experiments shows
that the proposed solution method consistently produces high-quality solutions
in reasonable computational times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03218</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03218</id><created>2015-04-13</created><authors><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author><author><keyname>Avgouleas</keyname><forenames>Ioannis</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Flexible Allocation of Heterogeneous Resources to Services on an IoT
  Device</title><categories>cs.NI</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Internet of Things (IoT), devices and gateways may be equipped with
multiple, heterogeneous network interfaces which should be utilized by a large
number of services. In this work, we model the problem of assigning services'
resource demands to a device's heterogeneous interfaces and give a Mixed
Integer Linear Program (MILP) formulation for it. For meaningful instance sizes
the MILP model gives optimal solutions to the presented computationally-hard
problem. We provide insightful results discussing the properties of the derived
solutions with respect to the splitting of services to different interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03239</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03239</id><created>2015-04-13</created><authors><author><keyname>Pai</keyname><forenames>Rekha R</forenames></author></authors><title>Global Value Numbering: A Precise and Efficient Algorithm</title><categories>cs.PL</categories><comments>6 pages, 3 figures, an extended version to be submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global Value Numbering (GVN) is an important static analysis to detect
equivalent expressions in a program. We present an iterative data-flow analysis
GVN algorithm in SSA for the purpose of detecting total redundancies. The
central challenge is defining a join operation to detect equivalences at a join
point in polynomial time such that later occurrences of redundant expressions
could be detected. For this purpose, we introduce the novel concept of value
$\phi$-function. We claim the algorithm is precise and takes only polynomial
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03240</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03240</id><created>2015-04-13</created><authors><author><keyname>Negusse</keyname><forenames>Senay</forenames></author><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Phase-Noise Mitigation in OFDM by Best Match Trajectories</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach to phase-noise compensation. The basic
idea is to approximate the phase-noise statistics by a finite number of
realizations, i.e., a phase-noise codebook. The receiver then uses an augmented
received signal model, where the codebook index is estimated along with other
parameters. The realization of the basic idea depends on the details of the air
interface, the phase-noise statistics, the propagation scenario and the
computational constraints. In this paper, we will focus on a MQAM-OFDM system
with pilot sub-carriers within each OFDM symbol. The channel is frequency
selective, fading and unknown. A decision-feedback method is employed to
further enhance performance of the system. Simulation results are shown for
uncoded and coded systems to illustrate the performance of the algorithm, which
is also compared with previously employed methods. Our simulations show that
for a 16-QAM coded OFDM system over a frequency selective Rayleigh fading
channel affected by phase noise with root-mean-square (RMS) of 14.4 degrees per
OFDM symbol, the proposed algorithm is 1.5dB from the ideal phase-noise free
case at a BER of $10^{-4}$. The performance of the best reference scheme is
2.5dB from the ideal case at BER of $10^{-4}$. The proposed scheme is also
computationally attractive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03242</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03242</id><created>2015-04-13</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Huang</keyname><forenames>Howard</forenames></author><author><keyname>Viswanathan</keyname><forenames>Harish</forenames></author></authors><title>Wide-area Wireless Communication Challenges for the Internet of Things</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aided by the ubiquitous wireless connectivity, declining communication costs,
and the emergence of cloud platforms, the deployment of Internet of Things
(IoT) devices and services is accelerating. Most major mobile network operators
view machine-to-machine (M2M) communication networks for supporting IoT as a
significant source of new revenue. In this paper, we motivate the need for
wide-area M2M wireless networks, especially for short data packet communication
to support a very large number of IoT devices. We first present a brief
overview of current and emerging technologies for supporting wide area M2M, and
then using communication theory principles, discuss the fundamental challenges
and potential solutions for these networks, highlighting tradeoffs and
strategies for random and scheduled access. We conclude with recommendations
for how future 5G networks should be designed for efficient wide-area M2M
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03246</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03246</id><created>2015-04-13</created><authors><author><keyname>Krishnasamy</keyname><forenames>Subhashini</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Gupta</keyname><forenames>Piyush</forenames></author></authors><title>On the Scaling of Interference Alignment Under Delay and Power
  Constraints</title><categories>cs.IT math.IT</categories><comments>Shorter version to appear in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless standards such as 5G envision dense wireless networks with
large number of simultaneously connected devices. In this context, interference
management becomes critical in achieving high spectral efficiency. Orthogonal
signaling, which limits the number of users utilizing the resource
simultaneously, gives a sum-rate that remains constant with increasing number
of users. An alternative approach called interference alignment promises a
throughput that scales linearly with the number of users. However, this
approach requires very high SNR or long time duration for sufficient channel
variation, and therefore may not be feasible in real wireless systems. We
explore ways to manage interference in large networks with delay and power
constraints. Specifically, we devise an interference phase alignment strategy
that combines precoding and scheduling without using power control to exploit
the diversity inherent in a system with large number of users. We show that
this scheme achieves a sum-rate that scales almost logarithmically with the
number of users. We also show that no scheme using single symbol phase
alignment, which is asymmetric complex signaling restricted to a single complex
symbol, can achieve better than logarithmic scaling of the sum-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03247</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03247</id><created>2015-04-13</created><authors><author><keyname>Afrati</keyname><forenames>Foto N.</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author><author><keyname>Vasilakopoulos</keyname><forenames>Angelos</forenames></author></authors><title>Handling Skew in Multiway Joins in Parallel Processing</title><categories>cs.DB</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handling skew is one of the major challenges in query processing. In
distributed computational environments such as MapReduce, uneven distribution
of the data to the servers is not desired. One of the dominant measures that we
want to optimize in distributed environments is communication cost. In a
MapReduce job this is the amount of data that is transferred from the mappers
to the reducers. In this paper we will introduce a novel technique for handling
skew when we want to compute a multiway join in one MapReduce round with
minimum communication cost. This technique is actually an adaptation of the
Shares algorithm [Afrati et. al, TKDE 2011].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03249</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03249</id><created>2015-03-28</created><authors><author><keyname>Baghsorkhi</keyname><forenames>Sina S.</forenames></author><author><keyname>Suetin</keyname><forenames>Sergey P.</forenames></author></authors><title>Embedding AC Power Flow with Voltage Control in the Complex Plane : The
  Case of Analytic Continuation via Pad\'e Approximants</title><categories>cs.SY math.CV math.NA</categories><comments>9 pages, 10 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to embed the AC power flow problem with voltage
magnitude constraints in the complex plane. Modeling the action of network
controllers that regulate the magnitude of voltage phasors is a challenging
task in the complex plane as it has to preserve the framework of holomorphicity
for obtention of these complex variables with fixed magnitude. Hence this paper
presents a significant step in the development of the idea of Holomorphic
Embedding Load Flow Method (HELM), introduced in 2012, that exploits the theory
of analytic continuation, especially the monodromy theorem for resolving issues
that have plagued conventional numerical methods for decades. This paper also
illustrates the indispensable role of Pad\'e approximants for analytic
continuation of complex functions, expressed as power series, beyond the
boundary of convergence of the series. Later the paper demonstrates the
superiority of the proposed method over the well-established Newton-Raphson as
well as the recently developed semidefinite and moment relaxation of power flow
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03253</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03253</id><created>2015-04-13</created><authors><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>Carli</keyname><forenames>Francesca P.</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author><author><keyname>Ljung</keyname><forenames>Lennart</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>Maximum entropy properties of discrete-time first-order stable spline
  kernel</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first order stable spline (SS-1) kernel is used extensively in
regularized system identification. In particular, the stable spline estimator
models the impulse response as a zero-mean Gaussian process whose covariance is
given by the SS-1 kernel. In this paper, we discuss the maximum entropy
properties of this prior. In particular, we formulate the exact maximum entropy
problem solved by the SS-1 kernel without Gaussian and uniform sampling
assumptions. Under general sampling schemes, we also explicitly derive the
special structure underlying the SS-1 kernel (e.g. characterizing the
tridiagonal nature of its inverse), also giving to it a maximum entropy
covariance completion interpretation. Along the way similar maximum entropy
properties of the Wiener kernel are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03256</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03256</id><created>2015-04-13</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>A Survey of Linguistic Structures for Application-level Fault-Tolerance</title><categories>cs.SE</categories><comments>Paper appeared in ACM Computing Surveys, Vol. 40, No. 2 (April 2008)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structures for the expression of fault-tolerance provisions into the
application software are the central topic of this paper. Structuring
techniques answer the questions &quot;How to incorporate fault-tolerance in the
application layer of a computer program&quot; and &quot;How to manage the fault-tolerant
code&quot;. As such, they provide means to control complexity, the latter being a
relevant factor for the introduction of design faults. This fact and the ever
increasing complexity of today's distributed software justify the need for
simple, coherent, and effective structures for the expression of
fault-tolerance in the application software. In this text we first define a
&quot;base&quot; of structural attributes with which application-level fault-tolerance
structures can be qualitatively assessed and compared with each other and with
respect to the above mentioned needs. This result is then used to provide an
elaborated survey of the state-of-the-art of application-level fault-tolerance
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03257</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03257</id><created>2015-04-13</created><authors><author><keyname>Arnosti</keyname><forenames>Nick</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>The (Non)-Existence of Stable Mechanisms in Incomplete Information
  Environments</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-sided matching markets, and study the incentives of agents to
circumvent a centralized clearing house by signing binding contracts with one
another. It is well-known that if the clearing house implements a stable match
and preferences are known, then no group of agents can profitably deviate in
this manner.
  We ask whether this property holds even when agents have incomplete
information about their own preferences or the preferences of others. We find
that it does not. In particular, when agents are uncertain about the
preferences of others, every mechanism is susceptible to deviations by groups
of agents. When, in addition, agents are uncertain about their own preferences,
every mechanism is susceptible to deviations in which a single pair of agents
agrees in advance to match to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03268</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03268</id><created>2015-04-13</created><authors><author><keyname>Anubi</keyname><forenames>Olugbenga Moses</forenames></author><author><keyname>Clemen</keyname><forenames>Layne</forenames></author></authors><title>Localization of Control Synthesis Problem for Large-Scale Interconnected
  System Using IQC and Dissipativity Theories</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synthesis problem for the compositional performance certification of
interconnected systems is considered. A fairly unified description of control
synthesis problem is given using integral quadratic constraints (IQC) and
dissipativity. Starting with a given large-scale interconnected system and a
global performance objective, an optimization problem is formulated to search
for admissible dissipativity properties of each subsystems. Local control laws
are then synthesized to certify the relevant dissipativity properties.
Moreover, the term localization is introduced to describe a finite collection
of syntheses problems, for the local subsystems, which are a feasibility
certificate for the global synthesis problem. Consequently, the problem of
localizing the global problem to a smaller collection of disjointed sets of
subsystems, called groups, is considered. This works looks promising as another
way of looking at decentralized control and also as a way of doing performance
specifications for components in a large-scale system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03274</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03274</id><created>2015-04-13</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Distributed Stochastic Market Clearing with High-Penetration Wind Power</title><categories>math.OC cs.DC</categories><comments>To appear in IEEE Transactions on Power Systems; 12 pages and 9
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating renewable energy into the modern power grid requires
risk-cognizant dispatch of resources to account for the stochastic availability
of renewables. Toward this goal, day-ahead stochastic market clearing with
high-penetration wind energy is pursued in this paper based on the DC optimal
power flow (OPF). The objective is to minimize the social cost which consists
of conventional generation costs, end-user disutility, as well as a risk
measure of the system re-dispatching cost. Capitalizing on the conditional
value-at-risk (CVaR), the novel model is able to mitigate the potentially high
risk of the recourse actions to compensate wind forecast errors. The resulting
convex optimization task is tackled via a distribution-free sample average
based approximation to bypass the prohibitively complex high-dimensional
integration. Furthermore, to cope with possibly large-scale dispatchable loads,
a fast distributed solver is developed with guaranteed convergence using the
alternating direction method of multipliers (ADMM). Numerical results tested on
a modified benchmark system are reported to corroborate the merits of the novel
framework and proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03275</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03275</id><created>2015-04-13</created><updated>2015-07-29</updated><authors><author><keyname>Mahmoody</keyname><forenames>Ahmad</forenames></author><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Wiggins: Detecting Valuable Information in Dynamic Networks Using
  Limited Resources</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting new information and events in a dynamic network by probing
individual nodes has many practical applications: discovering new webpages,
analyzing influence properties in network, and detecting failure propagation in
electronic circuits or infections in public drinkable water systems. In
practice, it is infeasible for anyone but the owner of the network (if
existent) to monitor all nodes at all times. In this work we study the
constrained setting when the observer can only probe a small set of nodes at
each time step to check whether new pieces of information (items) have reached
those nodes.
  We formally define the problem through an infinite time generating process
that places new items in subsets of nodes according to an unknown probability
distribution. Items have an exponentially decaying novelty, modeling their
decreasing value. The observer uses a probing schedule (i.e., a probability
distribution over the set of nodes) to choose, at each time step, a small set
of nodes to check for new items. The goal is to compute a schedule that
minimizes the average novelty of undetected items. We present an algorithm,
WIGGINS, to compute the optimal schedule through convex optimization, and then
show how it can be adapted when the parameters of the problem must be learned
or change over time. We also present a scalable variant of WIGGINS for the
MapReduce framework. The results of our experimental evaluation on real social
networks demonstrate the practicality of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03277</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03277</id><created>2015-04-13</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>The Algorithm of Pipelined Gossiping</title><categories>cs.DC</categories><comments>Paper published in the Journal of Systems Architecture, Vol. 52
  (2006). Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of gossiping algorithms depending on a parameter permutation is
introduced, formalized, and discussed. Several of its members are analyzed and
their asymptotic behaviour is revealed, including a member whose model and
performance closely follows the one of hardware pipelined processors. This
similarity is exposed. An optimizing algorithm is finally proposed and
discussed as a general strategy to increase the performance of the base
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03285</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03285</id><created>2015-04-13</created><authors><author><keyname>Radenovic</keyname><forenames>Filip</forenames></author><author><keyname>Jegou</keyname><forenames>Herve</forenames></author><author><keyname>Chum</keyname><forenames>Ondrej</forenames></author></authors><title>Multiple Measurements and Joint Dimensionality Reduction for Large Scale
  Image Search with Short Vectors - Extended Version</title><categories>cs.CV</categories><comments>Extended version of the ICMR 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the construction of a short-vector (128D) image
representation for large-scale image and particular object retrieval. In
particular, the method of joint dimensionality reduction of multiple
vocabularies is considered. We study a variety of vocabulary generation
techniques: different k-means initializations, different descriptor
transformations, different measurement regions for descriptor extraction. Our
extensive evaluation shows that different combinations of vocabularies, each
partitioning the descriptor space in a different yet complementary manner,
results in a significant performance improvement, which exceeds the
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03287</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03287</id><created>2015-04-13</created><authors><author><keyname>Khan</keyname><forenames>Mohammed Shafiul Alam</forenames></author><author><keyname>Mitchell</keyname><forenames>Chris J</forenames></author></authors><title>Improving Air Interface User Privacy in Mobile Telephony</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the security properties of 3G and 4G mobile networks have
significantly improved by comparison with 2G (GSM), significant shortcomings
remain with respect to user privacy. A number of possible modifications to 2G,
3G and 4G protocols have been proposed designed to provide greater user
privacy; however, they all require significant modifications to existing
deployed infrastructures, which are almost certainly impractical to achieve in
practice. In this article we propose an approach which does not require any
changes to the existing deployed network infrastructures or mobile devices, but
offers improved user identity protection over the air interface. The proposed
scheme makes use of multiple IMSIs for an individual USIM to offer a degree of
pseudonymity for a user. The only changes required are to the operation of the
authentication centre in the home network and to the USIM, and the scheme could
be deployed immediately since it is completely transparent to the existing
mobile telephony infrastructure. We present two different approaches to the use
and management of multiple IMSIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03293</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03293</id><created>2015-04-13</created><updated>2016-01-13</updated><authors><author><keyname>Zhang</keyname><forenames>Yuting</forenames></author><author><keyname>Sohn</keyname><forenames>Kihyuk</forenames></author><author><keyname>Villegas</keyname><forenames>Ruben</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author></authors><title>Improving Object Detection with Deep Convolutional Networks via Bayesian
  Optimization and Structured Prediction</title><categories>cs.CV</categories><comments>CVPR 2015</comments><doi>10.1109/CVPR.2015.7298621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection systems based on the deep convolutional neural network (CNN)
have recently made ground- breaking advances on several object detection
benchmarks. While the features learned by these high-capacity neural networks
are discriminative for categorization, inaccurate localization is still a major
source of error for detection. Building upon high-capacity CNN architectures,
we address the localization problem by 1) using a search algorithm based on
Bayesian optimization that sequentially proposes candidate regions for an
object bounding box, and 2) training the CNN with a structured loss that
explicitly penalizes the localization inaccuracy. In experiments, we
demonstrated that each of the proposed methods improves the detection
performance over the baseline method on PASCAL VOC 2007 and 2012 datasets.
Furthermore, two methods are complementary and significantly outperform the
previous state-of-the-art when combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03294</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03294</id><created>2015-04-13</created><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Peng</keyname><forenames>Pan</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Testing Cluster Structure of Graphs</title><categories>cs.DS</categories><comments>Full version of STOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recognizing the cluster structure of a graph in the
framework of property testing in the bounded degree model. Given a parameter
$\varepsilon$, a $d$-bounded degree graph is defined to be $(k,
\phi)$-clusterable, if it can be partitioned into no more than $k$ parts, such
that the (inner) conductance of the induced subgraph on each part is at least
$\phi$ and the (outer) conductance of each part is at most
$c_{d,k}\varepsilon^4\phi^2$, where $c_{d,k}$ depends only on $d,k$. Our main
result is a sublinear algorithm with the running time
$\widetilde{O}(\sqrt{n}\cdot\mathrm{poly}(\phi,k,1/\varepsilon))$ that takes as
input a graph with maximum degree bounded by $d$, parameters $k$, $\phi$,
$\varepsilon$, and with probability at least $\frac23$, accepts the graph if it
is $(k,\phi)$-clusterable and rejects the graph if it is $\varepsilon$-far from
$(k, \phi^*)$-clusterable for $\phi^* = c'_{d,k}\frac{\phi^2
\varepsilon^4}{\log n}$, where $c'_{d,k}$ depends only on $d,k$. By the lower
bound of $\Omega(\sqrt{n})$ on the number of queries needed for testing graph
expansion, which corresponds to $k=1$ in our problem, our algorithm is
asymptotically optimal up to polylogarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03303</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03303</id><created>2015-04-09</created><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author></authors><title>Ultimate Intelligence Part II: Physical Measure and Complexity of
  Intelligence</title><categories>cs.AI</categories><comments>This paper was initially submitted to ALT-2014. We are taking the
  valuable opinions of the anonymous reviewers into account. Many thanks to
  Laurent Orseau for his constructive comments on the draft, which inspired
  this revision. arXiv admin note: substantial text overlap with
  arXiv:1501.00601</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate physical measures and limits of intelligence that are
objective and useful. We propose a universal measure of operator induction
fitness, and show how it can be used in a reinforcement learning model, and a
self-preserving agent model based on the free energy principle. We extend
logical depth and conceptual jump size measures to stochastic domains, and
elucidate their relations. We introduce a graphical model of computational
complexity, apply physical complexity measures that involve space-time extents,
energy, and space to universal induction, and introduce new variants of
conceptual jump size. We show relations between energy, logical depth and
volume of computation for inductive inference. We also introduce an
energy-bounded algorithmic complexity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03306</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03306</id><created>2015-04-13</created><authors><author><keyname>Rapti</keyname><forenames>Angeliki</forenames></author><author><keyname>Tsichlas</keyname><forenames>Kostas</forenames></author><author><keyname>Sioutas</keyname><forenames>Spiros</forenames></author><author><keyname>Tzimas</keyname><forenames>Giannis</forenames></author></authors><title>Virus Propagation in Multiple Profile Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we have a virus or one competing idea/product that propagates over a
multiple profile (e.g., social) network. Can we predict what proportion of the
network will actually get &quot;infected&quot; (e.g., spread the idea or buy the
competing product), when the nodes of the network appear to have different
sensitivity based on their profile? For example, if there are two profiles
$\mathcal{A}$ and $\mathcal{B}$ in a network and the nodes of profile
$\mathcal{A}$ and profile $\mathcal{B}$ are susceptible to a highly spreading
virus with probabilities $\beta_{\mathcal{A}}$ and $\beta_{\mathcal{B}}$
respectively, what percentage of both profiles will actually get infected from
the virus at the end? To reverse the question, what are the necessary
conditions so that a predefined percentage of the network is infected? We
assume that nodes of different profiles can infect one another and we prove
that under realistic conditions, apart from the weak profile (great
sensitivity), the stronger profile (low sensitivity) will get infected as well.
First, we focus on cliques with the goal to provide exact theoretical results
as well as to get some intuition as to how a virus affects such a multiple
profile network. Then, we move to the theoretical analysis of arbitrary
networks. We provide bounds on certain properties of the network based on the
probabilities of infection of each node in it when it reaches the steady state.
Finally, we provide extensive experimental results that verify our theoretical
results and at the same time provide more insight on the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03315</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03315</id><created>2014-11-07</created><authors><author><keyname>Agarwal</keyname><forenames>Saurabh</forenames></author><author><keyname>Johari</keyname><forenames>Punit Kumar</forenames></author></authors><title>A Novel Approach to Develop a New Hybrid Technique for Trademark Image
  Retrieval</title><categories>cs.CV</categories><comments>12 Pages, International Journal on Information Theory (IJIT),Vol.3,
  No.4, October 2014</comments><doi>10.5121/ijit.2014.3403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trademark Image Retrieval is playing a vital role as a part of CBIR System.
Trademark is of great significance because it carries the status value of any
company. To retrieve such a fake or copied trademark we design a retrieval
system which is based on hybrid techniques. It contains a mixture of two
different feature vector which combined together to give a suitable retrieval
system. In the proposed system we extract the corner feature which is applied
on an edge pixel image. This feature is used to extract the relevant image and
to more purify the result we apply other feature which is the invariant moment
feature. From the experimental result we conclude that the system is 85 percent
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03340</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03340</id><created>2014-11-23</created><authors><author><keyname>Bodaghi</keyname><forenames>Amir Hosein</forenames></author></authors><title>A Novel Model for Integration of Information Security Management against
  Replication Attack Based on Biological Structures of the Body</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By developing communications and increase of access points, computer networks
have been vulnerable considerably against wide range of information attacks,
specially new and complicated attacks. Every day, replication attacks attack
millions of network and mobile users. Increase in amount of replication attack
may be a potential danger for income of SMS or network and causes losing
customers of these services provider. Humans or software can be used to
encounter these replication attacks. It is obvious that lonely absolute use of
each method will not result in a proper answer to encounter replication
attack`s problem. Since replication attack is one of the important problems of
information protection and security in organizations for computer and mobile
phone users, while reviewing types of replication attacks and methods of
encountering, this paper uses similarities between pathologies in body and
invader factors in replication attacks, a model is provided based on biological
simulation methods existing in body`s adapted immune system to encounter these
threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03342</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03342</id><created>2015-01-27</created><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>A Survey on Privacy and Security in Online Social Networks</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSN) are a permanent presence in today's personal and
professional lives of a huge segment of the population, with direct
consequences to offline activities. Built on a foundation of trust-users
connect to other users with common interests or overlapping personal
trajectories-online social networks and the associated applications extract an
unprecedented volume of personal information. Unsurprisingly, serious privacy
and security risks emerged, positioning themselves along two main types of
attacks: attacks that exploit the implicit trust embedded in declared social
relationships; and attacks that harvest user's personal information for
ill-intended use. This article provides an overview of the privacy and security
issues that emerged so far in OSNs. We introduce a taxonomy of privacy and
security attacks in OSNs, we overview existing solutions to mitigate those
attacks, and outline challenges still to overcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03363</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03363</id><created>2015-04-13</created><authors><author><keyname>Pivaro</keyname><forenames>G. F.</forenames></author><author><keyname>Fraindenraich</keyname><forenames>G.</forenames></author></authors><title>Outage Probability for Multi-Hop Full-Duplex Decode and Forward MIMO
  Relay</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multi-hop (MH) decode-and-forward (DF) multiple-input
multiple-output (MIMO) relay network has been studied. To consider a more
realistic scenario, Full-Duplex (FD) operation with Relay Self-Interference
(RSI) is employed.
  Assuming that the MIMO channels are subject to Rayleigh fading, a simple and
compact closed-form outage probability expression has been derived. The key
assumption to derive this result is that the mutual information of each channel
could be well approximated by a Gaussian random variable. In order to obtain
the resultant outage probability, a new excellent accurate approximation has
been obtained for the sum of Wishart distributed complex random matrices.
  Numerical Monte Carlo simulations have been performed to validate our result.
These simulations have shown that, for low and medium interference regime, FD
mode performs better than Half-Duplex (HD) mode. On the other hand, when RSI
increases, HD mode can outperforms FD mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03370</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03370</id><created>2015-04-13</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Esteve</keyname><forenames>Chantal</forenames></author><author><keyname>Chirivella</keyname><forenames>Javier</forenames></author><author><keyname>Gagliardo</keyname><forenames>Pablo</forenames></author></authors><title>Preprint Serious Game Based Dysphonic Rehabilitation Tool</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on 2015 International
  Conference on Virtual Rehabilitation (ICVR2015)</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 2015 International Conference on
Virtual Rehabilitation (ICVR2015). The purpose of this work is designing and
implementing a rehabilitation software for dysphonic patients. Constant
training is a key factor for this type of therapy. The patient can play the
game as well as conduct the voice training simultaneously guided by therapists
at clinic or exercise independently at home. The voice information can be
recorded and extracted for evaluating the long-time rehabilitation progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03371</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03371</id><created>2015-04-13</created><updated>2015-07-29</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Preprint Imagining In-Air Interaction for Hemiplegia Sufferer</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on 2015 International
  Conference on Virtual Rehabilitation (ICVR2015)</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on 2015 International Conference on
Virtual Rehabilitation (ICVR2015). In this paper, we described the imagination
scenarios of a touch-less interaction technology for hemiplegia, which can
support either hand or foot interaction with the smartphone or head mounted
device (HMD). The computer vision interaction technology is implemented in our
previous work, which provides a core support for gesture interaction by
accurately detecting and tracking the hand or foot gesture. The patients
interact with the application using hand/foot gesture motion in the camera
view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03374</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03374</id><created>2015-04-13</created><authors><author><keyname>Garc&#xed;a</keyname><forenames>Ruth</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Diego</forenames></author><author><keyname>Parra</keyname><forenames>Denis</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author></authors><title>Language, Twitter and Academic Conferences</title><categories>cs.SI physics.soc-ph</categories><comments>4 pages, 3 figures, 4 tables, submitted to ACM Hypertext and Social
  Media 2015</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Twitter during academic conferences is a way of engaging and connecting
an audience inherently multicultural by the nature of scientific collaboration.
English is expected to be the lingua franca bridging the communication and
integration between native speakers of different mother tongues. However,
little research has been done to support this assumption. In this paper we
analyzed how integrated language communities are by analyzing the scholars'
tweets used in 26 Computer Science conferences over a time span of five years.
We found that although English is the most popular language used to tweet
during conferences, a significant proportion of people also tweet in other
languages. In addition, people who tweet solely in English interact mostly
within the same group (English monolinguals), while people who speak other
languages tend to show a more diverse interaction with other lingua groups.
Finally, we also found that the people who interact with other Twitter users
show a more diverse language distribution, while people who do not interact
mostly post tweets in a single language. These results suggest a relation
between the number of languages a user speaks, which can affect the interaction
dynamics of online communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03376</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03376</id><created>2015-04-13</created><authors><author><keyname>Lloyd</keyname><forenames>Seth</forenames></author></authors><title>Any non-affine one-to-one binary gate suffices for computation</title><categories>quant-ph cs.OH</categories><comments>7 pages, plain TeX, 1992 Los Alamos Alamos preprint number
  LA-UR-92-996. Shows that any non-affine reversible binary logic gate is
  universal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any non-affine one-to-one binary gate can be wired together with suitable
inputs to give AND, OR, NOT and fan-out gates, and so suffices to construct a
general-purpose computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03385</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03385</id><created>2015-04-13</created><authors><author><keyname>Lee</keyname><forenames>Chen-Yu</forenames></author><author><keyname>Chen</keyname><forenames>Deng-Jyi</forenames></author></authors><title>A Content Creation and Protection Scheme for Medical Images</title><categories>cs.CR</categories><comments>15 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical images contain metadata information on where, when, and how an image
was acquired, and the majority of this information is stored as pixel data.
Image feature descriptions are often captured only as free text stored in the
image file or in the hospital information system. Correlations between the free
text and the location of the feature are often inaccurate, making it difficult
to link image observations to their corresponding image locations. This limits
the interpretation of image data from a clinical, research, and academic
standpoint. An efficient medical image protection design should allow for
compatibility, usability, and privacy. This paper proposes a medical-content
creation and protection scheme that contains a) a DICOM-compatible multimedia
annotation scheme for medical content creation; b) a DICOM-compatible partial
DRM scheme for medical record transmission under this scheme, authorized users
can view only information to which they have been granted to access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03386</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03386</id><created>2015-04-13</created><authors><author><keyname>Milani</keyname><forenames>Mostafa</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>Tractable Query Answering and Optimization for Extensions of
  Weakly-Sticky Datalog+-</title><categories>cs.DB cs.AI cs.LO</categories><comments>To appear in Proc. Alberto Mendelzon WS on Foundations of Data
  Management (AMW15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic
subclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that
of weakly-sticky (WS) programs, which appear in our applications to data
quality. For WChS programs we propose a practical, polynomial-time query
answering algorithm (QAA). We establish that the two classes are closed under
magic-sets rewritings. As a consequence, QAA can be applied to the optimized
programs. QAA takes as inputs the program (including the query) and semantic
information about the &quot;finiteness&quot; of predicate positions. For the syntactic
subclasses JWS and WS of WChS, this additional information is computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03391</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03391</id><created>2015-04-13</created><updated>2015-08-02</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS
  functions</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular and fractionally subadditive (or equivalently XOS) functions play
a fundamental role in combinatorial optimization, algorithmic game theory and
machine learning. Motivated by learnability of these classes of functions from
random examples, we consider the question of how well such functions can be
approximated by low-degree polynomials in $\ell_2$ norm over the uniform
distribution. This question is equivalent to understanding of the concentration
of Fourier weight on low-degree coefficients, a central concept in Fourier
analysis. We show that
  1. For any submodular function $f:\{0,1\}^n \rightarrow [0,1]$, there is a
polynomial of degree $O(\log (1/\epsilon) / \epsilon^{4/5})$ approximating $f$
within $\epsilon$ in $\ell_2$, and there is a submodular function that requires
degree $\Omega(1/\epsilon^{4/5})$.
  2. For any XOS function $f:\{0,1\}^n \rightarrow [0,1]$, there is a
polynomial of degree $O(1/\epsilon)$ and there exists an XOS function that
requires degree $\Omega(1/\epsilon)$.
  This improves on previous approaches that all showed an upper bound of
$O(1/\epsilon^2)$ for submodular and XOS functions. The best previous lower
bound was $\Omega(1/\epsilon^{2/3})$ for monotone submodular functions. Our
techniques reveal new structural properties of submodular and XOS functions and
the upper bounds lead to nearly optimal PAC learning algorithms for these
classes of functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03398</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03398</id><created>2015-04-13</created><authors><author><keyname>Rossman</keyname><forenames>Benjamin</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>An average-case depth hierarchy theorem for Boolean circuits</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an average-case depth hierarchy theorem for Boolean circuits over
the standard basis of $\mathsf{AND}$, $\mathsf{OR}$, and $\mathsf{NOT}$ gates.
Our hierarchy theorem says that for every $d \geq 2$, there is an explicit
$n$-variable Boolean function $f$, computed by a linear-size depth-$d$ formula,
which is such that any depth-$(d-1)$ circuit that agrees with $f$ on $(1/2 +
o_n(1))$ fraction of all inputs must have size $\exp({n^{\Omega(1/d)}}).$ This
answers an open question posed by H{\aa}stad in his Ph.D. thesis.
  Our average-case depth hierarchy theorem implies that the polynomial
hierarchy is infinite relative to a random oracle with probability 1,
confirming a conjecture of H{\aa}stad, Cai, and Babai. We also use our result
to show that there is no &quot;approximate converse&quot; to the results of Linial,
Mansour, Nisan and Boppana on the total influence of small-depth circuits, thus
answering a question posed by O'Donnell, Kalai, and Hatami.
  A key ingredient in our proof is a notion of \emph{random projections} which
generalize random restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03406</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03406</id><created>2015-04-13</created><authors><author><keyname>Mohammad</keyname><forenames>Omer K. Jasim</forenames></author><author><keyname>Abbas</keyname><forenames>Safia</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author><author><keyname>Salem</keyname><forenames>Abdel-Badeeh M.</forenames></author></authors><title>Innovative Method for enhancing Key generation and management in the
  AES-algorithm</title><categories>cs.CR</categories><comments>7 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1503.04796</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the extraordinary maturity of data exchange in network environments and
increasing the attackers capabilities, information security has become the most
important process for data storage and communication. In order to provide such
information security the confidentiality, data integrity, and data origin
authentication must be verified based on cryptographic encryption algorithms.
This paper presents a development of the advanced encryption standard (AES)
algorithm, which is considered as the most eminent symmetric encryption
algorithm. The development focuses on the generation of the integration between
the developed AES based S-Boxes, and the specific selected secret key generated
from the quantum key distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03409</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03409</id><created>2015-04-13</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Wan</keyname><forenames>Yi</forenames></author></authors><title>Clustering Assisted Fundamental Matrix Estimation</title><categories>cs.CV</categories><comments>12 pages, 8 figures, 3 tables, Second International Conference on
  Computer Science and Information Technology (COSIT 2015) March 21~22, 2015,
  Geneva, Switzerland</comments><doi>10.5121/csit.2015.50604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, the estimation of the fundamental matrix is a basic
problem that has been extensively studied. The accuracy of the estimation
imposes a significant influence on subsequent tasks such as the camera
trajectory determination and 3D reconstruction. In this paper we propose a new
method for fundamental matrix estimation that makes use of clustering a group
of 4D vectors. The key insight is the observation that among the 4D vectors
constructed from matching pairs of points obtained from the SIFT algorithm,
well-defined cluster points tend to be reliable inliers suitable for
fundamental matrix estimation. Based on this, we utilizes a recently proposed
efficient clustering method through density peaks seeking and propose a new
clustering assisted method. Experimental results show that the proposed
algorithm is faster and more accurate than currently commonly used methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03410</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03410</id><created>2015-04-13</created><authors><author><keyname>Lai</keyname><forenames>Hanjiang</forenames></author><author><keyname>Pan</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>Ye</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</title><categories>cs.CV</categories><comments>This paper has been accepted to IEEE International Conference on
  Pattern Recognition and Computer Vision (CVPR), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity-preserving hashing is a widely-used method for nearest neighbour
search in large-scale image retrieval tasks. For most existing hashing methods,
an image is first encoded as a vector of hand-engineering visual features,
followed by another separate projection or quantization step that generates
binary codes. However, such visual feature vectors may not be optimally
compatible with the coding process, thus producing sub-optimal hashing codes.
In this paper, we propose a deep architecture for supervised hashing, in which
images are mapped into binary codes via carefully designed deep neural
networks. The pipeline of the proposed deep architecture consists of three
building blocks: 1) a sub-network with a stack of convolution layers to produce
the effective intermediate image features; 2) a divide-and-encode module to
divide the intermediate image features into multiple branches, each encoded
into one hash bit; and 3) a triplet ranking loss designed to characterize that
one image is more similar to the second image than to the third one. Extensive
evaluations on several benchmark image datasets show that the proposed
simultaneous feature learning and hash coding pipeline brings substantial
improvements over other state-of-the-art supervised or unsupervised hashing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03411</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03411</id><created>2015-04-13</created><authors><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Dai</keyname><forenames>Xuchu</forenames></author></authors><title>The Private Key Capacity of a Cooperative Pairwise-Independent Network</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, IEEE ISIT 2015 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the private key generation of a cooperative
pairwise-independent network (PIN) with M+2 terminals (Alice, Bob and M
relays), M &gt;= 2. In this PIN, the correlated sources observed by every pair of
terminals are independent of those sources observed by any other pair of
terminal. All the terminals can communicate with each other over a public
channel which is also observed by Eve noiselessly. The objective is to generate
a private key between Alice and Bob under the help of the M relays; such a
private key needs to be protected not only from Eve but also from individual
relays simultaneously. The private key capacity of this PIN model is
established, whose lower bound is obtained by proposing a novel random binning
(RB) based key generation algorithm, and the upper bound is obtained based on
the construction of M enhanced source models. The two bounds are shown to be
exactly the same. Then, we consider a cooperative wireless network and use the
estimates of fading channels to generate private keys. It has been shown that
the proposed RB-based algorithm can achieve a multiplexing gain M-1, an
improvement in comparison with the existing XOR- based algorithm whose
achievable multiplexing gain is about [M]/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03413</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03413</id><created>2015-04-13</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Consensus based Detection in the Presence of Data Falsification Attacks</title><categories>cs.SY cs.DC stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of detection in distributed networks in the
presence of data falsification (Byzantine) attacks. Detection approaches
considered in the paper are based on fully distributed consensus algorithms,
where all of the nodes exchange information only with their neighbors in the
absence of a fusion center. In such networks, we characterize the negative
effect of Byzantines on the steady-state and transient detection performance of
the conventional consensus based detection algorithms. To address this issue,
we study the problem from the network designer's perspective. More
specifically, we first propose a distributed weighted average consensus
algorithm that is robust to Byzantine attacks. We show that, under reasonable
assumptions, the global test statistic for detection can be computed locally at
each node using our proposed consensus algorithm. We exploit the statistical
distribution of the nodes' data to devise techniques for mitigating the
influence of data falsifying Byzantines on the distributed detection system.
Since some parameters of the statistical distribution of the nodes' data might
not be known a priori, we propose learning based techniques to enable an
adaptive design of the local fusion or update rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03415</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03415</id><created>2015-04-14</created><authors><author><keyname>Wickramarachchi</keyname><forenames>D. C.</forenames></author><author><keyname>Robertson</keyname><forenames>B. L.</forenames></author><author><keyname>Reale</keyname><forenames>M.</forenames></author><author><keyname>Price</keyname><forenames>C. J.</forenames></author><author><keyname>Brown</keyname><forenames>J.</forenames></author></authors><title>HHCART: An Oblique Decision Tree</title><categories>stat.ML cs.LG</categories><comments>13 Pages, 1 Figure, 4 Tables, 1 Algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision trees are a popular technique in statistical data classification.
They recursively partition the feature space into disjoint sub-regions until
each sub-region becomes homogeneous with respect to a particular class. The
basic Classification and Regression Tree (CART) algorithm partitions the
feature space using axis parallel splits. When the true decision boundaries are
not aligned with the feature axes, this approach can produce a complicated
boundary structure. Oblique decision trees use oblique decision boundaries to
potentially simplify the boundary structure. The major limitation of this
approach is that the tree induction algorithm is computationally expensive. In
this article we present a new decision tree algorithm, called HHCART. The
method utilizes a series of Householder matrices to reflect the training data
at each node during the tree construction. Each reflection is based on the
directions of the eigenvectors from each classes' covariance matrix.
Considering axis parallel splits in the reflected training data provides an
efficient way of finding oblique splits in the unreflected training data.
Experimental results show that the accuracy and size of the HHCART trees are
comparable with some benchmark methods in the literature. The appealing feature
of HHCART is that it can handle both qualitative and quantitative features in
the same oblique split.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03425</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03425</id><created>2015-04-14</created><authors><author><keyname>Naim</keyname><forenames>Iftekhar</forenames><affiliation>Ehsan</affiliation></author><author><keyname>Tanveer</keyname><forenames>M. Iftekhar</forenames><affiliation>Ehsan</affiliation></author><author><keyname>Gildea</keyname><forenames>Daniel</forenames><affiliation>Ehsan</affiliation></author><author><keyname>Mohammed</keyname><affiliation>Ehsan</affiliation></author><author><keyname>Hoque</keyname></author></authors><title>Automated Analysis and Prediction of Job Interview Performance</title><categories>cs.HC cs.AI cs.CL</categories><comments>14 pages, 8 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computational framework for automatically quantifying verbal and
nonverbal behaviors in the context of job interviews. The proposed framework is
trained by analyzing the videos of 138 interview sessions with 69
internship-seeking undergraduates at the Massachusetts Institute of Technology
(MIT). Our automated analysis includes facial expressions (e.g., smiles, head
gestures, facial tracking points), language (e.g., word counts, topic
modeling), and prosodic information (e.g., pitch, intonation, and pauses) of
the interviewees. The ground truth labels are derived by taking a weighted
average over the ratings of 9 independent judges. Our framework can
automatically predict the ratings for interview traits such as excitement,
friendliness, and engagement with correlation coefficients of 0.75 or higher,
and can quantify the relative importance of prosody, language, and facial
expressions. By analyzing the relative feature weights learned by the
regression models, our framework recommends to speak more fluently, use less
filler words, speak as &quot;we&quot; (vs. &quot;I&quot;), use more unique words, and smile more.
We also find that the students who were rated highly while answering the first
interview question were also rated highly overall (i.e., first impression
matters). Finally, our MIT Interview dataset will be made available to other
researchers to further validate and expand our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03426</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03426</id><created>2015-04-14</created><updated>2015-10-14</updated><authors><author><keyname>Pan</keyname><forenames>Haoyuan</forenames></author><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Network-Coded Multiple Access with Higher-order Modulations</title><categories>cs.IT math.IT</categories><comments>Full-length version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first network-coded multiple access (NCMA) system
operated on higher-order modulations beyond BPSK. NCMA jointly exploits
physical-layer network coding (PNC) and multiuser decoding (MUD) to boost
throughput of multi-packet reception systems. Direct generalization of the
existing NCMA decoding algorithm, originally designed for BPSK, to higher-order
modulations, will lead to huge performance degradation. The throughput
degradation is caused by the relative phase offset between received signals
from different nodes. To circumvent the phase offset problem, this paper
investigates an NCMA system with multiple receive antennas at the access point
(AP), referred to as MIMO-NCMA. Specifically, we put forth a symbol-level NCMA
decoder that can maintain good performance in the presence of relative phase
offset, with low computational complexity. To demonstrate the feasibility of
higher-order modulated NCMA, we have implemented our designs on
software-defined radios. Our experimental results show that the throughput of
QPSK MIMO-NCMA is double that of BPSK NCMA and QPSK MUD, at SNR of 10dB. At
higher SNR, 16-QAM can be supported, for which the throughput of MIMO-NCMA can
be as high as 3.5 times that of BPSK NCMA. Overall, our findings validate the
feasibility and the performance gain of high-order modulated NCMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03437</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03437</id><created>2015-04-14</created><authors><author><keyname>Fan</keyname><forenames>YouZhe</forenames></author><author><keyname>Chen</keyname><forenames>Ji</forenames></author><author><keyname>Xia</keyname><forenames>ChenYang</forenames></author><author><keyname>Tsui</keyname><forenames>Chi-ying</forenames></author><author><keyname>Jin</keyname><forenames>Jie</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>Li</keyname><forenames>Bin</forenames></author></authors><title>Low-latency List Decoding Of Polar Codes With Double Thresholding</title><categories>cs.AR</categories><comments>5 pages, 7 figures, 1 table, to be presented in the 40th IEEE
  International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  2015</comments><doi>10.1109/ICASSP.2015.7178128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For polar codes with short-to-medium code length, list successive
cancellation decoding is used to achieve a good error-correcting performance.
However, list pruning in the current list decoding is based on the sorting
strategy and its timing complexity is high. This results in a long decoding
latency for large list size. In this work, aiming at a low-latency list
decoding implementation, a double thresholding algorithm is proposed for a fast
list pruning. As a result, with a negligible performance degradation, the list
pruning delay is greatly reduced. Based on the double thresholding, a
low-latency list decoding architecture is proposed and implemented using a UMC
90nm CMOS technology. Synthesis results show that, even for a large list size
of 16, the proposed low-latency architecture achieves a decoding throughput of
220 Mbps at a frequency of 641 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03439</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03439</id><created>2015-04-14</created><updated>2015-04-24</updated><authors><author><keyname>Shamsi</keyname><forenames>Zahid Hussain</forenames></author><author><keyname>Oh</keyname><forenames>Hyun Sook</forenames></author><author><keyname>Kim</keyname><forenames>Dai-Gyoung</forenames></author></authors><title>Image Denoising Using Low Rank Minimization With Modified Noise
  Estimation</title><categories>cs.CV</categories><comments>4 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the application of low rank minimization to image denoising has
shown remarkable denoising results which are equivalent or better than those of
the existing state-of-the-art algorithms. However, due to iterative nature of
low rank optimization, estimation of residual noise is an essential requirement
after each iteration. Currently, this noise is estimated by using the filtered
noise in the previous iteration without considering the geometric structure of
the given image. This estimate may be affected in the presence of moderate and
severe levels of noise. To obtain a more reliable estimate of residual noise,
we propose a modified algorithm (GWNNM) which includes the contribution of the
geometric structure of an image to the existing noise estimation. Furthermore,
the proposed algorithm exploits the difference of large and small singular
values to enhance the edges and textures during the denoising process.
Consequently, the proposed modifications achieve significant improvements in
the denoising results of the existing low rank optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03440</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03440</id><created>2015-04-14</created><authors><author><keyname>Kellaris</keyname><forenames>Georgios</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Stavros</forenames></author><author><keyname>Papadias</keyname><forenames>Dimitris</forenames></author></authors><title>Differentially Private Histograms for Range-Sum Queries: A Modular
  Approach</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the problem of differentially private histogram publication, for
range-sum query answering. Specifically, we derive a histogram from a given
dataset, such that (i) it satisfies $\epsilon$-differential privacy, and (ii)
it achieves high utility for queries that request the sum of contiguous
histogram bins. Existing schemes are distinguished into two categories: fast
but oblivious to utility optimizations that exploit the data characteristics,
and data-aware but slow. We are the first to address this problem with emphasis
on both efficiency and utility. Towards this goal, we formulate a principled
approach, which defines a small set of simple modules, based on which we can
devise a variety of more complex schemes. We first express the state-of-the-art
methods in terms of these modules, which allows us to identify the performance
bottlenecks. Next, we design novel efficient and effective schemes based on
non-trivial module combinations. We experimentally evaluate all mechanisms on
three real datasets with diverse characteristics, and demonstrate the benefits
of our proposals over previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03445</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03445</id><created>2015-04-14</created><authors><author><keyname>Ashraf</keyname><forenames>Mohammad</forenames></author><author><keyname>Mohammad</keyname><forenames>Ghulam</forenames></author></authors><title>$(1+2u)$-constacyclic codes over $\mathbb{Z}_4+u\mathbb{Z}_4$</title><categories>math.RA cs.IT math.IT</categories><msc-class>94B05, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R=\mathbb{Z}_4+u\mathbb{Z}_4,$ where $\mathbb{Z}_4$ denotes the ring of
integers modulo $4$ and $u^2=0$. In the present paper, we introduce a new Gray
map from $R^n$ to $\mathbb{Z}_{4}^{2n}.$ We study $(1+2u)$-constacyclic codes
over $R$ of odd lengths with the help of cyclic codes over $R$. It is proved
that the Gray image of $(1+2u)$-constacyclic codes of length $n$ over $R$ are
cyclic codes of length $2n$ over $\mathbb{Z}_4$. Further, a number of linear
codes over $\mathbb{Z}_4$ as the images of $(1+2u)$-constacyclic codes over $R$
are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03449</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03449</id><created>2015-04-14</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Design Tool To Express Failure Detection Protocols</title><categories>cs.DC</categories><comments>Published in IET Software, Vol. 4, No. 2, April 2010. Institution of
  Engineering and Technology (IET). 14 pages</comments><doi>10.1049/iet-sen.2009.0043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Failure detection protocols---a fundamental building block for crafting
fault-tolerant distributed systems---are in many cases described by their
authors making use of informal pseudo-codes of their conception. Often these
pseudo-codes use syntactical constructs that are not available in COTS
programming languages such as C or C++. This translates into informal
descriptions that call for ad hoc interpretations and implementations. Being
informal, these descriptions cannot be tested by their authors, which may
translate into insufficiently detailed or even faulty specifications. This
paper tackles this problem introducing a formal syntax for those constructs and
a C library that implements them---a tool-set to express and reason about
failure detection protocols. The resulting specifications are longer but non
ambiguous, and eligible for becoming a standard form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03451</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03451</id><created>2015-04-14</created><authors><author><keyname>Kim</keyname><forenames>Song-Ju</forenames></author><author><keyname>Naruse</keyname><forenames>Makoto</forenames></author><author><keyname>Aono</keyname><forenames>Masashi</forenames></author></authors><title>Harnessing Natural Fluctuations: Analogue Computer for Efficient
  Socially Maximal Decision Making</title><categories>cs.AI</categories><comments>30 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each individual handles many tasks of finding the most profitable option from
a set of options that stochastically provide rewards. Our society comprises a
collection of such individuals, and the society is expected to maximise the
total rewards, while the individuals compete for common rewards. Such
collective decision making is formulated as the `competitive multi-armed bandit
problem (CBP)', requiring a huge computational cost. Herein, we demonstrate a
prototype of an analog computer that efficiently solves CBPs by exploiting the
physical dynamics of numerous fluids in coupled cylinders. This device enables
the maximisation of the total rewards for the society without paying the
conventionally required computational cost; this is because the fluids estimate
the reward probabilities of the options for the exploitation of past knowledge
and generate random fluctuations for the exploration of new knowledge. Our
results suggest that to optimise the social rewards, the utilisation of
fluid-derived natural fluctuations is more advantageous than applying
artificial external fluctuations. Our analog computing scheme is expected to
trigger further studies for harnessing the huge computational power of natural
phenomena for resolving a wide variety of complex problems in modern
information society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03473</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03473</id><created>2015-04-14</created><authors><author><keyname>Luthmann</keyname><forenames>Lars</forenames><affiliation>TU Braunschweig</affiliation></author><author><keyname>Mennicke</keyname><forenames>Stephan</forenames><affiliation>TU Braunschweig</affiliation></author><author><keyname>Lochau</keyname><forenames>Malte</forenames><affiliation>TU Darmstadt</affiliation></author></authors><title>Towards an I/O Conformance Testing Theory for Software Product Lines
  based on Modal Interface Automata</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 182, 2015, pp. 1-13</journal-ref><doi>10.4204/EPTCS.182.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an adaptation of input/output conformance (ioco) testing
principles to families of similar implementation variants as appearing in
product line engineering. Our proposed product line testing theory relies on
Modal Interface Automata (MIA) as behavioral specification formalism. MIA
enrich I/O-labeled transition systems with may/must modalities to distinguish
mandatory from optional behavior, thus providing a semantic notion of intrinsic
behavioral variability. In particular, MIA constitute a restricted, yet fully
expressive subclass of I/O-labeled modal transition systems, guaranteeing
desirable refinement and compositionality properties. The resulting modal-ioco
relation defined on MIA is preserved under MIA refinement, which serves as
variant derivation mechanism in our product line testing theory. As a result,
modal-ioco is proven correct in the sense that it coincides with traditional
ioco to hold for every derivable implementation variant. Based on this result,
a family-based product line conformance testing framework can be established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03474</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03474</id><created>2015-04-14</created><authors><author><keyname>Belder</keyname><forenames>Tessa</forenames><affiliation>TU/e, Eindhoven, The Netherlands</affiliation></author><author><keyname>ter Beek</keyname><forenames>Maurice H.</forenames><affiliation>ISTI-CNR, Pisa, Italy</affiliation></author><author><keyname>de Vink</keyname><forenames>Erik P.</forenames><affiliation>TU/e, Eindhoven and CWI, Amsterdam, The Netherlands</affiliation></author></authors><title>Coherent branching feature bisimulation</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; F.3.2</acm-class><journal-ref>EPTCS 182, 2015, pp. 14-30</journal-ref><doi>10.4204/EPTCS.182.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progress in the behavioral analysis of software product lines at the family
level benefits from further development of the underlying semantical theory.
Here, we propose a behavioral equivalence for feature transition systems (FTS)
generalizing branching bisimulation for labeled transition systems (LTS). We
prove that branching feature bisimulation for an FTS of a family of products
coincides with branching bisimulation for the LTS projection of each the
individual products. For a restricted notion of coherent branching feature
bisimulation we furthermore present a minimization algorithm and show its
correctness. Although the minimization problem for coherent branching feature
bisimulation is shown to be intractable, application of the algorithm in the
setting of a small case study results in a significant speed-up of model
checking of behavioral properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03475</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03475</id><created>2015-04-14</created><authors><author><keyname>Pham</keyname><forenames>Thi-Kim-Zung</forenames></author><author><keyname>Dubois</keyname><forenames>Catherine</forenames></author><author><keyname>Levy</keyname><forenames>Nicole</forenames></author></authors><title>Towards correct-by-construction product variants of a software product
  line: GFML, a formal language for feature modules</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 182, 2015, pp. 44-55</journal-ref><doi>10.4204/EPTCS.182.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Product Line Engineering (SPLE) is a software engineering paradigm
that focuses on reuse and variability. Although feature-oriented programming
(FOP) can implement software product line efficiently, we still need a method
to generate and prove correctness of all product variants more efficiently and
automatically. In this context, we propose to manipulate feature modules which
contain three kinds of artifacts: specification, code and correctness proof. We
depict a methodology and a platform that help the user to automatically produce
correct-by-construction product variants from the related feature modules. As a
first step of this project, we begin by proposing a language, GFML, allowing
the developer to write such feature modules. This language is designed so that
the artifacts can be easily reused and composed. GFML files contain the
different artifacts mentioned above.The idea is to compile them into FoCaLiZe,
a language for specification, implementation and formal proof with some
object-oriented flavor. In this paper, we define and illustrate this language.
We also introduce a way to compose the feature modules on some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03476</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03476</id><created>2015-04-14</created><authors><author><keyname>ter Beek</keyname><forenames>Maurice H.</forenames><affiliation>ISTI-CNR, Pisa, Italy</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Inria, Rennes, France</affiliation></author><author><keyname>Lafuente</keyname><forenames>Alberto Lluch</forenames><affiliation>DTU, Lyngby, Denmark</affiliation></author><author><keyname>Vandin</keyname><forenames>Andrea</forenames><affiliation>University of Southampton, UK</affiliation></author></authors><title>Quantitative Analysis of Probabilistic Models of Software Product Lines
  with Statistical Model Checking</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; F.3.2; G.3</acm-class><journal-ref>EPTCS 182, 2015, pp. 56-70</journal-ref><doi>10.4204/EPTCS.182.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the suitability of statistical model checking techniques for
analysing quantitative properties of software product line models with
probabilistic aspects. For this purpose, we enrich the feature-oriented
language FLan with action rates, which specify the likelihood of exhibiting
particular behaviour or of installing features at a specific moment or in a
specific order. The enriched language (called PFLan) allows us to specify
models of software product lines with probabilistic configurations and
behaviour, e.g. by considering a PFLan semantics based on discrete-time Markov
chains. The Maude implementation of PFLan is combined with the distributed
statistical model checker MultiVeStA to perform quantitative analyses of a
simple product line case study. The presented analyses include the likelihood
of certain behaviour of interest (e.g. product malfunctioning) and the expected
average cost of products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03477</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03477</id><created>2015-04-14</created><authors><author><keyname>Peake</keyname><forenames>Ian D.</forenames><affiliation>RMIT University</affiliation></author><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames><affiliation>RMIT University</affiliation></author><author><keyname>Fernando</keyname><forenames>Lasith</forenames><affiliation>RMIT University</affiliation></author><author><keyname>Sharma</keyname><forenames>Divyasheel</forenames><affiliation>ABB Corporate Research, Bangalore</affiliation></author><author><keyname>Ramaswamy</keyname><forenames>Srini</forenames><affiliation>ABB Corporate Research, Bangalore</affiliation></author><author><keyname>Kande</keyname><forenames>Mallikarjun</forenames><affiliation>ABB Corporate Research, Bangalore</affiliation></author></authors><title>Analysis of Software Binaries for Reengineering-Driven Product Line
  Architecture&#xc3;&#xa2;&#xc2;&#x80;&#xc2;&#x94;An Industrial Case Study</title><categories>cs.SE</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><acm-class>D.2.5; D.2.m</acm-class><journal-ref>EPTCS 182, 2015, pp. 71-82</journal-ref><doi>10.4204/EPTCS.182.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for the recovering of software architectures
from a set of similar (but unrelated) software products in binary form. One
intention is to drive refactoring into software product lines and combine
architecture recovery with run time binary analysis and existing clustering
methods. Using our runtime binary analysis, we create graphs that capture the
dependencies between different software parts. These are clustered into smaller
component graphs, that group software parts with high interactions into larger
entities. The component graphs serve as a basis for further software product
line work. In this paper, we concentrate on the analysis part of the method and
the graph clustering. We apply the graph clustering method to a real
application in the context of automation / robot configuration software tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03483</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03483</id><created>2015-04-14</created><authors><author><keyname>Lesta</keyname><forenames>Uwe</forenames><affiliation>TU Braunschweig</affiliation></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames><affiliation>TU Braunschweig</affiliation></author><author><keyname>Winkelmann</keyname><forenames>Tim</forenames><affiliation>TU Braunschweig</affiliation></author></authors><title>Detecting and Explaining Conflicts in Attributed Feature Models</title><categories>cs.SE</categories><comments>In Proceedings FMSPLE 2015, arXiv:1504.03014</comments><proxy>EPTCS</proxy><acm-class>D.2.2; D.2.13</acm-class><journal-ref>EPTCS 182, 2015, pp. 31-43</journal-ref><doi>10.4204/EPTCS.182.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product configuration systems are often based on a variability model. The
development of a variability model is a time consuming and error-prone process.
Considering the ongoing development of products, the variability model has to
be adapted frequently. These changes often lead to mistakes, such that some
products cannot be derived from the model anymore, that undesired products are
derivable or that there are contradictions in the variability model. In this
paper, we propose an approach to discover and to explain contradictions in
attributed feature models efficiently in order to assist the developer with the
correction of mistakes. We use extended feature models with attributes and
arithmetic constraints, translate them into a constraint satisfaction problem
and explore those for contradictions. When a contradiction is found, the
constraints are searched for a set of contradicting relations by the
QuickXplain algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03490</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03490</id><created>2015-04-14</created><authors><author><keyname>Lampropoulos</keyname><forenames>N. K.</forenames></author><author><keyname>Karvelas</keyname><forenames>E. G.</forenames></author><author><keyname>Sarris</keyname><forenames>I. E.</forenames></author></authors><title>Computational Modeling of an MRI Guided Drug Delivery System Based on
  Magnetic Nanoparticle Aggregations for the Navigation of Paramagnetic
  Nanocapsules</title><categories>cond-mat.soft cs.CE</categories><comments>14 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computational method for magnetically guided drug delivery is presented and
the results are compared for the aggregation process of magnetic particles
within a fluid environment. The model is developed for the simulation of the
aggregation patterns of magnetic nanoparticles under the influence of MRI
magnetic coils. A novel approach for the calculation of the drag coefficient of
aggregates is presented. The comparison against experimental and numerical
results from the literature is showed that the proposed method predicts well
the aggregations in respect to their size and pattern dependance, on the
concentration and the strength of the magnetic field, as well as their velocity
when particles are driven through the fluid by magnetic gradients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03494</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03494</id><created>2015-04-14</created><authors><author><keyname>El-Ferik</keyname><forenames>Sami</forenames></author><author><keyname>Siddiqui</keyname><forenames>Bilal A.</forenames></author><author><keyname>Lewis</keyname><forenames>Frank L.</forenames></author></authors><title>Distributed Nonlinear MPC of Multi-Agent Systems with Data Compression
  and Random Delays - Extended Version</title><categories>cs.SY</categories><comments>19 pages, 3 Figures, accepted for publication, IEEE Transactions on
  Automatic Control, 2015, Technical Note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an extended version of a technical note accepted for publication in
IEEE Transactions on Automatic Control. The note proposes an Input to State
practically Stable (ISpS) formulation of distributed nonlinear model predictive
controller (NMPC) for formation control of constrained autonomous vehicles in
presence of communication bandwidth limitation and transmission delays. Planned
trajectories are compressed using neural networks resulting in considerable
reduction of data packet size, while being robust to propagation delays and
uncertainty in neighbors' trajectories. Collision avoidance is achieved by
means of spatially filtered potential field. Analytical results proving ISpS
and generalized small gain conditions are presented for both strongly- and
weakly-connected networks, and illustrated by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03498</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03498</id><created>2015-04-14</created><authors><author><keyname>Ed-Douibi</keyname><forenames>Hamza</forenames></author><author><keyname>Izquierdo</keyname><forenames>Javier Luis C&#xe1;novas</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Abel</forenames></author><author><keyname>Tisi</keyname><forenames>Massimo</forenames></author><author><keyname>Cabot</keyname><forenames>Jordi</forenames></author></authors><title>EMF-REST: Generation of RESTful APIs from Models</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the last years, RESTful Web services have become more and more popular as
a lightweight solution to connect remote systems in distributed and Cloud-based
architectures. However, being an architectural style rather than a
specification or standard, the proper design of RESTful Web services is not
trivial since developers have to deal with a plethora of recommendations and
best practices. Model-Driven Engineering (MDE) emphasizes the use of models and
model transformations to raise the level of abstraction and semi-automate the
development of software. In this paper we present an approach that leverages on
MDE techniques to generate RESTful services. The approach, called EMF-REST,
takes EMF data models as input and generates Web APIs following the REST
principles and relying on well-known libraries and standards, thus facilitating
its comprehension and maintainability. Additionally, EMF-REST integrates model
and Web-specific features to provide model validation and security
capabilities, respectively, to the generated API. For Web developers, our
approach brings more agility to the Web development process by providing
ready-to-run-and-test Web APIs out of data models. Also, our approach provides
MDE practitioners the basis to develop Cloud-based modeling solutions as well
as enhanced collaborative support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03503</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03503</id><created>2015-04-14</created><updated>2016-03-08</updated><authors><author><keyname>Han</keyname><forenames>Qiaoni</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Miao</keyname><forenames>Guowang</forenames></author><author><keyname>Chen</keyname><forenames>Cailian</forenames></author><author><keyname>Wang</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Guan</keyname><forenames>Xinping</forenames></author></authors><title>Backhaul-Aware User Association and Resource Allocation for
  Energy-Constrained HetNets</title><categories>cs.NI</categories><comments>35 pages, 12 figures</comments><doi>10.1109/TVT.2016.2533559</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growing attentions have been paid to renewable energy or hybrid energy
powered heterogeneous networks (HetNets). In this paper, focusing on
backhaul-aware joint user association and resource allocation for this type of
HetNets, we formulate an online optimization problem to maximize the network
utility reflecting proportional fairness. Since user association and resource
allocation are tightly coupled not only on resource consumption of the base
stations (BSs), but also in the constraints of their available energy and
backhaul, the closed-form solution is quite difficult to obtain. Thus, we solve
the problem distributively via employing some decomposition methods.
Specifically, at first, by adopting primal decomposition method, we decompose
the original problem into a lower-level resource allocation problem for each
BS, and a higher-level user association problem. For the optimal resource
allocation, we prove that a BS either assigns equal normalized resources or
provides equal long-term service rate to its served users. Then, the user
association problem is solved by Lagrange dual decomposition method, and a
completely distributed algorithm is developed. Moreover, applying results of
the subgradient method, we demonstrate the convergence of the proposed
distributed algorithm. Furthermore, in order to efficiently and reliably apply
the proposed algorithm to the future wireless networks with an extremely dense
BS deployment, we design a virtual user association and resource allocation
scheme based on the software-defined networking architecture. Lastly, numerical
results validate the convergence of the proposed algorithm and the significant
improvement on network utility, load balancing and user fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03504</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03504</id><created>2015-04-14</created><authors><author><keyname>Wang</keyname><forenames>Fang</forenames></author><author><keyname>Kang</keyname><forenames>Le</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author></authors><title>Sketch-based 3D Shape Retrieval using Convolutional Neural Networks</title><categories>cs.CV</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieving 3D models from 2D human sketches has received considerable
attention in the areas of graphics, image retrieval, and computer vision.
Almost always in state of the art approaches a large amount of &quot;best views&quot; are
computed for 3D models, with the hope that the query sketch matches one of
these 2D projections of 3D models using predefined features.
  We argue that this two stage approach (view selection -- matching) is
pragmatic but also problematic because the &quot;best views&quot; are subjective and
ambiguous, which makes the matching inputs obscure. This imprecise nature of
matching further makes it challenging to choose features manually. Instead of
relying on the elusive concept of &quot;best views&quot; and the hand-crafted features,
we propose to define our views using a minimalism approach and learn features
for both sketches and views. Specifically, we drastically reduce the number of
views to only two predefined directions for the whole dataset. Then, we learn
two Siamese Convolutional Neural Networks (CNNs), one for the views and one for
the sketches. The loss function is defined on the within-domain as well as the
cross-domain similarities. Our experiments on three benchmark datasets
demonstrate that our method is significantly better than state of the art
approaches, and outperforms them in all conventional metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03509</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03509</id><created>2015-04-14</created><updated>2015-09-04</updated><authors><author><keyname>Liu</keyname><forenames>Shuang</forenames></author><author><keyname>Chen</keyname><forenames>Cheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and
  Beyond</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the distributed stochastic multi-armed bandit
problem, where a global arm set can be accessed by multiple players
independently. The players are allowed to exchange their history of
observations with each other at specific points in time. We study the
relationship between regret and communication. When the time horizon is known,
we propose the Over-Exploration strategy, which only requires one-round
communication and whose regret does not scale with the number of players. When
the time horizon is unknown, we measure the frequency of communication through
a new notion called the density of the communication set, and give an exact
characterization of the interplay between regret and communication.
Specifically, a lower bound is established and stable strategies that match the
lower bound are developed. The results and analyses in this paper are specific
but can be translated into more general settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03516</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03516</id><created>2015-04-14</created><updated>2015-12-23</updated><authors><author><keyname>Liang</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Mixed-ADC Massive MIMO</title><categories>cs.IT math.IT</categories><comments>double column, 15 pages, 11 figures, accepted for publication in IEEE
  Journal on Selected Areas in Communications Special Issue on Energy-Efficient
  Techniques for 5G Wireless Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the demand for energy-efficient communication solutions in the
next generation cellular network, a mixed-ADC architecture for massive multiple
input multiple output (MIMO) systems is proposed, which differs from previous
works in that herein one-bit analog-to-digital converters (ADCs) partially
replace the conventionally assumed high-resolution ADCs. The
information-theoretic tool of generalized mutual information (GMI) is exploited
to analyze the achievable data rates of the proposed system architecture and an
array of analytical results of engineering interest are obtained. For fixed
single input multiple output (SIMO) channels, a closed-form expression of the
GMI is derived, based on which the linear combiner is optimized. The analysis
is then extended to ergodic fading channels, for which tight lower and upper
bounds of the GMI are obtained. Impacts of dithering and imperfect channel
state information (CSI) are also investigated, and it is shown that dithering
can remarkably improve the system performance while imperfect CSI only
introduces a marginal rate loss. Finally, the analytical framework is applied
to the multi-user access scenario. Numerical results demonstrate that the
mixed-ADC architecture with a relatively small number of high-resolution ADCs
is able to achieve a large fraction of the channel capacity of conventional
architecture, while reduce the energy consumption considerably even compared
with antenna selection, for both single-user and multi-user scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03517</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03517</id><created>2015-04-14</created><updated>2015-08-03</updated><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Bearing-Based Formation Maneuvering</title><categories>cs.SY</categories><comments>To appear in the 2015 IEEE Multi-Conference on Systems and Control
  (MSC2015); this is the final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of multi-agent formation maneuver control
where both of the centroid and scale of a formation are required to track given
velocity references while maintaining the formation shape. Unlike the
conventional approaches where the target formation is defined by inter-neighbor
relative positions or distances, we propose a bearing-based approach where the
target formation is defined by inter-neighbor bearings. Due to the invariance
of the bearings, the bearing-based approach provides a natural solution to
formation scale control. We assume the dynamics of each agent as a single
integrator and propose a globally stable proportional-integral formation
maneuver control law. It is shown that at least two leaders are required to
collaborate in order to control the centroid and scale of the formation whereas
the followers are not required to have access to any global information, such
as the velocities of the leaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03522</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03522</id><created>2015-04-14</created><authors><author><keyname>Neumann</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Matas</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Efficient Scene Text Localization and Recognition with Local Character
  Refinement</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unconstrained end-to-end text localization and recognition method is
presented. The method detects initial text hypothesis in a single pass by an
efficient region-based method and subsequently refines the text hypothesis
using a more robust local text model, which deviates from the common assumption
of region-based methods that all characters are detected as connected
components.
  Additionally, a novel feature based on character stroke area estimation is
introduced. The feature is efficiently computed from a region distance map, it
is invariant to scaling and rotations and allows to efficiently detect text
regions regardless of what portion of text they capture.
  The method runs in real time and achieves state-of-the-art text localization
and recognition results on the ICDAR 2013 Robust Reading dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03524</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03524</id><created>2015-04-14</created><authors><author><keyname>Miao</keyname><forenames>Zhixin</forenames></author><author><keyname>Fan</keyname><forenames>Lingling</forenames></author></authors><title>Achieving Economic Operation and Secondary Frequency Regulation
  Simultaneously Through Feedback Control</title><categories>cs.SY</categories><comments>submitted to IEEE PES letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an exciting finding for the power industry: the
parameters of secondary frequency control based on integral or proportional
integral control can be tuned to achieve economic operation and frequency
regulation simultaneously. We show that if the power imbalance is represented
by frequency deviation, an iterative dual decomposition based economic dispatch
solving is equivalent to integral control. An iterative method of multipliers
based economic dispatch is equivalent to proportional integral control.
Similarly, if the controller parameters of the secondary frequency controls are
chosen based on generator cost functions, these secondary frequency controllers
achieve both economic operation and frequency regulation simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03536</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03536</id><created>2015-04-14</created><authors><author><keyname>Karamshuk</keyname><forenames>Dmytro</forenames></author><author><keyname>Sastry</keyname><forenames>Nishanth</forenames></author><author><keyname>Secker</keyname><forenames>Andrew</forenames></author><author><keyname>Chandaria</keyname><forenames>Jigna</forenames></author></authors><title>ISP-friendly Peer-assisted On-demand Streaming of Long Duration Content
  in BBC iPlayer</title><categories>cs.NI</categories><comments>In Proceedings of IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In search of scalable solutions, CDNs are exploring P2P support. However, the
benefits of peer assistance can be limited by various obstacle factors such as
ISP friendliness - requiring peers to be within the same ISP, bitrate
stratification - the need to match peers with others needing similar bitrate,
and partial participation - some peers choosing not to redistribute content.
  This work relates potential gains from peer assistance to the average number
of users in a swarm, its capacity, and empirically studies the effects of these
obstacle factors at scale, using a month-long trace of over 2 million users in
London accessing BBC shows online. Results indicate that even when P2P swarms
are localised within ISPs, up to 88% of traffic can be saved. Surprisingly,
bitrate stratification results in 2 large sub-swarms and does not significantly
affect savings. However, partial participation, and the need for a minimum
swarm size do affect gains. We investigate improvements to gain from increasing
content availability through two well-studied techniques: content bundling -
combining multiple items to increase availability, and historical caching of
previously watched items. Bundling proves ineffective as increased server
traffic from larger bundles outweighs benefits of availability, but simple
caching can considerably boost traffic gains from peer assistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03539</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03539</id><created>2015-04-14</created><updated>2015-11-06</updated><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Sethi</keyname><forenames>Shuchi</forenames></author></authors><title>Detection of Information leakage in cloud</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research shows that colluded malware in different VMs sharing a single
physical host may use a resource as a channel to leak critical information.
Covert channels employ time or storage characteristics to transmit confidential
information to attackers leaving no trail.These channels were not meant for
communication and hence control mechanisms do not exist. This means these
remain undetected by traditional security measures employed in firewalls etc in
a network. The comprehensive survey to address the issue highlights that
accurate methods for fast detection in cloud are very expensive in terms of
storage and processing. The proposed framework builds signature by extracting
features which accurately classify the regular from covert traffic in cloud and
estimates difference in distribution of data under analysis by means of scores.
It then adds context to the signature and finally using machine learning
(Support Vector Machines),a model is built and trained for deploying in cloud.
The results show that the framework proposed is high in accuracy while being
low cost and robust as it is tested after adding noise which is likely to exist
in public cloud environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03542</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03542</id><created>2015-04-14</created><authors><author><keyname>Karamshuk</keyname><forenames>Dmytro</forenames></author><author><keyname>Sastry</keyname><forenames>Nishanth</forenames></author><author><keyname>Secker</keyname><forenames>Andrew</forenames></author><author><keyname>Chandaria</keyname><forenames>Jigna</forenames></author></authors><title>On Factors Affecting the Usage and Adoption of a Nation-wide TV
  Streaming Service</title><categories>cs.NI</categories><comments>In Proceedings of IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using nine months of access logs comprising 1.9 Billion sessions to BBC
iPlayer, we survey the UK ISP ecosystem to understand the factors affecting
adoption and usage of a high bandwidth TV streaming application across
different providers. We find evidence that connection speeds are important and
that external events can have a huge impact for live TV usage. Then, through a
temporal analysis of the access logs, we demonstrate that data usage caps
imposed by mobile ISPs significantly affect usage patterns, and look for
solutions. We show that product bundle discounts with a related fixed-line ISP,
a strategy already employed by some mobile providers, can better support user
needs and capture a bigger share of accesses. We observe that users regularly
split their sessions between mobile and fixed-line connections, suggesting a
straightforward strategy for offloading by speculatively pre-fetching content
from a fixed-line ISP before access on mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03543</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03543</id><created>2015-04-14</created><authors><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author><author><keyname>Wang</keyname><forenames>Pengming</forenames></author></authors><title>A Definability Dichotomy for Finite Valued CSPs</title><categories>cs.LO</categories><comments>23 pages</comments><msc-class>68Q19</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite valued constraint satisfaction problems are a formalism for describing
many natural optimization problems, where constraints on the values that
variables can take come with rational weights and the aim is to find an
assignment of minimal cost. Thapper and Zivny have recently established a
complexity dichotomy for finite valued constraint languages. They show that
each such language either gives rise to a polynomial-time solvable optimization
problem, or to an NP-hard one, and establish a criterion to distinguish the two
cases. We refine the dichotomy by showing that all optimization problems in the
first class are definable in fixed-point language with counting, while all
languages in the second class are not definable, even in infinitary logic with
counting. Our definability dichotomy is not conditional on any
complexity-theoretic assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03547</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03547</id><created>2015-04-14</created><authors><author><keyname>Disfani</keyname><forenames>Vahid Rasouli</forenames></author><author><keyname>Bozchalui</keyname><forenames>Mohammad Chehreghani</forenames></author><author><keyname>Sharma</keyname><forenames>Ratnesh</forenames></author></authors><title>SDP-based State Estimation of Multi-phase Active Distribution Networks
  using micro-PMUs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution system state estimation (DSSE) is an essential tool for
operation of distribution networks, the results of which enables the operator
to have a thorough observation of the system. Thus, most distribution
management systems (DMS) include a single-phase state estimator. Due to
non-convexity of the SE problem, heuristic and Newton methods do not guarantee
the global solution. In contrast, SDP based SE is more promising to guarantee
the globally optimal solution since it represents and solves the problem in a
convex format. However, the observability of the power system is highly
vulnerable to the set of measurements while employing the SDP-based SE, which
is addressed in this report. An algorithm is proposed to generate additional
measurements using the measurement data already gathered. The SDP-based SE is
very sensitive to the level of noise in large power networks. Also, bad data
detection algorithms proposed for Newton methods do not work for the SDP-based
SE method due to larger number of state variables in SDP representation of
power network. In this report, an algorithm is proposed to generate additional
measurements using the measurement data already gathered in order to solve the
observability issue. A network separation algorithm is developed to solve the
entire problem for smaller sub-networks which include micro-PMUs to mitigate
the adverse effects of noise for huge networks. An algorithm based on
redundancy test is also developed for bad data detection. The algorithms are
tested on single phase and multiphase test systems. The algorithms are applied
EPRI Circuit 5 (2998-bus) test feeder to demonstrate the flexibility of the
algorithms developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03553</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03553</id><created>2015-02-11</created><authors><author><keyname>Kokash</keyname><forenames>Natallia</forenames><affiliation>Leiden Institute of Advanced Computer Science</affiliation></author></authors><title>Handshaking Protocol for Distributed Implementation of Reo</title><categories>cs.DC cs.PL cs.SE</categories><comments>In Proceedings FOCLASA 2014, arXiv:1502.03157</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 175, 2015, pp. 1-17</journal-ref><doi>10.4204/EPTCS.175.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reo, an exogenous channel-based coordination language, is a model for service
coordination wherein services communicate through connectors formed by joining
binary communication channels. In order to establish transactional
communication among services as prescribed by connector semantics, distributed
ports exchange handshaking messages signalling which parties are ready to
provide or consume data. In this paper, we present a formal implementation
model for distributed Reo with communication delays and outline ideas for its
proof of correctness. To reason about Reo implementation formally, we introduce
Timed Action Constraint Automata (TACA) and explain how to compare TACA with
existing automata-based semantics for Reo. We use TACA to describe handshaking
behavior of Reo modeling primitives and argue that in any distributed circuit
remote Reo nodes and channels exposing such behavior commit to perform
transitions envisaged by the network semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03558</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03558</id><created>2015-04-13</created><authors><author><keyname>Van Minh</keyname><forenames>Nguyen</forenames></author><author><keyname>Son</keyname><forenames>Le Hoang</forenames></author></authors><title>Fuzzy approaches to context variable in fuzzy geographically weighted
  clustering</title><categories>cs.AI</categories><comments>11 pages</comments><msc-class>62H30</msc-class><acm-class>I.5.3</acm-class><doi>10.5121/csit.2015.50503</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Fuzzy Geographically Weighted Clustering (FGWC) is considered as a suitable
tool for the analysis of geo-demographic data that assists the provision and
planning of products and services to local people. Context variables were
attached to FGWC in order to accelerate the computing speed of the algorithm
and to focus the results on the domain of interests. Nonetheless, the
determination of exact, crisp values of the context variable is a hard task. In
this paper, we propose two novel methods using fuzzy approaches for that
determination. A numerical example is given to illustrate the uses of the
proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03561</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03561</id><created>2015-04-14</created><updated>2015-09-14</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Gagarin</keyname><forenames>Andrei</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author></authors><title>On the Workflow Satisfiability Problem with Class-Independent
  Constraints</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workflow specification defines sets of steps and users. An authorization
policy determines for each user a subset of steps the user is allowed to
perform. Other security requirements, such as separation-of-duty, impose
constraints on which subsets of users may perform certain subsets of steps. The
\emph{workflow satisfiability problem} (WSP) is the problem of determining
whether there exists an assignment of users to workflow steps that satisfies
all such authorizations and constraints. An algorithm for solving WSP is
important, both as a static analysis tool for workflow specifications, and for
the construction of run-time reference monitors for workflow management
systems. Given the computational difficulty of WSP, it is important,
particularly for the second application, that such algorithms are as efficient
as possible.
  We introduce class-independent constraints, enabling us to model scenarios
where the set of users is partitioned into groups, and the identities of the
user groups are irrelevant to the satisfaction of the constraint. We prove that
solving WSP is fixed-parameter tractable (FPT) for this class of constraints
and develop an FPT algorithm that is useful in practice. We compare the
performance of the FPT algorithm with that of SAT4J (a pseudo-Boolean SAT
solver) in computational experiments, which show that our algorithm
significantly outperforms SAT4J for many instances of WSP. User-independent
constraints, a large class of constraints including many practical ones, are a
special case of class-independent constraints for which WSP was proved to be
FPT (Cohen {\em et al.}, J. Artif. Intel. Res. 2014). Thus our results
considerably extend our knowledge of the fixed-parameter tractability of WSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03564</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03564</id><created>2015-03-28</created><authors><author><keyname>Khan</keyname><forenames>Sadeque Reza</forenames></author><author><keyname>Dristy</keyname><forenames>Farzana Sultana</forenames></author></authors><title>Android based security and home automation system</title><categories>cs.OH</categories><comments>10 pages,17 figures, Journal, International Journal of Ambient
  Systems and Applications, Volume 3, 2015</comments><doi>10.5121/ijasa.2014.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart mobile terminal operator platform Android is getting popular all
over the world with its wide variety of applications and enormous use in
numerous spheres of our daily life. Considering the fact of increasing demand
of home security and automation, an Android based control system is presented
in this paper where the proposed system can maintain the security of home main
entrance and also the car door lock. Another important feature of the designed
system is that it can control the overall appliances in a room. The mobile to
security system or home automation system interface is established through
Bluetooth. The hardware part is designed with the PIC microcontroller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03573</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03573</id><created>2015-04-14</created><authors><author><keyname>Brubaker</keyname><forenames>Marcus A.</forenames></author><author><keyname>Punjani</keyname><forenames>Ali</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author></authors><title>Building Proteins in a Day: Efficient 3D Molecular Reconstruction</title><categories>cs.CV q-bio.QM</categories><comments>To be presented at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering the 3D atomic structure of molecules such as proteins and viruses
is a fundamental research problem in biology and medicine. Electron
Cryomicroscopy (Cryo-EM) is a promising vision-based technique for structure
estimation which attempts to reconstruct 3D structures from 2D images. This
paper addresses the challenging problem of 3D reconstruction from 2D Cryo-EM
images. A new framework for estimation is introduced which relies on modern
stochastic optimization techniques to scale to large datasets. We also
introduce a novel technique which reduces the cost of evaluating the objective
function during optimization by over five orders or magnitude. The net result
is an approach capable of estimating 3D molecular structure from large scale
datasets in about a day on a single workstation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03580</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03580</id><created>2015-04-14</created><updated>2015-04-17</updated><authors><author><keyname>Burger</keyname><forenames>John Robert</forenames></author></authors><title>An New Type Of Artificial Brain Using Controlled Neurons</title><categories>cs.ET q-bio.NC</categories><comments>10 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plans for a new type of artificial brain are possible because of realistic
neurons in logically structured arrays of controlled toggles, one toggle per
neuron. Controlled toggles can be made to compute, in parallel, parameters of
critical importance for each of several complex images recalled from
associative long term memory. Controlled toggles are shown below to amount to a
new type of neural network that supports autonomous behavior and action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03581</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03581</id><created>2015-04-14</created><authors><author><keyname>de Erausquin</keyname><forenames>Ignacio</forenames></author><author><keyname>Gonzalez</keyname><forenames>Humberto</forenames></author></authors><title>Simultaneous Receding Horizon Estimation and Control of a Fencing Robot
  using a Single Camera</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for simultaneous Receding Horizon Estimation and Control
of a robotic arm equipped with a sword in an adversarial situation. Using a
single camera mounted on the arm, we solve the problem of blocking a opponent's
sword with the robot's sword. Our algorithm uses model-based sensing to
estimate the opponent's intentions from the camera's observations, while it
simultaneously applies a control action to both block the opponent's sword and
improve future camera observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03584</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03584</id><created>2015-04-14</created><updated>2016-01-08</updated><authors><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Angelini</keyname><forenames>Leonardo</forenames></author><author><keyname>Wu</keyname><forenames>Guorong</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jesus M.</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author></authors><title>Synergetic and redundant information flow detected by unnormalized
  Granger causality: application to resting state fMRI</title><categories>cs.IT math.IT physics.data-an q-bio.NC</categories><comments>6 figures. arXiv admin note: text overlap with arXiv:1403.5156</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze by means of Granger causality the effect of synergy and redundancy
in the inference (from time series data) of the information flow between
subsystems of a complex network. Whilst fully conditioned Granger causality is
not affected by synergy, the pairwise analysis fails to put in evidence
synergetic effects. Both redundancy and synergy render difficult to estimate
the neat flow of information from each driver variable to a given target.
  We show that adopting an {\it unnormalized} definition of Granger causality
one may put in evidence redundant multiplets of variables influencing the
target by maximizing the total Granger causality to a given target, over all
the possible partitions of the set of driving variables. Moreover we also
introduce a pairwise index of synergy which is zero when two independent
sources additively influence the future state of the system, differently from
previous definitions of synergy; the introduction of this pairwise index allows
use of methods from complex networks theory to the issue of redundancy and
synergy.
  We show the application of the proposed approach to resting state fMRI data
from the Human Connectome Project, and show that redundant pairs of regions
arise mainly due to space contiguity and interhemispheric symmetry, whilst
synergy occurs mainly between non-homologous pairs of regions in opposite
hemispheres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03586</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03586</id><created>2015-04-14</created><authors><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Je&#x17c;</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>V&#xe1;squez</keyname><forenames>&#xd3;scar C.</forenames></author></authors><title>Mechanism design for aggregating energy consumption and quality of
  service in speed scaling scheduling</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a strategic game, where players submit jobs to a machine that
executes all jobs in a way that minimizes energy while respecting the given
eadlines. The energy consumption is then charged to the players in some way.
Each player wants to minimize the sum of that charge and of their job's
deadline ultiplied by a priority weight. Two charging schemes are studied, the
proportional cost share which does not always admit pure Nash equilibria, and
the arginal cost share, which does always admit pure Nash equilibria, at the
price of overcharging by a constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03592</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03592</id><created>2015-04-14</created><authors><author><keyname>Dennis</keyname><forenames>Louise A.</forenames></author><author><keyname>Fisher</keyname><forenames>Michael</forenames></author><author><keyname>Winfield</keyname><forenames>Alan F. T.</forenames></author></authors><title>Towards Verifiably Ethical Robot Behaviour</title><categories>cs.AI</categories><comments>Presented at the 1st International Workshop on AI and Ethics, Sunday
  25th January 2015, Hill Country A, Hyatt Regency Austin. Will appear in the
  workshop proceedings published by AAAI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring that autonomous systems work ethically is both complex and
difficult. However, the idea of having an additional `governor' that assesses
options the system has, and prunes them to select the most ethical choices is
well understood. Recent work has produced such a governor consisting of a
`consequence engine' that assesses the likely future outcomes of actions then
applies a Safety/Ethical logic to select actions. Although this is appealing,
it is impossible to be certain that the most ethical options are actually
taken. In this paper we extend and apply a well-known agent verification
approach to our consequence engine, allowing us to verify the correctness of
its ethical decision-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03602</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03602</id><created>2015-04-14</created><authors><author><keyname>Anbalagan</keyname><forenames>Yogesh</forenames></author><author><keyname>Huang</keyname><forenames>Hao</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Norin</keyname><forenames>Sergey</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author><author><keyname>Wu</keyname><forenames>Hehui</forenames></author></authors><title>Large Supports are required for Well-Supported Nash Equilibria</title><categories>cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for any constant $k$ and any $\epsilon&lt;1$, there exist bimatrix
win-lose games for which every $\epsilon$-WSNE requires supports of cardinality
greater than $k$. To do this, we provide a graph-theoretic characterization of
win-lose games that possess $\epsilon$-WSNE with constant cardinality supports.
We then apply a result in additive number theory of Haight to construct
win-lose games that do not satisfy the requirements of the characterization.
These constructions disprove graph theoretic conjectures of Daskalakis, Mehta
and Papadimitriou, and Myers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03608</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03608</id><created>2015-04-14</created><authors><author><keyname>Koscov&#xe1;</keyname><forenames>Michaela</forenames></author><author><keyname>Macutek</keyname><forenames>J&#xe1;n</forenames></author><author><keyname>Kelih</keyname><forenames>Emmerich</forenames></author></authors><title>A data-based classification of Slavic languages: Indices of qualitative
  variation applied to grapheme frequencies</title><categories>stat.AP cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ord's graph is a simple graphical method for displaying frequency
distributions of data or theoretical distributions in the two-dimensional
plane. Its coordinates are proportions of the first three moments, either
empirical or theoretical ones. A modification of the Ord's graph based on
proportions of indices of qualitative variation is presented. Such a
modification makes the graph applicable also to data of categorical character.
In addition, the indices are normalized with values between 0 and 1, which
enables comparing data files divided into different numbers of categories. Both
the original and the new graph are used to display grapheme frequencies in
eleven Slavic languages. As the original Ord's graph requires an assignment of
numbers to the categories, graphemes were ordered decreasingly according to
their frequencies. Data were taken from parallel corpora, i.e., we work with
grapheme frequencies from a Russian novel and its translations to ten other
Slavic languages. Then, cluster analysis is applied to the graph coordinates.
While the original graph yields results which are not linguistically
interpretable, the modification reveals meaningful relations among the
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03609</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03609</id><created>2015-04-14</created><updated>2015-05-24</updated><authors><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Output agreement in networks with unmatched disturbances and algebraic
  constraints</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a problem of output agreement in heterogeneous networks
with dynamics on the nodes as well as on the edges. The control and disturbance
signals entering the nodal dynamics are &quot;unmatched&quot; meaning that some nodes are
only subject to disturbances, and are deprived of actuating signals. To further
enrich our model, we accommodate (solvable) algebraic constraints in a subset
of nodal dynamics. We show that appropriate dynamic feedback controllers
achieve output agreement on a desired vector. We also investigate the case of
an optimal steady-state control over the network. The proposed results are
applied to a heterogeneous microgrid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03620</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03620</id><created>2015-04-14</created><authors><author><keyname>Balc&#xe1;zar</keyname><forenames>Jos&#xe9; L.</forenames></author></authors><title>Quantitative Redundancy in Partial Implications</title><categories>cs.LO</categories><comments>Int. Conf. Formal Concept Analysis, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We survey the different properties of an intuitive notion of redundancy, as a
function of the precise semantics given to the notion of partial implication.
  The final version of this survey will appear in the Proceedings of the Int.
Conf. Formal Concept Analysis, 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03625</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03625</id><created>2015-04-14</created><authors><author><keyname>Eden</keyname><forenames>Alon</forenames></author><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Vardi</keyname><forenames>Adi</forenames></author></authors><title>Truthful Secretaries with Budgets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online auction settings in which agents arrive and depart
dynamically in a random (secretary) order, and each agent's private type
consists of the agent's arrival and departure times, value and budget. We
consider multi-unit auctions with additive agents for the allocation of both
divisible and indivisible items. For both settings, we devise truthful
mechanisms that give a constant approximation with respect to the auctioneer's
revenue, under a large market assumption. For divisible items, we devise in
addition a truthful mechanism that gives a constant approximation with respect
to the liquid welfare --- a natural efficiency measure for budgeted settings
introduced by Dobzinski and Paes Leme [ICALP'14]. Our techniques provide
high-level principles for transforming offline truthful mechanisms into online
ones, with or without budget constraints. To the best of our knowledge, this is
the first work that addresses the non-trivial challenge of combining online
settings with budgeted agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03628</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03628</id><created>2015-04-14</created><authors><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author></authors><title>Protograph-Based LDPC Code Design for Shaped Bit-Metric Decoding</title><categories>cs.IT math.IT</categories><comments>9 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1501.05595</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A protograph-based low-density parity-check (LDPC) code design technique for
bandwidth-efficient coded modulation is presented. The approach jointly
optimizes the LDPC code node degrees and the mapping of the coded bits to the
bit-interleaved coded modulation (BICM) bit-channels. For BICM with uniform
input and for BICM with probabilistic shaping, binary-input symmetric-output
surrogate channels for the code design are used. The constructed codes for
uniform inputs perform as good as the multi-edge type codes of Zhang and
Kschischang (2013). For 8-ASK and 64-ASK with probabilistic shaping, codes of
rates 2/3 and 5/6 with blocklength 64800 are designed, which operate within
0.63dB and 0.69dB of continuous AWGN capacity for a target frame error rate of
1e-3 at spectral efficiencies of 1.38 and 4.25 bits/channel use, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03632</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03632</id><created>2015-04-14</created><updated>2015-07-26</updated><authors><author><keyname>Bharath</keyname><forenames>B. N.</forenames></author><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author></authors><title>Caching with Unknown Popularity Profiles in Small Cell Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, Proceedings of IEEE Global Communications Conference, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heterogenous network is considered where the base stations (BSs), small
base stations (SBSs) and users are distributed according to independent Poisson
point processes (PPPs). We let the SBS nodes to posses high storage capacity
and are assumed to form a distributed caching network. Popular data files are
stored in the local cache of SBS, so that users can download the desired files
from one of the SBS in the vicinity subject to availability. The
offloading-loss is captured via a cost function that depends on a random
caching strategy proposed in this paper. The cost function depends on the
popularity profile, which is, in general, unknown. In this work, the popularity
profile is estimated at the BS using the available instantaneous demands from
the users in a time interval $[0,\tau]$. This is then used to find an estimate
of the cost function from which the optimal random caching strategy is devised.
The main results of this work are the following: First it is shown that the
waiting time $\tau$ to achieve an $\epsilon&gt;0$ difference between the achieved
and optimal costs is finite, provided the user density is greater than a
predefined threshold. In this case, $\tau$ is shown to scale as $N^2$, where
$N$ is the support of the popularity profile. Secondly, a transfer
learning-based approach is proposed to obtain an estimate of the popularity
profile used to compute the empirical cost function. A condition is derived
under which the proposed transfer learning-based approach performs better than
the random caching strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03641</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03641</id><created>2015-04-14</created><authors><author><keyname>Zagoruyko</keyname><forenames>Sergey</forenames></author><author><keyname>Komodakis</keyname><forenames>Nikos</forenames></author></authors><title>Learning to Compare Image Patches via Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to learn directly from image data (i.e., without
resorting to manually-designed features) a general similarity function for
comparing image patches, which is a task of fundamental importance for many
computer vision problems. To encode such a function, we opt for a CNN-based
model that is trained to account for a wide variety of changes in image
appearance. To that end, we explore and study multiple neural network
architectures, which are specifically adapted to this task. We show that such
an approach can significantly outperform the state-of-the-art on several
problems and benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03643</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03643</id><created>2015-04-14</created><authors><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Pinelli</keyname><forenames>Fabio</forenames></author><author><keyname>Gkoufas</keyname><forenames>Yiannis</forenames></author><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author><author><keyname>Calabrese</keyname><forenames>Francesco</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Inferring Unusual Crowd Events From Mobile Phone Call Detail Records</title><categories>cs.SI</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasiveness and availability of mobile phone data offer the opportunity
of discovering usable knowledge about crowd behaviors in urban environments.
Cities can leverage such knowledge in order to provide better services (e.g.,
public transport planning, optimized resource allocation) and safer cities.
Call Detail Record (CDR) data represents a practical data source to detect and
monitor unusual events considering the high level of mobile phone penetration,
compared with GPS equipped and open devices. In this paper, we provide a
methodology that is able to detect unusual events from CDR data that typically
has low accuracy in terms of space and time resolution. Moreover, we introduce
a concept of unusual event that involves a large amount of people who expose an
unusual mobility behavior. Our careful consideration of the issues that come
from coarse-grained CDR data ultimately leads to a completely general framework
that can detect unusual crowd events from CDR data effectively and efficiently.
Through extensive experiments on real-world CDR data for a large city in
Africa, we demonstrate that our method can detect unusual events with 16%
higher recall and over 10 times higher precision, compared to state-of-the-art
methods. We implement a visual analytics prototype system to help end users
analyze detected unusual crowd events to best suit different application
scenarios. To the best of our knowledge, this is the first work on the
detection of unusual events from CDR data with considerations of its temporal
and spatial sparseness and distinction between user unusual activities and
daily routines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03655</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03655</id><created>2015-04-14</created><updated>2016-01-10</updated><authors><author><keyname>Xie</keyname><forenames>Bo</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear component analysis such as kernel Principle Component Analysis
(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in
machine learning, statistics and data analysis, but they can not scale up to
big datasets. Recent attempts have employed random feature approximations to
convert the problem to the primal form for linear computational complexity.
However, to obtain high quality solutions, the number of random features should
be the same order of magnitude as the number of data points, making such
approach not directly applicable to the regime with millions of data points.
  We propose a simple, computationally efficient, and memory friendly algorithm
based on the &quot;doubly stochastic gradients&quot; to scale up a range of kernel
nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the
\emph{non-convex} nature of these problems, our method enjoys theoretical
guarantees that it converges at the rate $\tilde{O}(1/t)$ to the global
optimum, even for the top $k$ eigen subspace. Unlike many alternatives, our
algorithm does not require explicit orthogonalization, which is infeasible on
big datasets. We demonstrate the effectiveness and scalability of our algorithm
on large scale synthetic and real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03659</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03659</id><created>2015-04-14</created><authors><author><keyname>Dehghan</keyname><forenames>Azad</forenames></author></authors><title>Temporal ordering of clinical events</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes a minimalistic set of methods engineered to anchor
clinical events onto a temporal space. Specifically, we describe methods to
extract clinical events (e.g., Problems, Treatments and Tests), temporal
expressions (i.e., time, date, duration, and frequency), and temporal links
(e.g., Before, After, Overlap) between events and temporal entities. These
methods are developed and validated using high quality datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03664</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03664</id><created>2015-04-14</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L.</forenames></author></authors><title>An Improved Variable Step-size Zero-point Attracting Projection
  Algorithm</title><categories>cs.OH</categories><comments>5 pages, ICASSP 2015. arXiv admin note: substantial text overlap with
  arXiv:1312.2612</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an improved variable step-size (VSS) scheme for
zero-point attracting projection (ZAP) algorithm. The proposed VSS is
proportional to the sparseness difference between filter coefficients and the
true impulse response. Meanwhile, it works for both sparse and non-sparse
system identification, and simulation results demonstrate that the proposed
algorithm could provide both faster convergence rate and better tracking
ability than previous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03666</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03666</id><created>2015-04-14</created><authors><author><keyname>Boyac&#x131;</keyname><forenames>Arman</forenames></author><author><keyname>Ekim</keyname><forenames>T&#x131;naz</forenames></author><author><keyname>Shalom</keyname><forenames>Mordechai</forenames></author></authors><title>The Maximum Cut Problem in Co-bipartite Chain Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{co-bipartite chain} graph is a co-bipartite graph in which the
neighborhoods of the vertices in each clique can be linearly ordered with
respect to inclusion. It is known that the maximum cut problem (MaxCut) is
NP-Hard in co-bipartite graphs. We consider MaxCut in co-bipartite chain
graphs. We first consider the twin-free case and present an explicit solution.
We then show that MaxCut is polynomial time solvable in this graph class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03670</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03670</id><created>2015-04-14</created><updated>2015-07-09</updated><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author></authors><title>Coloring Graphs having Few Colorings over Path Decompositions</title><categories>cs.DS</categories><comments>Strengthened result from uniquely $k$-colorable graphs to graphs with
  few $k$-colorings. Also improved running time</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lokshtanov, Marx, and Saurabh SODA 2011 proved that there is no
$(k-\epsilon)^{\operatorname{pw}(G)}\operatorname{poly}(n)$ time algorithm for
deciding if an $n$-vertex graph $G$ with pathwidth $\operatorname{pw}(G)$
admits a proper vertex coloring with $k$ colors unless the Strong Exponential
Time Hypothesis (SETH) is false. We show here that nevertheless, when
$k&gt;\lfloor \Delta/2 \rfloor + 1$, where $\Delta$ is the maximum degree in the
graph $G$, there is a better algorithm, at least when there are few colorings.
We present a Monte Carlo algorithm that given a graph $G$ along with a path
decomposition of $G$ with pathwidth $\operatorname{pw}(G)$ runs in $(\lfloor
\Delta/2 \rfloor + 1)^{\operatorname{pw}(G)}\operatorname{poly}(n)s$ time, that
distinguishes between $k$-colorable graphs having at most $s$ proper
$k$-colorings and non-$k$-colorable graphs. We also show how to obtain a
$k$-coloring in the same asymptotic running time. Our algorithm avoids
violating SETH for one since high degree vertices still cost too much and the
mentioned hardness construction uses a lot of them.
  We exploit a new variation of the famous Alon--Tarsi theorem that has an
algorithmic advantage over the original form. The original theorem shows a
graph has an orientation with outdegree less than $k$ at every vertex, with a
different number of odd and even Eulerian subgraphs only if the graph is
$k$-colorable, but there is no known way of efficiently finding such an
orientation. Our new form shows that if we instead count another difference of
even and odd subgraphs meeting modular degree constraints at every vertex
picked uniformly at random, we have a fair chance of getting a non-zero value
if the graph has few $k$-colorings. Yet every non-$k$-colorable graph gives a
zero difference, so a random set of constraints stands a good chance of being
useful for separating the two cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03679</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03679</id><created>2015-04-14</created><authors><author><keyname>He</keyname><forenames>Hao</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>A Coalitional Game for Distributed Inference in Sensor Networks with
  Dependent Observations</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of collaborative inference in a sensor network with
heterogeneous and statistically dependent sensor observations. Each sensor aims
to maximize its inference performance by forming a coalition with other sensors
and sharing information within the coalition. It is proved that the inference
performance is a nondecreasing function of the coalition size. However, in an
energy constrained network, the energy consumption of inter-sensor
communication also increases with increasing coalition size, which discourages
the formation of the grand coalition (the set of all sensors). In this paper,
the formation of non-overlapping coalitions with statistically dependent
sensors is investigated under a specific communication constraint. We apply a
game theoretical approach to fully explore and utilize the information
contained in the spatial dependence among sensors to maximize individual sensor
performance. Before formulating the distributed inference problem as a
coalition formation game, we first quantify the gain and loss in forming a
coalition by introducing the concepts of diversity gain and redundancy loss for
both estimation and detection problems. These definitions, enabled by the
statistical theory of copulas, allow us to characterize the influence of
statistical dependence among sensor observations on inference performance. An
iterative algorithm based on merge-and-split operations is proposed for the
solution and the stability of the proposed algorithm is analyzed. Numerical
results are provided to demonstrate the superiority of our proposed game
theoretical approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03701</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03701</id><created>2015-04-14</created><authors><author><keyname>Vogt</keyname><forenames>Julia E.</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Stark</keyname><forenames>Stefan</forenames></author><author><keyname>Raman</keyname><forenames>Sudhir S.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Sandhya</forenames></author><author><keyname>Roth</keyname><forenames>Volker</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Gunnar</forenames></author></authors><title>Probabilistic Clustering of Time-Evolving Distance Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel probabilistic clustering model for objects that are
represented via pairwise distances and observed at different time points. The
proposed method utilizes the information given by adjacent time points to find
the underlying cluster structure and obtain a smooth cluster evolution. This
approach allows the number of objects and clusters to differ at every time
point, and no identification on the identities of the objects is needed.
Further, the model does not require the number of clusters being specified in
advance -- they are instead determined automatically using a Dirichlet process
prior. We validate our model on synthetic data showing that the proposed method
is more accurate than state-of-the-art clustering methods. Finally, we use our
dynamic clustering model to analyze and illustrate the evolution of brain
cancer patients over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03707</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03707</id><created>2015-04-14</created><authors><author><keyname>Xin</keyname><forenames>Bo</forenames></author><author><keyname>Tian</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Background Subtraction via Generalized Fused Lasso Foreground Modeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background Subtraction (BS) is one of the key steps in video analysis. Many
background models have been proposed and achieved promising performance on
public data sets. However, due to challenges such as illumination change,
dynamic background etc. the resulted foreground segmentation often consists of
holes as well as background noise. In this regard, we consider generalized
fused lasso regularization to quest for intact structured foregrounds. Together
with certain assumptions about the background, such as the low-rank assumption
or the sparse-composition assumption (depending on whether pure background
frames are provided), we formulate BS as a matrix decomposition problem using
regularization terms for both the foreground and background matrices. Moreover,
under the proposed formulation, the two generally distinctive background
assumptions can be solved in a unified manner. The optimization was carried out
via applying the augmented Lagrange multiplier (ALM) method in such a way that
a fast parametric-flow algorithm is used for updating the foreground matrix.
Experimental results on several popular BS data sets demonstrate the advantage
of the proposed model compared to state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03711</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03711</id><created>2015-04-14</created><updated>2015-07-29</updated><authors><author><keyname>Micinski</keyname><forenames>Kristopher</forenames></author><author><keyname>Fetter-Degges</keyname><forenames>Jonathan</forenames></author><author><keyname>Jeon</keyname><forenames>Jinseong</forenames></author><author><keyname>Foster</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Clarkson</keyname><forenames>Michael R.</forenames></author></authors><title>Checking Interaction-Based Declassification Policies for Android Using
  Symbolic Execution</title><categories>cs.CR</categories><comments>This research was supported in part by NSF grants CNS-1064997 and
  1421373, AFOSR grants FA9550-12-1-0334 and FA9550-14-1-0334, a partnership
  between UMIACS and the Laboratory for Telecommunication Sciences, and the
  National Security Agency</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile apps can access a wide variety of secure information, such as contacts
and location. However, current mobile platforms include only coarse access
control mechanisms to protect such data. In this paper, we introduce
interaction-based declassification policies, in which the user's interactions
with the app constrain the release of sensitive information. Our policies are
defined extensionally, so as to be independent of the app's implementation,
based on sequences of security-relevant events that occur in app runs. Policies
use LTL formulae to precisely specify which secret inputs, read at which times,
may be released. We formalize a semantic security condition, interaction-based
noninterference, to define our policies precisely. Finally, we describe a
prototype tool that uses symbolic execution to check interaction-based
declassification policies for Android, and we show that it enforces policies
correctly on a set of apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03713</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03713</id><created>2015-04-14</created><authors><author><keyname>Krishnasamy</keyname><forenames>Subhashini</forenames></author><author><keyname>Sen</keyname><forenames>Rajat</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Detecting Sponsored Recommendations</title><categories>cs.IR</categories><comments>Shorter version to appear in Sigmetrics, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a vast number of items, web-pages, and news to choose from, online
services and the customers both benefit tremendously from personalized
recommender systems. Such systems however provide great opportunities for
targeted advertisements, by displaying ads alongside genuine recommendations.
We consider a biased recommendation system where such ads are displayed without
any tags (disguised as genuine recommendations), rendering them
indistinguishable to a single user. We ask whether it is possible for a small
subset of collaborating users to detect such a bias. We propose an algorithm
that can detect such a bias through statistical analysis on the collaborating
users' feedback. The algorithm requires only binary information indicating
whether a user was satisfied with each of the recommended item or not. This
makes the algorithm widely appealing to real world issues such as
identification of search engine bias and pharmaceutical lobbying. We prove that
the proposed algorithm detects the bias with high probability for a broad class
of recommendation systems when sufficient number of users provide feedback on
sufficient number of recommendations. We provide extensive simulations with
real data sets and practical recommender systems, which confirm the trade offs
in the theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03715</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03715</id><created>2015-04-14</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>On the Requirements of New Software Development</title><categories>cs.SE</categories><comments>Published in Int.l Journal of Business Intelligence and Data Mining,
  Vol. 3, No. 3 (2008). Inderscience. 20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changes, they use to say, are the only constant in life. Everything changes
rapidly around us, and more and more key to survival is the ability to rapidly
adapt to changes. This consideration applies to many aspects of our lives.
Strangely enough, this nearly self-evident truth is not always considered by
software engineers with the seriousness that it calls for: The assumptions we
draw for our systems often do not take into due account that e.g., the run-time
environments, the operational conditions, or the available resources will vary.
Software is especially vulnerable to this threat, and with today's
software-dominated systems controlling crucial services in nuclear plants,
airborne equipments, health care systems and so forth, it becomes clear how
this situation may potentially lead to catastrophes. This paper discusses this
problem and defines some of the requirements towards its effective solution,
which we call &quot;New Software Development&quot; as a software equivalent of the
well-known concept of New Product Development. The paper also introduces and
discusses a practical example of a software tool designed taking those
requirements into account --- an adaptive data integrity provision in which the
degree of redundancy is not fixed once and for all at design time, but rather
it changes dynamically with respect to the disturbances experienced during the
run time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03719</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03719</id><created>2015-04-14</created><updated>2015-07-06</updated><authors><author><keyname>van Delft</keyname><forenames>Andre</forenames></author></authors><title>Some New Directions for ACP Research</title><categories>cs.LO</categories><comments>8 pages, 2 figures</comments><msc-class>68Q85</msc-class><acm-class>D.1.3; D.3.1; D.3.2; F.1.2; F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper lists some new directions for research related to the Algebra of
Communicating Processes (ACP). Most of these directions have been inspired by
work on SubScript, an ACP based extension to the programming language Scala.
SubScript applies several new ideas that build on ACP, but currently these lack
formal treatment. Some of these new ideas are rather fundamental. E.g. it
appears that the theory of ACP may well apply to structures of any kind of
items, rather than to just processes. The aim of this list is to raise
awareness of the research community about these new ideas; this could help both
the research area and the programming language SubScript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03725</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03725</id><created>2015-04-14</created><authors><author><keyname>Loyka</keyname><forenames>Sergey</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>An Algorithm for Global Maximization of Secrecy Rates in Gaussian MIMO
  Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal signaling for secrecy rate maximization in Gaussian MIMO wiretap
channels is considered. While this channel has attracted a significant
attention recently and a number of results have been obtained, including the
proof of the optimality of Gaussian signalling, an optimal transmit covariance
matrix is known for some special cases only and the general case remains an
open problem. An iterative custom-made algorithm to find a globally-optimal
transmit covariance matrix in the general case is developed in this paper, with
guaranteed convergence to a \textit{global} optimum. While the original
optimization problem is not convex and hence difficult to solve, its minimax
reformulation can be solved via the convex optimization tools, which is
exploited here. The proposed algorithm is based on the barrier method extended
to deal with a minimax problem at hand. Its convergence to a global optimum is
proved for the general case (degraded or not) and a bound for the optimality
gap is given for each step of the barrier method. The performance of the
algorithm is demonstrated via numerical examples. In particular, 20 to 40
Newton steps are already sufficient to solve the sufficient optimality
conditions with very high precision (up to the machine precision level), even
for large systems. Even fewer steps are required if the secrecy capacity is the
only quantity of interest. The algorithm can be significantly simplified for
the degraded channel case and can also be adopted to include the per-antenna
power constraints (instead or in addition to the total power constraint). It
also solves the dual problem of minimizing the total power subject to the
secrecy rate constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03728</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03728</id><created>2015-04-14</created><authors><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author></authors><title>Robustness of power systems under a democratic fiber bundle-like model</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 6 figures</comments><journal-ref>Physical Review E 91, 062811, June 2015</journal-ref><doi>10.1103/PhysRevE.91.062811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a power system with $N$ transmission lines whose initial loads
(i.e., power flows) $L_1, \ldots, L_N$ are independent and identically
distributed with $P_L(x)$. The capacity $C_i$ defines the maximum flow allowed
on line $i$, and is assumed to be given by $C_i=(1+\alpha)L_i$, with
$\alpha&gt;0$. We study the robustness of this power system against random attacks
(or, failures) that target a $p$-{\em fraction} of the lines, under a
democratic fiber bundle-like model. Namely, when a line fails, the load it was
carrying is redistributed equally among the remaining lines. Our contributions
are as follows: i) we show analytically that the final breakdown of the system
always takes place through a first-order transition at the critical attack size
$p^{\star}=1-\frac{E[L]}{\max\{P(L&gt;x)(\alpha x + E[L ~|~ L&gt;x])\}}~~~$; ii) we
derive conditions on the distribution $P_L(x)$ for which the first order break
down of the system occurs abruptly without any preceding diverging rate of
failure; iii) we provide a detailed analysis of the robustness of the system
under three specific load distributions: Uniform, Pareto, and Weibull, showing
that with the minimum load $L_{\textrm{min}}$ and mean load $E[L]$ fixed,
Pareto distribution is the worst (in terms of robustness) among the three,
whereas Weibull distribution is the best with shape parameter selected
relatively large; iv) we provide numerical results that confirm our mean-field
analysis; and v) we show that $p^{\star}$ is maximized when the load
distribution is a Dirac delta function centered at $E[L]$, i.e., when all lines
carry the same load; we also show that optimal $p^{\star}$ equals
$\frac{\alpha}{\alpha+1}$. This last finding is particularly surprising given
that heterogeneity is known to lead to high robustness against random failures
in many other systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03730</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03730</id><created>2015-04-14</created><updated>2015-10-14</updated><authors><author><keyname>Zeineddine</keyname><forenames>Khalid</forenames></author><author><keyname>Hammoud</keyname><forenames>Hussein</forenames></author><author><keyname>Abou-Faycal</keyname><forenames>Ibrahim</forenames></author></authors><title>Optimal Training for Non-Feedback Adaptive PSAM over Time-Varying
  Rayleigh Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-varying fast fading channels present a major challenge in the design of
wireless communication systems. Pilot Symbol Assisted Modulation (PSAM) has
been introduced to mitigate the effects of fading and allow coherent
demodulation. Our work studies the performance of \emph{non-feedback} adaptive
PSAM scheme over time-varying Rayleigh fading channels. A modular method is
introduced for computing the rates in an efficient manner. Moreover, four
transmission policies are analyzed and we show how optimal training in terms of
duration and power allocation varies with the channel conditions and from one
transmission policy to another. The performance of these schemes is measured in
terms of achievable rates using binary signaling. We formally show that, for a
causal estimation, placing all the power on the last pilot symbol is expected
to be optimal. Furthermore, the autocorrelation of the fading process is based
either on a stationary first order Gauss-Markov modeling of the process or on
Jakes' model when higher orders of correlation are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03731</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03731</id><created>2015-04-14</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Safety enhancement through situation-aware user interfaces</title><categories>cs.HC</categories><comments>Published in the Proc. of the 7th Int.l IET System Safety Conference,
  Edinburgh, UK, 15-18 October 2012. IET</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their privileged position halfway between the physical and the cyber
universes, user interfaces may play an important role in preventing,
tolerating, and learning from scenarios potentially affecting mission safety
and the user's quality of experience. This vision is embodied here in the main
ideas and a proof-of-concepts implementation of user interfaces that combine
dynamic profiling with context- and situation-awareness and autonomic software
adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03732</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03732</id><created>2015-04-14</created><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author><author><keyname>Micha&#x142;ek</keyname><forenames>Mateusz</forenames></author></authors><title>Abelian Tensors</title><categories>math.AG cs.CC</categories><msc-class>14N05, 68Q17, 15A69, 15A21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze tensors in the tensor product of three m-dimensional vector spaces
satisfying Strassen's equations for border rank m. Results include: two purely
geometric characterizations of the Coppersmith-Winograd tensor, a reduction to
the study of symmetric tensors under a mild genericity hypothesis, and numerous
additional equations and examples. This study is closely connected to the study
of the variety of m-dimensional abelian subspaces of the space of endomorphisms
of an m-dimensional vector space, and the subvariety consisting of the Zariski
closure of the variety of maximal tori, called the variety of reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03738</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03738</id><created>2015-04-14</created><authors><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Burkovski</keyname><forenames>Andreas</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Amplify-and-Forward Relaying in Two-Hop Diffusion-Based Molecular
  Communication Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures, 1 table. Submitted to the 2015 IEEE Global
  Communications Conference (GLOBECOM) on April 15, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a three-node network in which an intermediate
nano-transceiver, acting as a relay, is placed between a nano-transmitter and a
nano-receiver to improve the range of diffusion-based molecular communication.
Motivated by the relaying protocols used in traditional wireless communication
systems, we study amplify-and-forward (AF) relaying with fixed and variable
amplification factor for use in molecular communication systems. To this end,
we derive a closed-form expression for the expected end-to-end error
probability. Furthermore, we derive a closed-form expression for the optimal
amplification factor at the relay node for minimization of an approximation of
the expected error probability of the network. Our analytical and simulation
results show the potential of AF relaying to improve the overall performance of
nano-networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03744</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03744</id><created>2015-04-14</created><authors><author><keyname>Shoja</keyname><forenames>Mohammad Reza Khalili</forenames></author><author><keyname>Amariucai</keyname><forenames>George Traian</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Deng</keyname><forenames>Jing</forenames></author></authors><title>KERMAN: A Key Establishment Algorithm based on Harvesting Randomness in
  MANETs</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Establishing secret common randomness between two or multiple devices in a
network resides at the root of communication security. The problem is
traditionally decomposed into a randomness generation stage (randomness purity
is subject to employing often costly true random number generators) and a
key-agreement information exchange stage, which can rely on public-key
infrastructure or on key wrapping. In this paper, we propose KERMAN, an
alternative key establishment algorithm for ad-hoc networks which works by
harvesting randomness directly from the network routing metadata, thus
achieving both pure randomness generation and (implicitly) secret-key
agreement. Our algorithm relies on the route discovery phase of an ad-hoc
network employing the Dynamic Source Routing protocol, is lightweight, and
requires minimal communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03747</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03747</id><created>2015-04-14</created><authors><author><keyname>Wijesekera</keyname><forenames>Primal</forenames></author><author><keyname>Baokar</keyname><forenames>Arjun</forenames></author><author><keyname>Hosseini</keyname><forenames>Ashkan</forenames></author><author><keyname>Egelman</keyname><forenames>Serge</forenames></author><author><keyname>Wagner</keyname><forenames>David</forenames></author><author><keyname>Beznosov</keyname><forenames>Konstantin</forenames></author></authors><title>Android Permissions Remystified: A Field Study on Contextual Integrity</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the amount of data that smartphone applications can potentially
access, platforms enforce permission systems that allow users to regulate how
applications access protected resources. If users are asked to make security
decisions too frequently and in benign situations, they may become habituated
and approve all future requests without regard for the consequences. If they
are asked to make too few security decisions, they may become concerned that
the platform is revealing too much sensitive information. To explore this
tradeoff, we instrumented the Android platform to collect data regarding how
often and under what circumstances smartphone applications are accessing
protected resources regulated by permissions. We performed a 36-person field
study to explore the notion of &quot;contextual integrity,&quot; that is, how often are
applications accessing protected resources when users are not expecting it?
Based on our collection of 27 million data points and exit interviews with
participants, we examine the situations in which users would like the ability
to deny applications access to protected resources. We found out that at least
80% of our participants would have preferred to prevent at least one permission
request, and overall, they thought that over a third of requests were invasive
and desired a mechanism to block them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03749</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03749</id><created>2015-04-14</created><updated>2015-12-08</updated><authors><author><keyname>Carlberg</keyname><forenames>Kevin</forenames></author><author><keyname>Barone</keyname><forenames>Matthew</forenames></author><author><keyname>Antil</keyname><forenames>Harbir</forenames></author></authors><title>Galerkin v. least-squares Petrov--Galerkin projection in nonlinear model
  reduction</title><categories>cs.NA math.NA</categories><comments>Submitted to Journal of Computational Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Least-squares Petrov--Galerkin (LSPG) model-reduction techniques such as the
Gauss--Newton with Approximated Tensors (GNAT) method have shown promise, as
they have generated stable, accurate solutions for large-scale turbulent,
compressible flow problems where standard Galerkin techniques have failed.
However, there has been limited comparative analysis of the two approaches.
This is due in part to difficulties arising from the fact that Galerkin
techniques perform optimal projection at the time-continuous level, while LSPG
techniques do so at the time-discrete level.
  This work provides a detailed theoretical and computational comparison of the
two techniques for two common classes of time integrators: linear multistep
schemes and Runge--Kutta schemes. We present a number of new findings,
including conditions under which the LSPG ROM has a time-continuous
representation, conditions under which the two techniques are equivalent, and
time-discrete error bounds for the two approaches. Perhaps most surprisingly,
we demonstrate both theoretically and computationally that decreasing the time
step does not necessarily decrease the error for the LSPG ROM; instead, the
time step should be `matched' to the spectral content of the reduced basis. In
numerical experiments carried out on a turbulent compressible-flow problem with
over one million unknowns, we show that increasing the time step to an
intermediate value decreases both the error and the simulation time of the LSPG
reduced-order model by an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03750</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03750</id><created>2015-04-14</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Responding to complexity in socio-economic systems: How to build a smart
  and resilient society?</title><categories>physics.soc-ph cs.CY</categories><comments>For related publications see http://www.coss.ethz.ch and
  http://scholar.google.com/citations?user=ebrNfPAAAAAJ&amp;hl=en</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The world is changing at an ever-increasing pace. And it has changed in a
much more fundamental way than one would think, primarily because it has become
more connected and interdependent than in our entire history. Every new
product, every new invention can be combined with those that existed before,
thereby creating an explosion of complexity: structural complexity, dynamic
complexity, functional complexity, and algorithmic complexity. How to respond
to this challenge? And what are the costs?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03751</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03751</id><created>2015-04-14</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Societal, Economic, Ethical and Legal Challenges of the Digital
  Revolution: From Big Data to Deep Learning, Artificial Intelligence, and
  Manipulative Technologies</title><categories>cs.CY physics.soc-ph</categories><comments>For related publications see http://futurict.blogspot.com and
  http://coss.ethz.ch</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the wake of the on-going digital revolution, we will see a dramatic
transformation of our economy and most of our societal institutions. While the
benefits of this transformation can be massive, there are also tremendous risks
to our society. After the automation of many production processes and the
creation of self-driving vehicles, the automation of society is next. This is
moving us to a tipping point and to a crossroads: we must decide between a
society in which the actions are determined in a top-down way and then
implemented by coercion or manipulative technologies (such as personalized ads
and nudging) or a society, in which decisions are taken in a free and
participatory way and mutually coordinated. Modern information and
communication systems (ICT) enable both, but the latter has economic and
strategic benefits. The fundaments of human dignity, autonomous
decision-making, and democracies are shaking, but I believe that they need to
be vigorously defended, as they are not only core principles of livable
societies, but also the basis of greater efficiency and success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03754</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03754</id><created>2015-04-14</created><updated>2016-02-23</updated><authors><author><keyname>Mahdian</keyname><forenames>Milad</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author></authors><title>Throughput-Delay Tradeoffs in Content-Centric Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the throughput and delay characteristics of wireless networks based
on a content-centric network architecture, where users are mainly interested in
retrieving content stored in the network, rather than in maintaining
source-destination communication. Nodes are assumed to be uniformly distributed
in the network area. Each node has a limited-capacity content store, which it
uses to cache contents according to the proposed caching scheme. Requested
content follows a general popularity distribution, and users employ multi-hop
communication to retrieve the requested content from the closest cache. We
derive the throughput-delay tradeoff of the content-centric wireless network
model and solve the caching optimization problem. We then evaluate the network
performance for a Zipf content popularity distribution, letting the number of
content types and the network size both go to infinity. Finally, we extend our
analysis to heterogeneous wireless networks where, in addition to wireless
nodes, there are a number of base stations uniformly distributed at random in
the network area. We show that in order to achieve a better performance in a
heterogeneous network in the order sense, the number of base stations needs to
be greater than the ratio of the number of nodes to the number of content
types. Furthermore, we show that the heterogeneous network does not yield
performance advantages in the order sense if the Zipf content popularity
distribution exponent exceeds 3/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03761</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03761</id><created>2015-04-14</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Jungers</keyname><forenames>Raphael</forenames></author></authors><title>Lower Bounds on Complexity of Lyapunov Functions for Switched Linear
  Systems</title><categories>math.OC cs.CC cs.SY math.DS nlin.CD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any positive integer $d$, there are families of switched
linear systems---in fixed dimension and defined by two matrices only---that are
stable under arbitrary switching but do not admit (i) a polynomial Lyapunov
function of degree $\leq d$, or (ii) a polytopic Lyapunov function with $\leq
d$ facets, or (iii) a piecewise quadratic Lyapunov function with $\leq d$
pieces. This implies that there cannot be an upper bound on the size of the
linear and semidefinite programs that search for such stability certificates.
Several constructive and non-constructive arguments are presented which connect
our problem to known (and rather classical) results in the literature regarding
the finiteness conjecture, undecidability, and non-algebraicity of the joint
spectral radius. In particular, we show that existence of an extremal piecewise
algebraic Lyapunov function implies the finiteness property of the optimal
product, generalizing a result of Lagarias and Wang. As a corollary, we prove
that the finiteness property holds for sets of matrices with an extremal
Lyapunov function belonging to some of the most popular function classes in
controls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03763</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03763</id><created>2015-04-14</created><updated>2016-01-12</updated><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Memoli</keyname><forenames>Facundo</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Mutiscale Mapper: A Framework for Topological Summarization of Data and
  Maps</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarizing topological information from datasets and maps defined on them is
a central theme in topological data analysis. \textsf{Mapper}, a tool for such
summarization, takes as input both a possibly high dimensional dataset and a
map defined on the data, and produces a summary of the data by using a cover of
the codomain of the map. This cover, via a pullback operation to the domain,
produces a simplicial complex connecting the data points.
  The resulting view of the data through a cover of the codomain offers
flexibility in analyzing the data. However, it offers only a view at a fixed
scale at which the cover is constructed. Inspired by the concept, we explore a
notion of a tower of covers which induces a tower of simplicial complexes
connected by simplicial maps, which we call {\em multiscale mapper}. We study
the resulting structure, its stability, and design practical algorithms to
compute its associated persistence diagrams efficiently. Specifically, when the
domain is a simplicial complex and the map is a real-valued piecewise-linear
function, the algorithm can compute the exact persistence diagram only from the
1-skeleton of the input complex. For general maps, we present a combinatorial
version of the algorithm that acts only on \emph{vertex sets} connected by the
1-skeleton graph, and this algorithm approximates the exact persistence diagram
thanks to a stability result that we show to hold. We also relate the
multiscale mapper with the \v{C}ech complexes arising from a natural pullback
pseudometric defined on the input domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03770</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03770</id><created>2015-04-14</created><authors><author><keyname>Li</keyname><forenames>Xuhui</forenames></author><author><keyname>Liu</keyname><forenames>Mengchi</forenames></author><author><keyname>Wu</keyname><forenames>Xiaoying</forenames></author><author><keyname>Zhu</keyname><forenames>Shanfeng</forenames></author></authors><title>Design Issues of JPQ: a Pattern-based Query Language for Document
  Databases</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document databases are becoming popular, but how to present complex document
query to obtain useful information from the document remains an important topic
to study. In this paper, we describe the design issues of a pattern-based
document database query language named JPQ. JPQ uses various expressive
patterns to extract and construct document fragments following a JSON-like
document data model. It adopts tree-like extraction patterns with a coherent
pattern composition mechanism to extract data elements from hierarchically
structured documents and maintain the logical relationships among the elements.
Based on these relationships, JPQ deploys a deductive mechanism to
declaratively specify the data transformation requests and considers also data
filtering on hierarchical data structure. We use various examples to show the
features of the language and to demonstrate its expressiveness and
declarativeness in presenting complex document queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03777</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03777</id><created>2015-04-14</created><authors><author><keyname>Ni</keyname><forenames>Weiheng</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author><author><keyname>Lu</keyname><forenames>Wu-Sheng</forenames></author></authors><title>Near-Optimal Hybrid Processing for Massive MIMO Systems via Matrix
  Decomposition</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the practical implementation of massive multiple-input multiple-output
(MIMO) systems, the hybrid processing (precoding/combining) structure is
promising to reduce the high cost rendered by large number of RF chains of the
traditional processing structure. The hybrid processing is performed through
low-dimensional digital baseband processing combined with analog RF processing
enabled by phase shifters. We propose to design hybrid RF and baseband
precoders/combiners for multi-stream transmission in point-to-point massive
MIMO systems, by directly decomposing the pre-designed unconstrained digital
precoder/combiner of a large dimension. The constant amplitude constraint of
analog RF processing results in the matrix decomposition problem non-convex.
Based on an alternate optimization technique, the non-convex matrix
decomposition problem can be decoupled into a series of convex sub-problems and
effectively solved by restricting the phase increment of each entry in the RF
precoder/combiner within a small vicinity of its preceding iterate. A singular
value decomposition based technique is proposed to secure an initial point
sufficiently close to the global solution of the original non-convex problem.
Through simulation, the convergence of the alternate optimization for such a
matrix decomposition based hybrid processing (MD-HP) scheme is examined, and
the performance of the MD-HP scheme is demonstrated to be near-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03778</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03778</id><created>2015-04-14</created><authors><author><keyname>Benaloh</keyname><forenames>Josh</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Rivest</keyname><forenames>Ronald</forenames><affiliation>Massachusetts Institute of Technology</affiliation></author><author><keyname>Ryan</keyname><forenames>Peter Y. A.</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Stark</keyname><forenames>Philip</forenames><affiliation>University of California, Berkeley</affiliation></author><author><keyname>Teague</keyname><forenames>Vanessa</forenames><affiliation>University of Melbourne</affiliation></author><author><keyname>Vora</keyname><forenames>Poorvi</forenames><affiliation>George Washington University</affiliation></author></authors><title>End-to-end verifiability</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This pamphlet describes end-to-end election verifiability (E2E-V) for a
nontechnical audience: election officials, public policymakers, and anyone else
interested in secure, transparent, evidence-based electronic elections.
  This work is part of the Overseas Vote Foundation's End-to-End Verifiable
Internet Voting: Specification and Feasibility Assessment Study (E2E VIV
Project), funded by the Democracy Fund.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03803</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03803</id><created>2015-04-15</created><authors><author><keyname>Fritzsche</keyname><forenames>R.</forenames></author><author><keyname>Rost</keyname><forenames>P.</forenames></author><author><keyname>Fettweis</keyname><forenames>G.</forenames></author></authors><title>Robust Rate Adaptation and Proportional Fair Scheduling with Imperfect
  CSI</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages, 12 figures, accepted for publication in IEEE Transactions
  on Wireless Communications</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless fading channels, multi-user scheduling has the potential to boost
the spectral efficiency by exploiting diversity gains. In this regard,
proportional fair (PF) scheduling provides a solution for increasing the users'
quality of experience by finding a balance between system throughput
maximization and user fairness. For this purpose, precise instantaneous channel
state information (CSI) needs to be available at the transmitter side to
perform rate adaptation and scheduling. However, in practical setups, CSI is
impaired by, e.g., channel estimation errors, quantization and feedback delays.
Especially in centralized cloud based communication systems, where main parts
of the lower layer processing is shifted to a central entity, high backhaul
latency can cause substantial CSI imperfections, resulting in significant
performance degradations. In this work robust rate adaptation as well as robust
PF scheduling are presented, which account for CSI impairments. The proposed
rate adaptation solution guarantees a fixed target outage probability, which is
of interest for delay critical and data intensive applications, such as, video
conference systems. In addition to CSI imperfections the proposed scheduler is
able to account for delayed decoding acknowledgements from the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03809</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03809</id><created>2015-04-15</created><updated>2015-08-22</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Elwes</keyname><forenames>Richard</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andy</forenames></author></authors><title>From randomness to order: unperturbed Schelling segregation in two or
  three dimensions</title><categories>cs.MA</categories><comments>24 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schelling's model of segregation, first described in 1969, has become one of
the best known models of self-organising behaviour. While Schelling's explicit
concern was to understand the mechanisms underlying racial segregation in large
cities from a game theoretic perspective, the model should be seen as one of a
family, arising in fields as diverse as statistical mechanics, neural networks
and the social sciences, and which are concerned with interacting populations
situated on network structures. Despite extensive study, however, the
(unperturbed) Schelling model has largely resisted rigorous analysis, prior
results in the literature generally pertaining to variants of the model in
which noise is introduced into the dynamics of the system, the resulting model
then being amenable to standard techniques from statistical mechanics or
stochastic evolutionary game theory. A series of recent papers (one by Brandt,
Immorlica, Kamath, and Kleinberg, and two by the authors), has seen the first
rigorous analysis of the one dimensional version of the unperturbed model. Here
we provide the first rigorous analysis of the two and three dimensional
unperturbed models, establishing most of the phase diagram, and answering a
challenge from a recent paper by Brandt, Immorlica, Kamath, and Kleinberg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03810</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03810</id><created>2015-04-15</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>L.</keyname><forenames>Smitha M.</forenames></author></authors><title>Text Localization in Video Using Multiscale Weber's Local Descriptor</title><categories>cs.CV</categories><comments>IEEE SPICES, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel approach for detecting the text present in
videos and scene images based on the Multiscale Weber's Local Descriptor
(MWLD). Given an input video, the shots are identified and the key frames are
extracted based on their spatio-temporal relationship. From each key frame, we
detect the local region information using WLD with different radius and
neighborhood relationship of pixel values and hence obtained intensity enhanced
key frames at multiple scales. These multiscale WLD key frames are merged
together and then the horizontal gradients are computed using morphological
operations. The obtained results are then binarized and the false positives are
eliminated based on geometrical properties. Finally, we employ connected
component analysis and morphological dilation operation to determine the text
regions that aids in text localization. The experimental results obtained on
publicly available standard Hua, Horizontal-1 and Horizontal-2 video dataset
illustrate that the proposed method can accurately detect and localize texts of
various sizes, fonts and colors in videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03811</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03811</id><created>2015-04-15</created><authors><author><keyname>Chuang</keyname><forenames>Meng-Che</forenames></author><author><keyname>Hwang</keyname><forenames>Jenq-Neng</forenames></author><author><keyname>Williams</keyname><forenames>Kresimir</forenames></author><author><keyname>Towler</keyname><forenames>Richard</forenames></author></authors><title>Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos</title><categories>cs.CV</categories><comments>14 pages, 14 figures, 6 tables</comments><journal-ref>IEEE Trans. on Circuits and Systems for Video Technology, vol. 25,
  no. 1, pp.167-179, Jan. 2015</journal-ref><doi>10.1109/TCSVT.2014.2357093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-extractive fish abundance estimation with the aid of visual analysis has
drawn increasing attention. Unstable illumination, ubiquitous noise and low
frame rate video capturing in the underwater environment, however, make
conventional tracking methods unreliable. In this paper, we present a multiple
fish tracking system for low-contrast and low-frame-rate stereo videos with the
use of a trawl-based underwater camera system. An automatic fish segmentation
algorithm overcomes the low-contrast issues by adopting a histogram
backprojection approach on double local-thresholded images to ensure an
accurate segmentation on the fish shape boundaries. Built upon a reliable
feature-based object matching method, a multiple-target tracking algorithm via
a modified Viterbi data association is proposed to overcome the poor motion
continuity and frequent entrance/exit of fish targets under low-frame-rate
scenarios. In addition, a computationally efficient block-matching approach
performs successful stereo matching, which enables an automatic fish-body tail
compensation to greatly reduce segmentation error and allows for an accurate
fish length measurement. Experimental results show that an effective and
reliable tracking performance for multiple live fish with underwater stereo
cameras is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03812</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03812</id><created>2015-04-15</created><updated>2015-09-10</updated><authors><author><keyname>Hamann</keyname><forenames>Michael</forenames></author><author><keyname>Strasser</keyname><forenames>Ben</forenames></author></authors><title>Graph Bisection with Pareto-Optimization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce FlowCutter, a novel algorithm to compute a set of edge cuts or
node separators that optimize cut size and balance in the Pareto-sense. Our
core algorithm solves the balanced connected st-edge-cut problem, where two
given nodes s and t must be separated by removing edges to obtain two connected
parts. Using the core algorithm we build variants that compute node separators
and are independent of s and t. Using the Pareto-set we can identify cuts with
a particularly good trade-off between cut size and balance that can be used to
compute contraction and minimum fill-in orders, which can be used in
Customizable Contraction Hierarchies (CCH), a speed-up technique for shortest
path computations. Our core algorithm runs in O(cm) time where m is the number
of edges and c the cut size. This makes it well-suited for large graphs with
small cuts, such as road graphs, which are our primary application. For road
graphs we present an extensive experimental study demonstrating that FlowCutter
outperforms the current state of the art both in terms of cut sizes as well as
CCH performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03814</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03814</id><created>2015-04-15</created><updated>2015-09-16</updated><authors><author><keyname>Fahs</keyname><forenames>Jihad</forenames></author><author><keyname>Abou-Faycal</keyname><forenames>Ibrahim</forenames></author></authors><title>On the Finiteness of the Capacity of Continuous Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2015.2503403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating the channel capacity is one of many key problems in information
theory. In this work we derive rather-mild sufficient conditions under which
the capacity is finite and achievable. These conditions are derived for
generic, memoryless and possibly non-linear additive noise channels. The
results are based on a novel sufficient condition that guarantees the
convergence of differential entropies under point-wise convergence of
Probability Density Functions. Perhaps surprisingly, the finiteness of channel
capacity holds for the majority of setups, including those where inputs and
outputs have possibly infinite second-moments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03824</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03824</id><created>2015-04-15</created><updated>2015-07-19</updated><authors><author><keyname>Hu</keyname><forenames>Yawei</forenames></author><author><keyname>Xiao</keyname><forenames>Mingjun</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author><author><keyname>Cheng</keyname><forenames>Ruhong</forenames></author><author><keyname>Mao</keyname><forenames>Hualin</forenames></author></authors><title>Nearly Optimal Probabilistic Coverage for Roadside Advertisement
  Dissemination in Urban VANETs</title><categories>cs.NI cs.SI</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the proof of theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advertisement disseminations based on Roadside Access Points (RAPs) in
vehicular ad-hoc networks (VANETs) attract lots of attentions and have a
promising prospect. In this paper, we focus on a roadside advertisement
dissemination, including three basic elements: RAP Service Provider (RSP),
mobile vehicles and shops. The RSP has deployed many RAPs at different
locations in a city. A shop wants to rent some RAPs, which can disseminate
advertisements to vehicles with some probabilites. Then, it tries to select the
minimal number of RAPs to finish the advertisement dissemination, in order to
save the expenses. Meanwhile, the selected RAPs need to ensure that each
vehicle's probability of receiving advertisement successfully is not less than
a threshold. We prove that this RAP selection problem is NP-hard. In order to
solve this problem, we propose a greedy approximation algorithm, and give the
corresponding approximation ratio. Further, we conduct extensive simulations on
real world data sets to prove the good performance of this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1504.03834</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1504.03834</id><created>2015-04-15</created><updated>2015-04-16</updated><authors><author><keyname>Phukpattaranont</keyname><forenames>Pornchai</forenames></author></authors><title>Comparisons of wavelet functions in QRS signal to noise ratio
  enhancement and detection accuracy</title><categories>cs.CV cs.CE</categories><comments>16 pages, 8 figures, Article submitted to Journal of the Korean
  Physical Society for considering of publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the capability of wavelet functions used for noise removal in
preprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)
signal. The QRS signal to noise ratio enhancement and the detection accuracy of
each wavelet function are evaluated using three measures: (1) the ratio of the
maximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of
absolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet
functions from previous well-known publications are explored, i.e., Bior1.3,
Db10, and Mexican hat wavelet functions. Results evaluated with the ECG signal
from MIT-BIH arrhythmia database show that the Mexican hat wavelet function is
better than the others. While the scale 8 of Mexican hat wavelet function can
provide the best enhancement in QRS signal to noise ratio, the scale 4 of
Mexican hat wavelet function can provide the best detection accuracy. These
results may be combined and may enable the use of a single fixed threshold for
all ECG records leading to the reduction in computational complexity of the QRS
detection algorithm.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="75000" completeListSize="102538">1122234|76001</resumptionToken>
</ListRecords>
</OAI-PMH>
