<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:14:54Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|48001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3901</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3901</id><created>2013-07-15</created><authors><author><keyname>Z&#xf6;rlein</keyname><forenames>Henning</forenames></author><author><keyname>Akram</keyname><forenames>Faisal</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>Dictionary Adaptation in Sparse Recovery Based on Different Types of
  Coherence</title><categories>cs.IT math.IT</categories><comments>Contribution to the 2nd International Workshop on Compressed Sensing
  applied to Radar (CoSeRa) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sparse recovery, the unique sparsest solution to an under-determined
system of linear equations is of main interest. This scheme is commonly
proposed to be applied to signal acquisition. In most cases, the signals are
not sparse themselves, and therefore, they need to be sparsely represented with
the help of a so-called dictionary being specific to the corresponding signal
family. The dictionaries cannot be used for optimization of the resulting
under-determined system because they are fixed by the given signal family.
However, the measurement matrix is available for optimization and can be
adapted to the dictionary. Multiple properties of the resulting linear system
have been proposed which can be used as objective functions for optimization.
This paper discusses two of them which are both related to the coherence of
vectors. One property aims for having incoherent measurements, while the other
aims for insuring the successful reconstruction. In the following, the
influences of both criteria are compared with different reconstruction
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3911</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3911</id><created>2013-07-15</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>The Effects of Technology and Innovation on Society</title><categories>cs.CY</categories><comments>10 pages</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><journal-ref>Bahria University Journal of Information &amp; Communication
  Technology Vol. 5, Issue 1 December 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various models of the information society have been developed so far and they
are so different from country to country that it would be rather unwise to look
for a single, allencompassing definition. In our time a number of profound
socioeconomic changes are underway. The application of these theories and
schools on ICT is problematic in many respects. First, as we stated above,
there is not a single, widely used paradigm which has synthesised the various
schools and theories dealing with technology and society. Second, these
fragmented approaches do not have a fully fledged mode of application to the
relationship of ICT and (information) society. Third, SCOT, ANT, the
evolutionary or the systems approach to the history of technology when dealing
with information society does not take into account the results of approaches
studying the very essence of the information age: information, communication
and knowledge. The list of unnoticed or partially incorporated sciences, which
focuses on the role of ICT in human information processing and other cognitive
activities, is much longer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3913</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3913</id><created>2013-07-15</created><updated>2013-09-20</updated><authors><author><keyname>Nordstrom</keyname><forenames>Jakob</forenames><affiliation>MIT</affiliation></author></authors><title>Pebble Games, Proof Complexity, and Time-Space Trade-offs</title><categories>cs.CC cs.DM cs.LO math.CO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  13, 2013) lmcs:1111</journal-ref><doi>10.2168/LMCS-9(3:15)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pebble games were extensively studied in the 1970s and 1980s in a number of
different contexts. The last decade has seen a revival of interest in pebble
games coming from the field of proof complexity. Pebbling has proven to be a
useful tool for studying resolution-based proof systems when comparing the
strength of different subsystems, showing bounds on proof space, and
establishing size-space trade-offs. This is a survey of research in proof
complexity drawing on results and tools from pebbling, with a focus on proof
space lower bounds and trade-offs between proof size and proof space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3940</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3940</id><created>2013-07-15</created><authors><author><keyname>Ding</keyname><forenames>Wei</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author></authors><title>Large-scale MU-MIMO: It Is Necessary to Deploy Extra Antennas at Base
  Station</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the large-scale MU-MIMO system is considered where a base
station (BS) with extremely large number of antennas (N) serves relatively less
number of users (K). In order to achieve largest sum rate, it is proven that
the amount of users must be limited such that the number of antennas at the BS
is preponderant over that of the antennas at all the users. In other words, the
antennas at the BS should be excess. The extra antennas at the BS are no longer
just an optional approach to enhance the system performance but the
prerequisite to the largest sum rate. Based on this factor, for a fixed N, the
optimal K that maximizes the sum rate is further obtained. Additionally, it is
also pointed out that the sum rate can be substantially improved by only adding
a few antennas at the BS when the system is N=KM with M denoting the antennas
at each user. The derivations are under the assumption of N and M going to
infinity, and being implemented on different precoders. Numerical simulations
verify the tightness and accuracy of our asymptotic results even for small N
and M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3949</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3949</id><created>2013-07-15</created><updated>2014-06-24</updated><authors><author><keyname>Borgwardt</keyname><forenames>Steffen</forenames></author></authors><title>On Soft Power Diagrams</title><categories>cs.LG math.OC stat.ML</categories><msc-class>90C90, 90C46, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications in data analysis begin with a set of points in a Euclidean
space that is partitioned into clusters. Common tasks then are to devise a
classifier deciding which of the clusters a new point is associated to, finding
outliers with respect to the clusters, or identifying the type of clustering
used for the partition.
  One of the common kinds of clusterings are (balanced) least-squares
assignments with respect to a given set of sites. For these, there is a
'separating power diagram' for which each cluster lies in its own cell.
  In the present paper, we aim for efficient algorithms for outlier detection
and the computation of thresholds that measure how similar a clustering is to a
least-squares assignment for fixed sites. For this purpose, we devise a new
model for the computation of a 'soft power diagram', which allows a soft
separation of the clusters with 'point counting properties'; e.g. we are able
to prescribe how many points we want to classify as outliers.
  As our results hold for a more general non-convex model of free sites, we
describe it and our proofs in this more general way. Its locally optimal
solutions satisfy the aforementioned point counting properties. For our target
applications that use fixed sites, our algorithms are efficiently solvable to
global optimality by linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3964</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3964</id><created>2013-07-15</created><authors><author><keyname>Edera</keyname><forenames>Alejandro</forenames></author><author><keyname>Schl&#xfc;ter</keyname><forenames>Federico</forenames></author><author><keyname>Bromberg</keyname><forenames>Facundo</forenames></author></authors><title>Learning Markov networks with context-specific independences</title><categories>cs.AI cs.LG stat.ML</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the Markov network structure from data is a problem that has
received considerable attention in machine learning, and in many other
application fields. This work focuses on a particular approach for this purpose
called independence-based learning. Such approach guarantees the learning of
the correct structure efficiently, whenever data is sufficient for representing
the underlying distribution. However, an important issue of such approach is
that the learned structures are encoded in an undirected graph. The problem
with graphs is that they cannot encode some types of independence relations,
such as the context-specific independences. They are a particular case of
conditional independences that is true only for a certain assignment of its
conditioning set, in contrast to conditional independences that must hold for
all its assignments. In this work we present CSPC, an independence-based
algorithm for learning structures that encode context-specific independences,
and encoding them in a log-linear model, instead of a graph. The central idea
of CSPC is combining the theoretical guarantees provided by the
independence-based approach with the benefits of representing complex
structures by using features in a log-linear model. We present experiments in a
synthetic case, showing that CSPC is more accurate than the state-of-the-art IB
algorithms when the underlying distribution contains CSIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3975</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3975</id><created>2013-07-15</created><authors><author><keyname>Friedl</keyname><forenames>Katalin</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Some Improvements to Total Degree Tests</title><categories>cs.CC</categories><comments>A version of this paper appeared in Proceedings of the 3rd Israel
  Symposium on Theory of Computing and Systems, Tel Aviv, Israel, Jan 4-7,
  1995. This version corrects some typographical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low-degree test is a collection of simple, local rules for checking the
proximity of an arbitrary function to a low-degree polynomial. Each rule
depends on the function's values at a small number of places. If a function
satisfies many rules then it is close to a low-degree polynomial. Low-degree
tests play an important role in the development of probabilistically checkable
proofs.
  In this paper we present two improvements to the efficiency of low-degree
tests. Our first improvement concerns the smallest field size over which a
low-degree test can work. We show how to test that a function is a degree $d$
polynomial over prime fields of size only $d+2$.
  Our second improvement shows a better efficiency of the low-degree test of
Rubinfeld and Sudan (Proc. SODA 1992) than previously known. We show concrete
applications of this improvement via the notion of &quot;locally checkable codes&quot;.
This improvement translates into better tradeoffs on the size versus probe
complexity of probabilistically checkable proofs than previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3976</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3976</id><created>2013-07-15</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author><author><keyname>Garro</keyname><forenames>Alfredo</forenames></author></authors><title>Single-tape and Multi-tape Turing machines through the lens of the
  Grossone methodology</title><categories>cs.FL</categories><comments>21 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.3298</comments><msc-class>68Q05, 03D10</msc-class><acm-class>F.1.1; F.4.1</acm-class><journal-ref>Journal of Supercomputing, 2013, 65(2), 645-663</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates how the mathematical languages used to describe and to
observe automatic computations influence the accuracy of the obtained results.
In particular, we focus our attention on Single and Multi-tape Turing machines
which are described and observed through the lens of a new mathematical
language which is strongly based on three methodological ideas borrowed from
Physics and applied to Mathematics, namely: the distinction between the object
(we speak here about a mathematical object) of an observation and the
instrument used for this observation; interrelations holding between the object
and the tool used for the observation; the accuracy of the observation
determined by the tool. Results of the observation executed by the traditional
and new languages are compared and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4007</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4007</id><created>2013-07-15</created><updated>2014-01-08</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author></authors><title>Asymmetry of the Kolmogorov complexity of online predicting odd and even
  bits</title><categories>cs.IT math.IT</categories><comments>20 pages, 7 figures</comments><msc-class>68Q30, 94A17</msc-class><acm-class>E.4; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry of information states that $C(x) + C(y|x) = C(x,y) + O(\log C(x))$.
We show that a similar relation for online Kolmogorov complexity does not hold.
Let the even (online Kolmogorov) complexity of an n-bitstring $x_1x_2... x_n$
be the length of a shortest program that computes $x_2$ on input $x_1$,
computes $x_4$ on input $x_1x_2x_3$, etc; and similar for odd complexity. We
show that for all n there exist an n-bit x such that both odd and even
complexity are almost as large as the Kolmogorov complexity of the whole
string. Moreover, flipping odd and even bits to obtain a sequence
$x_2x_1x_4x_3\ldots$, decreases the sum of odd and even complexity to $C(x)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4030</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4030</id><created>2013-07-15</created><updated>2015-08-26</updated><authors><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Wider</keyname><forenames>Nicolas</forenames></author><author><keyname>Pfitzner</keyname><forenames>Rene</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Causality-Driven Slow-Down and Speed-Up of Diffusion in Non-Markovian
  Temporal Networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>31 pages, 13 figures, including supplementary information</comments><msc-class>05C82</msc-class><acm-class>C.2.1; G.2.2; H.1.2; H.2.8; H.3.3</acm-class><journal-ref>Nature Communications, Vol. 5, Sept 2014</journal-ref><doi>10.1038/ncomms6024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has highlighted limitations of studying complex systems with
time-varying topologies from the perspective of static, time-aggregated
networks. Non-Markovian characteristics resulting from the ordering of
interactions in temporal networks were identified as one important mechanism
that alters causality, and affects dynamical processes. So far, an analytical
explanation for this phenomenon and for the significant variations observed
across different systems is missing. Here we introduce a methodology that
allows to analytically predict causality-driven changes of diffusion speed in
non-Markovian temporal networks. Validating our predictions in six data sets,
we show that - compared to the time-aggregated network - non-Markovian
characteristics can lead to both a slow-down, or speed-up of diffusion which
can even outweigh the decelerating effect of community structures in the static
topology. Thus, non-Markovian properties of temporal networks constitute an
important additional dimension of complexity in time-varying complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4038</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4038</id><created>2013-07-15</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>An alternative Gospel of structure: order, composition, processes</title><categories>math.CT cs.CL quant-ph</categories><comments>Introductory chapter to C. Heunen, M. Sadrzadeh, and E. Grefenstette.
  Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse.
  Oxford University Press, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey some basic mathematical structures, which arguably are more
primitive than the structures taught at school. These structures are orders,
with or without composition, and (symmetric) monoidal categories. We list
several `real life' incarnations of each of these. This paper also serves as an
introduction to these structures and their current and potentially future uses
in linguistics, physics and knowledge representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4046</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4046</id><created>2013-07-15</created><updated>2013-07-17</updated><authors><author><keyname>Nagy</keyname><forenames>Marcin</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author><author><keyname>Ott</keyname><forenames>Joerg</forenames></author></authors><title>PeerShare: A System Secure Distribution of Sensitive Data Among Social
  Contacts</title><categories>cs.CR</categories><comments>Technical report of the PeerShare system</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design and implementation of the PeerShare, a system that can
be used by applications to securely distribute sensitive data to social
contacts of a user. PeerShare incorporates a generic framework that allows
different applications to distribute data with different security requirements.
By using interfaces available from existing popular social networks. PeerShare
is designed to be easy to use for both end users as well as developers of
applications. PeerShare can be used to distribute shared keys, public keys and
any other data that need to be distributed with authenticity and
confidentiality guarantees to an authorized set of recipients, specified in
terms of social relationships. We have used \peershare already in three
different applications and plan to make it available for developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4047</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4047</id><created>2013-07-15</created><authors><author><keyname>Elkin</keyname><forenames>Lisa</forenames></author><author><keyname>Pong</keyname><forenames>Ting Kei</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen</forenames></author></authors><title>Convex relaxation for finding planted influential nodes in a social
  network</title><categories>math.OC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing influence in a social network. We focus
on the case that the social network is a directed bipartite graph whose arcs
join senders to receivers. We consider both the case of deterministic networks
and probabilistic graphical models, that is, the so-called &quot;cascade&quot; model. The
problem is to find the set of the $k$ most influential senders for a given
integer $k$. Although this problem is NP-hard, there is a polynomial-time
approximation algorithm due to Kempe, Kleinberg and Tardos. In this work we
consider convex relaxation for the problem. We prove that convex optimization
can recover the exact optimizer in the case that the network is constructed
according to a generative model in which influential nodes are planted but then
obscured with noise. We also demonstrate computationally that the convex
relaxation can succeed on a more realistic generative model called the &quot;forest
fire&quot; model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4048</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4048</id><created>2013-07-15</created><authors><author><keyname>Kumar</keyname><forenames>D. S. Pavan</forenames></author><author><keyname>Prasad</keyname><forenames>N. Vishnu</forenames></author><author><keyname>Joshi</keyname><forenames>Vikas</forenames></author><author><keyname>Umesh</keyname><forenames>S.</forenames></author></authors><title>Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust
  Speech Recognition</title><categories>cs.LG cs.CV stat.ML</categories><comments>Submitted to Automatic Speech Recognition and Understanding (ASRU)
  2013 Workshop</comments><doi>10.1109/ASRU.2013.6707725</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a modification to the training process of the popular SPLICE
algorithm has been proposed for noise robust speech recognition. The
modification is based on feature correlations, and enables this stereo-based
algorithm to improve the performance in all noise conditions, especially in
unseen cases. Further, the modified framework is extended to work for
non-stereo datasets where clean and noisy training utterances, but not stereo
counterparts, are required. Finally, an MLLR-based computationally efficient
run-time noise adaptation method in SPLICE framework has been proposed. The
modified SPLICE shows 8.6% absolute improvement over SPLICE in Test C of
Aurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%
absolute improvements over Aurora-2 and Aurora-4 baseline models respectively.
Run-time adaptation shows 9.89% absolute improvement in modified framework as
compared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLR
adaptation on HMMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4062</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4062</id><created>2013-07-15</created><updated>2013-08-21</updated><authors><author><keyname>Mendez</keyname><forenames>Diego</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Baudry</keyname><forenames>Benoit</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Empirical Evidence of Large-Scale Diversity in API Usage of
  Object-Oriented Software</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>International Conference on Source Code Analysis and Manipulation
  (SCAM'2013), Netherlands (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on &quot;usage diversity'&quot;, defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4063</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4063</id><created>2013-07-15</created><authors><author><keyname>SalahEldeen</keyname><forenames>Hany M.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Reading the Correct History? Modeling Temporal Intention in Resource
  Sharing</title><categories>cs.IR</categories><comments>JCDL 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web is trapped in the &quot;perpetual now&quot;, and when users traverse from page
to page, they are seeing the state of the web resource (i.e., the page) as it
exists at the time of the click and not necessarily at the time when the link
was made. Thus, a temporal discrepancy can arise between the resource at the
time the page author created a link to it and the time when a reader follows
the link. This is especially important in the context of social media: the ease
of sharing links in a tweet or Facebook post allows many people to author web
content, but the space constraints combined with poor awareness by authors
often prevents sufficient context from being generated to determine the intent
of the post. If the links are clicked as soon as they are shared, the temporal
distance between sharing and clicking is so small that there is little to no
difference in content. However, not all clicks occur immediately, and a delay
of days or even hours can result in reading something other than what the
author intended. We introduce the concept of a user's temporal intention upon
publishing a link in social media. We investigate the features that could be
extracted from the post, the linked resource, and the patterns of social
dissemination to model this user intention. Finally, we analyze the historical
integrity of the shared resources in social media across time. In other words,
how much is the knowledge of the author's intent beneficial in maintaining the
consistency of the story being told through social posts and in enriching the
archived content coverage and depth of vulnerable resources?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4076</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4076</id><created>2013-07-13</created><authors><author><keyname>Karnavel</keyname><forenames>K.</forenames></author><author><keyname>Baladhandayutham</keyname><forenames>A.</forenames></author></authors><title>Shuffling: Improving Data Security in Ad Hoc Networks based on Unipath
  Routing</title><categories>cs.CR cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.3402,
  arXiv:1307.3550</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An ad hoc network is a self-organizing network with help of Access Point (AP)
of wireless links connecting nodes to another. The nodes can communicate
without infrastructure network. They form an random topology (BSS/ESS), where
the nodes play the role of routers and are free to move arbitrarily. Ad hoc
networks confirmed their efficiency being used in different fields but they are
highly vulnerable to security attacks and dealing with this is one of the main
challenges of these networks today. In recent times, a few solutions are
proposed to provide authentication, confidentiality, availability, secure
routing and intrusion prevention in ad hoc networks. Employ security in such
dynamically changing networks is a hard task. Ad hoc network uniqueness must be
taken into contemplation to be clever to design efficient clarification. Here
we spotlight on improving the flow transmission confidentiality in ad hoc
networks based on unipath routing. Definitely, we take advantage of the being
of multiple paths between nodes in an ad hoc network to increase the
confidentiality robustness of transmitted data with the help of Access Point.
In our approach the original message to secure is split into shares through
access point that are encrypted and combined then transmitted along different
disjointed existing paths between sender and receiver. Still if an attacker
succeeds to attain one or more transmitted contribute to the probability that
the unique message will be reconstituted is very low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4077</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4077</id><created>2013-07-15</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>A Conceptual Framework for Definition of the Correlation Between Company
  Size Categories and the Proliferation of Business Information Systems in
  Hungary</title><categories>cs.CY</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1303.0520, arXiv:1307.3760</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><journal-ref>'Club of Economics in Miskolc' TMP Vol. 8., Nr. 2., pp. 51-59.
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a conceptual model, this paper aims to explore the background of the
decision-making process leading to the introduction of business information
systems among enterprises in Hungary. Together with presenting the problems
arising in the course of the implementation of such systems, their usage
patterns are also investigated. A strong correlation is established between the
size of an enterprise, the scope of its business activities and the range of
the business information systems it applies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4097</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4097</id><created>2013-07-15</created><authors><author><keyname>Vavasis</keyname><forenames>Stephen</forenames></author></authors><title>Some notes on applying computational divided differencing in
  optimization</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of accurate computation of the finite difference
$f(\x+\s)-f(\x)$ when $\Vert\s\Vert$ is very small. Direct evaluation of this
difference in floating point arithmetic succumbs to cancellation error and
yields 0 when $\s$ is sufficiently small. Nonetheless, accurate computation of
this finite difference is required by many optimization algorithms for a
&quot;sufficient decrease&quot; test. Reps and Rall proposed a programmatic
transformation called &quot;computational divided differencing&quot; reminiscent of
automatic differentiation to compute these differences with high accuracy. The
running time to compute the difference is a small constant multiple of the
running time to compute $f$. Unlike automatic differentiation, however, the
technique is not fully general because of a difficulty with branching code
(i.e., `if' statements). We make several remarks about the application of
computational divided differencing to optimization. One point is that the
technique can be used effectively as a stagnation test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4101</identifier>
 <datestamp>2013-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4101</id><created>2013-07-15</created><updated>2013-09-25</updated><authors><author><keyname>de Barros</keyname><forenames>J. Acacio</forenames></author></authors><title>Decision Making for Inconsistent Expert Judgments Using Negative
  Probabilities</title><categories>stat.OT cs.AI math.ST quant-ph stat.TH</categories><comments>14 pages, revised version to appear in the Proceedings of the QI2013
  (Quantum Interactions) conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a simple random-variable example of inconsistent
information, and analyze it using three different approaches: Bayesian,
quantum-like, and negative probabilities. We then show that, at least for this
particular example, both the Bayesian and the quantum-like approaches have less
normative power than the negative probabilities one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4102</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4102</id><created>2013-07-15</created><updated>2014-04-22</updated><authors><author><keyname>Meirom</keyname><forenames>Eli. A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author></authors><title>Network Formation Games with Heterogeneous Players and the Internet
  Structure</title><categories>cs.GT</categories><doi>10.1145/2600057.2602862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure and evolution of the Internet's Autonomous System (AS)
interconnection topology as a game with heterogeneous players. In this network
formation game, the utility of a player depends on the network structure, e.g.,
the distances between nodes and the cost of links. We analyze static properties
of the game, such as the prices of anarchy and stability and provide explicit
results concerning the generated topologies. Furthermore, we discuss dynamic
aspects, demonstrating linear convergence rate and showing that only a
restricted subset of equilibria is feasible under realistic dynamics. We also
consider the case where utility (or monetary) transfers are allowed between the
players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4107</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4107</id><created>2013-07-15</created><updated>2013-09-11</updated><authors><author><keyname>Pliam</keyname><forenames>John O.</forenames></author></authors><title>Alternating Product Ciphers: A Case for Provable Security Comparisons
  (extended abstract)</title><categories>cs.CR</categories><comments>12 pages, 3 figures, (v1) initial version (v2) Indocrypt 2013
  preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formally study iterated block ciphers that alternate between two sequences
of independent and identically distributed (i.i.d.) rounds. It is demonstrated
that, in some cases the effect of alternating increases security, while in
other cases the effect may strictly decrease security relative to the
corresponding product of one of its component sequences. As this would appear
to contradict conventional wisdom based on the ideal cipher approximation, we
introduce new machinery for provable security comparisons. The comparisons made
here simultaneously establish a coherent ordering of security metrics ranging
from key-recovery cost to computational indistinguishability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4124</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4124</id><created>2013-07-15</created><authors><author><keyname>Bilal</keyname><forenames>Sardar M.</forenames></author><author><keyname>Dilber</keyname><forenames>Muhammad Naveed</forenames></author><author><keyname>Khan</keyname><forenames>Atta ur Rehman</forenames></author></authors><title>Routing Proposals for Multipath Interdomain Routing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet is composed of numbers of independent autonomous systems. BGP is
used to disseminate reachability information and establishing path between
autonomous systems. Each autonomous system is allowed to select a single route
to a destination and then export the selected route to its neighbors. The
selection of single best route imposes restrictions on the use of alternative
paths during interdomain link failure and thus, incurred packet loss. Packet
loss still occurs even when multiple paths exist between source and destination
but these paths have not been utilized. To minimize the packet loss, when
multiple paths exist, multipath routing techniques are introduced. Multipath
routing techniques ensure the use of alternative paths on link failure. It
computes set of paths which can be used when primary path is not available and
it also provides a way to transit domains to have control over the traffic
flow. This can be achieved by little modification to current BGP. This paper
highlights different multipath routing techniques and also discusses the
overhead incurred by each of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4127</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4127</id><created>2013-07-15</created><authors><author><keyname>Khan</keyname><forenames>Atta ur Rehman</forenames></author><author><keyname>Ali</keyname><forenames>Shahzad</forenames></author><author><keyname>Mustafa</keyname><forenames>Saad</forenames></author><author><keyname>Othman</keyname><forenames>Mazliza</forenames></author></authors><title>Impact of mobility models on clustering based routing protocols in
  mobile WSNs</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents comparison of different hierarchical (position and
non-position based) protocols with respect to different mobility models.
Previous work mainly focuses on static networks or at most a single mobility
model. Using only one mobility model may not predict the behavior of routing
protocol accurately. Simulation results show that mobility has large impact on
the behavior of WSN routing protocols. Also, position based routing protocols
performs better in terms of packet delivery compared to non position based
routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4129</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4129</id><created>2013-07-15</created><authors><author><keyname>Khana</keyname><forenames>Atta ur Rehman</forenames></author><author><keyname>Bilalb</keyname><forenames>Sardar M.</forenames></author><author><keyname>Othmana</keyname><forenames>Mazliza</forenames></author></authors><title>A Performance Comparison of Network Simulators for Wireless Networks</title><categories>cs.NI cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network simulation is the most useful and common methodology used to evaluate
different network to-pologies without real world implementation. Network
simulators are widely used by the research community to evaluate new theories
and hypotheses. There are a number of network simulators, for instance, ns-2,
ns-3, OMNET++, SWAN, OPNET, Jist, and GloMoSiM etc. Therefore, the selection of
a network simulator for evaluating research work is a crucial task for
researchers. The main focus of this paper is to compare the state-of-the-art,
open source network simulators based on the following parameters: CPU
utilization, memory usage, computational time, and scalability by simulating a
MANET routing protocol, to identify an optimal network simulator for the
research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4143</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4143</id><created>2013-07-15</created><updated>2013-09-11</updated><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Misha</forenames></author></authors><title>Storage Sizing and Placement through Operational and Uncertainty-Aware
  Simulations</title><categories>math.OC cs.SY physics.soc-ph</categories><comments>To Appear in proceedings of Hawaii International Conference on System
  Sciences (HICSS-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the penetration level of transmission-scale time-intermittent renewable
generation resources increases, control of flexible resources will become
important to mitigating the fluctuations due to these new renewable resources.
Flexible resources may include new or existing synchronous generators as well
as new energy storage devices. Optimal placement and sizing of energy storage
to minimize costs of integrating renewable resources is a difficult
optimization problem. Further,optimal planning procedures typically do not
consider the effect of the time dependence of operations and may lead to
unsatisfactory results. Here, we use an optimal energy storage control
algorithm to develop a heuristic procedure for energy storage placement and
sizing. We perform operational simulation under various time profiles of
intermittent generation, loads and interchanges (artificially generated or from
historical data) and accumulate statistics of the usage of storage at each node
under the optimal dispatch. We develop a greedy heuristic based on the
accumulated statistics to obtain a minimal set of nodes for storage placement.
The quality of the heuristic is explored by comparing our results to the
obvious heuristic of placing storage at the renewables for IEEE benchmarks and
real-world network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4145</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4145</id><created>2013-07-15</created><updated>2013-07-18</updated><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Jiayu</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>A Safe Screening Rule for Sparse Logistic Regression</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The l1-regularized logistic regression (or sparse logistic regression) is a
widely used method for simultaneous classification and feature selection.
Although many recent efforts have been devoted to its efficient implementation,
its application to high dimensional data still poses significant challenges. In
this paper, we present a fast and effective sparse logistic regression
screening rule (Slores) to identify the 0 components in the solution vector,
which may lead to a substantial reduction in the number of features to be
entered to the optimization. An appealing feature of Slores is that the data
set needs to be scanned only once to run the screening and its computational
cost is negligible compared to that of solving the sparse logistic regression
problem. Moreover, Slores is independent of solvers for sparse logistic
regression, thus Slores can be integrated with any existing solver to improve
the efficiency. We have evaluated Slores using high-dimensional data sets from
different applications. Extensive experimental results demonstrate that Slores
outperforms the existing state-of-the-art screening rules and the efficiency of
solving sparse logistic regression is improved by one magnitude in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4146</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4146</id><created>2013-07-15</created><updated>2013-07-18</updated><authors><author><keyname>He</keyname><forenames>Biao</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara D.</forenames></author></authors><title>Wireless Physical Layer Security with Imperfect Channel State
  Information: A Survey</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer security is an emerging technique to improve the wireless
communication security, which is widely regarded as a complement to
cryptographic technologies. To design physical layer security techniques under
practical scenarios, the uncertainty and imperfections in the channel knowledge
need to be taken into consideration. This paper provides a survey of recent
research and development in physical layer security considering the imperfect
channel state information (CSI) at communication nodes. We first present an
overview of the main information-theoretic measures of the secrecy performance
with imperfect CSI. Then, we describe several signal processing enhancements in
secure transmission designs, such as secure on-off transmission, beamforming
with artificial noise, and secure communication assisted by relay nodes or in
cognitive radio systems. The recent studies of physical layer security in
large-scale decentralized wireless networks are also summarized. Finally, the
open problems for the on-going and future research are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4149</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4149</id><created>2013-07-15</created><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Self-Interference Cancellation with Phase Noise Induced ICI Suppression
  for Full-Duplex Systems</title><categories>cs.IT math.IT</categories><comments>To be presented in Global Telecommunications Conference (GLOBECOM
  2013). arXiv admin note: text overlap with arXiv:1307.3796</comments><doi>10.1109/GLOCOM.2013.6831595</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main bottlenecks in practical full-duplex systems is the
oscillator phase noise, which bounds the possible cancellable self-interference
power. In this paper, a digitaldomain self-interference cancellation scheme for
full-duplex orthogonal frequency division multiplexing systems is proposed. The
proposed scheme increases the amount of cancellable selfinterference power by
suppressing the effect of both transmitter and receiver oscillator phase noise.
The proposed scheme consists of two main phases, an estimation phase and a
cancellation phase. In the estimation phase, the minimum mean square error
estimator is used to jointly estimate the transmitter and receiver phase noise
associated with the incoming self-interference signal. In the cancellation
phase, the estimated phase noise is used to suppress the intercarrier
interference caused by the phase noise associated with the incoming
self-interference signal. The performance of the proposed scheme is numerically
investigated under different operating conditions. It is demonstrated that the
proposed scheme could achieve up to 9dB more self-interference cancellation
than the existing digital-domain cancellation schemes that ignore the
intercarrier interference suppression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4150</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4150</id><created>2013-07-15</created><updated>2013-07-19</updated><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Huang</keyname><forenames>Cheng</forenames></author><author><keyname>Jenkins</keyname><forenames>Bob</forenames></author><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author></authors><title>Explicit Maximally Recoverable Codes with Locality</title><categories>cs.IT math.IT</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a systematic linear code where some (local) parity symbols depend on
few prescribed symbols, while other (heavy) parity symbols may depend on all
data symbols. Local parities allow to quickly recover any single symbol when it
is erased, while heavy parities provide tolerance to a large number of
simultaneous erasures. A code as above is maximally-recoverable if it corrects
all erasure patterns which are information theoretically recoverable given the
code topology. In this paper we present explicit families of
maximally-recoverable codes with locality. We also initiate the study of the
trade-off between maximal recoverability and alphabet size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4156</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4156</id><created>2013-07-15</created><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Efficient Mixed-Norm Regularization: Algorithms and Safe Screening
  Methods</title><categories>cs.LG stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1009.4766</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse learning has recently received increasing attention in many areas
including machine learning, statistics, and applied mathematics. The mixed-norm
regularization based on the l1q norm with q&gt;1 is attractive in many
applications of regression and classification in that it facilitates group
sparsity in the model. The resulting optimization problem is, however,
challenging to solve due to the inherent structure of the mixed-norm
regularization. Existing work deals with special cases with q=1, 2, infinity,
and they cannot be easily extended to the general case. In this paper, we
propose an efficient algorithm based on the accelerated gradient method for
solving the general l1q-regularized problem. One key building block of the
proposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Our
theoretical analysis reveals the key properties of EP_1q and illustrates why
EP_1q for the general q is significantly more challenging to solve than the
special cases. Based on our theoretical analysis, we develop an efficient
algorithm for EP_1q by solving two zero finding problems. To further improve
the efficiency of solving large dimensional mixed-norm regularized problems, we
propose a screening method which is able to quickly identify the inactive
groups, i.e., groups that have 0 components in the solution. This may lead to
substantial reduction in the number of groups to be entered to the
optimization. An appealing feature of our screening method is that the data set
needs to be scanned only once to run the screening. Compared to that of solving
the mixed-norm regularized problems, the computational cost of our screening
test is negligible. The key of the proposed screening method is an accurate
sensitivity analysis of the dual optimal solution when the regularization
parameter varies. Experimental results demonstrate the efficiency of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4162</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4162</id><created>2013-07-16</created><authors><author><keyname>Puppis</keyname><forenames>Gabriele</forenames></author><author><keyname>Villa</keyname><forenames>Tiziano</forenames></author></authors><title>Proceedings Fourth International Symposium on Games, Automata, Logics
  and Formal Verification</title><categories>cs.GT cs.FL cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013</journal-ref><doi>10.4204/EPTCS.119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Fourth International Symposium on
Games, Automata, Logic and Formal Verification (GandALF 2013). The symposium
took place in Borca di Cadore, Italy, from 29th to 31st of August 2013.
  The proceedings of the symposium contain the abstracts of three invited talks
and 17 papers that were accepted after a careful evaluation for presentation at
the conference. The topics of the accepted papers range over a wide spectrum,
including algorithmic and behavioral game theory, game semantics, formal
languages and automata theory, modal and temporal logics, software
verification, hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4164</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4164</id><created>2013-07-16</created><authors><author><keyname>Singh</keyname><forenames>Mohit</forenames></author><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author></authors><title>Approximating Minimum Cost Connectivity Orientation and Augmentation</title><categories>cs.DS cs.DM math.CO</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectivity augmentation and orientation are two fundamental classes of
problems related to graph connectivity. The former includes k-edge-connected
subgraph and more generally, survivable network design problem. In orientation
problems the goal is to find orientations of an undirected graph that achieve
prescribed connectivity properties such as global and rooted k-edge
connectivity. In this paper, we consider network design problems that address
combined augmentation and orientation settings.
  We give a polynomial time 6-approximation algorithm for finding a minimum
cost subgraph of an undirected graph G that admits an orientation covering a
nonnegative crossing G-supermodular demand function, as defined by Frank. The
most important example is (k,l)-edge-connectivity, a common generalization of
global and rooted edge-connectivity. Our algorithm is based on the iterative
rounding method though the application is not standard. First, we observe that
the standard linear program is not amenable to the iterative rounding method
and therefore use an alternative LP with partition and co-partition
constraints. To apply the iterative rounding framework, we first need to
introduce new uncrossing techniques to obtain a simple family of constraints
that characterize basic feasible solutions. Then we do a careful counting
argument based on this family of constraints.
  We also consider the directed network design problem with orientation
constraints where we are given an undirected graph G=(V,E) with costs c(u,v)
and c(v,u) for each edge (u,v) in E and an integer k. The goal is to find a
subgraph F of minimum cost which has an k-edge connected orientation A. Khanna
et al showed that the integrality gap of the natural linear program is at most
4 when k=1 and conjectured that it is constant for all k. We disprove this
conjecture by showing an O(|V|)-integrality gap even when k=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4165</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4165</id><created>2013-07-16</created><authors><author><keyname>Goel</keyname><forenames>Neetu</forenames></author><author><keyname>Garg</keyname><forenames>R. B.</forenames></author></authors><title>A Comparative Study of CPU Scheduling Algorithms</title><categories>cs.OS</categories><journal-ref>International Journal of Graphics &amp; Image Processing |Vol 2|issue
  4|November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing CPU scheduling algorithms and understanding their impact in
practice can be difficult and time consuming due to the need to modify and test
operating system kernel code and measure the resulting performance on a
consistent workload of real applications. As processor is the important
resource, CPU scheduling becomes very important in accomplishing the operating
system (OS) design goals. The intention should be allowed as many as possible
running processes at all time in order to make best use of CPU. This paper
presents a state diagram that depicts the comparative study of various
scheduling algorithms for a single CPU and shows which algorithm is best for
the particular situation. Using this representation, it becomes much easier to
understand what is going on inside the system and why a different set of
processes is a candidate for the allocation of the CPU at different time. The
objective of the study is to analyze the high efficient CPU scheduler on design
of the high quality scheduling algorithms which suits the scheduling goals. Key
Words:-Scheduler, State Diagrams, CPU-Scheduling, Performance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4167</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4167</id><created>2013-07-16</created><authors><author><keyname>Goel</keyname><forenames>Neetu</forenames></author><author><keyname>Garg</keyname><forenames>R. B.</forenames></author></authors><title>An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm</title><categories>cs.OS</categories><journal-ref>Published in National Conference on Information Communication &amp;
  Networks, Dated: April 6,2013 organized by Tecnia Institute of Advanced
  Studies</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to improve the Round Robin scheduling
algorithm using the dynamic time slice concept. CPU scheduling becomes very
important in accomplishing the operating system (OS) design goals. The
intention should be allowed as many as possible running processes at all time
in order to make best use of CPU. CPU scheduling has strong effect on resource
utilization as well as overall performance of the system. Round Robin algorithm
performs optimally in time-shared systems, but it is not suitable for soft real
time systems, because it gives more number of context switches, larger waiting
time and larger response time. In this paper, a new CPU scheduling algorithm
called An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm is
proposed, which calculates intelligent time slice and changes after every round
of execution. The suggested algorithm was evaluated on some CPU scheduling
objectives and it was observed that this algorithm gave good performance as
compared to the other existing CPU scheduling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4171</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4171</id><created>2013-07-16</created><authors><author><keyname>Shriram</keyname><forenames>Revati</forenames></author><author><keyname>Sundhararajan</keyname><forenames>M.</forenames></author><author><keyname>Daimiwal</keyname><forenames>Nivedita</forenames></author></authors><title>Human Brain Mapping based on COLD Signal Hemodynamic Response and
  Electrical Neuroimaging</title><categories>cs.ET q-bio.NC</categories><comments>5 pages. arXiv admin note: substantial text overlap with
  arXiv:1212.3786</comments><doi>10.5120/10464-5175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand Working of Human Brain measurements related to the brain
function are required. These measurements should be possibly non-invasive.
Brain should be disturbed as less as possible during the measurement.
Integration of various modalities plays a vital role in understanding the
cognitive and the behavioral changes in the human brain. It is an important
source of converging evidence about specific aspects of neural functions and
dysfunctions under certain pathological conditions. Focal changes in cortical
blood flow are tightly coupled with the changes in neuronal activity. This
constitutes the option to map the hemodynamic response and infer principles of
the cortical processing, even of complex tasks. The very high temporal
resolution of EEG and good spatial resolution by NIRS make this concurrent
measurement unique to study the spatio-temporal dynamics of large scale
neuronal networks in the human brain. Such integration of two techniques will
help to overcome the limitations of a specific method. Such as insensitivity of
electroencephalogram (EEG) to unsynchronized neural events or lack of near
infrared spectroscopy (NIRS) to low metabolic demand. A combination of EEG and
NIRS will be more informative than the two separate analyses in both
modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4174</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4174</id><created>2013-07-16</created><authors><author><keyname>Siddiqui</keyname><forenames>Farheen</forenames></author><author><keyname>Alam</keyname><forenames>M. Afshar</forenames></author></authors><title>Ontology Based Feature Driven Development Life Cycle</title><categories>cs.SE</categories><comments>6 papers, IJCSI International Journal of Computer Science Issues,
  Vol. 9, Issue 1, No 2, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The upcoming technology support for semantic web promises fresh directions
for Software Engineering community. Also semantic web has its roots in
knowledge engineering that provoke software engineers to look for application
of ontology applications throughout the Software Engineering life cycle. The
internal components of a semantic web are light weight and may be of less
quality standards than the externally visible modules. In fact the internal
components are generated from external (ontological) component. That is the
reason agile development approaches such as feature driven development are
suitable for applications internal component development. As yet there is no
particular procedure that describes the role of ontology in the processes.
Therefore we propose an ontology based feature driven development for semantic
web application that can be used form application model development to feature
design and implementation. Features are precisely defined in the OWL-based
domain model. Transition from OWL based domain model to feature list is
directly defined in transformation rules. On the other hand the ontology based
overall model can be easily validated through automated tools. Advantages of
ontology-based feature Driven development are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4186</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4186</id><created>2013-07-16</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author><author><keyname>Fister</keyname><forenames>Du&#x161;an</forenames></author></authors><title>A Brief Review of Nature-Inspired Algorithms for Optimization</title><categories>cs.NE</categories><journal-ref>Elektrotehni\v{s}ki vestnik, 80(3), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm intelligence and bio-inspired algorithms form a hot topic in the
developments of new algorithms inspired by nature. These nature-inspired
metaheuristic algorithms can be based on swarm intelligence, biological
systems, physical and chemical systems. Therefore, these algorithms can be
called swarm-intelligence-based, bio-inspired, physics-based and
chemistry-based, depending on the sources of inspiration. Though not all of
them are efficient, a few algorithms have proved to be very efficient and thus
have become popular tools for solving real-world problems. Some algorithms are
insufficiently studied. The purpose of this review is to present a relatively
comprehensive list of all the algorithms in the literature, so as to inspire
further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4191</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4191</id><created>2013-07-16</created><authors><author><keyname>Fulek</keyname><forenames>Radoslav</forenames></author></authors><title>Estimating the number of disjoint edges in simple topological graphs via
  cylindrical drawings</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A topological graph drawn on a cylinder whose base is horizontal is
\emph{angularly monotone} if every vertical line intersects every edge at most
once. Let $c(n)$ denote the maximum number $c$ such that every simple angularly
monotone drawing of a complete graph on $n$ vertices contains at least $c$
pairwise disjoint edges. We show that for every simple complete topological
graph $G$ there exists $\Delta$, $0&lt;\Delta&lt;n$, such that $G$ contains at least
$\max \{\frac n\Delta, c(\Delta)\}$ pairwise disjoint edges. By combining our
result with a result of T\'oth we obtain an alternative proof for the best
known lower bound of $\Omega(n^\frac 13)$ on the maximum number of pairwise
disjoint edges in a simple complete topological graph proved by Suk. Our proof
is based on a result of Ruiz-Vargas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4192</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4192</id><created>2013-07-16</created><updated>2014-01-31</updated><authors><author><keyname>&#x160;kraba</keyname><forenames>Primo&#x17e;</forenames></author><author><keyname>Costa</keyname><forenames>Jo&#xe3;o Pita</forenames></author></authors><title>A Lattice for Persistence</title><categories>math.RA cs.CG math.AT</categories><comments>20 pages + appendix</comments><msc-class>55N35, 06D20, 13P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intrinsic connection between lattice theory and topology is fairly well
established, For instance, the collection of open subsets of a topological
subspace always forms a distributive lattice. Persistent homology has been one
of the most prominent areas of research in computational topology in the past
20 years. In this paper we will introduce an alternative interpretation of
persistence based on the study of the order structure of its correspondent
lattice. Its algorithmic construction leads to two operations on homology
groups which describe a diagram of spaces as a complete Heyting algebra, which
is a generalization of a Boolean algebra. We investigate some of the properties
of this lattice, the algorithmic implications of it, and some possible
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4207</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4207</id><created>2013-07-16</created><updated>2015-02-25</updated><authors><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Branching-Time Model Checking Gap-Order Constraint Systems (Extended
  Version)</title><categories>cs.LO</categories><msc-class>03B70, 68Q42</msc-class><acm-class>D.2.4; F.1.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the model checking problem for Gap-order Constraint Systems (GCS)
w.r.t. the branching-time temporal logic CTL, and in particular its fragments
EG and EF.
  GCS are nondeterministic infinitely branching processes described by
evolutions of integer-valued variables, subject to Presburger constraints of
the form $x-y\ge k$, where $x$ and $y$ are variables or constants and
$k\in\mathbb{N}$ is a non-negative constant.
  We show that EG model checking is undecidable for GCS, while EF is decidable.
In particular, this implies the decidability of strong and weak bisimulation
equivalence between GCS and finite-state systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4209</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4209</id><created>2013-07-16</created><updated>2013-12-17</updated><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author></authors><title>Robust periodic stability implies uniform exponential stability of
  Markovian jump linear systems and random linear ordinary differential
  equations</title><categories>math.DS cs.SY math.OC</categories><comments>27 pages; submitted</comments><msc-class>37H05, 34D23, 93D09, 93C30, 15A18, 37N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that if a linear cocycle is robustly periodical stable
then it is uniformly stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4214</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4214</id><created>2013-07-16</created><authors><author><keyname>Apian-Bennewitz</keyname><forenames>Peter</forenames></author></authors><title>Review of simulating four classes of window materials for daylighting
  with non-standard BSDF using the simulation program Radiance</title><categories>physics.comp-ph cs.CE cs.GR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This review describes the currently available simulation models for window
material to calculate daylighting with the program &quot;Radiance&quot;. The review is
based on four abstract and general classes of window materials, depending on
their scattering and redirecting properties (bidirectional scatter distribution
function, BSDF). It lists potential and limits of the older models and includes
the most recent additions to the software. All models are demonstrated using an
exemplary indoor scene and two typical sky conditions. It is intended as
clarification for applying window material models in project work or teaching.
The underlying algorithmic problems apply to all lighting simulation programs,
so the scenarios of materials and skies are applicable to other lighting
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4215</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4215</id><created>2013-07-16</created><authors><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Marks</keyname><forenames>Trevor</forenames></author></authors><title>Design of a small-scale prototype for research in airborne wind energy</title><categories>cs.SY math.OC</categories><comments>This manuscript is a preprint of a paper submitted for possible
  publication on the IEEE/ASME Transactions on Mechatronics and is subject to
  IEEE Copyright. If accepted, the copy of record will be available at
  IEEEXplore library: http://ieeexplore.ieee.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airborne wind energy is a new renewable technology that promises to deliver
electricity at low costs and in large quantities. Despite the steadily growing
interest in this field, very limited results with real-world data have been
reported so far, due to the difficulty faced by researchers when realizing an
experimental setup. Indeed airborne wind energy prototypes are mechatronic
devices involving many multidisciplinary aspects, for which there are currently
no established design guidelines. With the aim of making research in airborne
wind energy accessible to a larger number of researchers, this work provides
such guidelines for a small-scale prototype. The considered system has no
energy generation capabilities, but it can be realized at low costs, used with
little restrictions and it allows one to test many aspects of the technology,
from sensors to actuators to wing design and materials. In addition to the
guidelines, the paper provides the details of the design and costs of an
experimental setup realized at the University of California, Santa Barbara, and
successfully used to develop and test sensor fusion and automatic control
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4221</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4221</id><created>2013-07-16</created><authors><author><keyname>Baisakh</keyname></author><author><keyname>Patel</keyname><forenames>Nileshkumar R.</forenames></author></authors><title>Energy Saving and Survival Routing Protocol for Mobile Ad Hoc Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.3548</comments><journal-ref>International Journal of Computer Applications, Vol. 48, No. 2,
  June 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method to enhance the life time as well as improve
the performance of the mobile ad hoc networks (MANET). Since MANET consists of
devices that run on batteries, having limited amount of energy and due to the
self-configuring and dynamic change of topology, all operations are performed
by the node itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4228</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4228</id><created>2013-07-16</created><updated>2013-09-17</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>A Model of Human Cooperation in Social Dilemmas</title><categories>cs.GT</categories><journal-ref>Capraro V (2013) A Model of Human Cooperation in Social Dilemmas.
  PLoS ONE 8(8): e72427</journal-ref><doi>10.1371/journal.pone.0072427</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social dilemmas are situations in which collective interests are at odds with
private interests: pollution, depletion of natural resources, and intergroup
conflicts, are at their core social dilemmas.
  Because of their multidisciplinarity and their importance, social dilemmas
have been studied by economists, biologists, psychologists, sociologists, and
political scientists. These studies typically explain tendency to cooperation
by dividing people in proself and prosocial types, or appealing to forms of
external control or, in iterated social dilemmas, to long-term strategies.
  But recent experiments have shown that cooperation is possible even in
one-shot social dilemmas without forms of external control and the rate of
cooperation typically depends on the payoffs. This makes impossible a
predictive division between proself and prosocial people and proves that people
have attitude to cooperation by nature.
  The key innovation of this article is in fact to postulate that humans have
attitude to cooperation by nature and consequently they do not act a priori as
single agents, as assumed by standard economic models, but they forecast how a
social dilemma would evolve if they formed coalitions and then they act
according to their most optimistic forecast. Formalizing this idea we propose
the first predictive model of human cooperation able to organize a number of
different experimental findings that are not explained by the standard model.
We show also that the model makes satisfactorily accurate quantitative
predictions of population average behavior in one-shot social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4230</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4230</id><created>2013-07-16</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>The State of Information and Communication Technology in Hungary, A
  Comparative Analysis</title><categories>cs.CY</categories><comments>6 pages</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><journal-ref>Informatica 35 (2011) 239-244</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel comparative research and analysis method is proposed and applied on
the Hungarian economic sectors. The question of what factors have an effect on
their net income is essential for enterprises. First, the potential indicators
related to economic sectors were studied and then compared to the net income of
the surveyed enterprises. The data resulting from the comparison showed that
the growing penetration of electronic marketpalces contributed to the change of
the net income of enterprises in various economic sectors to the extent of 37%.
Among all the potential indicators, only the indicator of electronic
marketplaces has a direct influence on the net income of enterprises. Two
clusters based on the potential indicators were indicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4257</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4257</id><created>2013-07-16</created><authors><author><keyname>Adamaszek</keyname><forenames>Anna</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>A QPTAS for Maximum Weight Independent Set of Polygons with
  Polylogarithmically Many Vertices</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Weight Independent Set of Polygons problem is a fundamental
problem in computational geometry. Given a set of weighted polygons in the
2-dimensional plane, the goal is to find a set of pairwise non-overlapping
polygons with maximum total weight. Due to its wide range of applications, the
MWISP problem and its special cases have been extensively studied both in the
approximation algorithms and the computational geometry community. Despite a
lot of research, its general case is not well-understood. Currently the best
known polynomial time algorithm achieves an approximation ratio of n^(epsilon)
[Fox and Pach, SODA 2011], and it is not even clear whether the problem is
APX-hard. We present a (1+epsilon)-approximation algorithm, assuming that each
polygon in the input has at most a polylogarithmic number of vertices. Our
algorithm has quasi-polynomial running time.
  We use a recently introduced framework for approximating maximum weight
independent set in geometric intersection graphs. The framework has been used
to construct a QPTAS in the much simpler case of axis-parallel rectangles. We
extend it in two ways, to adapt it to our much more general setting. First, we
show that its technical core can be reduced to the case when all input polygons
are triangles. Secondly, we replace its key technical ingredient which is a
method to partition the plane using only few edges such that the objects
stemming from the optimal solution are evenly distributed among the resulting
faces and each object is intersected only a few times. Our new procedure for
this task is not more complex than the original one, and it can handle the
arising difficulties due to the arbitrary angles of the polygons. Note that
already this obstacle makes the known analysis for the above framework fail.
Also, in general it is not well understood how to handle this difficulty by
efficient approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4258</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4258</id><created>2013-07-16</created><updated>2013-11-12</updated><authors><author><keyname>Gairing</keyname><forenames>Martin</forenames></author><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>Klimm</keyname><forenames>Max</forenames></author></authors><title>Complexity and Approximation of the Continuous Network Design Problem</title><categories>cs.GT cs.DS math.OC</categories><comments>27 pages</comments><msc-class>90-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a classical problem in transportation, known as the continuous
(bilevel) network design problem, CNDP for short. We are given a graph for
which the latency of each edge depends on the ratio of the edge flow and the
capacity installed. The goal is to find an optimal investment in edge
capacities so as to minimize the sum of the routing cost of the induced Wardrop
equilibrium and the investment cost. While this problem is considered as
challenging in the literature, its complexity status was still unknown. We
close this gap showing that CNDP is strongly NP-complete and APX-hard, both on
directed and undirected networks and even for instances with affine latencies.
  As for the approximation of the problem, we first provide a detailed analysis
for a heuristic studied by Marcotte for the special case of monomial latency
functions (Mathematical Programming, Vol.~34, 1986). Specifically, we derive a
closed form expression of its approximation guarantee for arbitrary sets S of
allowed latency functions. Second, we propose a different approximation
algorithm and show that it has the same approximation guarantee. As our final
-- and arguably most interesting -- result regarding approximation, we show
that using the better of the two approximation algorithms results in a strictly
improved approximation guarantee for which we give a closed form expression.
For affine latencies, e.g., this algorithm achieves a 1.195-approximation which
improves on the 5/4 that has been shown before by Marcotte. We finally discuss
the case of hard budget constraints on the capacity investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4259</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4259</id><created>2013-07-16</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Gmyr</keyname><forenames>Robert</forenames></author><author><keyname>Richa</keyname><forenames>Andrea W.</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author></authors><title>Ameba-inspired Self-organizing Particle Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle systems are physical systems of simple computational particles that
can bond to neighboring particles and use these bonds to move from one spot to
another (non-occupied) spot. These particle systems are supposed to be able to
self-organize in order to adapt to a desired shape without any central control.
Self-organizing particle systems have many interesting applications like
coating objects for monitoring and repair purposes and the formation of
nano-scale devices for surgery and molecular-scale electronic structures. While
there has been quite a lot of systems work in this area, especially in the
context of modular self-reconfigurable robotic systems, only very little
theoretical work has been done in this area so far. We attempt to bridge this
gap by proposing a model inspired by the behavior of ameba that allows rigorous
algorithmic research on self-organizing particle systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4264</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4264</id><created>2013-07-16</created><authors><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author></authors><title>A Data-driven Study of Influences in Twitter Communities</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a quantitative study of Twitter, one of the most popular
micro-blogging services, from the perspective of user influence. We crawl
several datasets from the most active communities on Twitter and obtain 20.5
million user profiles, along with 420.2 million directed relations and 105
million tweets among the users. User influence scores are obtained from
influence measurement services, Klout and PeerIndex. Our analysis reveals
interesting findings, including non-power-law influence distribution, strong
reciprocity among users in a community, the existence of homophily and
hierarchical relationships in social influences. Most importantly, we observe
that whether a user retweets a message is strongly influenced by the first of
his followees who posted that message. To capture such an effect, we propose
the first influencer (FI) information diffusion model and show through
extensive evaluation that compared to the widely adopted independent cascade
model, the FI model is more stable and more accurate in predicting influence
spreads in Twitter communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4274</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4274</id><created>2013-07-16</created><authors><author><keyname>Witt</keyname><forenames>Carsten</forenames></author></authors><title>The Fitness Level Method with Tail Bounds</title><categories>cs.NE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fitness-level method, also called the method of f-based partitions, is an
intuitive and widely used technique for the running time analysis of randomized
search heuristics. It was originally defined to prove upper and lower bounds on
the expected running time. Recently, upper tail bounds were added to the
technique; however, these tail bounds only apply to running times that are at
least twice as large as the expectation.
  We remove this restriction and supplement the fitness-level method with sharp
tail bounds, including lower tails. As an exemplary application, we prove that
the running time of randomized local search on OneMax is sharply concentrated
around n ln n - 0.1159 n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4279</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4279</id><created>2013-07-16</created><updated>2014-01-01</updated><authors><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author></authors><title>Cryptanalyzing a RGB image encryption algorithm based on DNA encoding
  and chaos map</title><categories>cs.CR</categories><doi>10.1016/j.optlastec.2014.01.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a RGB image encryption algorithm based on DNA encoding and chaos
map has been proposed. It was reported that the encryption algorithm can be
broken with four pairs of chosen plain-images and the corresponding
cipher-images. This paper re-evaluates the security of the encryption
algorithm, and finds that the encryption algorithm can be broken efficiently
with only one known plain-image. The effectiveness of the proposed
known-plaintext attack is supported by both rigorous theoretical analysis and
experimental results. In addition, two other security defects are also
reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4289</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4289</id><created>2013-07-16</created><updated>2014-09-19</updated><authors><author><keyname>Sitters</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Polynomial time approximation schemes for the traveling repairman and
  other minimum latency problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time, $(1+\epsilon)$-approximation algorithm for the
traveling repairman problem (TRP) in the Euclidean plane and on weighted trees.
This improves on the known quasi-polynomial time approximation schemes for
these problems. The algorithm is based on a simple technique that reduces the
TRP to what we call the \emph{segmented TSP}. Here, we are given numbers
$l_1,\dots,l_K$ and $n_1,\dots,n_K$ and we need to find a path that visits at
least $n_h$ points within path distance $l_h$ from the starting point for all
$h\in\{1,\dots,K\}$. A solution is $\alpha$-approximate if at least $n_h$
points are visited within distance $\alpha l_h$. It is shown that any algorithm
that is $\alpha$-approximate for \emph{every constant} $K$ in some metric
space, gives an $\alpha(1+\epsilon)$-approximation for the TRP in the same
metric space. Subsequently, approximation schemes are given for this segmented
TSP problem in the plane and on weighted trees. The segmented TSP with only one
segment ($K=1$) is equivalent to the $k$-TSP for which a
$(2+\epsilon)$-approximation is known for a general metric space. Hence, this
approach through the segmented TSP gives new impulse for improving on the
3.59-approximation for TRP in a general metric space. A similar reduction
applies to many other minimum latency problems. To illustrate the strength of
this approach we apply it to the well-studied scheduling problem of minimizing
total weighted completion time under precedence constraints, $1|prec|\sum
w_{j}C_{j}$, and present a polynomial time approximation scheme for the case of
interval order precedence constraints. This improves on the known
$3/2$-approximation for this problem. Both approximation schemes apply as well
if release dates are added to the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4292</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4292</id><created>2013-07-16</created><authors><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author></authors><title>Influence of media on collective debates</title><categories>physics.soc-ph cs.CY cs.SI physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information system (T.V., newspapers, blogs, social network platforms)
and its inner dynamics play a fundamental role on the evolution of collective
debates and thus on the public opinion. In this work we address such a process
focusing on how the current inner strategies of the information system
(competition, customer satisfaction) once combined with the gossip may affect
the opinions dynamics. A reinforcement effect is particularly evident in the
social network platforms where several and incompatible cultures coexist (e.g,
pro or against the existence of chemical trails and reptilians, the new world
order conspiracy and so forth). We introduce a computational model of opinion
dynamics which accounts for the coexistence of media and gossip as separated
but interdependent mechanisms influencing the opinions evolution. Individuals
may change their opinions under the contemporary pressure of the information
supplied by the media and the opinions of their social contacts. We stress the
effect of the media communication patterns by considering both the simple case
where each medium mimics the behavior of the most successful one (in order to
maximize the audience) and the case where there is polarization and thus
competition among media reported information (in order to preserve and satisfy
their segmented audience). Finally, we first model the information cycle as in
the case of traditional main stream media (i.e, when every medium knows about
the format of all the others) and then, to account for the effect of the
Internet, on more complex connectivity patterns (as in the case of the web
based information). We show that multiple and polarized information sources
lead to stable configurations where several and distant opinions coexist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4296</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4296</id><created>2013-07-15</created><updated>2013-12-13</updated><authors><author><keyname>Sinha</keyname><forenames>Shriprakash</forenames></author><author><keyname>Reinders</keyname><forenames>Marcel J. T.</forenames></author><author><keyname>Verhaegh</keyname><forenames>Wim</forenames></author></authors><title>Prior Biological Knowledge And Epigenetic Information Enhances
  Prediction Accuracy Of Bayesian Wnt Pathway</title><categories>q-bio.MN cs.CE</categories><comments>This paper has been withdrawn by the owner because it was submitted
  without consent of the co-authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational modeling of Wnt signaling pathway has gained prominence for its
use as computer aided diagnostic tool to develop therapeutic cancer target
drugs and predict of test samples as cancerous and non cancerous. This
manuscript focuses on development of simple static bayesian network models of
varying complexity that encompasses prior partially available biological
knowledge about intra and extra cellular factors affecting the Wnt pathway and
incorporates epigenetic information like methylation and histone modification
of a few genes known to have inhibitory affect on Wnt pathway. It might be
expected that such models not only increase cancer prediction accuracies and
also form basis for understanding Wnt signaling activity in different states of
tumorigenesis. Initial results in human colorectal cancer cases indicate that
incorporation of epigenetic information increases prediction accuracy of test
samples as being tumorous or normal. Receiver Operator Curves (ROC) and their
respective area under the curve (AUC) measurements, obtained from predictions
of state of test sample and corresponding predictions of the state of
activation of transcription complex of the Wnt pathway for the test sample,
indicate that there is significant difference between the Wnt pathway being on
(off) and its association with the sample being tumorous (normal). Two sample
Kolmogorov-Smirnov test confirm the statistical deviation between the
distributions of these predictions. At a preliminary stage, use of these models
may help in understanding the yet unknown effect of certain factors like DKK2,
DKK3-1 and SFRP-2/3/5 on {\beta}-catenin transcription complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4299</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4299</id><created>2013-07-15</created><authors><author><keyname>Singh</keyname><forenames>Jyoti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Part of Speech Tagging of Marathi Text Using Trigram Method</title><categories>cs.CL</categories><comments>International Journal of Advanced Information Technology (IJAIT) Vol.
  3, No.2, April2013</comments><doi>10.5121/ijait.2013.3203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a Marathi part of speech tagger. It is a
morphologically rich language. It is spoken by the native people of
Maharashtra. The general approach used for development of tagger is statistical
using trigram Method. The main concept of trigram is to explore the most likely
POS for a token based on given information of previous two tags by calculating
probabilities to determine which is the best sequence of a tag. In this paper
we show the development of the tagger. Moreover we have also shown the
evaluation done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4300</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4300</id><created>2013-07-15</created><authors><author><keyname>Bhalla</keyname><forenames>Deepti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Rule Based Transliteration Scheme for English to Punjabi</title><categories>cs.CL</categories><comments>International Journal on Natural Language Computing (IJNLC) Vol. 2,
  No.2, April 2013</comments><doi>10.5121/ijnlc.2013.2207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Transliteration has come out to be an emerging and a very important
research area in the field of machine translation. Transliteration basically
aims to preserve the phonological structure of words. Proper transliteration of
name entities plays a very significant role in improving the quality of machine
translation. In this paper we are doing machine transliteration for
English-Punjabi language pair using rule based approach. We have constructed
some rules for syllabification. Syllabification is the process to extract or
separate the syllable from the words. In this we are calculating the
probabilities for name entities (Proper names and location). For those words
which do not come under the category of name entities, separate probabilities
are being calculated by using relative frequency through a statistical machine
translation toolkit known as MOSES. Using these probabilities we are
transliterating our input text from English to Punjabi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4302</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4302</id><created>2013-07-15</created><authors><author><keyname>Kvasov</keyname><forenames>Dmitri E.</forenames></author><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Lipschitz gradients for global optimization in a one-point-based
  partitioning scheme</title><categories>math.OC cs.MS cs.NA math.NA</categories><comments>25 pages, 4 figures, 5 tables. arXiv admin note: text overlap with
  arXiv:1103.2056</comments><msc-class>65K05, 90C26, 90C56</msc-class><journal-ref>Journal of Computational and Applied Mathematics Volume 236, Issue
  16, October 2012, Pages 4042-4054</journal-ref><doi>10.1016/j.cam.2012.02.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A global optimization problem is studied where the objective function $f(x)$
is a multidimensional black-box function and its gradient $f'(x)$ satisfies the
Lipschitz condition over a hyperinterval with an unknown Lipschitz constant
$K$. Different methods for solving this problem by using an a priori given
estimate of $K$, its adaptive estimates, and adaptive estimates of local
Lipschitz constants are known in the literature. Recently, the authors have
proposed a one-dimensional algorithm working with multiple estimates of the
Lipschitz constant for $f'(x)$ (the existence of such an algorithm was a
challenge for 15 years). In this paper, a new multidimensional geometric method
evolving the ideas of this one-dimensional scheme and using an efficient
one-point-based partitioning strategy is proposed. Numerical experiments
executed on 800 multidimensional test functions demonstrate quite a promising
performance in comparison with popular DIRECT-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4308</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4308</id><created>2013-07-16</created><updated>2013-09-08</updated><authors><author><keyname>Fukuyama</keyname><forenames>Junichiro</forenames></author></authors><title>An Alternative Proof of the Exponential Monotone Complexity of the
  Clique Function</title><categories>cs.CC math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1305.3218</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1985, Razborov discovered a proof that the monotone circuit complexity of
the clique problem is super-polynomial. Alon and Boppana improved the result
into exponential lower bound exp(\Omega(n / \log n)^{1/3})) of a monotone
circuit C to compute cliques of size (1/4) (n / log n)^{2/3}, where n is the
number of vertices in a graph. Both proofs are based on the method of
approximations and Erdos and Rado's sunflower lemma. There has been an interest
in further generalization of the proof scheme.
  In this paper, we present a new approach to show the exponential monotone
complexity. Unlike the standard method, it dynamically constructs a counter
example: Assuming a monotone circuit C of sub-exponential size to compute
k-cliques c, an algorithm finds an edge set t containing no c in the
disjunctive normal form constructed at the root of C. We call such t a shift.
The proof shows that t is disjoint from an edge set z whose removal leaves no
k-cliques.
  We explore the set theoretical nature of computation by Boolean circuits. We
develop a theory by finding topological properties of the Hamming space 2^{[n]}
where [n]={1, 2, ..., n}. A structural theorem is presented, which is closely
related to the sunflower lemma and claims a stronger statement in most cases.
The theory lays the foundation of the above shift method. It also shows the
existence of a sunflower with small core in a family of sets, which is not an
obvious consequence of the sunflower lemma.
  Lastly, we point out that the new methodology has potential to apply to a
general circuit computing cliques due to the dynamic selection of t and z, and
to improve the Alon-Boppana bound exp(\Omega(n / \log n)^{1/3})).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4318</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4318</id><created>2013-07-16</created><updated>2014-03-03</updated><authors><author><keyname>Podolsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Critical slowing-down as indicator of approach to the loss of stability</title><categories>physics.soc-ph cs.SY</categories><comments>Shorter version submitted to IEEE SmartGridComm 2014; 6 pages, 4
  figures, discussion of autostructure functions added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic electro-mechanical dynamics of an overdamped power
system in the vicinity of the saddle-node bifurcation associated with the loss
of global stability such as voltage collapse or phase angle instability.
Fluctuations of the system state vector are driven by random variations of
loads and intermittent renewable generation. In the vicinity of collapse the
power system experiences so-called phenomenon of critical slowing-down
characterized by slowing and simultaneous amplification of the system state
vector fluctuations. In generic case of a co-dimension 1 bifurcation
corresponding to the threshold of instability it is possible to extract a
single mode of the system state vector responsible for this phenomenon. We
characterize stochastic fluctuations of the system state vector using the
formal perturbative expansion over the lowest (real) eigenvalue of the system
power flow Jacobian and verify the resulting expressions for correlation
functions of the state vector by direct numerical simulations. We conclude that
the onset of critical slowing-down is a good marker of approach to the
threshold of global instability. It can be straightforwardly detected from the
analysis of single-node autostructure and autocorrelation functions of system
state variables and thus does not require full observability of the grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4320</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4320</id><created>2013-07-16</created><authors><author><keyname>Cochran</keyname><forenames>William K.</forenames></author><author><keyname>Heath</keyname><forenames>Michael T.</forenames></author><author><keyname>McKiou</keyname><forenames>Kyle W.</forenames></author></authors><title>A Family of Hybrid Random Number Generators with Adjustable Quality and
  Speed</title><categories>math.NA cs.CR</categories><comments>10 pages, 3 figures, 1 table, 22 references</comments><msc-class>65C10, 11K45, 68P25, 68Q17, 68Q85</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional random number generators provide the speed but not necessarily
the high quality output streams needed for large-scale stochastic simulations.
Cryptographically-based generators, on the other hand, provide superior quality
output but are often deemed too slow to be practical for use in large
simulations. We combine these two approaches to construct a family of hybrid
generators that permit users to choose the desired tradeoff between quality and
speed for a given application. We demonstrate the effectiveness, performance,
and practicality of this approach using a standard battery of tests, which show
that high quality streams of random numbers can be obtained at a cost
comparable to that of fast conventional generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4329</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4329</id><created>2013-07-16</created><authors><author><keyname>Leal</keyname><forenames>Liliana Marcela Reina</forenames></author><author><keyname>Repiso</keyname><forenames>Rafael</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>H Index of scientific Nursing journals according to Google Scholar
  Metrics (2007-2011)</title><categories>cs.DL</categories><comments>9 pages</comments><report-no>EC35</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The aim of this report is to present a ranking of Nursing journals covered in
Google Scholar Metrics (GSM), a Google product launched in 2012 to assess the
impact of scientific journals from citation counts this receive on Google
Scholar. Google has chosen to include only those journals that have published
at least 100 papers and have at least one citation in a period of five years
(2007-2011). Journal rankings are sorted by languages (showing the 100 papers
with the greatest impact). This tool allows to sort by subject areas and
disciplines, but only in the case of journals in English. In this case, it only
shows the 20 journals with the highest h index. This option is not available
for journals in the other nine languages present in Google (Chinese,
Portuguese, German, Spanish, French, Korean, Japanese, Dutch and Italian).
  Google Scholar Metrics doesnt currently allow to group and sort all journals
belonging to a scientific discipline. In the case of Nursing, in the ten
listings displayed by GSM we can only locate 34 journals. Therefore, in an
attempt to overcome this limitation, we have used the diversity of search
procedures allowed by GSM to identify the greatest number of scientific
journals of Nursing with h index calculated by this bibliometric tool.
Bibliographic searches were conducted between 10th and 30th May 2013.
  The result is a ranking of 337 nursing journals sorted by the same h index,
and mean as discriminating value. Journals are also grouped by quartiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4332</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4332</id><created>2013-07-16</created><authors><author><keyname>Komenda</keyname><forenames>Jan</forenames></author><author><keyname>Masopust</keyname><forenames>Tomas</forenames></author><author><keyname>van Schuppen</keyname><forenames>Jan H.</forenames></author></authors><title>Coordination Control of Discrete-Event Systems Revisited</title><categories>math.OC cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revise and further investigate the coordination control
approach proposed for supervisory control of distributed discrete-event systems
with synchronous communication based on the Ramadge-Wonham automata framework.
The notions of conditional decomposability, conditional controllability, and
conditional closedness ensuring the existence of a solution are carefully
revised and simplified. The paper is generalized to non-prefix-closed
languages, that is, supremal conditionally controllable sublanguages of not
necessary prefix-closed languages are discussed. Non-prefix-closed languages
introduce the blocking issue into coordination control, hence a procedure to
compute a coordinator for nonblockingness is included. The optimization problem
concerning the size of a coordinator is under investigation. We prove that to
find the minimal extension of the coordinator event set for which a given
specification language is conditionally decomposable is NP-hard. In other
words, unless P=NP, it is not possible to find a polynomial algorithm to
compute the minimal coordinator with respect to the number of events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4334</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4334</id><created>2013-07-16</created><updated>2013-12-06</updated><authors><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author><author><keyname>Zambelli</keyname><forenames>Giacomo</forenames></author></authors><title>A polynomial projection-type algorithm for linear programming</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple O([n^5/\log n]L) algorithm for linear programming
feasibility, that can be considered as a polynomial-time implementation of the
relaxation method. Our work draws from Chubanov's &quot;Divide-and-Conquer&quot;
algorithm [4], where the recursion is replaced by a simple and more efficient
iterative method. A similar approach was used in a more recent paper of
Chubanov [6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4339</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4339</id><created>2013-07-16</created><updated>2014-11-19</updated><authors><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Computing Similarity Distances Between Rankings</title><categories>cs.DS cs.IT math.IT</categories><comments>32 pages, 14 figures. Corrected proof of unbalanced case</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of computing distances between rankings that take into
account similarities between candidates. The need for evaluating such distances
is governed by applications as diverse as rank aggregation, bioinformatics,
social sciences and data storage. The problem may be summarized as follows:
Given two rankings and a positive cost function on transpositions that depends
on the similarity of the candidates involved, find a smallest cost sequence of
transpositions that converts one ranking into another. Our focus is on costs
that may be described via special metric-tree structures and on complete
rankings modeled as permutations. The presented results include a
quadratic-time algorithm for finding a minimum cost decomposition for simple
cycles, and a quadratic-time, $4/3$-approximation algorithm for permutations
that contain multiple cycles. The proposed methods rely on investigating a
newly introduced balancing property of cycles embedded in trees, cycle-merging
methods, and shortest path optimization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4355</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4355</id><created>2013-07-16</created><updated>2015-03-18</updated><authors><author><keyname>Ahn</keyname><forenames>Kook Jin</forenames></author><author><keyname>Guha</keyname><forenames>Sudipto</forenames></author></authors><title>Near Linear Time Approximation Schemes for Uncapacitated and Capacitated
  b--Matching Problems in Nonbipartite Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first fully polynomial approximation schemes for the maximum
weighted (uncapacitated or capacitated) $b$--Matching problem for nonbipartite
graphs that run in time (near) linear in the number of edges, that is, given
any $\delta&gt;0$ the algorithm produces a $(1-\delta)$ approximation in $O(m
\poly(\delta^{-1},\log n))$ time. We provide fractional solutions for the
standard linear programming formulations for these problems and subsequently
also provide fully polynomial (near) linear time approximation schemes for
rounding the fractional solutions. Through these problems as a vehicle, we also
present several ideas in the context of solving linear programs approximately
using fast primal-dual algorithms. First, we show that approximation algorithms
can be used to reduce the width of the formulation, and as a consequence we
induce faster convergence. Second, even though the dual of these problems have
exponentially many variables and an efficient exact computation of dual weights
is infeasible, we can efficiently compute and use a sparse approximation of the
dual weights using a combination of (i) adding perturbation to the constraints
of the polytope and (ii) amplification followed by thresholding of the dual
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4359</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4359</id><created>2013-07-16</created><updated>2015-04-20</updated><authors><author><keyname>Ahn</keyname><forenames>Kook Jin</forenames></author><author><keyname>Guha</keyname><forenames>Sudipto</forenames></author></authors><title>Access to Data and Number of Iterations: Dual Primal Algorithms for
  Maximum Matching under Resource Constraints</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider graph algorithms in models of computation where the
space usage (random accessible storage, in addition to the read only input) is
sublinear in the number of edges $m$ and the access to input data is
constrained. These questions arises in many natural settings, and in particular
in the analysis of MapReduce or similar algorithms that model constrained
parallelism with sublinear central processing. In SPAA 2011, Lattanzi etal.
provided a $O(1)$ approximation of maximum matching using $O(p)$ rounds of
iterative filtering via mapreduce and $O(n^{1+1/p})$ space of central
processing for a graph with $n$ nodes and $m$ edges.
  We focus on weighted nonbipartite maximum matching in this paper. For any
constant $p&gt;1$, we provide an iterative sampling based algorithm for computing
a $(1-\epsilon)$-approximation of the weighted nonbipartite maximum matching
that uses $O(p/\epsilon)$ rounds of sampling, and $O(n^{1+1/p})$ space. The
results extends to $b$-Matching with small changes. This paper combines
adaptive sketching literature and fast primal-dual algorithms based on relaxed
Dantzig-Wolfe decision procedures. Each round of sampling is implemented
through linear sketches and executed in a single round of MapReduce. The paper
also proves that nonstandard linear relaxations of a problem, in particular
penalty based formulations, are helpful in mapreduce and similar settings in
reducing the adaptive dependence of the iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4380</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4380</id><created>2013-07-13</created><authors><author><keyname>Aldhafferi</keyname><forenames>Nahier</forenames></author><author><keyname>Watson</keyname><forenames>Charles</forenames></author><author><keyname>Sajeev</keyname><forenames>A. S. M</forenames></author></authors><title>A Smart Wizard System Suitable for Use With Internet Mobile Devices to
  Adjust Personal Information Privacy Settings</title><categories>cs.CY cs.CR</categories><comments>16 pages, 8 figures, 2 tables</comments><journal-ref>International Journal of Security, Privacy and Trust
  Management,Vol2,No3,2013</journal-ref><doi>10.5121/ijsptm.2013.2301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The privacy of personal information is an important issue affecting the
confidence of internet users. The widespread adoption of online social networks
and access to these platforms using mobile devices has encouraged developers to
make the systems and interfaces acceptable to users who seek privacy. The aim
of this study is to test a wizard that allows users to control the sharing of
personal information with others. We also assess the concerns of users in terms
of such sharing such as whether to hide personal data in current online social
network accounts. Survey results showed the wizard worked very well and that
females concealed more personal information than did males. In addition, most
users who were concerned about misuse of personal information hid those items.
The results can be used to upgrade current privacy systems or to design new
systems that work on mobile internet devices. The system can also be used to
save time when setting personal privacy settings and makes users more aware of
items that will be shared with others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4388</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4388</id><created>2013-07-16</created><updated>2014-04-14</updated><authors><author><keyname>Krishnan</keyname><forenames>Narayanan</forenames></author><author><keyname>Yates</keyname><forenames>Roy D.</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>Uplink Linear Receivers for Multi-cell Multiuser MIMO with Pilot
  Contamination: Large System Analysis</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base stations with a large number of transmit antennas have the potential to
serve a large number of users at high rates. However, the receiver processing
in the uplink relies on channel estimates which are known to suffer from pilot
interference. In this work, making use of the similarity of the uplink received
signal in CDMA with that of a multi-cell multi-antenna system, we perform a
large system analysis when the receiver employs an MMSE filter with a pilot
contaminated estimate. We assume a Rayleigh fading channel with different
received powers from users. We find the asymptotic Signal to Interference plus
Noise Ratio (SINR) as the number of antennas and number of users per base
station grow large while maintaining a fixed ratio. Through the SINR expression
we explore the scenario where the number of users being served are comparable
to the number of antennas at the base station. The SINR explicitly captures the
effect of pilot contamination and is found to be the same as that employing a
matched filter with a pilot contaminated estimate. We also find the exact
expression for the interference suppression obtained using an MMSE filter which
is an important factor when there are significant number of users in the system
as compared to the number of antennas. In a typical set up, in terms of the
five percentile SINR, the MMSE filter is shown to provide significant gains
over matched filtering and is within 5 dB of MMSE filter with perfect channel
estimate. Simulation results for achievable rates are close to large system
limits for even a 10-antenna base station with 3 or more users per cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4420</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4420</id><created>2013-07-16</created><updated>2013-07-19</updated><authors><author><keyname>Briggs</keyname><forenames>Lily</forenames></author></authors><title>On the Complexity of a Matching Problem with Asymmetric Weights</title><categories>cs.CC cs.DS</categories><comments>9 pages; v2 fixed typos, made minor clarifications, and added author
  affiliation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present complexity results regarding a matching-type problem related to
structural controllability of dynamical systems modelled on graphs.
Controllability of a dynamical system is the ability to choose certain inputs
in order to drive the system from any given state to any desired state; a graph
is said to be structurally controllable if it represents the structure of a
controllable system. We define the Orientation Control Matching problem (OCM)
to be the problem of orienting an undirected graph in a manner that maximizes
its structural controllability. A generalized version, the Asymmetric
Orientation Control Matching problem (AOCM), allows for asymmetric weights on
the possible directions of each edge. These problems are closely related to
2-matchings, disjoint path covers, and disjoint cycle covers. We prove using
reductions that OCM is polynomially solvable, while AOCM is much harder; we
show that it is NP-complete as well as APX-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4430</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4430</id><created>2013-07-16</created><authors><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Su</keyname><forenames>Wei</forenames></author><author><keyname>Dabin</keyname><forenames>Jason</forenames></author><author><keyname>Kanterakis</keyname><forenames>Emmanuel</forenames></author></authors><title>Modulation Classification of MIMO-OFDM Signals by Independent Component
  Analysis and Support Vector Machines</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing, May 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modulation classification (MC) scheme based on Independent Component
Analysis (ICA) in conjunction with either maximum likelihood (ML) or Support
Vector Machines (SVM) is proposed for MIMO-OFDM signals over frequency
selective, time varying channels. The method is blind in the sense that it is
assumed that the receiver has no information about the channel and transmitted
signals other than that the spatial streams of signals are statistically
independent. The processing consists of separation of the MIMO streams followed
by modulation classification of the separated signals. While in general, blind
separation of signals over frequency selective channels is a difficult problem,
the non-frequency selective nature of the channel experienced by individual
symbols in a MIMO-OFDM system enables the application of well-known ICA
algorithms. Modulation classification is implemented by maximum likelihood and
by an SVM-based modulation classification method relying on pre-selected
modulation-dependent features. To improve performance in time varying channels,
the invariance of the channel is exploited across the coherence bandwidth and
the time coherence. The proposed method is shown to perform with high
probability of correct classification over realistic ITU pedestrian and
vehicular channels. An upper bound on the probability of correct classification
is developed based on the Cramer Rao bound of channel estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4436</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4436</id><created>2013-07-14</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>Usage habits of business information system in Hungary</title><categories>cs.CY</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1303.0520, arXiv:1307.3760, arXiv:1307.4077</comments><msc-class>94A05</msc-class><acm-class>H.1.1</acm-class><journal-ref>International Journal of Engineering and Innovative Technology
  (IJEIT)Volume 2, Issue 8, February 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IT functions of the companies can be executed in different ways inhouse
solution, outsourcing, in sourcing, formation a spinoff company. Predominantly
this function is provided within the company in Hungary. The larger a company
is; it is more likely that a separate IT manager will be entrusted for the
supervision of IT functions. Only a very small number of smallsized enterprises
said that they paid special attention to formulating an IT strategy, while it
was not considered important by microenterprises at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4440</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4440</id><created>2013-07-16</created><authors><author><keyname>de Haan</keyname><forenames>Ronald</forenames></author><author><keyname>Roub&#xed;&#x10d;kov&#xe1;</keyname><forenames>Anna</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Parameterized Complexity Results for Plan Reuse</title><categories>cs.AI cs.CC</categories><comments>Proceedings of AAAI 2013, pp. 224-231, AAAI Press, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning is a notoriously difficult computational problem of high worst-case
complexity. Researchers have been investing significant efforts to develop
heuristics or restrictions to make planning practically feasible. Case-based
planning is a heuristic approach where one tries to reuse previous experience
when solving similar problems in order to avoid some of the planning effort.
Plan reuse may offer an interesting alternative to plan generation in some
settings.
  We provide theoretical results that identify situations in which plan reuse
is provably tractable. We perform our analysis in the framework of
parameterized complexity, which supports a rigorous worst-case complexity
analysis that takes structural properties of the input into account in terms of
parameters. A central notion of parameterized complexity is fixed-parameter
tractability which extends the classical notion of polynomial-time tractability
by utilizing the effect of structural properties of the problem input.
  We draw a detailed map of the parameterized complexity landscape of several
variants of problems that arise in the context of case-based planning. In
particular, we consider the problem of reusing an existing plan, imposing
various restrictions in terms of parameters, such as the number of steps that
can be added to the existing plan to turn it into a solution of the planning
instance at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4457</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4457</id><created>2013-07-16</created><updated>2013-07-22</updated><authors><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Sanjabi</keyname><forenames>Maziar</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>A Stochastic Successive Minimization Method for Nonsmooth Nonconvex
  Optimization with Applications to Transceiver Design in Wireless
  Communication Networks</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of minimizing the expected value of a cost function
parameterized by a random variable. The classical sample average approximation
(SAA) method for solving this problem requires minimization of an ensemble
average of the objective at each step, which can be expensive. In this paper,
we propose a stochastic successive upper-bound minimization method (SSUM) which
minimizes an approximate ensemble average at each iteration. To ensure
convergence and to facilitate computation, we require the approximate ensemble
average to be a locally tight upper-bound of the expected cost function and be
easily optimized. The main contributions of this work include the development
and analysis of the SSUM method as well as its applications in linear
transceiver design for wireless communication networks and online dictionary
learning. Moreover, using the SSUM framework, we extend the classical
stochastic (sub-)gradient (SG) method to the case of minimizing a nonsmooth
nonconvex objective function and establish its convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4462</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4462</id><created>2013-07-16</created><authors><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>An Outage Exponent Region based Coded f-Matching Framework for Channel
  Allocation in Multi-carrier Multi-access Channels</title><categories>cs.IT math.IT</categories><comments>19pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-carrier multi-access technique is widely adopt in future wireless
communication systems, such as IEEE 802.16m and 3GPP LTE-A. The channel
resources allocation in multi-carrier multi-access channel, which can greatly
improve the system throughput with QoS assurance, thus attracted much attention
from both academia and industry. There lacks, however, an analytic framework
with a comprehensive performance metric, such that it is difficult to fully
exploit the potentials of channel allocation. This paper will propose an
analytic coded fmatching framework, where the outage exponent region (OER) will
be defined as the performance metric. The OER determines the relationship of
the outage performance among all of the users in the full SNR range, and
converges to the diversity-multiplexing region (DMR) when SNR tends to
infinity. To achieve the optimal OER and DMR, the random bipartite graph (RBG)
approach, only depending on 1 bit CSI, will be proposed to formulate this
problem. Based on the RBG formulation, the optimal frequency-domain coding
based maximum f-matching method is then proposed. By analyzing the
combinatorial structure of the RBG based coded f-matching with the help of
saddlepoint approximation, the outage probability of each user, OER, and DMR
will be derived in closed-form formulas. It will be shown that all of the users
share the total multiplexing gain according to their rate requirements, while
achieving the full frequency diversity, i.e., the optimal OER and DMR. Based on
the principle of parallel computations, the parallel vertices expansion &amp;
random rotation based Hopcroft-Karp (PVER2HK) algorithm, which enjoys a
logarithmic polynomial complexity, will be proposed. The simulation results
will not only verify the theoretical derivations, but also show the significant
performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4463</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4463</id><created>2013-07-16</created><updated>2013-07-21</updated><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Tian</keyname><forenames>Shuang</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Distributed Raptor Coding for Erasure Channels: Partially and Fully
  Coded Cooperation</title><categories>cs.IT math.IT</categories><comments>To be Published in IEEE Transaction on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new rateless coded cooperation scheme for a
general multi-user cooperative wireless system. We develop cooperation methods
based on Raptor codes with the assumption that the channels face erasure with
specific erasure probabilities and transmitters have no channel state
information. A fully coded cooperation (FCC) and a partially coded cooperation
(PCC) strategy are developed to maximize the average system throughput. Both
PCC and FCC schemes have been analyzed through AND-OR tree analysis and a
linear programming optimization problem is then formulated to find the optimum
degree distribution for each scheme. Simulation results show that optimized
degree distributions can bring considerable throughput gains compared to
existing degree distributions which are designed for point-to-point binary
erasure channels. It is also shown that the PCC scheme outperforms the FCC
scheme in terms of average system throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4465</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4465</id><created>2013-07-16</created><authors><author><keyname>Gazda</keyname><forenames>Maciej</forenames></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames></author></authors><title>Zielonka's Recursive Algorithm: dull, weak and solitaire games and
  tighter bounds</title><categories>cs.LO cs.GT</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 7-20</journal-ref><doi>10.4204/EPTCS.119.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dull, weak and nested solitaire games are important classes of parity games,
capturing, among others, alternation-free mu-calculus and ECTL* model checking
problems. These classes can be solved in polynomial time using dedicated
algorithms. We investigate the complexity of Zielonka's Recursive algorithm for
solving these special games, showing that the algorithm runs in O(d (n + m)) on
weak games, and, somewhat surprisingly, that it requires exponential time to
solve dull games and (nested) solitaire games. For the latter classes, we
provide a family of games G, allowing us to establish a lower bound of 2^(n/3).
We show that an optimisation of Zielonka's algorithm permits solving games from
all three classes in polynomial time. Moreover, we show that there is a family
of (non-special) games M that permits us to establish a lower bound of 2^(n/3),
improving on the previous lower bound for the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4466</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4466</id><created>2013-07-16</created><authors><author><keyname>Huth</keyname><forenames>Michael</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Kuo</keyname><forenames>Jim Huan-Pu</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Piterman</keyname><forenames>Nir</forenames><affiliation>University of Leicester</affiliation></author></authors><title>The Rabin index of parity games</title><categories>cs.GT cs.CC</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 35-49</journal-ref><doi>10.4204/EPTCS.119.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the descriptive complexity of parity games by taking into account
the coloring of their game graphs whilst ignoring their ownership structure.
Colored game graphs are identified if they determine the same winning regions
and strategies, for all ownership structures of nodes. The Rabin index of a
parity game is the minimum of the maximal color taken over all equivalent
coloring functions. We show that deciding whether the Rabin index is at least k
is in PTIME for k=1 but NP-hard for all fixed k &gt; 1. We present an EXPTIME
algorithm that computes the Rabin index by simplifying its input coloring
function. When replacing simple cycle with cycle detection in that algorithm,
its output over-approximates the Rabin index in polynomial time. Experimental
results show that this approximation yields good values in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4468</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4468</id><created>2013-07-16</created><authors><author><keyname>Reynolds</keyname><forenames>Mark</forenames><affiliation>The University of Western Australia</affiliation></author></authors><title>A Faster Tableau for CTL*</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><acm-class>F.3.1</acm-class><journal-ref>EPTCS 119, 2013, pp. 50-63</journal-ref><doi>10.4204/EPTCS.119.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been several recent suggestions for tableau systems for deciding
satisfiability in the practically important branching time temporal logic known
as CTL*. In this paper we present a streamlined and more traditional tableau
approach built upon the author's earlier theoretical work.
  Soundness and completeness results are proved. A prototype implementation
demonstrates the significantly improved performance of the new approach on a
range of test formulas. We also see that it compares favourably to state of the
art, game and automata based decision procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4469</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4469</id><created>2013-07-16</created><authors><author><keyname>Bersani</keyname><forenames>Marcello Maria</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Rossi</keyname><forenames>Matteo</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Pietro</keyname><forenames>Pierluigi San</forenames><affiliation>Politecnico di Milano</affiliation></author></authors><title>Deciding the Satisfiability of MITL Specifications</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 119, 2013, pp. 64-78</journal-ref><doi>10.4204/EPTCS.119.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a satisfiability-preserving reduction from MITL
interpreted over finitely-variable continuous behaviors to Constraint LTL over
clocks, a variant of CLTL that is decidable, and for which an SMT-based bounded
satisfiability checker is available. The result is a new complete and effective
decision procedure for MITL. Although decision procedures for MITL already
exist, the automata-based techniques they employ appear to be very difficult to
realize in practice, and, to the best of our knowledge, no implementation
currently exists for them. A prototype tool for MITL based on the encoding
presented here has, instead, been implemented and is publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4470</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4470</id><created>2013-07-16</created><authors><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>Universit&#xe0; degli Studi di Verona</affiliation></author></authors><title>Improving HyLTL model checking of hybrid systems</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 79-92</journal-ref><doi>10.4204/EPTCS.119.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of model-checking hybrid systems is a long-time challenge in the
scientific community. Most of the existing approaches and tools are either
limited on the properties that they can verify, or restricted to simplified
classes of systems. To overcome those limitations, a temporal logic called
HyLTL has been recently proposed. The model checking problem for this logic has
been solved by translating the formula into an equivalent hybrid automaton,
that can be analized using existing tools. The original construction employs a
declarative procedure that generates exponentially many states upfront, and can
be very inefficient when complex formulas are involved. In this paper we solve
a technical issue in the construction that was not considered in previous
works, and propose a new algorithm to translate HyLTL into hybrid automata,
that exploits optimized techniques coming from the discrete LTL community to
build smaller automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4471</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4471</id><created>2013-07-16</created><authors><author><keyname>Fogarty</keyname><forenames>Seth</forenames><affiliation>Trinity University</affiliation></author><author><keyname>Kupferman</keyname><forenames>Orna</forenames><affiliation>Hebrew University of Jerusalem</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Rice University</affiliation></author><author><keyname>Wilke</keyname><forenames>Thomas</forenames><affiliation>Christian-Albrechts-Universit&#xe4;t zu Kiel</affiliation></author></authors><title>Profile Trees for B\&quot;uchi Word Automata, with Application to
  Determinization</title><categories>cs.FL cs.LO cs.SE</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; D.2.4</acm-class><journal-ref>EPTCS 119, 2013, pp. 107-121</journal-ref><doi>10.4204/EPTCS.119.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The determinization of Buchi automata is a celebrated problem, with
applications in synthesis, probabilistic verification, and multi-agent systems.
Since the 1960s, there has been a steady progress of constructions: by
McNaughton, Safra, Piterman, Schewe, and others. Despite the proliferation of
solutions, they are all essentially ad-hoc constructions, with little theory
behind them other than proofs of correctness. Since Safra, all optimal
constructions employ trees as states of the deterministic automaton, and
transitions between states are defined operationally over these trees. The
operational nature of these constructions complicates understanding,
implementing, and reasoning about them, and should be contrasted with
complementation, where a solid theory in terms of automata run DAGs underlies
modern constructions.
  In 2010, we described a profile-based approach to Buchi complementation,
where a profile is simply the history of visits to accepting states. We
developed a structural theory of profiles and used it to describe a
complementation construction that is deterministic in the limit. Here we extend
the theory of profiles to prove that every run DAG contains a profile tree with
at most a finite number of infinite branches. We then show that this property
provides a theoretical grounding for a new determinization construction where
macrostates are doubly preordered sets of states. In contrast to extant
determinization constructions, transitions in the new construction are
described declaratively rather than operationally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4472</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4472</id><created>2013-07-16</created><authors><author><keyname>Labai</keyname><forenames>Nadia</forenames><affiliation>Faculty of Computer Science, Technion-IIT, Haifa, Israel</affiliation></author><author><keyname>Makowsky</keyname><forenames>Johann A.</forenames><affiliation>Faculty of Computer Science, Technion-IIT, Haifa, Israel</affiliation></author></authors><title>Weighted Automata and Monadic Second Order Logic</title><categories>cs.LO cs.FL</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 122-135</journal-ref><doi>10.4204/EPTCS.119.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let S be a commutative semiring. M. Droste and P. Gastin have introduced in
2005 weighted monadic second order logic WMSOL with weights in S. They use a
syntactic fragment RMSOL of WMSOL to characterize word functions (power series)
recognizable by weighted automata, where the semantics of quantifiers is used
both as arithmetical operations and, in the boolean case, as quantification.
  Already in 2001, B. Courcelle, J.Makowsky and U. Rotics have introduced a
formalism for graph parameters definable in Monadic Second order Logic, here
called MSOLEVAL with values in a ring R. Their framework can be easily adapted
to semirings S. This formalism clearly separates the logical part from the
arithmetical part and also applies to word functions.
  In this paper we give two proofs that RMSOL and MSOLEVAL with values in S
have the same expressive power over words. One proof shows directly that
MSOLEVAL captures the functions recognizable by weighted automata. The other
proof shows how to translate the formalisms from one into the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4473</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4473</id><created>2013-07-16</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Loitzenbauer</keyname><forenames>Veronika</forenames></author></authors><title>Approximating the minimum cycle mean</title><categories>cs.DS</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 136-149</journal-ref><doi>10.4204/EPTCS.119.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider directed graphs where each edge is labeled with an integer weight
and study the fundamental algorithmic question of computing the value of a
cycle with minimum mean weight. Our contributions are twofold: (1) First we
show that the algorithmic question is reducible in O(n^2) time to the problem
of a logarithmic number of min-plus matrix multiplications of n-by-n matrices,
where n is the number of vertices of the graph. (2) Second, when the weights
are nonnegative, we present the first (1 + {\epsilon})-approximation algorithm
for the problem and the running time of our algorithm is \tilde(O)(n^\omega
log^3(nW/{\epsilon}) / {\epsilon}), where O(n^\omega) is the time required for
the classic n-by-n matrix multiplication and W is the maximum value of the
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4474</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4474</id><created>2013-07-16</created><authors><author><keyname>Di Pierro</keyname><forenames>Alessandra</forenames><affiliation>University of Verona</affiliation></author><author><keyname>Wiklicky</keyname><forenames>Herbert</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Probabilistic data flow analysis: a linear equational approach</title><categories>cs.PL</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 150-165</journal-ref><doi>10.4204/EPTCS.119.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speculative optimisation relies on the estimation of the probabilities that
certain properties of the control flow are fulfilled. Concrete or estimated
branch probabilities can be used for searching and constructing advantageous
speculative and bookkeeping transformations.
  We present a probabilistic extension of the classical equational approach to
data-flow analysis that can be used to this purpose. More precisely, we show
how the probabilistic information introduced in a control flow graph by branch
prediction can be used to extract a system of linear equations from a program
and present a method for calculating correct (numerical) solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4475</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4475</id><created>2013-07-16</created><authors><author><keyname>Dimovski</keyname><forenames>Aleksandar S.</forenames></author></authors><title>Slot Games for Detecting Timing Leaks of Programs</title><categories>cs.PL cs.CR cs.GT</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 166-179</journal-ref><doi>10.4204/EPTCS.119.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a method for verifying secure information flow of
programs, where apart from direct and indirect flows a secret information can
be leaked through covert timing channels. That is, no two computations of a
program that differ only on high-security inputs can be distinguished by
low-security outputs and timing differences. We attack this problem by using
slot-game semantics for a quantitative analysis of programs. We show how
slot-games model can be used for performing a precise security analysis of
programs, that takes into account both extensional and intensional properties
of programs. The practicality of this approach for automated verification is
also shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4476</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4476</id><created>2013-07-16</created><authors><author><keyname>Vester</keyname><forenames>Steen</forenames><affiliation>DTU Compute</affiliation></author></authors><title>Alternating-time temporal logic with finite-memory strategies</title><categories>cs.GT cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 194-207</journal-ref><doi>10.4204/EPTCS.119.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-checking the alternating-time temporal logics ATL and ATL* with
incomplete information is undecidable for perfect recall semantics. However,
when restricting to memoryless strategies the model-checking problem becomes
decidable. In this paper we consider two other types of semantics based on
finite-memory strategies. One where the memory size allowed is bounded and one
where the memory size is unbounded (but must be finite). This is motivated by
the high complexity of model-checking with perfect recall semantics and the
severe limitations of memoryless strategies. We show that both types of
semantics introduced are different from perfect recall and memoryless semantics
and next focus on the decidability and complexity of model-checking in both
complete and incomplete information games for ATL/ATL*. In particular, we show
that the complexity of model-checking with bounded-memory semantics is
Delta_2p-complete for ATL and PSPACE-complete for ATL* in incomplete
information games just as in the memoryless case. We also present a proof that
ATL and ATL* model-checking is undecidable for n &gt;= 3 players with
finite-memory semantics in incomplete information games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4477</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4477</id><created>2013-07-16</created><authors><author><keyname>Jamroga</keyname><forenames>Wojciech</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>M&#x229;ski</keyname><forenames>Artur</forenames><affiliation>Polish Academy of Sciences</affiliation></author><author><keyname>Szreter</keyname><forenames>Maciej</forenames><affiliation>Polish Academy of Sciences</affiliation></author></authors><title>Modularity and Openness in Modeling Multi-Agent Systems</title><categories>cs.MA cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 224-239</journal-ref><doi>10.4204/EPTCS.119.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the formalism of modular interpreted systems (MIS) which
encourages modular and open modeling of synchronous multi-agent systems. The
original formulation of MIS did not live entirely up to its promise. In this
paper, we propose how to improve modularity and openness of MIS by changing the
structure of interference functions. These relatively small changes allow for
surprisingly high flexibility when modeling actual multi-agent systems. We
demonstrate this on two well-known examples, namely the trains, tunnel and
controller, and the dining cryptographers.
  Perhaps more importantly, we propose how the notions of multi-agency and
openness, crucial for multi-agent systems, can be precisely defined based on
their MIS representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4478</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4478</id><created>2013-07-16</created><authors><author><keyname>Laroussinie</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LIAFA, Univ. Paris Diderot, France</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV, ENS Cachan, France</affiliation></author></authors><title>Satisfiability of ATL with strategy contexts</title><categories>cs.LO cs.GT cs.MA</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 208-223</journal-ref><doi>10.4204/EPTCS.119.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various extensions of the temporal logic ATL have recently been introduced to
express rich properties of multi-agent systems. Among these, ATLsc extends ATL
with strategy contexts, while Strategy Logic has first-order quantification
over strategies. There is a price to pay for the rich expressiveness of these
logics: model-checking is non-elementary, and satisfiability is undecidable.
  We prove in this paper that satisfiability is decidable in several special
cases. The most important one is when restricting to turn-based games. We prove
that decidability also holds for concurrent games if the number of moves
available to the agents is bounded. Finally, we prove that restricting strategy
quantification to memoryless strategies brings back undecidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4479</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4479</id><created>2013-07-16</created><authors><author><keyname>Della Monica</keyname><forenames>Dario</forenames></author><author><keyname>Napoli</keyname><forenames>Margherita</forenames></author><author><keyname>Parente</keyname><forenames>Mimmo</forenames></author></authors><title>Model checking coalitional games in shortage resource scenarios</title><categories>cs.LO cs.AI cs.CC</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><acm-class>F.2.0; F.4.1; F.4.3</acm-class><journal-ref>EPTCS 119, 2013, pp. 240-255</journal-ref><doi>10.4204/EPTCS.119.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification of multi-agents systems (MAS) has been recently studied taking
into account the need of expressing resource bounds. Several logics for
specifying properties of MAS have been presented in quite a variety of
scenarios with bounded resources. In this paper, we study a different
formalism, called Priced Resource-Bounded Alternating-time Temporal Logic
(PRBATL), whose main novelty consists in moving the notion of resources from a
syntactic level (part of the formula) to a semantic one (part of the model).
This allows us to track the evolution of the resource availability along the
computations and provides us with a formalisms capable to model a number of
real-world scenarios. Two relevant aspects are the notion of global
availability of the resources on the market, that are shared by the agents, and
the notion of price of resources, depending on their availability. In a
previous work of ours, an initial step towards this new formalism was
introduced, along with an EXPTIME algorithm for the model checking problem. In
this paper we better analyze the features of the proposed formalism, also in
comparison with previous approaches. The main technical contribution is the
proof of the EXPTIME-hardness of the the model checking problem for PRBATL,
based on a reduction from the acceptance problem for Linearly-Bounded
Alternating Turing Machines. In particular, since the problem has multiple
parameters, we show two fixed-parameter reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4500</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4500</id><created>2013-07-17</created><authors><author><keyname>Hong</keyname><forenames>Hyunsuk</forenames></author><author><keyname>Son</keyname><forenames>Seung-Woo</forenames></author></authors><title>Costly bilingualism model in a population with one zealot</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>5 pages, 3 figures, 2 tables</comments><journal-ref>Phys. Rev. E 88, 022807 (2013)</journal-ref><doi>10.1103/PhysRevE.88.022807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a costly bilingualism model in which one can take two strategies
in parallel. We investigate how a single zealot triggers the cascading behavior
and how the compatibility of the two strategies affects when interacting
patterns change. First, the role of the interaction range on the cascading is
studied by increasing the range from local to global. We find that people
sometimes do not favor to take the superior strategy even though its payoff is
higher than that of the inferior one. This is found to be caused by the local
interactions rather than the global ones. Applying this model to social
networks, we find that the location of the zealot is also important for larger
cascading in heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4502</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4502</id><created>2013-07-17</created><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author></authors><title>Universally Elevating the Phase Transition Performance of Compressed
  Sensing: Non-Isometric Matrices are Not Necessarily Bad Matrices</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>6pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1010.2236, arXiv:1004.0402</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing problems, $\ell_1$ minimization or Basis Pursuit was
known to have the best provable phase transition performance of recoverable
sparsity among polynomial-time algorithms. It is of great theoretical and
practical interest to find alternative polynomial-time algorithms which perform
better than $\ell_1$ minimization. \cite{Icassp reweighted l_1}, \cite{Isit
reweighted l_1}, \cite{XuScaingLaw} and \cite{iterativereweightedjournal} have
shown that a two-stage re-weighted $\ell_1$ minimization algorithm can boost
the phase transition performance for signals whose nonzero elements follow an
amplitude probability density function (pdf) $f(\cdot)$ whose $t$-th derivative
$f^{t}(0) \neq 0$ for some integer $t \geq 0$. However, for signals whose
nonzero elements are strictly suspended from zero in distribution (for example,
constant-modulus, only taking values `$+d$' or `$-d$' for some nonzero real
number $d$), no polynomial-time signal recovery algorithms were known to
provide better phase transition performance than plain $\ell_1$ minimization,
especially for dense sensing matrices. In this paper, we show that a
polynomial-time algorithm can universally elevate the phase-transition
performance of compressed sensing, compared with $\ell_1$ minimization, even
for signals with constant-modulus nonzero elements. Contrary to conventional
wisdoms that compressed sensing matrices are desired to be isometric, we show
that non-isometric matrices are not necessarily bad sensing matrices. In this
paper, we also provide a framework for recovering sparse signals when sensing
matrices are not isometric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4505</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4505</id><created>2013-07-17</created><authors><author><keyname>K</keyname><forenames>Deekshith P</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author><author><keyname>Rajesh</keyname><forenames>R</forenames></author></authors><title>AWGN Channel Capacity of Energy Harvesting Transmitters with a Finite
  Energy Buffer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an AWGN channel with a transmitter powered by an energy
harvesting source. The node is equipped with a finite energy buffer. Such a
system can be modelled as a channel with side information (about energy in the
energy buffer) causally known at the transmitter. The receiver may or may not
have the side information. We prove that Markov energy management policies are
sufficient to achieve the capacity of the system and provide a single letter
characterization for the capacity. The computation of the capacity is
expensive. Therefore, we discuss an achievable scheme that is easy to compute.
This achievable rate converges to the infinite buffer capacity as the buffer
length increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4514</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4514</id><created>2013-07-17</created><updated>2013-07-23</updated><authors><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author></authors><title>Supervised Metric Learning with Generalization Guarantees</title><categories>cs.LG stat.ML</categories><comments>PhD thesis defended on December 11, 2012 (Laboratoire Hubert Curien,
  University of Saint-Etienne)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crucial importance of metrics in machine learning algorithms has led to
an increasing interest in optimizing distance and similarity functions, an area
of research known as metric learning. When data consist of feature vectors, a
large body of work has focused on learning a Mahalanobis distance. Less work
has been devoted to metric learning from structured objects (such as strings or
trees), most of it focusing on optimizing a notion of edit distance. We
identify two important limitations of current metric learning approaches.
First, they allow to improve the performance of local algorithms such as
k-nearest neighbors, but metric learning for global algorithms (such as linear
classifiers) has not been studied so far. Second, the question of the
generalization ability of metric learning methods has been largely ignored. In
this thesis, we propose theoretical and algorithmic contributions that address
these limitations. Our first contribution is the derivation of a new kernel
function built from learned edit probabilities. Our second contribution is a
novel framework for learning string and tree edit similarities inspired by the
recent theory of (e,g,t)-good similarity functions. Using uniform stability
arguments, we establish theoretical guarantees for the learned similarity that
give a bound on the generalization error of a linear classifier built from that
similarity. In our third contribution, we extend these ideas to metric learning
from feature vectors by proposing a bilinear similarity learning method that
efficiently optimizes the (e,g,t)-goodness. Generalization guarantees are
derived for our approach, highlighting that our method minimizes a tighter
bound on the generalization error of the classifier. Our last contribution is a
framework for establishing generalization bounds for a large class of existing
metric learning algorithms based on a notion of algorithmic robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4516</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4516</id><created>2013-07-17</created><authors><author><keyname>Aroquiaraj</keyname><forenames>I. Laurence</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author></authors><title>Mammogram Edge Detection Using Hybrid Soft Computing Methods</title><categories>cs.CV</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is a crucial step in a wide range of method image
processing systems. It is useful in visualization of the different objects
present in the image. In spite of the several methods available in the
literature, image segmentation still a challenging problem in most of image
processing applications. The challenge comes from the fuzziness of image
objects and the overlapping of the different regions. Detection of edges in an
image is a very important step towards understanding image features. There are
large numbers of edge detection operators available, each designed to be
sensitive to certain types of edges. The Quality of edge detection can be
measured from several criteria objectively. Some criteria are proposed in terms
of mathematical measurement, some of them are based on application and
implementation requirements. Since edges often occur at image locations
representing object boundaries, edge detection is extensively used in image
segmentation when images are divided into areas corresponding to different
objects. This can be used specifically for enhancing the tumor area in
mammographic images. Different methods are available for edge detection like
Roberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novel
algorithms for edge detection has been proposed for mammographic images. Breast
boundary, pectoral region and tumor location can be seen clearly by using this
method. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edge
operators are used and their results are displayed. Experimental results
demonstrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4518</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4518</id><created>2013-07-17</created><updated>2013-07-17</updated><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Zeyu</forenames></author></authors><title>Ranking with Diverse Intents and Correlated Contents</title><categories>cs.DS cs.IR</categories><comments>14 pages</comments><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following document ranking problem: We have a collection of
documents, each containing some topics (e.g. sports, politics, economics). We
also have a set of users with diverse interests. Assume that user u is
interested in a subset I_u of topics. Each user u is also associated with a
positive integer K_u, which indicates that u can be satisfied by any K_u topics
in I_u. Each document s contains information for a subset C_s of topics. The
objective is to pick one document at a time such that the average satisfying
time is minimized, where a user's satisfying time is the first time that at
least K_u topics in I_u are covered in the documents selected so far. Our main
result is an O({\rho})-approximation algorithm for the problem, where {\rho} is
the algorithmic integrality gap of the linear programming relaxation of the set
cover instance defined by the documents and topics. This result generalizes the
constant approximations for generalized min-sum set cover and ranking with
unrelated intents and the logarithmic approximation for the problem of ranking
with submodular valuations (when the submodular function is the coverage
function), and can be seen as an interpolation between these results. We
further extend our model to the case when each user may interest in more than
one sets of topics and when the user's valuation function is XOS, and obtain
similar results for these models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4519</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4519</id><created>2013-07-17</created><authors><author><keyname>Pieris</keyname><forenames>Dhammika</forenames></author></authors><title>Extending the ER Model to relational Model novel transformation
  Algorithm: transforming relationship Types among Subtypes</title><categories>cs.DB</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach for creating ER conceptual models and an algorithm for
transforming them to the relational model has been developed by modifying and
extending the existing methods. A part of the new algorithm has previously been
presented. This paper presents the rest of the algorithm. One of the objectives
of this paper is to use it as a supportive document for ongoing empirical
evaluations of the new approach being conducted using the cognitive engagement
method and with the participation of different segments of the field as
respondents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4521</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4521</id><created>2013-07-17</created><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Bottleneck combinatorial optimization problems with uncertain costs and
  the OWA criterion</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1305.5339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a class of bottleneck combinatorial optimization problems with
uncertain costs is discussed. The uncertainty is modeled by specifying a
discrete scenario set containing a finite number of cost vectors, called
scenarios. In order to choose a solution the Ordered Weighted Averaging
aggregation operator (shortly OWA) is applied. The OWA operator generalizes
traditional criteria in decision making under uncertainty such as the maximum,
minimum, average, median, or Hurwicz criterion. New complexity and
approximation results in this area are provided. These results are general and
remain valid for many problems, in particular for a wide class of network
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4531</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4531</id><created>2013-07-17</created><authors><author><keyname>Mikians</keyname><forenames>Jakub</forenames></author><author><keyname>Gyarmati</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Erramilli</keyname><forenames>Vijay</forenames></author><author><keyname>Laoutaris</keyname><forenames>Nikolaos</forenames></author></authors><title>Crowd-assisted Search for Price Discrimination in E-Commerce: First
  results</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4532</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4532</id><created>2013-07-17</created><updated>2013-09-27</updated><authors><author><keyname>Ezerman</keyname><forenames>Martianus Frederic</forenames></author><author><keyname>Jitman</keyname><forenames>Somphong</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Xing-Ling Codes, Duals of their Subcodes, and Good Asymmetric Quantum
  Codes</title><categories>cs.IT math.IT</categories><comments>To appear in Designs, Codes and Cryptography (accepted Sep. 27, 2013)</comments><msc-class>81P45, 81P70, 94B05</msc-class><doi>10.1007/s10623-013-9885-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of powerful $q$-ary linear polynomial codes originally proposed by
Xing and Ling is deployed to construct good asymmetric quantum codes via the
standard CSS construction. Our quantum codes are $q$-ary block codes that
encode $k$ qudits of quantum information into $n$ qudits and correct up to
$\flr{(d_{x}-1)/2}$ bit-flip errors and up to $\flr{(d_{z}-1)/2}$ phase-flip
errors.. In many cases where the length $(q^{2}-q)/2 \leq n \leq (q^{2}+q)/2$
and the field size $q$ are fixed and for chosen values of $d_{x} \in
\{2,3,4,5\}$ and $d_{z} \ge \delta$, where $\delta$ is the designed distance of
the Xing-Ling (XL) codes, the derived pure $q$-ary asymmetric quantum CSS codes
possess the best possible size given the current state of the art knowledge on
the best classical linear block codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4538</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4538</id><created>2013-07-17</created><authors><author><keyname>Sacerdote</keyname><forenames>Laura</forenames></author><author><keyname>Garetto</keyname><forenames>Michele</forenames></author><author><keyname>Polito</keyname><forenames>Federico</forenames></author><author><keyname>Sereno</keyname><forenames>Matteo</forenames></author></authors><title>Superprocesses as models for information dissemination in the Future
  Internet</title><categories>cs.NI math.PR</categories><journal-ref>Proceedings of Mathematical Models and Methods for Planet Earth,
  157-170, Springer, 2014</journal-ref><doi>10.1007/978-3-319-02657-2_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future Internet will be composed by a tremendous number of potentially
interconnected people and devices, offering a variety of services, applications
and communication opportunities. In particular, short-range wireless
communications, which are available on almost all portable devices, will enable
the formation of the largest cloud of interconnected, smart computing devices
mankind has ever dreamed about: the Proximate Internet. In this paper, we
consider superprocesses, more specifically super Brownian motion, as a suitable
mathematical model to analyse a basic problem of information dissemination
arising in the context of Proximate Internet. The proposed model provides a
promising analytical framework to both study theoretical properties related to
the information dissemination process and to devise efficient and reliable
simulation schemes for very large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4541</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4541</id><created>2013-07-17</created><authors><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Cheng</keyname><forenames>Baisong</forenames></author><author><keyname>Zhao</keyname><forenames>Zhuang</forenames></author><author><keyname>Li</keyname><forenames>Daqing</forenames></author><author><keyname>Lu</keyname><forenames>Guangquan</forenames></author><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Xiao</keyname><forenames>Jinghua</forenames></author></authors><title>The resilience of interdependent transportation networks under targeted
  attack</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures</comments><doi>10.1209/0295-5075/103/68005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern world builds on the resilience of interdependent infrastructures
characterized as complex networks. Recently, a framework for analysis of
interdependent networks has been developed to explain the mechanism of
resilience in interdependent networks. Here we extend this interdependent
network model by considering flows in the networks and study the system's
resilience under different attack strategies. In our model, nodes may fail due
to either overload or loss of interdependency. Under the interaction between
these two failure mechanisms, it is shown that interdependent scale-free
networks show extreme vulnerability. The resilience of interdependent SF
networks is found in our simulation much smaller than single SF network or
interdependent SF networks without flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4543</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4543</id><created>2013-07-17</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author></authors><title>Complete Solutions for a Combinatorial Puzzle in Linear Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a single player game consisting of $n$ black checkers
and $m$ white checkers, called shifting the checkers. We have proved that the
minimum number of steps needed to play the game for general $n$ and $m$ is $nm
+ n + m$. We have also presented an optimal algorithm to generate an optimal
move sequence of the game consisting of $n$ black checkers and $m$ white
checkers, and finally, we present an explicit solution for the general game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4554</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4554</id><created>2013-07-17</created><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>Creative Telescoping for Holonomic Functions</title><categories>cs.SC</categories><comments>Tutorial and survey article, 24 pages</comments><journal-ref>In Carsten Schneider and Johannes Bluemlein (eds.): Computer
  Algebra in Quantum Field Theory: Integration, Summation and Special
  Functions. Texts &amp; Monographs in Symbolic Computation, Springer-Verlag Wien
  2013</journal-ref><doi>10.1007/978-3-7091-1616-6_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this article is twofold: on the one hand it is intended to serve
as a gentle introduction to the topic of creative telescoping, from a practical
point of view; for this purpose its application to several problems is
exemplified. On the other hand, this chapter has the flavour of a survey
article: the developments in this area during the last two decades are sketched
and a selection of references is compiled in order to highlight the impact of
creative telescoping in numerous contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4564</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4564</id><created>2013-07-17</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>From Bandits to Experts: A Tale of Domination and Independence</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the partial observability model for multi-armed bandits,
introduced by Mannor and Shamir. Our main result is a characterization of
regret in the directed observability model in terms of the dominating and
independence numbers of the observability graph. We also show that in the
undirected case, the learner can achieve optimal regret without even accessing
the observability graph before selecting an action. Both results are shown
using variants of the Exp3 algorithm operating on the observability graph in a
time-efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4566</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4566</id><created>2013-07-17</created><authors><author><keyname>Tschaikowski</keyname><forenames>Max</forenames></author><author><keyname>Tribastone</keyname><forenames>Mirco</forenames></author></authors><title>Spatial Fluid Limits for Stochastic Mobile Networks</title><categories>cs.NI cs.NA cs.PF math.PR</categories><msc-class>68M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers models of large-scale networks where nodes are
characterized by a set of states describing their local behavior, and by an
explicit mobility model over a two-dimensional lattice. A stochastic model is
given as a Markov population process that is, in general, infeasible to analyze
due to the massive state space sizes involved. Building on recent results on
fluid approximation, such a process admits a limit behavior as a system of
ordinary differential equations, whose size is, unfortunately, dependent on the
number of points in the lattice. Assuming an unbiased random walk model of
nodes mobility, we prove convergence of the stochastic process to the solution
of a system of partial differential equations of reaction-diffusion type. This
provides a macroscopic view of the model which becomes independent of the
lattice granularity, by approximating inherently discrete stochastic movements
with continuous, deterministic diffusions. We illustrate the practical
applicability of this result by modeling a network of mobile nodes with on/off
behavior performing file transfers with connectivity to 802.11 access points. A
numerical validation shows high quality of the approximation even for low
populations and coarse lattices, and excellent speed of convergence with
increasing system sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4567</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4567</id><created>2013-07-17</created><authors><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard</forenames></author><author><keyname>Weiland</keyname><forenames>Michele</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Guo</keyname><forenames>Xiaohu</forenames></author><author><keyname>Southern</keyname><forenames>James</forenames></author></authors><title>Benchmarking mixed-mode PETSc performance on high-performance
  architectures</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.5275</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trend towards highly parallel multi-processing is ubiquitous in all
modern computer architectures, ranging from handheld devices to large-scale HPC
systems; yet many applications are struggling to fully utilise the multiple
levels of parallelism exposed in modern high-performance platforms. In order to
realise the full potential of recent hardware advances, a mixed-mode between
shared-memory programming techniques and inter-node message passing can be
adopted which provides high-levels of parallelism with minimal overheads. For
scientific applications this entails that not only the simulation code itself,
but the whole software stack needs to evolve. In this paper, we evaluate the
mixed-mode performance of PETSc, a widely used scientific library for the
scalable solution of partial differential equations. We describe the addition
of OpenMP threaded functionality to the library, focusing on sparse
matrix-vector multiplication. We highlight key challenges in achieving good
parallel performance, such as explicit communication overlap using task-based
parallelism, and show how to further improve performance by explicitly load
balancing threads within MPI processes. Using a set of matrices extracted from
Fluidity, a CFD application code which uses the library as its linear solver
engine, we then benchmark the parallel performance of mixed-mode PETSc across
multiple nodes on several modern HPC architectures. We evaluate the parallel
scalability on Uniform Memory Access (UMA) systems, such as the Fujitsu
PRIMEHPC FX10 and IBM BlueGene/Q, as well as a Non-Uniform Memory Access (NUMA)
Cray XE6 platform. A detailed comparison is performed which highlights the
characteristics of each particular architecture, before demonstrating efficient
strong scalability of sparse matrix-vector multiplication with significant
speedups over the pure-MPI mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4579</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4579</id><created>2013-07-17</created><authors><author><keyname>Zhao</keyname><forenames>Yunbin</forenames></author></authors><title>RSP-Based Analysis for Sparsest and Least $\ell_1$-Norm Solutions to
  Underdetermined Linear Systems</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2013.2281030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the worse-case analysis, probabilistic analysis and empirical
justification have been employed to address the fundamental question: When does
$\ell_1$-minimization find the sparsest solution to an underdetermined linear
system? In this paper, a deterministic analysis, rooted in the classic linear
programming theory, is carried out to further address this question. We first
identify a necessary and sufficient condition for the uniqueness of least
$\ell_1$-norm solutions to linear systems. From this condition, we deduce that
a sparsest solution coincides with the unique least $\ell_1$-norm solution to a
linear system if and only if the so-called \emph{range space property} (RSP)
holds at this solution. This yields a broad understanding of the relationship
between $\ell_0$- and $\ell_1$-minimization problems. Our analysis indicates
that the RSP truly lies at the heart of the relationship between these two
problems. Through RSP-based analysis, several important questions in this field
can be largely addressed. For instance, how to efficiently interpret the gap
between the current theory and the actual numerical performance of
$\ell_1$-minimization by a deterministic analysis, and if a linear system has
multiple sparsest solutions, when does $\ell_1$-minimization guarantee to find
one of them? Moreover, new matrix properties (such as the \emph{RSP of order
$K$} and the \emph{Weak-RSP of order $K$}) are introduced in this paper, and a
new theory for sparse signal recovery based on the RSP of order $K$ is
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4581</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4581</id><created>2013-07-17</created><updated>2014-04-09</updated><authors><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Zhou</keyname><forenames>Yipeng</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>Smart Streaming for Online Video Services</title><categories>cs.MM cs.PF</categories><comments>This paper has been updated after checking the possible issues</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Bandwidth consumption is a significant concern for online video service
providers. Practical video streaming systems usually use some form of HTTP
streaming (progressive download) to let users download the video at a faster
rate than the video bitrate. Since users may quit before viewing the complete
video, however, much of the downloaded video will be &quot;wasted&quot;. To the extent
that users' departure behavior can be predicted, we develop smart streaming
that can be used to improve user QoE with limited server bandwidth or save
bandwidth cost with unlimited server bandwidth. Through measurement, we extract
certain user behavior properties for implementing such smart streaming, and
demonstrate its advantage using prototype implementation as well as
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4585</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4585</id><created>2013-07-17</created><authors><author><keyname>Terepeta</keyname><forenames>Michal</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author></authors><title>Pushdown Systems for Monotone Frameworks</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotone frameworks is one of the most successful frameworks for
intraprocedural data flow analysis extending the traditional class of bitvector
frameworks (like live variables and available expressions). Weighted pushdown
systems is similarly one of the most general frameworks for interprocedural
analysis of programs. However, it makes use of idempotent semirings to
represent the sets of properties and unfortunately they do not admit analyses
whose transfer functions are not strict (e.g., classical bitvector frameworks).
This motivates the development of algorithms for backward and forward
reachability of pushdown systems using sets of properties forming so-called
flow algebras that weaken some of the assumptions of idempotent semirings. In
particular they do admit the bitvector frameworks, monotone frameworks, as well
as idempotent semirings. We show that the algorithms are sound under mild
assumptions on the flow algebras, mainly that the set of properties constitutes
a join semi-lattice, and complete provided that the transfer functions are
suitably distributive (but not necessarily strict).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4592</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4592</id><created>2013-07-17</created><authors><author><keyname>Fehrenbach</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Weiss</keyname><forenames>Pierre</forenames></author></authors><title>Processing stationary noise: model and parameter selection in
  variational methods</title><categories>cs.CV math.OC stat.AP</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive or multiplicative stationary noise recently became an important
issue in applied fields such as microscopy or satellite imaging. Relatively few
works address the design of dedicated denoising methods compared to the usual
white noise setting. We recently proposed a variational algorithm to tackle
this issue. In this paper, we analyze this problem from a statistical point of
view and provide deterministic properties of the solutions of the associated
variational problems. In the first part of this work, we demonstrate that in
many practical problems, the noise can be assimilated to a colored Gaussian
noise. We provide a quantitative measure of the distance between a stationary
process and the corresponding Gaussian process. In the second part, we focus on
the Gaussian setting and analyze denoising methods which consist of minimizing
the sum of a total variation term and an $l^2$ data fidelity term. While the
constrained formulation of this problem allows to easily tune the parameters,
the Lagrangian formulation can be solved more efficiently since the problem is
strongly convex. Our second contribution consists in providing analytical
values of the regularization parameter in order to approximately satisfy
Morozov's discrepancy principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4610</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4610</id><created>2013-07-17</created><authors><author><keyname>Chahid</keyname><forenames>Makhlad</forenames></author><author><keyname>Bobin</keyname><forenames>Jerome</forenames></author><author><keyname>Mousavi</keyname><forenames>Hamed Shams</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Dahan</keyname><forenames>Maxime</forenames></author><author><keyname>Studer</keyname><forenames>Vincent</forenames></author></authors><title>Hyperspectral fluorescence microscopy based on Compressive Sampling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mathematical theory of compressed sensing (CS) asserts that one can
acquire signals from measurements whose rate is much lower than the total
bandwidth. Whereas the CS theory is now well developed, challenges concerning
hardware implementations of CS-based acquisition devices-especially in
optics-have only started being addressed. This paper presents an implementation
of compressive sensing in fluorescence microscopy and its applications to
biomedical imaging. Our CS microscope combines a dynamic structured wide-field
illumination and a fast and sensitive single-point fluorescence detection to
enable reconstructions of images of fluorescent beads, cells, and tissues with
undersampling ratios (between the number of pixels and number of measurements)
up to 32. We further demonstrate a hyperspectral mode and record images with
128 spectral channels and undersampling ratios up to 64, illustrating the
potential benefits of CS acquisition for higher-dimensional signals, which
typically exhibits extreme redundancy. Altogether, our results emphasize the
interest of CS schemes for acquisition at a significantly reduced rate and
point to some remaining challenges for CS fluorescence microscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4612</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4612</id><created>2013-07-17</created><updated>2013-11-22</updated><authors><author><keyname>Wang</keyname><forenames>Taotao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Joint Channel Estimation and Channel Decoding in Physical-Layer Network
  Coding Systems: An EM-BP Factor Graph Framework</title><categories>cs.IT math.IT</categories><comments>17 pages,15 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of joint channel estimation and channel
decoding in physical-layer network coding (PNC) systems. In PNC, multiple users
transmit to a relay simultaneously. PNC channel decoding is different from
conventional multi-user channel decoding: specifically, the PNC relay aims to
decode a network-coded message rather than the individual messages of the
users. Although prior work has shown that PNC can significantly improve the
throughput of a relay network, the improvement is predicated on the
availability of accurate channel estimates. Channel estimation in PNC, however,
can be particularly challenging because of 1) the overlapped signals of
multiple users; 2) the correlations among data symbols induced by channel
coding; and 3) time-varying channels. We combine the expectation-maximization
(EM) algorithm and belief propagation (BP) algorithm on a unified factor-graph
framework to tackle these challenges. In this framework, channel estimation is
performed by an EM subgraph, and channel decoding is performed by a BP subgraph
that models a virtual encoder matched to the target of PNC channel decoding.
Iterative message passing between these two subgraphs allow the optimal
solutions for both to be approached progressively. We present extensive
simulation results demonstrating the superiority of our PNC receivers over
other PNC receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4629</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4629</id><created>2013-07-17</created><authors><author><keyname>Groshaus</keyname><forenames>Marina</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Stacho</keyname><forenames>Juraj</forenames></author></authors><title>On edge-sets of bicliques in graphs</title><categories>math.CO cs.DM</categories><comments>This version corrects an error in Theorem 11 found after the paper
  went into print</comments><journal-ref>Discrete Applied Mathematics 160 (2012), pp. 2698-2708</journal-ref><doi>10.1016/j.dam.2012.02.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A biclique is a maximal induced complete bipartite subgraph of a graph. We
investigate the intersection structure of edge-sets of bicliques in a graph.
Specifically, we study the associated edge-biclique hypergraph whose hyperedges
are precisely the edge-sets of all bicliques. We characterize graphs whose
edge-biclique hypergraph is conformal (i.e., it is the clique hypergraph of its
2-section) by means of a single forbidden induced obstruction, the triangular
prism. Using this result, we characterize graphs whose edge-biclique hypergraph
is Helly and provide a polynomial time recognition algorithm. We further study
a hereditary version of this property and show that it also admits polynomial
time recognition, and, in fact, is characterized by a finite set of forbidden
induced subgraphs. We conclude by describing some interesting properties of the
2-section graph of the edge-biclique hypergraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4635</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4635</id><created>2013-07-17</created><authors><author><keyname>Desouter</keyname><forenames>Benoit</forenames></author><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author></authors><title>Integrating Datalog and Constraint Solving</title><categories>cs.PL cs.DB</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LP is a common formalism for the field of databases and CSP, both at the
theoretical level and the implementation level in the form of Datalog and CLP.
In the past, close correspondences have been made between both fields at the
theoretical level. Yet correspondence at the implementation level has been much
less explored. In this article we work towards relating them at the
implementation level. Concretely, we show how to derive the efficient Leapfrog
Triejoin execution algorithm of Datalog from a generic CP execution scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4641</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4641</id><created>2013-07-17</created><authors><author><keyname>Munera</keyname><forenames>Danny</forenames></author><author><keyname>Diaz</keyname><forenames>Daniel</forenames></author><author><keyname>Abreu</keyname><forenames>Salvador</forenames></author></authors><title>Experimenting with X10 for Parallel Constraint-Based Local Search</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we have investigated the adequacy of the PGAS parallel
language X10 to implement a Constraint-Based Local Search solver. We decided to
code in this language to benefit from the ease of use and architectural
independence from parallel resources which it offers. We present the
implementation strategy, in search of different sources of parallelism in the
context of an implementation of the Adaptive Search algorithm. We extensively
discuss the algorithm and its implementation. The performance evaluation on a
representative set of benchmarks shows close to linear speed-ups, in all the
problems treated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4642</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4642</id><created>2013-07-17</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>A Prolog Specification of Giant Number Arithmetic</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tree based representation described in this paper, hereditarily binary
numbers, applies recursively a run-length compression mechanism that enables
computations limited by the structural complexity of their operands rather than
by their bitsizes. While within constant factors from their traditional
counterparts for their worst case behavior, our arithmetic operations open the
doors for interesting numerical computations, impossible with traditional
number representations. We provide a complete specification of our algorithms
in the form of a purely declarative Prolog program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4644</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4644</id><created>2013-07-17</created><authors><author><keyname>Warren</keyname><forenames>David S.</forenames></author></authors><title>Interning Ground Terms in XSB</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an implementation of interning of ground terms in the XSB
Tabled Prolog system. This is related to the idea of hash-consing. I describe
the concept of interning atoms and discuss the issues around interning ground
structured terms, motivating why tabling Prolog systems may change the
cost-benefit tradeoffs from those of traditional Prolog systems. I describe the
details of the implementation of interning ground terms in the XSB Tabled
Prolog System and show some of its performance properties. This implementation
achieves the effects of that of Zhou and Have but is tuned for XSB's
representations and is arguably simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4648</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4648</id><created>2013-07-17</created><authors><author><keyname>Hanus</keyname><forenames>Michael</forenames></author><author><keyname>Reck</keyname><forenames>Fabian</forenames></author></authors><title>A Generic Analysis Server System for Functional Logic Programs</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system, called CASS, for the analysis of functional logic
programs. The system is generic so that various kinds of analyses (e.g.,
groundness, non-determinism, demanded arguments) can be easily integrated. In
order to analyze larger applications consisting of dozens or hundreds of
modules, CASS supports a modular and incremental analysis of programs.
Moreover, it can be used by different programming tools, like documentation
generators, analysis environments, program optimizers, as well as Eclipse-based
development environments. For this purpose, CASS can also be invoked as a
server system to get a language-independent access to its functionality. CASS
is completely implemented in the functional logic language Curry as a
master/worker architecture to exploit parallel or distributed execution
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4653</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4653</id><created>2013-07-17</created><authors><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author></authors><title>A New Convex Relaxation for Tensor Completion</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning a tensor from a set of linear measurements.
A prominent methodology for this problem is based on a generalization of trace
norm regularization, which has been used extensively for learning low rank
matrices, to the tensor setting. In this paper, we highlight some limitations
of this approach and propose an alternative convex relaxation on the Euclidean
ball. We then describe a technique to solve the associated regularization
problem, which builds upon the alternating direction method of multipliers.
Experiments on one synthetic dataset and two real datasets indicate that the
proposed method improves significantly over tensor trace norm regularization in
terms of estimation error, while remaining computationally tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4655</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4655</id><created>2013-07-17</created><authors><author><keyname>St&#xe9;phan</keyname><forenames>Igor</forenames></author></authors><title>Compilation for QCSP</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this article a framework for compilation of quantified
constraint satisfaction problems (QCSP). We establish the semantics of this
formalism by an interpretation to a QCSP. We specify an algorithm to compile a
QCSP embedded into a search algorithm and based on the inductive semantics of
QCSP. We introduce an optimality property and demonstrate the optimality of the
interpretation of the compiled QCSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4677</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4677</id><created>2013-07-17</created><authors><author><keyname>Audoux</keyname><forenames>Benjamin</forenames></author></authors><title>An application of Khovanov homology to quantum codes</title><categories>cs.IT math.GT math.IT quant-ph</categories><comments>20 pages</comments><msc-class>94B99, 57M27</msc-class><doi>10.4171/AIHPD/6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Khovanov homology to define families of LDPC quantum error-correcting
codes: unknot codes with asymptotical parameters
[[3^(2l+1)/sqrt(8{\pi}l);1;2^l]]; unlink codes with asymptotical parameters
[[sqrt(2/2{\pi}l)6^l;2^l;2^l]] and (2,l)-torus link codes with asymptotical
parameters [[n;1;d_n]] where d_n&gt;\sqrt(n)/1.62.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4682</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4682</id><created>2013-07-17</created><updated>2013-10-24</updated><authors><author><keyname>Bilkova</keyname><forenames>Marta</forenames><affiliation>Institute of Computer Science, Academy of Sciences of the Czech Republic</affiliation></author><author><keyname>Kurz</keyname><forenames>Alexander</forenames><affiliation>University of Leicester, UK</affiliation></author><author><keyname>Petrisan</keyname><forenames>Daniela</forenames><affiliation>University of Leicester, UK</affiliation></author><author><keyname>Velebil</keyname><forenames>Jiri</forenames><affiliation>Faculty of Electrical Engineering, Czech Technical University in Prague, Czech R</affiliation></author></authors><title>Relation lifting, with an application to the many-valued cover modality</title><categories>cs.LO</categories><comments>48 pages, accepted for publication in LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (October
  25, 2013) lmcs:742</journal-ref><doi>10.2168/LMCS-9(4:8)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce basic notions and results about relation liftings on categories
enriched in a commutative quantale. We derive two necessary and sufficient
conditions for a 2-functor T to admit a functorial relation lifting: one is the
existence of a distributive law of T over the &quot;powerset monad&quot; on categories,
one is the preservation by T of &quot;exactness&quot; of certain squares. Both
characterisations are generalisations of the &quot;classical&quot; results known for set
functors: the first characterisation generalises the existence of a
distributive law over the genuine powerset monad, the second generalises
preservation of weak pullbacks. The results presented in this paper enable us
to compute predicate liftings of endofunctors of, for example, generalised
(ultra)metric spaces. We illustrate this by studying the coalgebraic cover
modality in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4685</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4685</id><created>2013-07-17</created><updated>2013-09-20</updated><authors><author><keyname>Johnson</keyname><forenames>Samuel</forenames></author><author><keyname>Dominguez-Garcia</keyname><forenames>Virginia</forenames></author><author><keyname>Munoz</keyname><forenames>Miguel A.</forenames></author></authors><title>Factors determining nestedness in complex networks</title><categories>physics.soc-ph cs.SI q-bio.MN q-bio.NC</categories><comments>7 pages, 4 figures</comments><journal-ref>PLoS ONE 8(9): e74025. 2013</journal-ref><doi>10.1371/journal.pone.0074025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the causes and effects of network structural features is a key
task in deciphering complex systems. In this context, the property of network
nestedness has aroused a fair amount of interest as regards ecological
networks. Indeed, Bastolla et al. introduced a simple measure of network
nestedness which opened the door to analytical understanding, allowing them to
conclude that biodiversity is strongly enhanced in highly nested mutualistic
networks. Here, we suggest a slightly refined version of such a measure and go
on to study how it is influenced by the most basic structural properties of
networks, such as degree distribution and degree-degree correlations (i.e.
assortativity). We find that heterogeneity in the degree has a very strong
influence on nestedness. Once such an influence has been discounted, we find
that nestedness is strongly correlated with disassortativity and hence, as
random (neutral) networks have been recently found to be naturally
disassortative, they tend to be naturally nested just as the result of chance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4689</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4689</id><created>2013-07-17</created><authors><author><keyname>Di Liberto</keyname><forenames>Giovanni</forenames></author><author><keyname>Kadioglu</keyname><forenames>Serdar</forenames></author><author><keyname>Leo</keyname><forenames>Kevin</forenames></author><author><keyname>Malitsky</keyname><forenames>Yuri</forenames></author></authors><title>DASH: Dynamic Approach for Switching Heuristics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complete tree search is a highly effective method for tackling MIP problems,
and over the years, a plethora of branching heuristics have been introduced to
further refine the technique for varying problems. Recently, portfolio
algorithms have taken the process a step further, trying to predict the best
heuristic for each instance at hand. However, the motivation behind algorithm
selection can be taken further still, and used to dynamically choose the most
appropriate algorithm for each encountered subproblem. In this paper we
identify a feature space that captures both the evolution of the problem in the
branching tree and the similarity among subproblems of instances from the same
MIP models. We show how to exploit these features to decide the best time to
switch the branching heuristic and then show how such a system can be trained
efficiently. Experiments on a highly heterogeneous collection of MIP instances
show significant gains over the pure algorithm selection approach that for a
given instance uses only a single heuristic throughout the search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4695</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4695</id><created>2013-07-17</created><authors><author><keyname>Guinea</keyname><forenames>Alejandro Sanchez</forenames></author></authors><title>A Design Methodology for Software Measurement Programs</title><categories>cs.SE</categories><comments>Submitted to IEEE Transactions on Software Engineering</comments><acm-class>D.2; D.2.8; D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software measurement programs have emerged as compounds of several
measurement activities that are pursued as part of a combined effort of several
parties within a software organization, based on interests that the
organization has regarding the assessment of the different elements that
intervene in the development of software.
  This paper recognizes design of measurement programs as an essential activity
that, up until now, has been studied extensively, however, only in what
respects to the content of the programs. In addition, proper specification for
this kind of programs, accounting for preciseness and unambiguity, to
facilitate maintenance, evolution, and execution has not been thoroughly
considered. A methodology for designing programs that embody these and some
other desirable features is presented. The methodology is built in solid
ground. From software measurement literature, a goal-oriented approach is
considered for building the content of the program. On the other hand, a
successful technique from software development as modularization is utilized to
give coherent structure to the measurement program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4700</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4700</id><created>2013-07-17</created><authors><author><keyname>Carrillo</keyname><forenames>Rafael E.</forenames></author><author><keyname>Barner</keyname><forenames>Kenneth E.</forenames></author></authors><title>Lorentzian Iterative Hard Thresholding: Robust Compressed Sensing with
  Prior Information</title><categories>cs.IT math.IT</categories><comments>28 pages, 9 figures, accepted in IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, Vol. 61, No. 19, pp 4822 -
  4833. Oct. 2013</journal-ref><doi>10.1109/TSP.2013.2274275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commonly employed reconstruction algorithms in compressed sensing (CS) use
the $L_2$ norm as the metric for the residual error. However, it is well-known
that least squares (LS) based estimators are highly sensitive to outliers
present in the measurement vector leading to a poor performance when the noise
no longer follows the Gaussian assumption but, instead, is better characterized
by heavier-than-Gaussian tailed distributions. In this paper, we propose a
robust iterative hard Thresholding (IHT) algorithm for reconstructing sparse
signals in the presence of impulsive noise. To address this problem, we use a
Lorentzian cost function instead of the $L_2$ cost function employed by the
traditional IHT algorithm. We also modify the algorithm to incorporate prior
signal information in the recovery process. Specifically, we study the case of
CS with partially known support. The proposed algorithm is a fast method with
computational load comparable to the LS based IHT, whilst having the advantage
of robustness against heavy-tailed impulsive noise. Sufficient conditions for
stability are studied and a reconstruction error bound is derived. We also
derive sufficient conditions for stable sparse signal recovery with partially
known support. Theoretical analysis shows that including prior support
information relaxes the conditions for successful reconstruction. Simulation
results demonstrate that the Lorentzian-based IHT algorithm significantly
outperform commonly employed sparse reconstruction techniques in impulsive
environments, while providing comparable performance in less demanding,
light-tailed environments. Numerical results also demonstrate that the
partially known support inclusion improves the performance of the proposed
algorithm, thereby requiring fewer samples to yield an approximate
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4716</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4716</id><created>2013-07-17</created><updated>2013-08-04</updated><authors><author><keyname>Bahrami</keyname><forenames>Mehdi</forenames></author></authors><title>Cloud Template, a Big Data Solution</title><categories>cs.DC cs.NI cs.SE cs.SI</categories><comments>JSCSE Journal Publication</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 2, pp. 13-16, 2013</journal-ref><doi>10.7321/jscse.v3.n2.2</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Today cloud computing has become as a new concept for hosting and delivering
different services over the Internet for big data solutions. Cloud computing is
attractive to different business owners of both small and enterprise as it
eliminates the requirement for users to plan ahead for provisioning, and allows
enterprises to start from the small and increase resources only when there is a
rise in service demand. Despite the fact that cloud computing offers huge
opportunities to the IT industry, the development of cloud computing technology
is currently has several issues. This study presents an idea for introducing
cloud templates which will be used for analyzing, designing, developing and
implementing cloud computing systems. We will present a template based design
for cloud computing systems, highlighting its key concepts, architectural
principles and state of the art implementation, as well as research challenges
and future work requirements. The aim of this idea is to provide a better
understanding of the design challenges of cloud computing and identify
important research directions in this big data increasingly important area. We
will describe a series of studies by which we and other researchers have
assessed the effectiveness of these techniques in practical situations.
Finally, in this study we will show how this idea could be implemented in a
practical and useful way in industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4717</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4717</id><created>2013-07-17</created><authors><author><keyname>Dharani</keyname><forenames>T.</forenames></author><author><keyname>Aroquiaraj</keyname><forenames>I. Laurence</forenames></author></authors><title>Content Based Image Retrieval System using Feature Classification with
  Modified KNN Algorithm</title><categories>cs.CV</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature means countenance, remote sensing scene objects with similar
characteristics, associated to interesting scene elements in the image
formation process. They are classified into three types in image processing,
that is low, middle and high. Low level features are color, texture and middle
level feature is shape and high level feature is semantic gap of objects. An
image retrieval system is a computer system for browsing, searching and
retrieving images from a large image database. Content Based Image Retrieval is
a technique which uses visual features of image such as color, shape, texture
to search user required image from large image database according to user
requests in the form of a query. MKNN is an enhancing method of KNN. The
proposed KNN classification is called MKNN. MKNN contains two parts for
processing, they are validity of the train samples and applying weighted KNN.
The validity of each point is computed according to its neighbors. In our
proposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNN
so that the query label is approximated by weighting the neighbors of the
query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4731</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4731</id><created>2013-07-17</created><authors><author><keyname>Kelly</keyname><forenames>Jesse</forenames></author><author><keyname>Ghattas</keyname><forenames>Omar</forenames></author><author><keyname>Sundar</keyname><forenames>Hari</forenames></author></authors><title>A Nested Partitioning Scheme for Parallel Heterogeneous Clusters</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern supercomputers are increasingly requiring the presence of accelerators
and co-processors. However, it has not been easy to achieve good performance on
such heterogeneous clusters. The key challenge has been to ensure good load
balance and that neither the CPU nor the accelerator is left idle. Traditional
approaches have offloaded entire computations to the accelerator, resulting in
an idle CPU, or have opted for task-level parallelism requiring large data
transfers between the CPU and the accelerator. True work-parallelism has been
hard as the Accelerators cannot directly communicate with other CPUs (besides
the host) and Accelerators. In this work, we present a new nested partition
scheme to overcome this problem. By partitioning the work assignment on a given
node asymmetrically into boundary and interior work, and assigning the interior
to the accelerator, we are able to achieve excellent efficiency while ensure
proper utilization of both the CPU and Accelerator resources. The problem used
for evaluating the new partition is an $hp$ discontinuous Galerkin spectral
element method for a coupled elastic--acoustic wave propagation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4733</identifier>
 <datestamp>2015-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4733</id><created>2013-07-17</created><authors><author><keyname>Mohiuddin</keyname><forenames>Maaz M.</forenames></author><author><keyname>Maheshwari</keyname><forenames>Varun</forenames></author><author><keyname>V.</keyname><forenames>Sreejith T.</forenames></author><author><keyname>Kuchi</keyname><forenames>Kiran</forenames></author><author><keyname>Sharma</keyname><forenames>G. V. V.</forenames></author><author><keyname>Emami</keyname><forenames>Shahriar</forenames></author></authors><title>Performance Limits of a Cloud Radio</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation in a cellular network is seen as a key technique in managing
other cell interference to observe a gain in achievable rate. In this paper, we
present the achievable rate regions for a cloud radio network using a
sub-optimal zero forcing equalizer with dirty paper precoding. We show that
when complete channel state information is available at the cloud, rates close
to those achievable with total interference cancellation can be achieved. With
mean capacity gains, of up to 2 fold over the conventional cellular network in
both uplink and downlink, this precoding scheme shows great promise for
implementation in a cloud radio network. To simplify the analysis, we use a
stochastic geometric framework based of Poisson point processes instead of the
traditional grid based cellular network model.
  We also study the impact of limiting the channel state information and
geographical clustering to limit the cloud size on the achievable rate. We have
observed that using this zero forcing-dirty paper coding technique, the adverse
effect of inter-cluster interference can be minimized thereby transforming an
interference limited network into a noise limited network as experienced by an
average user in the network for low operating signal-to-noise-ratios. However,
for higher signal-to-noise-ratios, both the average achievable rate and
cell-edge achievable rate saturate as observed in literature. As the
implementation of dirty paper coding is practically not feasible, we present a
practical design of a cloud radio network using cloud a minimum mean square
equalizer for processing the uplink streams and use Tomlinson-Harashima
precoder as a sub-optimal substitute for a dirty paper precoder in downlink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4744</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4744</id><created>2013-07-06</created><updated>2014-07-27</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Nafie</keyname><forenames>Mohamed</forenames></author></authors><title>On the Coexistence of a Primary User with an Energy Harvesting Secondary
  User: A Case of Cognitive Cooperation</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in Wireless Communications &amp; Mobile
  Computing (WCMC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive scenario where an energy harvesting
secondary user (SU) shares the spectrum with a primary user (PU). The secondary
source helps the primary source in delivering its undelivered packets during
periods of silence of the primary source. The primary source has a queue for
storing its data packets, whereas the secondary source has two data queues; a
queue for storing its own packets and the other for storing the fraction of the
undelivered primary packets accepted for relaying. The secondary source is
assumed to be a battery-based node which harvests energy packets from the
environment. In addition to its data queues, the SU has an energy queue to
store the harvested energy packets. The secondary energy packets are used for
primary packets decoding and data packets transmission. More specifically, if
the secondary energy queue is empty, the secondary source can neither help the
primary source nor transmit a packet from the data queues. The energy queue is
modeled as a discrete time queue with Markov arrival and service processes. Due
to the interaction of the queues, we provide inner and outer bounds on the
stability region of the proposed system. We investigate the impact of the
energy arrival rate on the stability region. Numerical results show the
significant gain of cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4790</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4790</id><created>2013-07-17</created><authors><author><keyname>Matz</keyname><forenames>Gerald</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Time-Frequency Foundations of Communications</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures; to appear in the IEEE Signal Processing Magazine
  Special Issue on Time-Frequency Analysis and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the tradition of Gabor's 1946 landmark paper [1], we advocate a
time-frequency (TF) approach to communications. TF methods for communications
have been proposed very early (see the box History). While several tutorial
papers and book chapters on the topic are available (see, e.g., [2]-[4] and
references therein), the goal of this paper is to present the fundamental
aspects in a coherent and easily accessible manner. Specifically, we establish
the role of TF methods in communications across a range of subject areas
including TF dispersive channels, orthogonal frequency division multiplexing
(OFDM), information-theoretic limits, and system identification and channel
estimation. Furthermore, we present fundamental results that are stated in the
literature for the continuous-time case in simple linear algebra terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4798</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4798</id><created>2013-07-17</created><authors><author><keyname>Hodas</keyname><forenames>Nathan O.</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Attention and Visibility in an Information Rich World</title><categories>cs.SI nlin.AO physics.soc-ph</categories><comments>Appearing in 2nd International Workshop on Social Multimedia Research
  2013, in conjunction with IEEE International Conference on Multimedia &amp; Expo
  (ICME 2013)</comments><msc-class>91D30</msc-class><acm-class>J.4; D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the rate of content production grows, we must make a staggering number of
daily decisions about what information is worth acting on. For any flourishing
online social media system, users can barely keep up with the new content
shared by friends. How does the user-interface design help or hinder users'
ability to find interesting content? We analyze the choices people make about
which information to propagate on the social media sites Twitter and Digg. We
observe regularities in behavior which can be attributed directly to cognitive
limitations of humans, resulting from the different visibility policies of each
site. We quantify how people divide their limited attention among competing
sources of information, and we show how the user-interface design can mediate
information spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4799</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4799</id><created>2013-07-17</created><authors><author><keyname>Sengupta</keyname><forenames>Ayan</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Cooperative Relaying at Finite SNR -- Role of Quantize-Map-and-Forward</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications on 11th
  July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantize-Map-and-Forward (QMF) relaying has been shown to achieve the optimal
diversity-multiplexing trade-off (DMT) for arbitrary slow fading full-duplex
networks as well as for the single-relay half-duplex network. A key reason for
this is that quantizing at the noise level suffices to achieve the cut-set
bound approximately to within an additive gap, without any requirement of
instantaneous channel state information (CSI). However, DMT only captures the
high SNR performance and potentially, limited CSI at the relay can improve
performance at moderate SNRs. In this work we propose an optimization framework
for QMF relaying over slow fading channels. Focusing on vector Gaussian
quantizers, we optimize the outage probability for the full-duplex and
half-duplex single relay by finding the best quantization level and relay
schedule according to the available CSI at the relays. For the N-relay diamond
network, we derive an universal quantizer that sharpens the additive
approximation gap of QMF from the conventional \Theta(N) bits/s/Hz to
\Theta(log(N)) bits/s/Hz using only network topology information. Analytical
solutions to channel-aware optimal quantizers for two-relay and symmetric
N-relay diamond networks are also derived. In addition, we prove that suitable
hybridizations of our optimized QMF schemes with Decode-Forward (DF) or Dynamic
DF protocols provide significant finite SNR gains over the individual schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4801</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4801</id><created>2013-07-17</created><updated>2013-10-14</updated><authors><author><keyname>Khmou</keyname><forenames>Y.</forenames></author><author><keyname>Safi</keyname><forenames>S.</forenames></author></authors><title>Estimating 3D Signals with Kalman Filter</title><categories>cs.IT math.IT</categories><comments>8 pages, 9 figures and 1 Latex File</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the standard Kalman filter was implemented to denoise the
three dimensional signals affected by additive white Gaussian noise (AWGN), we
used fast algorithm based on Laplacian operator to measure the noise variance
and a fast median filter to predict the state variable. The Kalman algorithm is
modeled by adjusting its parameters for better performance in both filtering
and in reducing the computational load while conserving the information
contained in the signal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4815</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4815</id><created>2013-07-17</created><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Matyjas</keyname><forenames>John D.</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author></authors><title>Linear Precoder Design for MIMO Interference Channels with
  Finite-Alphabet Signaling</title><categories>cs.IT math.IT</categories><comments>15 pages, 13 figures, IEEE Transaction on Communications, accepted
  for publication</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper investigates the linear precoder design for $K$-user interference
channels of multiple-input multiple-output (MIMO) transceivers under finite
alphabet inputs. We first obtain general explicit expressions of the achievable
rate for users in the MIMO interference channel systems. We study optimal
transmission strategies in both low and high signal-to-noise ratio (SNR)
regions. Given finite alphabet inputs, we show that a simple power allocation
design achieves optimal performance at high SNR whereas the well-known
interference alignment technique for Gaussian inputs only utilizes a partial
interference-free signal space for transmission and leads to a constant rate
loss when applied naively to finite-alphabet inputs. Moreover, we establish
necessary conditions for the linear precoder design to achieve weighted
sum-rate maximization. We also present an efficient iterative algorithm for
determining precoding matrices of all the users. Our numerical results
demonstrate that the proposed iterative algorithm achieves considerably higher
sum-rate under practical QAM inputs than other known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4822</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4822</id><created>2013-07-17</created><authors><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>Outage Exponent: A Unified Performance Metric for Parallel Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>19 pages, 10 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 59, no. 3, March
  2013</journal-ref><doi>10.1109/TIT.2012.2227454</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel fading channel, which consists of finite number of subchannels,
is very important, because it can be used to formulate many practical
communication systems. The outage probability, on the other hand, is widely
used to analyze the relationship among the communication efficiency,
reliability, SNR, and channel fading. To the best of our knowledge, the
previous works only studied the asymptotic outage performance of the parallel
fading channel which are only valid for a large number of subchannels or high
SNRs. In this paper, a unified performance metric, which we shall refer to as
the outage exponent, will be proposed. Our approach is mainly based on the
large deviations theory and the Meijer's G-function. It is shown that the
proposed outage exponent is not only an accurate estimation of the outage
probability for any number of subchannels, any SNR, and any target transmission
rate, but also provides an easy way to compute the outage capacity, finite-SNR
diversity-multiplexing tradeoff, and SNR gain. The asymptotic performance
metrics, such as the delay-limited capacity, ergodic capacity, and
diversity-multiplexing tradeoff can be directly obtained by letting the number
of subchannels or SNR tends to infinity. Similar to Gallager's error exponent,
a reliable function for parallel fading channels, which illustrates a
fundamental relationship between the transmission reliability and efficiency,
can also be defined from the outage exponent. Therefore, the proposed outage
exponent provides a complete and comprehensive performance measure for parallel
fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4824</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4824</id><created>2013-07-17</created><authors><author><keyname>Elmehdwi</keyname><forenames>Yousef</forenames></author><author><keyname>Samanthula</keyname><forenames>Bharath K.</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author></authors><title>Secure k-Nearest Neighbor Query over Encrypted Data in Outsourced
  Environments</title><categories>cs.CR</categories><comments>23 pages, 8 figures, and 4 tables</comments><acm-class>D.4.6; E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past decade, query processing on relational data has been studied
extensively, and many theoretical and practical solutions to query processing
have been proposed under various scenarios. With the recent popularity of cloud
computing, users now have the opportunity to outsource their data as well as
the data management tasks to the cloud. However, due to the rise of various
privacy issues, sensitive data (e.g., medical records) need to be encrypted
before outsourcing to the cloud. In addition, query processing tasks should be
handled by the cloud; otherwise, there would be no point to outsource the data
at the first place. To process queries over encrypted data without the cloud
ever decrypting the data is a very challenging task. In this paper, we focus on
solving the k-nearest neighbor (kNN) query problem over encrypted database
outsourced to a cloud: a user issues an encrypted query record to the cloud,
and the cloud returns the k closest records to the user. We first present a
basic scheme and demonstrate that such a naive solution is not secure. To
provide better security, we propose a secure kNN protocol that protects the
confidentiality of the data, user's input query, and data access patterns.
Also, we empirically analyze the efficiency of our protocols through various
experiments. These results indicate that our secure protocol is very efficient
on the user end, and this lightweight scheme allows a user to use any mobile
device to perform the kNN query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4827</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4827</id><created>2013-07-18</created><authors><author><keyname>Poss</keyname><forenames>Raphael 'kena'</forenames></author></authors><title>Characterizing traits of coordination</title><categories>cs.PL</categories><comments>11 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can one recognize coordination languages and technologies? As this report
shows, the common approach that contrasts coordination with computation is
intellectually unsound: depending on the selected understanding of the word
&quot;computation&quot;, it either captures too many or too few programming languages.
Instead, we argue for objective criteria that can be used to evaluate how well
programming technologies offer coordination services. Of the various criteria
commonly used in this community, we are able to isolate three that are strongly
characterizing: black-box componentization, which we had identified previously,
but also interface extensibility and customizability of run-time optimization
goals. These criteria are well matched by Intel's Concurrent Collections and
AstraKahn, and also by OpenCL, POSIX and VMWare ESX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4839</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4839</id><created>2013-07-18</created><authors><author><keyname>Cordier</keyname><forenames>St&#xe9;phane</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Coullon</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>LIFO</affiliation></author><author><keyname>Delestre</keyname><forenames>Olivier</forenames><affiliation>JAD</affiliation></author><author><keyname>Laguerre</keyname><forenames>Christian</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Le</keyname><forenames>Minh Hoang</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Pierre</keyname><forenames>Daniel</forenames><affiliation>LAMFA, MAP5</affiliation></author><author><keyname>Sadaka</keyname><forenames>Georges</forenames><affiliation>LAMFA, MAP5</affiliation></author></authors><title>FullSWOF_Paral: Comparison of two parallelization strategies (MPI and
  SKELGIS) on a software designed for hydrology applications</title><categories>math.NA cs.DC cs.MS cs.NA</categories><comments>27 pages</comments><proxy>ccsd</proxy><journal-ref>ESAIM: Proc. 43 (2013) 59-79</journal-ref><doi>10.1051/proc/201343004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we perform a comparison of two approaches for the
parallelization of an existing, free software, FullSWOF 2D (http://www.
univ-orleans.fr/mapmo/soft/FullSWOF/ that solves shallow water equations for
applications in hydrology) based on a domain decomposition strategy. The first
approach is based on the classical MPI library while the second approach uses
Parallel Algorithmic Skeletons and more precisely a library named SkelGIS
(Skeletons for Geographical Information Systems). The first results presented
in this article show that the two approaches are similar in terms of
performance and scalability. The two implementation strategies are however very
different and we discuss the advantages of each one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4847</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4847</id><created>2013-07-18</created><updated>2013-10-01</updated><authors><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Efficient Exploration and Value Function Generalization in Deterministic
  Systems</title><categories>cs.LG cs.AI cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reinforcement learning over episodes of a
finite-horizon deterministic system and as a solution propose optimistic
constraint propagation (OCP), an algorithm designed to synthesize efficient
exploration and value function generalization. We establish that when the true
value function lies within a given hypothesis class, OCP selects optimal
actions over all but at most K episodes, where K is the eluder dimension of the
given hypothesis class. We establish further efficiency and asymptotic
performance guarantees that apply even if the true value function does not lie
in the given hypothesis class, for the special case where the hypothesis class
is the span of pre-specified indicator functions over disjoint sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4852</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4852</id><created>2013-07-18</created><authors><author><keyname>Ma</keyname><forenames>Yanbo</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Power Efficiency for Device-to-Device Communications</title><categories>cs.NI</categories><comments>Submitted to JSAC special issue on D2D</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of device-to-Device (D2D) communication as an underlay
coexistence with cellular networks gains many advantages of improving system
performance. In this paper, we model such a two-layer heterogenous network
based on stochastic geometry approach. We aim at minimizing the expected power
consumption of the D2D layer while satisfying the outage performance of both
D2D layer and cellular layer. We consider two kinds of power control schemes.
The first one is referred as to independent power control where the transmit
powers are statistically independent of the networks and all channel
conditions. The second is named as dependent power control where the transmit
power of each user is dependent on its own channel condition. A closed-form
expression of optimal independent power control is derived, and we point out
that the optimal power control for this case is fixed and not relevant to the
randomness of the network. For the dependent power control case, we propose an
efficient way to find the close-to-optimal solution for the power-efficiency
optimization problem. Numerical results show that dependent power control
scheme saves about half of power that the independent power control scheme
demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4879</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4879</id><created>2013-07-18</created><updated>2013-08-29</updated><authors><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Mendoza</keyname><forenames>Marcelo</forenames></author><author><keyname>Khan</keyname><forenames>Nasir</forenames></author></authors><title>Says who? Automatic Text-Based Content Analysis of Television News</title><categories>cs.CL cs.IR</categories><comments>In the 2013 workshop on Mining Unstructured Big Data Using Natural
  Language Processing, co-located with CIKM 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We perform an automatic analysis of television news programs, based on the
closed captions that accompany them. Specifically, we collect all the news
broadcasted in over 140 television channels in the US during a period of six
months. We start by segmenting, processing, and annotating the closed captions
automatically. Next, we focus on the analysis of their linguistic style and on
mentions of people using NLP methods. We present a series of key insights about
news providers, people in the news, and we discuss the biases that can be
uncovered by automatic means. These insights are contrasted by looking at the
data from multiple points of view, including qualitative assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4891</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4891</id><created>2013-07-18</created><updated>2015-08-21</updated><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Robust Subspace Clustering via Thresholding</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>final version, to appear in the IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering noisy and incompletely observed high-dimensional
data points into a union of low-dimensional subspaces and a set of outliers is
considered. The number of subspaces, their dimensions, and their orientations
are assumed unknown. We propose a simple low-complexity subspace clustering
algorithm, which applies spectral clustering to an adjacency matrix obtained by
thresholding the correlations between data points. In other words, the
adjacency matrix is constructed from the nearest neighbors of each data point
in spherical distance. A statistical performance analysis shows that the
algorithm exhibits robustness to additive noise and succeeds even when the
subspaces intersect. Specifically, our results reveal an explicit tradeoff
between the affinity of the subspaces and the tolerable noise level. We
furthermore prove that the algorithm succeeds even when the data points are
incompletely observed with the number of missing entries allowed to be (up to a
log-factor) linear in the ambient dimension. We also propose a simple scheme
that provably detects outliers, and we present numerical results on real and
synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4894</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4894</id><created>2013-07-18</created><authors><author><keyname>Chardon</keyname><forenames>Gilles</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author></authors><title>Source localization in reverberant rooms using sparse modeling and
  narrowband measurements</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two cases of acoustic source localization in a reverberant room,
from a number of point-wise narrowband measurements. In the first case, the
room is perfectly known. We show that using a sparse recovery algorithm with a
dictionary of sources computed a priori requires measurements at multiple
frequencies. Furthermore, we study the choice of frequencies for these
measurements, and show that one should avoid the modal frequencies of the room.
In the second case, when the shape and the boundary conditions of the room are
unknown, we propose a model of the acoustical field based on the Vekua theory,
still allowing the localization of sources, at the cost of an increased number
of measurements. Numerical results are given, using simple adaptations of
standard sparse recovery methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4897</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4897</id><created>2013-07-18</created><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Limaye</keyname><forenames>Nutan</forenames></author><author><keyname>Mahajan</keyname><forenames>Meena</forenames></author><author><keyname>Sreenivasaiah</keyname><forenames>Karteek</forenames></author></authors><title>Small Depth Proof Systems</title><categories>cs.CC</categories><comments>19 pages, 1 figure. To appear in MFCS 2013</comments><acm-class>F.2.2; F.1.3; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proof system for a language L is a function f such that Range(f) is exactly
L. In this paper, we look at proofsystems from a circuit complexity point of
view and study proof systems that are computationally very restricted. The
restriction we study is: they can be computed by bounded fanin circuits of
constant depth (NC^0), or of O(log log n) depth but with O(1) alternations
(polylog AC^0). Each output bit depends on very few input bits; thus such proof
systems correspond to a kind of local error-correction on a theorem-proof pair.
  We identify exactly how much power we need for proof systems to capture all
regular languages. We show that all regular language have polylog AC^0 proof
systems, and from a previous result (Beyersdorff et al, MFCS 2011, where NC^0
proof systems were first introduced), this is tight. Our technique also shows
that MAJ has polylog AC^0 proof system. We explore the question of whether TAUT
has NC^0 proof systems. Addressing this question about 2TAUT, and since 2TAUT
is closely related to reachability in graphs, we ask the same question about
Reachability. We show that both Undirected Reachability and Directed
UnReachability have NC^0 proof systems, but Directed Reachability is still
open.
  In the context of how much power is needed for proof systems for languages in
NP, we observe that proof systems for a good fraction of languages in NP do not
need the full power of AC^0; they have SAC^0 or coSAC^0 proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4901</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4901</id><created>2013-07-18</created><authors><author><keyname>Dybizba&#x144;ski</keyname><forenames>Janusz</forenames></author><author><keyname>Szepietowski</keyname><forenames>Andrzej</forenames></author></authors><title>Oriented chromatic number of Halin graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oriented chromatic number of an oriented graph $G$ is the minimum order of an
oriented graph $H$ such that $G$ admits a homomorphism to $H$. The oriented
chromatic number of an unoriented graph $G$ is the maximal chromatic number
over all possible orientations of $G$. In this paper, we prove that every Halin
graph has oriented chromatic number at most 8, improving a previous bound by
Hosseini Dolama and Sopena, and confirming the conjecture given by Vignal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4906</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4906</id><created>2013-07-18</created><authors><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author></authors><title>Functional framework for representing and transforming quantum channels</title><categories>quant-ph cs.SC</categories><comments>5 pages, no figures, presented at ACA2013, should have 'towards' in
  the title</comments><acm-class>D.1.1; J.2</acm-class><journal-ref>J.L. Galan Garcia, G. Aguilera Venegas, P. Rodriguez Cielos
  (eds.), Proc. Applications of Computer Algebra (ACA2013), Malaga, July 2013,
  pp. 276-280</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework which aims to simplify the analysis of quantum states
and quantum operations by harnessing the potential of function programming
paradigm. We show that the introduced framework allows a seamless manipulation
of quantum channels, in particular to convert between different representations
of quantum channels, and thus that the use of functional programming concepts
facilitates the manipulation of abstract objects used in the language of
quantum theory.
  For the purpose of our presentation we will use Mathematica computer algebra
system. This choice is motivated twofold. First, it offers a rich programming
language based on the functional paradigm. Second, this programming language is
combined with powerful symbolic and numeric manipulation capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4910</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4910</id><created>2013-07-18</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author></authors><title>Hard Asymptotic Sets for One-Dimensional Cellular Automata</title><categories>cs.CC cs.FL math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the (language of the) asymptotic set (and the nonwandering set)
of a one-dimensional cellular automaton can be $\SIGMA^1_1$-hard. We do not go
into much detail, since the constructions are relatively standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4927</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4927</id><created>2013-07-18</created><authors><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author><author><keyname>Oka</keyname><forenames>Keigo</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Linear-Time FPT Algorithms via Network Flow</title><categories>cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of parameterized complexity, to cope with NP-Hard problems, we
introduce a parameter k besides the input size n, and we aim to design
algorithms (called FPT algorithms) that run in O(f(k)n^d) time for some
function f(k) and constant d. Though FPT algorithms have been successfully
designed for many problems, typically they are not sufficiently fast because of
huge f(k) and d. In this paper, we give FPT algorithms with small f(k) and d
for many important problems including Odd Cycle Transversal and Almost 2-SAT.
More specifically, we can choose f(k) as a single exponential (4^k) and d as
one, that is, linear in the input size. To the best of our knowledge, our
algorithms achieve linear time complexity for the first time for these
problems. To obtain our algorithms for these problems, we consider a large
class of integer programs, called BIP2. Then we show that, in linear time, we
can reduce BIP2 to Vertex Cover Above LP preserving the parameter k, and we can
compute an optimal LP solution for Vertex Cover Above LP using network flow.
Then, we perform an exhaustive search by fixing half-integral values in the
optimal LP solution for Vertex Cover Above LP. A bottleneck here is that we
need to recompute an LP optimal solution after branching. To address this
issue, we exploit network flow to update the optimal LP solution in linear
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4934</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4934</id><created>2013-07-18</created><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Peretz</keyname><forenames>Ron</forenames></author></authors><title>Approximate Nash Equilibria via Sampling</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that in a normal form n-player game with m actions for each player,
there exists an approximate Nash equilibrium where each player randomizes
uniformly among a set of O(log(m) + log(n)) pure strategies. This result
induces an $N^{\log \log N}$ algorithm for computing an approximate Nash
equilibrium in games where the number of actions is polynomial in the number of
players (m=poly(n)), where $N=nm^n$ is the size of the game (the input size).
  In addition, we establish an inverse connection between the entropy of Nash
equilibria in the game, and the time it takes to find such an approximate Nash
equilibrium using the random sampling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4938</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4938</id><created>2013-07-18</created><authors><author><keyname>Ninagawa</keyname><forenames>Shigeru</forenames></author></authors><title>Computational Universality and 1/f Noise in Elementary Cellular Automata</title><categories>nlin.CG cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is speculated that there is a relationship between 1/f noise and
computational universality in cellular automata. We use genetic algorithms to
search for one-dimensional and two-state, five-neighbor cellular automata which
have 1/f-type spectrum. A power spectrum is calculated from the evolution
starting from a random initial configuration. The fitness is estimated from the
power spectrum in consideration of the similarity to 1/f-type spectrum. The
result shows that the rule with the highest average fitness has a propagating
structure like other computationally universal cellular automata, although
computational universality of the rule has not been proved yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4952</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4952</id><created>2013-07-18</created><authors><author><keyname>Mittal</keyname><forenames>Sudip</forenames></author><author><keyname>Gupta</keyname><forenames>Neha</forenames></author><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>The Pin-Bang Theory: Discovering The Pinterest World</title><categories>cs.SI cs.SY physics.soc-ph</categories><comments>15 pages, 10 figures, 5 tables</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pinterest is an image-based online social network, which was launched in the
year 2010 and has gained a lot of traction, ever since. Within 3 years,
Pinterest has attained 48.7 million unique users. This stupendous growth makes
it interesting to study Pinterest, and gives rise to multiple questions about
it's users, and content. We characterized Pinterest on the basis of large scale
crawls of 3.3 million user profiles, and 58.8 million pins. In particular, we
explored various attributes of users, pins, boards, pin sources, and user
locations, in detail and performed topical analysis of user generated textual
content. The characterization revealed most prominent topics among users and
pins, top image sources, and geographical distribution of users on Pinterest.
We then investigated this social network from a privacy and security
standpoint, and found traces of malware in the form of pin sources. Instances
of Personally Identifiable Information (PII) leakage were also discovered in
the form of phone numbers, BBM (Blackberry Messenger) pins, and email
addresses. Further, our analysis demonstrated how Pinterest is a potential
venue for copyright infringement, by showing that almost half of the images
shared on Pinterest go uncredited. To the best of our knowledge, this is the
first attempt to characterize Pinterest at such a large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4966</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4966</id><created>2013-07-18</created><authors><author><keyname>Suda</keyname><forenames>Martin</forenames></author></authors><title>Triggered Clause Pushing for IC3</title><categories>cs.LO</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an improvement of the famous IC3 algorithm for model checking
safety properties of finite state systems. We collect models computed by the
SAT-solver during the clause propagation phase of the algorithm and use them as
witnesses for why the respective clauses could not be pushed forward. It only
makes sense to recheck a particular clause for pushing when its witnessing
model falsifies a newly added clause. Since this trigger test is both
computationally cheap and sufficiently precise, we can afford to keep clauses
pushed as far as possible at all times. Experiments indicate that this strategy
considerably improves IC3's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4974</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4974</id><created>2013-07-18</created><updated>2015-05-29</updated><authors><author><keyname>Berthomieu</keyname><forenames>J{&#xe9;}r{&#xe9;}my</forenames><affiliation>LIP6, Syst{&#xe8;}mes Polynomiaux / Inria - LIP6</affiliation></author><author><keyname>Faug{&#xe8;}re</keyname><forenames>Jean-Charles</forenames><affiliation>Syst{&#xe8;}mes Polynomiaux / Inria - LIP6, LIP6</affiliation></author><author><keyname>Perret</keyname><forenames>Ludovic</forenames><affiliation>LIP6, Syst{&#xe8;}mes Polynomiaux / Inria - LIP6</affiliation></author></authors><title>Polynomial-Time Algorithms for Quadratic Isomorphism of Polynomials: The
  Regular Case</title><categories>cs.SC cs.CR</categories><comments>Published in Journal of Complexity, Elsevier, 2015, pp.39</comments><proxy>ccsd</proxy><doi>10.1016/j.jco.2015.04.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbf{f}=(f\_1,\ldots,f\_m)$ and $\mathbf{g}=(g\_1,\ldots,g\_m)$ be
two sets of $m\geq 1$ nonlinear polynomials over $\mathbb{K}[x\_1,\ldots,x\_n]$
($\mathbb{K}$ being a field). We consider the computational problem of finding
-- if any -- an invertible transformation on the variables mapping $\mathbf{f}$
to $\mathbf{g}$. The corresponding equivalence problem is known as {\tt
Isomorphism of Polynomials with one Secret} ({\tt IP1S}) and is a fundamental
problem in multivariate cryptography. The main result is a randomized
polynomial-time algorithm for solving {\tt IP1S} for quadratic instances, a
particular case of importance in cryptography and somewhat justifying {\it a
posteriori} the fact that {\it Graph Isomorphism} reduces to only cubic
instances of {\tt IP1S} (Agrawal and Saxena). To this end, we show that {\tt
IP1S} for quadratic polynomials can be reduced to a variant of the classical
module isomorphism problem in representation theory, which involves to test the
orthogonal simultaneous conjugacy of symmetric matrices. We show that we can
essentially {\it linearize} the problem by reducing quadratic-{\tt IP1S} to
test the orthogonal simultaneous similarity of symmetric matrices; this latter
problem was shown by Chistov, Ivanyos and Karpinski to be equivalent to finding
an invertible matrix in the linear space $\mathbb{K}^{n \times n}$ of $n \times
n$ matrices over $\mathbb{K}$ and to compute the square root in a matrix
algebra. While computing square roots of matrices can be done efficiently using
numerical methods, it seems difficult to control the bit complexity of such
methods. However, we present exact and polynomial-time algorithms for computing
the square root in $\mathbb{K}^{n \times n}$ for various fields (including
finite fields). We then consider \\#{\tt IP1S}, the counting version of {\tt
IP1S} for quadratic instances. In particular, we provide a (complete)
characterization of the automorphism group of homogeneous quadratic
polynomials. Finally, we also consider the more general {\it Isomorphism of
Polynomials} ({\tt IP}) problem where we allow an invertible linear
transformation on the variables \emph{and} on the set of polynomials. A
randomized polynomial-time algorithm for solving {\tt IP} when
\(\mathbf{f}=(x\_1^d,\ldots,x\_n^d)\) is presented. From an algorithmic point
of view, the problem boils down to factoring the determinant of a linear matrix
(\emph{i.e.}\ a matrix whose components are linear polynomials). This extends
to {\tt IP} a result of Kayal obtained for {\tt PolyProj}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4980</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4980</id><created>2013-07-18</created><updated>2015-12-09</updated><authors><author><keyname>Chen</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Cox</keyname><forenames>Ingemar J.</forenames></author><author><keyname>Kankanhalli</keyname><forenames>Mohan S.</forenames></author></authors><title>Multi-keyword multi-click advertisement option contracts for sponsored
  search</title><categories>cs.GT cs.IR</categories><comments>Chen, Bowei and Wang, Jun and Cox, Ingemar J. and Kankanhalli, Mohan
  S. (2015) Multi-keyword multi-click advertisement option contracts for
  sponsored search. ACM Transactions on Intelligent Systems and Technology, 7
  (1). pp. 1-29. ISSN: 2157-6904</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sponsored search, advertisement (abbreviated ad) slots are usually sold by
a search engine to an advertiser through an auction mechanism in which
advertisers bid on keywords. In theory, auction mechanisms have many desirable
economic properties. However, keyword auctions have a number of limitations
including: the uncertainty in payment prices for advertisers; the volatility in
the search engine's revenue; and the weak loyalty between advertiser and search
engine. In this paper we propose a special ad option that alleviates these
problems. In our proposal, an advertiser can purchase an option from a search
engine in advance by paying an upfront fee, known as the option price. He then
has the right, but no obligation, to purchase among the pre-specified set of
keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks
in a specified period of time. The proposed option is closely related to a
special exotic option in finance that contains multiple underlying assets
(multi-keyword) and is also multi-exercisable (multi-click). This novel
structure has many benefits: advertisers can have reduced uncertainty in
advertising; the search engine can improve the advertisers' loyalty as well as
obtain a stable and increased expected revenue over time. Since the proposed ad
option can be implemented in conjunction with the existing keyword auctions,
the option price and corresponding fixed CPCs must be set such that there is no
arbitrage between the two markets. Option pricing methods are discussed and our
experimental results validate the development. Compared to keyword auctions, a
search engine can have an increased expected revenue by selling an ad option.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4983</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4983</id><created>2013-07-18</created><authors><author><keyname>Alirezaei</keyname><forenames>Gholamreza</forenames></author></authors><title>A Sharp Double Inequality for the Inverse Tangent Function</title><categories>cs.IT math.IT</categories><comments>Submitted to the Transactions on Information Theory</comments><msc-class>26D05, 26D07, 26D15, 33B10, 39B62</msc-class><acm-class>G.1.2; G.1.6; E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inverse tangent function can be bounded by different inequalities, for
example by Shafer's inequality. In this publication, we propose a new sharp
double inequality, consisting of a lower and an upper bound, for the inverse
tangent function. In particular, we sharpen Shafer's inequality and calculate
the best corresponding constants. The maximum relative errors of the obtained
bounds are approximately smaller than 0.27% and 0.23% for the lower and upper
bound, respectively. Furthermore, we determine an upper bound on the relative
errors of the proposed bounds in order to describe their tightness
analytically. Moreover, some important properties of the obtained bounds are
discussed in order to describe their behavior and achieved accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4986</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4986</id><created>2013-07-18</created><authors><author><keyname>Krivochen</keyname><forenames>Diego Gabriel</forenames></author></authors><title>On the Necessity of Mixed Models: Dynamical Frustrations in the Mind</title><categories>nlin.CD cs.CL math.DS</categories><comments>29 pages. Ms. currently under review</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the present work we will present and analyze some basic processes at the
local and global level in linguistic derivations that seem to go beyond the
limits of Markovian or Turing-like computation, and require, in our opinion, a
quantum processor. We will first present briefly the working hypothesis and
then focus on the empirical domain. At the same time, we will argue that a
model appealing to only one kind of computation (be it quantum or not) is
necessarily insufficient, and thus both linear and non-linear formal models are
to be invoked in order to pursue a fuller understanding of mental computations
within a unified framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.4990</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.4990</id><created>2013-07-18</created><updated>2013-11-15</updated><authors><author><keyname>Banerjee</keyname><forenames>Purnendu</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Video Text Localization using Wavelet and Shearlet Transforms</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1101.0553 by other authors</comments><doi>10.1117/12.2036077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text in video is useful and important in indexing and retrieving the video
documents efficiently and accurately. In this paper, we present a new method of
text detection using a combined dictionary consisting of wavelets and a
recently introduced transform called shearlets. Wavelets provide optimally
sparse expansion for point-like structures and shearlets provide optimally
sparse expansions for curve-like structures. By combining these two features we
have computed a high frequency sub-band to brighten the text part. Then K-means
clustering is used for obtaining text pixels from the Standard Deviation (SD)
of combined coefficient of wavelets and shearlets as well as the union of
wavelets and shearlets features. Text parts are obtained by grouping
neighboring regions based on geometric properties of the classified output
frame of unsupervised K-means classification. The proposed method tested on a
standard as well as newly collected database shows to be superior to some
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5001</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5001</id><created>2013-07-18</created><updated>2014-01-02</updated><authors><author><keyname>Guzman</keyname><forenames>Cristobal</forenames></author><author><keyname>Nemirovski</keyname><forenames>Arkadi</forenames></author></authors><title>On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization</title><categories>math.OC cs.CC</categories><comments>Submitted version (minor changes)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive lower bounds on the black-box oracle complexity of large-scale
smooth convex minimization problems, with emphasis on minimizing smooth (with
Holder continuous, with a given exponent and constant, gradient) convex
functions over high-dimensional ||.||_p-balls, 1&lt;=p&lt;=\infty. Our bounds turn
out to be tight (up to logarithmic in the design dimension factors), and can be
viewed as a substantial extension of the existing lower complexity bounds for
large-scale convex minimization covering the nonsmooth case and the 'Euclidean'
smooth case (minimization of convex functions with Lipschitz continuous
gradients over Euclidean balls). As a byproduct of our results, we demonstrate
that the classical Conditional Gradient algorithm is near-optimal, in the sense
of Information-Based Complexity Theory, when minimizing smooth convex functions
over high-dimensional ||.||_\infty-balls and their matrix analogies -- spectral
norm balls in the spaces of square matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5004</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5004</id><created>2013-07-18</created><updated>2013-07-19</updated><authors><author><keyname>Reichert</keyname><forenames>Julien</forenames></author></authors><title>On The Complexity of Counter Reachability Games</title><categories>cs.GT cs.LO</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counter reachability games are played by two players on a graph with labelled
edges. Each move consists in picking an edge from the current location and
adding its label to a counter vector. The objective is to reach a given counter
value in a given location. We distinguish three semantics for counter
reachability games, according to what happens when a counter value would become
negative: the edge is either disabled, or enabled but the counter value becomes
zero, or enabled. We consider the problem of deciding the winner in counter
reachability games and show that, in most cases, it has the same complexity
under all semantics. Surprisingly, under one semantics, the complexity in
dimension one depends on whether the objective value is zero or any other
integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5030</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5030</id><created>2013-07-18</created><updated>2013-08-07</updated><authors><author><keyname>Keng</keyname><forenames>Wah Loon</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author></authors><title>The Yao Graph $Y_5$ is a Spanner</title><categories>cs.CG</categories><msc-class>65D18, 68U05, 52C99</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that $Y_5$, the Yao graph with five cones, is a
spanner with stretch factor $\rho = 2+\sqrt{3} \approx 3.74$. Since $Y_5$ is
the only Yao graph whose status of being a spanner or not was open, this
completes the picture of the Yao graphs that are spanners: a Yao graph $Y_k$ is
a spanner if and only if $k \geq 4$.
  We complement the above result with a lower bound of 2.87 on the stretch
factor of $Y_5$. We also show that $YY_5$, the Yao-Yao graph with five cones,
is not a spanner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5057</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5057</id><created>2013-07-18</created><authors><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Avoiding Whitewashing in Unstructured Peer-to-Peer Resource Sharing
  Network</title><categories>cs.NI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In peer-to-peer file sharing network, it is hard to distinguish between a
legitimate newcomer and a whitewasher. This makes whitewashing a big problem in
peer-to-peer networks. Although the problem of whitewashing can be solved using
permanent identities, it may take away the right of anonymity for users. In
this paper, we a have proposed a novel algorithm to avoid this problem when
network uses free temporary identities. In this algorithm, the initial
reputation is adjusted according to the level of whitewashing in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5076</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5076</id><created>2013-07-18</created><authors><author><keyname>Cioaca</keyname><forenames>Alexandru</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Low-rank Approximations for Computing Observation Impact in 4D-Var Data
  Assimilation</title><categories>cs.CE</categories><report-no>CSL-TR-3-2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient computational framework to quantify the impact of
individual observations in four dimensional variational data assimilation. The
proposed methodology uses first and second order adjoint sensitivity analysis,
together with matrix-free algorithms to obtain low-rank approximations of ob-
servation impact matrix. We illustrate the application of this methodology to
important applications such as data pruning and the identification of faulty
sensors for a two dimensional shallow water test system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5085</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5085</id><created>2013-07-18</created><authors><author><keyname>Qin</keyname><forenames>Peng</forenames></author><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Huang</keyname><forenames>Benxiong</forenames></author><author><keyname>Xu</keyname><forenames>Guan</forenames></author></authors><title>DCE: A Novel Delay Correlation Measurement for Tomography with Passive
  Realization</title><categories>cs.NI</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tomography is important for network design and routing optimization. Prior
approaches require either precise time synchronization or complex cooperation.
Furthermore, active tomography consumes explicit probeing resulting in limited
scalability. To address the first issue we propose a novel Delay Correlation
Estimation methodology named DCE with no need of synchronization and special
cooperation. For the second issue we develop a passive realization mechanism
merely using regular data flow without explicit bandwidth consumption.
Extensive simulations in OMNeT++ are made to evaluate its accuracy where we
show that DCE measured delay correlation is highly identical with the true
value. Also from test result we find that mechanism of passive realization is
able to achieve both regular data transmission and purpose of tomography with
excellent robustness versus different background traffic and package size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5090</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5090</id><created>2013-07-18</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Manokaran</keyname><forenames>Rajsekar</forenames></author><author><keyname>Wenner</keyname><forenames>Cenny</forenames></author></authors><title>On the NP-Hardness of Approximating Ordering Constraint Satisfaction
  Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show improved NP-hardness of approximating Ordering Constraint
Satisfaction Problems (OCSPs). For the two most well-studied OCSPs, Maximum
Acyclic Subgraph and Maximum Betweenness, we prove inapproximability of
$14/15+\epsilon$ and $1/2+\epsilon$.
  An OCSP is said to be approximation resistant if it is hard to approximate
better than taking a uniformly random ordering. We prove that the Maximum
Non-Betweenness Problem is approximation resistant and that there are width-$m$
approximation-resistant OCSPs accepting only a fraction $1 / (m/2)!$ of
assignments. These results provide the first examples of
approximation-resistant OCSPs subject only to P $\neq$ \NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5095</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5095</id><created>2013-07-18</created><updated>2014-04-09</updated><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Karakonstantis</keyname><forenames>Georgios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Enabling Complexity-Performance Trade-Offs for Successive Cancellation
  Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted for publication at IEEE International Symposium on
  Information Theory 2014, Honolulu, Hawaii</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are one of the most recent advancements in coding theory and they
have attracted significant interest. While they are provably capacity achieving
over various channels, they have seen limited practical applications.
Unfortunately, the successive nature of successive cancellation based decoders
hinders fine-grained adaptation of the decoding complexity to design
constraints and operating conditions. In this paper, we propose a systematic
method for enabling complexity-performance trade-offs by constructing polar
codes based on an optimization problem which minimizes the complexity under a
suitably defined mutual information based performance constraint. Moreover, a
low-complexity greedy algorithm is proposed in order to solve the optimization
problem efficiently for very large code lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5101</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5101</id><created>2013-07-18</created><updated>2013-11-25</updated><authors><author><keyname>Yu</keyname><forenames>Hsiang-Fu</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Large-scale Multi-label Learning with Missing Labels</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-label classification problem has generated significant interest in
recent years. However, existing approaches do not adequately address two key
challenges: (a) the ability to tackle problems with a large number (say
millions) of labels, and (b) the ability to handle data with missing labels. In
this paper, we directly address both these problems by studying the multi-label
problem in a generic empirical risk minimization (ERM) framework. Our
framework, despite being simple, is surprisingly able to encompass several
recent label-compression based methods which can be derived as special cases of
our method. To optimize the ERM problem, we develop techniques that exploit the
structure of specific loss functions - such as the squared loss function - to
offer efficient algorithms. We further show that our learning framework admits
formal excess risk bounds even in the presence of missing labels. Our risk
bounds are tight and demonstrate better generalization performance for low-rank
promoting trace-norm regularization when compared to (rank insensitive)
Frobenius norm regularization. Finally, we present extensive empirical results
on a variety of benchmark datasets and show that our methods perform
significantly better than existing label compression based methods and can
scale up to very large datasets such as the Wikipedia dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5102</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5102</id><created>2013-07-18</created><authors><author><keyname>Gonella</keyname><forenames>Stefano</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis D.</forenames></author></authors><title>Automated Defect Localization via Low Rank Plus Outlier Modeling of
  Propagating Wavefield Data</title><categories>cs.CV</categories><comments>16 pages, 9 figures, Submitted to the IEEE Transactions on
  Ultrasonics, Ferroelectrics and Frequency Control on August 30th 2012</comments><journal-ref>IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency
  Control, v. 60, n.12, pp. 2553 - 2565</journal-ref><doi>10.1109/TUFFC.2013.2854</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes an agnostic inference strategy for material diagnostics,
conceived within the context of laser-based non-destructive evaluation methods,
which extract information about structural anomalies from the analysis of
acoustic wavefields measured on the structure's surface by means of a scanning
laser interferometer. The proposed approach couples spatiotemporal windowing
with low rank plus outlier modeling, to identify a priori unknown deviations in
the propagating wavefields caused by material inhomogeneities or defects, using
virtually no knowledge of the structural and material properties of the medium.
This characteristic makes the approach particularly suitable for diagnostics
scenarios where the mechanical and material models are complex, unknown, or
unreliable. We demonstrate our approach in a simulated environment using
benchmark point and line defect localization problems based on propagating
flexural waves in a thin plate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5108</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5108</id><created>2013-07-18</created><authors><author><keyname>Goemans</keyname><forenames>Michel X.</forenames></author><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>Polynomiality for Bin Packing with a Constant Number of Item Types</title><categories>cs.DS cs.CG math.CO</categories><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the bin packing problem with d different item sizes s_i and item
multiplicities a_i, where all numbers are given in binary encoding. This
problem formulation is also known as the 1-dimensional cutting stock problem.
  In this work, we provide an algorithm which, for constant d, solves bin
packing in polynomial time. This was an open problem for all d &gt;= 3.
  In fact, for constant d our algorithm solves the following problem in
polynomial time: given two d-dimensional polytopes P and Q, find the smallest
number of integer points in P whose sum lies in Q.
  Our approach also applies to high multiplicity scheduling problems in which
the number of copies of each job type is given in binary encoding and each type
comes with certain parameters such as release dates, processing times and
deadlines. We show that a variety of high multiplicity scheduling problems can
be solved in polynomial time if the number of job types is constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5118</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5118</id><created>2013-07-18</created><authors><author><keyname>Mori</keyname><forenames>Syogo</forenames></author><author><keyname>Tangkaratt</keyname><forenames>Voot</forenames></author><author><keyname>Zhao</keyname><forenames>Tingting</forenames></author><author><keyname>Morimoto</keyname><forenames>Jun</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Model-Based Policy Gradients with Parameter-Based Exploration by
  Least-Squares Conditional Density Estimation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of reinforcement learning (RL) is to let an agent learn an optimal
control policy in an unknown environment so that future expected rewards are
maximized. The model-free RL approach directly learns the policy based on data
samples. Although using many samples tends to improve the accuracy of policy
learning, collecting a large number of samples is often expensive in practice.
On the other hand, the model-based RL approach first estimates the transition
model of the environment and then learns the policy based on the estimated
transition model. Thus, if the transition model is accurately learned from a
small amount of data, the model-based approach can perform better than the
model-free approach. In this paper, we propose a novel model-based RL method by
combining a recently proposed model-free policy search method called policy
gradients with parameter-based exploration and the state-of-the-art transition
model estimator called least-squares conditional density estimation. Through
experiments, we demonstrate the practical usefulness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5161</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5161</id><created>2013-07-19</created><updated>2014-03-28</updated><authors><author><keyname>Roig</keyname><forenames>Gemma</forenames></author><author><keyname>Boix</keyname><forenames>Xavier</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Random Binary Mappings for Kernel Learning and Efficient SVM</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machines (SVMs) are powerful learners that have led to
state-of-the-art results in various computer vision problems. SVMs suffer from
various drawbacks in terms of selecting the right kernel, which depends on the
image descriptors, as well as computational and memory efficiency. This paper
introduces a novel kernel, which serves such issues well. The kernel is learned
by exploiting a large amount of low-complex, randomized binary mappings of the
input feature. This leads to an efficient SVM, while also alleviating the task
of kernel selection. We demonstrate the capabilities of our kernel on 6
standard vision benchmarks, in which we combine several common image
descriptors, namely histograms (Flowers17 and Daimler), attribute-like
descriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).
Results show that our kernel learning adapts well to the different descriptors
types, achieving the performance of the kernels specifically tuned for each
image descriptor, and with similar evaluation cost as efficient SVM methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5182</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5182</id><created>2013-07-19</created><updated>2013-07-21</updated><authors><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author><author><keyname>Song</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Automated Small-Cell Deployment for Heterogeneous Cellular Networks</title><categories>cs.NI</categories><comments>19 pages, 6 figures</comments><journal-ref>Published in IEEE Communications Magazine, vol.51, no.5, pp.46-53,
  May 2013</journal-ref><doi>10.1109/MCOM.2013.6515046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing the cellular network's cell locations is one of the most
fundamental problems of network design. The general objective is to provide the
desired Quality-of-Service (QoS) with the minimum system cost. In order to meet
a growing appetite for mobile data services, heterogeneous networks have been
proposed as a cost- and energy-efficient method of improving local spectral
efficiency. Whilst unarticulated cell deployments can lead to localized
improvements, there is a significant risk posed to network-wide performance due
to the additional interference.
  The first part of the paper focuses on state-of-the-art modelling and
radio-planning methods based on stochastic geometry and Monte-Carlo
simulations, and the emerging automatic deployment prediction technique for
low-power nodes (LPNs) in heterogeneous networks. The technique advises a LPN
where it should be deployed, given certain knowledge of the network. The second
part of the paper focuses on algorithms that utilize interference and physical
environment knowledge to assist LPN deployment. The proposed techniques can not
only improve network performance, but also reduce radio-planning complexity,
capital expenditure, and energy consumption of the cellular network. The
theoretical work is supported by numerical results from system-level
simulations that employ real cellular network data and physical environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5186</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5186</id><created>2013-07-19</created><authors><author><keyname>Szepietowski</keyname><forenames>Andrzej</forenames></author></authors><title>Coloring directed cycles</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sopena in his survey [E. Sopena, The oriented chromatic number of graphs: A
short survey, preprint 2013] writes, without any proof, that an oriented cycle
$\vec C$ can be colored with three colors if and only if $\lambda(\vec C)=0$,
where $\lambda(\vec C)$ is the number of forward arcs minus the number of
backward arcs in $\vec C$. This is not true. In this paper we show that $\vec
C$ can be colored with three colors if and only if $\lambda(\vec C)=0(\bmod~3)$
or $\vec C$ does not contain three consecutive arcs going in the same
direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5210</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5210</id><created>2013-07-19</created><updated>2015-06-11</updated><authors><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Vuffray</keyname><forenames>Marc</forenames></author></authors><title>Approaching the Rate-Distortion Limit with Spatial Coupling, Belief
  propagation and Decimation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate an encoding scheme for lossy compression of a binary symmetric
source based on simple spatially coupled Low-Density Generator-Matrix codes.
The degree of the check nodes is regular and the one of code-bits is Poisson
distributed with an average depending on the compression rate. The performance
of a low complexity Belief Propagation Guided Decimation algorithm is
excellent. The algorithmic rate-distortion curve approaches the optimal curve
of the ensemble as the width of the coupling window grows. Moreover, as the
check degree grows both curves approach the ultimate Shannon rate-distortion
limit. The Belief Propagation Guided Decimation encoder is based on the
posterior measure of a binary symmetric test-channel. This measure can be
interpreted as a random Gibbs measure at a &quot;temperature&quot; directly related to
the &quot;noise level of the test-channel&quot;. We investigate the links between the
algorithmic performance of the Belief Propagation Guided Decimation encoder and
the phase diagram of this Gibbs measure. The phase diagram is investigated
thanks to the cavity method of spin glass theory which predicts a number of
phase transition thresholds. In particular the dynamical and condensation
&quot;phase transition temperatures&quot; (equivalently test-channel noise thresholds)
are computed. We observe that: (i) the dynamical temperature of the spatially
coupled construction saturates towards the condensation temperature; (ii) for
large degrees the condensation temperature approaches the temperature (i.e.
noise level) related to the information theoretic Shannon test-channel noise
parameter of rate-distortion theory. This provides heuristic insight into the
excellent performance of the Belief Propagation Guided Decimation algorithm.
The paper contains an introduction to the cavity method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5216</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5216</id><created>2013-07-19</created><authors><author><keyname>Duetting</keyname><forenames>Paul</forenames></author><author><keyname>Fischer</keyname><forenames>Felix</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Expressiveness and Robustness of First-Price Position Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since economic mechanisms are often applied to very different instances of
the same problem, it is desirable to identify mechanisms that work well in a
wide range of circumstances. We pursue this goal for a position auction setting
and specifically seek mechanisms that guarantee good outcomes under both
complete and incomplete information. A variant of the generalized first-price
mechanism with multi-dimensional bids turns out to be the only standard
mechanism able to achieve this goal, even when types are one-dimensional. The
fact that expressiveness beyond the type space is both necessary and sufficient
for this kind of robustness provides an interesting counterpoint to previous
work on position auctions that has highlighted the benefits of simplicity. From
a technical perspective our results are interesting because they establish
equilibrium existence for a multi-dimensional bid space, where standard
techniques break down. The structure of the equilibrium bids moreover provides
an intuitive explanation for why first-price payments may be able to support
equilibria in a wider range of circumstances than second-price payments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5228</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5228</id><created>2013-07-19</created><authors><author><keyname>Ozyurt</keyname><forenames>Serdar</forenames></author><author><keyname>Torlak</keyname><forenames>Murat</forenames></author></authors><title>Unified Performance Analysis of Orthogonal Transmit Beamforming Methods
  with User Selection</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures, 2 tables</comments><journal-ref>IEEE Transactions on Wireless Communications, Volume:12 , Issue:3,
  March 2013, Pages:1026-1037, ISSN:1536-1276</journal-ref><doi>10.1109/TWC.2012.011713.112304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous multiuser beamforming in multiantenna downlink channels can
entail dirty paper (DP) precoding (optimal and high complexity) or linear
precoding (suboptimal and low complexity) approaches. The system performance is
typically characterized by the sum capacity with homogenous users with perfect
channel state information at the transmitter. The sum capacity performance
analysis requires the exact probability distributions of the user
signal-to-noise ratios (SNRs) or signal-to-interference plus noise ratios
(SINRs). The standard techniques from order statistics can be sufficient to
obtain the probability distributions of SNRs for DP precoding due to the
removal of known interference at the transmitter. Derivation of such
probability distributions for linear precoding techniques on the other hand is
much more challenging. For example, orthogonal beamforming techniques do not
completely cancel the interference at the user locations, thereby requiring the
analysis with SINRs. In this paper, we derive the joint probability
distributions of the user SINRs for two orthogonal beamforming methods combined
with user scheduling: adaptive orthogonal beamforming and orthogonal linear
beamforming. We obtain compact and unified solutions for the joint probability
distributions of the scheduled users' SINRs. Our analytical results can be
applied for similar algorithms and are verified by computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5230</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5230</id><created>2013-07-19</created><updated>2014-06-28</updated><authors><author><keyname>Bagaria</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Pananjady</keyname><forenames>Ashwin</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Optimally Approximating the Coverage Lifetime of Wireless Sensor
  Networks</title><categories>cs.NI cs.DS</categories><comments>submitted to IEEE/ACM Transactions on Networking, 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing the lifetime of coverage (MLCP) of
targets in a wireless sensor network with battery-limited sensors. We first
show that the MLCP cannot be approximated within a factor less than $\ln n$ by
any polynomial time algorithm, where $n$ is the number of targets. This
provides closure to the long-standing open problem of showing optimality of
previously known $\ln n$ approximation algorithms. We also derive a new $\ln n$
approximation to the MLCP by showing a $\ln n$ approximation to the maximum
disjoint set cover problem (DSCP), which has many advantages over previous MLCP
algorithms, including an easy extension to the $k$-coverage problem. We then
present an improvement (in certain cases) to the $\ln n$ algorithm in terms of
a newly defined quantity &quot;expansiveness&quot; of the network. For the special
one-dimensional case, where each sensor can monitor a contiguous region of
possibly different lengths, we show that the MLCP solution is equal to the DSCP
solution, and can be found in polynomial time. Finally, for the special
two-dimensional case, where each sensor can monitor a circular area with a
given radius around itself, we combine existing results to derive a
$1+\epsilon$ approximation algorithm for solving MLCP for any $\epsilon &gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5240</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5240</id><created>2013-07-19</created><authors><author><keyname>Ozyurt</keyname><forenames>Serdar</forenames></author><author><keyname>Torlak</keyname><forenames>Murat</forenames></author></authors><title>Performance Analysis of Optimum Zero-Forcing Beamforming with Greedy
  User Selection</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><journal-ref>IEEE Communications Letters, Volume:16, Issue:4, April 2012,
  Pages:446-449, ISSN:1089-7798</journal-ref><doi>10.1109/LCOMM.2012.022112.112036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, an exact performance analysis is presented on the sum rate of
zero-forcing beamforming with a greedy user scheduling algorithm in a downlink
system. Adopting water-filling power allocation, we derive a compact form for
the joint probability density function of the scheduled users' squared
subchannel gains when a transmitter with multiple antennas sends information to
at most two scheduled users with each having a single antenna. The analysis is
verified by numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5251</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5251</id><created>2013-07-17</created><updated>2013-08-03</updated><authors><author><keyname>Smith</keyname><forenames>Reginald D.</forenames></author></authors><title>Period doubling, information entropy, and estimates for Feigenbaum's
  constants</title><categories>nlin.AO cs.IT math.IT nlin.CD</categories><comments>5 pages; accepted to the International Journal of Bifurcation and
  Chaos</comments><journal-ref>Int. J. Bifurcation Chaos, 23, 1350190 (2013)</journal-ref><doi>10.1142/S0218127413501903</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between period doubling bifurcations and Feigenbaum's
constants has been studied for nearly 40 years and this relationship has helped
uncover many fundamental aspects of universal scaling across multiple nonlinear
dynamical systems. This paper will combine information entropy with symbolic
dynamics to demonstrate how period doubling can be defined using these tools
alone. In addition, the technique allows us to uncover some unexpected, simple
estimates for Feigenbaum's constants which relate them to log 2 and the golden
ratio, phi, as well as to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5260</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5260</id><created>2013-07-02</created><authors><author><keyname>Herrouz</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Khentout</keyname><forenames>Chabane</forenames></author><author><keyname>Djoudi</keyname><forenames>Mahieddine</forenames></author></authors><title>Navigation Assistance and Web Accessibility Helper</title><categories>cs.HC</categories><comments>07 pages. arXiv admin note: substantial text overlap with
  arXiv:1304.2610</comments><journal-ref>Herrouz, A., Khentout, C., Djoudi, M. Navigation Assistance and
  Web Accessibility Helper. IJAIEM International Journal of Application or
  Innovation in Engineering &amp; Management, Vol.2, Issue 5, May 2013, pp.
  517-523, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web accessibility is actually the most important aspect for providing access
to information and interaction for people with disabilities. However, it seems
that the ability of users with disabilities to navigate over the Web is not
dependent on the graphical complexity, but on the markup used to create the
structure of the website. Consequently, it is necessary to design some software
assistants to help all users to mark themselves in space during a navigation
session. In this paper, we propose an assistant for browsing on the Internet to
allow user to get one s bearings within Web navigation. We describe our
approach which puts at the disposal of the user a visited site map, thus giving
an explicit representation of virtual space. Different levels of visualization
are implemented in order to make the map more visible and less overloaded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5264</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5264</id><created>2013-07-19</created><updated>2013-07-27</updated><authors><author><keyname>Chatzis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Smaragdakis</keyname><forenames>Georgios</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author></authors><title>On the importance of Internet eXchange Points for today's Internet
  ecosystem</title><categories>cs.NI cs.CY</categories><comments>10 pages, keywords: Internet Exchange Point, Internet Architecture,
  Peering, Content Delivery</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet eXchange Points (IXPs) are generally considered to be the successors
of the four Network Access Points that were mandated as part of the
decommissioning of the NSFNET in 1994/95 to facilitate the transition from the
NSFNET to the &quot;public Internet&quot; as we know it today. While this popular view
does not tell the whole story behind the early beginnings of IXPs, what is true
is that since around 1994, the number of operational IXPs worldwide has grown
to more than 300 (as of May 2013), with the largest IXPs handling daily traffic
volumes comparable to those carried by the largest Tier-1 ISPs, but IXPs have
never really attracted any attention from the networking research community. At
first glance, this lack of interest seems understandable as IXPs have
apparently little to do with current &quot;hot&quot; topic areas such as data centers and
cloud services or software defined networking (SDN) and mobile communication.
However, we argue in this article that, in fact, IXPs are all about data
centers and cloud services and even SDN and mobile communication and should be
of great interest to networking researchers interested in understanding the
current and future Internet ecosystem. To this end, we survey the existing but
largely unknown sources of publicly available information about IXPs to
describe their basic technical and operational aspects and highlight the
critical differences among the various IXPs in the different regions of the
world, especially in Europe and North America. More importantly, we illustrate
the important role that IXPs play in today's Internet ecosystem and discuss how
IXP-driven innovation in Europe is shaping and redefining the Internet
marketplace, not only in Europe but increasingly so around the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5277</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5277</id><created>2013-07-19</created><authors><author><keyname>Glew</keyname><forenames>Neal</forenames></author><author><keyname>Sweeney</keyname><forenames>Tim</forenames></author><author><keyname>Petersen</keyname><forenames>Leaf</forenames></author></authors><title>Formalisation of the lambda aleph Runtime</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work we describe a novel approach to dependent typing based on a
multivalued term language. In this technical report we formalise the runtime, a
kind of operational semantics, for that language. We describe a fairly
comprehensive core language, and then give a small-step operational semantics
based on an abstract machine. Errors are explicit in the semantics. We also
prove several simple properties: that every non-terminated machine state steps
to something and that reduction is deterministic once input is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5290</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5290</id><created>2013-07-19</created><updated>2013-07-23</updated><authors><author><keyname>Dams</keyname><forenames>Johannes</forenames></author><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author></authors><title>Jamming-Resistant Learning in Wireless Networks</title><categories>cs.DS cs.GT cs.NI</categories><comments>22 pages, 2 figures, typos removed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider capacity maximization in wireless networks under adversarial
interference conditions. There are n links, each consisting of a sender and a
receiver, which repeatedly try to perform a successful transmission. In each
time step, the success of attempted transmissions depends on interference
conditions, which are captured by an interference model (e.g. the SINR model).
Additionally, an adversarial jammer can render a (1-delta)-fraction of time
steps unsuccessful. For this scenario, we analyze a framework for distributed
learning algorithms to maximize the number of successful transmissions. Our
main result is an algorithm based on no-regret learning converging to an
O(1/delta)-approximation. It provides even a constant-factor approximation when
the jammer exactly blocks a (1-delta)-fraction of time steps. In addition, we
consider a stochastic jammer, for which we obtain a constant-factor
approximation after a polynomial number of time steps. We also consider more
general settings, in which links arrive and depart dynamically, and where each
sender tries to reach multiple receivers. Our algorithms perform favorably in
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5295</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5295</id><created>2013-07-19</created><updated>2014-09-16</updated><authors><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>A decomposition based proof for fast mixing of a Markov chain over
  balanced realizations of a joint degree matrix</title><categories>math.CO cs.DM</categories><comments>submitted, 18 pages, 4 figures</comments><msc-class>05C30, 05C81, 68R10</msc-class><journal-ref>SIAM J. Discrete Mathematics 29 (1) (2015), 481-499</journal-ref><doi>10.1137/130929874</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A joint degree matrix (JDM) specifies the number of connections between nodes
of given degrees in a graph, for all degree pairs and uniquely determines the
degree sequence of the graph. We consider the space of all balanced
realizations of an arbitrary JDM, realizations in which the links between any
two degree groups are placed as uniformly as possible. We prove that a swap
Markov Chain Monte Carlo (MCMC) algorithm in the space of all balanced
realizations of an {\em arbitrary} graphical JDM mixes rapidly, i.e., the
relaxation time of the chain is bounded from above by a polynomial in the
number of nodes $n$. To prove fast mixing, we first prove a general
factorization theorem similar to the Martin-Randall method for disjoint
decompositions (partitions). This theorem can be used to bound from below the
spectral gap with the help of fast mixing subchains within every partition and
a bound on an auxiliary Markov chain between the partitions. Our proof of the
general factorization theorem is direct and uses conductance based methods
(Cheeger inequality).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5296</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5296</id><created>2013-07-19</created><updated>2013-10-07</updated><authors><author><keyname>Khare</keyname><forenames>Monik</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>First-Come-First-Served for Online Slot Allocation and Huffman Coding</title><categories>cs.DS cs.IT math.IT</categories><comments>ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014</comments><msc-class>68W40, 68Q87</msc-class><acm-class>F.1.2; F.2.0; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can one choose a good Huffman code on the fly, without knowing the underlying
distribution? Online Slot Allocation (OSA) models this and similar problems:
There are n slots, each with a known cost. There are n items. Requests for
items are drawn i.i.d. from a fixed but hidden probability distribution p.
After each request, if the item, i, was not previously requested, then the
algorithm (knowing the slot costs and the requests so far, but not p) must
place the item in some vacant slot j(i). The goal is to minimize the sum, over
the items, of the probability of the item times the cost of its assigned slot.
  The optimal offline algorithm is trivial: put the most probable item in the
cheapest slot, the second most probable item in the second cheapest slot, etc.
The optimal online algorithm is First Come First Served (FCFS): put the first
requested item in the cheapest slot, the second (distinct) requested item in
the second cheapest slot, etc. The optimal competitive ratios for any online
algorithm are 1+H(n-1) ~ ln n for general costs and 2 for concave costs. For
logarithmic costs, the ratio is, asymptotically, 1: FCFS gives cost opt + O(log
opt).
  For Huffman coding, FCFS yields an online algorithm (one that allocates
codewords on demand, without knowing the underlying probability distribution)
that guarantees asymptotically optimal cost: at most opt + 2 log(1+opt) + 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5299</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5299</id><created>2013-07-19</created><authors><author><keyname>Duetting</keyname><forenames>Paul</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>Polymatroid Prophet Inequalities</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a gambler and a prophet who observe a sequence of independent,
non-negative numbers. The gambler sees the numbers one-by-one whereas the
prophet sees the entire sequence at once. The goal of both is to decide on
fractions of each number they want to keep so as to maximize the weighted
fractional sum of the numbers chosen.
  The classic result of Krengel and Sucheston (1977-78) asserts that if both
the gambler and the prophet can pick one number, then the gambler can do at
least half as well as the prophet. Recently, Kleinberg and Weinberg (2012) have
generalized this result to settings where the numbers that can be chosen are
subject to a matroid constraint.
  In this note we go one step further and show that the bound carries over to
settings where the fractions that can be chosen are subject to a polymatroid
constraint. This bound is tight as it is already tight for the simple setting
where the gambler and the prophet can pick only one number. An interesting
application of our result is in mechanism design, where it leads to improved
results for various problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5302</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5302</id><created>2013-07-19</created><updated>2014-06-12</updated><authors><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Strathmann</keyname><forenames>Heiko</forenames></author><author><keyname>Garcia</keyname><forenames>Maria Lomeli</forenames></author><author><keyname>Andrieu</keyname><forenames>Christophe</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author></authors><title>Kernel Adaptive Metropolis-Hastings</title><categories>stat.ML cs.LG</categories><comments>Proceedings of the 31st International Conference on Machine Learning,
  Beijing, China, 2014; JMLR: W&amp;CP volume 32(2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the
purpose of sampling from a target distribution with strongly nonlinear support.
The algorithm embeds the trajectory of the Markov chain into a reproducing
kernel Hilbert space (RKHS), such that the feature space covariance of the
samples informs the choice of proposal. The procedure is computationally
efficient and straightforward to implement, since the RKHS moves can be
integrated out analytically: our proposal distribution in the original space is
a normal distribution whose mean and covariance depend on where the current
sample lies in the support of the target distribution, and adapts to its local
covariance structure. Furthermore, the procedure requires neither gradients nor
any other higher order information about the target, making it particularly
attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive
Metropolis-Hastings outperforms competing fixed and adaptive samplers on
multivariate, highly nonlinear target distributions, arising in both real-world
and synthetic examples. Code may be downloaded at
https://github.com/karlnapf/kameleon-mcmc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5304</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5304</id><created>2013-07-19</created><updated>2014-03-10</updated><authors><author><keyname>Grabowicz</keyname><forenames>Przemyslaw A.</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author><author><keyname>Goncalves</keyname><forenames>Bruno</forenames></author><author><keyname>Eguiluz</keyname><forenames>Victor M.</forenames></author></authors><title>Entangling mobility and interactions in social media</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages, 19 figures</comments><journal-ref>PLoS ONE 9(3): e92196 (2014)</journal-ref><doi>10.1371/journal.pone.0092196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Daily interactions naturally define social circles. Individuals tend to be
friends with the people they spend time with and they choose to spend time with
their friends, inextricably entangling physical location and social
relationships. As a result, it is possible to predict not only someone's
location from their friends' locations but also friendship from spatial and
temporal co-occurrence. While several models have been developed to separately
describe mobility and the evolution of social networks, there is a lack of
studies coupling social interactions and mobility. In this work, we introduce a
new model that bridges this gap by explicitly considering the feedback of
mobility on the formation of social ties. Data coming from three online social
networks (Twitter, Gowalla and Brightkite) is used for validation. Our model
reproduces various topological and physical properties of these networks such
as: i) the size of the connected components, ii) the distance distribution
between connected users, iii) the dependence of the reciprocity on the
distance, iv) the variation of the social overlap and the clustering with the
distance. Besides numerical simulations, a mean-field approach is also used to
study analytically the main statistical features of the networks generated by
the model. The robustness of the results to changes in the model parameters is
explored, finding that a balance between friend visits and long-range random
connections is essential to reproduce the geographical features of the
empirical networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5321</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5321</id><created>2013-07-16</created><updated>2016-02-14</updated><authors><author><keyname>Bozga</keyname><forenames>Marius</forenames></author><author><keyname>Iosif</keyname><forenames>Radu</forenames></author><author><keyname>Konecny</keyname><forenames>Filip</forenames></author></authors><title>The Complexity of Reachability Problems for Flat Counter Machines with
  Periodic Loops</title><categories>cs.CC cs.LO</categories><comments>43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves the NP-completeness of the reachability problem for the
class of flat counter machines with difference bounds and, more generally,
octagonal relations, labeling the transitions on the loops. The proof is based
on the fact that the sequence of powers $\{R^i\}_{i=1}^\infty$ of such
relations can be encoded as a periodic sequence of matrices, and that both the
prefix and the period of this sequence are $2^{\mathcal{O}(\bin{R})}$ in the
size of the binary encoding $\bin{R}$ of a relation $R$. This result allows to
characterize the complexity of the reachability problem for one of the most
studied class of counter machines \cite{cav10,comon-jurski98}, and has a
potential impact for other problems in program verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5322</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5322</id><created>2013-07-19</created><authors><author><keyname>Santos</keyname><forenames>Emanuel</forenames></author><author><keyname>Faria</keyname><forenames>Daniel</forenames></author><author><keyname>Pesquita</keyname><forenames>C&#xe1;tia</forenames></author><author><keyname>Couto</keyname><forenames>Francisco</forenames></author></authors><title>Ontology alignment repair through modularization and confidence-based
  heuristics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology Matching aims to find a set of semantic correspondences, called an
alignment, between related ontologies. In recent years, there has been a
growing interest in efficient and effective matching methods for large
ontologies. However, most of the alignments produced for large ontologies are
logically incoherent. It was only recently that the use of repair techniques to
improve the quality of ontology alignments has been explored. In this paper we
present a novel technique for detecting incoherent concepts based on ontology
modularization, and a new repair algorithm that minimizes the incoherence of
the resulting alignment and the number of matches removed from the input
alignment. An implementation was done as part of a lightweight version of
AgreementMaker system, a successful ontology matching platform, and evaluated
using a set of four benchmark biomedical ontology matching tasks. Our results
show that our implementation is efficient and produces better alignments with
respect to their coherence and f-measure than the state of the art repairing
tools. They also show that our implementation is a better alternative for
producing coherent silver standard alignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5329</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5329</id><created>2013-07-19</created><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author><author><keyname>Saari</keyname><forenames>Kalle</forenames></author></authors><title>Suffix conjugates for a class of morphic subshifts</title><categories>math.DS cs.DM math.CO</categories><journal-ref>Ergod. Th. Dynam. Sys. 35 (2014) 1767-1782</journal-ref><doi>10.1017/etds.2014.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A be a finite alphabet and f: A^* --&gt; A^* be a morphism with an iterative
fixed point f^\omega(\alpha), where \alpha{} is in A. Consider the subshift (X,
T), where X is the shift orbit closure of f^\omega(\alpha) and T: X --&gt; X is
the shift map. Let S be a finite alphabet that is in bijective correspondence
via a mapping c with the set of nonempty suffixes of the images f(a) for a in
A. Let calS be a subset S^N be the set of infinite words s = (s_n)_{n\geq 0}
such that \pi(s):= c(s_0)f(c(s_1)) f^2(c(s_2))... is in X. We show that if f is
primitive and f(A) is a suffix code, then there exists a mapping H: calS --&gt;
calS such that (calS, H) is a topological dynamical system and \pi: (calS, H)
--&gt; (X, T) is a conjugacy; we call (calS, H) the suffix conjugate of (X, T). In
the special case when f is the Fibonacci or the Thue-Morse morphism, we show
that the subshift (calS, T) is sofic, that is, the language of calS is regular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5330</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5330</id><created>2013-07-19</created><updated>2015-10-30</updated><authors><author><keyname>Gallet</keyname><forenames>Matteo</forenames></author><author><keyname>Rahkooy</keyname><forenames>Hamid</forenames></author><author><keyname>Zafeirakopoulos</keyname><forenames>Zafeirakis</forenames></author></authors><title>On Computing the Elimination Ideal Using Resultants with Applications to
  Gr\&quot;obner Bases</title><categories>math.AC cs.SC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resultants and Gr\&quot;obner bases are crucial tools in studying polynomial
elimination theory. We investigate relations between the variety of the
resultant of two polynomials and the variety of the ideal they generate. Then
we focus on the bivariate case, in which the elimination ideal is principal. We
study - by means of elementary tools - the difference between the multiplicity
of the factors of the generator of the elimination ideal and the multiplicity
of the factors of the resultant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5336</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5336</id><created>2013-07-19</created><updated>2013-07-23</updated><authors><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Takala</keyname><forenames>Pyry</forenames></author><author><keyname>Korhonen</keyname><forenames>Pekka</forenames></author><author><keyname>Wallenius</keyname><forenames>Jyrki</forenames></author></authors><title>Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts</title><categories>cs.CL cs.IR q-fin.CP</categories><comments>To be published in Journal of the American Society for Information
  Science and Technology</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of robo-readers to analyze news texts is an emerging technology trend
in computational finance. In recent research, a substantial effort has been
invested to develop sophisticated financial polarity-lexicons that can be used
to investigate how financial sentiments relate to future company performance.
However, based on experience from other fields, where sentiment analysis is
commonly applied, it is well-known that the overall semantic orientation of a
sentence may differ from the prior polarity of individual words. The objective
of this article is to investigate how semantic orientations can be better
detected in financial and economic news by accommodating the overall
phrase-structure information and domain-specific use of language. Our three
main contributions are: (1) establishment of a human-annotated finance
phrase-bank, which can be used as benchmark for training and evaluating
alternative models; (2) presentation of a technique to enhance financial
lexicons with attributes that help to identify expected direction of events
that affect overall sentiment; (3) development of a linearized phrase-structure
model for detecting contextual semantic orientations in financial and economic
news texts. The relevance of the newly added lexicon features and the benefit
of using the proposed learning-algorithm are demonstrated in a comparative
study against previously used general sentiment models as well as the popular
word frequency models used in recent financial studies. The proposed framework
is parsimonious and avoids the explosion in feature-space caused by the use of
conventional n-gram features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5348</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5348</id><created>2013-07-19</created><authors><author><keyname>Semerci</keyname><forenames>Oguz</forenames></author><author><keyname>Hao</keyname><forenames>Ning</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha E.</forenames></author><author><keyname>Miller</keyname><forenames>Eric L.</forenames></author></authors><title>Tensor-based formulation and nuclear norm regularization for
  multi-energy computed tomography</title><categories>cs.CV physics.med-ph</categories><doi>10.1109/TIP.2014.2305840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of energy selective, photon counting X-ray detectors allows
for a wide range of new possibilities in the area of computed tomographic image
formation. Under the assumption of perfect energy resolution, here we propose a
tensor-based iterative algorithm that simultaneously reconstructs the X-ray
attenuation distribution for each energy. We use a multi-linear image model
rather than a more standard &quot;stacked vector&quot; representation in order to develop
novel tensor-based regularizers. Specifically, we model the multi-spectral
unknown as a 3-way tensor where the first two dimensions are space and the
third dimension is energy. This approach allows for the design of tensor
nuclear norm regularizers, which like its two dimensional counterpart, is a
convex function of the multi-spectral unknown. The solution to the resulting
convex optimization problem is obtained using an alternating direction method
of multipliers (ADMM) approach. Simulation results shows that the generalized
tensor nuclear norm can be used as a stand alone regularization technique for
the energy selective (spectral) computed tomography (CT) problem and when
combined with total variation regularization it enhances the regularization
capabilities especially at low energy images where the effects of noise are
most prominent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5368</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5368</id><created>2013-07-19</created><updated>2013-11-09</updated><authors><author><keyname>Guha</keyname><forenames>Saikat</forenames></author><author><keyname>Hayden</keyname><forenames>Patrick</forenames></author><author><keyname>Krovi</keyname><forenames>Hari</forenames></author><author><keyname>Lloyd</keyname><forenames>Seth</forenames></author><author><keyname>Lupo</keyname><forenames>Cosmo</forenames></author><author><keyname>Shapiro</keyname><forenames>Jeffrey H.</forenames></author><author><keyname>Takeoka</keyname><forenames>Masahiro</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Quantum enigma machines and the locking capacity of a quantum channel</title><categories>quant-ph cs.IT math.IT</categories><comments>37 pages</comments><journal-ref>Physical Review X vol. 4, no. 1, page 011016 (January 2014)</journal-ref><doi>10.1103/PhysRevX.4.011016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The locking effect is a phenomenon which is unique to quantum information
theory and represents one of the strongest separations between the classical
and quantum theories of information. The Fawzi-Hayden-Sen (FHS) locking
protocol harnesses this effect in a cryptographic context, whereby one party
can encode n bits into n qubits while using only a constant-size secret key.
The encoded message is then secure against any measurement that an eavesdropper
could perform in an attempt to recover the message, but the protocol does not
necessarily meet the composability requirements needed in quantum key
distribution applications. In any case, the locking effect represents an
extreme violation of Shannon's classical theorem, which states that
information-theoretic security holds in the classical case if and only if the
secret key is the same size as the message. Given this intriguing phenomenon,
it is of practical interest to study the effect in the presence of noise, which
can occur in the systems of both the legitimate receiver and the eavesdropper.
This paper formally defines the locking capacity of a quantum channel as the
maximum amount of locked information that can be reliably transmitted to a
legitimate receiver by exploiting many independent uses of a quantum channel
and an amount of secret key sublinear in the number of channel uses. We provide
general operational bounds on the locking capacity in terms of other well-known
capacities from quantum Shannon theory. We also study the important case of
bosonic channels, finding limitations on these channels' locking capacity when
coherent-state encodings are employed and particular locking protocols for
these channels that might be physically implementable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5377</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5377</id><created>2013-07-20</created><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author></authors><title>Homology and Bisimulation of Asynchronous Transition Systems and Petri
  Nets</title><categories>cs.LO math.AT</categories><comments>21 pages</comments><msc-class>68Q85, 18G35, 18B20, 55U10, 55U15</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Homology groups of labelled asynchronous transition systems and Petri nets
are introduced. Examples of computing the homology groups are given. It is
proved that if labelled asynchronous transition systems are bisimulation
equivalent, then they have isomorphic homology groups. A method of constructing
a Petri net with given homology groups is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5393</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5393</id><created>2013-07-20</created><authors><author><keyname>Patel</keyname><forenames>Miral</forenames></author><author><keyname>Balani</keyname><forenames>Prem</forenames></author></authors><title>Clustering Algorithm for Gujarati Language</title><categories>cs.CL</categories><journal-ref>International Journal for Scientific Research &amp; Development Vol 1,
  Issue 3, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language processing area is still under research. But now a day it is
on platform for worldwide researchers. Natural language processing includes
analyzing the language based on its structure and then tagging of each word
appropriately with its grammar base. Here we have 50,000 tagged words set and
we try to cluster those Gujarati words based on proposed algorithm, we have
defined our own algorithm for processing. Many clustering techniques are
available Ex. Single linkage, complete, linkage,average linkage, Hear no of
clusters to be formed are not known, so it is all depends on the type of data
set provided . Clustering is preprocess for stemming . Stemming is the process
where root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and
plural form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5410</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5410</id><created>2013-07-20</created><authors><author><keyname>Eder</keyname><forenames>Thomas</forenames></author><author><keyname>Rodler</keyname><forenames>Michael</forenames></author><author><keyname>Vymazal</keyname><forenames>Dieter</forenames></author><author><keyname>Zeilinger</keyname><forenames>Markus</forenames></author></authors><title>ANANAS - A Framework For Analyzing Android Applications</title><categories>cs.CR</categories><comments>Paper accepted at First Int. Workshop on Emerging Cyberthreats and
  Countermeasures ECTCM 2013</comments><doi>10.1109/ARES.2013.93</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android is an open software platform for mobile devices with a large market
share in the smartphone sector. The openness of the system as well as its wide
adoption lead to an increasing amount of malware developed for this platform.
ANANAS is an expandable and modular framework for analyzing Android
applications. It takes care of common needs for dynamic malware analysis and
provides an interface for the development of plugins. Adaptability and
expandability have been main design goals during the development process. An
abstraction layer for simple user interaction and phone event simulation is
also part of the framework. It allows an analyst to script the required user
simulation or phone events on demand or adjust the simulation to his needs. Six
plugins have been developed for ANANAS. They represent well known techniques
for malware analysis, such as system call hooking and network traffic analysis.
The focus clearly lies on dynamic analysis, as five of the six plugins are
dynamic analysis methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5420</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5420</id><created>2013-07-20</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>How do Viruses Attack Anti-Virus Programs</title><categories>cs.CR</categories><comments>8 pages</comments><journal-ref>Umakant Mishra, &quot;How do Viruses Attack Anti-virus Programs&quot;,
  TRIZsite Journal, June 2012,
  http://trizsite.tk/trizsite/articles/default.asp?month=Jun&amp;year=2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the anti-viruses run in a trusted kernel level any loophole in the
anti-virus program can enable attackers to take full control over the computer
system and steal data or do serious damages. Hence the anti-virus engines must
be developed with proper security in mind. The ant-virus should be able to any
type of specially created executable files, compression packages or documents
that are intentionally created to exploit the anti-virus weakness.
  Viruses are present in almost every system even though there are anti-viruses
installed. This is because every anti-virus, however good it may be, leads to
some extent of false positives and false negatives. Our faith on the anti-virus
system often makes us more careless about hygienic habits which increases the
possibility of infection. It is necessary for an anti-virus to detect and
destroy the malware before its own files are detected and destroyed by the
malware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5423</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5423</id><created>2013-07-20</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on Keyboard key layout: A TRIZ based analysis</title><categories>cs.HC</categories><comments>TRIZsite Journal, January 2005 (also available at SSRN,
  http://papers.ssrn.com/abstract=931227)</comments><doi>10.2139/ssrn.931227</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard QWERTY keyboard was developed in the late 1800s for the
typewriters. As people were acquainted with that the same layout was retained
for the computer keyboards. Many people feel that the QWERTY layout is not very
efficient layout and there have been many inventions on different layouts of
character keys.
  In order to improve the key arrangement, two major issues should be
addressed. (i) The improved key arrangement should offer significantly improved
productivity. (ii) The training time for learning the improved key arrangement
should be minimized.
  This article analyzes 10 inventions from US patent database each of which
have proposed a new layout giving us some specific advantage. The inventions
try to achieve one or more of the following advantages, viz., to reduce finger
movements during typing, to achieve speed in data entry, to reduce errors in
typing, making keyboard easy to learn, making easy for children to find keys,
reduce stress in hands and finger and suitability for special purpose
computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5426</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5426</id><created>2013-07-20</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on improving keyboard efficiency: A TRIZ based analysis</title><categories>cs.HC</categories><comments>Umakant Mishra, 10 Inventions on improving keyboard efficiency: A
  TRIZ based analysis, TRIZsite Journal, January 2005, (also Available at SSRN:
  http://ssrn.com/abstract=931230)</comments><doi>10.2139/ssrn.931230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A keyboard is the most important input device for a computer. With the
development of technology a basic keyboard does not want to remain confined
within the basic functionalities of a keyboard, rather it wants to go beyond.
There are several inventions which attempt to improve the efficiency of a
conventional keyboard.
  This article illustrates 10 inventions from US Patent database all of which
have proposed very interesting methods for improving the efficiency of a
computer keyboard. Some interesting inventions include, adding toolbar buttons
on the keyboard, power saving method, keyboard-cooling mechanism, robust
keyboard for public use, keyboard enhancement for children`s use.
  Each of these inventions have been analyzed from TRIZ perspective, in many
cases formulating the Ideal Final Result, contradictions and highlighting the
Inventive Principles used in the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5431</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5431</id><created>2013-07-20</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>10 Inventions on keyboard attachments-A TRIZ based analysis</title><categories>cs.HC</categories><comments>Umakant Mishra, 10 Inventions on Keyboard attachments: A TRIZ based
  analysis, TRIZsite Journal, January 2005, (Also available at SSRN
  http://papers.ssrn.com/abstract=931232)</comments><doi>10.2139/ssrn.931232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the primary objective of the keyboard to input data into the
computer, the advanced keyboards keep various other things in mind, such as,
how to use the same keyboard for various other purposes, or how to use the same
keyboard efficiently by using various other attachments to the keyboard. This
objective led to various inventions on keyboard attachments, some of which are
illustrated below in this article.
  This article illustrates 10 inventions on various keyboard attachments from
US patent database. Each different attachment improves the usability of the
keyboard in some way or other. Some attachments illustrated are template
holders, paper holders, pointing device attachments, mouse pad attachments,
wrist rest attachments, storage device attachments, and multimedia attachments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5435</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5435</id><created>2013-07-20</created><authors><author><keyname>Mohammadi</keyname><forenames>Arash</forenames></author><author><keyname>Asif</keyname><forenames>Amir</forenames></author><author><keyname>Zhong</keyname><forenames>Xionghu</forenames></author><author><keyname>Premkumar</keyname><forenames>A. B.</forenames></author></authors><title>Distributed Computation of the Conditional PCRLB for Quantized
  Decentralized Particle Filters</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conditional posterior Cramer-Rao lower bound (PCRLB) is an effective
sensor resource management criteria for large, geographically distributed
sensor networks. Existing algorithms for distributed computation of the PCRLB
(dPCRLB) are based on raw observations leading to significant communication
overhead to the estimation mechanism. This letter derives distributed
computational techniques for determining the conditional dPCRLB for quantized,
decentralized sensor networks (CQ/dPCRLB). Analytical expressions for the
CQ/dPCRLB are derived, which are particularly useful for particle filter-based
estimators. The CQ/dPCRLB is compared for accuracy with its centralized
counterpart through Monte-Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5437</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5437</id><created>2013-07-20</created><authors><author><keyname>Yadav</keyname><forenames>Chanchal</forenames></author><author><keyname>Wang</keyname><forenames>Shuliang</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Algorithm and approaches to handle large Data- A Survey</title><categories>cs.DB</categories><comments>5 pages</comments><journal-ref>International Journal of computer science and network, vol 2,
  issue 3, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining environment produces a large amount of data, that need to be
analyzed, patterns have to be extracted from that to gain knowledge. In this
new era with boom of data both structured and unstructured, in the field of
genomics, meteorology, biology, environmental research and many others, it has
become difficult to process, manage and analyze patterns using traditional
databases and architectures. So, a proper architecture should be understood to
gain knowledge about the Big Data. This paper presents a review of various
algorithms from 1994-2013 necessary for handling such large data set. These
algorithms define various structures and methods implemented to handle Big
Data, also in the paper are listed various tool that were developed for
analyzing them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5438</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5438</id><created>2013-07-20</created><updated>2014-10-05</updated><authors><author><keyname>Li</keyname><forenames>Xiang-yang</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Zhou</keyname><forenames>Yaqin</forenames></author></authors><title>Towards Distribution-Free Multi-Armed Bandits with Combinatorial
  Strategies</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a generalized version of classical multi-armed bandits
(MABs) problem by allowing for arbitrary constraints on constituent bandits at
each decision point. The motivation of this study comes from many situations
that involve repeatedly making choices subject to arbitrary constraints in an
uncertain environment: for instance, regularly deciding which advertisements to
display online in order to gain high click-through-rate without knowing user
preferences, or what route to drive home each day under uncertain weather and
traffic conditions. Assume that there are $K$ unknown random variables (RVs),
i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. At
each decision epoch, we select a strategy, i.e., a subset of RVs, subject to
arbitrary constraints on constituent RVs.
  We then gain a reward that is a linear combination of observations on
selected RVs.
  The performance of prior results for this problem heavily depends on the
distribution of strategies generated by corresponding learning policy. For
example, if the reward-difference between the best and second best strategy
approaches zero, prior result may lead to arbitrarily large regret.
  Meanwhile, when there are exponential number of possible strategies at each
decision point, naive extension of a prior distribution-free policy would cause
poor performance in terms of regret, computation and space complexity.
  To this end, we propose an efficient Distribution-Free Learning (DFL) policy
that achieves zero regret, regardless of the probability distribution of the
resultant strategies.
  Our learning policy has both $O(K)$ time complexity and $O(K)$ space
complexity. In successive generations, we show that even if finding the optimal
strategy at each decision point is NP-hard, our policy still allows for
approximated solutions while retaining near zero-regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5442</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5442</id><created>2013-07-20</created><updated>2013-12-17</updated><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Li</keyname><forenames>Baochun</forenames></author></authors><title>Reducing Electricity Demand Charge for Data Centers with Partial
  Execution</title><categories>cs.NI cs.DC</categories><comments>12 pages</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data centers consume a large amount of energy and incur substantial
electricity cost. In this paper, we study the familiar problem of reducing data
center energy cost with two new perspectives. First, we find, through an
empirical study of contracts from electric utilities powering Google data
centers, that demand charge per kW for the maximum power used is a major
component of the total cost. Second, many services such as Web search tolerate
partial execution of the requests because the response quality is a concave
function of processing time. Data from Microsoft Bing search engine confirms
this observation.
  We propose a simple idea of using partial execution to reduce the peak power
demand and energy cost of data centers. We systematically study the problem of
scheduling partial execution with stringent SLAs on response quality. For a
single data center, we derive an optimal algorithm to solve the workload
scheduling problem. In the case of multiple geo-distributed data centers, the
demand of each data center is controlled by the request routing algorithm,
which makes the problem much more involved. We decouple the two aspects, and
develop a distributed optimization algorithm to solve the large-scale request
routing problem. Trace-driven simulations show that partial execution reduces
cost by $3\%--10.5\%$ for one data center, and by $15.5\%$ for geo-distributed
data centers together with request routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5448</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5448</id><created>2013-07-20</created><updated>2014-01-29</updated><authors><author><keyname>Wilson</keyname><forenames>Greg</forenames></author></authors><title>Software Carpentry: Lessons Learned</title><categories>cs.GL cs.CY physics.ed-ph</categories><comments>11 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Over the last 15 years, Software Carpentry has evolved from a week-long
training course at the US national laboratories into a worldwide volunteer
effort to raise standards in scientific computing. This article explains what
we have learned along the way the challenges we now face, and our plans for the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5449</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5449</id><created>2013-07-20</created><updated>2014-12-22</updated><authors><author><keyname>Besbes</keyname><forenames>O.</forenames></author><author><keyname>Gur</keyname><forenames>Y.</forenames></author><author><keyname>Zeevi</keyname><forenames>A.</forenames></author></authors><title>Non-stationary Stochastic Optimization</title><categories>math.PR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a non-stationary variant of a sequential stochastic optimization
problem, in which the underlying cost functions may change along the horizon.
We propose a measure, termed variation budget, that controls the extent of said
change, and study how restrictions on this budget impact achievable
performance. We identify sharp conditions under which it is possible to achieve
long-run-average optimality and more refined performance measures such as rate
optimality that fully characterize the complexity of such problems. In doing
so, we also establish a strong connection between two rather disparate strands
of literature: adversarial online convex optimization; and the more traditional
stochastic approximation paradigm (couched in a non-stationary setting). This
connection is the key to deriving well performing policies in the latter, by
leveraging structure of optimal policies in the former. Finally, tight bounds
on the minimax regret allow us to quantify the &quot;price of non-stationarity,&quot;
which mathematically captures the added complexity embedded in a temporally
changing environment versus a stationary one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5459</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5459</id><created>2013-07-20</created><updated>2013-10-03</updated><authors><author><keyname>Carli</keyname><forenames>Francesca P.</forenames></author><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author></authors><title>Convex Clustering via Optimal Mass Transport</title><categories>cs.SY</categories><comments>12 pages, 12 figures</comments><msc-class>93A, 94A, 41A</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider approximating distributions within the framework of optimal mass
transport and specialize to the problem of clustering data sets. Distances
between distributions are measured in the Wasserstein metric. The main problem
we consider is that of approximating sample distributions by ones with sparse
support. This provides a new viewpoint to clustering. We propose different
relaxations of a cardinality function which penalizes the size of the support
set. We establish that a certain relaxation provides the tightest convex lower
approximation to the cardinality penalty. We compare the performance of
alternative relaxations on a numerical study on clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5483</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5483</id><created>2013-07-20</created><updated>2013-09-15</updated><authors><author><keyname>Xu</keyname><forenames>Yun</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Approaching Gaussian Relay Network Capacity in the High SNR Regime:
  End-to-End Lattice Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a natural and low-complexity technique for achieving the capacity
of the Gaussian relay network in the high SNR regime. Specifically, we propose
the use of end-to-end structured lattice codes with the amplify-and-forward
strategy, where the source uses a nested lattice code to encode the messages
and the destination decodes the messages by lattice decoding. All intermediate
relays simply amplify and forward the received signals over the network to the
destination. We show that the end-to-end lattice-coded amplify-and-forward
scheme approaches the capacity of the layered Gaussian relay network in the
high SNR regime. Next, we extend our scheme to non-layered Gaussian relay
networks under the amplify-and-forward scheme, which can be viewed as a
Gaussian intersymbol interference (ISI) channel. Compared with other schemes,
our approach is significantly simpler and requires only the end-to-end design
of the lattice precoding and decoding. It does not require any knowledge of the
network topology or the individual channel gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5494</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5494</id><created>2013-07-20</created><authors><author><keyname>Balzano</keyname><forenames>Laura</forenames></author><author><keyname>Wright</keyname><forenames>Stephen J.</forenames></author></authors><title>On GROUSE and Incremental SVD</title><categories>cs.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incremental
algorithm for identifying a subspace of Rn from a sequence of vectors in this
subspace, where only a subset of components of each vector is revealed at each
iteration. Recent analysis has shown that GROUSE converges locally at an
expected linear rate, under certain assumptions. GROUSE has a similar flavor to
the incremental singular value decomposition algorithm, which updates the SVD
of a matrix following addition of a single column. In this paper, we modify the
incremental SVD approach to handle missing data, and demonstrate that this
modified approach is equivalent to GROUSE, for a certain choice of an
algorithmic parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5497</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5497</id><created>2013-07-21</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>A scalable stage-wise approach to large-margin multi-class loss based
  boosting</title><categories>cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable and effective classification model to train multi-class
boosting for multi-class classification problems. Shen and Hao introduced a
direct formulation of multi- class boosting in the sense that it directly
maximizes the multi- class margin [C. Shen and Z. Hao, &quot;A direct formulation
for totally-corrective multi- class boosting&quot;, in Proc. IEEE Conf. Comp. Vis.
Patt. Recogn., 2011]. The major problem of their approach is its high
computational complexity for training, which hampers its application on
real-world problems. In this work, we propose a scalable and simple stage-wise
multi-class boosting method, which also directly maximizes the multi-class
margin. Our approach of- fers a few advantages: 1) it is simple and
computationally efficient to train. The approach can speed up the training time
by more than two orders of magnitude without sacrificing the classification
accuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice of
parameters and empirically demonstrates excellent generalization performance.
Experimental results on challenging multi-class machine learning and vision
tasks demonstrate that the proposed approach substantially improves the
convergence rate and accuracy of the final visual detector at no additional
computational cost compared to existing multi-class boosting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5503</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5503</id><created>2013-07-21</created><authors><author><keyname>Ganczarek</keyname><forenames>Wojciech</forenames></author></authors><title>Mathematical models for epidemic spreading on complex networks</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>Master Thesis in Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model for epidemic spreading on a finite complex network with a
restriction to at most one contamination per time step. Because of a highly
discrete character of the process, the analysis cannot use the continous
approximation, widely exploited for most of the models. Using discrete approach
we investigate the epidemic threshold and the quasi-stationary distribution.
The main result is a theorem about mixing time for the process, which scales
like logarithm of the network size and which is proportional to the inverse of
the distance from the epidemic threshold. In order to present the model in the
full context, we review modern approach to epidemic spreading modeling based on
complex networks and present necessary information about random networks,
discrete-time Markov chains and their quasi-stationary distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5509</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5509</id><created>2013-07-21</created><updated>2016-01-31</updated><authors><author><keyname>Park</keyname><forenames>Jeongmi</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author></authors><title>The double competition multigraph of a digraph</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><msc-class>05C20, 05C75</msc-class><journal-ref>Discrete Mathematics and Theoretical Computer Science, Vol. 17,
  No. 2 (2015) 303-310</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we introduce the notion of the double competition multigraph
of a digraph. We give characterizations of the double competition multigraphs
of arbitrary digraphs, loopless digraphs, reflexive digraphs, and acyclic
digraphs in terms of edge clique partitions of the multigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5510</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5510</id><created>2013-07-21</created><authors><author><keyname>Goldin</keyname><forenames>Dina</forenames></author><author><keyname>Burshtein</keyname><forenames>David</forenames></author></authors><title>Improved Bounds on the Finite Length Scaling of Polar Codes</title><categories>cs.IT math.IT</categories><comments>submitted for publication, IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improved bounds on the blocklength required to communicate over binary-input
channels using polar codes, below some given error probability, are derived.
For that purpose, an improved bound on the number of non-polarizing channels is
obtained. The main result is that the blocklength required to communicate
reliably scales at most as $O((I(W)-R)^{-5.77})$ where $R$ is the code rate and
$I(W)$ the symmetric capacity of the channel, $W$. The results are then
extended to polar lossy source coding at rate $R$ of a source with symmetric
distortion-rate function $D(\cdot)$. The blocklength required scales at most as
$O((D_N-D(R))^{-5.77})$ where $D_N$ is the actual distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5519</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5519</id><created>2013-07-21</created><authors><author><keyname>Eremeev</keyname><forenames>Anton V.</forenames></author><author><keyname>Kovalenko</keyname><forenames>Julia V.</forenames></author></authors><title>Optimal Recombination in Genetic Algorithms</title><categories>cs.NE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys results on complexity of the optimal recombination problem
(ORP), which consists in finding the best possible offspring as a result of a
recombination operator in a genetic algorithm, given two parent solutions. We
consider efficient reductions of the ORPs, allowing to establish polynomial
solvability or NP-hardness of the ORPs, as well as direct proofs of hardness
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5524</identifier>
 <datestamp>2013-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5524</id><created>2013-07-21</created><updated>2013-08-28</updated><authors><author><keyname>Domb</keyname><forenames>Yuval</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>The Random Coding Bound Is Tight for the Average Linear Code or Lattice</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1973, Gallager proved that the random-coding bound is exponentially tight
for the random code ensemble at all rates, even below expurgation. This result
explained that the random-coding exponent does not achieve the expurgation
exponent due to the properties of the random ensemble, irrespective of the
utilized bounding technique. It has been conjectured that this same behavior
holds true for a random ensemble of linear codes. This conjecture is proved in
this paper. Additionally, it is shown that this property extends to Poltyrev's
random-coding exponent for a random ensemble of lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5534</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5534</id><created>2013-07-21</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>A New Optimization Approach Based on Rotational Mutation and Crossover
  Operator</title><categories>cs.NE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating a global optimal point in many global optimization problems in
large space is required to more calculations. In this paper, there is presented
a new approach for the continuous functions optimization with rotational
mutation and crossover operator. This proposed method (RMC) starts from the
point which has best fitness value by elitism mechanism and after that
rotational mutation and crossover operator are used to reach optimal point. RMC
method is implemented by GA (Briefly RMCGA) and is compared with other
wellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] and
numerical and simulating results show that RMCGA achieve global optimal point
with more decision by smaller generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5541</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5541</id><created>2013-07-21</created><authors><author><keyname>Subramanian</keyname><forenames>Vijay G.</forenames></author><author><keyname>Honig</keyname><forenames>Mike</forenames></author><author><keyname>Berry</keyname><forenames>Randy</forenames></author></authors><title>Comparative Statics On The Allocation Of Spectrum</title><categories>cs.NI cs.GT</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Allocation of spectrum is an important policy issue and decisions taken have
ramifications for future growth of wireless communications and achieving
universal connectivity. In this paper, on a common footing we compare the
social welfare obtained from the allocation of new spectrum under different
alternatives: to licensed providers in monopolistic, oligopolistic and
perfectly competitive settings, and for unlicensed access. For this purpose we
use mathematical models of competition in congestible resources. Initially we
assume that any new bandwidth is available for free, but we also generalize our
results to include investment decisions when prices are charged for bandwidth
acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5547</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5547</id><created>2013-07-21</created><authors><author><keyname>McConnell</keyname><forenames>Ross M.</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author></authors><title>Linear-Time Recognition of Probe Interval Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interval graph for a set of intervals on a line consists of one vertex
for each interval, and an edge for each intersecting pair of intervals. A probe
interval graph is a variant that is motivated by an application to genomics,
where the intervals are partitioned into two sets: probes and non-probes. The
graph has an edge between two vertices if they intersect and at least one of
them is a probe. We give a linear-time algorithm for determining whether a
given graph and partition of vertices into probes and non-probes is a probe
interval graph. If it is, we give a layout of intervals that proves this. We
can also determine whether the layout of the intervals is uniquely constrained
within the same time bound. As part of the algorithm, we solve the
consecutive-ones probe matrix problem in linear time, develop algorithms for
operating on PQ trees, and give results that relate PQ trees for different
submatrices of a consecutive-ones matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5549</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5549</id><created>2013-07-21</created><authors><author><keyname>Wu</keyname><forenames>Youlong</forenames></author><author><keyname>Minero</keyname><forenames>Paolo</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Insufficiency of Linear-Feedback Schemes In Gaussian Broadcast Channels
  with Common Message</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the $K\geq 2$-user memoryless Gaussian broadcast channel (BC)
with feedback and common message only. We show that linear-feedback schemes
with a message point, in the spirit of Schalkwijk &amp; Kailath's scheme for
point-to-point channels or Ozarow &amp; Leung's scheme for BCs with private
messages, are strictly suboptimal for this setup. Even with perfect feedback,
the largest rate achieved by these schemes is strictly smaller than capacity
$C$ (which is the same with and without feedback). In the extreme case where
the number of receivers $K\to \infty$, the largest rate achieved by
linear-feedback schemes with a message point tends to 0.
  To contrast this negative result, we describe a scheme for
\emph{rate-limited} feedback that uses the feedback in an intermittent way,
i.e., the receivers send feedback signals only in few channel uses. This scheme
achieves all rates $R$ up to capacity $C$ with an $L$-th order exponential
decay of the probability of error if the feedback rate $R_{\textnormal{fb}}$ is
at least $(L-1)R$ for some positive integer $L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5551</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5551</id><created>2013-07-21</created><authors><author><keyname>Ferradans</keyname><forenames>Sira</forenames></author><author><keyname>Papadakis</keyname><forenames>Nicolas</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames></author><author><keyname>Aujol</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Regularized Discrete Optimal Transport</title><categories>cs.CV cs.DM math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a generalization of the discrete optimal transport,
with applications to color image manipulations. This new formulation includes a
relaxation of the mass conservation constraint and a regularization term. These
two features are crucial for image processing tasks, which necessitate to take
into account families of multimodal histograms, with large mass variation
across modes.
  The corresponding relaxed and regularized transportation problem is the
solution of a convex optimization problem. Depending on the regularization
used, this minimization can be solved using standard linear programming methods
or first order proximal splitting schemes.
  The resulting transportation plan can be used as a color transfer map, which
is robust to mass variation across images color palettes. Furthermore, the
regularization of the transport plan helps to remove colorization artifacts due
to noise amplification.
  We also extend this framework to the computation of barycenters of
distributions. The barycenter is the solution of an optimization problem, which
is separately convex with respect to the barycenter and the transportation
plans, but not jointly convex. A block coordinate descent scheme converges to a
stationary point of the energy. We show that the resulting algorithm can be
used for color normalization across several images. The relaxed and regularized
barycenter defines a common color palette for those images. Applying color
transfer toward this average palette performs a color normalization of the
input images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5552</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5552</id><created>2013-07-21</created><authors><author><keyname>Wu</keyname><forenames>Youlong</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Any Positive Feedback Rate Increases the Capacity of Strictly Less-Noisy
  Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>To be presented at ITW 2013 in Seville</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two coding schemes for discrete memoryless broadcast channels
(DMBCs) with rate-limited feedback from only one receiver. For any positive
feedback rate and for the class of strictly less-noisy DMBCs, our schemes
strictly improve over the no-feedback capacity region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5582</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5582</id><created>2013-07-21</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Random Rates for 0-Extension and Low-Diameter Decompositions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of partitioning an arbitrary metric space into pieces of
diameter at most \Delta, such every pair of points is separated with relatively
low probability. We propose a rate-based algorithm inspired by
multiplicatively-weighted Voronoi diagrams, and prove it has optimal
trade-offs. This also gives us another logarithmic approximation algorithm for
the 0-extension problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5583</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5583</id><created>2013-07-21</created><authors><author><keyname>Hollmann</keyname><forenames>Henk D. L.</forenames></author><author><keyname>Poh</keyname><forenames>Wencin</forenames></author></authors><title>Characterizations and construction methods for linear functional-repair
  storage codes</title><categories>cs.IT math.IT</categories><comments>ISIT 2013, Istanbul, Turkey, 8-12 July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a precise characterization of linear functional-repair storage
codes in terms of {\em admissible states/}, with each state made up from a
collection of vector spaces over some fixed finite field. To illustrate the
usefulness of our characterization, we provide several applications. We first
describe a simple construction of functional-repair storage codes for a family
of code parameters meeting the cutset bound outside the MBR and MSR points;
these codes are conjectured to have optimal rate with respect to their repair
locality. Then, we employ our characterization to develop a construction method
to obtain functional repair codes for given parameters using symmetry groups,
which can be used both to find new codes and to improve known ones. As an
example of the latter use, we describe a beautiful functional-repair storage
code that was found by this method, with parameters belonging to the family
investigated earlier, which can be specified in terms of only eight different
vector spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5591</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5591</id><created>2013-07-22</created><authors><author><keyname>Mukherjee</keyname><forenames>Subra</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>A Novel Equation based Classifier for Detecting Human in Images</title><categories>cs.CV</categories><comments>published with international journal of Computer Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 72(6):9-16, June
  2013</journal-ref><doi>10.5120/12496-7272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape based classification is one of the most challenging tasks in the field
of computer vision. Shapes play a vital role in object recognition. The basic
shapes in an image can occur in varying scale, position and orientation. And
specially when detecting human, the task becomes more challenging owing to the
largely varying size, shape, posture and clothing of human. So, in our work we
detect human, based on the head-shoulder shape as it is the most unvarying part
of human body. Here, firstly a new and a novel equation named as the Omega
Equation that describes the shape of human head-shoulder is developed and based
on this equation, a classifier is designed particularly for detecting human
presence in a scene. The classifier detects human by analyzing some of the
discriminative features of the values of the parameters obtained from the Omega
equation. The proposed method has been tested on a variety of shape dataset
taking into consideration the complexities of human head-shoulder shape. In all
the experiments the proposed method demonstrated satisfactory results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5592</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5592</id><created>2013-07-22</created><updated>2013-11-25</updated><authors><author><keyname>Hou</keyname><forenames>Zhe</forenames></author><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author></authors><title>Proof search for propositional abstract separation logics via labelled
  sequents</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation logics are a family of extensions of Hoare logic for reasoning
about programs that mutate memory. These logics are &quot;abstract&quot; because they are
independent of any particular concrete memory model. Their assertion languages,
called propositional abstract separation logics, extend the logic of (Boolean)
Bunched Implications (BBI) in various ways.
  We develop a modular proof theory for various propositional abstract
separation logics using cut-free labelled sequent calculi. We first extend the
cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et
al's original logic of separation algebras by adding sound rules for
partial-determinism and cancellativity, while preserving cut-elimination. We
prove the completeness of our calculus via a sound intermediate calculus that
enables us to construct counter-models from the failure to find a proof. We
then capture other propositional abstract separation logics by adding sound
rules for indivisible unit and disjointness, while maintaining completeness. We
present a theorem prover based on our labelled calculus for these propositional
abstract separation logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5599</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5599</id><created>2013-07-22</created><authors><author><keyname>Kumar</keyname><forenames>M. Naresh</forenames></author></authors><title>Performance comparison of State-of-the-art Missing Value Imputation
  Algorithms on Some Bench mark Datasets</title><categories>cs.LG stat.ML</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making from data involves identifying a set of attributes that
contribute to effective decision making through computational intelligence. The
presence of missing values greatly influences the selection of right set of
attributes and this renders degradation in classification accuracies of the
classifiers. As missing values are quite common in data collection phase during
field experiments or clinical trails appropriate handling would improve the
classifier performance. In this paper we present a review of recently developed
missing value imputation algorithms and compare their performance on some bench
mark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5612</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5612</id><created>2013-07-22</created><updated>2013-07-23</updated><authors><author><keyname>Agarwal</keyname><forenames>Jatin</forenames></author><author><keyname>Moidu</keyname><forenames>Nadeem</forenames></author><author><keyname>Kothapalli</keyname><forenames>Kishore</forenames></author><author><keyname>Srinathan</keyname><forenames>Kannan</forenames></author></authors><title>Efficient Range Reporting of Convex Hull</title><categories>cs.CG</categories><comments>This work was previously submitted to IWOCA 2013 and was rejected.
  The work with better results will appear in proceedings of CCCG 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reporting convex hull points in an orthogonal
range query in two dimensions. Formally, let $P$ be a set of $n$ points in
$\mathbb{R}^{2}$. A point lies on the convex hull of a point set $S$ if it lies
on the boundary of the minimum convex polygon formed by $S$. In this paper, we
are interested in finding the points that lie on the boundary of the convex
hull of the points in $P$ that also fall with in an orthogonal
range$[x_{lt},x_{rt}]\times{}[y_b, y_t]$. We propose a $O(n \log^{2} n) $ space
data structure that can support reporting points on a convex hull inside an
orthogonal range query, in time $O(\log^{3} n + h)$. Here $h$ is the size of
the output. This work improves the result of (Brass et al. 2013) \cite{brass}
that builds a data structure that uses $O(n \log^{2} n)$ space and has a
$O(\log^{5} n + h)$ query time. Additionally, we show that our data structure
can be modified slightly to solve other related problems. For instance, for
counting the number of points on the convex hull in an orthogonal query
rectangle, we propose an $O(n \log^{2}n)$ space data structure that can be
queried upon in $O(\log^{3} n)$ time. We also propose a $O(n \log^{2} n) $
space data structure that can compute the $area$ and $perimeter$ of the convex
hull inside an orthogonal range query in $O(\log^{3} n$) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5613</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5613</id><created>2013-07-22</created><updated>2014-12-22</updated><authors><author><keyname>Chatzidiamantis</keyname><forenames>Nestor</forenames></author><author><keyname>Matskani</keyname><forenames>Evangelia</forenames></author><author><keyname>Georgiadis</keyname><forenames>Leonidas</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Optimal Primary-Secondary user Cooperation Policies in Cognitive Radio
  Networks</title><categories>cs.NI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks, secondary users (SUs) may cooperate with the
primary user (PU), so that the success probability of PU transmissions are
improved, while SUs obtain more transmission opportunities. Thus, SUs have to
take intelligent decisions on whether to cooperate or not and with what power
level, in order to maximize their throughput subject to average power
constraints. Cooperation policies in this framework require the solution of a
constrained Markov decision problem with infinite state space. In our work, we
restrict attention to the class of stationary policies that take randomized
decisions in every time slot based only on spectrum sensing. The proposed class
of policies is shown to achieve the same set of SU rates as the more general
policies, and enlarge the stability region of PU queue. Moreover, algorithms
for the distributed calculation of the set of probabilities used by the
proposed class of policies are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5617</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5617</id><created>2013-07-22</created><updated>2015-12-03</updated><authors><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>von Falkenhausen</keyname><forenames>Philipp</forenames></author></authors><title>Robust Quantitative Comparative Statics for a Multimarket Paradox</title><categories>cs.GT q-fin.GN</categories><comments>23 pages, 1 figure</comments><msc-class>91B66</msc-class><acm-class>J.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a quantitative approach to comparative statics that allows to
bound the maximum effect of an exogenous parameter change on a system's
equilibrium. The motivation for this approach is a well known paradox in
multimarket Cournot competition, where a positive price shock on a monopoly
market may actually reduce the monopolist's profit. We use our approach to
quantify for the first time the worst case profit reduction for multimarket
oligopolies exposed to arbitrary positive price shocks. For markets with affine
price functions and firms with convex cost technologies, we show that the
relative profit loss of any firm is at most 25% no matter how many firms
compete in the oligopoly. We further investigate the impact of positive price
shocks on total profit of all firms as well as on social welfare. We find tight
bounds also for these measures showing that total profit and social welfare
decreases by at most 25% and 16.6%, respectively. Finally, we show that in our
model, mixed, correlated and coarse correlated equilibria are essentially
unique, thus, all our bounds apply to these game solutions as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5619</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5619</id><created>2013-07-22</created><authors><author><keyname>Abraham</keyname><forenames>Uri</forenames></author><author><keyname>Amram</keyname><forenames>Gal</forenames></author></authors><title>On the Mailbox Problem</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Mailbox Problem was described and solved by Aguilera, Gafni, and Lamport
in their 2010 DC paper with an algorithm that uses two flag registers that
carry 14 values each. An interesting problem that they ask is whether there is
a mailbox algorithm with smaller flag values. We give a positive answer by
describing a mailbox algorithm with 6 and 4 values in the two flag registers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5636</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5636</id><created>2013-07-22</created><updated>2015-06-03</updated><authors><author><keyname>Maathuis</keyname><forenames>Marloes H.</forenames></author><author><keyname>Colombo</keyname><forenames>Diego</forenames></author></authors><title>A generalized back-door criterion</title><categories>stat.ME cs.AI</categories><comments>Published at http://dx.doi.org/10.1214/14-AOS1295 in the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1295</report-no><journal-ref>Annals of Statistics 2015, Vol. 43, No. 3, 1060-1088</journal-ref><doi>10.1214/14-AOS1295</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Pearl's back-door criterion for directed acyclic graphs (DAGs)
to more general types of graphs that describe Markov equivalence classes of
DAGs and/or allow for arbitrarily many hidden variables. We also give easily
checkable necessary and sufficient graphical criteria for the existence of a
set of variables that satisfies our generalized back-door criterion, when
considering a single intervention and a single outcome variable. Moreover, if
such a set exists, we provide an explicit set that fulfills the criterion. We
illustrate the results in several examples. R-code is available in the
R-package pcalg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5641</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5641</id><created>2013-07-22</created><authors><author><keyname>Dinger</keyname><forenames>Steven</forenames></author><author><keyname>Dickens</keyname><forenames>John</forenames></author><author><keyname>Pantanowitz</keyname><forenames>Adam</forenames></author></authors><title>Robotic Arm for Remote Surgery</title><categories>cs.RO</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in telecommunications have enabled surgeons to operate
remotely on patients with the use of robotics. The investigation and testing of
remote surgery using a robotic arm is presented. The robotic arm is designed to
have four degrees of freedom that track the surgeon's x, y, z positions and the
rotation angle of the forearm {\theta}. The system comprises two main
subsystems viz. the detecting and actuating systems. The detection system uses
infrared light-emitting diodes, a retroreflective bracelet and two infrared
cameras which as a whole determine the coordinates of the surgeon's forearm.
The actuation system, or robotic arm, is based on a lead screw mechanism which
can obtain a maximum speed of 0.28 m/s with a 1.5 degree/step for the
end-effector. The infrared detection and encoder resolutions are below 0.6
mm/pixel and 0.4 mm respectively, which ensures the robotic arm can operate
precisely. The surgeon is able to monitor the patient with the use of a
graphical user interface on the display computer. The lead screw system is
modelled and compared to experimentation results. The system is controlled
using a simple proportional-integrator (PI) control scheme which is implemented
on a dSpace control unit. The control design results in a rise time of less
than 0.5 s, a steady-state error of less than 1 mm and settling time of less
than 1.4 s. The system accumulates, over an extended period of time, an error
of approximately 4 mm due to inertial effects of the robotic arm. The results
show promising system performance characteristics for a relatively inexpensive
solution to a relatively advanced application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5647</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5647</id><created>2013-07-22</created><authors><author><keyname>Piazza</keyname><forenames>Roberto</forenames></author></authors><title>On house renovation and coauthoring (with a little excursus on the Holy
  Grail of bibliometrics)</title><categories>physics.soc-ph cs.DL</categories><comments>Because of the reasons stated in the abstract, I am not planning to
  submit this paper to any journal. But, of course, I am not against it. (4
  pages, 3 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More than a paper, this is just a little divertissement about coauthoring,
the Hirsch h-index, and bibliometric evaluation in general. Without pretending
to yield any general conclusions, what I found rummaging through the physics
literature made me think quite a bit. I hope the same will happen to my
readers, even it they will likely be much less than 25, which is the audience
one of the greatest Italian writers (whom, is left to the reader to single out)
addresses to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5653</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5653</id><created>2013-07-22</created><authors><author><keyname>Chau</keyname><forenames>Duc Phu</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Badie</keyname><forenames>Julien</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bremond</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Thonnat</keyname><forenames>Monique</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Online Tracking Parameter Adaptation based on Evaluation</title><categories>cs.CV</categories><comments>IEEE International Conference on Advanced Video and Signal-based
  Surveillance (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter tuning is a common issue for many tracking algorithms. In order to
solve this problem, this paper proposes an online parameter tuning to adapt a
tracking algorithm to various scene contexts. In an offline training phase,
this approach learns how to tune the tracker parameters to cope with different
contexts. In the online control phase, once the tracking quality is evaluated
as not good enough, the proposed approach computes the current context and
tunes the tracking parameters using the learned values. The experimental
results show that the proposed approach improves the performance of the
tracking algorithm and outperforms recent state of the art trackers. This paper
brings two contributions: (1) an online tracking evaluation, and (2) a method
to adapt online tracking parameters to scene contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5655</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5655</id><created>2013-07-22</created><updated>2013-07-26</updated><authors><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Fast polynomial evaluation and composition</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The library \emph{fast\_polynomial} for Sage compiles multivariate
polynomials for subsequent fast evaluation. Several evaluation schemes are
handled, such as H\&quot;orner, divide and conquer and new ones can be added easily.
Notably, a new scheme is introduced that improves the classical divide and
conquer scheme when the number of terms is not a pure power of two. Natively,
the library handles polynomials over gmp big integers, boost intervals, python
numeric types. And any type that supports addition and multiplication can
extend the library thanks to the template design. Finally, the code is
parallelized for the divide and conquer schemes, and memory allocation is
localized and optimized for the different evaluation schemes. This extended
abstract presents the concepts behind the \emph{fast\_polynomial} library. The
sage package can be downloaded at
\url{http://trac.sagemath.org/sage_trac/ticket/13358}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5664</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5664</id><created>2013-07-22</created><updated>2015-03-26</updated><authors><author><keyname>Tang</keyname><forenames>Bin</forenames></author><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author><author><keyname>Lu</keyname><forenames>Sanglu</forenames></author></authors><title>Expander Chunked Codes</title><categories>cs.IT math.IT</categories><comments>26 pages, 3 figures, submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chunked codes are efficient random linear network coding (RLNC) schemes with
low computational cost, where the input packets are encoded into small chunks
(i.e., subsets of the coded packets). During the network transmission, RLNC is
performed within each chunk. In this paper, we first introduce a simple
transfer matrix model to characterize the transmission of chunks, and derive
some basic properties of the model to facilitate the performance analysis. We
then focus on the design of overlapped chunked codes, a class of chunked codes
whose chunks are non-disjoint subsets of input packets, which are of special
interest since they can be encoded with negligible computational cost and in a
causal fashion. We propose expander chunked (EC) codes, the first class of
overlapped chunked codes that have an analyzable performance,where the
construction of the chunks makes use of regular graphs. Numerical and
simulation results show that in some practical settings, EC codes can achieve
rates within 91 to 97 percent of the optimum and outperform the
state-of-the-art overlapped chunked codes significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5667</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5667</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>New Optimization Approach Using Clustering-Based Parallel Genetic
  Algorithm</title><categories>cs.NE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many global Optimization Problems, it is required to evaluate a global
point (min or max) in large space that calculation effort is very high. In this
paper is presented new approach for optimization problem with subdivision
labeling method (SLM) but in this method for higher dimensional has high
calculation effort. Clustering-Based Parallel Genetic Algorithm (CBPGA) in
optimization problems is one of the solutions of this problem. That the initial
population is crossing points and subdividing in each step is according to
mutation. After labeling all of crossing points, selecting is according to
polytope that has complete label. In this method we propose an algorithm, based
on parallelization scheme using master-slave. SLM algorithm is implemented by
CBPGA and compared the experimental results. The numerical examples and
numerical results show that SLMCBPGA is improved speed up and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5674</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5674</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>Solving Traveling Salesman Problem by Marker Method</title><categories>cs.NE cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use marker method and propose a new mutation operator that
selects the nearest neighbor among all near neighbors solving Traveling
Salesman Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5675</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5675</id><created>2013-07-22</created><authors><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Models, Entropy and Information of Temporal Social Networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>21 pages, 9 figures; Temporal Networks, P. Holme, and J. Saram\&quot;aki
  (Eds.) Understanding Complex Systems Series, Springer (2013)</comments><doi>10.1007/978-3-642-36461-7_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal social networks are characterized by {heterogeneous} duration of
contacts, which can either follow a power-law distribution, such as in
face-to-face interactions, or a Weibull distribution, such as in mobile-phone
communication. Here we model the dynamics of face-to-face interaction and
mobile phone communication by a reinforcement dynamics, which explains the data
observed in these different types of social interactions. We quantify the
information encoded in the dynamics of these networks by the entropy of
temporal networks. Finally, we show evidence that human dynamics is able to
modulate the information present in social network dynamics when it follows
circadian rhythms and when it is interfacing with a new technology such as the
mobile-phone communication technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5679</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5679</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>Sub-Dividing Genetic Method for Optimization Problems</title><categories>cs.NE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, optimization problem have more application in all major but they
have problem in computation. Computation global point in continuous functions
have high calculation and this became clearer in large space .In this paper, we
proposed Sub- Dividing Genetic Method(SGM) that have less computation than
other method for achieving global points . This method userotation mutation and
crossover based sub-division method that sub diving method is used for minimize
search space and rotation mutation with crossover is used for finding global
optimal points. In experimental, SGM algorithm is implemented on De Jong
function. The numerical examples show that SGM is performed more optimal than
other methods such as Grefensstette, Random Value, and PNG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5684</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5684</id><created>2013-07-22</created><authors><author><keyname>Satel</keyname><forenames>Jason</forenames></author><author><keyname>Story</keyname><forenames>Ross</forenames></author><author><keyname>Hilchey</keyname><forenames>Matthew D.</forenames></author><author><keyname>Wang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Klein</keyname><forenames>Raymond M.</forenames></author></authors><title>Using a Dynamic Neural Field Model to Explore a Direct Collicular
  Inhibition Account of Inhibition of Return</title><categories>q-bio.NC cs.CV</categories><report-no>ISACS/2013/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the interval between a transient ash of light (a &quot;cue&quot;) and a second
visual response signal (a &quot;target&quot;) exceeds at least 200ms, responding is
slowest in the direction indicated by the first signal. This phenomenon is
commonly referred to as inhibition of return (IOR). The dynamic neural field
model (DNF) has proven to have broad explanatory power for IOR, effectively
capturing many empirical results. Previous work has used a short-term
depression (STD) implementation of IOR, but this approach fails to explain many
behavioral phenomena observed in the literature. Here, we explore a variant
model of IOR involving a combination of STD and delayed direct collicular
inhibition. We demonstrate that this hybrid model can better reproduce
established behavioural results. We use the results of this model to propose
several experiments that would yield particularly valuable insight into the
nature of the neurophysiological mechanisms underlying IOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5685</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5685</id><created>2013-07-22</created><authors><author><keyname>Brunelle</keyname><forenames>Justin F.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>An Evaluation of Caching Policies for Memento TimeMaps</title><categories>cs.DL</categories><comments>JCDL2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As defined by the Memento Framework, TimeMaps are ma-chine-readable lists of
time-specific copies -- called &quot;mementos&quot; -- of an archived original resource.
In theory, as an archive acquires additional mementos over time, a TimeMap
should be monotonically increasing. However, there are reasons why the number
of mementos in a TimeMap would decrease, for example: archival redaction of
some or all of the mementos, archival restructuring, and transient errors on
the part of one or more archives. We study TimeMaps for 4,000 original
resources over a three month period, note their change patterns, and develop a
caching algorithm for TimeMaps suitable for a reverse proxy in front of a
Memento aggregator. We show that TimeMap cardinality is constant or
monotonically increasing for 80.2% of all TimeMap downloads observed in the
observation period. The goal of the caching algorithm is to exploit the ideally
monotonically increasing nature of TimeMaps and not cache responses with fewer
mementos than the already cached TimeMap. This new caching algorithm uses
conditional cache replacement and a Time To Live (TTL) value to ensure the user
has access to the most complete TimeMap available. Based on our empirical data,
a TTL of 15 days will minimize the number of mementos missed by users, and
minimize the load on archives contributing to TimeMaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5691</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5691</id><created>2013-07-22</created><authors><author><keyname>Riche</keyname><forenames>Nicolas</forenames></author><author><keyname>Duvinage</keyname><forenames>Matthieu</forenames></author><author><keyname>Mancas</keyname><forenames>Matei</forenames></author><author><keyname>Gosselin</keyname><forenames>Bernard</forenames></author><author><keyname>Dutoit</keyname><forenames>Thierry</forenames></author></authors><title>A study of parameters affecting visual saliency assessment</title><categories>cs.CV</categories><report-no>ISACS/2013/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the early 2000s, computational visual saliency has been a very active
research area. Each year, more and more new models are published in the main
computer vision conferences. Nowadays, one of the big challenges is to find a
way to fairly evaluate all of these models. In this paper, a new framework is
proposed to assess models of visual saliency. This evaluation is divided into
three experiments leading to the proposition of a new evaluation framework.
Each experiment is based on a basic question: 1) there are two ground truths
for saliency evaluation: what are the differences between eye fixations and
manually segmented salient regions?, 2) the properties of the salient regions:
for example, do large, medium and small salient regions present different
difficulties for saliency models? and 3) the metrics used to assess saliency
models: what advantages would there be to mix them with PCA? Statistical
analysis is used here to answer each of these three questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5693</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5693</id><created>2013-07-22</created><authors><author><keyname>Kavak</keyname><forenames>Yasin</forenames></author><author><keyname>Erdem</keyname><forenames>Erkut</forenames></author><author><keyname>Erdem</keyname><forenames>Aykut</forenames></author></authors><title>Visual saliency estimation by integrating features using multiple kernel
  learning</title><categories>cs.CV</categories><report-no>ISACS/2013/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few decades, significant achievements have been attained in
predicting where humans look at images through different computational models.
However, how to determine contributions of different visual features to overall
saliency still remains an open problem. To overcome this issue, a recent class
of models formulates saliency estimation as a supervised learning problem and
accordingly apply machine learning techniques. In this paper, we also address
this challenging problem and propose to use multiple kernel learning (MKL) to
combine information coming from different feature dimensions and to perform
integration at an intermediate level. Besides, we suggest to use responses of a
recently proposed filterbank of object detectors, known as Object-Bank, as
additional semantic high-level features. Here we show that our MKL-based
framework together with the proposed object-specific features provide
state-of-the-art performance as compared to SVM or AdaBoost-based saliency
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5697</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5697</id><created>2013-07-22</created><updated>2014-04-30</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Mladenov</keyname><forenames>Martin</forenames></author><author><keyname>Selman</keyname><forenames>Erkal</forenames></author></authors><title>Dimension Reduction via Colour Refinement</title><categories>cs.DS cs.DM cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Colour refinement is a basic algorithmic routine for graph isomorphism
testing, appearing as a subroutine in almost all practical isomorphism solvers.
It partitions the vertices of a graph into &quot;colour classes&quot; in such a way that
all vertices in the same colour class have the same number of neighbours in
every colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, and
Ullman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997)
established a tight correspondence between colour refinement and fractional
isomorphisms of graphs, which are solutions to the LP relaxation of a natural
ILP formulation of graph isomorphism.
  We introduce a version of colour refinement for matrices and extend existing
quasilinear algorithms for computing the colour classes. Then we generalise the
correspondence between colour refinement and fractional automorphisms and
develop a theory of fractional automorphisms and isomorphisms of matrices.
  We apply our results to reduce the dimensions of systems of linear equations
and linear programs. Specifically, we show that any given LP L can efficiently
be transformed into a (potentially) smaller LP L' whose number of variables and
constraints is the number of colour classes of the colour refinement algorithm,
applied to a matrix associated with the LP. The transformation is such that we
can easily (by a linear mapping) map both feasible and optimal solutions back
and forth between the two LPs. We demonstrate empirically that colour
refinement can indeed greatly reduce the cost of solving linear programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5702</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5702</id><created>2013-07-22</created><authors><author><keyname>Dodge</keyname><forenames>Samuel F.</forenames></author><author><keyname>Karam</keyname><forenames>Lina J.</forenames></author></authors><title>Is Bottom-Up Attention Useful for Scene Recognition?</title><categories>cs.CV</categories><report-no>ISACS/2013/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human visual system employs a selective attention mechanism to understand
the visual world in an eficient manner. In this paper, we show how
computational models of this mechanism can be exploited for the computer vision
application of scene recognition. First, we consider saliency weighting and
saliency pruning, and provide a comparison of the performance of different
attention models in these approaches in terms of classification accuracy.
Pruning can achieve a high degree of computational savings without
significantly sacrificing classification accuracy. In saliency weighting,
however, we found that classification performance does not improve. In
addition, we present a new method to incorporate salient and non-salient
regions for improved classification accuracy. We treat the salient and
non-salient regions separately and combine them using Multiple Kernel Learning.
We evaluate our approach using the UIUC sports dataset and find that with a
small training size, our method improves upon the classification accuracy of
the baseline bag of features approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5708</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5708</id><created>2013-07-22</created><authors><author><keyname>Shuman</keyname><forenames>David I</forenames></author><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Vertex-Frequency Analysis on Graphs</title><categories>math.FA cs.IT cs.SI math.IT</categories><comments>Submitted to Applied and Computational Harmonic Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in the area of signal processing on graphs is to
design dictionaries and transform methods to identify and exploit structure in
signals on weighted graphs. To do so, we need to account for the intrinsic
geometric structure of the underlying graph data domain. In this paper, we
generalize one of the most important signal processing tools - windowed Fourier
analysis - to the graph setting. Our approach is to first define generalized
convolution, translation, and modulation operators for signals on graphs, and
explore related properties such as the localization of translated and modulated
graph kernels. We then use these operators to define a windowed graph Fourier
transform, enabling vertex-frequency analysis. When we apply this transform to
a signal with frequency components that vary along a path graph, the resulting
spectrogram matches our intuition from classical discrete-time signal
processing. Yet, our construction is fully generalized and can be applied to
analyze signals on any undirected, connected, weighted graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5710</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5710</id><created>2013-07-22</created><authors><author><keyname>T&#xfc;nnermann</keyname><forenames>Jan</forenames></author><author><keyname>Enns</keyname><forenames>Dieter</forenames></author><author><keyname>Mertsching</keyname><forenames>B&#xe4;rbel</forenames></author></authors><title>Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based
  Artificial Visual Attention</title><categories>cs.CV</categories><report-no>ISACS/2013/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region-based artificial attention constitutes a framework for bio-inspired
attentional processes on an intermediate abstraction level for the use in
computer vision and mobile robotics. Segmentation algorithms produce regions of
coherently colored pixels. These serve as proto-objects on which the
attentional processes determine image portions of relevance. A single
region---which not necessarily represents a full object---constitutes the focus
of attention. For many post-attentional tasks, however, such as identifying or
tracking objects, single segments are not sufficient. Here, we present a
saliency-guided approach that groups regions that potentially belong to the
same object based on proximity and similarity of motion. We compare our results
to object selection by thresholding saliency maps and a further
attention-guided strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5713</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5713</id><created>2013-07-22</created><authors><author><keyname>Zhao</keyname><forenames>Min</forenames></author><author><keyname>Marquez</keyname><forenames>Andre G.</forenames></author></authors><title>Understanding Humans' Strategies in Maze Solving</title><categories>cs.CV cs.AI q-bio.NC</categories><report-no>ISACS/2013/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigating through a visual maze relies on the strategic use of eye movements
to select and identify the route. When navigating the maze, there are
trade-offs between exploring to the environment and relying on memory. This
study examined strategies used to navigating through novel and familiar mazes
that were viewed from above and traversed by a mouse cursor. Eye and mouse
movements revealed two modes that almost never occurred concurrently:
exploration and guidance. Analyses showed that people learned mazes and were
able to devise and carry out complex, multi-faceted strategies that traded-off
visual exploration against active motor performance. These strategies took into
account available visual information, memory, confidence, the estimated cost in
time for exploration, and idiosyncratic tolerance for error. Understanding the
strategies humans used for maze solving is valuable for applications in
cognitive neuroscience as well as in AI, robotics and human-robot interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5714</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5714</id><created>2013-07-22</created><authors><author><keyname>Di Pietro</keyname><forenames>Roberto</forenames></author><author><keyname>Oligeri</keyname><forenames>Gabriele</forenames></author></authors><title>Silence is Golden: exploiting jamming and radio silence to communicate</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jamming techniques require just moderate resources to be deployed, while
their effectiveness in disrupting communications is unprecedented. In this
paper we introduce several contributions to jamming mitigation. In particular,
we introduce a novel adversary model that has both (unlimited) jamming reactive
capabilities as well as powerful (but limited) proactive jamming capabilities.
Under this powerful but yet realistic adversary model, the communication
bandwidth provided by current anti-jamming solutions drops to zero. We then
present Silence is Golden (SiG): a novel anti jamming protocol that,
introducing a tunable, asymmetric communication channel, is able to mitigate
the adversary capabilities, enabling the parties to communicate. For instance,
with SiG it is possible to deliver a 128 bits long message with a probability
greater than 99% in 4096 time slots in the presence of a jammer that jams all
the on-the-fly communications and the 74% of the silent radio spectrum---while
competing proposals simply fail. The provided solution enjoys a thorough
theoretical analysis and is supported by extensive experimental results,
showing the viability of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5720</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5720</id><created>2013-07-22</created><authors><author><keyname>Colombini</keyname><forenames>Esther L.</forenames></author><author><keyname>Sim&#xf5;es</keyname><forenames>Alexandre S.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Carlos H. C.</forenames></author></authors><title>Top-down and Bottom-up Feature Combination for Multi-sensor Attentive
  Robots</title><categories>cs.RO cs.CV</categories><report-no>ISACS/2013/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information available to robots in real tasks is widely distributed both
in time and space, requiring the agent to search for relevant data. In humans,
that face the same problem when sounds, images and smells are presented to
their sensors in a daily scene, a natural system is applied: Attention. As
vision plays an important role in our routine, most research regarding
attention has involved this sensorial system and the same has been replicated
to the robotics field. However,most of the robotics tasks nowadays do not rely
only in visual data, that are still costly. To allow the use of attentive
concepts with other robotics sensors that are usually used in tasks such as
navigation, self-localization, searching and mapping, a generic attentional
model has been previously proposed. In this work, feature mapping functions
were designed to build feature maps to this attentive model from data from
range scanner and sonar sensors. Experiments were performed in a high fidelity
simulated robotics environment and results have demonstrated the capability of
the model on dealing with both salient stimuli and goal-driven attention over
multiple features extracted from multiple sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5725</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5725</id><created>2013-07-22</created><updated>2014-11-24</updated><authors><author><keyname>Artina</keyname><forenames>Marco</forenames></author><author><keyname>Fornasier</keyname><forenames>Massimo</forenames></author><author><keyname>Peter</keyname><forenames>Steffen</forenames></author></authors><title>Damping Noise-Folding and Enhanced Support Recovery in Compressed
  Sensing</title><categories>math.NA cs.IT math.IT</categories><comments>33 pages</comments><msc-class>94A12, 94A20, 65F22, 90C05, 90C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The practice of compressed sensing suffers importantly in terms of the
efficiency/accuracy trade-off when acquiring noisy signals prior to
measurement. It is rather common to find results treating the noise affecting
the measurements, avoiding in this way to face the so-called
$\textit{noise-folding}$ phenomenon, related to the noise in the signal,
eventually amplified by the measurement procedure. In this paper, we present
two new decoding procedures, combining $\ell_1$-minimization followed by either
a regularized selective least $p$-powers or an iterative hard thresholding,
which not only are able to reduce this component of the original noise, but
also have enhanced properties in terms of support identification with respect
to the sole $\ell_1$-minimization or iteratively re-weighted
$\ell_1$-minimization. We prove such features, providing relatively simple and
precise theoretical guarantees. We additionally confirm and support the
theoretical results by extensive numerical simulations, which give a statistics
of the robustness of the new decoding procedures with respect to more classical
$\ell_1$-minimization and iteratively re-weighted $\ell_1$-minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5730</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5730</id><created>2013-07-22</created><authors><author><keyname>Zhang</keyname><forenames>Xiaowan</forenames></author><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author></authors><title>A New Strategy of Cost-Free Learning in the Class Imbalance Problem</title><categories>cs.LG</categories><comments>Classification, class imbalance, cost-free learning, cost-sensitive
  learning, abstaining, mutual information, ROC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we define cost-free learning (CFL) formally in comparison with
cost-sensitive learning (CSL). The main difference between them is that a CFL
approach seeks optimal classification results without requiring any cost
information, even in the class imbalance problem. In fact, several CFL
approaches exist in the related studies, such as sampling and some
criteria-based pproaches. However, to our best knowledge, none of the existing
CFL and CSL approaches are able to process the abstaining classifications
properly when no information is given about errors and rejects. Based on
information theory, we propose a novel CFL which seeks to maximize normalized
mutual information of the targets and the decision outputs of classifiers.
Using the strategy, we can deal with binary/multi-class classifications
with/without abstaining. Significant features are observed from the new
strategy. While the degree of class imbalance is changing, the proposed
strategy is able to balance the errors and rejects accordingly and
automatically. Another advantage of the strategy is its ability of deriving
optimal rejection thresholds for abstaining classifications and the
&quot;equivalent&quot; costs in binary classifications. The connection between rejection
thresholds and ROC curve is explored. Empirical investigation is made on
several benchmark data sets in comparison with other existing approaches. The
classification results demonstrate a promising perspective of the strategy in
machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5736</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5736</id><created>2013-07-19</created><authors><author><keyname>Sandanalakshmi</keyname><forenames>R.</forenames></author><author><keyname>Viji</keyname><forenames>P. Abinaya</forenames></author><author><keyname>Kiruthiga</keyname><forenames>M.</forenames></author><author><keyname>Manjari</keyname><forenames>M.</forenames></author><author><keyname>Sharina</keyname><forenames>M.</forenames></author></authors><title>Speaker Independent Continuous Speech to Text Converter for Mobile
  Application</title><categories>cs.CL cs.NE cs.SD</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An efficient speech to text converter for mobile application is presented in
this work. The prime motive is to formulate a system which would give optimum
performance in terms of complexity, accuracy, delay and memory requirements for
mobile environment. The speech to text converter consists of two stages namely
front-end analysis and pattern recognition. The front end analysis involves
preprocessing and feature extraction. The traditional voice activity detection
algorithms which track only energy cannot successfully identify potential
speech from input because the unwanted part of the speech also has some energy
and appears to be speech. In the proposed system, VAD that calculates energy of
high frequency part separately as zero crossing rate to differentiate noise
from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as
feature extraction method and Generalized Regression Neural Network is used as
recognizer. MFCC provides low word error rate and better feature extraction.
Neural Network improves the accuracy. Thus a small database containing all
possible syllable pronunciation of the user is sufficient to give recognition
accuracy closer to 100%. Thus the proposed technique entertains realization of
real time speaker independent applications like mobile phones, PDAs etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5748</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5748</id><created>2013-07-22</created><authors><author><keyname>Satta</keyname><forenames>Riccardo</forenames></author></authors><title>Appearance Descriptors for Person Re-identification: a Comprehensive
  Review</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In video-surveillance, person re-identification is the task of recognising
whether an individual has already been observed over a network of cameras.
Typically, this is achieved by exploiting the clothing appearance, as classical
biometric traits like the face are impractical in real-world video surveillance
scenarios. Clothing appearance is represented by means of low-level
\textit{local} and/or \textit{global} features of the image, usually extracted
according to some part-based body model to treat different body parts (e.g.
torso and legs) independently. This paper provides a comprehensive review of
current approaches to build appearance descriptors for person
re-identification. The most relevant techniques are described in detail, and
categorised according to the body models and features used. The aim of this
work is to provide a structured body of knowledge and a starting point for
researchers willing to conduct novel investigations on this challenging topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5756</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5756</id><created>2013-07-22</created><updated>2014-03-11</updated><authors><author><keyname>Neumann</keyname><forenames>Christoph</forenames></author><author><keyname>Heen</keyname><forenames>Olivier</forenames></author><author><keyname>Onno</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>DNStamp: Short-lived Trusted Timestamping</title><categories>cs.NI cs.CR</categories><comments>The extended version of this work is to appear in Elsevier Computer
  Networks 64 (2014) pp. 208-224</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trusted timestamping consists in proving that certain data existed at a
particular point in time. Existing timestamping methods require either a
centralized and dedicated trusted service or the collaboration of other
participants using the timestamping service. We propose a novel trusted
timestamping scheme, called DNStamp, that does not require a dedicated service
nor collaboration between participants. DNStamp produces shortlived timestamps
with a validity period of several days. The generation and verification
involves a large number of Domain Name System cache resolvers, thus removing
any single point of failure and any single point of trust. Any host with
Internet access may request or verify a timestamp, with no need to register to
any timestamping service. We provide a full description and analysis of
DNStamp. We analyze the security against various adversaries and show
resistance to forward-dating, back-dating and erasure attacks. Experiments with
our implementation of DNStamp show that one can set and then reliably verify
timestamps even under continuous attack conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5776</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5776</id><created>2013-07-22</created><updated>2013-07-29</updated><authors><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>An exact algorithm for 1-in-3 SAT</title><categories>cs.CC</categories><comments>this paper has been withdrawn since there are better results already
  known. (Note that 1-in-3 SAT is also known as X3SAT)</comments><msc-class>68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  1-in-3 SAT is an NP-complete variant of 3-SAT\ where a &quot;clause&quot; is satisfied
iff exactly one of its three literal is satisfied. We present here an exact
algorithm solving \oit\ in time $O^*(1.260^n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5800</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5800</id><created>2013-07-22</created><authors><author><keyname>Mukherjee</keyname><forenames>Subra</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>An Adaptive GMM Approach to Background Subtraction for Application in
  Real Time Surveillance</title><categories>cs.CV</categories><comments>5 Pages</comments><journal-ref>International Journal of Research in Engineering and
  Technology,Volume 2, Issue 1, Jan-2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient security management has become an important parameter in todays
world. As the problem is growing, there is an urgent need for the introduction
of advanced technology and equipment to improve the state-of art of
surveillance. In this paper we propose a model for real time background
subtraction using AGMM. The proposed model is robust and adaptable to dynamic
background, fast illumination changes, repetitive motion. Also we have
incorporated a method for detecting shadows using the Horpresert color model.
The proposed model can be employed for monitoring areas where movement or entry
is highly restricted. So on detection of any unexpected events in the scene an
alarm can be triggered and hence we can achieve real time surveillance even in
the absence of constant human monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5827</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5827</id><created>2013-07-22</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Cooperative Energy Harvesting Networks with Spatially Random Users</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2013.2284800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cooperative network with multiple source-destination
pairs and one energy harvesting relay. The outage probability experienced by
users in this network is characterized by taking the spatial randomness of user
locations into consideration. In addition, the cooperation among users is
modeled as a canonical coalitional game and the grand coalition is shown to be
stable in the addressed scenario. Simulation results are provided to
demonstrate the accuracy of the developed analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5829</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5829</id><created>2013-07-22</created><updated>2014-03-20</updated><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>Fagerberg</keyname><forenames>Rolf</forenames></author><author><keyname>Keng</keyname><forenames>Wah Loon</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Taslakian</keyname><forenames>Perouz</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author></authors><title>New and Improved Spanning Ratios for Yao Graphs</title><categories>cs.CG</categories><comments>12 pages. This paper is the result of merging the previous version
  with arXiv:1307.5030 [cs.CG]. This paper appears in SoCG 2014</comments><journal-ref>Journal of Computational Geometry, 6(2):19-53, 2015. Special issue
  for SoCG 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set of points in the plane and a fixed integer $k &gt; 0$, the Yao graph
$Y_k$ partitions the space around each point into $k$ equiangular cones of
angle $\theta=2\pi/k$, and connects each point to a nearest neighbor in each
cone. It is known for all Yao graphs, with the sole exception of $Y_5$, whether
or not they are geometric spanners. In this paper we close this gap by showing
that for odd $k \geq 5$, the spanning ratio of $Y_k$ is at most
$1/(1-2\sin(3\theta/8))$, which gives the first constant upper bound for $Y_5$,
and is an improvement over the previous bound of $1/(1-2\sin(\theta/2))$ for
odd $k \geq 7$. We further reduce the upper bound on the spanning ratio for
$Y_5$ from $10.9$ to $2+\sqrt{3} \approx 3.74$, which falls slightly below the
lower bound of $3.79$ established for the spanning ratio of $\Theta_5$
($\Theta$-graphs differ from Yao graphs only in the way they select the closest
neighbor in each cone). This is the first such separation between a Yao and
$\Theta$-graph with the same number of cones. We also give a lower bound of
$2.87$ on the spanning ratio of $Y_5$. Finally, we revisit the $Y_6$ graph,
which plays a particularly important role as the transition between the graphs
($k &gt; 6$) for which simple inductive proofs are known, and the graphs ($k \le
6$) whose best spanning ratios have been established by complex arguments. Here
we reduce the known spanning ratio of $Y_6$ from $17.6$ to $5.8$, getting
closer to the spanning ratio of 2 established for $\Theta_6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5837</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5837</id><created>2013-07-21</created><authors><author><keyname>Tucci</keyname><forenames>Robert R.</forenames></author></authors><title>An Information Theoretic Measure of Judea Pearl's Identifiability and
  Causal Influence</title><categories>cs.IT cs.AI math.IT</categories><comments>54 pages(32 files: 1 .tex, 1 .sty,30 .eps)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a new information theoretic measure that we call the
&quot;uprooted information&quot;. We show that a necessary and sufficient condition for a
probability $P(s|do(t))$ to be &quot;identifiable&quot; (in the sense of Pearl) in a
graph $G$ is that its uprooted information be non-negative for all models of
the graph $G$. In this paper, we also give a new algorithm for deciding, for a
Bayesian net that is semi-Markovian, whether a probability $P(s|do(t))$ is
identifiable, and, if it is identifiable, for expressing it without allusions
to confounding variables. Our algorithm is closely based on a previous
algorithm by Tian and Pearl, but seems to correct a small flaw in theirs. In
this paper, we also find a {\it necessary and sufficient graphical condition}
for a probability $P(s|do(t))$ to be identifiable when $t$ is a singleton set.
So far, in the prior literature, it appears that only a {\it sufficient
graphical condition} has been given for this. By &quot;graphical&quot; we mean that it is
directly based on Judea Pearl's 3 rules of do-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5838</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5838</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>Rotational Mutation Genetic Algorithm on optimization Problems</title><categories>cs.NE math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1307.5534, arXiv:1307.5679,
  arXiv:1307.5840</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization problem, nowadays, have more application in all major but they
have problem in computation. Calculation of the optimum point in the spaces
with the above dimensions is very time consuming. In this paper, there is
presented a new approach for the optimization of continuous functions with
rotational mutation that is called RM. The proposed algorithm starts from the
point which has best fitness value by elitism mechanism. Then, method of
rotational mutation is used to reach optimal point. In this paper, RM algorithm
is implemented by GA(Briefly RMGA) and is compared with other well- known
algorithms: DE, PGA, Grefensstette and Eshelman [15, 16] and numerical and
simulation results show that RMGA achieve global optimal point with more
decision by smaller generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5839</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5839</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>A New Approach for Finding the Global Optimal Point Using Subdividing
  Labeling Method (SLM)</title><categories>cs.NE math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1307.5667, arXiv:1307.5840</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most global optimization problems, finding global optimal point inthe
multidimensional and great search space needs high computations. In this paper,
we present a new approach to find global optimal point with the low computation
and few steps using subdividing labeling method (SLM) which can also be used in
the multi-dimensional and great search space. In this approach, in each step,
crossing points will be labeled and complete label polytope search space of
selected polytope will be subdivided after being selected. SLM algorithm finds
the global point until h (subdivision function) turns into zero. SLM will be
implemented on five applications and compared with the latest techniques such
as random search, random search-walk and simulated annealing method. The
results of the proposed method demonstrate that our new approach is faster and
more reliable and presents an optimal time complexity O (logn).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5840</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5840</id><created>2013-07-22</created><authors><author><keyname>Vali</keyname><forenames>Masoumeh</forenames></author></authors><title>Sub- Diving Labeling Method for Optimization Problem by Genetic
  Algorithm</title><categories>cs.NE math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.5667,
  arXiv:1307.5679, arXiv:1307.5838</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many global Optimization Problems, it is required to evaluate a global
point (min or max) in large space that calculation effort is very high. In this
paper is presented new approach for optimization problem with subdivision
labeling method (SLM) but in this method for higher dimensional has high
computational. SLM Genetic Algorithm (SLMGA) in optimization problems is one of
the solutions of this problem. In proposed algorithm the initial population is
crossing points and subdividing in each step is according to mutation. RSLMGA
is compared with other well known algorithms: DE, PGA, Grefensstette and
Eshelman and numerical results show that RSLMGA achieve global optimal point
with more decision by smaller generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5866</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5866</id><created>2013-07-22</created><authors><author><keyname>Barash</keyname><forenames>L. Yu.</forenames></author><author><keyname>Shchur</keyname><forenames>L. N.</forenames></author></authors><title>RNGSSELIB: Program library for random number generation. More
  generators, parallel streams of random numbers and Fortran compatibility</title><categories>physics.comp-ph cs.MS</categories><comments>6 pages, 1 table</comments><journal-ref>Computer Physics Communications 184 (2013) pp. 2367-2369</journal-ref><doi>10.1016/j.cpc.2013.04.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this update, we present the new version of the random number generator
(RNG) library RNGSSELIB, which, in particular, contains fast SSE realizations
of a number of modern and most reliable generators \cite{RNGSSELIB1}. The new
features are: i) Fortran compatibility and examples of using the library in
Fortran; ii) new modern and reliable generators; iii) the abilities to jump
ahead inside RNG sequence and to initialize up to $10^{19}$ independent random
number streams with block splitting method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5869</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5869</id><created>2013-07-22</created><updated>2014-02-17</updated><authors><author><keyname>Barash</keyname><forenames>L. Yu.</forenames></author><author><keyname>Shchur</keyname><forenames>L. N.</forenames></author></authors><title>PRAND: GPU accelerated parallel random number generation library: Using
  most reliable algorithms and applying parallelism of modern GPUs and CPUs</title><categories>physics.comp-ph cs.MS</categories><comments>29 pages, 1 figure, 7 tables</comments><journal-ref>Computer Physics Communications 185 (2014) 1343-1353</journal-ref><doi>10.1016/j.cpc.2014.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The library PRAND for pseudorandom number generation for modern CPUs and GPUs
is presented. It contains both single-threaded and multi-threaded realizations
of a number of modern and most reliable generators recently proposed and
studied in [1,2,3,4,5] and the efficient SIMD realizations proposed in [6]. One
of the useful features for using PRAND in parallel simulations is the ability
to initialize up to $10^{19}$ independent streams. Using massive parallelism of
modern GPUs and SIMD parallelism of modern CPUs substantially improves
performance of the generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5870</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5870</id><created>2013-07-22</created><updated>2013-08-15</updated><authors><author><keyname>Mu</keyname><forenames>Cun</forenames></author><author><keyname>Huang</keyname><forenames>Bo</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Goldfarb</keyname><forenames>Donald</forenames></author></authors><title>Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery</title><categories>stat.ML cs.LG</categories><comments>Slight modifications are made in this second version (mainly, Lemma
  5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering a low-rank tensor from incomplete information is a recurring
problem in signal processing and machine learning. The most popular convex
relaxation of this problem minimizes the sum of the nuclear norms of the
unfoldings of the tensor. We show that this approach can be substantially
suboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank
$r$ from Gaussian measurements requires $\Omega(r n^{K-1})$ observations. In
contrast, a certain (intractable) nonconvex formulation needs only $O(r^K +
nrK)$ observations. We introduce a very simple, new convex relaxation, which
partially bridges this gap. Our new formulation succeeds with $O(r^{\lfloor K/2
\rfloor}n^{\lceil K/2 \rceil})$ observations. While these results pertain to
Gaussian measurements, simulations strongly suggest that the new norm also
outperforms the sum of nuclear norms for tensor completion from a random subset
of entries.
  Our lower bound for the sum-of-nuclear-norms model follows from a new result
on recovering signals with multiple sparse structures (e.g. sparse, low rank),
which perhaps surprisingly demonstrates the significant suboptimality of the
commonly used recovery approach via minimizing the sum of individual sparsity
inducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-rank
tensor recovery however opens the possibility in reducing the sample complexity
by exploiting several structures jointly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5894</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5894</id><created>2013-07-22</created><authors><author><keyname>Bhuiyan</keyname><forenames>Mansurul A</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author></authors><title>MIRAGE: An Iterative MapReduce based FrequentSubgraph Mining Algorithm</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequent subgraph mining (FSM) is an important task for exploratory data
analysis on graph data. Over the years, many algorithms have been proposed to
solve this task. These algorithms assume that the data structure of the mining
task is small enough to fit in the main memory of a computer. However, as the
real-world graph data grows, both in size and quantity, such an assumption does
not hold any longer. To overcome this, some graph database-centric methods have
been proposed in recent years for solving FSM; however, a distributed solution
using MapReduce paradigm has not been explored extensively. Since, MapReduce is
becoming the de- facto paradigm for computation on massive data, an efficient
FSM algorithm on this paradigm is of huge demand. In this work, we propose a
frequent subgraph mining algorithm called MIRAGE which uses an iterative
MapReduce based framework. MIRAGE is complete as it returns all the frequent
subgraphs for a given user-defined support, and it is efficient as it applies
all the optimizations that the latest FSM algorithms adopt. Our experiments
with real life and large synthetic datasets validate the effectiveness of
MIRAGE for mining frequent subgraphs from large graph datasets. The source code
of MIRAGE is available from www.cs.iupui.edu/alhasan/software/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5899</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5899</id><created>2013-07-22</created><updated>2013-07-26</updated><authors><author><keyname>Lindstrom</keyname><forenames>Peter</forenames></author><author><keyname>Rajan</keyname><forenames>Deepak</forenames></author></authors><title>Optimal Hierarchical Layouts for Cache-Oblivious Search Trees</title><categories>cs.DS</categories><comments>Extended version with proofs added to the appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a general framework for generating cache-oblivious
layouts for binary search trees. A cache-oblivious layout attempts to minimize
cache misses on any hierarchical memory, independent of the number of memory
levels and attributes at each level such as cache size, line size, and
replacement policy. Recursively partitioning a tree into contiguous subtrees
and prescribing an ordering amongst the subtrees, Hierarchical Layouts
generalize many commonly used layouts for trees such as in-order, pre-order and
breadth-first. They also generalize the various flavors of the van Emde Boas
layout, which have previously been used as cache-oblivious layouts.
Hierarchical Layouts thus unify all previous attempts at deriving layouts for
search trees.
  The paper then derives a new locality measure (the Weighted Edge Product)
that mimics the probability of cache misses at multiple levels, and shows that
layouts that reduce this measure perform better. We analyze the various degrees
of freedom in the construction of Hierarchical Layouts, and investigate the
relative effect of each of these decisions in the construction of
cache-oblivious layouts. Optimizing the Weighted Edge Product for complete
binary search trees, we introduce the MinWEP layout, and show that it
outperforms previously used cache-oblivious layouts by almost 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5906</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5906</id><created>2013-07-22</created><authors><author><keyname>Arslan</keyname><forenames>Suayb. S.</forenames></author><author><keyname>Lee</keyname><forenames>Jaewook</forenames></author><author><keyname>Goker</keyname><forenames>Turguy</forenames></author></authors><title>Embedding Noise Prediction into List-Viterbi Decoding using Error
  Detection Codes for Magnetic Tape Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, Proceedings of the ASME 2013 Conference on
  information storage and processing systems (ISPS 2013)</comments><report-no>ISPS2013-2835</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A List Viterbi detector produces a rank ordered list of the N globally best
candidates in a trellis search. A List Viterbi detector structure is proposed
that incorporates the noise prediction with periodic state-metric updates based
on outer error detection codes (EDCs). More specifically, a periodic decision
making process is utilized for a non-overlapping sliding windows of P bits
based on the use of outer EDCs. In a number of magnetic recording applications,
Error Correction Coding (ECC) is adversely effected by the presence of long and
dominant error events. Unlike the conventional post processing methods that are
usually tailored to a specific set of dominant error events or the joint
modulation code trellis architectures that are operating on larger state spaces
at the expense of increased implementation complexity, the proposed detector
does not use any a priori information about the error event distributions and
operates at reduced state trellis. We present pre ECC bit error rate
performance as well as the post ECC codeword failure rates of the proposed
detector using perfect detection scenario as well as practical detection codes
as the EDCs are not essential to the overall design. Furthermore, it is
observed that proposed algorithm does not introduce new error events.
Simulation results show that the proposed algorithm gives improved bit error
and post ECC codeword failure rates at the expense of some increase in
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5910</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5910</id><created>2013-07-22</created><authors><author><keyname>Idrissi</keyname><forenames>Abdellah</forenames></author></authors><title>How to minimize the energy consumption in mobile ad-hoc networks</title><categories>cs.AI cs.NI</categories><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.2, March 2012</journal-ref><doi>10.5121/ijaia.2012.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we are interested in the problem of energy management in Mobile
Ad-hoc Network (MANET). The solving and optimization of MANET allow assisting
the users to efficiently use their devices in order to minimize the batteries
power consumption. In this framework, we propose a modelling of the MANET in
form of a Constraint Optimization Problem called COMANET. Then, in the
objective to minimize the consumption of batteries power, we present an
approach based on an adaptation of the A star algorithm to the MANET problem
called MANED. Finally, we expose some experimental results showing utility of
this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5934</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5934</id><created>2013-07-22</created><updated>2015-06-06</updated><authors><author><keyname>Chen</keyname><forenames>Xiao Alison</forenames></author><author><keyname>Wang</keyname><forenames>Zizhuo</forenames></author></authors><title>A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems
  with Concave Returns</title><categories>cs.DS cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an online matching problem with concave returns. This problem is
a significant generalization of the Adwords allocation problem and has vast
applications in online advertising. In this problem, a sequence of items arrive
sequentially and each has to be allocated to one of the bidders, who bid a
certain value for each item. At each time, the decision maker has to allocate
the current item to one of the bidders without knowing the future bids and the
objective is to maximize the sum of some concave functions of each bidder's
aggregate value. In this work, we propose an algorithm that achieves
near-optimal performance for this problem when the bids arrive in a random
order and the input data satisfies certain conditions. The key idea of our
algorithm is to learn the input data pattern dynamically: we solve a sequence
of carefully chosen partial allocation problems and use their optimal solutions
to assist with the future decision. Our analysis belongs to the primal-dual
paradigm, however, the absence of linearity of the objective function and the
dynamic feature of the algorithm makes our analysis quite unique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5942</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5942</id><created>2013-07-23</created><updated>2014-02-04</updated><authors><author><keyname>Rossi</keyname><forenames>Roberto</forenames></author><author><keyname>Kilic</keyname><forenames>Onur A.</forenames></author><author><keyname>Tarim</keyname><forenames>S. Armagan</forenames></author></authors><title>A unified modeling approach for the static-dynamic uncertainty strategy
  in stochastic lot-sizing</title><categories>math.OC cs.SY math.PR</categories><comments>38 pages, working draft</comments><journal-ref>OMEGA - the International Journal of Management Science, Elsevier,
  Vol. 50:126-140, 2015</journal-ref><doi>10.1016/j.omega.2014.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop mixed integer linear programming models to compute
near-optimal policy parameters for the non-stationary stochastic lot sizing
problem under Bookbinder and Tan's static-dynamic uncertainty strategy. Our
models build on piecewise linear upper and lower bounds of the first order loss
function. We discuss different formulations of the stochastic lot sizing
problem, in which the quality of service is captured by means of backorder
penalty costs, non-stockout probability, or fill rate constraints. These models
can be easily adapted to operate in settings in which unmet demand is
backordered or lost. The proposed approach has a number of advantages with
respect to existing methods in the literature: it enables seamless modelling of
different variants of the above problem, which have been previously tackled via
ad-hoc solution methods; and it produces an accurate estimation of the expected
total cost, expressed in terms of upper and lower bounds. Our computational
study demonstrates the effectiveness and flexibility of our models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5944</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5944</id><created>2013-07-23</created><updated>2016-01-19</updated><authors><author><keyname>Hall</keyname><forenames>Eric C.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author></authors><title>Online Optimization in Dynamic Environments</title><categories>stat.ML cs.LG math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1301.1254</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing - Signal
  Processing for Big Data, vol. 9, no 4. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-velocity streams of high-dimensional data pose significant &quot;big data&quot;
analysis challenges across a range of applications and settings. Online
learning and online convex programming play a significant role in the rapid
recovery of important or anomalous information from these large datastreams.
While recent advances in online learning have led to novel and rapidly
converging algorithms, these methods are unable to adapt to nonstationary
environments arising in real-world problems. This paper describes a dynamic
mirror descent framework which addresses this challenge, yielding low
theoretical regret bounds and accurate, adaptive, and computationally efficient
algorithms which are applicable to broad classes of problems. The methods are
capable of learning and adapting to an underlying and possibly time-varying
dynamical model. Empirical results in the context of dynamic texture analysis,
solar flare detection, sequential compressed sensing of a dynamic scene,
traffic surveillance,tracking self-exciting point processes and network
behavior in the Enron email corpus support the core theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5964</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5964</id><created>2013-07-23</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>The predictability of the Hirsch index evolution</title><categories>physics.soc-ph cs.DL</categories><comments>6 pages, 3 figures, to be published in the Proceedings of 18th Int.
  Conf. Science and Technology Indicators (STI2013), Berlin</comments><journal-ref>in Translational twists and turns: Science as a socio-economic
  endeavor, Hrsg.: S. Hinze, A. Lottmann, Proc. STI 2013 Berlin (Institute for
  Research Information and Quality Assurance, Berlin), S. 366-372 (2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The h-index can be used as a predictor of itself. However, the evolution of
the h-index with time is shown in the present investigation to be dominated for
several years by citations to previous publications rather than by new
scientific achievements. This inert behaviour of the h-index raises questions,
whether the h-index can be used profitably in academic appointment processes or
for the allocation of research resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.5996</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.5996</id><created>2013-07-23</created><updated>2014-08-26</updated><authors><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Bayesian Fusion of Multi-Band Images</title><categories>cs.CV physics.data-an stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a Bayesian fusion technique for remotely sensed multi-band
images is presented. The observed images are related to the high spectral and
high spatial resolution image to be recovered through physical degradations,
e.g., spatial and spectral blurring and/or subsampling defined by the sensor
characteristics. The fusion problem is formulated within a Bayesian estimation
framework. An appropriate prior distribution exploiting geometrical
consideration is introduced. To compute the Bayesian estimator of the scene of
interest from its posterior distribution, a Markov chain Monte Carlo algorithm
is designed to generate samples asymptotically distributed according to the
target distribution. To efficiently sample from this high-dimension
distribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs
sampling strategy. The efficiency of the proposed fusion method is evaluated
with respect to several state-of-the-art fusion techniques. In particular, low
spatial resolution hyperspectral and multispectral images are fused to produce
a high spatial resolution hyperspectral image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6008</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6008</id><created>2013-07-23</created><authors><author><keyname>Yang</keyname><forenames>Guang</forenames></author><author><keyname>Hipwell</keyname><forenames>John H.</forenames></author><author><keyname>Hawkes</keyname><forenames>David J.</forenames></author><author><keyname>Arridge</keyname><forenames>Simon R.</forenames></author></authors><title>Numerical Methods for Coupled Reconstruction and Registration in Digital
  Breast Tomosynthesis</title><categories>cs.CV physics.med-ph</categories><comments>29 pages, 22 figures; The Annals of the British Machine Vision
  Association and Society for Pattern Recognition (BMVA) 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Digital Breast Tomosynthesis (DBT) provides an insight into the fine details
of normal fibroglandular tissues and abnormal lesions by reconstructing a
pseudo-3D image of the breast. In this respect, DBT overcomes a major
limitation of conventional X-ray mammography by reducing the confounding
effects caused by the superposition of breast tissue. In a breast cancer
screening or diagnostic context, a radiologist is interested in detecting
change, which might be indicative of malignant disease. To help automate this
task image registration is required to establish spatial correspondence between
time points. Typically, images, such as MRI or CT, are first reconstructed and
then registered. This approach can be effective if reconstructing using a
complete set of data. However, for ill-posed, limited-angle problems such as
DBT, estimating the deformation is complicated by the significant artefacts
associated with the reconstruction, leading to severe inaccuracies in the
registration. This paper presents a mathematical framework, which couples the
two tasks and jointly estimates both image intensities and the parameters of a
transformation.
  We evaluate our methods using various computational digital phantoms,
uncompressed breast MR images, and in-vivo DBT simulations. Firstly, we compare
both iterative and simultaneous methods to the conventional, sequential method
using an affine transformation model. We show that jointly estimating image
intensities and parametric transformations gives superior results with respect
to reconstruction fidelity and registration accuracy. Also, we incorporate a
non-rigid B-spline transformation model into our simultaneous method. The
results demonstrate a visually plausible recovery of the deformation with
preservation of the reconstruction fidelity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6018</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6018</id><created>2013-07-23</created><updated>2014-05-19</updated><authors><author><keyname>Wang</keyname><forenames>Liyao</forenames></author><author><keyname>Madiman</keyname><forenames>Mokshay</forenames></author></authors><title>Beyond the entropy power inequality, via rearrangements</title><categories>cs.IT math.FA math.IT math.PR</categories><comments>32 pages. v2: Several minor edits + Section X added</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 9, pp.
  5116-5137, September 2014</journal-ref><doi>10.1109/TIT.2014.2338852</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lower bound on the R\'enyi differential entropy of a sum of independent
random vectors is demonstrated in terms of rearrangements. For the special case
of Boltzmann-Shannon entropy, this lower bound is better than that given by the
entropy power inequality. Several applications are discussed, including a new
proof of the classical entropy power inequality and an entropy inequality
involving symmetrization of L\'evy processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6023</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6023</id><created>2013-07-23</created><authors><author><keyname>AL-Saati</keyname><forenames>Dr. Najla Akram</forenames></author><author><keyname>Abd-AlKareem</keyname><forenames>Marwa</forenames></author></authors><title>The Use of Cuckoo Search in Estimating the Parameters of Software
  Reliability Growth Models</title><categories>cs.AI cs.SE</categories><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 11, No. 6, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims to investigate the reliability of software products as an
important attribute of computer programs; it helps to decide the degree of
trustworthiness a program has in accomplishing its specific functions. This is
done using the Software Reliability Growth Models (SRGMs) through the
estimation of their parameters. The parameters are estimated in this work based
on the available failure data and with the search techniques of Swarm
Intelligence, namely, the Cuckoo Search (CS) due to its efficiency,
effectiveness and robustness. A number of SRGMs is studied, and the results are
compared to Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO)
and extended ACO. Results show that CS outperformed both PSO and ACO in finding
better parameters tested using identical datasets. It was sometimes
outperformed by the extended ACO. Also in this work, the percentages of
training data to testing data are investigated to show their impact on the
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6029</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6029</id><created>2013-07-23</created><authors><author><keyname>Angel</keyname><forenames>Omer</forenames></author><author><keyname>Shinkar</keyname><forenames>Igor</forenames></author></authors><title>A Tight Upper Bound on Acquaintance Time of Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we confirm a conjecture raised by Benjamini et al. \cite{BST} on
the acquaintance time of graphs, proving that for all graphs $G$ with $n$
vertices it holds that $\AC(G) = O(n^{3/2})$, which is tight up to a
multiplicative constant. This is done by proving that for all graphs $G$ with
$n$ vertices and maximal degree $\Delta$ it holds that $\AC(G) \leq 20 \Delta
n$. Combining this with the bound $\AC(G) \leq O(n^2/\Delta)$ from \cite{BST}
gives the foregoing uniform upper bound of all $n$-vertex graphs.
  We also prove that for the $n$-vertex path $P_n$ it holds that
$\AC(P_n)=n-2$. In addition we show that the barbell graph $B_n$ consisting of
two cliques of sizes $\ceil{n/2}$ and $\floor{n/2}$ connected by a single edge
also has $\AC(B_n) = n-2$. This shows that it is possible to add $\Omega(n^2)$
edges to $P_n$ without changing the $\AC$ value of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6033</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6033</id><created>2013-07-23</created><authors><author><keyname>Ezzeldin</keyname><forenames>Yahya H.</forenames></author><author><keyname>Sultan</keyname><forenames>Radwa A.</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>Sparse Reconstruction-based Detection of Spatial Dimension Holes in
  Cognitive Radio Networks</title><categories>cs.IT cs.NI math.IT math.OC</categories><comments>accepted for PIMRC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a spectrum sensing algorithm for detecting
spatial dimension holes in Multiple Inputs Multiple Outputs (MIMO)
transmissions for OFDM systems using Compressive Sensing (CS) tools. This
extends the energy detector to allow for detecting transmission opportunities
even if the band is already energy filled. We show that the task described
above is not performed efficiently by regular MIMO decoders (such as MMSE
decoder) due to possible sparsity in the transmit signal. Since CS
reconstruction tools take into account the sparsity order of the signal, they
are more efficient in detecting the activity of the users. Building on
successful activity detection by the CS detector, we show that the use of a
CS-aided MMSE decoders yields better performance rather than using either
CS-based or MMSE decoders separately. Simulations are conducted to verify the
gains from using CS detector for Primary user activity detection and the
performance gain in using CS-aided MMSE decoders for decoding the PU
information for future relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6041</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6041</id><created>2013-07-23</created><authors><author><keyname>Wang</keyname><forenames>Shi</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author><author><keyname>Zhang</keyname><forenames>Guofeng</forenames></author><author><keyname>James</keyname><forenames>Matthew R.</forenames></author></authors><title>Quantum Optical Realization of Classical Linear Stochastic Systems</title><categories>quant-ph cs.SY</categories><comments>8 pages, 6 figures. To appear in Automatica J. IFAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to show how a class of classical linear
stochastic systems can be physically implemented using quantum optical
components. Quantum optical systems typically have much higher bandwidth than
electronic devices, meaning faster response and processing times, and hence has
the potential for providing better performance than classical systems. A
procedure is provided for constructing the quantum optical realization. The
paper also describes the use of the quantum optical realization in a
measurement feedback loop. Some examples are given to illustrate the
application of the main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6042</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6042</id><created>2013-07-23</created><authors><author><keyname>Ezzeldin</keyname><forenames>Yahya H.</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>Pseudo-Lattice Treatment for Subspace Aligned Interference Signals</title><categories>cs.IT math.IT</categories><doi>10.1109/TVT.2014.2317753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For multi-input multi-output (MIMO) K-user interference networks, we propose
the use of a channel transformation technique for joint detection of the useful
and interference signals in an interference alignment scenario. We coin our
detection technique as &quot;pseudo-lattice treatment&quot; and show that applying our
technique, we can alleviate limitations facing Lattice Interference Alignment
(L-IA). We show that for a 3-user interference network, two of the users can
have their interference aligned in lattice structure through precoding. For the
remaining user, performance gains in decoding subspace interference aligned
signals at the receiver are achieved using our channel transformation
technique. Our &quot;pseudo-lattice&quot; technique can also be applied at all users in
case of Subspace Interference Alignment (S-IA). We investigate different
solutions for applying channel transformation at the third receiver and
evaluate performance for these techniques. Simulations are conducted to show
the performance gain in using our pseudo-lattice method over other decoding
techniques using different modulation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6059</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6059</id><created>2013-07-23</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Entropy of Closure Operators</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.6558</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy of a closure operator has been recently proposed for the study of
network coding and secret sharing. In this paper, we study closure operators in
relation to their entropy. We first introduce four different kinds of rank
functions for a given closure operator, which determine bounds on the entropy
of that operator. This yields new axioms for matroids based on their closure
operators. We also determine necessary conditions for a large class of closure
operators to be solvable. We then define the Shannon entropy of a closure
operator, and use it to prove that the set of closure entropies is dense.
Finally, we justify why we focus on the solvability of closure operators only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6066</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6066</id><created>2013-07-23</created><authors><author><keyname>Shi</keyname><forenames>Xuelin</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Liu</keyname><forenames>JiangChuan</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>Continuous Double Auction Mechanism and Bidding Strategies in Cloud
  Computing Markets</title><categories>cs.DC cs.GT</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has been an emerging model which aims at allowing customers
to utilize computing resources hosted by Cloud Service Providers (CSPs). More
and more consumers rely on CSPs to supply computing and storage service on the
one hand, and CSPs try to attract consumers on favorable terms on the other. In
such competitive cloud computing markets, pricing policies are critical to
market efficiency. While CSPs often publish their prices and charge users
according to the amount of resources they consume, auction mechanism is rarely
applied. In fact a feasible auction mechanism is the most effective method for
allocation of resources, especially double auction is more efficient and
flexible for it enables buyers and sellers to enter bids and offers
simultaneously. In this paper we bring up an electronic auction platform for
cloud, and a cloud Continuous Double Auction (CDA) mechanism is formulated to
match orders and facilitate trading based on the platform. Some evaluating
criteria are defined to analyze the efficiency of markets and strategies.
Furthermore, the selection of bidding strategies for the auction plays a very
important role for each player to maximize its own profit, so we developed a
novel bidding strategy for cloud CDA, BH-strategy, which is a two-stage game
bidding strategy. At last we designed three simulation scenarios to compare the
performance of our strategy with other dominating bidding strategies and proved
that BH-strategy has better performance on surpluses, successful transactions
and market efficiency. In addition, we discussed that our cloud CDA mechanism
is feasible for cloud computing resource allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6080</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6080</id><created>2013-07-23</created><updated>2013-07-24</updated><authors><author><keyname>Lefortier</keyname><forenames>Damien</forenames></author><author><keyname>Ostroumova</keyname><forenames>Liudmila</forenames></author><author><keyname>Samosvat</keyname><forenames>Egor</forenames></author><author><keyname>Serdyukov</keyname><forenames>Pavel</forenames></author></authors><title>Timely crawling of high-quality ephemeral new content</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, more and more people use the Web as their primary source of
up-to-date information. In this context, fast crawling and indexing of newly
created Web pages has become crucial for search engines, especially because
user traffic to a significant fraction of these new pages (like news, blog and
forum posts) grows really quickly right after they appear, but lasts only for
several days.
  In this paper, we study the problem of timely finding and crawling of such
ephemeral new pages (in terms of user interest). Traditional crawling policies
do not give any particular priority to such pages and may thus crawl them not
quickly enough, and even crawl already obsolete content. We thus propose a new
metric, well thought out for this task, which takes into account the decrease
of user interest for ephemeral pages over time.
  We show that most ephemeral new pages can be found at a relatively small set
of content sources and present a procedure for finding such a set. Our idea is
to periodically recrawl content sources and crawl newly created pages linked
from them, focusing on high-quality (in terms of user interest) content. One of
the main difficulties here is to divide resources between these two activities
in an efficient way. We find the adaptive balance between crawls and recrawls
by maximizing the proposed metric. Further, we incorporate search engine click
logs to give our crawler an insight about the current user demands. Efficiency
of our approach is finally demonstrated experimentally on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6082</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6082</id><created>2013-07-23</created><authors><author><keyname>Book</keyname><forenames>Theodore</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>A Case of Collusion: A Study of the Interface Between Ad Libraries and
  their Apps</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing concern with advertisement libraries on Android is their ability to
exfiltrate personal information from their host applications. While previous
work has looked at the libraries' abilities to measure private information on
their own, advertising libraries also include APIs through which a host
application can deliberately leak private information about the user. This
study considers a corpus of 114,000 apps. We reconstruct the APIs for 103 ad
libraries used in the corpus, and study how the privacy leaking APIs from the
top 20 ad libraries are used by the applications. Notably, we have found that
app popularity correlates with privacy leakage; the marginal increase in
advertising revenue, multiplied over a larger user base, seems to incentivize
these app vendors to violate their users' privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6102</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6102</id><created>2013-07-23</created><updated>2014-09-03</updated><authors><author><keyname>Prestwich</keyname><forenames>S. D.</forenames></author><author><keyname>Tarim</keyname><forenames>S. A.</forenames></author><author><keyname>Rossi</keyname><forenames>R.</forenames></author><author><keyname>Hnich</keyname><forenames>B.</forenames></author></authors><title>Forecasting Intermittent Demand by Hyperbolic-Exponential Smoothing</title><categories>cs.OH</categories><comments>Earlier versions of this work were presented at the 25th European
  Conference on Operations Research, 2012; and at the 54th Annual Conference of
  the UK Operational Research Society, 2012. A journal version is in
  preparation</comments><journal-ref>International Journal of Forecasting, Elsevier, 30(4):928-933,
  2014</journal-ref><doi>10.1016/j.ijforecast.2014.01.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Croston's method is generally viewed as superior to exponential smoothing
when demand is intermittent, but it has the drawbacks of bias and an inability
to deal with obsolescence, in which an item's demand ceases altogether. Several
variants have been reported, some of which are unbiased on certain types of
demand, but only one recent variant addresses the problem of obsolescence. We
describe a new hybrid of Croston's method and Bayesian inference called
Hyperbolic-Exponential Smoothing, which is unbiased on non-intermittent and
stochastic intermittent demand, decays hyperbolically when obsolescence occurs
and performs well in experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6110</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6110</id><created>2013-07-23</created><updated>2014-01-22</updated><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chua</keyname><forenames>Kee-Chaing</forenames></author></authors><title>Secrecy Wireless Information and Power Transfer with MISO Beamforming</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Transactions on Signal Processing. Longer version of
  arXiv:1306.0969</comments><doi>10.1109/TSP.2014.2303422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dual use of radio signals for simultaneous wireless information and power
transfer (SWIPT) has recently drawn significant attention. To meet the
practical requirement that energy receivers (ERs) operate with significantly
higher received power as compared to information receivers (IRs), ERs need to
be deployed in more proximity to the transmitter than IRs. However, due to the
broadcast nature of wireless channels, one critical issue arises that the
messages sent to IRs can be eavesdropped by ERs, which possess better channels
from the transmitter. In this paper, we address this new secrecy communication
problem in a multiuser multiple-input single-output (MISO) SWIPT system where
one multi-antenna transmitter sends information and energy simultaneously to an
IR and multiple ERs, each with one single antenna. To optimally design transmit
beamforming vectors and their power allocation, two problems are investigated
with different aims: the first problem maximizes the secrecy rate for IR
subject to individual harvested energy constraints of ERs, while the second
problem maximizes the weighted sum-energy transferred to ERs subject to a
secrecy rate constraint for IR. We solve these two non-convex problems
optimally by reformulating each of them into a two-stage problem. First, by
fixing the signal-to-interference-plus-noise ratio (SINR) target for ERs (for
the first problem) or IR (for the second problem), we obtain the optimal
beamforming and power allocation solution by applying the technique of
semidefinite relaxation (SDR). Then, the original problems are solved by a
one-dimension search over the optimal SINR target for ERs or IR. Furthermore,
for each of the two studied problems, suboptimal solutions of lower complexity
are also proposed in which the information and energy beamforming vectors are
separately designed with their power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6125</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6125</id><created>2013-07-23</created><updated>2014-11-07</updated><authors><author><keyname>Sun</keyname><forenames>Ruoyu</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Interference alignment using finite and dependent channel extensions:
  the single beam case</title><categories>cs.IT math.IT</categories><comments>43 pages. Revised version; title changed. A shorter version (without
  proofs for simple cases) accepted by IEEE Trans. on Info. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector space interference alignment (IA) is known to achieve high degrees of
freedom (DoF) with infinite independent channel extensions, but its performance
is largely unknown for a finite number of possibly dependent channel
extensions. In this paper, we consider a $K$-user $M_t \times M_r$ MIMO
interference channel (IC) with arbitrary number of channel extensions $T$ and
arbitrary channel diversity order $L$ (i.e., each channel matrix is a generic
linear combination of $L$ fixed basis matrices). We study the maximum DoF
achievable via vector space IA in the single beam case (i.e. each user sends
one data stream). We prove that the total number of users $K$ that can
communicate interference-free using linear transceivers is upper bounded by
$NL+N^2/4$, where $N = \min\{M_tT, M_rT \}$. An immediate consequence of this
upper bound is that for a SISO IC the DoF in the single beam case is no more
than $\min\left\{\sqrt{ 5K/4}, L + T/4\right\}$. When the channel extensions
are independent, i.e. $ L$ achieves the maximum $M_r M_t T $, we show that this
maximum DoF lies in $[M_r+M_t-1, M_r+M_t]$ regardless of $T$. Unlike the
well-studied constant MIMO IC case, the main difficulty is how to deal with a
hybrid system of equations (zero-forcing condition) and inequalities (full rank
condition). Our approach combines algebraic tools that deal with equations with
an induction analysis that indirectly considers the inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6134</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6134</id><created>2013-07-23</created><updated>2014-02-14</updated><authors><author><keyname>Reverdy</keyname><forenames>Paul</forenames></author><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi E.</forenames></author></authors><title>Modeling Human Decision-making in Generalized Gaussian Multi-armed
  Bandits</title><categories>cs.LG math.OC stat.ML</categories><comments>To appear in the Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formal model of human decision-making in explore-exploit tasks
using the context of multi-armed bandit problems, where the decision-maker must
choose among multiple options with uncertain rewards. We address the standard
multi-armed bandit problem, the multi-armed bandit problem with transition
costs, and the multi-armed bandit problem on graphs. We focus on the case of
Gaussian rewards in a setting where the decision-maker uses Bayesian inference
to estimate the reward values. We model the decision-maker's prior knowledge
with the Bayesian prior on the mean reward. We develop the upper credible limit
(UCL) algorithm for the standard multi-armed bandit problem and show that this
deterministic algorithm achieves logarithmic cumulative expected regret, which
is optimal performance for uninformative priors. We show how good priors and
good assumptions on the correlation structure among arms can greatly enhance
decision-making performance, even over short time horizons. We extend to the
stochastic UCL algorithm and draw several connections to human decision-making
behavior. We present empirical data from human experiments and show that human
performance is efficiently captured by the stochastic UCL algorithm with
appropriate parameters. For the multi-armed bandit problem with transition
costs and the multi-armed bandit problem on graphs, we generalize the UCL
algorithm to the block UCL algorithm and the graphical block UCL algorithm,
respectively. We show that these algorithms also achieve logarithmic cumulative
expected regret and require a sub-logarithmic expected number of transitions
among arms. We further illustrate the performance of these algorithms with
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6143</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6143</id><created>2013-07-23</created><updated>2013-07-24</updated><authors><author><keyname>Brummer</keyname><forenames>Niko</forenames></author></authors><title>Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier</title><categories>stat.ML cs.LG</categories><comments>Research Report, BOSARIS 2012 Speaker Recognition Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report works out the details of a closed-form, fully Bayesian,
multiclass, openset, generative pattern classifier using multivariate Gaussian
likelihoods, with conjugate priors. The generative model has a common
within-class covariance, which is proportional to the between-class covariance
in the conjugate prior. The scalar proportionality constant is the only plugin
parameter. All other model parameters are intergated out in closed form. An
expression is given for the model evidence, which can be used to make plugin
estimates for the proportionality constant. Pattern recognition is done via the
predictive likeihoods of classes for which training data is available, as well
as a predicitve likelihood for any as yet unseen class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6145</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6145</id><created>2013-07-23</created><authors><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>Online Communities: Visualization and Formalization</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>9 pages, 2 figures, Cyberpatterns 2013: Second International Workshop
  on Cyberpatterns; Proceedings of the Second International Workshop on
  Cyberpatterns: Unifying Design Patterns with Security, Attack and Forensic
  Patterns. Abingdon, UK, 8-9 July 2013.
  http://tech.brookes.ac.uk/CyberPatterns2013/</comments><acm-class>D.2.4; H.5.2; K.6.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online communities have increased in size and importance dramatically over
the last decade. The fact that many communities are online means that it is
possible to extract information about these communities and the connections
between their members much more easily using software tools, despite their
potentially very large size. The links between members of the community can be
presented visually and often this can make patterns in the structure of
sub-communities immediately obvious. The links and structures of layered
communities can also be formalized to gain a better understanding of their
modelling. This paper explores these links with some specific examples,
including visualization of these relationships and a formalized model of
communities using the Z notation. It also considers the development of such
communities within the Community of Practice social science framework. Such
approaches may be applicable for communities associated with cybersecurity and
could be combined for a better understanding of their development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6163</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6163</id><created>2013-07-23</created><updated>2013-07-24</updated><authors><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Darbari</keyname><forenames>Hemant</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Human and Automatic Evaluation of English-Hindi Machine Translation</title><categories>cs.CL</categories><comments>in Hindi, International Joint Rajbhasha Conference on Science and
  Technology, Oct 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past 60 years, Research in machine translation is going on. For the
development in this field, a lot of new techniques are being developed each
day. As a result, we have witnessed development of many automatic machine
translators. A manager of machine translation development project needs to know
the performance increase/decrease, after changes have been done in his system.
Due to this reason, a need for evaluation of machine translation systems was
felt. In this article, we shall present the evaluation of some machine
translators. This evaluation will be done by a human evaluator and by some
automatic evaluation metrics, which will be done at sentence, document and
system level. In the end we shall also discuss the comparison between the
evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6170</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6170</id><created>2013-07-22</created><updated>2013-07-30</updated><authors><author><keyname>Paletta</keyname><forenames>Lucas</forenames><affiliation>JOANNEUM RESEARCH Forschungsgesellschaft mbH</affiliation></author><author><keyname>Itti</keyname><forenames>Laurent</forenames><affiliation>University of Southern California, USA</affiliation></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen, Germany</affiliation></author><author><keyname>Fang</keyname><forenames>Fang</forenames><affiliation>Peking University, China</affiliation></author></authors><title>6th International Symposium on Attention in Cognitive Systems 2013</title><categories>cs.CV</categories><comments>conference</comments><report-no>ISACS/2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers accepted at the 6th International Symposium
on Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5,
2013. The aim of this symposium is to highlight the central role of attention
on various kinds of performance in cognitive systems processing. It brings
together researchers and developers from both academia and industry, from
computer vision, robotics, perception psychology, psychophysics and
neuroscience, in order to provide an interdisciplinary forum to present and
communicate on computational models of attention, with the focus on
interdependencies with visual cognition. Furthermore, it intends to investigate
relevant objectives for performance comparison, to document and to investigate
promising application domains, and to discuss visual attention with reference
to other aspects of AI enabled systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6179</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6179</id><created>2013-07-22</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author></authors><title>Multi-horizon solar radiation forecasting for Mediterranean locations
  using time series models</title><categories>physics.ao-ph cs.NE</categories><comments>Renewable and Sustainable Energy Reviews (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the grid manager's point of view, needs in terms of prediction of
intermittent energy like the photovoltaic resource can be distinguished
according to the considered horizon: following days (d+1, d+2 and d+3), next
day by hourly step (h+24), next hour (h+1) and next few minutes (m+5 e.g.).
Through this work, we have identified methodologies using time series models
for the prediction horizon of global radiation and photovoltaic power. What we
present here is a comparison of different predictors developed and tested to
propose a hierarchy. For horizons d+1 and h+1, without advanced ad hoc time
series pre-processing (stationarity) we find it is not easy to differentiate
between autoregressive moving average (ARMA) and multilayer perceptron (MLP).
However we observed that using exogenous variables improves significantly the
results for MLP . We have shown that the MLP were more adapted for horizons
h+24 and m+5. In summary, our results are complementary and improve the
existing prediction techniques with innovative tools: stationarity, numerical
weather prediction combination, MLP and ARMA hybridization, multivariate
analysis, time index, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6209</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6209</id><created>2013-07-23</created><updated>2014-03-05</updated><authors><author><keyname>Kreutzer</keyname><forenames>Moritz</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author><author><keyname>Fehske</keyname><forenames>Holger</forenames></author><author><keyname>Bishop</keyname><forenames>Alan R.</forenames></author></authors><title>A unified sparse matrix data format for efficient general sparse
  matrix-vector multiply on modern processors with wide SIMD units</title><categories>cs.MS cs.DC</categories><comments>23 pages, 7 figures, 6 listings</comments><journal-ref>SIAM Journal on Scientific Computing 2014 36:5, C401-C423</journal-ref><doi>10.1137/130930352</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse matrix-vector multiplication (spMVM) is the most time-consuming kernel
in many numerical algorithms and has been studied extensively on all modern
processor and accelerator architectures. However, the optimal sparse matrix
data storage format is highly hardware-specific, which could become an obstacle
when using heterogeneous systems. Also, it is as yet unclear how the wide
single instruction multiple data (SIMD) units in current multi- and many-core
processors should be used most efficiently if there is no structure in the
sparsity pattern of the matrix. We suggest SELL-C-sigma, a variant of Sliced
ELLPACK, as a SIMD-friendly data format which combines long-standing ideas from
General Purpose Graphics Processing Units (GPGPUs) and vector computer
programming. We discuss the advantages of SELL-C-sigma compared to established
formats like Compressed Row Storage (CRS) and ELLPACK and show its suitability
on a variety of hardware platforms (Intel Sandy Bridge, Intel Xeon Phi and
Nvidia Tesla K20) for a wide range of test matrices from different application
areas. Using appropriate performance models we develop deep insight into the
data transfer properties of the SELL-C-sigma spMVM kernel. SELL-C-sigma comes
with two tuning parameters whose performance impact across the range of test
matrices is studied and for which reasonable choices are proposed. This leads
to a hardware-independent (&quot;catch-all&quot;) sparse matrix format, which achieves
very high efficiency for all test matrices across all hardware platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6235</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6235</id><created>2013-07-18</created><updated>2013-10-08</updated><authors><author><keyname>Biswas</keyname><forenames>Anindya Kumar</forenames></author></authors><title>Graphical law beneath each written natural language</title><categories>physics.gen-ph cs.CL</categories><comments>92 pages, 26 figures, all tables given.Graphical law conjecture
  extended to verbs,adverbs,adjectives. Existence of graphical law shown for
  Chinese Usages;for Lakher(Mara) language for words,nouns,verbs,adverbs and
  adjectives. Mistakes in Italiano,Urdu,SA English plots rectified. Four
  successive normalisations considered. A theoretical scenario behind the
  graphical law envisaged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study twenty four written natural languages. We draw in the log scale,
number of words starting with a letter vs rank of the letter, both normalised.
We find that all the graphs are of the similar type. The graphs are
tantalisingly closer to the curves of reduced magnetisation vs reduced
temperature for magnetic materials. We make a weak conjecture that a curve of
magnetisation underlies a written natural language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6239</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6239</id><created>2013-07-23</created><updated>2014-06-16</updated><authors><author><keyname>Nguyen</keyname><forenames>Phuc C.</forenames></author><author><keyname>Tobin-Hochstadt</keyname><forenames>Sam</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Soft Contract Verification</title><categories>cs.PL</categories><comments>ICFP '14, September 1-6, 2014, Gothenburg, Sweden</comments><acm-class>D.2.4; D.3.1</acm-class><doi>10.1145/2628136.2628156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavioral software contracts are a widely used mechanism for governing the
flow of values between components. However, run-time monitoring and enforcement
of contracts imposes significant overhead and delays discovery of faulty
components to run-time.
  To overcome these issues, we present soft contract verification, which aims
to statically prove either complete or partial contract correctness of
components, written in an untyped, higher-order language with first-class
contracts. Our approach uses higher-order symbolic execution, leveraging
contracts as a source of symbolic values including unknown behavioral values,
and employs an updatable heap of contract invariants to reason about
flow-sensitive facts. We prove the symbolic execution soundly approximates the
dynamic semantics and that verified programs can't be blamed.
  The approach is able to analyze first-class contracts, recursive data
structures, unknown functions, and control-flow-sensitive refinements of
values, which are all idiomatic in dynamic languages. It makes effective use of
an off-the-shelf solver to decide problems without heavy encodings. The
approach is competitive with a wide range of existing tools---including type
systems, flow analyzers, and model checkers---on their own benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6246</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6246</id><created>2013-07-23</created><authors><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Frantsev</keyname><forenames>Anton</forenames></author><author><keyname>Deb</keyname><forenames>Kalyanmoy</forenames></author></authors><title>Finding Optimal Strategies in a Multi-Period Multi-Leader-Follower
  Stackelberg Game Using an Evolutionary Algorithm</title><categories>cs.GT</categories><comments>To be published in Computers and Operations Research</comments><acm-class>G.1.6</acm-class><doi>10.1016/j.cor.2013.07.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stackelberg games are a classic example of bilevel optimization problems,
which are often encountered in game theory and economics. These are complex
problems with a hierarchical structure, where one optimization task is nested
within the other. Despite a number of studies on handling bilevel optimization
problems, these problems still remain a challenging territory, and existing
methodologies are able to handle only simple problems with few variables under
assumptions of continuity and differentiability. In this paper, we consider a
special case of a multi-period multi-leader-follower Stackelberg competition
model with non-linear cost and demand functions and discrete production
variables. The model has potential applications, for instance in aircraft
manufacturing industry, which is an oligopoly where a few giant firms enjoy a
tremendous commitment power over the other smaller players. We solve cases with
different number of leaders and followers, and show how the entrance or exit of
a player affects the profits of the other players. In the presence of various
model complexities, we use a computationally intensive nested evolutionary
strategy to find an optimal solution for the model. The strategy is evaluated
on a test-suite of bilevel problems, and it has been shown that the method is
successful in handling difficult bilevel problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6250</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6250</id><created>2013-07-23</created><authors><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Frantsev</keyname><forenames>Anton</forenames></author><author><keyname>Deb</keyname><forenames>Kalyanmoy</forenames></author></authors><title>Multi-objective Stackelberg Game Between a Regulating Authority and a
  Mining Company: A Case Study in Environmental Economics</title><categories>cs.GT</categories><comments>In: Proceedings of IEEE Congress on Evolutionary Computation
  (CEC-2013). IEEE Press</comments><doi>10.1109/CEC.2013.6557607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilevel programming problems are often found in practice. In this paper, we
handle one such bilevel application problem from the domain of environmental
economics. The problem is a Stakelberg game with multiple objectives at the
upper level, and a single objective at the lower level. The leader in this case
is the regulating authority, and it tries to maximize its total tax revenue
over multiple periods while trying to minimize the environmental damages caused
by a mining company. The follower is the mining company whose sole objective is
to maximize its total profit over multiple periods under the limitations set by
the leader. The solution to the model contains the optimal taxation and
extraction decisions to be made by the players in each of the time periods. We
construct a simplistic model for the Stackelberg game and provide an analytical
solution to the problem. Thereafter, the model is extended to incorporate
realism and is solved using a bilevel evolutionary algorithm capable of
handling multiple objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6272</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6272</id><created>2013-07-23</created><updated>2013-10-13</updated><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szwarc</keyname><forenames>R.</forenames></author></authors><title>On Axiomatization of Inconsistency Indicators for Pairwise Comparisons</title><categories>cs.DM cs.GT</categories><comments>Enhanced text, with 21 pages and 3 figures, proves that arbitrarily
  inaccurate pairwise matrices are considered acceptable by theories with a
  inconsistency based on the principal eigenvalue (e.g., AHP). CPC (corner
  pairwise comparisons) matrix is the crucial part of this study as it
  invalidates any eigenvalue-based inconsistency. All comments are highly
  appreciated</comments><report-no>2013-10-04 (jrn revised submission)</report-no><msc-class>65F30</msc-class><acm-class>C.4; D.2.8; G.1.2; G.1.6; H.1.1; H.4.2</acm-class><journal-ref>Fundamenta Informaticae, Volume 132, Number 4 / 2014, pages
  485-500</journal-ref><doi>10.3233/FI-2014-1055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the notion of inconsistency in pairwise comparisons and propose an
axiomatization which is independent of any method of approximation or the
inconsistency indicator definition (e.g., Analytic Hierarchy Process, AHP). It
has been proven that the eigenvalue-based inconsistency (proposed as a part of
AHP) is incorrect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6285</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6285</id><created>2013-07-23</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Wireless Energy and Information Transfer Tradeoff for Limited Feedback
  Multi-Antenna Systems with Energy Beamforming</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures. IEEE Transactions on Vehicular Technology, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multi-antenna system where the receiver should
harvest energy from the transmitter by wireless energy transfer to support its
wireless information transmission. In order to maximize the harvesting energy,
we propose to perform adaptive energy beamforming according to the
instantaneous channel state information (CSI). To help the transmitter to
obtain the CSI for energy beamforming, we further propose a win-win CSI
quantization feedback strategy, so as to improve the efficiencies of both power
and information transmission. The focus of this paper is on the tradeoff of
wireless energy and information transfer by adjusting the transfer duration
with a total duration constraint. Through revealing the relationship between
transmit power, transfer duration and feedback amount, we derive two wireless
energy and information transfer tradeoff schemes by maximizing an upper bound
and an approximate lower bound of the average information transmission rate,
respectively. Moreover, the impact of imperfect CSI at the receiver is
investigated and the corresponding wireless energy and information transfer
tradeoff scheme is also given. Finally, numerical results validate the
effectiveness of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6291</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6291</id><created>2013-07-23</created><authors><author><keyname>Wang</keyname><forenames>Xili</forenames></author></authors><title>A novel approach of solving the CNF-SAT problem</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discussed CNF-SAT problem (NP-Complete problem) and
analysis two solutions that can solve the problem, the PL-Resolution algorithm
and the WalkSAT algorithm. PL-Resolution is a sound and complete algorithm that
can be used to determine satisfiability and unsatisfiability with certainty.
WalkSAT can determine satisfiability if it finds a model, but it cannot
guarantee to find a model even there exists one. However, WalkSAT is much
faster than PL-Resolution, which makes WalkSAT more practical; and we have
analysis the performance between these two algorithms, and the performance of
WalkSAT is acceptable if the problem is not so hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6303</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6303</id><created>2013-07-24</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Chan</keyname><forenames>Kap Luk</forenames></author></authors><title>Matching-Constrained Active Contours</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In object segmentation by active contours, the initial contour is often
required. Conventionally, the initial contour is provided by the user. This
paper extends the conventional active contour model by incorporating feature
matching in the formulation, which gives rise to a novel matching-constrained
active contour. The numerical solution to the new optimization model provides
an automated framework of object segmentation without user intervention. The
main idea is to incorporate feature point matching as a constraint in active
contour models. To this effect, we obtain a mathematical model of interior
points to boundary contour such that matching of interior feature points gives
contour alignment, and we formulate the matching score as a constraint to
active contour model such that the feature matching of maximum score that gives
the contour alignment provides the initial feasible solution to the constrained
optimization model of segmentation. The constraint also ensures that the
optimal contour does not deviate too much from the initial contour.
Projected-gradient descent equations are derived to solve the constrained
optimization. In the experiments, we show that our method is capable of
achieving the automatic object segmentation, and it outperforms the related
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6307</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6307</id><created>2013-07-24</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Is there currently a scientific revolution in scientometrics?</title><categories>cs.DL physics.soc-ph stat.OT</categories><comments>Accepted for publication in the Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author of this letter to the editor would like to set forth the argument
that scientometrics is currently in a phase in which a taxonomic change, and
hence a revolution, is taking place. One of the key terms in scientometrics is
scientific impact which nowadays is understood to mean not only the impact on
science but the impact on every area of society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6318</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6318</id><created>2013-07-24</created><updated>2013-09-20</updated><authors><author><keyname>Hirschowitz</keyname><forenames>Tom</forenames><affiliation>CNRS, Universit&#xe9; de Savoie</affiliation></author></authors><title>Cartesian closed 2-categories and permutation equivalence in
  higher-order rewriting</title><categories>cs.LO cs.PL math.CT</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  4, 2013) lmcs:1132</journal-ref><doi>10.2168/LMCS-9(3:10)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a semantics for permutation equivalence in higher-order rewriting.
This semantics takes place in cartesian closed 2-categories, and is proved
sound and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6321</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6321</id><created>2013-07-24</created><authors><author><keyname>Nam</keyname><forenames>Sangnam</forenames><affiliation>LATP</affiliation></author></authors><title>An Uncertainty Principle for Discrete Signals</title><categories>cs.IT math.IT</categories><comments>SampTA, Bremen : Germany (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By use of window functions, time-frequency analysis tools like Short Time
Fourier Transform overcome a shortcoming of the Fourier Transform and enable us
to study the time- frequency characteristics of signals which exhibit transient
os- cillatory behavior. Since the resulting representations depend on the
choice of the window functions, it is important to know how they influence the
analyses. One crucial question on a window function is how accurate it permits
us to analyze the signals in the time and frequency domains. In the continuous
domain (for functions defined on the real line), the limit on the accuracy is
well-established by the Heisenberg's uncertainty principle when the
time-frequency spread is measured in terms of the variance measures. However,
for the finite discrete signals (where we consider the Discrete Fourier
Transform), the uncertainty relation is not as well understood. Our work fills
in some of the gap in the understanding and states uncertainty relation for a
subclass of finite discrete signals. Interestingly, the result is a close
parallel to that of the continuous domain: the time-frequency spread measure
is, in some sense, natural generalization of the variance measure in the
continuous domain, the lower bound for the uncertainty is close to that of the
continuous domain, and the lower bound is achieved approximately by the
'discrete Gaussians'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6328</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6328</id><created>2013-07-24</created><authors><author><keyname>Khan</keyname><forenames>Mohammad Ibrahim</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Maklachur</forenames></author><author><keyname>Sarker</keyname><forenames>Md. Iqbal Hasan</forenames></author></authors><title>Digital Watermarking for Image AuthenticationBased on Combined DCT, DWT
  and SVD Transformation</title><categories>cs.MM</categories><comments>8 pages, 7 figures and 2 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 3, No 1, May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hybrid digital image watermarking based on Discrete
Wavelet Transform (DWT), Discrete Cosine Transform (DCT) and Singular Value
Decomposition (SVD) in a zigzag order. From DWT we choose the high band to
embed the watermark that facilities to add more information, gives more
invisibility and robustness against some attacks. Such as geometric attack.
Zigzag method is applied to map DCT coefficients into four quadrants that
represent low, mid and high bands. Finally, SVD is applied to each quadrant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6333</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6333</id><created>2013-07-24</created><authors><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>Synthesis from Knowledge-Based Specifications</title><categories>cs.LO</categories><comments>An extended abstract of this paper appeared in CONCUR'98</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In program synthesis, we transform a specification into a program that is
guaranteed to satisfy the specification. In synthesis of reactive systems, the
environment in which the program operates may behave nondeterministically,
e.g., by generating different sequences of inputs in different runs of the
system. To satisfy the specification, the program needs to act so that the
specification holds in every computation generated by its interaction with the
environment. Often, the program cannot observe all attributes of its
environment. In this case, we should transform a specification into a program
whose behavior depends only on the observable history of the computation. This
is called synthesis with incomplete information. In such a setting, it is
desirable to have a knowledge-based specification, which can refer to the
uncertainty the program has about the environment's behavior. In this work we
solve the problem of synthesis with incomplete information with respect to
specifications in the logic of knowledge and time. We show that the problem has
the same worst-case complexity as synthesis with complete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6343</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6343</id><created>2013-07-24</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Xia</forenames></author></authors><title>The wireless router based on the linux system</title><categories>cs.NI</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the expansion of computer networks,the mobile terminal with wireless
access capability experience a sharp increase in the number of wireless
routers, especially low cost wireless routers are becoming very important
network equipment. This paper designs a wireless router based on the ARM
platform, the Linux system. First, there is a research and analysis on the
working principle and implementation of Network Address Translation (NAT)
technology. Then I study the IPTABLES components under the Linux system and use
it when processing data packets which go into the chain and the table and
finally using laptop with Ethernet card and the WIFI card to build the Linux
operating system. Related routing forwarding rules are defined between the two
cards and use IPTABLES to achieve a laptop as a wireless WIFI hotspot providing
routing and network connections to other computer services. It proves this
paper's design, and production feasibility. The paper also discusses the design
and production method of the wireless router with ARM board feasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6345</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6345</id><created>2013-07-24</created><authors><author><keyname>Chernyakova</keyname><forenames>Tanya</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Fourier Domain Beamforming: The Path to Compressed Ultrasound Imaging</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sonography techniques use multiple transducer elements for tissue
visualization. Signals detected at each element are sampled prior to digital
beamforming. The sampling rates required to perform high resolution digital
beamforming are significantly higher than the Nyquist rate of the signal and
result in considerable amount of data, that needs to be stored and processed. A
recently developed technique, compressed beamforming, based on the finite rate
of innovation model, compressed sensing (CS) and Xampling ideas, allows to
reduce the number of samples needed to reconstruct an image comprised of strong
reflectors. A drawback of this method is its inability to treat speckle, which
is of significant importance in medical imaging. Here we build on previous work
and extend it to a general concept of beamforming in frequency. This allows to
exploit the low bandwidth of the ultrasound signal and bypass the oversampling
dictated by digital implementation of beamforming in time. Using beamforming in
frequency, the same image quality is obtained from far fewer samples. We next
present a CS-technique that allows for further rate reduction, using only a
portion of the beamformed signal's bandwidth. We demonstrate our methods on in
vivo cardiac data and show that reductions up to 1/28 over standard beamforming
rates are possible. Finally, we present an implementation on an ultrasound
machine using sub-Nyquist sampling and processing. Our results prove that the
concept of sub-Nyquist processing is feasible for medical ultrasound, leading
to the potential of considerable reduction in future ultrasound machines size,
power consumption and cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6348</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6348</id><created>2013-07-24</created><updated>2013-07-25</updated><authors><author><keyname>Ciucanu</keyname><forenames>Radu</forenames></author><author><keyname>Staworko</keyname><forenames>Slawek</forenames></author></authors><title>Learning Schemas for Unordered XML</title><categories>cs.DB</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unordered XML, where the relative order among siblings is
ignored, and we investigate the problem of learning schemas from examples given
by the user. We focus on the schema formalisms proposed in [10]: disjunctive
multiplicity schemas (DMS) and its restriction, disjunction-free multiplicity
schemas (MS). A learning algorithm takes as input a set of XML documents which
must satisfy the schema (i.e., positive examples) and a set of XML documents
which must not satisfy the schema (i.e., negative examples), and returns a
schema consistent with the examples. We investigate a learning framework
inspired by Gold [18], where a learning algorithm should be sound i.e., always
return a schema consistent with the examples given by the user, and complete
i.e., able to produce every schema with a sufficiently rich set of examples.
Additionally, the algorithm should be efficient i.e., polynomial in the size of
the input. We prove that the DMS are learnable from positive examples only, but
they are not learnable when we also allow negative examples. Moreover, we show
that the MS are learnable in the presence of positive examples only, and also
in the presence of both positive and negative examples. Furthermore, for the
learnable cases, the proposed learning algorithms return minimal schemas
consistent with the examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6349</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6349</id><created>2013-07-24</created><authors><author><keyname>Jiang</keyname><forenames>Zhiping</forenames></author><author><keyname>Zhao</keyname><forenames>Jizhong</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Xi</keyname><forenames>Wei</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Han</keyname><forenames>Jinsong</forenames></author></authors><title>Communicating Is Crowdsourcing: Wi-Fi Indoor Localization with CSI-based
  Speed Estimation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous indoor localization techniques have been proposed recently to meet
the intensive demand for location based service, and Wi-Fi fingerprint-based
approaches are the most popular and inexpensive solutions. Among them, one of
the main trends is to incorporate the built-in sensors of smartphone and to
exploit crowdsourcing potentials. However the noisy built-in sensors and
multi-tasking limitation of underline OS often hinder the effectiveness of
these schemes. In this work, we propose a passive crowdsourcing CSI-based
indoor localization scheme, C2 IL. Our scheme C2 IL only requires the
locating-device (e.g., a phone) to have a 802.11n wireless connection, and it
does not rely on inertial sensors only existing in some smartphones. C2 IL is
built upon our innovative method to accurately estimate the moving distance
purely based on 802.11n Channel State Information (CSI). Our extensive
evaluations show that the moving distance estimation error of our scheme is
within 3% of the actual moving distance regardless of varying speeds and
environment. Relying on the accurate moving distance estimation as constraints,
we are able to construct a more accurate mapping between RSS fingerprints and
location. To address the challenges of collecting fingerprints, a
crowdsourcing- based scheme is designed to gradually establish the mapping and
populate the fingerprints. In C2 IL, we design a trajectory clustering-based
localization algorithm to provide precise real-time indoor localization and
tracking. We developed and deployed a practical working system of C2 IL in a
large office environment. Extensive evaluation results indicate that our scheme
C2 IL provides accurate localization with error 2m at 80% at very complex
indoor environment with minimal overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6350</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6350</id><created>2013-07-24</created><authors><author><keyname>Rosati</keyname><forenames>Stefano</forenames></author><author><keyname>Kruzelecki</keyname><forenames>Karol</forenames></author><author><keyname>Traynard</keyname><forenames>Louis</forenames></author><author><keyname>Rimoldi</keyname><forenames>Bixio</forenames></author></authors><title>Speed-Aware Routing for UAV Ad-Hoc Networks</title><categories>cs.NI</categories><comments>submitted to GlobeCom'13 Workshop - Wi-UAV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine mobile ad-hoc networks (MANET) composed by unmanned
aerial vehicles (UAVs). Due to the high-mobility of the nodes, these networks
are very dynamic and the existing routing protocols partly fail to provide a
reliable communication. We present Predictive-OLSR an extension to the
Optimized Link-State Routing (OLSR) protocol: it enables efficient routing in
very dynamic conditions. The key idea is to exploit GPS information to aid the
routing protocol. Predictive-OLSR weights the expected transmission count (ETX)
metric, taking into account the relative speed between the nodes. We provide
numerical results obtained by a MAC-layer emulator that integrates a flight
simulator to reproduce realistic flight conditions. These numerical results
show that Predictive-OLSR significantly outperforms OLSR and BABEL, providing a
reliable communication even in very dynamic conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6354</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6354</id><created>2013-07-24</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Protecting Anti-virus Programs From Viral Attacks</title><categories>cs.CR</categories><comments>TRIZsite Journal, Jul 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During a fight between viruses and anti-viruses it is not always predictable
that the anti-virus is going to win. There are many malicious viruses which
target to attack and paralyze the anti-viruses. It is necessary for an
anti-virus to detect and destroy the malware before its own files are detected
and destroyed by the malware. The anti-virus may follow thorough testing and
auditing procedures to fix all its bugs before releasing the software in the
market. Besides the anti-virus may use all the obfuscation techniques like
polymorphism that the viruses generally use to hide their codes. This article
also shows how to use TRIZ Inventive Standards to solve the harmful effects of
the viruses on the anti-virus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6357</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6357</id><created>2013-07-24</created><updated>2013-09-20</updated><authors><author><keyname>Mori</keyname><forenames>Takakazu</forenames><affiliation>Kyoto Sangyo University</affiliation></author><author><keyname>Tsujii</keyname><forenames>Yoshiki</forenames><affiliation>Kyoto Sangyo University</affiliation></author><author><keyname>Yasugi</keyname><forenames>Mariko</forenames><affiliation>Kyoto Sangyo University</affiliation></author></authors><title>Computability of Probability Distributions and Characteristic Functions</title><categories>cs.CC cs.LO math.PR</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  3, 2013) lmcs:888</journal-ref><doi>10.2168/LMCS-9(3:9)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a part of our works on effective properties of probability distributions,
we deal with the corresponding characteristic functions. A sequence of
probability distributions is computable if and only if the corresponding
sequence of characteristic functions is computable. As for the onvergence
problem, the effectivized Glivenko's theorem holds. Effectivizations of
Bochner's theorem and de Moivre-Laplace central limit theorem are also proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6360</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6360</id><created>2013-07-24</created><authors><author><keyname>Harrison</keyname><forenames>Karl</forenames></author><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author><author><keyname>Bowen</keyname><forenames>Alice M.</forenames></author></authors><title>Electronic Visualisation in Chemistry: From Alchemy to Art</title><categories>cs.GR physics.chem-ph</categories><comments>8 pages, 27 figures, EVA London 2013</comments><acm-class>I.3.m; J.3; J.5</acm-class><journal-ref>EVA London 2013 Conference Proceedings, Electronic Workshops in
  Computing (eWiC), British Computer Society, 29-31 July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemists now routinely use software as part of their work. For example,
virtual chemistry allows chemical reactions to be simulated. In particular, a
selection of software is available for the visualisation of complex
3-dimensional molecular structures. Many of these are very beautiful in their
own right. As well as being included as illustrations in academic papers, such
visualisations are often used on the covers of chemistry journals as
artistically decorative and attractive motifs. Chemical images have also been
used as the basis of artworks in exhibitions. This paper explores the
development of the relationship of chemistry, art, and IT. It covers some of
the increasingly sophisticated software used to generate these projections
(e.g., UCSF Chimera) and their progressive use as a visual art form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6365</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6365</id><created>2013-07-24</created><updated>2013-12-23</updated><authors><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Wistuba</keyname><forenames>Martin</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>Time-Series Classification Through Histograms of Symbolic Polynomials</title><categories>cs.AI cs.DB cs.LG</categories><doi>10.1109/TKDE.2014.2377746</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-series classification has attracted considerable research attention due
to the various domains where time-series data are observed, ranging from
medicine to econometrics. Traditionally, the focus of time-series
classification has been on short time-series data composed of a unique pattern
with intraclass pattern distortions and variations, while recently there have
been attempts to focus on longer series composed of various local patterns.
This study presents a novel method which can detect local patterns in long
time-series via fitting local polynomial functions of arbitrary degrees. The
coefficients of the polynomial functions are converted to symbolic words via
equivolume discretizations of the coefficients' distributions. The symbolic
polynomial words enable the detection of similar local patterns by assigning
the same words to similar polynomials. Moreover, a histogram of the frequencies
of the words is constructed from each time-series' bag of words. Each row of
the histogram enables a new representation for the series and symbolize the
existence of local patterns and their frequencies. Experimental evidence
demonstrates outstanding results of our method compared to the state-of-art
baselines, by exhibiting the best classification accuracies in all the datasets
and having statistically significant improvements in the absolute majority of
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6369</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6369</id><created>2013-07-24</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on reducing number of keys on a Computer Keyboard -- A TRIZ
  based analysis</title><categories>cs.HC</categories><comments>8 pages</comments><doi>10.2139/ssrn.931458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computer keyboard consists of several sections and each section consists of
several numbers of keys. The text entry section contains the standard character
keys, navigation section contains cursor movement and page control keys,
numeric keypad contains numeric keys and function keys section contain function
keys and special keys. Although the increased number of keys helps smooth
interaction with a computer there are situations where it is necessary to
reduce the number of keys.
  The need for reducing number of keys in a keyboard creates a technical
contradiction. Generally more number of keys can support more number of
functions and less number of keys can support less number of functions. We need
more number of functions (all the functions available in a full size keyboard),
but we want only less number of keys (Contradiction). This contradiction has
been solved by using multi-stroke mechanism.
  There may be different purposes to reduce the number of keys in a keyboard.
Some of them intend to reduce the size of the keyboard while some others intend
to increase the speed of typing. But all of these inventions use multi-stroke
mechanism to generate more number of signals from less number of keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6372</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6372</id><created>2013-07-24</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Soft Keyboards -- A TRIZ Based Analysis</title><categories>cs.HC</categories><comments>7 pages. TRIZsite Journal, February 2005</comments><doi>10.2139/ssrn.931461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The soft keyboards are onscreen representation of physical keyboard having
alphanumeric characters and other controls. The user operates the soft keyboard
with the mouse, a stylus or other pointing device. The soft keys dont have any
mechanical component.
  The soft keyboards are used in many public places for informational purpose,
educational systems and financial transactional systems. A soft keyboard is
convenient in some cases where a hard keyboard is difficult to manage. The soft
keyboard is a substitute of a physical keyboard and is displayed on the screen.
It displays the same type of alphanumeric and control keys like the keys on the
actual keyboard.
  There are many inventions on a soft keyboard which makes the soft keyboard
more efficient and effective. This article illustrates some inventions on soft
keyboards from US patent database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6373</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6373</id><created>2013-07-24</created><updated>2014-03-25</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Effect of Spatial Interference Correlation on the Performance of Maximum
  Ratio Combining</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2014.041714.131330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the performance of maximum ratio combining (MRC) is well understood for
a single isolated link, the same is not true in the presence of interference,
which is typically correlated across antennas due to the common locations of
interferers. For tractability, prior work focuses on the two extreme cases
where the interference power across antennas is either assumed to be fully
correlated or fully uncorrelated. In this paper, we address this shortcoming
and characterize the performance of MRC in the presence of spatially-correlated
interference across antennas. Modeling the interference field as a Poisson
point process, we derive the exact distribution of the signal-to-interference
ratio (SIR) for the case of two receive antennas, and upper and lower bounds
for the general case. Using these results, we study the diversity behavior of
MRC and characterize the critical density of simultaneous transmissions for a
given outage constraint. The exact SIR distribution is also useful in
benchmarking simpler correlation models. We show that the full-correlation
assumption is considerably pessimistic (up to 30% higher outage probability for
typical values) and the no-correlation assumption is significantly optimistic
compared to the true performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6378</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6378</id><created>2013-07-24</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Keyboard Illumination - A TRIZ Based Analysis</title><categories>cs.HC</categories><comments>8 pages</comments><doi>10.2139/ssrn.931466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional computers are not usable in dark, as the user cannot see the
keyboard to operate properly. But there are many situations where the user may
like to work on his computer under low light conditions, such as, during
nights, while traveling in flights/ trains having low lights, working in the
garden during evening etc. So it is necessary to provide some light to the
laptop to operate the keyboard under low light situations.
  Luminescence of the keyboard is a desirable feature for portable computers.
There may be several methods to illuminate the keyboard, the most crude may be
carrying an external light and fixing at a point where it can light the
keyboard. But solutions like this are not very convenient. The inventors have
been looking for better solutions. This article analyses 6 patents on keyboard
illumination.
  A good illumination system do not consume more battery, do not increase the
size and weight of the laptop, do not increase heat inside the laptop box, and
do not cost much to manufacture. The light should uniformly illuminate the
whole keyboard and should not throw light on other areas that are not required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6398</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6398</id><created>2013-07-24</created><updated>2014-05-29</updated><authors><author><keyname>Boumal</keyname><forenames>Nicolas</forenames></author><author><keyname>Cheng</keyname><forenames>Xiuyuan</forenames></author></authors><title>Concentration of the Kirchhoff index for Erdos-Renyi graphs</title><categories>cs.IT math.IT</categories><doi>10.1016/j.sysconle.2014.10.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph, the resistance distance between two nodes is the
resistance one would measure between these two nodes in an electrical network
if edges were resistors. Summing these distances over all pairs of nodes yields
the so-called Kirchhoff index of the graph, which measures its overall
connectivity. In this work, we consider Erdos-Renyi random graphs. Since the
graphs are random, their Kirchhoff indices are random variables. We give
formulas for the expected value of the Kirchhoff index and show it concentrates
around its expectation. We achieve this by studying the trace of the
pseudoinverse of the Laplacian of Erdos-Renyi graphs. For synchronization (a
class of estimation problems on graphs) our results imply that acquiring
pairwise measurements uniformly at random is a good strategy, even if only a
vanishing proportion of the measurements can be acquired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6406</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6406</id><created>2013-07-24</created><authors><author><keyname>Paikaray</keyname><forenames>Bijay</forenames></author></authors><title>Relative Performance of a Multi-level Cache with Last-Level Cache
  Replacement: An Analytic Review</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current day processors employ multi-level cache hierarchy with one or two
levels of private caches and a shared last-level cache (LLC). An efficient
cache replacement policy at LLC is essential for reducing the off-chip memory
transfer as well as conflict for memory bandwidth. Cache replacement techniques
for inclusive LLCs may not be efficient for multilevel cache as it can be
shared by enormous applications with varying access behavior, running
simultaneously. One application may dominate another by flooding of cache
requests and evicting the useful data of the other application. From the
performance point of view, an exclusive LLC make the replacement policies more
demanding, as compared to an inclusive LLC. This paper analyzes some of the
existing replacement techniques on the LLC with their performance assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6408</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6408</id><created>2013-07-24</created><updated>2014-03-19</updated><authors><author><keyname>Klouda</keyname><forenames>Karel</forenames></author><author><keyname>Starosta</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author></authors><title>An Algorithm Enumerating All Infinite Repetitions in a D0L System</title><categories>math.CO cs.FL</categories><comments>10 pages, one figure</comments><msc-class>68R15</msc-class><acm-class>G.2.1</acm-class><journal-ref>Journal of Discrete Algorithms 33 (2015), 130-138</journal-ref><doi>10.1016/j.jda.2015.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple algorithm which, for a given D0L system, returns all
factors $v$ such that $v^k$ is in the language of the system for all $k$. This
algorithm can be used to decide whether a D0L system is repetitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6409</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6409</id><created>2013-07-24</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>A cryptographic image encryption technique for facial-blurring of images</title><categories>cs.CR</categories><comments>7 pages. International Journal of Advanced Technology &amp; Engineering
  Research (IJATER), 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Protection of faces in pictures and videos of people in connection with
sensitive information, activism, abused cases and others on public broadcasting
media and social net- works is very important. On social networks like
YouTube,facebook, Twitter and others, videos are being posted with blurring
techniques of which some of them cannot be recoverable. Most blurring
techniques used can easily be recoverable using off-the-shelf software. The
ones that are difficult to be recovered also can easily be used by abusers and
ther wrong doers.
  This paper proposes an image encryption technique that will make it possible
for selected facial area to be encrypted based on RGB pixel shuffling of an m*n
size image. This will make it difficult for off-the-shelf software to restore
the ncrypted image and also make it easy for the law enforcement agencies to
reconstruct the face back in case the picture or video is related to an abuse
case. The implementation of the encryption method will be done using MATLAB. At
the and, there will be no change in the total size of the image during
encryption and decryption process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6410</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6410</id><created>2013-07-24</created><authors><author><keyname>Boguslawski</keyname><forenames>Bartosz</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Seguin</keyname><forenames>Fabrice</forenames></author><author><keyname>Heitzmann</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Storing non-uniformly distributed messages in networks of neural cliques</title><categories>cs.NE cs.SY</categories><comments>21 pages, 8 figures, submitted to Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative memories are data structures that allow retrieval of stored
messages from part of their content. They thus behave similarly to human brain
that is capable for instance of retrieving the end of a song given its
beginning. Among different families of associative memories, sparse ones are
known to provide the best efficiency (ratio of the number of bits stored to
that of bits used). Nevertheless, it is well known that non-uniformity of the
stored messages can lead to dramatic decrease in performance. We introduce
several strategies to allow efficient storage of non-uniform messages in
recently introduced sparse associative memories. We analyse and discuss the
methods introduced. We also present a practical application example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6414</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6414</id><created>2013-07-24</created><authors><author><keyname>Knauer</keyname><forenames>Christian</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Stefan</forenames></author><author><keyname>Werner</keyname><forenames>Daniel</forenames></author></authors><title>Fixed Parameter Complexity and Approximability of Norm Maximization</title><categories>cs.CC cs.CG math.MG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing the $p$-th power of a $p$-norm over a
halfspace-presented polytope in $\R^d$ is a convex maximization problem which
plays a fundamental role in computational convexity. It has been shown in 1986
that this problem is $\NP$-hard for all values $p \in \mathbb{N}$, if the
dimension $d$ of the ambient space is part of the input. In this paper, we use
the theory of parametrized complexity to analyze how heavily the hardness of
norm maximization relies on the parameter $d$.
  More precisely, we show that for $p=1$ the problem is fixed parameter
tractable but that for all $p \in \mathbb{N} \setminus \{1\}$ norm maximization
is W[1]-hard.
  Concerning approximation algorithms for norm maximization, we show that for
fixed accuracy, there is a straightforward approximation algorithm for norm
maximization in FPT running time, but there is no FPT approximation algorithm,
the running time of which depends polynomially on the accuracy.
  As with the $\NP$-hardness of norm maximization, the W[1]-hardness
immediately carries over to various radius computation tasks in Computational
Convexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6422</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6422</id><created>2013-07-24</created><authors><author><keyname>Nguyen</keyname><forenames>Van Tien</forenames><affiliation>LIUPPA</affiliation></author><author><keyname>Sallaberry</keyname><forenames>Christian</forenames><affiliation>LIUPPA</affiliation></author><author><keyname>Gaio</keyname><forenames>Mauro</forenames><affiliation>LIUPPA</affiliation></author></authors><title>Mesure de la similarit\'e entre termes et labels de concepts
  ontologiques</title><categories>cs.IR</categories><proxy>ccsd</proxy><journal-ref>CORIA 2013, Neufch\^atel : Suisse (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper a method for measuring the similarity between
ontological concepts and terms. Our metric can take into account not only the
common words of two strings to compare but also other features such as the
position of the words in these strings, or the number of deletion, insertion or
replacement of words required for the construction of one of the two strings
from each other. The proposed method was then used to determine the ontological
concepts which are equivalent to the terms that qualify toponymes. It aims to
find the topographical type of the toponyme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6429</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6429</id><created>2013-07-24</created><updated>2014-06-26</updated><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author></authors><title>Generalized Wong sequences and their applications to Edmonds' problems</title><categories>cs.CC cs.DS</categories><comments>25 pages; improved presentation; fix some gaps</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design two deterministic polynomial time algorithms for variants of a
problem introduced by Edmonds in 1967: determine the rank of a matrix M whose
entries are homogeneous linear polynomials over the integers. Given a linear
subspace B of the n by n matrices over some field F, we consider the following
problems: symbolic matrix rank (SMR) is the problem to determine the maximum
rank among matrices in B, symbolic determinant identity testing (SDIT) is the
question to decide whether there exists a nonsingular matrix in B. The
constructive versions of these problems are asking to find a matrix of maximum
rank, respectively a nonsingular matrix, if there exists one.
  Our first algorithm solves the constructive SMR when B is spanned by unknown
rank one matrices, answering an open question of Gurvits. Our second algorithm
solves the constructive SDIT when B is spanned by triangularizable matrices,
but the triangularization is not given explicitly. Both algorithms work over
finite fields of size at least n+1 and over the rational numbers, and the first
algorithm actually solves (the non-constructive) SMR independently from the
field size. Our main tool to obtain these results is to generalize Wong
sequences, a classical method to deal with pairs of matrices, to the case of
pairs of matrix spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6436</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6436</id><created>2013-07-24</created><updated>2014-05-23</updated><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author></authors><title>Birth and death of links control disease spreading in empirical contact
  networks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><journal-ref>Scientific Reports 4, 4999 (2014)</journal-ref><doi>10.1038/srep04999</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate what structural aspects of a collection of twelve empirical
temporal networks of human contacts are important to disease spreading. We scan
the entire parameter spaces of the two canonical models of infectious disease
epidemiology -- the Susceptible-Infectious-Susceptible (SIS) and
Susceptible-Infectious-Removed (SIR) models. The results from these simulations
are compared to reference data where we eliminate structures in the interevent
intervals, the time to the first contact in the data, or the time from the last
contact to the end of the sampling. The picture we find is that the birth and
death of links, and the total number of contacts over a link, are essential to
predict outbreaks. On the other hand, the exact times of contacts between the
beginning and end, or the interevent interval distribution, do not matter much.
In other words, a simplified picture of these empirical data sets that suffices
for epidemiological purposes is that links are born, is active with some
intensity, and die.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6444</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6444</id><created>2013-07-24</created><updated>2014-08-05</updated><authors><author><keyname>&#x10c;adek</keyname><forenames>Martin</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Marek</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Luk&#xe1;&#x161;</forenames></author></authors><title>Algorithmic solvability of the lifting-extension problem</title><categories>math.AT cs.CG</categories><comments>52 pages</comments><msc-class>55P91, 55S35, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ and $Y$ be finite simplicial sets (e.g. finite simplicial complexes),
both equipped with a free simplicial action of a finite group $G$. Assuming
that $Y$ is $d$-connected and $\dim X\le 2d$, for some $d\geq 1$, we provide an
algorithm that computes the set of all equivariant homotopy classes of
equivariant continuous maps $|X|\to|Y|$; the existence of such a map can be
decided even for $\dim X\leq 2d+1$. For fixed $G$ and $d$, the algorithm runs
in polynomial time. This yields the first algorithm for deciding topological
embeddability of a $k$-dimensional finite simplicial complex into
$\mathbb{R}^n$ under the conditions $k\leq\frac 23 n-1$.
  More generally, we present an algorithm that, given a lifting-extension
problem satisfying an appropriate stability assumption, computes the set of all
homotopy classes of solutions. This result is new even in the non-equivariant
situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6446</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6446</id><created>2013-07-24</created><updated>2013-08-13</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>Integral population control of a quadratic dimerization process</title><categories>math.OC cs.SY q-bio.MN</categories><comments>7 pages; 3 figures; accepted at the 52nd IEEE Conference on Decision
  and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moment control of a simple quadratic reaction network describing a
dimerization process is addressed. It is shown that the moment closure problem
can be circumvented without invoking any moment closure technique. Local
stabilization and convergence of the average dimer population to any desired
reference value is ensured using a pure integral control law. Explicit bounds
on the controller gain are provided and shown to be valid for any reference
value. As a byproduct, an explicit upper-bound of the variance of the monomer
species, acting on the system as unknown input due to the moment openness, is
obtained. The obtained results are illustrated by an example relying on the
simulation of a cell population using stochastic simulation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6458</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6458</id><created>2013-07-24</created><updated>2014-03-28</updated><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author><author><keyname>Gaborit</keyname><forenames>Philippe</forenames></author><author><keyname>Gauthier-Uma&#xf1;a</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Distinguisher-Based Attacks on Public-Key Cryptosystems Using
  Reed-Solomon Codes</title><categories>cs.CR cs.IT math.IT</categories><comments>A short version appeared in the proceedings of the workshop on Coding
  and Cryptography 2013 (WCC' 2013). This paper supersedes arXiv:1203.6686
  [cs.CR] and arXiv:1204.6459 [cs.CR]</comments><msc-class>11T71, 94B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of their interesting algebraic properties, several authors promote
the use of generalized Reed-Solomon codes in cryptography. Niederreiter was the
first to suggest an instantiation of his cryptosystem with them but Sidelnikov
and Shestakov showed that this choice is insecure. Wieschebrink proposed a
variant of the McEliece cryptosystem which consists in concatenating a few
random columns to a generator matrix of a secretly chosen generalized
Reed-Solomon code. More recently, new schemes appeared which are the
homomorphic encryption scheme proposed by Bogdanov and Lee, and a variation of
the McEliece cryptosystem proposed by Baldi et \textit{al.} which hides the
generalized Reed-Solomon code by means of matrices of very low rank.
  In this work, we show how to mount key-recovery attacks against these
public-key encryption schemes. We use the concept of distinguisher which aims
at detecting a behavior different from the one that one would expect from a
random code. All the distinguishers we have built are based on the notion of
component-wise product of codes. It results in a powerful tool that is able to
recover the secret structure of codes when they are derived from generalized
Reed-Solomon codes. Lastly, we give an alternative to Sidelnikov and Shestakov
attack by building a filtration which enables to completely recover the support
and the non-zero scalars defining the secret generalized Reed-Solomon code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6459</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6459</id><created>2013-07-24</created><updated>2013-08-15</updated><authors><author><keyname>&#xdc;nsal</keyname><forenames>Ay&#x15f;e</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author></authors><title>Distortion bounds and Two-Way Protocols for One-Shot Transmission of
  Correlated Random Variables</title><categories>cs.IT math.IT</categories><comments>This paper was presented [in part] at EUSIPCO 2012, European Signal
  Processing Conference, August, 27-31, 2012, Bucharest, and SCC 2013, 9th
  International ITG Conference on Systems, Communications and Coding, January
  21-24, 2013, Munich, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides lower bounds on the reconstruction error for transmission
of two continuous correlated random vectors sent over both sum and parallel
channels using the help of two causal feedback links from the decoder to the
encoders connected to each sensor. This construction is considered for both
uniformly and normally distributed sources with zero mean and unit variance.
Additionally, a two-way retransmission protocol, which is a non-coherent
adaptation of the original work by Yamamoto is introduced for an additive white
Gaussian noise channel with one degree of freedom. Furthermore, the novel
protocol of a single source is extended to the dual-source case again for two
different source distributions. Asymptotic optimality of the protocols are
analyzed and upper bounds on the distortion level are derived for two-rounds
considering two extreme cases of high and low correlation among the sources. It
is shown by both the upper and lower-bounds that collaboration can be achieved
through energy accumulation. Analytical results are supported by numerical
analysis for both the single and dual-source cases to show the improvement in
terms of distortion to be gained by retransmission subject to the average
energy used by protocol . To cover a more realistic scenario, the same protocol
of a single source is adapted to a wireless channel and their performances are
compared through numerical evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6462</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6462</id><created>2013-07-24</created><authors><author><keyname>Ferrada</keyname><forenames>Hector</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Hirvola</keyname><forenames>Tommi</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>AliBI: An Alignment-Based Index for Genomic Datasets</title><categories>cs.DS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With current hardware and software, a standard computer can now hold in RAM
an index for approximate pattern matching on about half a dozen human genomes.
Sequencing technologies have improved so quickly, however, that scientists will
soon demand indexes for thousands of genomes. Whereas most researchers who have
addressed this problem have proposed completely new kinds of indexes, we
recently described a simple technique that scales standard indexes to work on
more genomes. Our main idea was to filter the dataset with LZ77, build a
standard index for the filtered file, and then create a hybrid of that standard
index and an LZ77-based index. In this paper we describe how to our technique
to use alignments instead of LZ77, in order to simplify and speed up both
preprocessing and random access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6468</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6468</id><created>2013-07-24</created><authors><author><keyname>Becker</keyname><forenames>Florent</forenames></author><author><keyname>Chapelle</keyname><forenames>Mathieu</forenames></author><author><keyname>Durand-Lose</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Levorato</keyname><forenames>Vincent</forenames></author><author><keyname>Senot</keyname><forenames>Maxime</forenames></author></authors><title>Abstract Geometrical Computation 8: Small Machines, Accumulations and
  Rationality</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of abstract geometrical computation, computing with colored
line segments, we study the possibility of having an accumulation with small
signal machines, ie, signal machines having only a very limited number of
distinct speeds. The cases of 2 and 4 speeds are trivial: we provide a proof
that no machine can produce an accumulation in the case of 2 speeds and exhibit
an accumulation with 4 speeds. The main result is the twofold case of 3 speeds.
On the one hand, we prove that accumulations cannot happen when all ratios
between speeds and all ratios between initial distances are rational. On the
other hand, we provide examples of an accumulation in the case of an irrational
ratio between 2 speeds and in the case of an irrational ratio between two
distances in the initial configuration. This dichotomy is explained by the
presence of a phenomenon computing Euclid's algorithm (gcd): it stops if and
only if its input is commensurate (ie, of rational ratio).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6476</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6476</id><created>2013-07-24</created><authors><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Rigid Body Localization Using Sensor Networks: Position and Orientation
  Estimation</title><categories>cs.IT math.IT</categories><comments>30 pages, draft submitted to IEEE Trans. on Signal Processing, 10
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel framework called rigid body localization
for joint position and orientation estimation of a rigid body. We consider a
setup in which a few sensors are mounted on a rigid body. The absolute position
of the sensors on the rigid body, or the absolute position of the rigid body
itself is not known. However, we know how the sensors are mounted on the rigid
body, i.e., the sensor topology is known. Using range-only measurements between
the sensors and a few anchors (nodes with known absolute positions), and
without using any inertial measurements (e.g., accelerometers), we estimate the
position and orientation of the rigid body. For this purpose, the absolute
position of the sensors is expressed as an affine function of the Stiefel
manifold. In other words, we represent the orientation as a rotation matrix,
and absolute position as a translation vector. We propose a least-squares (LS),
simplified unitarily constrained LS (SUC-LS), and optimal unitarily constrained
least-squares (OUC-LS) estimator, where the latter is based on Newton's method.
As a benchmark, we derive a unitarily constrained Cram\'er-Rao bound (UC-CRB).
The known topology of the sensors can sometimes be perturbed during
fabrication. To take these perturbations into account, a simplified unitarily
constrained total-least-squares (SUC-TLS), and an optimal unitarily constrained
total-least-squares (OUC-TLS) estimator are also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6477</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6477</id><created>2013-07-24</created><authors><author><keyname>Bah</keyname><forenames>Bubacarr</forenames></author><author><keyname>Tanner</keyname><forenames>Jared</forenames></author></authors><title>On construction and analysis of sparse random matrices and expander
  graphs with applications to compressed sensing</title><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1207.3094</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the probabilistic construction of sparse random matrices where
each column has a fixed number of nonzeros whose row indices are drawn
uniformly at random. These matrices have a one-to-one correspondence with the
adjacency matrices of lossless expander graphs. We present tail bounds on the
probability that the cardinality of the set of neighbors for these graphs will
be less than the expected value. The bounds are derived through the analysis of
collisions in unions of sets using a {\em dyadic splitting} technique. This
analysis led to the derivation of better constants that allow for quantitative
theorems on existence of lossless expander graphs and hence the sparse random
matrices we consider and also quantitative compressed sensing sampling theorems
when using sparse non mean-zero measurement matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6488</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6488</id><created>2013-07-24</created><authors><author><keyname>Blazewicz</keyname><forenames>Marek</forenames><affiliation>Pozna&#x144; Supercomputing and Networking Center</affiliation><affiliation>Pozna&#x144; University of Technology</affiliation></author><author><keyname>Hinder</keyname><forenames>Ian</forenames><affiliation>Albert Einstein Institute</affiliation></author><author><keyname>Koppelman</keyname><forenames>David M.</forenames><affiliation>Center for Computation and Technology, LSU</affiliation><affiliation>Division of Electrical Computer Engineering, LSU</affiliation></author><author><keyname>Brandt</keyname><forenames>Steven R.</forenames><affiliation>Center for Computation and Technology, LSU</affiliation><affiliation>Division of Computer Science, LSU</affiliation></author><author><keyname>Ciznicki</keyname><forenames>Milosz</forenames><affiliation>Pozna&#x144; Supercomputing and Networking Center</affiliation></author><author><keyname>Kierzynka</keyname><forenames>Michal</forenames><affiliation>Pozna&#x144; Supercomputing and Networking Center</affiliation><affiliation>Pozna&#x144; University of Technology</affiliation></author><author><keyname>L&#xf6;ffler</keyname><forenames>Frank</forenames><affiliation>Center for Computation and Technology, LSU</affiliation></author><author><keyname>Schnetter</keyname><forenames>Erik</forenames><affiliation>Perimeter Institute for Theoretical Physics</affiliation><affiliation>Department of Physics, University of Guelph</affiliation><affiliation>Center for Computation and Technology, LSU</affiliation></author><author><keyname>Tao</keyname><forenames>Jian</forenames><affiliation>Center for Computation and Technology, LSU</affiliation></author></authors><title>From Physics Model to Results: An Optimizing Framework for
  Cross-Architecture Code Generation</title><categories>physics.comp-ph cs.MS gr-qc</categories><comments>18 pages, 4 figures, accepted for publication in Scientific
  Programming</comments><report-no>AEI-2013-227</report-no><doi>10.3233/SPR-130360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from a high-level problem description in terms of partial
differential equations using abstract tensor notation, the Chemora framework
discretizes, optimizes, and generates complete high performance codes for a
wide range of compute architectures. Chemora extends the capabilities of
Cactus, facilitating the usage of large-scale CPU/GPU systems in an efficient
manner for complex applications, without low-level code tuning. Chemora
achieves parallelism through MPI and multi-threading, combining OpenMP and
CUDA. Optimizations include high-level code transformations, efficient loop
traversal strategies, dynamically selected data and instruction cache usage
strategies, and JIT compilation of GPU code tailored to the problem
characteristics. The discretization is based on higher-order finite differences
on multi-block domains. Chemora's capabilities are demonstrated by simulations
of black hole collisions. This problem provides an acid test of the framework,
as the Einstein equations contain hundreds of variables and thousands of terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6505</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6505</id><created>2013-07-24</created><updated>2015-03-13</updated><authors><author><keyname>Ilic</keyname><forenames>Aleksandar</forenames></author></authors><title>On the variable common due date, minimal tardy jobs bicriteria
  two-machine flow shop problem with ordered machines</title><categories>cs.DS</categories><comments>6 pages, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a special case of the ordinary NP-hard two-machine flow shop
problem with the objective of determining simultaneously a minimal common due
date and the minimal number of tardy jobs. In [S. S. Panwalkar, C. Koulamas, An
O(n^2) algorithm for the variable common due date, minimal tardy jobs
bicriteria two-machine flow shop problem with ordered machines, European
Journal of Operational Research 221 (2012), 7-13.], the authors presented
quadratic algorithm for the problem when each job has its smaller processing
time on the first machine. In this note, we improve the running time of the
algorithm to O(n log n) by efficient implementation using recently introduced
modified binary tree data structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6512</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6512</id><created>2013-07-24</created><authors><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Optimal Grouping for Group Minimax Hypothesis Testing</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian hypothesis testing and minimax hypothesis testing represent extreme
instances of detection in which the prior probabilities of the hypotheses are
either completely and precisely known, or are completely unknown. Group
minimax, also known as Gamma-minimax, is a robust intermediary between Bayesian
and minimax hypothesis testing that allows for coarse or partial advance
knowledge of the hypothesis priors by using information on sets in which the
prior lies. Existing work on group minimax, however, does not consider the
question of how to define the sets or groups of priors; it is assumed that the
groups are given. In this work, we propose a novel intermediate detection
scheme formulated through the quantization of the space of prior probabilities
that optimally determines groups and also representative priors within the
groups. We show that when viewed from a quantization perspective, group minimax
amounts to determining centroids with a minimax Bayes risk error divergence
distortion criterion: the appropriate Bregman divergence for this task.
Moreover, the optimal partitioning of the space of prior probabilities is a
Bregman Voronoi diagram. Together, the optimal grouping and representation
points are an epsilon-net with respect to Bayes risk error divergence, and
permit a rate-distortion type asymptotic analysis of detection performance with
the number of groups. Examples of detecting signals corrupted by additive white
Gaussian noise and of distinguishing exponentially-distributed signals are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6515</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6515</id><created>2013-07-24</created><authors><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Narayanan</keyname><forenames>Srivatsan</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Cluster Trees on Manifolds</title><categories>stat.ML cs.LG</categories><comments>28 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of estimating the cluster tree for a
density $f$ supported on or near a smooth $d$-dimensional manifold $M$
isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a
$k$-nearest neighbor based algorithm recently proposed by Chaudhuri and
Dasgupta. The main results of this paper show that under mild assumptions on
$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on
the ambient dimension $D$. We also show that similar (albeit non-algorithmic)
results can be obtained for kernel density estimators. We sketch a construction
of a sample complexity lower bound instance for a natural class of manifold
oblivious clustering algorithms. We further briefly consider the known manifold
case and show that in this case a spatially adaptive algorithm achieves better
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6528</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6528</id><created>2013-07-24</created><authors><author><keyname>Naghizadeh</keyname><forenames>Parinaz</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Incentives, Quality, and Risks: A Look Into the NSF Proposal Review
  Pilot</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>10 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The National Science Foundation (NSF) will be experimenting with a new
distributed approach to reviewing proposals, whereby a group of principal
investigators (PIs) or proposers in a subfield act as reviewers for the
proposals submitted by the same set of PIs. To encourage honesty, PIs' chances
for getting funded are tied to the quality of their reviews (with respect to
the reviews provided by the entire group), in addition to the quality of their
proposals. Intuitively, this approach can more fairly distribute the review
workload, discourage frivolous proposal submission, and encourage high quality
reviews. On the other hand, this method has already raised concerns about the
integrity of the process and the possibility of strategic manipulation. In this
paper, we take a closer look at three specific issues in an attempt to gain a
better understanding of the strengths and limitations of the new process beyond
first impressions and anecdotal evidence. We start by considering the benefits
and drawbacks of bundling the quality of PIs' reviews with the scientific merit
of their proposals. We then consider the issue of collusion and favoritism.
Finally, we examine whether the new process puts controversial proposals at a
disadvantage. We conclude that some benefits of using review quality as an
incentive mechanism may outweigh its drawbacks. On the other hand, even a
coalition of two PIs can cause significant harm to the process, as the built-in
incentives are not strong enough to deter collusion. While we also confirm the
common suspicion that the process is skewed toward non-controversial proposals,
the more unexpected finding is that among equally controversial proposals,
those of lower quality get a leg up through this process. Thus the process not
only favors non-controversial proposals, but in some sense, mediocrity. We also
discuss possible ways to improve this review process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6530</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6530</id><created>2013-07-24</created><updated>2014-11-17</updated><authors><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Turner</keyname><forenames>Katharine</forenames></author><author><keyname>Bendich</keyname><forenames>Paul</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayan</forenames></author><author><keyname>Mattingly</keyname><forenames>Jonathan</forenames></author><author><keyname>Harer</keyname><forenames>John</forenames></author></authors><title>Probabilistic Fr\'echet Means for Time Varying Persistence Diagrams</title><categories>math.PR cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to use persistence diagrams as a true statistical tool, it would be
very useful to have a good notion of mean and variance for a set of diagrams.
In 2011, Mileyko and his collaborators made the first study of the properties
of the Fr\'echet mean in $(\mathcal{D}_p,W_p)$, the space of persistence
diagrams equipped with the p-th Wasserstein metric. In particular, they showed
that the Fr\'echet mean of a finite set of diagrams always exists, but is not
necessarily unique. The means of a continuously-varying set of diagrams do not
themselves (necessarily) vary continuously, which presents obvious problems
when trying to extend the Fr\'echet mean definition to the realm of vineyards.
  We fix this problem by altering the original definition of Fr\'echet mean so
that it now becomes a probability measure on the set of persistence diagrams;
in a nutshell, the mean of a set of diagrams will be a weighted sum of atomic
measures, where each atom is itself a persistence diagram determined using a
perturbation of the input diagrams. This definition gives for each $N$ a map
$(\mathcal{D}_p)^N \to \mathbb{P}(\mathcal{D}_p)$. We show that this map is
H\&quot;older continuous on finite diagrams and thus can be used to build a useful
statistic on time-varying persistence diagrams, better known as vineyards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6542</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6542</id><created>2013-07-09</created><authors><author><keyname>'Uyun</keyname><forenames>Shofwatul</forenames></author><author><keyname>Hartati</keyname><forenames>Sri</forenames></author><author><keyname>Harjoko</keyname><forenames>Agus</forenames></author><author><keyname>Subanar</keyname></author></authors><title>Selection Mammogram Texture Descriptors Based on Statistics Properties
  Backpropagation Structure</title><categories>cs.CV</categories><comments>5 pages, International Journal of Computer Science and Information
  Security (IJCSIS) Vol. 11, No. 5, May 2013 Article ID 26041306, arXiv admin
  note: substantial text overlap with arXiv:1306.5960; arXiv:1306.6489</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computer Aided Diagnosis (CAD) system has been developed for the early
detection of breast cancer, one of the most deadly cancer for women. The benign
of mammogram has different texture from malignant. There are fifty mammogram
images used in this work which are divided for training and testing. Therefore,
the selection of the right texture to determine the level of accuracy of CAD
system is important. The first and second order statistics are the texture
feature extraction methods which can be used on a mammogram. This work
classifies texture descriptor into nine groups where the extraction of features
is classified using backpropagation learning with two types of multi-layer
perceptron (MLP). The best texture descriptor as selected when the value of
regression 1 appears in both the MLP-1 and the MLP-2 with the number of epoches
less than 1000. The results of testing show that the best selected texture
descriptor is the second order (combination) using all direction (0, 45, 90 and
135) that have twenty four descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6544</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6544</id><created>2013-07-17</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author></authors><title>Veni Vidi Vici, A Three-Phase Scenario For Parameter Space Analysis in
  Image Analysis and Visualization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic analysis of the enormous sets of images is a critical task in life
sciences. This faces many challenges such as: algorithms are highly
parameterized, significant human input is intertwined, and lacking a standard
meta-visualization approach. This paper proposes an alternative iterative
approach for optimizing input parameters, saving time by minimizing the user
involvement, and allowing for understanding the workflow of algorithms and
discovering new ones. The main focus is on developing an interactive
visualization technique that enables users to analyze the relationships between
sampled input parameters and corresponding output. This technique is
implemented as a prototype called Veni Vidi Vici, or &quot;I came, I saw, I
conquered.&quot; This strategy is inspired by the mathematical formulas of numbering
computable functions and is developed atop ImageJ, a scientific image
processing program. A case study is presented to investigate the proposed
framework. Finally, the paper explores some potential future issues in the
application of the proposed approach in parameter space analysis in
visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6548</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6548</id><created>2013-06-16</created><authors><author><keyname>Moumen</keyname><forenames>Abdelkarim</forenames></author><author><keyname>Zatni</keyname><forenames>Abdelkarim</forenames></author><author><keyname>Elkaaouachi</keyname><forenames>Abdelhamid</forenames></author><author><keyname>Elyamani</keyname><forenames>Abdenabi</forenames></author><author><keyname>Bousseta</keyname><forenames>Hamza</forenames></author></authors><title>Time-Domain Large Signal Investigation on Dynamic Responses of the GDCC
  Quarterly Wavelength Shifted Distributed Feedback Semiconductor Laser</title><categories>cs.NI</categories><comments>6 pages, 6 figures,
  http://thesai.org/Downloads/Volume3No9/Paper_24-Time-Domain_Large_Signal_Investigation_on_Dynamic_Responses_of_the_GDCC_Quarterly_Wavelength_Shifted_Distributed_Feedback_Semiconductor_Laser.pdf</comments><msc-class>68M10</msc-class><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Vol.3, No.9, 2012 pages 165-170</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A numerical investigation on the dynamic large-signal analysis using a
time-domain traveling wave model of quarter wave-shifted distributed feedback
semiconductor lasers diode with a Gaussian distribution of the coupling
coefficient (GDCC) is presented. It is found that the single-mode behavior and
the more hole-burning effect corrections of quarter wave-shifted distributed
feedback laser with large coupling coefficient can be improved significantly by
this new proposed light source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6549</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6549</id><created>2013-07-19</created><authors><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Glashoff</keyname><forenames>Klaus</forenames></author><author><keyname>Loring</keyname><forenames>Terry A.</forenames></author></authors><title>Making Laplacians commute</title><categories>cs.CV cs.GR math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct multimodal spectral geometry by finding a pair of
closest commuting operators (CCO) to a given pair of Laplacians. The CCOs are
jointly diagonalizable and hence have the same eigenbasis. Our construction
naturally extends classical data analysis tools based on spectral geometry,
such as diffusion maps and spectral clustering. We provide several synthetic
and real examples of applications in dimensionality reduction, shape analysis,
and clustering, demonstrating that our method better captures the inherent
structure of multi-modal data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6574</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6574</id><created>2013-07-24</created><authors><author><keyname>Chakraborty</keyname><forenames>Abhirup</forenames></author><author><keyname>Singh</keyname><forenames>Ajit</forenames></author></authors><title>Parallelizing Windowed Stream Joins in a Shared-Nothing Cluster</title><categories>cs.DC cs.DB</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of large number of processing nodes in a parallel and
distributed computing environment enables sophisticated real time processing
over high speed data streams, as required by many emerging applications.
Sliding window stream joins are among the most important operators in a stream
processing system. In this paper, we consider the issue of parallelizing a
sliding window stream join operator over a shared nothing cluster. We propose a
framework, based on fixed or predefined communication pattern, to distribute
the join processing loads over the shared-nothing cluster. We consider various
overheads while scaling over a large number of nodes, and propose solution
methodologies to cope with the issues. We implement the algorithm over a
cluster using a message passing system, and present the experimental results
showing the effectiveness of the join processing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6590</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6590</id><created>2013-07-24</created><authors><author><keyname>Calin</keyname><forenames>Georgel</forenames></author><author><keyname>Derevenetc</keyname><forenames>Egor</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Meyer</keyname><forenames>Roland</forenames></author></authors><title>A Theory of Partitioned Global Address Spaces</title><categories>cs.LO cs.DC</categories><msc-class>68Q60, 68Q45</msc-class><acm-class>D.2.4; D.1.3; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioned global address space (PGAS) is a parallel programming model for
the development of applications on clusters. It provides a global address space
partitioned among the cluster nodes, and is supported in programming languages
like C, C++, and Fortran by means of APIs. In this paper we provide a formal
model for the semantics of single instruction, multiple data programs using
PGAS APIs. Our model reflects the main features of popular real-world APIs such
as SHMEM, ARMCI, GASNet, GPI, and GASPI.
  A key feature of PGAS is the support for one-sided communication: a node may
directly read and write the memory located at a remote node, without explicit
synchronization with the processes running on the remote side. One-sided
communication increases performance by decoupling process synchronization from
data transfer, but requires the programmer to reason about appropriate
synchronizations between reads and writes. As a second contribution, we propose
and investigate robustness, a criterion for correct synchronization of PGAS
programs. Robustness corresponds to acyclicity of a suitable happens-before
relation defined on PGAS computations. The requirement is finer than the
classical data race freedom and rules out most false error reports.
  Our main result is an algorithm for checking robustness of PGAS programs. The
algorithm makes use of two insights. Using combinatorial arguments we first
show that, if a PGAS program is not robust, then there are computations in a
certain normal form that violate happens-before acyclicity. Intuitively,
normal-form computations delay remote accesses in an ordered way. We then
devise an algorithm that checks for cyclic normal-form computations.
Essentially, the algorithm is an emptiness check for a novel automaton model
that accepts normal-form computations in streaming fashion. Altogether, we
prove the robustness problem is PSpace-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6609</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6609</id><created>2013-07-24</created><authors><author><keyname>Ingber</keyname><forenames>Amir</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Compression for Quadratic Similarity Queries</title><categories>cs.IT math.IT</categories><comments>39 pages, 6 figures, submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of performing similarity queries on compressed data is
considered. We focus on the quadratic similarity measure, and study the
fundamental tradeoff between compression rate, sequence length, and reliability
of queries performed on compressed data. For a Gaussian source, we show that
queries can be answered reliably if and only if the compression rate exceeds a
given threshold - the identification rate - which we explicitly characterize.
Moreover, when compression is performed at a rate greater than the
identification rate, responses to queries on the compressed data can be made
exponentially reliable. We give a complete characterization of this exponent,
which is analogous to the error and excess-distortion exponents in channel and
source coding, respectively.
  For a general source we prove that, as with classical compression, the
Gaussian source requires the largest compression rate among sources with a
given variance. Moreover, a robust scheme is described that attains this
maximal rate for any source distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6616</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6616</id><created>2013-07-24</created><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Zeng</keyname><forenames>Jingshan</forenames></author><author><keyname>Fang</keyname><forenames>Jian</forenames></author></authors><title>Does generalization performance of $l^q$ regularization learning depend
  on $q$? A negative example</title><categories>cs.LG stat.ML</categories><comments>35 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  $l^q$-regularization has been demonstrated to be an attractive technique in
machine learning and statistical modeling. It attempts to improve the
generalization (prediction) capability of a machine (model) through
appropriately shrinking its coefficients. The shape of a $l^q$ estimator
differs in varying choices of the regularization order $q$. In particular,
$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth
ridge regression. This makes the order $q$ a potential tuning parameter in
applications. To facilitate the use of $l^{q}$-regularization, we intend to
seek for a modeling strategy where an elaborative selection on $q$ is
avoidable. In this spirit, we place our investigation within a general
framework of $l^{q}$-regularized kernel learning under a sample dependent
hypothesis space (SDHS). For a designated class of kernel functions, we show
that all $l^{q}$ estimators for $0&lt; q &lt; \infty$ attain similar generalization
error bounds. These estimated bounds are almost optimal in the sense that up to
a logarithmic factor, the upper and lower bounds are asymptotically identical.
This finding tentatively reveals that, in some modeling contexts, the choice of
$q$ might not have a strong impact in terms of the generalization capability.
From this perspective, $q$ can be arbitrarily specified, or specified merely by
other no generalization criteria like smoothness, computational complexity,
sparsity, etc..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6622</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6622</id><created>2013-07-24</created><authors><author><keyname>Liu</keyname><forenames>Xia</forenames></author><author><keyname>Fan</keyname><forenames>Li</forenames></author></authors><title>Priority-aware Gray-box Placement of Virtual Machines in Cloud Platforms</title><categories>cs.NI cs.DC</categories><comments>submitted to Advances in Computer Science and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual machine (VM) placement is very important for cloud platforms. While
techniques, such as live virtual machine migration, are very useful to balance
the load in the data centers, they are expensive operations. In this position
paper, we propose to minimize the chance of the load hot spots in the data
center by applying the workload patterns of the VMs in the virtual machine
placement algorithms - place VMs that require a lot of same type of resource
across different physical servers. In this way, the resource competition of VMs
on the same physical server is significantly mitigated. Meanwhile, we also
consider the priorities of applications and VMs in our virtual machine
placement algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6626</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6626</id><created>2013-07-24</created><authors><author><keyname>Chen</keyname><forenames>Zhixiong</forenames></author><author><keyname>Niu</keyname><forenames>Zhihua</forenames></author><author><keyname>Wu</keyname><forenames>Chenhuang</forenames></author></authors><title>On the $k$-error linear complexity of binary sequences derived from
  polynomial quotients</title><categories>cs.CR</categories><comments>2 figures</comments><msc-class>94A55, 94A60, 65C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the $k$-error linear complexity of $p^2$-periodic binary
sequences defined from the polynomial quotients (including the well-studied
Fermat quotients), which is defined by $$ q_{p,w}(u)\equiv \frac{u^w-u^{wp}}{p}
\bmod p ~ \mathrm{with} 0 \le q_{p,w}(u) \le p-1, ~u\ge 0, $$ where $p$ is an
odd prime and $1\le w&lt;p$. Indeed, first for all integers $k$, we determine
exact values of the $k$-error linear complexity over the finite field $\F_2$
for these binary sequences under the assumption of f2 being a primitive root
modulo $p^2$, and then we determine their $k$-error linear complexity over the
finite field $\F_p$ for either $0\le k&lt;p$ when $w=1$ or $0\le k&lt;p-1$ when $2\le
w&lt;p$. Theoretical results obtained indicate that such sequences possess `good'
error linear complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6627</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6627</id><created>2013-07-25</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Minimum d-dimensional arrangement with fixed points</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Minimum $d$-Dimensional Arrangement Problem (d-dimAP) we are given a
graph with edge weights, and the goal is to find a 1-1 map of the vertices into
$\mathbb{Z}^d$ (for some fixed dimension $d\geq 1$) minimizing the total
weighted stretch of the edges. This problem arises in VLSI placement and chip
design.
  Motivated by these applications, we consider a generalization of d-dimAP,
where the positions of some of the vertices (pins) is fixed and specified as
part of the input. We are asked to extend this partial map to a map of all the
vertices, again minimizing the weighted stretch of edges. This generalization,
which we refer to as d-dimAP+, arises naturally in these application domains
(since it can capture blocked-off parts of the board, or the requirement of
power-carrying pins to be in certain locations, etc.). Perhaps surprisingly,
very little is known about this problem from an approximation viewpoint.
  For dimension $d=2$, we obtain an $O(k^{1/2} \cdot \log n)$-approximation
algorithm, based on a strengthening of the spreading-metric LP for 2-dimAP. The
integrality gap for this LP is shown to be $\Omega(k^{1/4})$. We also show that
it is NP-hard to approximate 2-dimAP+ within a factor better than
$\Omega(k^{1/4-\eps})$. We also consider a (conceptually harder, but
practically even more interesting) variant of 2-dimAP+, where the target space
is the grid $\mathbb{Z}_{\sqrt{n}} \times \mathbb{Z}_{\sqrt{n}}$, instead of
the entire integer lattice $\mathbb{Z}^2$. For this problem, we obtain a $O(k
\cdot \log^2{n})$-approximation using the same LP relaxation. We complement
this upper bound by showing an integrality gap of $\Omega(k^{1/2})$, and an
$\Omega(k^{1/2-\eps})$-inapproximability result.
  Our results naturally extend to the case of arbitrary fixed target dimension
$d\geq 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6628</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6628</id><created>2013-07-25</created><authors><author><keyname>Shahbaz</keyname><forenames>Kaveh</forenames></author></authors><title>Applied Similarity Problems Using Frechet Distance</title><categories>cs.CG cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1003.0460 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this thesis, we consider an instance of Frechet distance
problem in which the speed of traversal along each segment of the curves is
restricted to be within a specfied range. This setting is more realistic than
the classical Frechet distance setting, specially in GIS applications. We also
study this problem in the setting where the polygonal curves are inside a
simple polygon.
  In the second part of this thesis, we present a data structure, called the
free-space map, that enables us to solve several variants of the Frechet
distance problem efficiently. Our data structure encapsulates all the
information available in the free-space diagram, yet it is capable of answering
more general type of queries efficiently. Given that the free-space map has the
same size and construction time as the standard free-space diagram, it can be
viewed as a powerful alternative to it. As part of the results in Part II of
the thesis, we exploit the free-space map to improve the long-standing bound
for computing the partial Frechet distance and obtain improved algorithms for
computing the Frechet distance between two closed curves, and the so-called
minimum/maximum walk problem. We also improve the map matching algorithm for
the case when the map is a directed acyclic graph.
  As the last part of this thesis, given a point set S and a polygonal curve P
in R^d, we study the problem of finding a polygonal curve Q through S, which
has a minimum Frechet distance to P. Furthermore, if the problem requires that
curve Q visits every point in S, we show it is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6638</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6638</id><created>2013-07-25</created><authors><author><keyname>Jhurani</keyname><forenames>Chetan</forenames></author><author><keyname>Austin</keyname><forenames>Travis M.</forenames></author><author><keyname>Heroux</keyname><forenames>Michael A.</forenames></author><author><keyname>Willenbring</keyname><forenames>James M.</forenames></author></authors><title>Supporting 64-bit global indices in Epetra and other Trilinos packages
  -- Techniques used and lessons learned</title><categories>cs.MS cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Trilinos Project is an effort to facilitate the design, development,
integration and ongoing support of mathematical software libraries within an
object-oriented framework. It is intended for large-scale, complex multiphysics
engineering and scientific applications. Epetra is one of its basic packages.
It provides serial and parallel linear algebra capabilities. Before Trilinos
version 11.0, released in 2012, Epetra used the C++ int data-type for storing
global and local indices for degrees of freedom (DOFs). Since int is typically
32-bit, this limited the largest problem size to be smaller than approximately
two billion DOFs. This was true even if a distributed memory machine could
handle larger problems. We have added optional support for C++ long long
data-type, which is at least 64-bit wide, for global indices. To save memory,
maintain the speed of memory-bound operations, and reduce further changes to
the code, the local indices are still 32-bit. We document the changes required
to achieve this feature and how the new functionality can be used. We also
report on the lessons learned in modifying a mature and popular package from
various perspectives -- design goals, backward compatibility, engineering
decisions, C++ language features, effects on existing users and other packages,
and build integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6649</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6649</id><created>2013-07-25</created><authors><author><keyname>Gunjan</keyname><forenames>Kumar</forenames></author><author><keyname>Tiwari</keyname><forenames>R. K.</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Towards Securing APIs in Cloud Computing</title><categories>cs.DC cs.CR</categories><comments>International Journal of Computer Engineering and Applications, June
  2013. arXiv admin note: text overlap with arXiv:0901.0131 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every organisation today wants to adopt cloud computing paradigm and leverage
its various advantages. Today everyone is aware of its characteristics which
have made it so popular and how it can help the organisations focus on their
core activities leaving all IT services development and maintenance to the
cloud service providers. Application Programming Interfaces (APIs) act as the
interface between the CSPs and the consumers. This paper proposes an improved
access control mechanism for securing the Cloud APIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6658</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6658</id><created>2013-07-25</created><authors><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>A Reputation Based Framework to Avoid Free-riding in Unstructured
  Peer-to-Peer network</title><categories>cs.NI</categories><doi>10.1007/s12083-015-0389-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Free riding is a major problem in peer-to-peer networks. Reputation
management systems are generally proposed to overcome this problem. In this
paper we have discussed a possible way of resource allocation on the basis of
reputation management system i.e. probabilistic allocation based on reputation.
This seems to be a better way for allocation of resources because in this case
nodes that do not have very good reputation about each other, may also serve
each other at least some amount of resource with finite probability. This
avoids disconnect between them. Algorithms are presented for optimizing the
shared capacity, reputation based probabilistic allocation that is optimal for
a node, and formation of interest groups on the basis of similarity between
interests of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6663</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6663</id><created>2013-07-25</created><updated>2013-08-22</updated><authors><author><keyname>Faria</keyname><forenames>Lu&#xe9;rbio</forenames></author><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Hsiang-Hsuan</forenames></author><author><keyname>Wang</keyname><forenames>Tao-Ming</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>On Complexities of Minus Domination</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function f: V \rightarrow \{-1,0,1\} is a minus-domination function of a
graph G=(V,E) if the values over the vertices in each closed neighborhood sum
to a positive number. The weight of f is the sum of f(x) over all vertices x
\in V. The minus-domination number \gamma^{-}(G) is the minimum weight over all
minus-domination functions. The size of a minus domination is the number of
vertices that are assigned 1. In this paper we show that the minus-domination
problem is fixed-parameter tractable for d-degenerate graphs when parameterized
by the size of the minus-dominating set and by d. The minus-domination problem
is polynomial for graphs of bounded rankwidth and for strongly chordal graphs.
It is NP-complete for splitgraphs. Unless P=NP there is no fixed-parameter
algorithm for minus-domination. 79,1 5%
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6665</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6665</id><created>2013-07-25</created><authors><author><keyname>Zhang</keyname><forenames>Hai</forenames></author></authors><title>Architecture of Network and Client-Server model</title><categories>cs.NI</categories><comments>submitted to journal Advances in Electrical Engineering Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of Internet technology, the Web is becoming more and
more important in our lives so that it has even become an essential element.
The application of the Web has never been limited to computers; it has been
opened to all kinds of intelligent digital devices like mobile ones. In this
paper, we discuss the main and substantial difference between UDP and TCP, and
how to implement Client-Server model. We also discuss the efficiency of
multi-thread server and its relationship with internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6673</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6673</id><created>2013-07-25</created><authors><author><keyname>Jakobsen</keyname><forenames>Sune K.</forenames></author></authors><title>Mutual information matrices are not always positive semi-definite</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For discrete random variables X_1,..., X_n we construct an n by n matrix. In
the (i,j) entry we put the mutual information I(X_i;X_j) between X_i and X_j.
In particular, in the (i,i) entry we put the entropy H(X_i)=I(X_i;X_i) of X_i.
This matrix, called the mutual information matrix of (X_1,...,X_n), has been
conjectured to be positive semi-definite. In this note, we give counterexamples
to the conjecture, and show that the conjecture holds for up to three random
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6679</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6679</id><created>2013-07-25</created><updated>2014-05-01</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Peng</keyname><forenames>Li</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Expurgated Random-Coding Ensembles: Exponents, Refinements and
  Connections</title><categories>cs.IT math.IT</categories><comments>(v1) Submitted to IEEE Transactions on Information Theory (v2)
  Extended version of the revision submitted to IEEE Transactions on
  Information Theory (v3) Extended version of final paper to appear in IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies expurgated random-coding bounds and exponents for channel
coding with a given (possibly suboptimal) decoding rule. Variations of
Gallager's analysis are presented, yielding several asymptotic and
non-asymptotic bounds on the error probability for an arbitrary codeword
distribution. A simple non-asymptotic bound is shown to attain an exponent of
Csisz\'ar and K\&quot;orner under constant-composition coding. Using Lagrange
duality, this exponent is expressed in several forms, one of which is shown to
permit a direct derivation via cost-constrained coding which extends to
infinite and continuous alphabets. The method of type class enumeration is
studied, and it is shown that this approach can yield improved exponents and
better tightness guarantees for some codeword distributions. A generalization
of this approach is shown to provide a multi-letter exponent which extends
immediately to channels with memory. Finally, a refined analysis expurgated
i.i.d. random coding is shown to yield a O\big(\frac{1}{\sqrt{n}}\big)
prefactor, thus improving on the standard O(1) prefactor. Moreover, the implied
constant is explicitly characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6702</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6702</id><created>2013-07-25</created><updated>2016-02-25</updated><authors><author><keyname>Martina</keyname><forenames>Valentina</forenames></author><author><keyname>Garetto</keyname><forenames>Michele</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author></authors><title>A unified approach to the performance analysis of caching systems</title><categories>cs.NI cs.PF</categories><comments>in ACM TOMPECS 20016. Preliminary version published at IEEE Infocom
  2014</comments><acm-class>C.2.1; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unified methodology to analyse the performance of caches (both
isolated and interconnected), by extending and generalizing a decoupling
technique originally known as Che's approximation, which provides very accurate
results at low computational cost. We consider several caching policies, taking
into account the effects of temporal locality. In the case of interconnected
caches, our approach allows us to do better than the Poisson approximation
commonly adopted in prior work. Our results, validated against simulations and
trace-driven experiments, provide interesting insights into the performance of
caching systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6711</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6711</id><created>2013-07-25</created><updated>2013-08-26</updated><authors><author><keyname>Upadhyay</keyname><forenames>Rahul R</forenames></author></authors><title>Study of Encryption and Decryption of Wave File in Image Formats</title><categories>cs.CR cs.MM</categories><comments>6 pages, 16 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel method of encrypting wave files in popular image
formats like JPEG, TIF and PNG along with retrieving them from these image
files. MATLAB software is used to perform matrix manipulation to encrypt and
decrypt sound files into and from image files. This method is not only a
stenographic means but also a data compression technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6716</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6716</id><created>2013-07-25</created><updated>2013-07-30</updated><authors><author><keyname>Soudjani</keyname><forenames>Sadegh Esmaeil Zadeh</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Aggregation and Control of Populations of Thermostatically Controlled
  Loads by Formal Abstractions</title><categories>cs.SY math.OC math.PR</categories><comments>40 pages, 21 figures; the paper generalizes the result of conference
  publication: S. Esmaeil Zadeh Soudjani and A. Abate, &quot;Aggregation of
  Thermostatically Controlled Loads by Formal Abstractions,&quot; Proceedings of the
  European Control Conference 2013, pp. 4232-4237. version 2: added references
  for section 1</comments><msc-class>60J05, 68Q60, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work discusses a two-step procedure, based on formal abstractions, to
generate a finite-space stochastic dynamical model as an aggregation of the
continuous temperature dynamics of a homogeneous population of Thermostatically
Controlled Loads (TCL). The temperature of a single TCL is described by a
stochastic difference equation and the TCL status (ON, OFF) by a deterministic
switching mechanism. The procedure is formal as it allows the exact
quantification of the error introduced by the abstraction -- as such it builds
and improves on a known, earlier approximation technique in the literature.
Further, the contribution discusses the extension to the case of a
heterogeneous population of TCL by means of two approaches resulting in the
notion of approximate abstractions. It moreover investigates the problem of
global (population-level) regulation and load balancing for the case of TCL
that are dependent on a control input. The procedure is tested on a case study
and benchmarked against the mentioned alternative approach in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6726</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6726</id><created>2013-07-25</created><authors><author><keyname>Piantadosi</keyname><forenames>Steven T.</forenames></author><author><keyname>Tily</keyname><forenames>Harry</forenames></author><author><keyname>Gibson</keyname><forenames>Edward</forenames></author></authors><title>Information content versus word length in natural language: A reply to
  Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751]</title><categories>cs.CL math.PR physics.data-an</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]
argued that an observed linear relationship between word length and average
surprisal (Piantadosi, Tily, &amp; Gibson, 2011) is not evidence for communicative
efficiency in human language. We discuss several shortcomings of their approach
and critique: their model critically rests on inaccurate assumptions, is
incapable of explaining key surprisal patterns in language, and is incompatible
with recent behavioral results. More generally, we argue that statistical
models must not critically rely on assumptions that are incompatible with the
real system under study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6738</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6738</id><created>2013-07-25</created><authors><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Efficient quantum protocols for XOR functions</title><categories>cs.CC</categories><comments>11 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any Boolean function f on {0,1}^n, the bounded-error quantum
communication complexity of XOR functions $f\circ \oplus$ satisfies that
$Q_\epsilon(f\circ \oplus) = O(2^d (\log\|\hat f\|_{1,\epsilon} + \log
\frac{n}{\epsilon}) \log(1/\epsilon))$, where d is the F2-degree of f, and
$\|\hat f\|_{1,\epsilon} = \min_{g:\|f-g\|_\infty \leq \epsilon} \|\hat f\|_1$.
This implies that the previous lower bound $Q_\epsilon(f\circ \oplus) =
\Omega(\log\|\hat f\|_{1,\epsilon})$ by Lee and Shraibman \cite{LS09} is tight
for f with low F2-degree. The result also confirms the quantum version of the
Log-rank Conjecture for low-degree XOR functions. In addition, we show that the
exact quantum communication complexity satisfies $Q_E(f) = O(2^d \log \|\hat
f\|_0)$, where $\|\hat f\|_0$ is the number of nonzero Fourier coefficients of
f. This matches the previous lower bound $Q_E(f(x,y)) = \Omega(\log rank(M_f))$
by Buhrman and de Wolf \cite{BdW01} for low-degree XOR functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6747</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6747</id><created>2013-07-25</created><authors><author><keyname>Kniesburges</keyname><forenames>Sebastian</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author></authors><title>CONE-DHT: A distributed self-stabilizing algorithm for a heterogeneous
  storage system</title><categories>cs.DC</categories><comments>Full version of DISC 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of managing a dynamic heterogeneous storage system in
a distributed way so that the amount of data assigned to a host in that system
is related to its capacity. Two central problems have to be solved for this:
(1) organizing the hosts in an overlay network with low degree and diameter so
that one can efficiently check the correct distribution of the data and route
between any two hosts, and (2) distributing the data among the hosts so that
the distribution respects the capacities of the hosts and can easily be adapted
as the set of hosts or their capacities change. We present distributed
protocols for these problems that are self-stabilizing and that do not need any
global knowledge about the system such as the number of nodes or the overall
capacity of the system. Prior to this work no solution was known satisfying
these properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6760</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6760</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Quantitative CV-based indicators for research quality, validated by peer
  review</title><categories>cs.DL</categories><comments>2 pages, 1 figure</comments><journal-ref>Proceedings of ISSI 2007, 11th International Conference of the
  International Society for Scientometrics and Informetrics, CSIC, Madrid,
  Spain, 25-27 June 2007, 930-931</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a university, research assessments are organized at different policy
levels (faculties, research council) in different contexts (funding, council
membership, personnel evaluations). Each evaluation requires its own focus and
methodology. To conduct a coherent research policy however, data on which
different assessments are based should be well coordinated. A common set of
core indicators for any type of research assessment can provide a supportive
and objectivating tool for evaluations at different institutional levels and at
the same time promote coherent decision-making. The same indicators can also
form the basis for a 'light touch' monitoring instrument, signalling when and
where a more thorough evaluation could be considered. This poster paper shows
how peer review results were used to validate a set of quantitative indicators
for research quality for a first series of disciplines. The indicators
correspond to categories in the university's standard CV-format. Per
discipline, specific indicators are identified corresponding to their own
publication and funding characteristics. Also more globally valid indicators
are identified after normalization for discipline-characteristic performance
levels. The method can be applied to any system where peer ratings and
quantitative performance measures, both reliable and sufficiently detailed, can
be combined for the same entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6765</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6765</id><created>2013-07-25</created><authors><author><keyname>Amez</keyname><forenames>Lucy</forenames></author><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Composing a Publication List for Individual Researcher Assessment by
  Merging Information from Different Sources</title><categories>cs.DL</categories><comments>3 pages</comments><journal-ref>Book of Abstracts, 10th International Conference on Science and
  Technology Indicators, Vienna, Austria, 17-20 September 2008, 435-437</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation and publication profiles are gaining importance for the evaluation
of top researchers when it comes to the appropriation of funding for excellence
programs or career promotion judgments. Indicators like the Normalized Mean
Citation Rate, the hindex or other distinguishing measures are increasingly
used to picture the characteristics of individual scholars. Using bibliometric
techniques for individual assessment is known to be particularly delicate, as
the chance of errors being averaged away becomes smaller whereas a minor
incompleteness can have a significant influence on the evaluation outcome. The
quality of the data becomes as such crucial to the legitimacy of the methods
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6769</identifier>
 <datestamp>2013-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6769</id><created>2013-07-25</created><updated>2013-11-20</updated><authors><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author><author><keyname>Boyd</keyname><forenames>Nicholas</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author><author><keyname>Wilson</keyname><forenames>Ashia C.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Streaming Variational Bayes</title><categories>stat.ML cs.LG</categories><comments>25 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,
(A)synchronous computation of a Bayesian posterior. The framework makes
streaming updates to the estimated posterior according to a user-specified
approximation batch primitive. We demonstrate the usefulness of our framework,
with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet
allocation model to two large-scale document collections. We demonstrate the
advantages of our algorithm over stochastic variational inference (SVI) by
comparing the two after a single pass through a known amount of data---a case
where SVI may be applied---and in the streaming setting, where SVI does not
apply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6770</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6770</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author><author><keyname>Amez</keyname><forenames>Lucy</forenames></author></authors><title>Impact Vitality - A Measure for Excellent Scientists</title><categories>cs.DL</categories><comments>3 pages</comments><journal-ref>Book of Abstracts, 10th International Conference on Science and
  Technology Indicators, Vienna, Austria, 17-20 September 2008, 211-213</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many countries and at European level, research policy increasingly focuses
on 'excellent' researchers. The concept of excellence however is complex and
multidimensional. For individual scholars it involves talents for innovative
knowledge creation and successful transmission to peers, as well as management
capacities. Excellence is also a comparative concept, implying the ability to
surpass others [TIJSSEN, 2003]. Grants are in general awarded based on
assessments by expert committees. While peer review is a widely accepted
practice, it nevertheless is also subject to criticism. At higher aggregation
levels, peer assessments are often supported by quantitative measures. At
individual level, most of these measures are much less appropriate and there is
a need for new, dedicated indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6773</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6773</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author><author><keyname>De Bruyn</keyname><forenames>Arlette</forenames></author></authors><title>Quality related publication categories in social sciences and
  humanities, based on a university's peer review assessments</title><categories>cs.DL</categories><comments>2 pages</comments><journal-ref>Book of Abstracts, 11th International Conference on Science and
  Technology Indicators &quot;Creating Value for Users&quot;, Leiden, The Netherlands,
  9-11 September 2010, 229-230</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometric analysis has firmly conquered its place as an instrument for
evaluation and international comparison of performance levels. Consequently,
differences in coverage by standard bibliometric databases installed a
dichotomy between on the one hand the well covered 'exact' sciences, and on the
other hand most of the social sciences and humanities with a more limited
coverage (Nederhof, 2006). Also the latter domains need to be able to soundly
demonstrate their level of performance and claim or legitimate funding
accordingly. An important part of the output volume in social sciences appears
as books, book chapters and national literature (Hicks, 2004). To proceed from
publication data to performance measurement, quantitative publication counts
need to be combined with qualitative information, for example from peer
assessment or validation (European Expert Group on Assessment of
University-Based Research, 2010), to identify those categories that represent
research quality as perceived by peers. An accurate focus is crucial in order
to stimulate, recognize and reward high quality achievements only. This paper
demonstrates how such a selection of publication categories can be based on
correlations with peer judgments. It is also illustrated that the selection
should be sufficiently precise, to avoid subcategories negatively correlated
with peer judgments. The findings indicate that, also in social sciences and
humanities, publications in journals with an international referee system are
the most important category for evaluating quality. Book chapters with
international referee system and contributions in international conference
proceedings follow them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6778</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6778</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Output and citation impact of interdisciplinary networks: Experiences
  from a dedicated funding program</title><categories>cs.DL</categories><comments>2 pages, 1 figure</comments><journal-ref>Book of Abstracts, 11th International Conference on Science and
  Technology Indicators &quot;Creating Value for Users&quot;, Leiden, The Netherlands,
  9-11 September 2010, 227-228</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a context of ever more specialized scientists, interdisciplinarity
receives increasing attention as innovating ideas are often situated where the
disciplines meet. In many countries science policy makers installed dedicated
funding programs and policies. This induces a need for specific tools for their
support. There is however not yet a generally accepted quantitative method or
set of criteria to recognize and evaluate interdisciplinary research outputs
(Tracking and evaluating interdisciplinary research: metrics and maps, 12th
ISSI Conference, 2009). Interdisciplinarity also takes on very different forms,
as distinguished in overviews from the first codifications (Klein, 1990) to the
latest reference work (Frodeman et al., 2010). In the specific context of
research measurement and evaluation, interdisciplinarity was discussed e.g. by
Rinia (2007) and Porter et al. (2006). This empirical study aims to contribute
to the understanding and the measuring of interdisciplinary research at the
micro level, in the form of new synergies between disciplines. Investigation of
a specialized funding program shows how a new interdisciplinary synergy and its
citation impact are visible in co-publications and cocitations, and that these
are important parameters for assessment. The results also demonstrate the
effect of funding, which is clearly present after about three years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6779</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6779</id><created>2013-07-25</created><updated>2013-07-28</updated><authors><author><keyname>Levi</keyname><forenames>Ilia</forenames></author><author><keyname>Vilenchik</keyname><forenames>Dan</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author></authors><title>Zero vs. epsilon Error in Interference Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional studies of multi-source, multi-terminal interference channels
typically allow a vanishing probability of error in communication. Motivated by
the study of network coding, this work addresses the task of quantifying the
loss in rate when insisting on zero error communication in the context of
interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6780</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6780</id><created>2013-07-25</created><updated>2015-08-12</updated><authors><author><keyname>Cozzo</keyname><forenames>Emanuele</forenames></author><author><keyname>Kivel&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Albert</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Structure of Triadic Relations in Multiplex Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Main text + Supplementary Material included in a single file.
  Published in New Journal of Physics</comments><journal-ref>New Journal of Physics, Vol. 17, No. 7: 073029, 2015</journal-ref><doi>10.1088/1367-2630/17/7/073029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in the study of networked systems have highlighted that our
interconnected world is composed of networks that are coupled to each other
through different &quot;layers&quot; that each represent one of many possible subsystems
or types of interactions. Nevertheless, it is traditional to aggregate
multilayer networks into a single weighted network in order to take advantage
of existing tools. This is admittedly convenient, but it is also extremely
problematic, as important information can be lost as a result. It is therefore
important to develop multilayer generalizations of network concepts. In this
paper, we analyze triadic relations and generalize the idea of transitivity to
multiplex networks. By focusing on triadic relations, which yield the simplest
type of transitivity, we generalize the concept and computation of clustering
coefficients to multiplex networks. We show how the layered structure of such
networks introduces a new degree of freedom that has a fundamental effect on
transitivity. We compute multiplex clustering coefficients for several real
multiplex networks and illustrate why one must take great care when
generalizing standard network concepts to multiplex networks. We also derive
analytical expressions for our clustering coefficients for ensemble averages of
networks in a family of random multiplex networks. Our analysis illustrates
that social networks have a strong tendency to promote redundancy by closing
triads at every layer and that they thereby have a different type of multiplex
transitivity from transportation networks, which do not exhibit such a
tendency. These insights are invisible if one only studies aggregated networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6784</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6784</id><created>2013-07-25</created><updated>2013-08-12</updated><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Interdisciplinary Research Collaborations: Evaluation of a Funding
  Program</title><categories>cs.DL</categories><comments>14 pages, 4 figures; Article in Collnet Journal of Scientometrics and
  Information Management based on the paper presented at the Sixth
  International Conference on Webometrics, Informetrics and Scientometrics &amp;
  Eleventh COLLNET Meeting held in Mysore, India, 19-22 October 2010</comments><journal-ref>Collnet Journal of Scientometrics and Information Management,
  5(1), 17-32, 2011</journal-ref><doi>10.1080/09737766.2011.10700900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Innovative ideas are often situated where disciplines meet, and
socio-economic problems generally require contributions from several
disciplines. Ways to stimulate interdisciplinary research collaborations are
therefore an increasing point of attention for science policy. There is concern
that 'regular' funding programs, involving advice from disciplinary experts and
discipline-bound viewpoints, may not adequately stimulate, select or evaluate
this kind of research. This has led to specific policies aimed at
interdisciplinary research in many countries. There is however at this moment
no generally accepted method to adequately select and evaluate
interdisciplinary research. In the vast context of different forms of
interdisciplinarity, this paper aims to contribute to the debate on best
practices to stimulate and support interdisciplinary research collaborations.
It describes the selection procedures and results of a university program
supporting networks formed 'bottom up', integrating expertise from different
disciplines. The program's recent evaluation indicates that it is successful in
selecting and supporting the interdisciplinary synergies aimed for, responding
to a need experienced in the field. The analysis further confirms that
potential for interdisciplinary collaboration is present in all disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6786</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6786</id><created>2013-07-25</created><updated>2014-03-27</updated><authors><author><keyname>Altarelli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Lage-Castellanos</keyname><forenames>Alejandro</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Bayesian inference of epidemics on networks via Belief Propagation</title><categories>q-bio.QM cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. Lett. 112, 118701 (2014)</journal-ref><doi>10.1103/PhysRevLett.112.118701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study several bayesian inference problems for irreversible stochastic
epidemic models on networks from a statistical physics viewpoint. We derive
equations which allow to accurately compute the posterior distribution of the
time evolution of the state of each node given some observations. At difference
with most existing methods, we allow very general observation models, including
unobserved nodes, state observations made at different or unknown times, and
observations of infection times, possibly mixed together. Our method, which is
based on the Belief Propagation algorithm, is efficient, naturally distributed,
and exact on trees. As a particular case, we consider the problem of finding
the &quot;zero patient&quot; of a SIR or SI epidemic given a snapshot of the state of the
network at a later unknown time. Numerical simulations show that our method
outperforms previous ones on both synthetic and real networks, often by a very
large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6789</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6789</id><created>2013-07-25</created><updated>2013-07-31</updated><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author></authors><title>Optimal Top-k Document Retrieval</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{D}$ be a collection of $D$ documents, which are strings over an
alphabet of size $\sigma$, of total length $n$. We describe a data structure
that uses linear space and and reports $k$ most relevant documents that contain
a query pattern $P$, which is a string of length $p$, in time $O(p/\log_\sigma
n+k)$, which is optimal in the RAM model in the general case where $\lg D =
\Theta(\log n)$, and involves a novel RAM-optimal suffix tree search. Our
construction supports an ample set of important relevance measures... [clip]
  When $\lg D = o(\log n)$, we show how to reduce the space of the data
structure from $O(n\log n)$ to $O(n(\log\sigma+\log D+\log\log n))$ bits...
[clip]
  We also consider the dynamic scenario, where documents can be inserted and
deleted from the collection. We obtain linear space and query time
$O(p(\log\log n)^2/\log_\sigma n+\log n + k\log\log k)$, whereas insertions and
deletions require $O(\log^{1+\epsilon} n)$ time per symbol, for any constant
$\epsilon&gt;0$.
  Finally, we consider an extended static scenario where an extra parameter
$par(P,d)$ is defined, and the query must retrieve only documents $d$ such that
$par(P,d)\in [\tau_1,\tau_2]$, where this range is specified at query time. We
solve these queries using linear space and $O(p/\log_\sigma n +
\log^{1+\epsilon} n + k\log^\epsilon n)$ time, for any constant $\epsilon&gt;0$.
  Our technique is to translate these top-$k$ problems into multidimensional
geometric search problems. As an additional bonus, we describe some
improvements to those problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6791</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6791</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Research Excellence Milestones of BRIC and N-11 Countries</title><categories>cs.DL</categories><comments>3 pages, 1 figure</comments><journal-ref>Proceedings of ISSI 2011, 13th Conference of the International
  Society for Scientometrics and Informetrics, Durban, South Africa, 04-07 July
  2011. Ed Noyons, Patrick Ngulube and Jacqueline Leta (Eds.), Vol. 2,
  1049-1051</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While scientific performance is an important aspect of a stable and healthy
economy, measures for it have yet to gain their place in economic country
profiles. As useful indicators for this performance dimension, this paper
introduces the concept of milestones for research excellence, as points of
transition to higher-level contributions at the leading edge of science. The
proposed milestones are based on two indicators associated with research
excellence, the impact vitality profile and the production of review type
publications, both applied to a country's publications in the top journals
Nature and Science. The milestones are determined for two distinct groups of
emerging market economies: the BRIC countries, which outperformed the relative
growth expected at their identification in 2001, and the N-11 or Next Eleven
countries, identified in 2005 as potential candidates for a BRIC-like
evolution. Results show how these two groups at different economic levels can
be clearly distinguished based on the research milestones, indicating a
potential utility as parameters in an economic context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6792</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6792</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Characteristics of International versus Non-International Scientific
  Publication Media in Team- and Author-Based Data</title><categories>cs.DL</categories><comments>2 pages</comments><journal-ref>Proceedings of STI 2012 Montr\'eal, 17th International Conference
  on Science and Technology Indicators, Montr\'eal, Qu\'ebec, Canada, 05-08
  September 2012, Eric Archambault, Yves Gingras and Vincent Larivi\`ere
  (Eds.), Vol. 2, 888-889</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enlarged coverage of the international publication and citation databases
Web of Science and Scopus towards local media in social sciences was a welcome
response to an increased usage of these databases in evaluation and funding
systems. The mostly international journals available earlier were the basis for
the development of current standard bibliometric indicators. The same
indicators may no longer measure exactly the same concepts when applied to
newly introduced or extended media categories, with possibly different
characteristics than those of international journals. This paper investigates
differences between media with and without international dimension in
publication data at team and author level. The findings relate the
international publication categories to research quality, important for
validation of their usage in evaluation or funding models that aim to stimulate
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6797</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6797</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Groups of Highly Cited Publications: Stability in Content with Citation
  Window Length</title><categories>cs.DL</categories><comments>2 pages, 1 figure</comments><journal-ref>Proceedings of ISSI 2013, 14th International Society of
  Scientometrics and Informetrics Conference, Vienna, Austria, 15-19 July 2013,
  Vol. 2, 1998-2000</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing focus in research policy worldwide on top scientists makes it
increasingly important to define adequate supporting measures to help identify
excellent scientists. Highly cited publications have since long been associated
to research excellence. At the same time, the analysis of the high-end of
citation distributions still is a challenging topic in evaluative
bibliometrics. Evaluations typically require indicators that generate
sufficiently stable results when applied to recent publication records of
limited size. Highly cited publications have been identified using two
techniques in particular: pre-set percentiles, and the parameter free
Characteristic Scores and Scales (CSS) (Gl\&quot;anzel &amp; Schubert, 1988). The
stability required in assessments of relatively small publication records,
concerns size as well as content of groups of highly cited publications.
Influencing factors include domain delineation and citation window length.
Stability in size is evident for the pre-set percentiles, and has been
demonstrated for the CSS-methodology beyond an initial citation period of about
three years (Gl\&quot;anzel, 2007). Stability in content is less straightforward,
considering for instance that more highly cited publications can have a later
citation peak, as observed by Abt (1981) for astronomical papers. This paper
investigates the stability in content of groups of highly cited publications,
i.e. the extent to which individual publications enter and leave the group as
the citation window is enlarged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6804</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6804</id><created>2013-07-25</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Partition-based Field Normalization: An approach to highly specialized
  publication records</title><categories>cs.DL</categories><comments>14 pages, 1 figure</comments><journal-ref>Journal of Informetrics, 6(1), 1-10, 2012</journal-ref><doi>10.1016/j.joi.2011.09.008 10.1016/j.joi.2012.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Field normalized citation rates are well-established indicators for research
performance from the broadest aggregation levels such as countries, down to
institutes and research teams. When applied to still more specialized
publication sets at the level of individual scientists, also a more accurate
delimitation is required of the reference domain that provides the expectations
to which a performance is compared. This necessity for sharper accuracy
challenges standard methodology based on predefined subject categories. This
paper proposes a way to define a reference domain that is more strongly
delimited than in standard methodology, by building it up out of cells of the
partition created by the pre-defined subject categories and their
intersections. This partition approach can be applied to different existing
field normalization variants. The resulting reference domain lies between those
generated by standard field normalization and journal normalization. Examples
based on fictive and real publication records illustrate how the potential
impact on results can exceed or be smaller than the effect of other currently
debated normalization variants, depending on the case studied. The proposed
Partition-based Field Normalization is expected to offer advantages in
particular at the level of individual scientists and other very specific
publication records, such as publication output from interdisciplinary
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6809</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6809</id><created>2013-07-25</created><updated>2016-02-28</updated><authors><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author></authors><title>A strongly polynomial algorithm for generalized flow maximization</title><categories>cs.DS cs.DM math.OC</categories><comments>minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strongly polynomial algorithm is given for the generalized flow
maximization problem. It uses a new variant of the scaling technique, called
continuous scaling. The main measure of progress is that within a strongly
polynomial number of steps, an arc can be identified that must be tight in
every dual optimal solution, and thus can be contracted. As a consequence of
the result, we also obtain a strongly polynomial algorithm for the linear
feasibility problem with at most two nonzero entries per column in the
constraint matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6814</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6814</id><created>2013-07-25</created><authors><author><keyname>Bhatia</keyname><forenames>Shveta Kundra</forenames></author><author><keyname>Dixit</keyname><forenames>V. S.</forenames></author></authors><title>A Propound Method for the Improvement of Cluster Quality</title><categories>cs.LG</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper Knockout Refinement Algorithm (KRA) is proposed to refine
original clusters obtained by applying SOM and K-Means clustering algorithms.
KRA Algorithm is based on Contingency Table concepts. Metrics are computed for
the Original and Refined Clusters. Quality of Original and Refined Clusters are
compared in terms of metrics. The proposed algorithm (KRA) is tested in the
educational domain and results show that it generates better quality clusters
in terms of improved metric values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6843</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6843</id><created>2013-07-25</created><updated>2016-01-19</updated><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author></authors><title>Optimal Quantization for Distribution Synthesis</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite precision approximations of discrete probability distributions are
considered, applicable for distribution synthesis, e.g., probabilistic shaping.
Two algorithms are presented that find the optimal $M$-type approximation $Q$
of a distribution $P$ in terms of the variational distance $| Q-P|_1$ and the
informational divergence $\mathbb{D}(Q| P)$. Bounds on the approximation errors
are derived and shown to be asymptotically tight. Several examples illustrate
that the variational distance optimal approximation can be quite different from
the informational divergence optimal approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6864</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6864</id><created>2013-07-25</created><authors><author><keyname>Demanet</keyname><forenames>Laurent</forenames></author><author><keyname>Jugnon</keyname><forenames>Vincent</forenames></author></authors><title>Convex recovery from interferometric measurements</title><categories>math.NA cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note formulates a deterministic recovery result for vectors $x$ from
quadratic measurements of the form $(Ax)_i \conj{(Ax)_j}$ for some
left-invertible $A$. Recovery is exact, or stable in the noisy case, when the
couples $(i,j)$ are chosen as edges of a well-connected graph. One possible way
of obtaining the solution is as a feasible point of a simple semidefinite
program. Furthermore, we show how the proportionality constant in the error
estimate depends on the spectral gap of a data-weighted graph Laplacian. Such
quadratic measurements have found applications in phase retrieval, angular
synchronization, and more recently interferometric waveform inversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6883</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6883</id><created>2013-07-25</created><updated>2013-08-04</updated><authors><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames><affiliation>ADAI-LAETA Department of Mechanical Engineering University of Coimbra</affiliation></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio Rodrigues</forenames><affiliation>ADAI-LAETA Department of Mechanical Engineering University of Coimbra</affiliation></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames><affiliation>INESCC Department of Electrical and Computer Engineering, University of Coimbra</affiliation></author></authors><title>A gradient descent technique coupled with a dynamic simulation to
  determine the near optimum orientation of floor plan designs</title><categories>cs.CE cs.AI</categories><comments>10 pages, 6 figures, conference paper; Proceedings of CLIMA 2013
  16-19 June 2013 Prague Czech Republic (2013)</comments><msc-class>68T20</msc-class><acm-class>G.1.6; J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prototype tool to assist architects during the early design stage of floor
plans has been developed, consisting of an Evolutionary Program for the Space
Allocation Problem (EPSAP), which generates sets of floor plan alternatives
according to the architect's preferences; and a Floor Plan Performance
Optimization Program (FPOP), which optimizes the selected solutions according
to thermal performance criteria. The design variables subject to optimization
are window position and size, overhangs, fins, wall positioning, and building
orientation. A procedure using a transformation operator with gradient descent,
such as behavior, coupled with a dynamic simulation engine was developed for
the thermal evaluation and optimization process. However, the need to evaluate
all possible alternatives regarding designing variables being used during the
optimization process leads to an intensive use of thermal simulation, which
dramatically increases the simulation time, rendering it unpractical. An
alternative approach is a smart optimization approach, which utilizes an
oriented and adaptive search technique to efficiently find the near optimum
solution. This paper presents the search methodology for the building
orientation of floor plan designs, and the corresponding efficiency and
effectiveness indicators. The calculations are based on 100 floor plan designs
generated by EPSAP. All floor plans have the same design program, location, and
weather data, changing only their geometry. Dynamic simulation of buildings was
effectively used together with the optimization procedure in this approach to
significantly improve the designs. The use of the orientation variable has been
included in the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6887</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6887</id><created>2013-07-25</created><authors><author><keyname>Azar</keyname><forenames>Mohammad Gheshlaghi</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author></authors><title>Sequential Transfer in Multi-armed Bandit with Finite Set of Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning from prior tasks and transferring that experience to improve future
performance is critical for building lifelong learning agents. Although results
in supervised and reinforcement learning show that transfer may significantly
improve the learning performance, most of the literature on transfer is focused
on batch learning tasks. In this paper we study the problem of
\textit{sequential transfer in online learning}, notably in the multi-armed
bandit framework, where the objective is to minimize the cumulative regret over
a sequence of tasks by incrementally transferring knowledge from prior tasks.
We introduce a novel bandit algorithm based on a method-of-moments approach for
the estimation of the possible tasks and derive regret bounds for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6889</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6889</id><created>2013-07-25</created><authors><author><keyname>Magliocca</keyname><forenames>N. R.</forenames><affiliation>Department of Geography and Environmental Systems, University of Maryland, Baltimore County, Baltimore, Maryland, USA</affiliation></author><author><keyname>Ellis</keyname><forenames>E. C.</forenames><affiliation>Department of Geography and Environmental Systems, University of Maryland, Baltimore County, Baltimore, Maryland, USA</affiliation></author><author><keyname>Oates</keyname><forenames>T.</forenames><affiliation>Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, Maryland, USA</affiliation></author><author><keyname>Schmill</keyname><forenames>M.</forenames><affiliation>Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, Maryland, USA</affiliation></author></authors><title>Contextualizing the global relevance of local land change observations</title><categories>stat.AP cs.CY physics.ao-ph</categories><comments>5 pages, 4 figures, white paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand global changes in the Earth system, scientists must generalize
globally from observations made locally and regionally. In land change science
(LCS), local field-based observations are costly and time consuming, and
generally obtained by researchers working at disparate local and regional
case-study sites chosen for different reasons. As a result, global synthesis
efforts in LCS tend to be based on non-statistical inferences subject to
geographic biases stemming from data limitations and fragmentation. Thus, a
fundamental challenge is the production of generalized knowledge that links
evidence of the causes and consequences of local land change to global patterns
and vice versa. The GLOBE system was designed to meet this challenge. GLOBE
aims to transform global change science by enabling new scientific workflows
based on statistically robust, globally relevant integration of local and
regional observations using an online social-computational and geovisualization
system. Consistent with the goals of Digital Earth, GLOBE has the capability to
assess the global relevance of local case-study findings within the context of
over 50 global biophysical, land-use, climate, and socio-economic datasets. We
demonstrate the implementation of one such assessment - a representativeness
analysis - with a recently published meta-study of changes in swidden
agriculture in tropical forests. The analysis provides a standardized indicator
to judge the global representativeness of the trends reported in the
meta-study, and a geovisualization is presented that highlights areas for which
sampling efforts can be reduced and those in need of further study. GLOBE will
enable researchers and institutions to rapidly share, compare, and synthesize
local and regional studies within the global context, as well as contributing
to the larger goal of creating a Digital Earth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6894</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6894</id><created>2013-07-25</created><authors><author><keyname>Rupel</keyname><forenames>Dylan</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author></authors><title>The operad of temporal wiring diagrams: formalizing a graphical language
  for discrete-time processes</title><categories>math.CT cs.PL q-bio.NC</categories><msc-class>08A70, 18B20, 18D50, 68Q05, 91B74, 92B20, 93A13</msc-class><acm-class>B.5.2; B.7.2; C.0; C.1; D.2.2; D.2.6; D.3.3; F.1.1; F.4.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We investigate the hierarchical structure of processes using the mathematical
theory of operads. Information or material enters a given process as a stream
of inputs, and the process converts it to a stream of outputs. Output streams
can then be supplied to other processes in an organized manner, and the
resulting system of interconnected processes can itself be considered a macro
process. To model the inherent structure in this kind of system, we define an
operad $\mathcal{W}$ of black boxes and directed wiring diagrams, and we define
a $\mathcal{W}$-algebra $\mathcal{P}$ of processes (which we call propagators,
after Radul and Sussman). Previous operadic models of wiring diagrams use
undirected wires without length, useful for modeling static systems of
constraints, whereas we use directed wires with length, useful for modeling
dynamic flows of information. We give multiple examples throughout to ground
the ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6901</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6901</id><created>2013-07-25</created><authors><author><keyname>Attie</keyname><forenames>Paul C.</forenames></author><author><keyname>Zaraket</keyname><forenames>Fadi A.</forenames></author><author><keyname>Noureddine</keyname><forenames>Mohamad</forenames></author><author><keyname>El-Hariri</keyname><forenames>Farah</forenames></author></authors><title>Specification Construction Using Behaviours, Equivalences, and SMT
  Solvers</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to write and check a specification including quantifiers
using behaviors, i.e., input-output pairs. Our method requires the following
input from the user: (1) answers to a finite number of queries, each of which
presents some behavior to the user, who responds informing whether the behavior
is &quot;correct&quot; or not; and (2) an &quot;equivalence&quot; theory (set of formulae), which
represents the users opinion about which pairs of behaviors are equivalent with
respect to the specification; and (3) a &quot;vocabulary&quot;, i.e., a set of formulae
which provide the basic building blocks for the specification to be written.
Alternatively, the user can specify a type theory and a simple relational
grammar, and our method can generate the vocabulary and equivalence theories.
Our method automatically generates behaviors using a satisfiability modulo
theory solver. Since writing a specification consists of formalizing ideas that
are initially informal, there must, by definition, be at least one &quot;initial&quot;
step where an informal notion is formalized by the user in an ad hoc manner.
This step is the provision of the equivalence theory and vocabulary; we call it
the primitive formalization step. We contend that it is considerably easier to
write an equivalence theory and vocabulary than to write a full-blown formal
specification from scratch, and we provide experimental evidence for this
claim. We also show how vocabularies can be constructed hierarchically, with a
specification at one level providing vocabulary material for the next level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6903</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6903</id><created>2013-07-25</created><authors><author><keyname>Borgstr&#xf6;m</keyname><forenames>Johannes</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Luttik</keyname><forenames>Bas</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Proceedings Combined 20th International Workshop on Expressiveness in
  Concurrency and 10th Workshop on Structural Operational Semantics</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><acm-class>F3.2; F4.1; D3.1</acm-class><journal-ref>EPTCS 120, 2013</journal-ref><doi>10.4204/EPTCS.120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Combined 20th International
Workshop on Expressiveness in Concurrency and the 10th Workshop on Structural
Operational Semantics (EXPRESS/SOS 2013) which was held on 26th August, 2013 in
Buenos Aires, Argentina, as an affiliated workshop of CONCUR 2013, the 24th
International Conference on Concurrency Theory.
  The EXPRESS workshops aim at bringing together researchers interested in the
expressiveness of various formal systems and semantic notions, particularly in
the field of concurrency. Their focus has traditionally been on the comparison
between programming concepts (such as concurrent, functional, imperative, logic
and object-oriented programming) and between mathematical models of computation
(such as process algebras, Petri nets, event structures, modal logics, and
rewrite systems) on the basis of their relative expressive power. The EXPRESS
workshop series has run successfully since 1994 and over the years this focus
has become broadly construed.
  The SOS workshops aim at being a forum for researchers, students and
practitioners interested in new developments, and directions for future
investigation, in the field of structural operational semantics. One of the
specific goals of the SOS workshop series is to establish synergies between the
concurrency and programming language communities working on the theory and
practice of SOS. Reports on applications of SOS to other fields are also most
welcome, including: modelling and analysis of biological systems, security of
computer systems programming, modelling and analysis of embedded systems,
specification of middle-ware and coordination languages, programming language
semantics and implementation, static analysis software and hardware
verification, and semantics for domain-specific languages and model-based
engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6921</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6921</id><created>2013-07-26</created><authors><author><keyname>Pershin</keyname><forenames>Y. V.</forenames></author><author><keyname>Di Ventra</keyname><forenames>M.</forenames></author></authors><title>Memcapacitive neural networks</title><categories>cond-mat.dis-nn cs.ET cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that memcapacitive (memory capacitive) systems can be used as
synapses in artificial neural networks. As an example of our approach, we
discuss the architecture of an integrate-and-fire neural network based on
memcapacitive synapses. Moreover, we demonstrate that the
spike-timing-dependent plasticity can be simply realized with some of these
devices. Memcapacitive synapses are a low-energy alternative to memristive
synapses for neuromorphic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6923</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6923</id><created>2013-07-26</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>Wark</keyname><forenames>Tim</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author></authors><title>A Deterministic Construction of Projection matrix for Adaptive
  Trajectory Compression</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing, which offers exact reconstruction of sparse signal from
a small number of measurements, has tremendous potential for trajectory
compression. In order to optimize the compression, trajectory compression
algorithms need to adapt compression ratio subject to the compressibility of
the trajectory. Intuitively, the trajectory of an object moving in starlight
road is more compressible compared to the trajectory of a object moving in
winding roads, therefore, higher compression is achievable in the former case
compared to the later. We propose an in-situ compression technique underpinning
the support vector regression theory, which accurately predicts the
compressibility of a trajectory given the mean speed of the object and then
apply compressive sensing to adapt the compression to the compressibility of
the trajectory. The conventional encoding and decoding process of compressive
sensing uses predefined dictionary and measurement (or projection) matrix
pairs. However, the selection of an optimal pair is nontrivial and exhaustive,
and random selection of a pair does not guarantee the best compression
performance. In this paper, we propose a deterministic and data driven
construction for the projection matrix which is obtained by applying singular
value decomposition to a sparsifying dictionary learned from the dataset. We
analyze case studies of pedestrian and animal trajectory datasets including GPS
trajectory data from 127 subjects. The experimental results suggest that the
proposed adaptive compression algorithm, incorporating the deterministic
construction of projection matrix, offers significantly better compression
performance compared to the state-of-the-art alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6927</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6927</id><created>2013-07-26</created><updated>2013-08-01</updated><authors><author><keyname>Hooshmand</keyname><forenames>Reza</forenames></author><author><keyname>Shooshtari</keyname><forenames>Masoumeh Koochak</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Secret Key Cryptosystem based on Polar Codes over Binary Erasure Channel</title><categories>cs.CR cs.IT math.IT</categories><comments>This paper was submitted to ISCISC 2013 on 19 May 2013 and accepted
  on 25 July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient secret key cryptosystem based on polar codes
over Binary Erasure Channel. We introduce a method, for the first time to our
knowledge, to hide the generator matrix of the polar codes from an attacker. In
fact, our main goal is to achieve secure and reliable communication using
finite-length polar codes. The proposed cryptosystem has a significant security
advantage against chosen plaintext attacks in comparison with the Rao-Nam
cryptosystem. Also, the key length is decreased after applying a new
compression algorithm. Moreover, this scheme benefits from high code rate and
proper error performance for reliable communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6930</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6930</id><created>2013-07-26</created><authors><author><keyname>Lone</keyname><forenames>Faisal Rasheed</forenames></author><author><keyname>Puri</keyname><forenames>Arjun</forenames></author><author><keyname>Kumar</keyname><forenames>Sudesh</forenames></author></authors><title>Performance Comparison of Reed Solomon Code and BCH Code over Rayleigh
  Fading Channel</title><categories>cs.IT math.IT</categories><comments>4 pages,3 figures,1 table. &quot;Published with International Journal of
  Computer Applications (IJCA)&quot;</comments><journal-ref>International Journal of Computer Applications 71(20):23-26, June
  2013</journal-ref><doi>10.5120/12603-9397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data transmission over a communication channel is prone to a number of
factors that can render the data unreliable or inconsistent by introducing
noise, crosstalk or various other disturbances. A mechanism has to be in place
that detects these anomalies in the received data and corrects it to get the
data back as it was meant to be sent by the sender. Over the years a number of
error detection and correction methodologies have been devised to send and
receive the data in a consistent and correct form. The best of these
methodologies ensure that the data is received correctly by the receiver in
minimum number of retransmissions. In this paper performance of Reed Solomon
Code (RS) and BCH Code is compared over Rayleigh fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6937</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6937</id><created>2013-07-26</created><authors><author><keyname>Mudgal</keyname><forenames>Renu</forenames></author><author><keyname>Madaan</keyname><forenames>Rosy</forenames></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author><author><keyname>Dixit</keyname><forenames>Ashutosh</forenames></author></authors><title>A Novel Architecture For Question Classification Based Indexing Scheme
  For Efficient Question Answering</title><categories>cs.IR cs.CL</categories><comments>International Journal of Computer Engineering and Applications,
  April-June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Question answering system can be seen as the next step in information
retrieval, allowing users to pose question in natural language and receive
compact answers. For the Question answering system to be successful, research
has shown that the correct classification of question with respect to the
expected answer type is requisite. We propose a novel architecture for question
classification and searching in the index, maintained on the basis of expected
answer types, for efficient question answering. The system uses the criteria
for Answer Relevance Score for finding the relevance of each answer returned by
the system. On analysis of the proposed system, it has been found that the
system has shown promising results than the existing systems based on question
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6939</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6939</id><created>2013-07-26</created><authors><author><keyname>Csaba</keyname><forenames>B&#xe9;la</forenames></author><author><keyname>Plick</keyname><forenames>Thomas A.</forenames></author><author><keyname>Shokoufandeh</keyname><forenames>Ali</forenames></author></authors><title>Optimal Random Matchings, Tours, and Spanning Trees in Hierarchically
  Separated Trees</title><categories>cs.DM math.CO</categories><comments>24 pages, to appear in TCS</comments><doi>10.1016/j.tcs.2013.05.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive tight bounds on the expected weights of several combinatorial
optimization problems for random point sets of size $n$ distributed among the
leaves of a balanced hierarchically separated tree. We consider {\it
monochromatic} and {\it bichromatic} versions of the minimum matching, minimum
spanning tree, and traveling salesman problems. We also present tight
concentration results for the monochromatic problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6941</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6941</id><created>2013-07-26</created><authors><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Google Scholar Metrics 2013: nothing new under the sun</title><categories>cs.DL</categories><comments>10 pages, in Spanish, 8 figures</comments><report-no>EC3 12</report-no><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Main characteristics of Google Scholar Metrics new version (july 2013) are
presented. We outline the novelties and the weaknesses detected after a first
analysis. As main conclusion, we remark the lack of new functionalities with
respect to last editions, as the only modification is the update of the
timeframe (2008-2012). Hence, problems pointed out in our last reviews still
remain active. Finally, it seems Google Scholar Metrics will be updated in a
yearly basis
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6948</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6948</id><created>2013-07-26</created><updated>2013-09-18</updated><authors><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Spin glass approach to the feedback vertex set problem</title><categories>cs.CC cond-mat.dis-nn</categories><comments>9 pages, including 4 figures. Title slightly changed. Under
  consideration in EPJB</comments><journal-ref>European Physical Journal B 86: 455 (2013)</journal-ref><doi>10.1140/epjb/e2013-40690-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A feedback vertex set (FVS) of an undirected graph is a set of vertices that
contains at least one vertex of each cycle of the graph. The feedback vertex
set problem consists of constructing a FVS of size less than a certain given
value. This combinatorial optimization problem has many practical applications,
but it is in the nondeterministic polynomial-complete class of worst-case
computational complexity. In this paper we define a spin glass model for the
FVS problem and then study this model on the ensemble of finite-connectivity
random graphs. In our model the global cycle constraints are represented
through the local constraints on all the edges of the graph, and they are then
treated by distributed message-passing procedures such as belief propagation.
Our belief propagation-guided decimation algorithm can construct nearly optimal
feedback vertex sets for single random graph instances and regular lattices. We
also design a spin glass model for the FVS problem on a directed graph. Our
work will be very useful for identifying the set of vertices that contribute
most significantly to the dynamical complexity of a large networked system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6958</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6958</id><created>2013-07-26</created><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Derrick</keyname><forenames>John</forenames></author></authors><title>Simplifying proofs of linearisability using layers of abstraction</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearisability has become the standard correctness criterion for concurrent
data structures, ensuring that every history of invocations and responses of
concurrent operations has a matching sequential history. Existing proofs of
linearisability require one to identify so-called linearisation points within
the operations under consideration, which are atomic statements whose execution
causes the effect of an operation to be felt. However, identification of
linearisation points is a non-trivial task, requiring a high degree of
expertise. For sophisticated algorithms such as Heller et al's lazy set, it
even is possible for an operation to be linearised by the concurrent execution
of a statement outside the operation being verified. This paper proposes an
alternative method for verifying linearisability that does not require
identification of linearisation points. Instead, using an interval-based logic,
we show that every behaviour of each concrete operation over any interval is a
possible behaviour of a corresponding abstraction that executes with
coarse-grained atomicity. This approach is applied to Heller et al's lazy set
to show that verification of linearisability is possible without having to
consider linearisation points within the program code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6962</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6962</id><created>2013-07-26</created><updated>2013-12-01</updated><authors><author><keyname>Bastanlar</keyname><forenames>Yalin</forenames></author></authors><title>Reduced egomotion estimation drift using omnidirectional views</title><categories>cs.CV cs.RO</categories><comments>Another publisher does not want this article to be shared at
  arxiv.org in order to publish it</comments><journal-ref>Electronic Letters on Computer Vision and Image Analysis,
  vol.13(3), p.1-12, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of camera motion from a given image sequence becomes degraded as
the length of the sequence increases. In this letter, this phenomenon is
demonstrated and an approach to increase the estimation accuracy is proposed.
The proposed method uses an omnidirectional camera in addition to the
perspective one and takes advantage of its enlarged view by exploiting the
correspondences between the omnidirectional and perspective images. Simulated
and real image experiments show that the proposed approach improves the
estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6975</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6975</id><created>2013-07-26</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Francis</keyname><forenames>Leena Mary</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>LogMin: A Model For Call Log Mining In Mobile Devices</title><categories>cs.CY</categories><comments>12 Pages, 5 Figures</comments><report-no>1839-7662</report-no><msc-class>68U35</msc-class><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 3, No.4, July 2013</journal-ref><doi>10.5121/ijfcst.2013.3405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's instant communication era, mobile phones play an important role in
the efficient communication with respect to both individual and official
communication strata. With the drastic explosion in the quantity of calls
received and made, there is a need for analyses of patterns in these call logs
to assist the user of the mobile device in the optimal utilization. This paper
proposes a model termed &quot;LogMin&quot; (Log Mining of Calls in Mobile devices) which
is aimed towards mining of call log in mobile phones to discover patterns and
keep the user informed about the trends in the log. The logging of calls would
facilitate the user to get an insight into patterns based on the six different
parameters identified by the proposed LogMin model. The proposed model is
validated with a prototype implementation in the Android platform and various
experiments were conducted on it. The results of the experiments in the LogMin
Android implementation validate the efficiency of the proposed model with
respect to user's relevancy metric which is computed as 96.52%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6976</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6976</id><created>2013-07-26</created><authors><author><keyname>Azizi</keyname><forenames>Reza</forenames></author></authors><title>Performance study and simulation of an anycast protocol for wireless
  mobile ad hoc networks</title><categories>cs.NI cs.PF</categories><comments>15 pages, 20 figures, 1 table</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 3, June 2013</journal-ref><doi>10.5121/ijwmn.2013.5302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper conducts a detailed simulation study of stateless anycast routing
in a mobile wireless ad hoc network. The model covers all the fundamental
aspects of such networks with a routing mechanism using a scheme of
orientation-dependent inter-node communication links. The simulation system
Winsim is used which explicitly represents parallelism of events and processes
in the network. The purpose of these simulations is to investigate the effect
of node s maximum speed, and different TTL over the network performance under
two different scenarios. Simulation study investigates five practically
important performance metrics of a wireless mobile ad hoc network and shows the
dependence of this metrics on the transmission radius, link availability, and
maximal possible node speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6979</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6979</id><created>2013-07-26</created><authors><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author></authors><title>Towards a Better Understanding of Multi-User Cooperation: A Tradeoff
  between Transmission Reliability and Rate</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Multi-Media Review Letters, vol.3(4), pp. 20-22, Nov 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a review of recent advances in multi-user cooperative
data transmission. The focus is on the inherent trade-off between achievable
throughput and reliability of cooperative transmission. Research has shown that
under a fixed transmit energy budget, increased cooperation doesn't necessarily
lead to increased reliability. In fact, careful cooperation partner selection
and power allocation is needed in order to fully exploit the benefits of
cooperative transmission. Furthermore, depending on the multi-media content,
different cooperation strategies may need to be considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6980</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6980</id><created>2013-07-26</created><authors><author><keyname>Anisetti</keyname><forenames>Marco</forenames></author><author><keyname>Ardagna</keyname><forenames>Claudio A.</forenames></author><author><keyname>Bezzi</keyname><forenames>Michele</forenames></author><author><keyname>Damiani</keyname><forenames>Ernesto</forenames></author><author><keyname>Sabetta</keyname><forenames>Antonino</forenames></author></authors><title>Machine-Readable Privacy Certificates for Services</title><categories>cs.CR</categories><comments>20 pages, 6 figures</comments><doi>10.1007/978-3-642-41030-7_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy-aware processing of personal data on the web of services requires
managing a number of issues arising both from the technical and the legal
domain. Several approaches have been proposed to matching privacy requirements
(on the clients side) and privacy guarantees (on the service provider side).
Still, the assurance of effective data protection (when possible) relies on
substantial human effort and exposes organizations to significant
(non-)compliance risks. In this paper we put forward the idea that a privacy
certification scheme producing and managing machine-readable artifacts in the
form of privacy certificates can play an important role towards the solution of
this problem. Digital privacy certificates represent the reasons why a privacy
property holds for a service and describe the privacy measures supporting it.
Also, privacy certificates can be used to automatically select services whose
certificates match the client policies (privacy requirements).
  Our proposal relies on an evolution of the conceptual model developed in the
Assert4Soa project and on a certificate format specifically tailored to
represent privacy properties. To validate our approach, we present a worked-out
instance showing how privacy property Retention-based unlinkability can be
certified for a banking financial service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6982</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6982</id><created>2013-07-26</created><updated>2014-03-13</updated><authors><author><keyname>Stankovi&#x107;</keyname><forenames>Milo&#x161; S.</forenames></author><author><keyname>Stankovi&#x107;</keyname><forenames>Sr\djan S.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Distributed Blind Calibration via Output Synchronization in Lossy Sensor
  Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel distributed algorithm for blind macro calibration in
sensor networks based on output synchronization is proposed. The algorithm is
formulated as a set of gradient-type recursions for estimating parameters of
sensor calibration functions, starting from local criteria defined as weighted
sums of mean square differences between the outputs of neighboring sensors. It
is proved, on the basis of an originally developed methodology for treating
higher-order consensus (or output synchronization) schemes, that the algorithm
achieves asymptotic agreement for sensor gains and offsets, in the mean square
sense and with probability one. In the case of additive measurement noise,
additive inter-agent communication noise, and communication outages, a
modification of the original algorithm based on instrumental variables is
proposed. It is proved using stochastic approximation arguments that the
modified algorithm achieves asymptotic consensus for sensor gains and offsets,
in the mean square sense and with probability one. Special attention is paid to
the situation when a subset of sensors in the network remains with fixed
characteristics. Illustrative simulation examples are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.6995</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.6995</id><created>2013-07-26</created><updated>2013-08-02</updated><authors><author><keyname>Bereza</keyname><forenames>Andrey</forenames></author><author><keyname>Lyashov</keyname><forenames>Maksim</forenames></author><author><keyname>Blanco</keyname><forenames>Luis</forenames></author></authors><title>Finite State Machine Synthesis for Evolutionary Hardware</title><categories>cs.NE cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers application of genetic algorithms for finite machine
synthesis. The resulting genetic finite state machines synthesis algorithm
allows for creation of machines with less number of states and within shorter
time. This makes it possible to use hardware-oriented genetic finite machines
synthesis algorithm in autonomous systems on reconfigurable platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7000</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7000</id><created>2013-07-26</created><authors><author><keyname>Roos</keyname><forenames>Stefanie</forenames></author><author><keyname>Salah</keyname><forenames>Hani</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>Comprehending Kademlia Routing - A Theoretical Framework for the Hop
  Count Distribution</title><categories>cs.NI</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The family of Kademlia-type systems represents the most efficient and most
widely deployed class of internet-scale distributed systems. Its success has
caused plenty of large scale measurements and simulation studies, and several
improvements have been introduced. Its character of parallel and
non-deterministic lookups, however, so far has prevented any concise formal
analysis. This paper introduces the first comprehensive formal model of the
routing of the entire family of systems that is validated against previous
measurements. It sheds light on the overall hop distribution and lookup delays
of the different variations of the original protocol. It additionally shows
that several of the recent improvements to the protocol in fact have been
counter-productive and identifies preferable designs with regard to routing
overhead and resilience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7009</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7009</id><created>2013-07-26</created><authors><author><keyname>Jafri</keyname><forenames>M. R.</forenames></author><author><keyname>Ahmed</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Ahmad</keyname><forenames>Z.</forenames></author><author><keyname>Qureshi</keyname><forenames>R. J.</forenames></author></authors><title>AMCTD: Adaptive Mobility of Courier nodes in Threshold-optimized DBR
  Protocol for Underwater Wireless Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>8th International Conference on Broadband and Wireless Computing,
  Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dense underwater sensor networks (UWSN), the major confronts are high
error probability, incessant variation in topology of sensor nodes, and much
energy consumption for data transmission. However, there are some remarkable
applications of UWSN such as management of seabed and oil reservoirs,
exploration of deep sea situation and prevention of aqueous disasters. In order
to accomplish these applications, ignorance of the limitations of acoustic
communications such as high delay and low bandwidth is not feasible. In this
paper, we propose Adaptive mobility of Courier nodes in Threshold-optimized
Depth-based routing (AMCTD), exploring the proficient amendments in depth
threshold and implementing the optimal weight function to achieve longer
network lifetime. We segregate our scheme in 3 major phases of weight updating,
depth threshold variation and adaptive mobility of courier nodes. During data
forwarding, we provide the framework for alterations in threshold to cope with
the sparse condition of network. We ultimately perform detailed simulations to
scrutinize the performance of our proposed scheme and its comparison with other
two notable routing protocols in term of network lifetime and other essential
parameters. The simulations results verify that our scheme performs better than
the other techniques and near to optimal in the field of UWSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7024</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7024</id><created>2013-07-26</created><authors><author><keyname>Sun</keyname><forenames>Shiliang</forenames></author></authors><title>Multi-view Laplacian Support Vector Machines</title><categories>cs.LG stat.ML</categories><comments>Lecture Notes in Computer Science, 2011, 7121: 209-222</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach, multi-view Laplacian support vector machines
(SVMs), for semi-supervised learning under the multi-view scenario. It
integrates manifold regularization and multi-view regularization into the usual
formulation of SVMs and is a natural extension of SVMs from supervised learning
to multi-view semi-supervised learning. The function optimization problem in a
reproducing kernel Hilbert space is converted to an optimization in a
finite-dimensional Euclidean space. After providing a theoretical bound for the
generalization performance of the proposed method, we further give a
formulation of the empirical Rademacher complexity which affects the bound
significantly. From this bound and the empirical Rademacher complexity, we can
gain insights into the roles played by different regularization terms to the
generalization performance. Experimental results on synthetic and real-world
data sets are presented, which validate the effectiveness of the proposed
multi-view Laplacian SVMs approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7028</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7028</id><created>2013-07-26</created><authors><author><keyname>Sun</keyname><forenames>Shiliang</forenames></author></authors><title>Infinite Mixtures of Multivariate Gaussian Processes</title><categories>cs.LG stat.ML</categories><comments>Proceedings of the International Conference on Machine Learning and
  Cybernetics, 2013, pages 1011-1016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new model called infinite mixtures of multivariate
Gaussian processes, which can be used to learn vector-valued functions and
applied to multitask learning. As an extension of the single multivariate
Gaussian process, the mixture model has the advantages of modeling multimodal
data and alleviating the computationally cubic complexity of the multivariate
Gaussian process. A Dirichlet process prior is adopted to allow the (possibly
infinite) number of mixture components to be automatically inferred from
training data, and Markov chain Monte Carlo sampling techniques are used for
parameter and latent variable inference. Preliminary experimental results on
multivariate regression show the feasibility of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7031</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7031</id><created>2013-07-26</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author><author><keyname>Spruyt</keyname><forenames>Eric</forenames></author></authors><title>Reliability and Comparability of Peer Review Results</title><categories>cs.DL</categories><comments>14 pages, 3 figures; Conference papers, New Frontiers in Evaluation,
  Vienna, Austria, 24-25 April 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper peer review reliability is investigated based on peer ratings
of research teams at two Belgian universities. It is found that outcomes can be
substantially influenced by the different ways in which experts attribute
ratings. To increase reliability of peer ratings, procedures creating a uniform
reference level should be envisaged. One should at least check for signs of low
reliability, which can be obtained from an analysis of the outcomes of the peer
evaluation itself. The peer review results are compared to outcomes from a
citation analysis of publications by the same teams, in subject fields well
covered by citation indexes. It is illustrated how, besides reliability,
comparability of results depends on the nature of the indicators, on the
subject area and on the intrinsic characteristics of the methods. The results
further confirm what is currently considered as good practice: the presentation
of results for not one but for a series of indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7033</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7033</id><created>2013-07-26</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author><author><keyname>De Bruyn</keyname><forenames>Arlette</forenames></author><author><keyname>Cornelis</keyname><forenames>Jan</forenames></author></authors><title>Research evaluation per discipline: a peer-review method and its
  outcomes</title><categories>cs.DL</categories><comments>21 pages, 4 figures</comments><journal-ref>Research Evaluation, 17(1), 45-57, 2008</journal-ref><doi>10.3152/095820208X240208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the method for ex-post peer review evaluation per
research discipline used at the Vrije Universiteit Brussel (VUB) and summarizes
the outcomes obtained from it. The method produces pertinent advice and
triggers responses - at the level of the individual researcher, the research
team and the university's research management - for the benefit of research
quality, competitivity and visibility. Imposed reflection and contacts during
and after the evaluation procedure modify the individual researcher's attitude,
improve the research teams' strategies and allow for the extraction of general
recommendations that are used as discipline-dependent guidelines in the
university's research management. The deep insights gained in the different
research disciplines and the substantial data sets on their research, support
the university management in its policy decisions and in building policy
instruments. Moreover, the results are used as a basis for comparison with
other assessments, leading to a better understanding of the possibilities and
limitations of different evaluation processes. The peer review method can be
applied systematically in a pluri-annual cycle of research discipline
evaluations to build up a complete overview, or it can be activated on an ad
hoc basis for a particular discipline, based on demands from research teams or
on strategic or policy arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7035</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7035</id><created>2013-07-26</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author><author><keyname>Amez</keyname><forenames>Lucy</forenames></author></authors><title>Impact vitality: an indicator based on citing publications in search of
  excellent scientists</title><categories>cs.DL</categories><comments>12 pages</comments><journal-ref>Research Evaluation, 18(3), 233-241, 2009</journal-ref><doi>10.3152/095820209X470563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes to the quest for an operational definition of
'research excellence' and proposes a translation of the excellence concept into
a bibliometric indicator. Starting from a textual analysis of funding program
calls aimed at individual researchers and from the challenges for an indicator
at this level in particular, a new type of indicator is proposed. The Impact
Vitality indicator [RONS &amp; AMEZ, 2008] reflects the vitality of the impact of a
researcher's publication output, based on the change in volume over time of the
citing publications. The introduced metric is shown to posses attractive
operational characteristics and meets a number of criteria which are desirable
when comparing individual researchers. The validity of one of the possible
indicator variants is tested using a small dataset of applicants for a senior
full time Research Fellowship. Options for further research involve testing
various indicator variants on larger samples linked to different kinds of
evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7037</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7037</id><created>2013-07-26</created><authors><author><keyname>Lu&#x10d;anin</keyname><forenames>Dra&#x17e;en</forenames></author><author><keyname>Brandi&#x107;</keyname><forenames>Ivona</forenames></author></authors><title>Take a break: cloud scheduling optimized for real-time electricity
  pricing</title><categories>cs.DC</categories><comments>8 pages, to appear at CGC 2013
  (http://socialcloud.aifb.uni-karlsruhe.de/confs/CGC2013/)</comments><doi>10.1109/CGC.2013.25</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud computing revolutionised the industry with its elastic, on-demand
approach to computational resources, but has lead to a tremendous impact on the
environment. Data centers constitute 1.1-1.5% of total electricity usage in the
world. Taking a more informed view of the electrical grid by analysing
real-time electricity prices, we set the foundations of a grid-conscious cloud.
We propose a scheduling algorithm that predicts electricity price peaks and
throttles energy consumption by pausing virtual machines. We evaluate the
approach on the OpenStack cloud manager through an empirical approach and show
reductions in energy consumption and costs. Finally, we define green instances
in which cloud providers can offer such services to their customers under
better pricing options.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7042</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7042</id><created>2013-07-26</created><authors><author><keyname>Kapanowski</keyname><forenames>Andrzej</forenames></author></authors><title>Python for education: permutations</title><categories>cs.MS math.HO</categories><comments>26 pages, 1 figure, 2 tables</comments><journal-ref>The Python Papers 9, 3 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Python implementation of permutations is presented. Three classes are
introduced: Perm for permutations, Group for permutation groups, and PermError
to report any errors for both classes. The class Perm is based on Python
dictionaries and utilize cycle notation. The methods of calculation for the
perm order, parity, ranking and unranking are given. A random permutation
generation is also shown. The class Group is very simple and it is also based
on dictionaries. It is mainly the presentation of the permutation groups
interface with methods for the group order, subgroups (normalizer, centralizer,
center, stabilizer), orbits, and several tests. The corresponding Python code
is contained in the modules perms and groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7048</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7048</id><created>2013-07-26</created><updated>2014-12-29</updated><authors><author><keyname>Duncan</keyname><forenames>Ross</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames><affiliation>CNRS</affiliation></author></authors><title>Pivoting makes the ZX-calculus complete for real stabilizers</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 171, 2014, pp. 50-62</journal-ref><doi>10.4204/EPTCS.171.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that pivoting property of graph states cannot be derived from the
axioms of the ZX-calculus, and that pivoting does not imply local
complementation of graph states. Therefore the ZX-calculus augmented with
pivoting is strictly weaker than the calculus augmented with the Euler
decomposition of the Hadamard gate. We derive an angle-free version of the
ZX-calculus and show that it is complete for real stabilizer quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7050</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7050</id><created>2013-07-26</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Hasan</keyname><forenames>Atif N</forenames></author></authors><title>A Comprehensive Evaluation of Machine Learning Techniques for Cancer
  Class Prediction Based on Microarray Data</title><categories>cs.LG cs.CE</categories><comments>8 pages, 3 figures and 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prostate cancer is among the most common cancer in males and its
heterogeneity is well known. Its early detection helps making therapeutic
decision. There is no standard technique or procedure yet which is full-proof
in predicting cancer class. The genomic level changes can be detected in gene
expression data and those changes may serve as standard model for any random
cancer data for class prediction. Various techniques were implied on prostate
cancer data set in order to accurately predict cancer class including machine
learning techniques. Huge number of attributes and few number of sample in
microarray data leads to poor machine learning, therefore the most challenging
part is attribute reduction or non significant gene reduction. In this work we
have compared several machine learning techniques for their accuracy in
predicting the cancer class. Machine learning is effective when number of
attributes (genes) are larger than the number of samples which is rarely
possible with gene expression data. Attribute reduction or gene filtering is
absolutely required in order to make the data more meaningful as most of the
genes do not participate in tumor development and are irrelevant for cancer
prediction. Here we have applied combination of statistical techniques such as
inter-quartile range and t-test, which has been effective in filtering
significant genes and minimizing noise from data. Further we have done a
comprehensive evaluation of ten state-of-the-art machine learning techniques
for their accuracy in class prediction of prostate cancer. Out of these
techniques, Bayes Network out performed with an accuracy of 94.11% followed by
Navie Bayes with an accuracy of 91.17%. To cross validate our results, we
modified our training dataset in six different way and found that average
sensitivity, specificity, precision and accuracy of Bayes Network is highest
among all other techniques used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7052</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7052</id><created>2013-07-26</created><authors><author><keyname>Haider</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Amjad</keyname><forenames>N.</forenames></author><author><keyname>Awan</keyname><forenames>A. A.</forenames></author><author><keyname>Khan</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>N.</forenames></author></authors><title>REECH-ME: Regional Energy Efficient Cluster Heads based on Maximum
  Energy Routing Protocol for WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Regional Energy Efficient Cluster Heads based on
Maximum Energy (REECH-ME) Routing Protocol for Wireless Sensor Networks (WSNs)
. The main purpose of this protocol is to improve the network lifetime and
particularly the stability period of the network. In REECH-ME, the node with
the maximum energy in a region becomes Cluster Head (CH) of that region for
that particular round and the number of the cluster heads in each round remains
the same. Our technique outperforms LEACH which uses probabilistic approach for
the selection of CHs. We also implement the Uniform Random Distribution Model
to find the packet drop to make this protocol more practical. We also calculate
the confidence interval of all our results which helps us to visualize the
possible deviation of our graphs from the mean value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7057</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7057</id><created>2013-07-26</created><authors><author><keyname>Khan</keyname><forenames>I.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Ullah</keyname><forenames>M. N.</forenames></author><author><keyname>Mahmood</keyname><forenames>A.</forenames></author><author><keyname>Farooq</keyname><forenames>M. U.</forenames></author></authors><title>A Survey of Home Energy Management Systems in Future Smart Grid
  Communications</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a systematic review of various home energy
management (HEM) schemes. Employment of home energy management programs will
make the electricity consumption smarter and more efficient. Advantages of HEM
include, increased savings for consumers as well as utilities, reduced peak to
average ratio (PAR) and peak demand. Where there are numerous applications of
smart grid technologies, home energy management is probably the most important
one to be addressed. Utilities across the globe have taken various steps for
efficient consumption of electricity. New pricing schemes like, Real Time
Pricing (RTP), Time of Use (ToU), Inclining Block Rates (IBR), Critical Peak
Pricing (CPP) etc, have been proposed for smart grid. Distributed Energy
Resources (DER) (local generation) and/or home appliances coordination along
with different tariff schemes lead towards efficient consumption of
electricity. This work also discusses a HEM systems general architecture and
various challenges in implementation of this architecture in smart grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7058</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7058</id><created>2013-07-25</created><authors><author><keyname>Zhuang</keyname><forenames>Guanxi</forenames></author><author><keyname>Zhang</keyname><forenames>Hai</forenames></author><author><keyname>Liu</keyname><forenames>Xia</forenames></author></authors><title>The Economic Trend of Video Game Industry</title><categories>cs.CY</categories><comments>submitted to journal Advances in Digital Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the game industry has had a huge growth. We've seen new game
consoles, great looking games and an increase in the number of people playing
them. We are presently in the seventh generation of video games which focuses
on consoles released since 2004. For home consoles,the seventh generation began
on November 22, 2005 with the release of Xbox 360 and continued with the
release of PlayStation 3 on November 11, 2006, and Wii on November 19, 2006.
The current generation is having a console battle between Nintendo's Wii,
Microsoft's Xbox 360, and Sony's PlayStation 3.The appearance of the three new
consoles not only offers various purchase choices, but also greatly affects
economy and culture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7059</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7059</id><created>2013-07-26</created><authors><author><keyname>Mahmood</keyname><forenames>D.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Mahmood</keyname><forenames>S.</forenames></author><author><keyname>Qureshi</keyname><forenames>S.</forenames></author><author><keyname>Memon</keyname><forenames>A. M.</forenames></author><author><keyname>Zaman</keyname><forenames>T.</forenames></author></authors><title>MODLEACH: A Variant of LEACH for WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are appearing as an emerging need for mankind.
Though, Such networks are still in research phase however, they have high
potential to be applied in almost every field of life. Lots of research is done
and a lot more is awaiting to be standardized. In this work, cluster based
routing in wireless sensor networks is studied precisely. Further, we modify
one of the most prominent wireless sensor network's routing protocol &quot;LEACH&quot; as
modified LEACH (MODLEACH) by introducing \emph{efficient cluster head
replacement scheme} and \emph{dual transmitting power levels}. Our modified
LEACH, in comparison with LEACH out performs it using metrics of cluster head
formation, through put and network life. Afterwards, hard and soft thresholds
are implemented on modified LEACH (MODLEACH) that boast the performance even
more. Finally a brief performance analysis of LEACH, Modified LEACH (MODLEACH),
MODLEACH with hard threshold (MODLEACHHT) and MODLEACH with soft threshold
(MODLEACHST) is undertaken considering metrics of throughput, network life and
cluster head replacements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7066</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7066</id><created>2013-07-26</created><updated>2014-11-27</updated><authors><author><keyname>Valmari</keyname><forenames>Antti</forenames></author></authors><title>Asymptotic Proportion of Hard Instances of the Halting Problem</title><categories>cs.LO cs.CC cs.FL</categories><comments>18 pages. The differences between this version and arXiv:1307.7066v1
  are significant. They have been listed in the last paragraph of Section 1.
  Excluding layout, this arXiv version is essentially identical to the Acta
  Cybernetica version</comments><msc-class>68Q17</msc-class><acm-class>F.1.1</acm-class><journal-ref>Acta Cybernetica 21 (2014) 307--330</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the halting problem is undecidable, imperfect testers that fail on
some instances are possible. Such instances are called hard for the tester. One
variant of imperfect testers replies &quot;I don't know&quot; on hard instances, another
variant fails to halt, and yet another replies incorrectly &quot;yes&quot; or &quot;no&quot;. Also
the halting problem has three variants: does a given program halt on the empty
input, does a given program halt when given itself as its input, or does a
given program halt on a given input. The failure rate of a tester for some size
is the proportion of hard instances among all instances of that size. This
publication investigates the behaviour of the failure rate as the size grows
without limit. Earlier results are surveyed and new results are proven. Some of
them use C++ on Linux as the computational model. It turns out that the
behaviour is sensitive to the details of the programming language or
computational model, but in many cases it is possible to prove that the
proportion of hard instances does not vanish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7068</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7068</id><created>2013-07-26</created><authors><author><keyname>Tauqir</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Akram</keyname><forenames>S.</forenames></author><author><keyname>Rao</keyname><forenames>A.</forenames></author><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author></authors><title>Distance Aware Relaying Energy-efficient: DARE to Monitor Patients in
  Multi-hop Body Area Sensor Networks</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, interests in the applications of Wireless Body Area Sensor
Network (WBASN) is noticeably developed. WBASN is playing a significant role to
get the real time and precise data with reduced level of energy consumption. It
comprises of tiny, lightweight and energy restricted sensors, placed in/on the
human body, to monitor any ambiguity in body organs and measure various
biomedical parameters. In this study, a protocol named Distance Aware Relaying
Energy-efficient (DARE) to monitor patients in multi-hop Body Area Sensor
Networks (BASNs) is proposed. The protocol operates by investigating the ward
of a hospital comprising of eight patients, under different topologies by
positioning the sink at different locations or making it static or mobile.
Seven sensors are attached to each patient, measuring different parameters of
Electrocardiogram (ECG), pulse rate, heart rate, temperature level, glucose
level, toxins level and motion. To reduce the energy consumption, these sensors
communicate with the sink via an on-body relay, affixed on the chest of each
patient. The body relay possesses higher energy resources as compared to the
body sensors as, they perform aggregation and relaying of data to the sink
node. A comparison is also conducted conducted with another protocol of BAN
named, Mobility-supporting Adaptive Threshold-based Thermal-aware
Energy-efficient Multi-hop ProTocol (M-ATTEMPT). The simulation results show
that, the proposed protocol achieves increased network lifetime and efficiently
reduces the energy consumption, in relative to M-ATTEMPT protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7075</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7075</id><created>2013-07-26</created><authors><author><keyname>Amjad</keyname><forenames>N.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Haider</keyname><forenames>A.</forenames></author><author><keyname>Awan</keyname><forenames>A. A.</forenames></author><author><keyname>Rahman</keyname><forenames>M.</forenames></author></authors><title>DREEM-ME: Distributed Regional Energy Efficient Multi-hop Routing
  Protocol based on Maximum Energy in WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless distributed sensor network consists of randomly deployed sensors
having low energy assets. These networks can be used for monitoring a variety
of environments. Major problems of these networks are energy constraints and
their finite lifetimes. To overcome these problems different routing protocols
and clustering techniques are introduced. We propose DREEM-ME which uses a
unique technique for clustering to overcome these two problems efficiently.
DREEM-ME elects a fix number of cluster heads (CHs) in each round instead of
probabilistic selection of CHs. Packet Drop Technique is also implemented in
our protocol to make it more comprehensive and practical. In DREEM-ME
confidence interval is also shown in each graph which helps in visualising the
maximum deviation from original course. Our simulations and results show that
DREEM-ME is much better than existing protocols of the same nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7087</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7087</id><created>2013-07-26</created><authors><author><keyname>Gabrys</keyname><forenames>Ryan</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Dolecek</keyname><forenames>Lara</forenames></author></authors><title>Correcting Grain-Errors in Magnetic Media</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies new bounds and constructions that are applicable to the
combinatorial granular channel model previously introduced by Sharov and Roth.
We derive new bounds on the maximum cardinality of a grain-error-correcting
code and propose constructions of codes that correct grain-errors. We
demonstrate that a permutation of the classical group codes (e.g.,
Constantin-Rao codes) can correct a single grain-error. In many cases of
interest, our results improve upon the currently best known bounds and
constructions. Some of the approaches adopted in the context of grain-errors
may have application to other channel models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7089</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7089</id><created>2013-07-26</created><authors><author><keyname>Tong</keyname><forenames>Weitian</forenames></author><author><keyname>Chen</keyname><forenames>Zhi-Zhong</forenames></author><author><keyname>Wang</keyname><forenames>Lusheng</forenames></author><author><keyname>Xu</keyname><forenames>Yinfeng</forenames></author><author><keyname>Xu</keyname><forenames>Jiuping</forenames></author><author><keyname>Goebel</keyname><forenames>Randy</forenames></author><author><keyname>Lin</keyname><forenames>Guohui</forenames></author></authors><title>An approximation algorithm for the Bandpass-2 problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general Bandpass-$B$ problem is NP-hard and can be approximated by a
reduction into the weighted $B$-set packing problem, with a worst case
performance ratio of $O(B^2)$. When $B = 2$, a maximum weight matching gives a
2-approximation to the problem. In this paper, we call the Bandpass-2 problem
simply the Bandpass problem. The Bandpass problem can be viewed as a variation
of the maximum traveling salesman problem, in which the edge weights are
dynamic rather than given at the front. We present a ${426}{227}$-approximation
algorithm for the problem. Such an improved approximation is built on an
intrinsic structural property proven for the optimal solution and several novel
schemes to partition a $b$-matching into desired matchings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7096</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7096</id><created>2013-07-26</created><authors><author><keyname>Abbasipour</keyname><forenames>Mahin</forenames></author></authors><title>Toward Recovering Complete SRS for Softbody Simulation System and a
  Sample Application -- a Team 9a SOEN6481 W13 Project Report</title><categories>cs.SE</categories><comments>51 pages, 8 figures, 8 tables</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document aims at specifying the requirements and capturing the needs of
users for building a softbody simulation system. This system has different
applications ranging from computer games to surgery training which facilitates
the creation and visualization of a certain softbody object. It also allows
users to interact with created object at real time. A softbody or deformable
object is an object whose shape changes due to an external force. Deformation
type varies depending on the amount of object deformation. Each object can have
multiple layers and each layer can have its own properties. So layers can be
different in pressure, density and motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7102</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7102</id><created>2013-07-26</created><authors><author><keyname>Nadeem</keyname><forenames>Q.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author><author><keyname>Khan</keyname><forenames>M. Y.</forenames></author><author><keyname>Sarfraz</keyname><forenames>S.</forenames></author><author><keyname>Gull</keyname><forenames>M.</forenames></author></authors><title>SIMPLE: Stable Increased-throughput Multi-hop Protocol for Link
  Efficiency in Wireless Body Area Networks</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a reliable, power efficient and high throughput
routing protocol for Wireless Body Area Networks (WBANs). We use multi-hop
topology to achieve minimum energy consumption and longer network lifetime. We
propose a cost function to select parent node or forwarder. Proposed cost
function selects a parent node which has high residual energy and minimum
distance to sink. Residual energy parameter balances the energy consumption
among the sensor nodes while distance parameter ensures successful packet
delivery to sink. Simulation results show that our proposed protocol maximize
the network stability period and nodes stay alive for longer period. Longer
stability period contributes high packet delivery to sink which is major
interest for continuous patient monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7105</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7105</id><created>2013-07-26</created><authors><author><keyname>Nadeem</keyname><forenames>Q.</forenames></author><author><keyname>Rasheed</keyname><forenames>M. B.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Maqsood</keyname><forenames>Y.</forenames></author><author><keyname>Din</keyname><forenames>A.</forenames></author></authors><title>M-GEAR: Gateway-Based Energy-Aware Multi-Hop Routing Protocol for WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research work, we advise gateway based energy-efficient routing
protocol (M-GEAR) for Wireless Sensor Networks (WSNs). We divide the sensor
nodes into four logical regions on the basis of their location in the sensing
field. We install Base Station (BS) out of the sensing area and a gateway node
at the centre of the sensing area. If the distance of a sensor node from BS or
gateway is less than predefined distance threshold, the node uses direct
communication. We divide the rest of nodes into two equal regions whose
distance is beyond the threshold distance. We select cluster heads (CHs)in each
region which are independent of the other region. These CHs are selected on the
basis of a probability. We compare performance of our protocol with LEACH (Low
Energy Adaptive Clustering Hierarchy). Performance analysis and compared
statistic results show that our proposed protocol perform well in terms of
energy consumption and network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7107</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7107</id><created>2013-07-26</created><authors><author><keyname>Akram</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Tauqir</keyname><forenames>A.</forenames></author><author><keyname>Rao</keyname><forenames>A.</forenames></author><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author></authors><title>THE-FAME: THreshold based Energy-efficient FAtigue MEasurment for
  Wireless Body Area Sensor Networks using Multiple Sinks</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Body Area Sensor Network (WBASN) is a technology employed mainly for
patient health monitoring. New research is being done to take the technology to
the next level i.e. player's fatigue monitoring in sports. Muscle fatigue is
the main cause of player's performance degradation. This type of fatigue can be
measured by sensing the accumulation of lactic acid in muscles. Excess of
lactic acid makes muscles feel lethargic. Keeping this in mind we propose a
protocol \underline{TH}reshold based \underline{E}nergy-efficient
\underline{FA}tigue \underline{ME}asurement (THE-FAME) for soccer players using
WBASN. In THE-FAME protocol, a composite parameter has been used that consists
of a threshold parameter for lactic acid accumulation and a parameter for
measuring distance covered by a particular player. When any parameters's value
in this composite parameter shows an increase beyond threshold, the players is
declared to be in a fatigue state. The size of battery and sensor should be
very small for the sake of players' best performance. These sensor nodes,
implanted inside player's body, are made energy efficient by using multiple
sinks instead of a single sink. Matlab simulation results show the
effectiveness of THE-FAME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7111</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7111</id><created>2013-07-26</created><authors><author><keyname>Khan</keyname><forenames>Y.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>M. J.</forenames></author><author><keyname>Ahmad</keyname><forenames>Y.</forenames></author><author><keyname>Zubair</keyname><forenames>M. H.</forenames></author><author><keyname>Shah</keyname><forenames>S. A.</forenames></author></authors><title>LPCH and UDLPCH: Location-aware Routing Techniques in WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor nodes along with Base Station (BS) constitute a Wireless
Sensor Network (WSN). Nodes comprise of tiny power battery. Nodes sense the
data and send it to BS. WSNs need protocol for efficient energy consumption of
the network. In direct transmission and minimum transmission energy routing
protocols, energy consumption is not well distributed. However, LEACH
(Low-Energy Adaptive Clustering Hierarchy) is a clustering protocol; randomly
selects the Cluster Heads (CHs) in each round. However, random selection of CHs
does not guarantee efficient energy consumption of the network. Therefore, we
proposed new clustering techniques in routing protocols, Location-aware
Permanent CH (LPCH) and User Defined Location-aware Permanent CH (UDLPCH). In
both protocols, network field is physically divided in to two regions, equal
number of nodes are randomly deployed in each region. In LPCH, number of CHs
are selected by LEACH algorithm in first round. However in UDLPCH, equal and
optimum number of CHs are selected in each region, throughout the network life
time number of CHs are remain same. Simulation results show that stability
period and throughput of LPCH is greater than LEACH, stability period and
throughput of UDLPCH is greater than LPCH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7118</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7118</id><created>2013-07-26</created><authors><author><keyname>Mhalla</keyname><forenames>Mehdi</forenames></author><author><keyname>Prost</keyname><forenames>Frederic</forenames></author></authors><title>Gardner's Minichess Variant is solved</title><categories>cs.GT</categories><msc-class>91A35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 5x5 board is the smallest board on which one can set up all kind of chess
pieces as a start position. We consider Gardner's minichess variant in which
all pieces are set as in a standard chessboard (from Rook to King). This game
has roughly 9x10^{18} legal positions and is comparable in this respect with
checkers. We weakly solve this game, that is we prove its game-theoretic value
and give a strategy to draw against best play for White and Black sides. Our
approach requires surprisingly small computing power. We give a human readable
proof. The way the result is obtained is generic and could be generalized to
bigger chess settings or to other games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7122</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7122</id><created>2013-07-26</created><updated>2013-08-12</updated><authors><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Saad</keyname><forenames>George</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author></authors><title>The Power of Mediation in an Extended El Farol Game</title><categories>cs.GT</categories><comments>24 pages, 7 figures. Accepted in SAGT'13
  (http://algo.rwth-aachen.de/sagt2013/accepted.php)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mediator implements a correlated equilibrium when it proposes a strategy to
each player confidentially such that the mediator's proposal is the best
interest for every player to follow. In this paper, we present a mediator that
implements the best correlated equilibrium for an extended El Farol game with
symmetric players. The extended El Farol game we consider incorporates both
negative and positive network effects.
  We study the degree to which this type of mediator can decrease the overall
social cost. In particular, we give an exact characterization of Mediation
Value (MV) and Enforcement Value (EV) for this game. MV is the ratio of the
minimum social cost over all Nash equilibria to the minimum social cost over
all mediators, and EV is the ratio of the minimum social cost over all
mediators to the optimal social cost. This sort of exact characterization is
uncommon for games with both kinds of network effects. An interesting outcome
of our results is that both the MV and EV values can be unbounded for our game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7127</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7127</id><created>2013-07-26</created><authors><author><keyname>Ahuja</keyname><forenames>Piyush</forenames></author></authors><title>Man and Machine: Questions of Risk, Trust and Accountability in Today's
  AI Technology</title><categories>cs.CY cs.AI</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Intelligence began as a field probing some of the most fundamental
questions of science - the nature of intelligence and the design of intelligent
artifacts. But it has grown into a discipline that is deeply entwined with
commerce and society. Today's AI technology, such as expert systems and
intelligent assistants, pose some difficult questions of risk, trust and
accountability. In this paper, we present these concerns, examining them in the
context of historical developments that have shaped the nature and direction of
AI research. We also suggest the exploration and further development of two
paradigms, human intelligence-machine cooperation, and a sociological view of
intelligence, which might help address some of these concerns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7129</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7129</id><created>2013-07-26</created><authors><author><keyname>Fujita</keyname><forenames>Megumi</forenames></author><author><keyname>Goto</keyname><forenames>Yuki</forenames></author><author><keyname>Nide</keyname><forenames>Naoyuki</forenames></author><author><keyname>Satoh</keyname><forenames>Ken</forenames></author><author><keyname>Hosobe</keyname><forenames>Hiroshi</forenames></author></authors><title>An Architecture for Autonomously Controlling Robot with Embodiment in
  Real World</title><categories>cs.RO cs.AI</categories><comments>Submission for proc. of KRR-ICLP2013 (resubmitted using LLNCS style)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the real world, robots with embodiment face various issues such as dynamic
continuous changes of the environment and input/output disturbances. The key to
solving these issues can be found in daily life; people `do actions associated
with sensing' and `dynamically change their plans when necessary'. We propose
the use of a new concept, enabling robots to do these two things, for
autonomously controlling mobile robots. We implemented our concept to make two
experiments under static/dynamic environments. The results of these experiments
show that our idea provides a way to adapt to dynamic changes of the
environment in the real world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7138</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7138</id><created>2013-07-26</created><updated>2015-03-13</updated><authors><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Reconstruction of Network Coded Sources From Incomplete Datasets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of recovering source information
from an incomplete set of network coded data. We first study the theoretical
performance of such systems under maximum a posteriori (MAP) decoding and
derive the upper bound on the probability of decoding error as a function of
the system parameters. We also establish the sufficient conditions on the
number of network coded symbols required to achieve decoding error probability
below a certain level. We then propose a low complexity iterative decoding
algorithm based on message passing for decoding the network coded data of a
particular class of statistically dependent sources that present pairwise
linear correlation. The algorithm operates on a graph that captures the network
coding constraints, while the knowledge about the source correlation is
directly incorporated in the messages exchanged over the graph. We test the
proposed method on both synthetic data and correlated image sequences and
demonstrate that the prior knowledge about the source correlation can be
effectively exploited at the decoder in order to provide a good reconstruction
of the transmitted data in cases where the network coded data available at the
decoder is not sufficient for exact decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7141</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7141</id><created>2013-07-26</created><updated>2014-05-23</updated><authors><author><keyname>Abunadi</keyname><forenames>Ibrahim</forenames></author></authors><title>Influence of Culture on e-Government Acceptance in Saudi Arabia</title><categories>cs.CY</categories><comments>PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this research is to determine the influence of culture on
e-government (electronic government) acceptance in the Kingdom of Saudi Arabia
(KSA). Citizens' transactional interactions (electronic transactions or
e-transactions) with the government via the Internet are examined. Multiple
models and theories in extant literature were referred to in identifying the
research context requirements that enabled the analysis of e-transaction
acceptance. A new research model that fits the research context was developed
to predict and explain how acceptance is affected. A sample of 671 Saudi
citizens was recruited using an online survey. Structural equation modelling
was used to assess the impact on intention to use e-transactions. Preference
for using e-transactions as a communication method, perceptions of the
compatibility of e-transactions with values and citizens' needs,
communicability of the results of using e-transactions, trust in the Internet
as a medium of communication with the government, and conservation values are
positive significant determinants of e-transaction acceptance. Conversely, the
influence of trust in government agencies and motivation towards gaining
prestige and possessing dominance over people and resources was negative. The
findings of this study include design and implementation strategies that can
serve as guidance for the Saudi government, as well as for the developers and
implementers of e-transactions in the KSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7142</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7142</id><created>2013-07-28</created><authors><author><keyname>P&#xe1;lovics</keyname><forenames>R&#xf3;bert</forenames></author><author><keyname>Bencz&#xfa;r</keyname><forenames>Andr&#xe1;s A.</forenames></author></authors><title>Temporal influence over the Last.fm social network</title><categories>cs.SI physics.soc-ph</categories><comments>2013 IEEE/ACM International Conference on Advances in Social Networks
  Analysis and Mining, ASONAM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent results show the influence of social contacts to spread
certain properties over the network, but others question the methodology of
these experiments by proposing that the measured effects may be due to
homophily or a shared environment. In this paper we justify the existence of
the social influence by considering the temporal behavior of Last.fm users. In
order to clearly distinguish between friends sharing the same interest,
especially since Last.fm recommends friends based on similarity of taste, we
separated the timeless effect of similar taste from the temporal impulses of
immediately listening to the same artist after a friend. We measured strong
increase of listening to a completely new artist in a few hours period after a
friend compared to non-friends representing a simple trend or external
influence. In our experiment to eliminate network independent elements of
taste, we improved collaborative filtering and trend based methods by blending
with simple time aware recommendations based on the influence of friends. Our
experiments are carried over the two-year &quot;scrobble&quot; history of 70,000 Last.fm
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7154</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7154</id><created>2013-07-26</created><updated>2013-12-09</updated><authors><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Fast Polar Decoders: Algorithm and Implementation</title><categories>cs.AR cs.IT math.IT</categories><comments>Submitted to the IEEE Journal on Selected Areas in Communications
  (JSAC) on May 15th, 2013. 11 pages, 7 figures, 6 tables</comments><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 32, no. 5,
  May 2014, pp. 946-957</journal-ref><doi>10.1109/JSAC.2014.140514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes provably achieve the symmetric capacity of a memoryless channel
while having an explicit construction. This work aims to increase the
throughput of polar decoder hardware by an order of magnitude relative to the
state of the art successive-cancellation decoder. We present an algorithm,
architecture, and FPGA implementation of a gigabit-per-second polar decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7159</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7159</id><created>2013-07-26</created><updated>2014-03-25</updated><authors><author><keyname>Barra</keyname><forenames>Aleams</forenames></author><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>MacWilliams Extension Theorems and the Local-Global Property for Codes
  over Rings</title><categories>cs.IT math.IT math.RA</categories><msc-class>94B05 (Primary), 16L60 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MacWilliams extension theorem is investigated for various weight
functions over finite Frobenius rings. The problem is reformulated in terms of
a local-global property for subgroups of the general linear group. Among other
things, it is shown that the extension theorem holds true for poset weights if
and only if the underlying poset is hierarchical. Specifically, the
Rosenbloom-Tsfasman weight for vector codes satisfies the extension theorem,
whereas the Niederreiter-Rosenbloom-Tsfasman weight for matrix codes does not.
A short character-theoretic proof of the well-known MacWilliams extension
theorem for the homogeneous weight is provided. Moreover it is shown that the
extension theorem carries over to direct products of weights, but not to
symmetrized products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7164</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7164</id><created>2013-07-26</created><authors><author><keyname>Ghaderi</keyname><forenames>Majid</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>On the Scalability of Reliable Data Transfer in High Speed Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers reliable data transfer in a high-speed network (HSN) in
which the per-connection capacity is very large. We focus on sliding window
protocols employing selective repeat for reliable data transfer and study two
reliability mechanisms based on ARQ and FEC. The question we ask is which
mechanism is more suitable for an HSN in which the scalability of reliable data
transfer in terms of receiver's buffer requirement and achievable delay and
throughput is a concern. To efficiently utilize the large bandwidth available
to a connection in an HSN, sliding window protocols require a large
transmission window. In this regime of large transmission windows, we show that
while both mechanisms achieve the same asymptotic throughput in the presence of
packet losses, their delay and buffer requirements are different. Specifically,
an FEC-based mechanism has delay and receiver's buffer requirement that are
asymptotically smaller than that of an ARQ-based selective repeat mechanism by
a factor of log W, where W is the window size of the selective repeat
mechanism. This result is then used to investigate the implications of each
reliability mechanism on protocol design in an HSN in terms of throughput,
delay, buffer requirement, and control overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7170</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7170</id><created>2013-07-26</created><updated>2015-06-19</updated><authors><author><keyname>Franchi</keyname><forenames>Antonio</forenames></author><author><keyname>Stegagno</keyname><forenames>Paolo</forenames></author><author><keyname>Oriolo</keyname><forenames>Giuseppe</forenames></author></authors><title>Decentralized Multi-Robot Encirclement of a 3D Target with Guaranteed
  Collision Avoidance</title><categories>cs.SY cs.MA cs.RO math.OC</categories><comments>Accepted for Autonomous Robots - Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a control framework for achieving encirclement of a target moving
in 3D using a multi-robot system. Three variations of a basic control strategy
are proposed for different versions of the encirclement problem, and their
effectiveness is formally established. An extension ensuring maintenance of a
safe inter-robot distance is also discussed. The proposed framework is fully
decentralized and only requires local communication among robots; in
particular, each robot locally estimates all the relevant global quantities. We
validate the proposed strategy through simulations on kinematic point robots
and quadrotor UAVs, as well as experiments on differential-drive wheeled mobile
robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7172</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7172</id><created>2013-07-26</created><updated>2013-07-31</updated><authors><author><keyname>Keegan</keyname><forenames>Brian</forenames></author><author><keyname>Horn</keyname><forenames>Dan</forenames></author><author><keyname>Finholt</keyname><forenames>Thomas A.</forenames></author><author><keyname>Kaye</keyname><forenames>Joseph &quot;Jofish&quot;</forenames></author></authors><title>Structure and Dynamics of Coauthorship, Citation, and Impact within CSCW</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>14 pages; 14 figures</comments><acm-class>K.2; K.4.3; H.3.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  CSCW has stabilized as an interdisciplinary venue for computer, information,
cognitive, and social scientists but has also undergone significant changes in
its format in recent years. This paper uses methods from social network
analysis and bibliometrics to re-examine the structures of CSCW a decade after
its last systematic analysis. Using data from the ACM Digital Library, we
analyze changes in structures of coauthorship and citation between 1986 and
2013. Statistical models reveal significant but distinct patterns between
papers and authors in how brokerage and closure in these networks affects
impact as measured by citations and downloads. Specifically, impact is unduly
influenced by structural position, such that ideas introduced by those in the
core of the CSCW community (e.g., elite researchers) are advantaged over those
introduced by peripheral participants (e.g., newcomers). This finding is
examined in the context of recent changes to the CSCW conference that may have
the effect of upsetting the preference for contributions from the core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7176</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7176</id><created>2013-07-26</created><authors><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Nelson</keyname><forenames>Aaron A.</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>Phase retrieval from very few measurements</title><categories>math.FA cs.CC cs.IT math.IT</categories><comments>18 pages, 1 figure</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In many applications, signals are measured according to a linear process, but
the phases of these measurements are often unreliable or not available. To
reconstruct the signal, one must perform a process known as phase retrieval.
This paper focuses on completely determining signals with as few intensity
measurements as possible, and on efficient phase retrieval algorithms from such
measurements. For the case of complex M-dimensional signals, we construct a
measurement ensemble of size 4M-4 which yields injective intensity
measurements; this is conjectured to be the smallest such ensemble. For the
case of real signals, we devise a theory of &quot;almost&quot; injective intensity
measurements, and we characterize such ensembles. Later, we show that phase
retrieval from M+1 almost injective intensity measurements is NP-hard,
indicating that computationally efficient phase retrieval must come at the
price of measurement redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7185</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7185</id><created>2013-07-26</created><updated>2014-01-21</updated><authors><author><keyname>Parzysz</keyname><forenames>Fanny</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author><author><keyname>Gagnon</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Impact of Propagation Environment on Energy-Efficient Relay Placement:
  Model and Performance Analysis</title><categories>cs.NI</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a relay-based cellular network is greatly affected by the
relay location within a cell. Existing results for optimal relay placement do
not reflect how the radio propagation environment and choice of the coding
scheme can impact system performance. In this paper, we analyze the impact on
relaying performance of node distances, relay height and line-of-sight
conditions for both uplink and downlink transmissions, using several relay
coding schemes. Our first objective is to propose a geometrical model for
energy-efficient relay placement that requires only a small number of
characteristic distances. Our second objective is to estimate the maximum cell
coverage of a relay-aided cell given power constraints, and conversely, the
averaged energy consumption given a cell radius. We show that the practical
full decode-forward scheme performs close to the energy-optimized partial
decode-forward scheme when the relay is ideally located. However, away from
this optimum relay location, performance rapidly degrades and more advanced
coding scheme, such as partial decode-forward, is needed to maintain good
performance and allow more freedom in the relay placement. Finally, we define a
trade-off between cell coverage and energy efficiency, and show that there
exists a relay location for which increasing the cell coverage has a minimal
impact on the average energy consumed per unit area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7192</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7192</id><created>2013-07-26</created><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>MixedGrad: An O(1/T) Convergence Rate Algorithm for Stochastic Smooth
  Optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the optimal convergence rate for stochastic
optimization of smooth functions is $O(1/\sqrt{T})$, which is same as
stochastic optimization of Lipschitz continuous convex functions. This is in
contrast to optimizing smooth functions using full gradients, which yields a
convergence rate of $O(1/T^2)$. In this work, we consider a new setup for
optimizing smooth functions, termed as {\bf Mixed Optimization}, which allows
to access both a stochastic oracle and a full gradient oracle. Our goal is to
significantly improve the convergence rate of stochastic optimization of smooth
functions by having an additional small number of accesses to the full gradient
oracle. We show that, with an $O(\ln T)$ calls to the full gradient oracle and
an $O(T)$ calls to the stochastic oracle, the proposed mixed optimization
algorithm is able to achieve an optimization error of $O(1/T)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7195</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7195</id><created>2013-07-26</created><authors><author><keyname>Bruglieri</keyname><forenames>Maurizio</forenames></author><author><keyname>Colorni</keyname><forenames>Alberto</forenames></author><author><keyname>Lu&#xe8;</keyname><forenames>Alessandro</forenames></author></authors><title>The vehicle relocation problem for the one-way electric vehicle sharing</title><categories>math.OC cs.RO</categories><comments>This is the pre-print version of the paper submitted to the special
  issue on Networks of AIRO2012 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional car-sharing services are based on the two-way scheme, where the
user picks up and returns the vehicle at the same parking station. Some
services permits also one-way trips, which allows the user to return the
vehicle in another station. The one-way scheme is quite more attractive for the
users, but may pose a problem for the distribution of the vehicles, due to a
possible unbalancing between the user demand and the availability of vehicles
or free slots at the stations. Such a problem is more complicated in the case
of electrical car sharing, where the travel range depends on the level of
charge of the vehicles. The paper presents a new approach for the Electric
Vehicle Relocation Problem, where cars are moved by personnel of the service
operator to keep the system balanced. Such a problem generates a challenging
pickup and delivery problem with new features that to the best of our knowledge
never have been considered in the literature. We yield a Mixed Integer Linear
Programming formulation and some valid inequalities to speed up its solution
through a state-of-the art solver (CPLEX). We test our approach on verisimilar
instances built on the Milan road network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7198</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7198</id><created>2013-07-26</created><authors><author><keyname>Okuma</keyname><forenames>Kenji</forenames></author><author><keyname>Lowe</keyname><forenames>David G.</forenames></author><author><keyname>Little</keyname><forenames>James J.</forenames></author></authors><title>Self-Learning for Player Localization in Sports Video</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel self-learning framework that automates the
label acquisition process for improving models for detecting players in
broadcast footage of sports games. Unlike most previous self-learning
approaches for improving appearance-based object detectors from videos, we
allow an unknown, unconstrained number of target objects in a more generalized
video sequence with non-static camera views. Our self-learning approach uses a
latent SVM learning algorithm and deformable part models to represent the shape
and colour information of players, constraining their motions, and learns the
colour of the playing field by a gentle Adaboost algorithm. We combine those
image cues and discover additional labels automatically from unlabelled data.
In our experiments, our approach exploits both labelled and unlabelled data in
sparsely labelled videos of sports games, providing a mean performance
improvement of over 20% in the average precision for detecting sports players
and improved tracking, when videos contain very few labelled images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7202</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7202</id><created>2013-07-26</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Students learning center strategy based on e-learning and blogs</title><categories>cs.CY</categories><comments>6 pages, Seminar Nasional Sains dan Teknologi (SNST) ke- 4 tahun
  2013, Universitas Wahid Hasyim Semarang, 2013</comments><journal-ref>Seminar Nasional Sains dan Teknologi (SNST) ke- 4 tahun 2013,
  Universitas Wahid Hasyim Semarang, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Education is the main infrastructure for promoting a nation and increasing
their competitiveness in globalization era that involves the use of information
technology (IT). This paper has goal to expand the alternative for learning
strategy based on IT for final year students that will conduct a research for
their final report. The final year students are expected have the ability to
learn independently to manage the needness of their learning supply. In this
paper, the author will discuss how to use the e-learning media and blogs to
manage the independently learning environment or self-learning. E-learning
offers the flexibilities in term of time and place in supporting the learning
activities as well as a media for faculty/lecturer to disseminate learning
materials. While the blog has been a free medium for the publication of variety
contents, including academic contents. In this paper, author suggests some the
institutions regulations, lecturers roles, and students participations are
involve in set the learning environment based on e-learning and blog. These
activities able to support students in learning independently or self-learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7208</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7208</id><created>2013-07-26</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Mao</keyname><forenames>Wenli</forenames></author></authors><title>Clustering Chinese Regional Cultures with Online-gaming Data</title><categories>cs.CY cs.SI physics.data-an physics.soc-ph</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To identify cluster of societies is not easy in subject to the availability
of data. In this study, from the prospective of computational social science,
we propose a novel method to cluster Chinese regional cultures. Using millions
of geotagged online-gaming data of Chinese internet users playing online card
and board games with regional features, 336 Chinese cities are grouped into
several main clusters. The geographic boundaries of clusters coincide with the
boundaries of provincial regions. The north regions in China have more
geographical proximity, when regional variations in south regions are more
evident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7210</identifier>
 <datestamp>2013-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7210</id><created>2013-07-26</created><updated>2013-09-25</updated><authors><author><keyname>Joseph</keyname><forenames>Vinay</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author></authors><title>NOVA: QoE-driven Optimization of DASH-based Video Delivery in Networks</title><categories>cs.NI cs.MM</categories><comments>This is an extended version of (9-page) conference version available
  at http://wncg.org/publications/dl.php?file=VJGdV_conf_13.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimizing video delivery for a network supporting
video clients streaming stored video. Specifically, we consider the problem of
jointly optimizing network resource allocation and video quality adaptation.
Our objective is to fairly maximize video clients' Quality of Experience (QoE)
realizing tradeoffs among the mean quality, temporal variability in quality,
and fairness, incorporating user preferences on rebuffering and cost of video
delivery. We present a simple asymptotically optimal online algorithm, NOVA, to
solve the problem. NOVA is asynchronous, and using minimal communication,
distributes the tasks of resource allocation to network controller, and quality
adaptation to respective video clients. Video quality adaptation in NOVA is
also optimal for standalone video clients, and is well suited for use with DASH
framework. Further, we extend NOVA for use with more general QoE models,
networks shared with other traffic loads and networks using fixed/legacy
resource allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7211</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7211</id><created>2013-07-26</created><authors><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Physical Layer Security in Downlink Multi-Antenna Cellular Networks</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study physical layer security for the downlink of cellular
networks, where the confidential messages transmitted to each mobile user can
be eavesdropped by both (i) the other users in the same cell and (ii) the users
in the other cells. The locations of base stations and mobile users are modeled
as two independent two-dimensional Poisson point processes. Using the proposed
model, we analyze the secrecy rates achievable by regularized channel inversion
(RCI) precoding by performing a large-system analysis that combines tools from
stochastic geometry and random matrix theory. We obtain approximations for the
probability of secrecy outage and the mean secrecy rate, and characterize
regimes where RCI precoding achieves a nonzero secrecy rate. We find that
unlike isolated cells, the secrecy rate in a cellular network does not grow
monotonically with the transmit power, and the network tends to be in secrecy
outage if the transmit power grows unbounded. Furthermore, we show that there
is an optimal value for the base station deployment density that maximizes the
secrecy rate, and this value is a decreasing function of the signal-to-noise
ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7220</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7220</id><created>2013-07-27</created><updated>2014-08-26</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Kais</keyname><forenames>Sabre</forenames></author></authors><title>Multiple Network Alignment on Quantum Computers</title><categories>quant-ph cs.CC</categories><doi>10.1007/s11128-014-0818-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparative analyses of graph structured datasets underly diverse problems.
Examples of these problems include identification of conserved functional
components (biochemical interactions) across species, structural similarity of
large biomolecules, and recurring patterns of interactions in social networks.
A large class of such analyses methods quantify the topological similarity of
nodes across networks. The resulting correspondence of nodes across networks,
also called node alignment, can be used to identify invariant subgraphs across
the input graphs.
  Given $k$ graphs as input, alignment algorithms use topological information
to assign a similarity score to each $k$-tuple of nodes, with elements (nodes)
drawn from each of the input graphs. Nodes are considered similar if their
neighbors are also similar. An alternate, equivalent view of these network
alignment algorithms is to consider the Kronecker product of the input graphs,
and to identify high-ranked nodes in the Kronecker product graph. Conventional
methods such as PageRank and HITS (Hypertext Induced Topic Selection) can be
used for this purpose. These methods typically require computation of the
principal eigenvector of a suitably modified Kronecker product matrix of the
input graphs. We adopt this alternate view of the problem to address the
problem of multiple network alignment. Using the phase estimation algorithm, we
show that the multiple network alignment problem can be efficiently solved on
quantum computers. We characterize the accuracy and performance of our method,
and show that it can deliver exponential speedups over conventional
(non-quantum) methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7223</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7223</id><created>2013-07-27</created><updated>2013-12-13</updated><authors><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Universal Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes, invented by Arikan in 2009, are known to achieve the capacity of
any binary-input memoryless output-symmetric channel. One of the few drawbacks
of the original polar code construction is that it is not universal. This means
that the code has to be tailored to the channel if we want to transmit close to
capacity.
  We present two &quot;polar-like&quot; schemes which are capable of achieving the
compound capacity of the whole class of binary-input memoryless
output-symmetric channels with low complexity.
  Roughly speaking, for the first scheme we stack up $N$ polar blocks of length
$N$ on top of each other but shift them with respect to each other so that they
form a &quot;staircase.&quot; Coding then across the columns of this staircase with a
standard Reed-Solomon code, we can achieve the compound capacity using a
standard successive decoder to process the rows (the polar codes) and in
addition a standard Reed-Solomon erasure decoder to process the columns.
Compared to standard polar codes this scheme has essentially the same
complexity per bit but a block length which is larger by a factor $O(N
\log_2(N)/\epsilon)$, where $\epsilon$ is the gap to capacity.
  For the second scheme we first show how to construct a true polar code which
achieves the compound capacity for a finite number of channels. We achieve this
by introducing special &quot;polarization&quot; steps which &quot;align&quot; the good indices for
the various channels. We then show how to exploit the compactness of the space
of binary-input memoryless output-symmetric channels to reduce the compound
capacity problem for this class to a compound capacity problem for a finite set
of channels. This scheme is similar in spirit to standard polar codes, but the
price for universality is a considerably larger blocklength.
  We close with what we consider to be some interesting open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7226</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7226</id><created>2013-07-27</created><authors><author><keyname>Wen</keyname><forenames>Fuxi</forenames></author></authors><title>Diffusion Least Mean P-Power Algorithms for Distributed Estimation in
  Alpha-Stable Noise Environments</title><categories>cs.IT math.IT</categories><journal-ref>Electronics Letters, vol. 49, no. 21, 2013, pp. 1355-1356</journal-ref><doi>10.1049/el.2013.2331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a diffusion least mean p-power (LMP) algorithm for distributed
estimation in alpha stable noise environments, which is one of the widely used
models that appears in various environments. Compared with the diffusion least
mean squares (LMS) algorithm, better performance is obtained for the diffusion
LMP methods when the noise is with alpha-stable distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7231</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7231</id><created>2013-07-27</created><authors><author><keyname>Ogierman</keyname><forenames>Adrian</forenames></author><author><keyname>Richa</keyname><forenames>Andrea</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Zhang</keyname><forenames>Jin</forenames></author></authors><title>Competitive MAC under Adversarial SINR</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1007.1189</comments><acm-class>C.2.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of how to efficiently share a wireless
medium which is subject to harsh external interference or even jamming. While
this problem has already been studied intensively for simplistic single-hop or
unit disk graph models, we make a leap forward and study MAC protocols for the
SINR interference model (a.k.a. the physical model).
  We make two contributions. First, we introduce a new adversarial SINR model
which captures a wide range of interference phenomena. Concretely, we consider
a powerful, adaptive adversary which can jam nodes at arbitrary times and which
is only limited by some energy budget. The second contribution of this paper is
a distributed MAC protocol which provably achieves a constant competitive
throughput in this environment: we show that, with high probability, the
protocol ensures that a constant fraction of the non-blocked time periods is
used for successful transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7233</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7233</id><created>2013-07-27</created><authors><author><keyname>Banerjee</keyname><forenames>Arijit</forenames></author><author><keyname>Maas</keyname><forenames>Dustin</forenames></author><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author><author><keyname>Kasera</keyname><forenames>Sneha</forenames></author></authors><title>Through Wall People Localization Exploiting Radio Windows</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and investigate the ability of an attacker to surreptitiously
use an otherwise secure wireless network to detect moving people through walls,
in an area in which people expect their location to be private. We call this
attack on location privacy of people an &quot;exploiting radio windows&quot; (ERW)
attack. We design and implement the ERW attack methodology for through wall
people localization that relies on reliably detecting when people cross the
link lines by using physical layer measurements between the legitimate
transmitters and the attack receivers. We also develop a method to estimate the
direction of movement of a person from the sequence of link lines crossed
during a short time interval. Additionally, we describe how an attacker may
estimate any artificial changes in transmit power (used as a countermeasure),
compensate for these power changes using measurements from sufficient number of
links, and still detect line crossings. We implement our methodology on WiFi
and ZigBee nodes and experimentally evaluate the ERW attack by monitoring
people movements through walls in two real-world settings. We find that our
methods achieve very high accuracy in detecting line crossings and determining
direction of motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7238</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7238</id><created>2013-07-27</created><authors><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author><author><keyname>Ashraf</keyname><forenames>M. J.</forenames></author><author><keyname>Wasiq</keyname><forenames>S.</forenames></author><author><keyname>Iqbal</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author></authors><title>Analysis and Modeling of Network Connectivity in Routing Protocols for
  MANETs and VANETs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a framework is presented for node distribution with respect to
density, network connectivity and communication time. According to modeled
framework we evaluate and compare the performance of three routing protocols;
Ad-hoc On-demand Distance Vector (AODV), Dynamic Source Routing (DSR) and
Fisheye State Routing (FSR) in MANETs and VANETs using two Mac-layer protocols;
802.11 and 802.11p. We have further modified these protocols by changing their
routing information exchange intervals; MOD AODV, MOD DSR and MOD FSR. A
comprehensive simulation work is performed in NS-2 for the comparison of these
routing protocols for varying mobilities and scalabilities of nodes. To
evaluate their efficiency; throughput, End-to-End Delay (E2ED) and Normalized
Routing Load (NRL) of these protocols are taken into account as performance
parameters. After extensive simulations, we observe that AODV outperforms both
with MANETs and VANETs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7240</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7240</id><created>2013-07-27</created><authors><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author><author><keyname>Wasiq</keyname><forenames>S.</forenames></author><author><keyname>Arshad</keyname><forenames>W.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khattak</keyname><forenames>S.</forenames></author><author><keyname>Ashraf</keyname><forenames>M. J.</forenames></author></authors><title>Modeling Probability of Path Loss for DSDV, OLSR and DYMO above 802.11
  and 802.11p</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents path loss model along with framework for probability
distribution function for VANETs. Furthermore, we simulate three routing
protocols Destination Sequenced Distance Vector (DSDV), Optimized Link State
Routing (OLSR) and Dynamic MANET On-demand (DYMO) in NS-2 to evaluate and
compare their performance using two Mac-layer Protocols 802.11 and 802.11p. A
novel approach of this work is modifications in existing parameters to achieve
high efficiency. After extensive simulations, we observe that DSDV out performs
with 802.11p while DYMO gives best performance with 802.11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7241</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7241</id><created>2013-07-27</created><authors><author><keyname>Tauqir</keyname><forenames>A.</forenames></author><author><keyname>Akram</keyname><forenames>S.</forenames></author><author><keyname>Khan</keyname><forenames>A. H.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Akbar</keyname><forenames>M.</forenames></author></authors><title>Non-Invasive Induction Link Model for Implantable Biomedical
  Microsystems: Pacemaker to Monitor Arrhythmic Patients in Body Area Networks</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a non-invasive inductive link model for an Implantable
Biomedical Microsystems (IBMs) such as, a pacemaker to monitor Arrhythmic
Patients (APs) in Body Area Networks (BANs) is proposed. The model acts as a
driving source to keep the batteries charged, inside a device called,
pacemaker. The device monitors any drift from natural human heart beats, a
condition of arrythmia and also in turn, produces electrical pulses that create
forced rhythms that, matches with the original normal heart rhythms. It
constantly sends a medical report to the health center to keep the medical
personnel aware of the patient's conditions and let them handle any critical
condition, before it actually happens. Two equivalent models are compared by
carrying the simulations, based on the parameters of voltage gain and link
efficiency. Results depict that the series tuned primary and parallel tuned
secondary circuit achieves the best results for both the parameters, keeping in
view the constraint of coupling co-efficient (k), which should be less than a
value \emph{0.45} as, desirable for the safety of body tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7242</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7242</id><created>2013-07-27</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Faisal</keyname><forenames>S.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Nayab</keyname><forenames>D.</forenames></author><author><keyname>Zahid</keyname><forenames>M.</forenames></author></authors><title>Measuring Fatigue of Soldiers in Wireless Body Area Sensor Networks</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Body Area Sensor Networks (WBASNs) consist of on-body or in-body
sensors placed on human body for health monitoring. Energy conservation of
these sensors, while guaranteeing a required level of performance, is a
challenging task. Energy efficient routing schemes are designed for the
longevity of network lifetime. In this paper, we propose a routing protocol for
measuring fatigue of a soldier. Three sensors are attached to soldier's body
that monitor specific parameters. Our proposed protocol is an event driven
protocol and takes three scenarios for measuring the fatigue of a soldier. We
evaluate our proposed work in terms of network lifetime, throughput, remaining
energy of sensors and fatigue of a soldier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7245</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7245</id><created>2013-07-27</created><authors><author><keyname>Rao</keyname><forenames>A.</forenames></author><author><keyname>Akbar</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author><author><keyname>Sarfraz</keyname><forenames>S.</forenames></author></authors><title>AM-DisCNT: Angular Multi-hop DIStance based Circular Network
  Transmission Protocol for WSNs</title><categories>cs.NI</categories><comments>IEEE 8th International Conference on Broadband and Wireless
  Computing, Communication and Applications (BWCCA'13), Compiegne, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nodes in wireless sensor networks (WSNs) contain limited energy
resources, which are needed to transmit data to base station (BS). Routing
protocols are designed to reduce the energy consumption. Clustering algorithms
are best in this aspect. Such clustering algorithms increase the stability and
lifetime of the network. However, every routing protocol is not suitable for
heterogeneous environments. AM-DisCNT is proposed and evaluated as a new energy
efficient protocol for wireless sensor networks. AM-DisCNT uses circular
deployment for even consumption of energy in entire wireless sensor network.
Cluster-head selection is on the basis of energy. Highest energy node becomes
CH for that round. Energy is again compared in the next round to check the
highest energy node of that round. The simulation results show that AM-DisCNT
performs better than the existing heterogeneous protocols on the basis of
network lifetime, throughput and stability of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7249</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7249</id><created>2013-07-27</created><updated>2014-08-25</updated><authors><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Access Point Density and Bandwidth Partitioning in Ultra Dense Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the impact of system parameters such as access point
density and bandwidth partitioning on the performance of randomly deployed,
interference-limited, dense wireless networks. While much progress has been
achieved in analyzing randomly deployed networks via tools from stochastic
geometry, most existing works either assume a very large user density compared
to that of access points which does not hold in a dense network, and/or
consider only the user signal-to-interference-ratio as the system figure of
merit which provides only partial insight on user rate, as the effect of
multiple access is ignored. In this paper, the user rate distribution is
obtained analytically, taking into account the effects of multiple access as
well as the SIR outage. It is shown that user rate outage probability is
dependent on the number of bandwidth partitions (subchannels) and the way they
are utilized by the multiple access scheme. The optimal number of partitions is
lower bounded for the case of large access point density. In addition, an upper
bound of the minimum access point density required to provide an asymptotically
small rate outage probability is provided in closed form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7252</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7252</id><created>2013-07-27</created><updated>2014-01-23</updated><authors><author><keyname>Blanco-Chac&#xf3;n</keyname><forenames>Iv&#xe1;n</forenames></author><author><keyname>Rem&#xf3;n</keyname><forenames>Dion&#xed;s</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Alsina</keyname><forenames>Montserrat</forenames></author></authors><title>Fuchsian codes for AWGN channels</title><categories>cs.IT math.IT</categories><comments>This is an extended version of a work with the same title which has
  been already published in the Proceedings of the Workshop on Cryptography and
  Coding Theory (Bergen, 2013). I particular, it contains extra material on the
  generation of the constellation. In the first version, there were some
  simulation errors that have now been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new transmission scheme for additive white Gaussian noisy (AWGN)
single-input single-output (SISO) channels without fading based on arithmetic
Fuchsian groups. The properly discontinuous character of the action of these
groups on the upper half-plane translates into logarithmic decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7259</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7259</id><created>2013-07-27</created><authors><author><keyname>Oliveira</keyname><forenames>Leonardo I. L.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author></authors><title>The predecessor-existence problem for k-reversible processes</title><categories>cs.DS nlin.CG</categories><journal-ref>Theoretical Computer Science 562 (2015), 406-418</journal-ref><doi>10.1016/j.tcs.2014.10.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For k&gt;=1, we consider the graph dynamical system known as a k-reversible
process. In such process, each vertex in the graph has one of two possible
states at each discrete time. Each vertex changes its state between the present
time and the next if and only if it currently has at least k neighbors in a
state different than its own. Given a k-reversible process and a configuration
of states assigned to the vertices, the Predecessor Existence problem consists
of determining whether this configuration can be generated by the process from
another configuration within exactly one time step. We can also extend the
problem by asking for the number of configurations from which a given
configuration is reachable within one time step. Predecessor Existence can be
solved in polynomial time for k=1, but for k&gt;1 we show that it is NP-complete.
When the graph in question is a tree we show how to solve it in O(n) time and
how to count the number of predecessor configurations in O(n^2) time. We also
solve Predecessor Existence efficiently for the specific case of 2-reversible
processes when the maximum degree of a vertex in the graph is no greater than
3. For this case we present an algorithm that runs in O(n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7261</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7261</id><created>2013-07-27</created><authors><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames></author></authors><title>The Power of Spreadsheet Computations</title><categories>cs.PL</categories><comments>36 pages. Electronic appendices in Excel's xlsx format available from
  author's Web page</comments><acm-class>K.8.1; F.1.2; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the expressive power of spreadsheets. We consider spreadsheets
which contain only formulas, and assume that they are small templates, which
can be filled to a larger area of the grid to process input data of variable
size. Therefore we can compare them to well-known machine models of
computation. We consider a number of classes of spreadsheets defined by
restrictions on their reference structure. Two of the classes correspond
closely to parallel complexity classes: we prove a direct correspondence
between the dimensions of the spreadsheet and amount of hardware and time used
by a parallel computer to compute the same function. As a tool, we produce
spreadsheets which are universal in these classes, i.e. can emulate any other
spreadsheet from them. In other cases we implement in the spreadsheets in
question instances of a polynomial-time complete problem, which indicates that
the the spreadsheets are unlikely to have efficient parallel evaluation
algorithms. Thus we get a picture how the computational power of spreadsheets
depends on their dimensions and structure of references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7263</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7263</id><created>2013-07-27</created><authors><author><keyname>Vasile</keyname><forenames>Cristian Ioan</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Sampling-Based Temporal Logic Path Planning</title><categories>cs.RO</categories><comments>8 pages, 4 figures; extended version of the paper presented at IROS
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a sampling-based motion planning algorithm that
finds an infinite path satisfying a Linear Temporal Logic (LTL) formula over a
set of properties satisfied by some regions in a given environment. The
algorithm has three main features. First, it is incremental, in the sense that
the procedure for finding a satisfying path at each iteration scales only with
the number of new samples generated at that iteration. Second, the underlying
graph is sparse, which guarantees the low complexity of the overall method.
Third, it is probabilistically complete. Examples illustrating the usefulness
and the performance of the method are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7271</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7271</id><created>2013-07-27</created><authors><author><keyname>Ciucu</keyname><forenames>Florin</forenames></author><author><keyname>Schmitt</keyname><forenames>Jens</forenames></author></authors><title>On the Catalyzing Effect of Randomness on the Per-Flow Throughput in
  Wireless Networks</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the throughput capacity of a flow crossing a
multi-hop wireless network, whose geometry is characterized by general
randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both
the nodes' densities and the number of hops. The key contribution is to
demonstrate \textit{how} the \textit{per-flow throughput} depends on the
distribution of 1) the number of nodes $N_j$ inside hops' interference sets, 2)
the number of hops $K$, and 3) the degree of spatial correlations. The
randomness in both $N_j$'s and $K$ is advantageous, i.e., it can yield larger
scalings (as large as $\Theta(n)$) than in non-random settings. An interesting
consequence is that the per-flow capacity can exhibit the opposite behavior to
the network capacity, which was shown to suffer from a logarithmic decrease in
the presence of randomness. In turn, spatial correlations along the end-to-end
path are detrimental by a logarithmic term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7281</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7281</id><created>2013-07-27</created><updated>2014-03-31</updated><authors><author><keyname>Samanta</keyname><forenames>Roopsha</forenames></author><author><keyname>Olivo</keyname><forenames>Oswaldo</forenames></author><author><keyname>Emerson</keyname><forenames>E. Allen</forenames></author></authors><title>Cost-Aware Automatic Program Repair</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formal framework for repairing infinite-state, imperative,
sequential programs, with (possibly recursive) procedures and multiple
assertions; the framework can generate repaired programs by modifying the
original erroneous program in multiple program locations, and can ensure the
readability of the repaired program using user-defined expression templates;
the framework also generates a set of inductive assertions that serve as a
proof of correctness of the repaired program. As a step toward integrating
programmer intent and intuition in automated program repair, we present a &quot;
cost-aware&quot; formulation - given a cost function associated with permissible
statement modifications, the goal is to ensure that the total program
modification cost does not exceed a given repair budget. As part of our
predicate abstraction-based solution framework, we present a sound and complete
algorithm for repair of Boolean programs. We have developed a prototype tool
based on SMT solving and used it successfully to repair diverse errors in
benchmark C programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7286</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7286</id><created>2013-07-27</created><authors><author><keyname>Kaur</keyname><forenames>Harjinder</forenames></author><author><keyname>Singh</keyname><forenames>Gurpreet</forenames></author><author><keyname>Minhas</keyname><forenames>Jaspreet</forenames></author></authors><title>A Review of Machine Learning based Anomaly Detection Techniques</title><categories>cs.LG cs.CR</categories><comments>3 pages. arXiv admin note: text overlap with arXiv:1204.6416 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion detection is so much popular since the last two decades where
intrusion is attempted to break into or misuse the system. It is mainly of two
types based on the intrusions, first is Misuse or signature based detection and
the other is Anomaly detection. In this paper Machine learning based methods
which are one of the types of Anomaly detection techniques is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7291</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7291</id><created>2013-07-27</created><authors><author><keyname>Correa</keyname><forenames>Denzil</forenames></author><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Fit or Unfit : Analysis and Prediction of 'Closed Questions' on Stack
  Overflow</title><categories>cs.SI cs.IR cs.SE</categories><comments>13 pages, 14 figures, 10 tables, version 1.0</comments><acm-class>H.3.3; H.3.4; H.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Stack Overflow is widely regarded as the most popular Community driven
Question Answering (CQA) website for programmers. Questions posted on Stack
Overflow which are not related to programming topics, are marked as 'closed' by
experienced users and community moderators. A question can be 'closed' for five
reasons - duplicate, off-topic, subjective, not a real question and too
localized. In this work, we present the first study of 'closed' questions in
Stack Overflow. We download 4 years of publicly available data which contains
3.4 Million questions. We first analyze and characterize the complete set of
0.1 Million 'closed' questions. Next, we use a machine learning framework and
build a predictive model to identify a 'closed' question at the time of
question creation.
  One of our key findings is that despite being marked as 'closed', subjective
questions contain high information value and are very popular with the users.
We observe an increasing trend in the percentage of closed questions over time
and find that this increase is positively correlated to the number of newly
registered users. In addition, we also see a decrease in community
participation to mark a 'closed' question which has led to an increase in
moderation job time. We also find that questions closed with the Duplicate and
Off Topic labels are relatively more prone to reputation gaming. For the
'closed' question prediction task, we make use of multiple genres of feature
sets based on - user profile, community process, textual style and question
content. We use a state-of-art machine learning classifier based on an ensemble
learning technique and achieve an overall accuracy of 73%. To the best of our
knowledge, this is the first experimental study to analyze and predict 'closed'
questions on Stack Overflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7296</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7296</id><created>2013-07-27</created><updated>2013-09-20</updated><authors><author><keyname>Mikulski</keyname><forenames>Lukasz</forenames><affiliation>Nicolaus Copernicus University</affiliation></author></authors><title>Algebraic Structure of Combined Traces</title><categories>cs.LO cs.FL</categories><comments>Short variant of this paper, with no proofs, appeared in Proceedings
  of CONCUR 2012 conference</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (August 30,
  2013) lmcs:856</journal-ref><doi>10.2168/LMCS-9(3:8)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traces and their extension called combined traces (comtraces) are two formal
models used in the analysis and verification of concurrent systems. Both models
are based on concepts originating in the theory of formal languages, and they
are able to capture the notions of causality and simultaneity of atomic actions
which take place during the process of a system's operation. The aim of this
paper is a transfer to the domain of comtraces and developing of some
fundamental notions, which proved to be successful in the theory of traces. In
particular, we introduce and then apply the notion of indivisible steps, the
lexicographical canonical form of comtraces, as well as the representation of a
comtrace utilising its linear projections to binary action subalphabets. We
also provide two algorithms related to the new notions. Using them, one can
solve, in an efficient way, the problem of step sequence equivalence in the
context of comtraces. One may view our results as a first step towards the
development of infinite combined traces, as well as recognisable languages of
combined traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7303</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7303</id><created>2013-07-27</created><authors><author><keyname>Mueller</keyname><forenames>Martin E.</forenames></author><author><keyname>Thosar</keyname><forenames>Madhura D.</forenames></author></authors><title>Learning to Understand by Evolving Theories</title><categories>cs.LG cs.AI</categories><comments>KRR Workshop at ICLP 2013</comments><acm-class>D.1.6; I.2.4; I.2.6; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an approach that enables an autonomous system to
infer the semantics of a command (i.e. a symbol sequence representing an
action) in terms of the relations between changes in the observations and the
action instances. We present a method of how to induce a theory (i.e. a
semantic description) of the meaning of a command in terms of a minimal set of
background knowledge. The only thing we have is a sequence of observations from
which we extract what kinds of effects were caused by performing the command.
This way, we yield a description of the semantics of the action and, hence, a
definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7305</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7305</id><created>2013-07-27</created><authors><author><keyname>Brummitt</keyname><forenames>Charles D.</forenames></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>D'Souza</keyname><forenames>Raissa M.</forenames></author></authors><title>Transdisciplinary electric power grid science</title><categories>physics.soc-ph cs.CY</categories><comments>Pages 1--2 are the opinion article published in PNAS 110 (30) 12159
  (2013). Pages 4--10 are supplementary material, with more explanation of the
  main components and feedbacks in electric power systems, as well as a list of
  useful sources of data on power grids</comments><journal-ref>PNAS 110 (30) 12159 (2013)</journal-ref><doi>10.1073/pnas.1309151110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 20th-century engineering feat that most improved the quality of human
life, the electric power system, now faces discipline-spanning challenges that
threaten that distinction. So multilayered and complex that they resemble
ecosystems, power grids face risks from their interdependent cyber, physical,
social and economic layers. Only with a holistic understanding of the dynamics
of electricity infrastructure and human operators, automatic controls,
electricity markets, weather, climate and policy can we fortify worldwide
access to electricity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7307</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7307</id><created>2013-07-27</created><authors><author><keyname>Daadaa</keyname><forenames>Yessine</forenames></author><author><keyname>Jamshed</keyname><forenames>Asif</forenames></author><author><keyname>Shabbir</keyname><forenames>Mudassir</forenames></author></authors><title>Network Decontamination with a Single Agent</title><categories>math.CO cs.DM cs.DS cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faults and viruses often spread in networked environments by propagating from
site to neighboring site. We model this process of {\em network contamination}
by graphs. Consider a graph $G=(V,E)$, whose vertex set is contaminated and our
goal is to decontaminate the set $V(G)$ using mobile decontamination agents
that traverse along the edge set of $G$. Temporal immunity $\tau(G) \ge 0$ is
defined as the time that a decontaminated vertex of $G$ can remain continuously
exposed to some contaminated neighbor without getting infected itself. The
\emph{immunity number} of $G$, $\iota_k(G)$, is the least $\tau$ that is
required to decontaminate $G$ using $k$ agents. We study immunity number for
some classes of graphs corresponding to network topologies and present upper
bounds on $\iota_1(G)$, in some cases with matching lower bounds. Variations of
this problem have been extensively studied in literature, but proposed
algorithms have been restricted to {\em monotone} strategies, where a vertex,
once decontaminated, may not be recontaminated. We exploit nonmonotonicity to
give bounds which are strictly better than those derived using monotone
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7309</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7309</id><created>2013-07-27</created><updated>2013-09-20</updated><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Yun</keyname><forenames>Donggyu</forenames></author><author><keyname>Ok</keyname><forenames>Jungseul</forenames></author><author><keyname>Yi</keyname><forenames>Yung</forenames></author></authors><title>Optimal Rate Sampling in 802.11 Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>52 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 802.11 systems, Rate Adaptation (RA) is a fundamental mechanism allowing
transmitters to adapt the coding and modulation scheme as well as the MIMO
transmission mode to the radio channel conditions, and in turn, to learn and
track the (mode, rate) pair providing the highest throughput. So far, the
design of RA mechanisms has been mainly driven by heuristics. In contrast, in
this paper, we rigorously formulate such design as an online stochastic
optimisation problem. We solve this problem and present ORS (Optimal Rate
Sampling), a family of (mode, rate) pair adaptation algorithms that provably
learn as fast as it is possible the best pair for transmission. We study the
performance of ORS algorithms in both stationary radio environments where the
successful packet transmission probabilities at the various (mode, rate) pairs
do not vary over time, and in non-stationary environments where these
probabilities evolve. We show that under ORS algorithms, the throughput loss
due to the need to explore sub-optimal (mode, rate) pairs does not depend on
the number of available pairs, which is a crucial advantage as evolving 802.11
standards offer an increasingly large number of (mode, rate) pairs. We
illustrate the efficiency of ORS algorithms (compared to the state-of-the-art
algorithms) using simulations and traces extracted from 802.11 test-beds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7322</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7322</id><created>2013-07-27</created><authors><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Reisch</keyname><forenames>Yannick</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Schend</keyname><forenames>Lena</forenames></author></authors><title>Complexity of Manipulation, Bribery, and Campaign Management in Bucklin
  and Fallback Voting</title><categories>cs.GT</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central theme in computational social choice is to study the extent to
which voting systems computationally resist manipulative attacks seeking to
influence the outcome of elections, such as manipulation (i.e., strategic
voting), control, and bribery. Bucklin and fallback voting are among the voting
systems with the broadest resistance (i.e., NP-hardness) to control attacks.
However, only little is known about their behavior regarding manipulation and
bribery attacks. We comprehensively investigate the computational resistance of
Bucklin and fallback voting for many of the common manipulation and bribery
scenarios; we also complement our discussion by considering several campaign
management problems for Bucklin and fallback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7326</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7326</id><created>2013-07-27</created><authors><author><keyname>Li</keyname><forenames>Zhong</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Yang</keyname><forenames>Siqian</forenames></author><author><keyname>Jiang</keyname><forenames>Changjun</forenames></author><author><keyname>Stojmenovic</keyname><forenames>Ivan</forenames></author></authors><title>Improving Data Forwarding in Mobile Social Networks with Infrastructure
  Support: A Space-Crossing Community Approach</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we study two tightly coupled issues: space-crossing community
detection and its influence on data forwarding in Mobile Social Networks (MSNs)
by taking the hybrid underlying networks with infrastructure support into
consideration. The hybrid underlying network is composed of large numbers of
mobile users and a small portion of Access Points (APs). Because APs can
facilitate the communication among long-distance nodes, the concept of physical
proximity community can be extended to be one across the geographical space. In
this work, we first investigate a space-crossing community detection method for
MSNs. Based on the detection results, we design a novel data forwarding
algorithm SAAS (Social Attraction and AP Spreading), and show how to exploit
the space-crossing communities to improve the data forwarding efficiency. We
evaluate our SAAS algorithm on real-life data from MIT Reality Mining and UIM.
Results show that space-crossing community plays a positive role in data
forwarding in MSNs in terms of deliver ratio and delay. Based on this new type
of community, SAAS achieves a better performance than existing social
community-based data forwarding algorithms in practice, including Bubble Rap
and Nguyen's Routing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7328</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7328</id><created>2013-07-27</created><authors><author><keyname>Alhyasat</keyname><forenames>Eiad Basher</forenames></author><author><keyname>Al-Dalahmeh</keyname><forenames>Mahmoud</forenames></author></authors><title>Data Warehouse Success and Strategic Oriented Business Intelligence: A
  Theoretical Framework</title><categories>cs.DB</categories><journal-ref>Journal of Management Research 5(3), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of the data warehouses as supportive decision making
tools, organizations are increasingly looking forward for a complete data
warehouse success model that would manage the enormous amounts of growing data.
It is therefore important to measure the success of these massive projects.
While general IS success models have received great deals of attention, few
research has been conducted to assess the success of data warehouses for
strategic business intelligence purposes. The framework developed in this study
consists of the following nine measures: Vendors and Consultants, Management
Actions, System Quality, Information Quality, Data Warehouse Usage, Perceived
utility, Individual Decision Making Impact, Organizational Decision Making
Impact, and Corporate Strategic Goals Attainment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7331</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7331</id><created>2013-07-28</created><updated>2014-06-22</updated><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Mao</keyname><forenames>Wenli</forenames></author><author><keyname>Liu</keyname><forenames>Chen</forenames></author></authors><title>How far does scientific community look back?</title><categories>cs.DL physics.data-an physics.soc-ph</categories><comments>11 pages, 7 figures</comments><journal-ref>Journal of Informetrics, 2014, 8(3), 562-568</journal-ref><doi>10.1016/j.joi.2014.04.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does the published scientific literature used by scientific community?
Many previous studies make analysis on the static usage data. In this research,
we propose the concept of dynamic usage data. Based on the platform of
realtime.springer.com, we have been monitoring and recording the dynamic usage
data of Scientometrics articles round the clock. Our analysis find that papers
published in recent four years have many more downloads than papers published
four years ago. According to our quantitative calculation, papers down-loaded
on one day have an average lifetime of 4.1 years approximately. Classic papers
are still being downloaded frequently even long after their publication.
Additionally, we find that social media may reboot the attention of old
scientific literature in a short time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7332</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7332</id><created>2013-07-28</created><authors><author><keyname>Kurve</keyname><forenames>Aditya</forenames></author><author><keyname>Miller</keyname><forenames>David J</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Multicategory Crowdsourcing Accounting for Plurality in Worker Skill and
  Intention, Task Difficulty, and Task Heterogeneity</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing allows to instantly recruit workers on the web to annotate
image, web page, or document databases. However, worker unreliability prevents
taking a workers responses at face value. Thus, responses from multiple workers
are typically aggregated to more reliably infer ground-truth answers. We study
two approaches for crowd aggregation on multicategory answer spaces stochastic
modeling based and deterministic objective function based. Our stochastic model
for answer generation plausibly captures the interplay between worker skills,
intentions, and task difficulties and allows us to model a broad range of
worker types. Our deterministic objective based approach does not assume a
model for worker response generation. Instead, it aims to maximize the average
aggregate confidence of weighted plurality crowd decision making. In both
approaches, we explicitly model the skill and intention of individual workers,
which is exploited for improved crowd aggregation. Our methods are applicable
in both unsupervised and semisupervised settings, and also when the batch of
tasks is heterogeneous. As observed experimentally, the proposed methods can
defeat tyranny of the masses, they are especially advantageous when there is a
minority of skilled workers amongst a large crowd of unskilled and malicious
workers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7340</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7340</id><created>2013-07-28</created><updated>2014-02-11</updated><authors><author><keyname>Zhu</keyname><forenames>Ruihao</forenames></author><author><keyname>Liu</keyname><forenames>Dongxin</forenames></author><author><keyname>Wu</keyname><forenames>Fan</forenames></author><author><keyname>Chen</keyname><forenames>and Guihai</forenames></author></authors><title>PRINCE: Privacy-Preserving Mechanisms for Influence Diffusion in Online
  Social Networks</title><categories>cs.SI cs.GT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial sign error in
equation 1. With the advance of online social networks, there has been
extensive research on how to spread influence in online social networks, and
many algorithms and models have been proposed. However, many fundamental
problems have also been overlooked. Among those, the most important problems
are the incentive aspect and the privacy aspect (eg, nodes' relationships) of
the influence propagation in online social networks. Bearing these defects in
mind, and incorporating the powerful tool from differential privacy, we propose
PRINCE, which is a series of \underline{PR}ivacy preserving mechanisms for
\underline{IN}fluen\underline{CE} diffusion in online social networks to solve
the problems. We not only theoretically prove many elegant properties of
PRINCE, but also implement PRINCE to evaluate its performance extensively. The
evaluation results show that PRINCE achieves good performances. To the best of
our knowledge, PRINCE is the first differentially private mechanism for
influence diffusion in online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7342</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7342</id><created>2013-07-28</created><authors><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Multi-command Chest Tactile Brain Computer Interface for Small Vehicle
  Robot Navigation</title><categories>q-bio.NC cs.HC cs.RO</categories><comments>accepted as a full paper for The 2013 International Conference on
  Brain and Health Informatics; to appear in Lecture Notes in Computer Science
  (LNCS), Springer Verlag Berlin Heidelberg, 2013; http://link.springer.com/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presented study explores the extent to which tactile stimuli delivered to
five chest positions of a healthy user can serve as a platform for a brain
computer interface (BCI) that could be used in an interactive application such
as robotic vehicle operation. The five chest locations are used to evoke
tactile brain potential responses, thus defining a tactile brain computer
interface (tBCI). Experimental results with five subjects performing online
tBCI provide a validation of the chest location tBCI paradigm, while the
feasibility of the concept is illuminated through information-transfer rates.
Additionally an offline classification improvement with a linear SVM classifier
is presented through the case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7345</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7345</id><created>2013-07-28</created><authors><author><keyname>Ilya</keyname><forenames>Fiks</forenames></author></authors><title>A novel method based on the Tikhonov functional for non-negative
  solution of a system of linear equations with non-negative coefficients</title><categories>cs.NA math.NA</categories><comments>13 pages</comments><msc-class>15A09, 15A29, 65F22</msc-class><acm-class>F.2.1; G.1.0</acm-class><doi>10.1142/S0219876213500710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for a solution of a system of linear equations with
the non-negativity condition. The method is based on the Tikhonov functional
and has better accuracy and stability than other well-known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7351</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7351</id><created>2013-07-28</created><updated>2013-08-01</updated><authors><author><keyname>Bastianelli</keyname><forenames>Emanuele</forenames></author><author><keyname>Bloisi</keyname><forenames>Domenico</forenames></author><author><keyname>Capobianco</keyname><forenames>Roberto</forenames></author><author><keyname>Gemignani</keyname><forenames>Guglielmo</forenames></author><author><keyname>Iocchi</keyname><forenames>Luca</forenames></author><author><keyname>Nardi</keyname><forenames>Daniele</forenames></author></authors><title>Knowledge Representation for Robots through Human-Robot Interaction</title><categories>cs.AI cs.RO</categories><comments>Knowledge Representation and Reasoning in Robotics Workshop at ICLP
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The representation of the knowledge needed by a robot to perform complex
tasks is restricted by the limitations of perception. One possible way of
overcoming this situation and designing &quot;knowledgeable&quot; robots is to rely on
the interaction with the user. We propose a multi-modal interaction framework
that allows to effectively acquire knowledge about the environment where the
robot operates. In particular, in this paper we present a rich representation
framework that can be automatically built from the metric map annotated with
the indications provided by the user. Such a representation, allows then the
robot to ground complex referential expressions for motion commands and to
devise topological navigation plans to achieve the target locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7364</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7364</id><created>2013-07-28</created><updated>2015-11-15</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Hod</keyname><forenames>Rani</forenames></author><author><keyname>Weinstein</keyname><forenames>Amit</forenames></author></authors><title>On active and passive testing</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>16 pages</comments><journal-ref>Combinator. Probab. Comp. 25 (2016) 1-20</journal-ref><doi>10.1017/S0963548315000292</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a property of Boolean functions, what is the minimum number of queries
required to determine with high probability if an input function satisfies this
property or is &quot;far&quot; from satisfying it? This is a fundamental question in
Property Testing, where traditionally the testing algorithm is allowed to pick
its queries among the entire set of inputs. Balcan, Blais, Blum and Yang have
recently suggested to restrict the tester to take its queries from a smaller
random subset of polynomial size of the inputs. This model is called active
testing, and in the extreme case when the size of the set we can query from is
exactly the number of queries performed it is known as passive testing.
  We prove that passive or active testing of k-linear functions (that is, sums
of k variables among n over Z_2) requires Theta(k*log n) queries, assuming k is
not too large. This extends the case k=1, (that is, dictator functions),
analyzed by Balcan et. al.
  We also consider other classes of functions including low degree polynomials,
juntas, and partially symmetric functions. Our methods combine algebraic,
combinatorial, and probabilistic techniques, including the Talagrand
concentration inequality and the Erdos--Rado theorem on Delta-systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7365</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7365</id><created>2013-07-28</created><authors><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Bit of Secrecy for Gaussian Source Compression</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the compression of an independent and identically distributed
Gaussian source sequence is studied in an unsecure network. Within a game
theoretic setting for a three-party noiseless communication network (sender
Alice, legitimate receiver Bob, and eavesdropper Eve), the problem of how to
efficiently compress a Gaussian source with limited secret key in order to
guarantee that Bob can reconstruct with high fidelity while preventing Eve from
estimating an accurate reconstruction is investigated. It is assumed that Alice
and Bob share a secret key with limited rate. Three scenarios are studied, in
which the eavesdropper ranges from weak to strong in terms of the causal side
information she has. It is shown that one bit of secret key per source symbol
is enough to achieve perfect secrecy performance in the Gaussian squared error
setting, and the information theoretic region is not optimized by joint
Gaussian random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7375</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7375</id><created>2013-07-28</created><updated>2013-08-22</updated><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Flow-level performance of random wireless networks</title><categories>cs.IT math.IT</categories><comments>technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the flow-level performance of random wireless networks. The
locations of base stations (BSs) follow a Poisson point process. The number and
positions of active users are dynamic. We associate a queue to each BS. The
performance and stability of a BS depend on its load. In some cases, the full
distribution of the load can be derived. Otherwise we derive formulas for the
first and second moments. Networks on the line and on the plane are considered.
Our model is generic enough to include features of recent wireless networks
such as 4G (LTE) networks. In dense networks, we show that the inter-cell
interference power becomes normally distributed, simplifying many computations.
Numerical experiments demonstrate that in cases of practical interest, the
loads distribution can be well approximated by a gamma distribution with known
mean and variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7382</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7382</id><created>2013-07-28</created><authors><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author></authors><title>Learning Frames from Text with an Unsupervised Latent Variable Model</title><categories>cs.CL</categories><comments>21 pages; technical report for Data Analysis Project requirement,
  Machine Learning Department, Carnegie Mellon University</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We develop a probabilistic latent-variable model to discover semantic
frames---types of events and their participants---from corpora. We present a
Dirichlet-multinomial model in which frames are latent categories that explain
the linking of verb-subject-object triples, given document-level sparsity. We
analyze what the model learns, and compare it to FrameNet, noting it learns
some novel and interesting frames. This document also contains a discussion of
inference issues, including concentration parameter learning; and a small-scale
error analysis of syntactic parsing accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7385</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7385</id><created>2013-07-28</created><authors><author><keyname>Albert</keyname><forenames>Reka</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Mobasheri</keyname><forenames>Nasim</forenames></author></authors><title>Some Perspectives on Network Modeling in Therapeutic Target Prediction</title><categories>q-bio.MN cs.CE cs.DM q-bio.QM</categories><msc-class>92C42, 05C40, 05C85, 68R10</msc-class><acm-class>G.2.2; J.3</acm-class><journal-ref>Biomedical Engineering and Computational Biology, 5, 17-24, 2013</journal-ref><doi>10.4137/BECB.S10793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drug target identification is of significant commercial interest to
pharmaceutical companies, and there is a vast amount of research done related
to the topic of therapeutic target identification. Interdisciplinary research
in this area involves both the biological network community and the graph
algorithms community. Key steps of a typical therapeutic target identification
problem include synthesizing or inferring the complex network of interactions
relevant to the disease, connecting this network to the disease-specific
behavior, and predicting which components are key mediators of the behavior.
All of these steps involve graph theoretical or graph algorithmic aspects. In
this perspective, we provide modelling and algorithmic perspectives for
therapeutic target identification and highlight a number of algorithmic
advances, which have gotten relatively little attention so far, with the hope
of strengthening the ties between these two research communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7389</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7389</id><created>2013-07-28</created><authors><author><keyname>Colman</keyname><forenames>ER</forenames></author><author><keyname>Rodgers</keyname><forenames>GJ</forenames></author></authors><title>Complex scale-free networks with tunable power-law exponent and
  clustering</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 10 figures, accepted journal paper</comments><doi>10.1016/j.physa.2013.06.063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a network evolution process motivated by the network of
citations in the scientific literature. In each iteration of the process a node
is born and directed links are created from the new node to a set of target
nodes already in the network. This set includes $m$ &quot;ambassador&quot; nodes and $l$
of each ambassador's descendants where $m$ and $l$ are random variables
selected from any choice of distributions $p_{l}$ and $q_{m}$. The process
mimics the tendency of authors to cite varying numbers of papers included in
the bibliographies of the other papers they cite. We show that the degree
distributions of the networks generated after a large number of iterations are
scale-free and derive an expression for the power-law exponent. In a particular
case of the model where the number of ambassadors is always the constant $m$
and the number of selected descendants from each ambassador is the constant
$l$, the power-law exponent is $(2l+1)/l$. For this example we derive
expressions for the degree distribution and clustering coefficient in terms of
$l$ and $m$. We conclude that the proposed model can be tuned to have the same
power law exponent and clustering coefficient of a broad range of the
scale-free distributions that have been studied empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7398</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7398</id><created>2013-07-28</created><authors><author><keyname>Andres</keyname><forenames>Benjamin</forenames></author><author><keyname>Obermeier</keyname><forenames>Philipp</forenames></author><author><keyname>Sabuncu</keyname><forenames>Orkunt</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Rajaratnam</keyname><forenames>David</forenames></author></authors><title>ROSoClingo: A ROS package for ASP-based robot control</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge representation and reasoning capacities are vital to cognitive
robotics because they provide higher level cognitive functions for reasoning
about actions, environments, goals, perception, etc. Although Answer Set
Programming (ASP) is well suited for modelling such functions, there was so far
no seamless way to use ASP in a robotic environment. We address this
shortcoming and show how a recently developed reactive ASP system can be
harnessed to provide appropriate reasoning capacities within a robotic system.
To be more precise, we furnish a package integrating the reactive ASP solver
oClingo with the popular open-source robotic middleware ROS. The resulting
system, ROSoClingo, provides a generic way by which an ASP program can be used
to control the behaviour of a robot and to respond to the results of the
robot's actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7401</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7401</id><created>2013-07-28</created><updated>2013-10-18</updated><authors><author><keyname>Dar</keyname><forenames>Ronen</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author><author><keyname>Mecozzi</keyname><forenames>Antonio</forenames></author><author><keyname>Shtaif</keyname><forenames>Mark</forenames></author></authors><title>Properties of nonlinear noise in long, dispersion-uncompensated fiber
  links</title><categories>physics.optics cs.IT math.IT</categories><comments>15 pages, 5 figures</comments><doi>10.1364/OE.21.025685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the properties of nonlinear interference noise (NLIN) in fiber-optic
communications systems with large accumulated dispersion. Our focus is on
settling the discrepancy between the results of the Gaussian noise (GN) model
(according to which NLIN is additive Gaussian) and a recently published
time-domain analysis, which attributes drastically different properties to the
NLIN. Upon reviewing the two approaches we identify several unjustified
assumptions that are key in the derivation of the GN model, and that are
responsible for the discrepancy. We derive the true NLIN power and verify that
the NLIN is not additive Gaussian, but rather it depends strongly on the data
transmitted in the channel of interest. In addition we validate the time-domain
model numerically and demonstrate the strong dependence of the NLIN on the
interfering channels' modulation format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7405</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7405</id><created>2013-07-28</created><authors><author><keyname>Wa&#x142;&#x229;ga</keyname><forenames>P. A.</forenames></author></authors><title>Reasoning for Moving Blocks Problem: Formal Representation and
  Implementation</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combined approach of the Qualitative Reasoning and Probabilistic
Functions for the knowledge representation is proposed. The method aims at
represent uncertain, qualitative knowledge that is essential for the moving
blocks task's execution. The attempt to formalize the commonsense knowledge is
performed with the Situation Calculus language for reasoning and robot's
beliefs representation. The method is implemented in the Prolog programming
language and tested for a specific simulated scenario. In most cases the
implementation enables us to solve a given task, i.e., move blocks to desired
positions. The example of robot's reasoning and main parts of the implemented
program's code are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7411</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7411</id><created>2013-07-28</created><updated>2013-08-14</updated><authors><author><keyname>Dhifli</keyname><forenames>Wajdi</forenames></author><author><keyname>Moussaoui</keyname><forenames>Mohamed</forenames></author><author><keyname>Saidi</keyname><forenames>Rabie</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>Towards an Efficient Discovery of the Topological Representative
  Subgraphs</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of graph databases, the task of frequent subgraph
discovery has been extensively addressed. Although the proposed approaches in
the literature have made this task feasible, the number of discovered frequent
subgraphs is still very high to be efficiently used in any further exploration.
Feature selection for graph data is a way to reduce the high number of frequent
subgraphs based on exact or approximate structural similarity. However, current
structural similarity strategies are not efficient enough in many real-world
applications, besides, the combinatorial nature of graphs makes it
computationally very costly. In order to select a smaller yet structurally
irredundant set of subgraphs, we propose a novel approach that mines the top-k
topological representative subgraphs among the frequent ones. Our approach
allows detecting hidden structural similarities that existing approaches are
unable to detect such as the density or the diameter of the subgraph. In
addition, it can be easily extended using any user defined structural or
topological attributes depending on the sought properties. Empirical studies on
real and synthetic graph datasets show that our approach is fast and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7416</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7416</id><created>2013-07-28</created><authors><author><keyname>Cui</keyname><forenames>Wenzhi</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author></authors><title>DiFS: Distributed Flow Scheduling for Data Center Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data center networks leverage multiple parallel paths connecting end host
pairs to offer high bisection bandwidth for cluster computing applications.
However, state of the art distributed multi-pathing protocols such as Equal
Cost Multipath (ECMP) use static flow-to-link assignment, which is
load-oblivious. They may cause bandwidth loss due to \emph{flow collisions} on
a same link. Recently proposed centralized scheduling algorithm or host-based
multi-pathing may suffer from scalability problems.
  In this paper, we present Distributed Flow Scheduling (DiFS) for data center
networks, which is a switch-only distributed solution. DiFS allows switches
cooperate to avoid over-utilized links and find available paths without
centralized control. DiFS is scalable and can react quickly to dynamic traffic,
because it is independently executed on switches and requires no
synchronization. Extensive experiments show that the aggregate bisection
bandwidth of DiFS using various traffic patterns is much better than that of
ECMP, and is similar to or higher than that of a recent proposed centralized
scheduling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7429</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7429</id><created>2013-07-28</created><authors><author><keyname>Sangar</keyname><forenames>Amin Babazadeh</forenames></author><author><keyname>Khaze</keyname><forenames>Seyyed Reza</forenames></author><author><keyname>Ebrahimi</keyname><forenames>Laya</forenames></author></authors><title>Participation anticipating in elections using data mining methods</title><categories>cs.CY cs.LG</categories><journal-ref>International Journal on Cybernetics &amp; Informatics ( IJCI) Vol.2,
  No.2, April2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anticipating the political behavior of people will be considerable help for
election candidates to assess the possibility of their success and to be
acknowledged about the public motivations to select them. In this paper, we
provide a general schematic of the architecture of participation anticipating
system in presidential election by using KNN, Classification Tree and Na\&quot;ive
Bayes and tools orange based on crisp which had hopeful output. To test and
assess the proposed model, we begin to use the case study by selecting 100
qualified persons who attend in 11th presidential election of Islamic republic
of Iran and anticipate their participation in Kohkiloye &amp; Boyerahmad. We
indicate that KNN can perform anticipation and classification processes with
high accuracy in compared with two other algorithms to anticipate
participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7430</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7430</id><created>2013-07-28</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Williams</keyname><forenames>Tyson</forenames></author></authors><title>Holographic Algorithms Beyond Matchgates</title><categories>cs.DS cs.CC</categories><comments>35 pages, 1 figure</comments><msc-class>68Q25</msc-class><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holographic algorithms were first introduced by Valiant as a new methodology
to derive polynomial time algorithms. The algorithms introduced by Valiant are
based on matchgates, which are intrinsically for problems over planar
structures. In this paper we introduce two new families of holographic
algorithms. These algorithms work over general, i.e., not necessarily planar,
graphs. Instead of matchgates, the two underlying families of constraint
functions are of the affine type and of the product type. These play the role
of Kasteleyn's algorithm for counting planar perfect matchings. The new
algorithms are obtained by transforming a problem to one of these two families
by holographic reductions.
  The tractability of affine and product type constraint functions is known.
The real challenge is to determine when some concrete problem, expressed by its
constraint functions, has such a holographic reduction. We present a polynomial
time algorithm to decide if a given counting problem has a holographic
algorithm using the affine or product type constraint functions. Our algorithm
also finds a holographic transformation when one exists. We exhibit concrete
problems that can be solved by the new holographic algorithms. When the
constraint functions are symmetric, we further present a polynomial time
algorithm for the same decision and search problems, where the complexity is
measured in terms of the (exponentially more) succinct presentation of
symmetric constraint functions. The algorithm for the symmetric case also shows
that the recent dichotomy theorem for Holant problems with symmetric
constraints is efficiently decidable. Our proof techniques are mainly
algebraic, e.g., stabilizers and orbits of group actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7432</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7432</id><created>2013-07-28</created><authors><author><keyname>Gharehchopogh</keyname><forenames>Farhad Soleimanian</forenames></author><author><keyname>Khaze</keyname><forenames>Seyyed Reza</forenames></author></authors><title>Data mining application for cyber space users tendency in blog writing:
  a case study</title><categories>cs.CY cs.LG</categories><journal-ref>International Journal of Computer Applications (IJCA), Vol.47,
  No.18, June2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blogs are the recent emerging media which relies on information technology
and technological advance. Since the mass media in some less-developed and
developing countries are in government service and their policies are developed
based on governmental interests, so blogs are provided for ideas and exchanging
opinions. In this paper, we highlighted performed simulations from obtained
information from 100 users and bloggers in Kohkiloye and Boyer Ahmad Province
and using Weka 3.6 tool and c4.5 algorithm by applying decision tree with more
than %82 precision for getting future tendency anticipation of users to
blogging and using in strategically areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7433</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7433</id><created>2013-07-28</created><authors><author><keyname>Chen</keyname><forenames>Zhili</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author><author><keyname>Li</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Miao</keyname><forenames>Haibo</forenames></author><author><keyname>Tian</keyname><forenames>Miaomiao</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author></authors><title>PS-TRUST: Provably Secure Solution for Truthful Double Spectrum Auctions</title><categories>cs.CR cs.GT</categories><comments>9 pages, 4 figures, submitted to Infocom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Truthful spectrum auctions have been extensively studied in recent years.
Truthfulness makes bidders bid their true valuations, simplifying greatly the
analysis of auctions. However, revealing one's true valuation causes severe
privacy disclosure to the auctioneer and other bidders. To make things worse,
previous work on secure spectrum auctions does not provide adequate security.
In this paper, based on TRUST, we propose PS-TRUST, a provably secure solution
for truthful double spectrum auctions. Besides maintaining the properties of
truthfulness and special spectrum reuse of TRUST, PS-TRUST achieves provable
security against semi-honest adversaries in the sense of cryptography.
Specifically, PS-TRUST reveals nothing about the bids to anyone in the auction,
except the auction result. To the best of our knowledge, PS-TRUST is the first
provably secure solution for spectrum auctions. Furthermore, experimental
results show that the computation and communication overhead of PS-TRUST is
modest, and its practical applications are feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7435</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7435</id><created>2013-07-28</created><authors><author><keyname>Gharehchopogh</keyname><forenames>Farhad Soleimanian</forenames></author><author><keyname>Maleki</keyname><forenames>Isa</forenames></author><author><keyname>Khaze</keyname><forenames>Seyyed Reza</forenames></author></authors><title>A new approach in dynamic traveling salesman problem: a hybrid of ant
  colony optimization and descending gradient</title><categories>cs.NE</categories><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT), Vol. 3, No. 2, December 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays swarm intelligence-based algorithms are being used widely to
optimize the dynamic traveling salesman problem (DTSP). In this paper, we have
used mixed method of Ant Colony Optimization (AOC)and gradient descent to
optimize DTSP which differs with ACO algorithm in evaporation rate and
innovative data. This approach prevents premature convergence and scape from
local optimum spots and also makes it possible to find better solutions for
algorithm. In this paper, we are going to offer gradient descent and ACO
algorithm which in comparison to some former methods it shows that algorithm
has significantly improved routes optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7440</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7440</id><created>2013-07-28</created><authors><author><keyname>Cervesato</keyname><forenames>Iliano</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Sacchini</keyname><forenames>Jorge Luis</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Towards Meta-Reasoning in the Concurrent Logical Framework CLF</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 120, 2013, pp. 2-16</journal-ref><doi>10.4204/EPTCS.120.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concurrent logical framework CLF is an extension of the logical framework
LF designed to specify concurrent and distributed languages. While it can be
used to define a variety of formalisms, reasoning about such languages within
CLF has proved elusive. In this paper, we propose an extension of LF that
allows us to express properties of CLF specifications. We illustrate the
approach with a proof of safety for a small language with a parallel semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7441</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7441</id><created>2013-07-28</created><authors><author><keyname>Arbach</keyname><forenames>Youssef</forenames><affiliation>Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Peters</keyname><forenames>Kirstin</forenames><affiliation>Technische Universit&#xe4;t Berlin, Germany</affiliation></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames><affiliation>Technische Universit&#xe4;t Berlin, Germany</affiliation></author></authors><title>Adding Priority to Event Structures</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 120, 2013, pp. 17-31</journal-ref><doi>10.4204/EPTCS.120.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event Structures (ESs) are mainly concerned with the representation of causal
relationships between events, usually accompanied by other event relations
capturing conflicts and disabling. Among the most prominent variants of ESs are
Prime ESs, Bundle ESs, Stable ESs, and Dual ESs, which differ in their
causality models and event relations. Yet, some application domains require
further kinds of relations between events. Here, we add the possibility to
express priority relationships among events.
  We exemplify our approach on Prime, Bundle, Extended Bundle, and Dual ESs.
Technically, we enhance these variants in the same way. For each variant, we
then study the interference between priority and the other event relations.
From this, we extract the redundant priority pairs-notably differing for the
types of ESs-that enable us to provide a comparison between the extensions. We
also exhibit that priority considerably complicates the definition of partial
orders in ESs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7442</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7442</id><created>2013-07-28</created><authors><author><keyname>Gebler</keyname><forenames>Daniel</forenames><affiliation>Department of Computer Science, VU University Amsterdam</affiliation></author><author><keyname>Tini</keyname><forenames>Simone</forenames><affiliation>Department of Scienza e Alta Tecnologia, University of Insubria</affiliation></author></authors><title>Compositionality of Approximate Bisimulation for Probabilistic Systems</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.2</acm-class><journal-ref>EPTCS 120, 2013, pp. 32-46</journal-ref><doi>10.4204/EPTCS.120.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic transition system specifications using the rule format
ntmuft-ntmuxt provide structural operational semantics for Segala-type systems
and guarantee that probabilistic bisimilarity is a congruence. Probabilistic
bisimilarity is for many applications too sensitive to the exact probabilities
of transitions. Approximate bisimulation provides a robust semantics that is
stable with respect to implementation and measurement errors of probabilistic
behavior. We provide a general method to quantify how much a process combinator
expands the approximate bisimulation distance. As a direct application we
derive an appropriate rule format that guarantees compositionality with respect
to approximate bisimilarity. Moreover, we describe how specification formats
for non-standard compositionality requirements may be derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7443</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7443</id><created>2013-07-28</created><authors><author><keyname>Guha</keyname><forenames>Shibashis</forenames><affiliation>Indian Institute of Technology Delhi</affiliation></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames><affiliation>Indian Institute of Technology Bombay</affiliation></author><author><keyname>Narayan</keyname><forenames>Chinmay</forenames><affiliation>Indian Institute of Technology Delhi</affiliation></author><author><keyname>Arun-Kumar</keyname><forenames>S.</forenames><affiliation>Indian Institute of Technology Delhi</affiliation></author></authors><title>A Unifying Approach to Decide Relations for Timed Automata and their
  Game Characterization</title><categories>cs.FL cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 120, 2013, pp. 47-62</journal-ref><doi>10.4204/EPTCS.120.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a unifying approach for deciding various
bisimulations, simulation equivalences and preorders between two timed automata
states. We propose a zone based method for deciding these relations in which we
eliminate an explicit product construction of the region graphs or the zone
graphs as in the classical methods. Our method is also generic and can be used
to decide several timed relations. We also present a game characterization for
these timed relations and show that the game hierarchy reflects the hierarchy
of the timed relations. One can obtain an infinite game hierarchy and thus the
game characterization further indicates the possibility of defining new timed
relations which have not been studied yet. The game characterization also helps
us to come up with a formula which encodes the separation between two states
that are not timed bisimilar. Such distinguishing formulae can also be
generated for many relations other than timed bisimilarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7444</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7444</id><created>2013-07-28</created><authors><author><keyname>Gebler</keyname><forenames>Daniel</forenames><affiliation>Department of Computer Science, VU University Amsterdam</affiliation></author><author><keyname>Goriac</keyname><forenames>Eugen-Ioan</forenames><affiliation>ICE-TCS, School of Computer Science, Reykjavik University, Iceland</affiliation></author><author><keyname>Mousavi</keyname><forenames>Mohammad Reza</forenames><affiliation>Center for Research on Embedded Systems</affiliation></author></authors><title>Algebraic Meta-Theory of Processes with Data</title><categories>cs.LO cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 120, 2013, pp. 63-77</journal-ref><doi>10.4204/EPTCS.120.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exists a rich literature of rule formats guaranteeing different
algebraic properties for formalisms with a Structural Operational Semantics.
Moreover, there exist a few approaches for automatically deriving
axiomatizations characterizing strong bisimilarity of processes. To our
knowledge, this literature has never been extended to the setting with data
(e.g. to model storage and memory). We show how the rule formats for algebraic
properties can be exploited in a generic manner in the setting with data.
Moreover, we introduce a new approach for deriving sound and ground-complete
axiom schemata for a notion of bisimilarity with data, called stateless
bisimilarity, based on intuitive auxiliary function symbols for handling the
store component. We do restrict, however, the axiomatization to the setting
where the store component is only given in terms of constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7445</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7445</id><created>2013-07-28</created><authors><author><keyname>Cuijpers</keyname><forenames>P. J. L.</forenames><affiliation>Technische Universiteit Eindhoven</affiliation></author></authors><title>The categorical limit of a sequence of dynamical systems</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 120, 2013, pp. 78-92</journal-ref><doi>10.4204/EPTCS.120.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling a sequence of design steps, or a sequence of parameter settings,
yields a sequence of dynamical systems. In many cases, such a sequence is
intended to approximate a certain limit case. However, formally defining that
limit turns out to be subject to ambiguity. Depending on the interpretation of
the sequence, i.e. depending on how the behaviors of the systems in the
sequence are related, it may vary what the limit should be. Topologies, and in
particular metrics, define limits uniquely, if they exist. Thus they select one
interpretation implicitly and leave no room for other interpretations. In this
paper, we define limits using category theory, and use the mentioned relations
between system behaviors explicitly. This resolves the problem of ambiguity in
a more controlled way. We introduce a category of prefix orders on executions
and partial history preserving maps between them to describe both discrete and
continuous branching time dynamics. We prove that in this category all
projective limits exist, and illustrate how ambiguity in the definition of
limits is resolved using an example. Moreover, we show how various problems
with known topological approaches are now resolved, and how the construction of
projective limits enables us to approximate continuous time dynamics as a
sequence of discrete time systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7446</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7446</id><created>2013-07-28</created><authors><author><keyname>Aceto</keyname><forenames>Luca</forenames><affiliation>ICE-TCS, School of Computer Science, Reykjavik University, Iceland</affiliation></author><author><keyname>Goriac</keyname><forenames>Eugen-Ioan</forenames><affiliation>ICE-TCS, School of Computer Science, Reykjavik University, Iceland</affiliation></author><author><keyname>Ingolfsdottir</keyname><forenames>Anna</forenames><affiliation>ICE-TCS, School of Computer Science, Reykjavik University, Iceland</affiliation></author></authors><title>Meta SOS - A Maude Based SOS Meta-Theory Framework</title><categories>cs.LO cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2013, arXiv:1307.6903</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 120, 2013, pp. 93-107</journal-ref><doi>10.4204/EPTCS.120.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meta SOS is a software framework designed to integrate the results from the
meta-theory of structural operational semantics (SOS). These results include
deriving semantic properties of language constructs just by syntactically
analyzing their rule-based definition, as well as automatically deriving sound
and ground-complete axiomatizations for languages, when considering a notion of
behavioural equivalence. This paper describes the Meta SOS framework by
blending aspects from the meta-theory of SOS, details on their implementation
in Maude, and running examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7447</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7447</id><created>2013-07-28</created><authors><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Biao</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Wireless Information and Power Transfer in Two-Way Amplify-and-Forward
  Relaying Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The various wireless networks have made the ambient radio frequency signals
around the world. Wireless information and power transfer enables the devices
to recycle energy from these ambient radio frequency signals and process
information simultaneously. In this paper, we develop a wireless information
and power transfer protocol in two-way amplify-and-forward relaying channels,
where two sources exchange information via an energy harvesting relay node. The
relay node collects energy from the received signals and uses it to provide the
transmission power to forward the received signals. We analytically derive the
exact expressions of the outage probability, the ergodic capacity and the
finite-SNR diversity-multiplexing trade-off (DMT). Furthermore, the tight
closed-form upper and lower bounds of the outage probability and the ergodic
capacity are then developed. Moreover, the impact of the power splitting ratio
is also evaluated and analyzed. Finally, we show that compared to the
non-cooperative relaying scheme, the proposed protocol is a green solution to
offer higher transmission rate and more reliable communication without
consuming additional resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7451</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7451</id><created>2013-07-28</created><updated>2013-12-08</updated><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Li</keyname><forenames>Baochun</forenames></author></authors><title>RepFlow: Minimizing Flow Completion Times with Replicated Flows in Data
  Centers</title><categories>cs.NI cs.DC cs.PF</categories><comments>To appear in IEEE INFOCOM 2014</comments><acm-class>C.2.1; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short TCP flows that are critical for many interactive applications in data
centers are plagued by large flows and head-of-line blocking in switches.
Hash-based load balancing schemes such as ECMP aggravate the matter and result
in long-tailed flow completion times (FCT). Previous work on reducing FCT
usually requires custom switch hardware and/or protocol changes. We propose
RepFlow, a simple yet practically effective approach that replicates each short
flow to reduce the completion times, without any change to switches or host
kernels. With ECMP the original and replicated flows traverse distinct paths
with different congestion levels, thereby reducing the probability of having
long queueing delay. We develop a simple analytical model to demonstrate the
potential improvement of RepFlow. Extensive NS-3 simulations and Mininet
implementation show that RepFlow provides 50%--70% speedup in both mean and
99-th percentile FCT for all loads, and offers near-optimal FCT when used with
DCTCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7454</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7454</id><created>2013-07-28</created><updated>2013-08-21</updated><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Relative Errors for Deterministic Low-Rank Matrix Approximations</title><categories>cs.DS cs.NA</categories><comments>16 pages, 0 figures</comments><msc-class>68W40 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider processing an n x d matrix A in a stream with row-wise updates
according to a recent algorithm called Frequent Directions (Liberty, KDD 2013).
This algorithm maintains an l x d matrix Q deterministically, processing each
row in O(d l^2) time; the processing time can be decreased to O(d l) with a
slight modification in the algorithm and a constant increase in space. We show
that if one sets l = k+ k/eps and returns Q_k, a k x d matrix that is the best
rank k approximation to Q, then we achieve the following properties: ||A -
A_k||_F^2 &lt;= ||A||_F^2 - ||Q_k||_F^2 &lt;= (1+eps) ||A - A_k||_F^2 and where
pi_{Q_k}(A) is the projection of A onto the rowspace of Q_k then ||A -
pi_{Q_k}(A)||_F^2 &lt;= (1+eps) ||A - A_k||_F^2.
  We also show that Frequent Directions cannot be adapted to a sparse version
in an obvious way that retains the l original rows of the matrix, as opposed to
a linear combination or sketch of the rows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7461</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7461</id><created>2013-07-29</created><authors><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Patoglu</keyname><forenames>Volkan</forenames></author><author><keyname>Sch&#xfc;ller</keyname><forenames>Peter</forenames></author></authors><title>Levels of Integration between Low-Level Reasoning and Task Planning</title><categories>cs.RO cs.AI</categories><comments>In Workshop on Knowledge Representation and Reasoning in Robotics
  (KRR) (International Conference on Logic Programming (ICLP) 2013)</comments><msc-class>68T40</msc-class><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a systematic analysis of levels of integration between discrete
high-level reasoning and continuous low-level reasoning to address hybrid
planning problems in robotics. We identify four distinct strategies for such an
integration: (i) low-level checks are done for all possible cases in advance
and then this information is used during plan generation, (ii) low-level checks
are done exactly when they are needed during the search for a plan, (iii) first
all plans are computed and then infeasible ones are filtered, and (iv) by means
of replanning, after finding a plan, low-level checks identify whether it is
infeasible or not; if it is infeasible, a new plan is computed considering the
results of previous low- level checks. We perform experiments on hybrid
planning problems in robotic manipulation and legged locomotion domains
considering these four methods of integration, as well as some of their
combinations. We analyze the usefulness of levels of integration in these
domains, both from the point of view of computational efficiency (in time and
space) and from the point of view of plan quality relative to its feasibility.
We discuss advantages and disadvantages of each strategy in the light of
experimental results and provide some guidelines on choosing proper strategies
for a given domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7464</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7464</id><created>2013-07-29</created><authors><author><keyname>Guntuku</keyname><forenames>Sharath Chandra</forenames></author><author><keyname>Narang</keyname><forenames>Pratik</forenames></author><author><keyname>Hota</keyname><forenames>Chittaranjan</forenames></author></authors><title>Real-time Peer-to-Peer Botnet Detection Framework based on Bayesian
  Regularized Neural Network</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, the Cyberspace has seen an increasing number of attacks
coming from botnets using the Peer-to-Peer (P2P) architecture. Peer-to-Peer
botnets use a decentralized Command &amp; Control architecture. Moreover, a large
number of such botnets already exist, and newer versions- which significantly
differ from their parent bot- are also discovered practically every year. In
this work, the authors propose and implement a novel hybrid framework for
detecting P2P botnets in live network traffic by integrating Neural Networks
with Bayesian Regularization. Bayesian Regularization helps in achieving better
generalization of the dataset, thereby enabling the detection of botnet
activity even of those bots which were never used in training the Neural
Network. Hence such a framework is suitable for detection of newer and unseen
botnets in live traffic of a network. This was verified by testing the
Framework on test data unseen to the Detection module (using untrained botnet
dataset), and the authors were successful in detecting this activity with an
accuracy of 99.2 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7466</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7466</id><created>2013-07-29</created><authors><author><keyname>Duff</keyname><forenames>Damien Jade</forenames></author><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Patoglu</keyname><forenames>Volkan</forenames></author></authors><title>Integration of 3D Object Recognition and Planning for Robotic
  Manipulation: A Preliminary Report</title><categories>cs.AI cs.CV cs.RO</categories><comments>Knowledge Representation and Reasoning in Robotics Workshop at ICLP
  2013, Istanbul, Turkey</comments><acm-class>D.1.6; I.2.10; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate different approaches to integrating object recognition and
planning in a tabletop manipulation domain with the set of objects used in the
2012 RoboCup@Work competition. Results of our preliminary experiments show
that, with some approaches, close integration of perception and planning
improves the quality of plans, as well as the computation times of feasible
plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7472</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7472</id><created>2013-07-29</created><authors><author><keyname>Kumar</keyname><forenames>Puneet</forenames></author><author><keyname>Kumar</keyname><forenames>Dharminder</forenames></author><author><keyname>Kumar</keyname><forenames>Narendra</forenames></author></authors><title>Improved Service Delivery and Cost Effective Framework for e-Governance
  in India</title><categories>cs.CY</categories><comments>4 pages, 3 figures Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 74(2):20-23, July
  2013. New York, USA</journal-ref><doi>10.5120/12858-9577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In current era, the involvement of technologies like virtualization,
consolidation and cloud computing, and adoption of free and open source
software in designing and deploying e-governance that can reduce the total cost
associated with and hence the financial burden abide by the state and central
governments. The success of any e-governance project depends upon its
utilization by the intended group and so there accessibility needs to be
enhanced drastically by reengineered framework. Here, we design an Improved
Service Delivery and Cost Effective Framework for e-Governance that will be
useful for success of e-governance projects and the delivery mechanism in India
by using free and open access software for development and deployment of
e-governance applications, virtualization and consolidation techniques for
management of e-services and cloud computing for enhancing the accessibility of
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7474</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7474</id><created>2013-07-29</created><authors><author><keyname>Boss</keyname><forenames>R. Subash Chandra</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author><author><keyname>Daniel</keyname><forenames>D. Arul Pon</forenames></author></authors><title>Automatic Mammogram image Breast Region Extraction and Removal of
  Pectoral Muscle</title><categories>cs.CV</categories><comments>8 Pages, 5 Figures</comments><journal-ref>International Journal of Scientific &amp; Engineering Research, Volume
  4, Issue 5, May-2013 1727</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently Mammography is a most effective imaging modality used by
radiologists for the screening of breast cancer. Finding an accurate, robust
and efficient breast region segmentation technique still remains a challenging
problem in digital mammography. Extraction of the breast profile region and the
removal of pectoral muscle are essential pre-processing steps in Computer Aided
Diagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allows
the search for abnormalities to be limited to the region of the breast tissue
without undue influence from the background of the mammogram. The presence of
pectoral muscle in mammograms biases detection procedures, which recommends
removing the pectoral muscle during mammogram image pre-processing. The
presence of pectoral muscle in mammograms may disturb or influence the
detection of breast cancer as the pectoral muscle and mammographic parenchymas
appear similar. The goal of breast region extraction is reducing the image size
without losing anatomic information, it improve the accuracy of the overall CAD
system. The main objective of this study is to propose an automated method to
identify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. In
this paper, we proposed histogram based 8-neighborhood connected component
labelling method for breast region extraction and removal of pectoral muscle.
The proposed method is evaluated by using the mean values of accuracy and
error. The comparative analysis shows that the proposed method identifies the
breast region more accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7477</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7477</id><created>2013-07-29</created><updated>2014-03-10</updated><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author></authors><title>Manipulation of Stable Matchings using Minimal Blacklists</title><categories>cs.GT</categories><comments>Hebrew University of Jerusalem Center for the Study of Rationality
  discussion paper 643</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gale and Sotomayor (1985) have shown that in the Gale-Shapley matching
algorithm (1962), the proposed-to side W (referred to as women there) can
strategically force the W-optimal stable matching as the M-optimal one by
truncating their preference lists, each woman possibly blacklisting all but one
man. As Gusfield and Irving have already noted in 1989, no results are known
regarding achieving this feat by means other than such preference-list
truncation, i.e. by also permuting preference lists.
  We answer Gusfield and Irving's open question by providing tight upper bounds
on the amount of blacklists and their combined size, that are required by the
women to force a given matching as the M-optimal stable matching, or, more
generally, as the unique stable matching. Our results show that the coalition
of all women can strategically force any matching as the unique stable
matching, using preference lists in which at most half of the women have
nonempty blacklists, and in which the average blacklist size is less than 1.
This allows the women to manipulate the market in a manner that is far more
inconspicuous, in a sense, than previously realized. When there are less women
than men, we show that in the absence of blacklists for men, the women can
force any matching as the unique stable matching without blacklisting anyone,
while when there are more women than men, each to-be-unmatched woman may have
to blacklist as many as all men. Together, these results shed light on the
question of how much, if at all, do given preferences for one side a priori
impose limitations on the set of stable matchings under various conditions. All
of the results in this paper are constructive, providing efficient algorithms
for calculating the desired strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7478</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7478</id><created>2013-07-29</created><authors><author><keyname>Marfisi-Schottman</keyname><forenames>Iza</forenames><affiliation>LIP6</affiliation></author><author><keyname>Labat</keyname><forenames>Jean-Marc</forenames><affiliation>LIP6</affiliation></author><author><keyname>Carron</keyname><forenames>Thibault</forenames><affiliation>LIP6</affiliation></author></authors><title>Building on the Case Teaching Method to Generate Learning Games Relevant
  to Numerous Educational Fields</title><categories>cs.CY</categories><comments>International Conference on Advanced Learning Technologies, ICALT,
  Beijing : China (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  University teachers often feel the need to try innovative learning
technologies such as Learning Games to motivate the new generation of students.
However, the typically limited resources of universities coupled with the high
cost of designing and developing Learning Games result in it rarely being
feasible to meet this need. To address this challenging problem, we have
designed a framework that allows teachers to create their own Learning Games
with very little or no help from developers and graphic designers. This
framework, tested and validated by several university teachers, is suited to a
wide variety of educational fields because it generates Learning Games based on
the widely-used case teaching method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7494</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7494</id><created>2013-07-29</created><authors><author><keyname>Dogmus</keyname><forenames>Zeynep</forenames></author><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Patoglu</keyname><forenames>Volkan</forenames></author></authors><title>ReAct! An Interactive Tool for Hybrid Planning in Robotics</title><categories>cs.AI cs.LO cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ReAct!, an interactive tool for high-level reasoning for cognitive
robotic applications. ReAct! enables robotic researchers to describe robots'
actions and change in dynamic domains, without having to know about the
syntactic and semantic details of the underlying formalism in advance, and
solve planning problems using state-of-the-art automated reasoners, without
having to learn about their input/output language or usage. In particular,
ReAct! can be used to represent sophisticated dynamic domains that feature
concurrency, indirect effects of actions, and state/transition constraints. It
allows for embedding externally defined calculations (e.g., checking for
collision-free continuous trajectories) into representations of hybrid domains
that require a tight integration of (discrete) high-level reasoning with
(continuous) geometric reasoning. ReAct! also enables users to solve planning
problems that involve complex goals. Such variety of utilities are useful for
robotic researchers to work on interesting and challenging domains, ranging
from service robotics to cognitive factories. ReAct! provides sample
formalizations of some action domains (e.g., multi-agent path planning, Tower
of Hanoi), as well as dynamic simulations of plans computed by a
state-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7495</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7495</id><created>2013-07-29</created><updated>2013-12-27</updated><authors><author><keyname>Sasoglu</keyname><forenames>Eren</forenames></author><author><keyname>Wang</keyname><forenames>Lele</forenames></author></authors><title>Universal Polarization</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method to polarize channels universally is introduced. The method is based
on combining two distinct channels in each polarization step, as opposed to
Arikan's original method of combining identical channels. This creates an equal
number of only two types of channels, one of which becomes progressively better
as the other becomes worse. The locations of the good polarized channels are
independent of the underlying channel, guaranteeing universality. Polarizing
the good channels further with Arikan's method results in universal polar codes
of rate 1/2. The method is generalized to construct codes of arbitrary rates.
  It is also shown that the less noisy ordering of channels is preserved under
polarization, and thus a good polar code for a given channel will perform well
over a less noisy one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7498</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7498</id><created>2013-07-29</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Barth</keyname><forenames>Andreas</forenames></author></authors><title>The normalization of citation counts based on classification systems</title><categories>cs.DL physics.soc-ph stat.OT</categories><comments>Accepted for publication in the journal publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If we want to assess whether the paper in question has had a particularly
high or low citation impact compared to other papers, the standard practice in
bibliometrics is to normalize citations in respect of the subject category and
publication year. A number of proposals for an improved procedure in the
normalization of citation impact have been put forward in recent years. Against
the background of these proposals this study describes an ideal solution for
the normalization of citation impact: in a first step, the reference set for
the publication in question is collated by means of a classification scheme,
where every publication is associated with a single principal research field or
subfield entry (e. g. via Chemical Abstracts sections) and a publication year.
In a second step, percentiles of citation counts are calculated for this set
and used to assign the normalized citation impact score to the publications
(and also to the publication in question).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7505</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7505</id><created>2013-07-29</created><updated>2015-07-12</updated><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Interactive Logic Programming via Choice-Disjunctive Clauses</title><categories>cs.LO</categories><comments>9 pages. A new execution model is added to the previous version.
  arXiv admin note: substantial text overlap with arXiv:1211.6535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding interaction to logic programming is an essential task. Expressive
logics such as linear logic provide a theoretical basis for such a mechanism.
Unfortunately, none of the existing linear logic languages can model
interactions with the user. This is because they uses provability as the sole
basis for computation. We propose to use the game semantics instead of
provability as the basis for computation to allow for more active participation
from the user. We illustrate our idea via muprolog, an extension of Prolog with
choice-disjunctive clauses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7513</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7513</id><created>2013-07-29</created><authors><author><keyname>Sena</keyname><forenames>P Vasanth</forenames></author></authors><title>An Approach Finding Frequent Items In Text Or Transactional Data Base By
  Using BST To Improve The Efficiency Of Apriori Algorithm</title><categories>cs.DB cs.DS</categories><comments>1 Algorithm. arXiv admin note: text overlap with arXiv:1009.4982 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining techniques have been widely used in various applications. Binary
search tree based frequent items is an effective method for automatically
recognize the most frequent items, least frequent items and average frequent
items. This paper presents a new approach in order to find out frequent items.
The word frequent item refers to how many times the item appeared in the given
input. This approach is used to find out item sets in any order using familiar
approach binary search tree. The method adapted here is in order to find out
frequent items by comparing and incrementing the counter variable in existing
transactional data base or text data. We are also representing different
approaches in frequent item sets and also propose an algorithmic approach for
the problem solving
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7521</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7521</id><created>2013-07-29</created><updated>2016-02-16</updated><authors><author><keyname>Joneidi</keyname><forenames>Mohsen</forenames></author><author><keyname>Ahmadi</keyname><forenames>Parvin</forenames></author><author><keyname>Sadeghi</keyname><forenames>Mostafa</forenames></author><author><keyname>Rahnavard</keyname><forenames>Nazanin</forenames></author></authors><title>Union of Low-Rank Subspaces Detector</title><categories>cs.IT cs.CV math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of signal detection using a flexible and general model is
considered. Due to applicability and flexibility of sparse signal
representation and approximation, it has attracted a lot of attention in many
signal processing areas. In this paper, we propose a new detection method based
on sparse decomposition in a union of subspaces (UoS) model. Our proposed
detector uses a dictionary that can be interpreted as a bank of matched
subspaces. This improves the performance of signal detection, as it is a
generalization for detectors. Low-rank assumption for the desired signals
implies that the representations of these signals in terms of some proper bases
would be sparse. Our proposed detector exploits sparsity in its decision rule.
We demonstrate the high efficiency of our method in the cases of voice activity
detection in speech processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7533</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7533</id><created>2013-07-29</created><authors><author><keyname>Zaidi</keyname><forenames>Ali A.</forenames></author><author><keyname>Oechtering</keyname><forenames>Tobias J.</forenames></author><author><keyname>Yuksel</keyname><forenames>Serdar</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Stabilization of Linear Systems Over Gaussian Networks</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of remotely stabilizing a noisy linear time invariant plant over
a Gaussian relay network is addressed. The network is comprised of a sensor
node, a group of relay nodes and a remote controller. The sensor and the relay
nodes operate subject to an average transmit power constraint and they can
cooperate to communicate the observations of the plant's state to the remote
controller. The communication links between all nodes are modeled as Gaussian
channels. Necessary as well as sufficient conditions for mean-square
stabilization over various network topologies are derived. The sufficient
conditions are in general obtained using delay-free linear policies and the
necessary conditions are obtained using information theoretic tools. Different
settings where linear policies are optimal, asymptotically optimal (in certain
parameters of the system) and suboptimal have been identified. For the case
with noisy multi-dimensional sources controlled over scalar channels, it is
shown that linear time varying policies lead to minimum capacity requirements,
meeting the fundamental lower bound. For the case with noiseless sources and
parallel channels, non-linear policies which meet the lower bound have been
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7534</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7534</id><created>2013-07-29</created><authors><author><keyname>Fontein</keyname><forenames>Felix</forenames></author><author><keyname>Schneider</keyname><forenames>Michael</forenames></author><author><keyname>Wagner</keyname><forenames>Urs</forenames></author></authors><title>PotLLL: A Polynomial Time Version of LLL With Deep Insertions</title><categories>cs.CR</categories><comments>17 pages, 8 figures; extended version of arXiv:1212.5100 [cs.CR]</comments><msc-class>68R05, 94A60, 68R05, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice reduction algorithms have numerous applications in number theory,
algebra, as well as in cryptanalysis. The most famous algorithm for lattice
reduction is the LLL algorithm. In polynomial time it computes a reduced basis
with provable output quality. One early improvement of the LLL algorithm was
LLL with deep insertions (DeepLLL). The output of this version of LLL has
higher quality in practice but the running time seems to explode. Weaker
variants of DeepLLL, where the insertions are restricted to blocks, behave
nicely in practice concerning the running time. However no proof of polynomial
running time is known. In this paper PotLLL, a new variant of DeepLLL with
provably polynomial running time, is presented. We compare the practical
behavior of the new algorithm to classical LLL, BKZ as well as blockwise
variants of DeepLLL regarding both the output quality and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7544</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7544</id><created>2013-07-29</created><updated>2014-02-20</updated><authors><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Thompson</keyname><forenames>Andrew</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>On block coherence of frames</title><categories>cs.IT math.IT</categories><msc-class>15A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block coherence of matrices plays an important role in analyzing the
performance of block compressed sensing recovery algorithms (Bajwa and Mixon,
2012). In this paper, we characterize two block coherence metrics: worst-case
and average block coherence. First, we present lower bounds on worst-case block
coherence, in both the general case and also when the matrix is constrained to
be a union of orthobases. We then present deterministic matrix constructions
based upon Kronecker products which obtain these lower bounds. We also
characterize the worst-case block coherence of random subspaces. Finally, we
present a flipping algorithm that can improve the average block coherence of a
matrix, while maintaining the worst-case block coherence of the original
matrix. We provide numerical examples which demonstrate that our proposed
deterministic matrix construction performs well in block compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7545</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7545</id><created>2013-07-29</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Xiang</keyname><forenames>Lin</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Beamforming for Secure Communication in Systems with
  Wireless Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in PIMRC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study power allocation for secure communication in a
multiuser multiple-input single-output (MISO) downlink system with simultaneous
wireless information and power transfer. The receivers are able to harvest
energy from the radio frequency when they are idle. We propose a
multi-objective optimization problem for power allocation algorithm design
which incorporates two conflicting system objectives: total transmit power
minimization and energy harvesting efficiency maximization. The proposed
problem formulation takes into account a quality of service (QoS) requirement
for the system secrecy capacity. Our designs advocate the dual use of
artificial noise in providing secure communication and facilitating efficient
energy harvesting. The multi-objective optimization problem is non-convex and
is solved by a semidefinite programming (SDP) relaxation approach which results
in an approximate of solution.
  A sufficient condition for the global optimal solution is revealed and the
accuracy of the approximation is examined. To strike a balance between
computational complexity and system performance, we propose two suboptimal
power allocation schemes. Numerical results not only demonstrate the excellent
performance of the proposed suboptimal schemes compared to baseline schemes,
but also unveil an interesting trade-off between energy harvesting efficiency
and total transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7553</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7553</id><created>2013-07-29</created><authors><author><keyname>Xu</keyname><forenames>Yuzhe</forenames></author><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Distributed Association Control and Relaying in MillimeterWave Wireless
  Access Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In millimeterWave wireless networks the rapidly varying wireless channels
demand fast and dynamic resource allocation mechanisms. This challenge is
hereby addressed by a distributed approach that optimally solves the
fundamental resource allocation problem of joint client association and
relaying. The problem is posed as a multi assignment optimization, for which a
novel solution method is established by a series of transformations that lead
to a tractable minimum cost flow problem. The method allows to design
distributed auction solution algorithms where the clients and relays act
asynchronously. The computational complexity of the new algorithms is much
better than centralized general-purpose solvers. It is shown that the
algorithms always converge to a solution that maximizes the total network
throughput within a desired bound. Both theoretical and numerical results
evince numerous useful properties in comparison to standard approaches and the
potential applications to forthcoming millimeterWave wireless access networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7562</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7562</id><created>2013-07-29</created><authors><author><keyname>Pedroche</keyname><forenames>Francisco</forenames></author><author><keyname>Rebollo</keyname><forenames>Miguel</forenames></author><author><keyname>Carrascosa</keyname><forenames>Carlos</forenames></author><author><keyname>Palomares</keyname><forenames>Alberto</forenames></author></authors><title>On the convergence of weighted-average consensus</title><categories>math.OC cs.SY</categories><msc-class>65F10, 15B48</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we give sufficient conditions for the convergence of the
iterative algorithm called weighted-average consensus in directed graphs. We
study the discrete-time form of this algorithm. We use standard techniques from
matrix theory to prove the main result. As a particular case one can obtain
well-known results for non-weighted average consensus. We also give a corollary
for undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7563</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7563</id><created>2013-07-29</created><authors><author><keyname>Joy</keyname><forenames>Preetha Theresa</forenames></author><author><keyname>Jacob</keyname><forenames>K. Poulose</forenames></author></authors><title>Cooperative Caching Framework for Mobile Cloud Computing</title><categories>cs.NI cs.DC</categories><journal-ref>Global Journal of Computer Science and Technology, Volume 13 Issue
  8 Version 1.0 Year 2013 Network, Web &amp; Security Volume 13 Issue 8 Version 1.0
  Year 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the advancement in mobile devices and wireless networks mobile cloud
computing, which combines mobile computing and cloud computing has gained
momentum since 2009. The characteristics of mobile devices and wireless network
makes the implementation of mobile cloud computing more complicated than for
fixed clouds. This section lists some of the major issues in Mobile Cloud
Computing. One of the key issues in mobile cloud computing is the end to end
delay in servicing a request. Data caching is o ne of the techniques widely
used in wired and wireless networks to improve data access efficiency. In this
paper we explore the possibility of a cooperative caching approach to enhance
data access efficiency in mobile cloud computing. The proposed approach is
based on cloudlets, one of the architecture designed for mobile cloud
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7569</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7569</id><created>2013-07-29</created><updated>2014-09-12</updated><authors><author><keyname>Chang</keyname><forenames>Chang</forenames></author><author><keyname>Tang</keyname><forenames>Chao</forenames></author></authors><title>Community detection for networks with unipartite and bipartite structure</title><categories>physics.soc-ph cs.SI q-bio.QM</categories><comments>27 pages, 8 figures.
  (http://iopscience.iop.org/1367-2630/16/9/093001)</comments><journal-ref>New Journal of Physics 16, 093001 (2014)</journal-ref><doi>10.1088/1367-2630/16/9/093001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding community structures in networks is important in network science,
technology, and applications. To date, most algorithms that aim to find
community structures only focus either on unipartite or bipartite networks. A
unipartite network consists of one set of nodes and a bipartite network
consists of two nonoverlapping sets of nodes with only links joining the nodes
in different sets. However, a third type of network exists, defined here as the
mixture network. Just like a bipartite network, a mixture network also consists
of two sets of nodes, but some nodes may simultaneously belong to two sets,
which breaks the nonoverlapping restriction of a bipartite network. The mixture
network can be considered as a general case, with unipartite and bipartite
networks viewed as its limiting cases. A mixture network can represent not only
all the unipartite and bipartite networks, but also a wide range of real-world
networks that cannot be properly represented as either unipartite or bipartite
networks in fields such as biology and social science. Based on this
observation, we first propose a probabilistic model that can find modules in
unipartite, bipartite, and mixture networks in a unified framework based on the
link community model for a unipartite undirected network [B Ball et al (2011
Phys. Rev. E 84 036103)]. We test our algorithm on synthetic networks (both
overlapping and nonoverlapping communities) and apply it to two real-world
networks: a southern women bipartite network and a human transcriptional
regulatory mixture network. The results suggest that our model performs well
for all three types of networks, is competitive with other algorithms for
unipartite or bipartite networks, and is applicable to real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7577</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7577</id><created>2013-07-29</created><updated>2014-05-12</updated><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Zhao</keyname><forenames>Zheng</forenames></author><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Safe Screening With Variational Inequalities and Its Application to
  LASSO</title><categories>cs.LG stat.ML</categories><comments>Accepted by International Conference on Machine Learning 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse learning techniques have been routinely used for feature selection as
the resulting model usually has a small number of non-zero entries. Safe
screening, which eliminates the features that are guaranteed to have zero
coefficients for a certain value of the regularization parameter, is a
technique for improving the computational efficiency. Safe screening is gaining
increasing attention since 1) solving sparse learning formulations usually has
a high computational cost especially when the number of features is large and
2) one needs to try several regularization parameters to select a suitable
model. In this paper, we propose an approach called &quot;Sasvi&quot; (Safe screening
with variational inequalities). Sasvi makes use of the variational inequality
that provides the sufficient and necessary optimality condition for the dual
problem. Several existing approaches for Lasso screening can be casted as
relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe
screening rule. We further study the monotone properties of Sasvi for Lasso,
based on which a sure removal regularization parameter can be identified for
each feature. Experimental results on both synthetic and real data sets are
reported to demonstrate the effectiveness of the proposed Sasvi for Lasso
screening.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7584</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7584</id><created>2013-07-29</created><updated>2013-08-01</updated><authors><author><keyname>Ciucu</keyname><forenames>Florin</forenames></author><author><keyname>Khalili</keyname><forenames>Ramin</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author><author><keyname>Cui</keyname><forenames>Yong</forenames></author></authors><title>Towards a System Theoretic Approach to Wireless Network Capacity in
  Finite Time and Space</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In asymptotic regimes, both in time and space (network size), the derivation
of network capacity results is grossly simplified by brushing aside queueing
behavior in non-Jackson networks. This simplifying double-limit model, however,
lends itself to conservative numerical results in finite regimes. To properly
account for queueing behavior beyond a simple calculus based on average rates,
we advocate a system theoretic methodology for the capacity problem in finite
time and space regimes. This methodology also accounts for spatial correlations
arising in networks with CSMA/CA scheduling and it delivers rigorous
closed-form capacity results in terms of probability distributions. Unlike
numerous existing asymptotic results, subject to anecdotal practical concerns,
our transient one can be used in practical settings: for example, to compute
the time scales at which multi-hop routing is more advantageous than single-hop
routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7597</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7597</id><created>2013-07-05</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author><author><keyname>Skokov</keyname><forenames>Oleg</forenames></author></authors><title>Context-aware QR-codes</title><categories>cs.IT cs.CY cs.NI math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new model for presenting local information based on
the network proximity. We present a novelty mobile mashup which combines Wi-Fi
proximity measurements with QR-codes. Our mobile mashup automatically adds
context information the content presented by QR-codes. It simplifies the
deployment schemes and allows to use unified presentation for all data points,
for example. This paper describes how to combine QR-codes and network proximity
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7602</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7602</id><created>2013-06-07</created><authors><author><keyname>Jiang</keyname><forenames>Sheng</forenames></author><author><keyname>Trinkle</keyname><forenames>Matthew</forenames></author><author><keyname>Wang</keyname><forenames>Hongyuan</forenames></author></authors><title>Survey on Positioning System: Sampling methods</title><categories>cs.IT cs.NI math.IT</categories><comments>8 pages, draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-accuracy Ultra-Wideband (UWB) positioning systems using the Time
Difference Of Arrival (TDOA) algorithm are able to be utilized in military and
many other important applications. Previous research on UWB positioning system
has achieved up to mm or sub-mm accuracy. However, one bottleneck in UWB system
is at sampling high resolution UWB signals, as well as high resolution timing
information. In this paper, UWB positioning systems are surveyed and we focus
on sampling methods for handling UWB signals. Among different sampling methods,
one traditional way is the sequential sampling method, which is not a real time
sampling method and blocks UWB positioning system to achieve higher precision.
Another way is by applying Compressed Sensing (CS) to UWB system for achieving
sub-mm positioning accuracy. In this paper, we compare different TDOA-based UWB
systems with different sampling methods. In particular, several CS-UWB
algorithms for UWB signal reconstruction are compared in terms of positioning
accuracy. Simulation results in 2D and 3D experiments demonstrate performance
of different algorithms including typical BCS, OMP and BP algorithms. CS-UWB is
also compared with UWB positioning system based on the sequential sampling
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7612</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7612</id><created>2013-07-29</created><authors><author><keyname>Zwickl</keyname><forenames>Patrick</forenames></author><author><keyname>Fuxjaeger</keyname><forenames>Paul</forenames></author><author><keyname>Gojmerac</keyname><forenames>Ivan</forenames></author><author><keyname>Reichl</keyname><forenames>Peter</forenames></author></authors><title>Wi-Fi Offload: Tragedy of the Commons or Land of Milk and Honey?</title><categories>cs.NI cs.GT</categories><comments>Workshop on Spectrum Sharing Strategies for Wireless Broadband
  Services, IEEE PIMRC'13, to appear 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fueled by its recent success in provisioning on-site wireless Internet
access, Wi-Fi is currently perceived as the best positioned technology for
pervasive mobile macro network offloading. However, the broad transitions of
multiple collocated operators towards this new paradigm may result in fierce
competition for the common unlicensed spectrum at hand. In this light, our
paper game-theoretically dissects market convergence scenarios by assessing the
competition between providers in terms of network performance, capacity
constraints, cost reductions, and revenue prospects. We will closely compare
the prospects and strategic positioning of fixed line operators offering Wi-Fi
services with respect to competing mobile network operators utilizing
unlicensed spectrum. Our results highlight important dependencies upon
inter-operator collaboration models, and more importantly, upon the ratio
between backhaul and Wi-Fi access bit-rates. Furthermore, our investigation of
medium- to long-term convergence scenarios indicates that a rethinking of
control measures targeting the large-scale monetization of unlicensed spectrum
may be required, as otherwise the used free bands may become subject to
tragedy-of-commons type of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7615</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7615</id><created>2013-07-29</created><updated>2013-11-12</updated><authors><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Association schemes, non-commutative polynomial concentration, and
  sum-of-squares lower bounds for planted clique</title><categories>cs.CC cs.DS math.CO math.PR</categories><comments>This paper has been withdrawn due to an error; &quot;Theorem 1.6&quot; from the
  original manuscript which is used crucially in the proof of the main result
  is not correct. We thank Gilles Pisier for pointing this out</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding cliques in random graphs and the closely related &quot;planted&quot; clique
variant, where a clique of size t is planted in a random G(n,1/2) graph, have
been the focus of substantial study in algorithm design. Despite much effort,
the best known polynomial-time algorithms only solve the problem for t =
Theta(sqrt(n)). Here we show that beating sqrt(n) would require substantially
new algorithmic ideas, by proving a lower bound for the problem in the
sum-of-squares (or Lasserre) hierarchy, the most powerful class of
semi-definite programming algorithms we know of: r rounds of the sum-of-squares
hierarchy can only solve the planted clique for t &gt; sqrt(n)/(C log n)^(r^2).
Previously, no nontrivial lower bounds were known. Our proof is formulated as a
degree lower bound in the Positivstellensatz algebraic proof system, which is
equivalent to the sum-of-squares hierarchy. The heart of our (average-case)
lower bound is a proof that a certain random matrix derived from the input
graph is (with high probability) positive semidefinite. Two ingredients play an
important role in this proof. The first is the classical theory of association
schemes, applied to the average and variance of that random matrix. The second
is a new large deviation inequality for matrix-valued polynomials. Our new tail
estimate seems to be of independent interest and may find other applications,
as it generalizes both the estimates on real-valued polynomials and on sums of
independent random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7622</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7622</id><created>2013-07-29</created><updated>2015-04-13</updated><authors><author><keyname>Gregoratti</keyname><forenames>David</forenames></author><author><keyname>Matamoros</keyname><forenames>Javier</forenames></author></authors><title>Distributed Energy Trading: The Multiple-Microgrid Case</title><categories>math.OC cs.MA</categories><comments>24 pages, 8 figures; new version answering reviewers' comments; the
  paper is now accepted for publication in the IEEE Transactions on Industrial
  Electronics; the paper is now published</comments><journal-ref>IEEE Transactions on Industrial Electronics, vol. 62, no. 4, pp.
  2551-2559, Apr. 2015</journal-ref><doi>10.1109/TIE.2014.2352592</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a distributed convex optimization framework is developed for
energy trading between islanded microgrids. More specifically, the problem
consists of several islanded microgrids that exchange energy flows by means of
an arbitrary topology. Due to scalability issues and in order to safeguard
local information on cost functions, a subgradient-based cost minimization
algorithm is proposed that converges to the optimal solution in a practical
number of iterations and with a limited communication overhead. Furthermore,
this approach allows for a very intuitive economics interpretation that
explains the algorithm iterations in terms of &quot;supply--demand model&quot; and
&quot;market clearing&quot;. Numerical results are given in terms of convergence rate of
the algorithm and attained costs for different network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7633</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7633</id><created>2013-07-29</created><updated>2014-03-09</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Upcoming Standards in Wireless Local Area Networks</title><categories>cs.NI</categories><comments>9 pages, 4 figures, 4 tables. A longer version of this paper could be
  found at arXiv:1307.2661. There is also substantial text overlap with the
  mentioned arXiv paper</comments><journal-ref>Wireless &amp; Mobile Technologies, Volume 1,Issue 1, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network technologies are traditionally centered on wireline solutions.
Wireless broadband technologies nowadays provide unlimited broadband usage to
users that have been previously offered simply to wireline users. In this
paper, we discuss some of the upcoming standards of one of the emerging
wireless broadband technology i.e. IEEE 802.11. The newest and the emerging
standards fix technology issues or add functionality that will be expected to
overcome many of the current standing problems with IEEE 802.11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7661</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7661</id><created>2013-07-29</created><updated>2013-07-30</updated><authors><author><keyname>Nguyen</keyname><forenames>Thi-Minh-Tam</forenames></author><author><keyname>Vu</keyname><forenames>Viet-Trung</forenames></author><author><keyname>Doan</keyname><forenames>The-Vinh</forenames></author><author><keyname>Tran</keyname><forenames>Duc-Khanh</forenames></author></authors><title>Resolution in Linguistic Propositional Logic based on Linear Symmetrical
  Hedge Algebra</title><categories>cs.LO</categories><comments>KSE 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a propositional linguistic logic that serves as the
basis for automated uncertain reasoning with linguistic information. First, we
build a linguistic logic system with truth value domain based on a linear
symmetrical hedge algebra. Then, we consider G\&quot;{o}del's t-norm and t-conorm to
define the logical connectives for our logic. Next, we present a resolution
inference rule, in which two clauses having contradictory linguistic truth
values can be resolved. We also give the concept of reliability in order to
capture the approximative nature of the resolution inference rule. Finally, we
propose a resolution procedure with the maximal reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7666</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7666</id><created>2013-07-29</created><authors><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Tight Lower Bounds for Homology Inference</title><categories>stat.ML cs.CG math.ST stat.TH</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The homology groups of a manifold are important topological invariants that
provide an algebraic summary of the manifold. These groups contain rich
topological information, for instance, about the connected components, holes,
tunnels and sometimes the dimension of the manifold. In earlier work, we have
considered the statistical problem of estimating the homology of a manifold
from noiseless samples and from noisy samples under several different noise
models. We derived upper and lower bounds on the minimax risk for this problem.
In this note we revisit the noiseless case. In previous work we used Le Cam's
lemma to establish a lower bound that differed from the upper bound of Niyogi,
Smale and Weinberger by a polynomial factor in the condition number.
  In this note we use a different construction based on the direct analysis of
the likelihood ratio test to show that the upper bound of Niyogi, Smale and
Weinberger is in fact tight, thus establishing rate optimal asymptotic minimax
bounds for the problem. The techniques we use here extend in a straightforward
way to the noisy settings considered in our earlier work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7720</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7720</id><created>2013-07-29</created><authors><author><keyname>Talamadupula</keyname><forenames>Kartik</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Herding the Crowd: Automated Planning for Crowdsourced Planning</title><categories>cs.AI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant interest in crowdsourcing and human computation.
One subclass of human computation applications are those directed at tasks that
involve planning (e.g. travel planning) and scheduling (e.g. conference
scheduling). Much of this work appears outside the traditional automated
planning forums, and at the outset it is not clear whether automated planning
has much of a role to play in these human computation systems. Interestingly
however, work on these systems shows that even primitive forms of automated
oversight of the human planner does help in significantly improving the
effectiveness of the humans/crowd. In this paper, we will argue that the
automated oversight used in these systems can be viewed as a primitive
automated planner, and that there are several opportunities for more
sophisticated automated planning in effectively steering crowdsourced planning.
Straightforward adaptation of current planning technology is however hampered
by the mismatch between the capabilities of human workers and automated
planners. We identify two important challenges that need to be overcome before
such adaptation of planning technology can occur: (i) interpreting the inputs
of the human workers (and the requester) and (ii) steering or critiquing the
plans being produced by the human workers armed only with incomplete domain and
preference models. In this paper, we discuss approaches for handling these
challenges, and characterize existing human computation systems in terms of the
specific choices they make in handling these challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7729</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7729</id><created>2013-07-29</created><authors><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Spectral methods for network community detection and graph partitioning</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>11 pages, 5 figures</comments><journal-ref>Phys. Rev. E 88, 042822 (2013)</journal-ref><doi>10.1103/PhysRevE.88.042822</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider three distinct and well studied problems concerning network
structure: community detection by modularity maximization, community detection
by statistical inference, and normalized-cut graph partitioning. Each of these
problems can be tackled using spectral algorithms that make use of the
eigenvectors of matrix representations of the network. We show that with
certain choices of the free parameters appearing in these spectral algorithms
the algorithms for all three problems are, in fact, identical, and hence that,
at least within the spectral approximations used here, there is no difference
between the modularity- and inference-based community detection methods, or
between either and graph partitioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7751</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7751</id><created>2013-07-29</created><updated>2014-04-07</updated><authors><author><keyname>Tang</keyname><forenames>Guoming</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Lei</keyname><forenames>Jingsheng</forenames></author><author><keyname>Bi</keyname><forenames>Zhongqin</forenames></author><author><keyname>Tang</keyname><forenames>Jiuyang</forenames></author></authors><title>From Landscape to Portrait: A New Approach for Outlier Detection in Load
  Curve Data</title><categories>cs.DC cs.DS</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In power systems, load curve data is one of the most important datasets that
are collected and retained by utilities. The quality of load curve data,
however, is hard to guarantee since the data is subject to communication
losses, meter malfunctions, and many other impacts. In this paper, a new
approach to analyzing load curve data is presented. The method adopts a new
view, termed \textit{portrait}, on the load curve data by analyzing the
periodic patterns in the data and re-organizing the data for ease of analysis.
Furthermore, we introduce algorithms to build the virtual portrait load curve
data, and demonstrate its application on load curve data cleansing. Compared to
existing regression-based methods, our method is much faster and more accurate
for both small-scale and large-scale real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7752</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7752</id><created>2013-07-29</created><authors><author><keyname>Bhatia</keyname><forenames>Harsh</forenames></author><author><keyname>Wang</keyname><forenames>Bei</forenames></author><author><keyname>Norgard</keyname><forenames>Gregory</forenames></author><author><keyname>Pascucci</keyname><forenames>Valerio</forenames></author><author><keyname>Bremer</keyname><forenames>Peer-Timo</forenames></author></authors><title>Local, Smooth, and Consistent Jacobi Set Simplification</title><categories>cs.CG cs.DS</categories><comments>24 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation between two Morse functions defined on a common domain can be
studied in terms of their Jacobi set. The Jacobi set contains points in the
domain where the gradients of the functions are aligned. Both the Jacobi set
itself as well as the segmentation of the domain it induces have shown to be
useful in various applications. Unfortunately, in practice functions often
contain noise and discretization artifacts causing their Jacobi set to become
unmanageably large and complex. While there exist techniques to simplify Jacobi
sets, these are unsuitable for most applications as they lack fine-grained
control over the process and heavily restrict the type of simplifications
possible.
  In this paper, we introduce a new framework that generalizes critical point
cancellations in scalar functions to Jacobi sets in two dimensions. We focus on
simplifications that can be realized by smooth approximations of the
corresponding functions and show how this implies simultaneously simplifying
contiguous subsets of the Jacobi set. These extended cancellations form the
atomic operations in our framework, and we introduce an algorithm to
successively cancel subsets of the Jacobi set with minimal modifications
according to some user-defined metric. We prove that the algorithm is correct
and terminates only once no more local, smooth and consistent simplifications
are possible. We disprove a previous claim on the minimal Jacobi set for
manifolds with arbitrary genus and show that for simply connected domains, our
algorithm reduces a given Jacobi set to its simplest configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7757</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7757</id><created>2013-07-29</created><updated>2014-05-19</updated><authors><author><keyname>Tang</keyname><forenames>Guoming</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author><author><keyname>Tang</keyname><forenames>Jiuyang</forenames></author><author><keyname>Lei</keyname><forenames>Jingsheng</forenames></author></authors><title>Household Electricity Consumption Data Cleansing</title><categories>cs.CE</categories><comments>12 pages, 12 figures; update: modified title and introduction, and
  corrected some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load curve data in power systems refers to users' electrical energy
consumption data periodically collected with meters. It has become one of the
most important assets for modern power systems. Many operational decisions are
made based on the information discovered in the data. Load curve data, however,
usually suffers from corruptions caused by various factors, such as data
transmission errors or malfunctioning meters. To solve the problem, tremendous
research efforts have been made on load curve data cleansing. Most existing
approaches apply outlier detection methods from the supply side (i.e.,
electricity service providers), which may only have aggregated load data. In
this paper, we propose to seek aid from the demand side (i.e., electricity
service users). With the help of readily available knowledge on consumers'
appliances, we present a new appliance-driven approach to load curve data
cleansing. This approach utilizes data generation rules and a Sequential Local
Optimization Algorithm (SLOA) to solve the Corrupted Data Identification
Problem (CDIP). We evaluate the performance of SLOA with real-world trace data
and synthetic data. The results indicate that, comparing to existing load data
cleansing methods, such as B-spline smoothing, our approach has an overall
better performance and can effectively identify consecutive corrupted data.
Experimental results also demonstrate that our method is robust in various
tests. Our method provides a highly feasible and reliable solution to an
emerging industry application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7760</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7760</id><created>2013-07-29</created><updated>2015-03-26</updated><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Wang</keyname><forenames>Bei</forenames></author><author><keyname>Zheng</keyname><forenames>Yan</forenames></author></authors><title>Geometric Inference on Kernel Density Estimates</title><categories>cs.CG</categories><comments>To appear in SoCG 2015. 36 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that geometric inference of a point cloud can be calculated by
examining its kernel density estimate with a Gaussian kernel. This allows one
to consider kernel density estimates, which are robust to spatial noise,
subsampling, and approximate computation in comparison to raw point sets. This
is achieved by examining the sublevel sets of the kernel distance, which
isomorphically map to superlevel sets of the kernel density estimate. We prove
new properties about the kernel distance, demonstrating stability results and
allowing it to inherit reconstruction results from recent advances in
distance-based topological reconstruction. Moreover, we provide an algorithm to
estimate its topology using weighted Vietoris-Rips complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7766</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7766</id><created>2013-07-29</created><updated>2013-07-30</updated><authors><author><keyname>Meredith</keyname><forenames>Lucius G</forenames></author><author><keyname>Stay</keyname><forenames>Mike</forenames></author><author><keyname>Drossopoulou</keyname><forenames>Sophia</forenames></author></authors><title>Policy as Types</title><categories>cs.CR cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drossopoulou and Noble argue persuasively for the need for a means to express
policy in object-capability-based systems. We investigate a practical means to
realize their aim via the Curry-Howard isomorphism. Specifically, we
investigate representing policy as types in a behavioral type system for the
RHO-calculus, a reflective higher-order variant of the pi-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7770</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7770</id><created>2013-07-29</created><authors><author><keyname>Schieler</keyname><forenames>Curt</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>A Connection between Good Rate-distortion Codes and Backward DMCs</title><categories>cs.IT math.IT</categories><comments>ITW 2013, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X^n\in\mathcal{X}^n$ be a sequence drawn from a discrete memoryless
source, and let $Y^n\in\mathcal{Y}^n$ be the corresponding reconstruction
sequence that is output by a good rate-distortion code. This paper establishes
a property of the joint distribution of $(X^n,Y^n)$. It is shown that for
$D&gt;0$, the input-output statistics of a $R(D)$-achieving rate-distortion code
converge (in normalized relative entropy) to the output-input statistics of a
discrete memoryless channel (dmc). The dmc is &quot;backward&quot; in that it is a
channel from the reconstruction space $\mathcal{Y}^n$ to source space
$\mathcal{X}^n$. It is also shown that the property does not necessarily hold
when normalized relative entropy is replaced by variational distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7779</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7779</id><created>2013-07-29</created><authors><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Ye</keyname><forenames>Qiaoyang</forenames></author><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet</forenames></author></authors><title>An Overview of Load Balancing in HetNets: Old Myths and Open Problems</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted, IEEE Wireless Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching the demand for resources (&quot;load&quot;) with the supply of resources
(&quot;capacity&quot;) is a basic problem occurring across many fields of engineering,
logistics, and economics, and has been considered extensively both in the
Internet and in wireless networks. The ongoing evolution of cellular
communication networks into dense, organic, and irregular heterogeneous
networks (&quot;HetNets&quot;) has elevated load-awareness to a central problem, and
introduces many new subtleties. This paper explains how several long-standing
assumptions about cellular networks need to be rethought in the context of a
load-balanced HetNet: we highlight these as three deeply entrenched myths that
we then dispel. We survey and compare the primary technical approaches to
HetNet load balancing: (centralized) optimization, game theory, Markov decision
processes, and the newly popular cell range expansion (a.k.a. &quot;biasing&quot;), and
draw design lessons for OFDMA-based cellular systems. We also identify several
open areas for future exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7786</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7786</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>A Hybrid Cryptosystem Based On Vigenere Cipher and Columnar
  Transposition Cipher</title><categories>cs.CR</categories><comments>7 pages. International Journal of Advanced Technology &amp; Engineering
  Research Volume 3 Issue 1 (IJATER), 2013</comments><journal-ref>International Journal of Advanced Technology and Engineering
  Research (IJATER) Vol 3 Issue 1 pp141-147. 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Privacy is one of the key issues addressed by information Security. Through
cryptographic encryption methods, one can prevent a third party from
understanding transmitted raw data over unsecured channel during signal
transmission. The cryptographic methods for enhancing the security of digital
contents have gained high significance in the current era. Breach of security
and misuse of confidential information that has been intercepted by
unauthorized parties are key problems that information security tries to solve.
This paper sets out to contribute to the general body of knowledge in the area
of classical cryptography by develop- ing a new hybrid way of encryption of
plaintext. The cryptosystem performs its encryption by encrypting the plaintext
using columnar transposition cipher and further using the ciphertext to encrypt
the plaintext again using Vigen\`ere ci- pher. At the end, cryptanalysis was
performed on the ciphertext. The implementation will be done using java
program- ming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7787</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7787</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author><author><keyname>Koumadi</keyname></author><author><keyname>M</keyname><forenames>Koudjo</forenames></author><author><keyname>Quaynor</keyname><forenames>Nii Narku</forenames></author></authors><title>An Integrated Geographic Information System and Marketing Information
  System Model</title><categories>cs.OH</categories><comments>6 pages. International Journal of Advanced Technology &amp; Engineering
  Research (IJATER) Volume 2 issue 6, 2013</comments><journal-ref>International Journal of Advanced Technology and Engineering
  Research Vol. 2 Issue 6 (2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Maintaining competitive advantage is significant in this present day of
globalization, knowledge management and enormous economic activities. An
organization's future developments are influenced by its managements'
decisions. Businesses today are facing a lot of challenges in terms of
competition and they have to be in the lead by strengthening their research and
development strategies with the aid of cutting edge technologies. Hence
marketing intelligence is now a key to the success of any business in today's
rapidly changing business environment. With all the technologies available in
marketing research, businesses still struggle with how to gather information
and make decisions in a short time and real-time about their customers' needs
and purchasing patterns in various geographical areas.
  This paper is set out to contribute to the body of knowledge in the area of
the application of Geographic Information Systems technology solutions to
businesses by developing a model for integrating Geographic Information Systems
into existing Marketing Information Systems for effective marketing research.
This model will interconnect organizations at the highest levels, providing
reassurance to enable broad scope of checks and balances as well as benefiting
many business activities including operational, tactical and strategic decision
making due to its analytical and solution driven functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7788</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7788</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>Computer Aided Investigation: Visualization and Analysis of data from
  Mobile communication devices using Formal Concept Analysis</title><categories>cs.CY</categories><comments>12 pages. 2nd CMI and GTUC International Conference on Applications
  of Mobile Communications in Africa: Prospects and Challenges, 2013, 2013</comments><journal-ref>2nd CMI &amp; GTUC International Conference on Applications of Mobile
  Communications.pp1-12.2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this world of terrorism, it is very important to know the network of
individual suspects. It is also important to analyze the attributes of members
of a network and the relationships that exist between them either directly or
indirectly. This will make it easy for concepts to be built in aiding criminal
investigations. However traditional approaches cannot be used to visualize and
analyze data collected on individuals. With this current day where information
systems play critical role in everyday life of every individual, it is easier
to depend on digital information in fighting crime.Effective computer tools and
intelligent systems that are automated to analyze and interpret criminal data
in real time effectively and efficiently are needed in fighting crime. These
current computer systems should have the capability of providing intelligence
from raw data and creating a visual graph which will make it easy for new
concepts to be built and generated from crime data in order to solve understand
and analyze crime patterns easily. This paper proposes a new method of computer
aided investigation by visualizing and analyzing data of mobile communication
devices using Formal Concept Analysis, or Galois Lattices, a data analysis
technique grounded on Lattice Theory and Propositional Calculus. This method
considered the set of common and distinct attributes of data in such a way that
categorizations are done based on related data with respect to time and events.
This will help in building a more defined and conceptual systems for analysis
of crime data that can easily be visualized and intelligently analyzed by
computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7789</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7789</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>The Role of Rural Banks in Providing Mobile Money Services to Rural Poor
  Communities: An effective integration approach of Rural Banks and existing
  mobile communications infrastructure</title><categories>cs.CY</categories><comments>10 pages. Accepted and presented at International Conference on
  Mobile Money Uptake, by Ghana Technology University College (GTUC) in
  partnership with the Institute for Money, Technology and Financial Inclusion
  (IMTFI) USA, at Accra, Ghana, 2013, 2013; Int. Conf on Mobile Money
  Uptake.2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The rapid spread of mobile phones means that the number of mobile users may
already exceed the number of banked people in many low income countries. Mobile
phones can also offer a communications channel for initiating and executing
on-line financial transactions. This channel may not only reduce the cost of
financial transactions for provider and customer, but also allow new entrants
to the financial sector, and new relationships to be formed for distributing
services. These changes hold the prospect of accelerating access to financial
services on the back of the mobile infrastructure. Mobile telephony offers
tremendous promise to facilitate the flow of money among rural and poor
families at much lower transaction costs, bringing the bank to those currently
unbanked. Realizing this promise will require close collaboration among all
stakeholders. But most rural banks do not have mobile banking services for
their customers. This made it difficult for the full potential and benefits of
mobile money financial services to be realized. Most telecommunication service
providers run mobile money service solely for their subscribers without an
integrated approach of incorporating and integrating rural banking systems into
their existing services this makes it difficult for a full fledge exploitation
of the mobile financial market. This paper looks at the existing mobile money
services and takes critical look at the positive advantages of effective
integration approach of Rural Banks and existing mobile communications
infrastructure as well as proposing a model for such integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7790</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7790</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>Using SOA with Web Services for effective Integration of Hospital
  Information Systems via an Enterprise Service Bus</title><categories>cs.SE</categories><comments>6 pages. International Journal of Research in Engineering &amp; Advanced
  Technology (IJREAT), 2013. arXiv admin note: text overlap with
  arXiv:1204.0179 by other authors without attribution</comments><journal-ref>International Journal of Research in Engineering &amp; Advanced
  Technology (IJREAT).pp: 1-6.1.2.(2013)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Hospitals are distributed across geographical areas and it is important for
all hospitals to share information as well as integrate their systems for
effective researching and health delivery. Health personals and institutions in
need of information from hospitals with respect to geographical areas can
easily do researches on patients, treatments, disease outbreaks, and effects of
drugs. This research work is aimed at integrating of database systems of
hospital across geographical areas via a service bus. A centralized service bus
was used to facilitate interoperability of applications across platforms and
enhance communication within the hospital infrastructure as well as creating
enabling environment for new layer of abstractions to be added without
modification of the entire system. Concept of Service Oriented Architecture
with web services was used for rapid integration solution in solving the
challenges faced during integration of multiple incompatible applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7791</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7791</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>A Visual Cryptographic Encryption Technique for Securing Medical Images</title><categories>cs.CR</categories><comments>5 pages. International Journal of Emerging Technology and Advanced
  Engineering (IJREAT), 2013</comments><journal-ref>International Journal of Emerging Technology and Advanced
  Engineering (IJETAE), 3.6 (2013): pp: 496-500</journal-ref><doi>10.5120/16500-5752</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The increased growth in the use of transmission of multimedia medical
contents over unsecured and open networks provides insecurity for confidential
patient information over these networks. Digital encryption of medical images
before transmission and storage is proposed as a way to effectively provide
protection of patient information. Encryption before watermarking of these
images is necessary in order to ensure inaccessibility of information to
unauthorized personnel with patient. This paper presented a visual
cryptographic technique for encrypting of medical images before transmission or
storage of them. This will make such images inaccessible by unauthorized
personnel and also ensures confidentiality. The process made use of an
encryption technique that is based on pixel shuffling and a secret key
generated from the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7792</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7792</id><created>2013-07-29</created><authors><author><keyname>Huang</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Sun</keyname><forenames>Yu-e</forenames></author><author><keyname>Xu</keyname><forenames>Hongli</forenames></author><author><keyname>Huang</keyname><forenames>Liusheng</forenames></author></authors><title>PPS: Privacy-Preserving Strategyproof Social-Efficient Spectrum Auction
  Mechanisms</title><categories>cs.NI cs.GT</categories><comments>10 pages, 4 figures</comments><msc-class>68M10</msc-class><acm-class>C.2.1; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many spectrum auction mechanisms have been proposed for spectrum allocation
problem, and unfortunately, few of them protect the bid privacy of bidders and
achieve good social efficiency. In this paper, we propose PPS, a Privacy
Preserving Strategyproof spectrum auction framework. Then, we design two
schemes based on PPS separately for 1) the Single-Unit Auction model (SUA),
where only single channel to be sold in the spectrum market; and 2) the
Multi-Unit Auction model (MUA), where the primary user subleases multi-unit
channels to the secondary users and each of the secondary users wants to access
multi-unit channels either. Since the social efficiency maximization problem is
NP-hard in both auction models, we present allocation mechanisms with
approximation factors of $(1+\epsilon)$ and 32 separately for SUA and MUA, and
further judiciously design strategyproof auction mechanisms with privacy
preserving based on them. Our extensive evaluations show that our mechanisms
achieve good social efficiency and with low computation and communication
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7793</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7793</id><created>2013-07-29</created><authors><author><keyname>Lim</keyname><forenames>Yongsub</forenames></author><author><keyname>Jung</keyname><forenames>Kyomin</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>Multi-dimensional Parametric Mincuts for Constrained MAP Inference</title><categories>cs.LG cs.AI</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose novel algorithms for inferring the Maximum a
Posteriori (MAP) solution of discrete pairwise random field models under
multiple constraints. We show how this constrained discrete optimization
problem can be formulated as a multi-dimensional parametric mincut problem via
its Lagrangian dual, and prove that our algorithm isolates all constraint
instances for which the problem can be solved exactly. These multiple solutions
enable us to even deal with `soft constraints' (higher order penalty
functions). Moreover, we propose two practical variants of our algorithm to
solve problems with hard constraints. We also show how our method can be
applied to solve various constrained discrete optimization problems such as
submodular minimization and shortest path computation. Experimental evaluation
using the foreground-background image segmentation problem with statistic
constraints reveals that our method is faster and its results are closer to the
ground truth labellings compared with the popular continuous relaxation based
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7795</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7795</id><created>2013-07-29</created><authors><author><keyname>Simha</keyname><forenames>Ramanuja</forenames></author><author><keyname>Shatkay</keyname><forenames>Hagit</forenames></author></authors><title>Protein (Multi-)Location Prediction: Using Location Inter-Dependencies
  in a Probabilistic Framework</title><categories>q-bio.QM cs.CE cs.LG q-bio.GN</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing the location of a protein within the cell is important for
understanding its function, role in biological processes, and potential use as
a drug target. Much progress has been made in developing computational methods
that predict single locations for proteins, assuming that proteins localize to
a single location. However, it has been shown that proteins localize to
multiple locations. While a few recent systems have attempted to predict
multiple locations of proteins, they typically treat locations as independent
or capture inter-dependencies by treating each locations-combination present in
the training set as an individual location-class. We present a new method and a
preliminary system we have developed that directly incorporates
inter-dependencies among locations into the multiple-location-prediction
process, using a collection of Bayesian network classifiers. We evaluate our
system on a dataset of single- and multi-localized proteins. Our results,
obtained by incorporating inter-dependencies are significantly higher than
those obtained by classifiers that do not use inter-dependencies. The
performance of our system on multi-localized proteins is comparable to a top
performing system (YLoc+), without restricting predictions to be based only on
location-combinations present in the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7796</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7796</id><created>2013-07-29</created><authors><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhang</keyname><forenames>Zike</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Huang</keyname><forenames>Zi-Gang</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Emergence of scaling in human-interest dynamics</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human behaviors are often driven by human interests. Despite intense recent
efforts in exploring the dynamics of human behaviors, little is known about
human-interest dynamics, partly due to the extreme difficulty in accessing the
human mind from observations. However, the availability of large-scale data,
such as those from e-commerce and smart-phone communications, makes it possible
to probe into and quantify the dynamics of human interest. Using three
prototypical &quot;big data&quot; sets, we investigate the scaling behaviors associated
with human-interest dynamics. In particular, from the data sets we uncover
power-law scaling associated with the three basic quantities: (1) the length of
continuous interest, (2) the return time of visiting certain interest, and (3)
interest ranking and transition. We argue that there are three basic
ingredients underlying human-interest dynamics: preferential return to
previously visited interests, inertial effect, and exploration of new
interests. We develop a biased random-walk model, incorporating the three
ingredients, to account for the observed power-law scaling relations. Our study
represents the first attempt to understand the dynamical processes underlying
human interest, which has significant applications in science and engineering,
commerce, as well as defense, in terms of specific tasks such as recommendation
and human-behavior prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7800</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7800</id><created>2013-07-29</created><authors><author><keyname>Lim</keyname><forenames>Yongsub</forenames></author><author><keyname>Jung</keyname><forenames>Kyomin</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>Efficient Energy Minimization for Enforcing Statistics</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy minimization algorithms, such as graph cuts, enable the computation of
the MAP solution under certain probabilistic models such as Markov random
fields. However, for many computer vision problems, the MAP solution under the
model is not the ground truth solution. In many problem scenarios, the system
has access to certain statistics of the ground truth. For instance, in image
segmentation, the area and boundary length of the object may be known. In these
cases, we want to estimate the most probable solution that is consistent with
such statistics, i.e., satisfies certain equality or inequality constraints.
  The above constrained energy minimization problem is NP-hard in general, and
is usually solved using Linear Programming formulations, which relax the
integrality constraints. This paper proposes a novel method that finds the
discrete optimal solution of such problems by maximizing the corresponding
Lagrangian dual. This method can be applied to any constrained energy
minimization problem whose unconstrained version is polynomial time solvable,
and can handle multiple, equality or inequality, and linear or non-linear
constraints. We demonstrate the efficacy of our method on the
foreground/background image segmentation problem, and show that it produces
impressive segmentation results with less error, and runs more than 20 times
faster than the state-of-the-art LP relaxation based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7806</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7806</id><created>2013-07-30</created><authors><author><keyname>Kapun</keyname><forenames>Evgeny</forenames></author><author><keyname>Tsarev</keyname><forenames>Fedor</forenames></author></authors><title>On NP-Hardness of the Paired de Bruijn Sound Cycle Problem</title><categories>cs.DS q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paired de Bruijn graph is an extension of de Bruijn graph incorporating
mate pair information for genome assembly proposed by Mevdedev et al. However,
unlike in an ordinary de Bruijn graph, not every path or cycle in a paired de
Bruijn graph will spell a string, because there is an additional soundness
constraint on the path. In this paper we show that the problem of checking if
there is a sound cycle in a paired de Bruijn graph is NP-hard in general case.
We also explore some of its special cases, as well as a modified version where
the cycle must also pass through every edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7807</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7807</id><created>2013-07-30</created><authors><author><keyname>Wang</keyname><forenames>Hongwei</forenames></author><author><keyname>Yu</keyname><forenames>F. Richard</forenames></author><author><keyname>Zhu</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Tao</forenames></author><author><keyname>Ning</keyname><forenames>Bin</forenames></author></authors><title>Finite-State Markov Modeling of Tunnel Channels in Communication-based
  Train Control (CBTC) Systems</title><categories>cs.DM</categories><comments>6 pages, 4 figures, conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Communication-based train control (CBTC) is gradually adopted in urban rail
transit systems, as it can significantly enhance railway network efficiency,
safety and capacity. Since CBTC systems are mostly deployed in underground
tunnels and trains move in high speed, building a train-ground wireless
communication system for CBTC is a challenging task. Modeling the tunnel
channels is very important to design and evaluate the performance of CBTC
systems. Most of existing works on channel modeling do not consider the unique
characteristics in CBTC systems, such as high mobility speed, deterministic
moving direction, and accurate train location information. In this paper, we
develop a finite state Markov channel (FSMC) model for tunnel channels in CBTC
systems. The proposed FSMC model is based on real field CBTC channel
measurements obtained from a business operating subway line. Unlike most
existing channel models, which are not related to specific locations, the
proposed FSMC channel model takes train locations into account to have a more
accurate channel model. The distance between the transmitter and the receiver
is divided into intervals, and an FSMC model is applied in each interval. The
accuracy of the proposed FSMC model is illustrated by the simulation results
generated from the model and the real field measurement results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7808</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7808</id><created>2013-07-30</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Instituto Tecnologico de Buenos Aires</affiliation></author></authors><title>Automated Attack Planning</title><categories>cs.AI cs.CR</categories><comments>PhD Thesis. 171 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Penetration Testing is a methodology for assessing network security, by
generating and executing possible attacks. Doing so automatically allows for
regular and systematic testing. A key question then is how to automatically
generate the attacks. A natural way to address this issue is as an attack
planning problem. In this thesis, we are concerned with the specific context of
regular automated pentesting, and use the term &quot;attack planning&quot; in that sense.
The following three research directions are investigated.
  First, we introduce a conceptual model of computer network attacks, based on
an analysis of the penetration testing practices. We study how this attack
model can be represented in the PDDL language. Then we describe an
implementation that integrates a classical planner with a penetration testing
tool. This allows us to automatically generate attack paths for real world
pentesting scenarios, and to validate these attacks by executing them.
  Secondly, we present efficient probabilistic planning algorithms,
specifically designed for this problem, that achieve industrial-scale runtime
performance (able to solve scenarios with several hundred hosts and exploits).
These algorithms take into account the probability of success of the actions
and their expected cost (for example in terms of execution time, or network
traffic generated).
  Finally, we take a different direction: instead of trying to improve the
efficiency of the solutions developed, we focus on improving the model of the
attacker. We model the attack planning problem in terms of partially observable
Markov decision processes (POMDP). This grounds penetration testing in a
well-researched formalism. POMDPs allow the modelling of information gathering
as an integral part of the problem, thus providing for the first time a means
to intelligently mix scanning actions with actual exploits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7809</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7809</id><created>2013-07-30</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author><author><keyname>Buffet</keyname><forenames>Olivier</forenames><affiliation>INRIA</affiliation></author><author><keyname>Hoffmann</keyname><forenames>Joerg</forenames><affiliation>INRIA</affiliation></author></authors><title>Les POMDP font de meilleurs hackers: Tenir compte de l'incertitude dans
  les tests de penetration</title><categories>cs.AI cs.CR</categories><comments>JFPDA 2012 (7\`emes Journ\'ees Francophones Planification,
  D\'ecision, et Apprentissage pour la conduite de syst\`emes), Nancy, France</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Penetration Testing is a methodology for assessing network security, by
generating and executing possible hacking attacks. Doing so automatically
allows for regular and systematic testing. A key question is how to generate
the attacks. This is naturally formulated as planning under uncertainty, i.e.,
under incomplete knowledge about the network configuration. Previous work uses
classical planning, and requires costly pre-processes reducing this uncertainty
by extensive application of scanning methods. By contrast, we herein model the
attack planning problem in terms of partially observable Markov decision
processes (POMDP). This allows to reason about the knowledge available, and to
intelligently employ scanning actions as part of the attack. As one would
expect, this accurate solution does not scale. We devise a method that relies
on POMDPs to find good attacks on individual machines, which are then composed
into an attack on the network as a whole. This decomposition exploits network
structure to the extent possible, making targeted approximations (only) where
needed. Evaluating this method on a suitably adapted industrial test suite, we
demonstrate its effectiveness in both runtime and solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7810</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7810</id><created>2013-07-30</created><authors><author><keyname>Duma</keyname><forenames>Denisa</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Alpert</keyname><forenames>Matthew</forenames></author><author><keyname>Close</keyname><forenames>Timothy J.</forenames></author><author><keyname>Ciardo</keyname><forenames>Gianfranco</forenames></author><author><keyname>Lonardi</keyname><forenames>Stefano</forenames></author></authors><title>Accurate Decoding of Pooled Sequenced Data Using Compressed Sensing</title><categories>q-bio.QM cs.CE cs.IT math.IT q-bio.GN</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to overcome the limitations imposed by DNA barcoding when
multiplexing a large number of samples in the current generation of
high-throughput sequencing instruments, we have recently proposed a new
protocol that leverages advances in combinatorial pooling design (group
testing) doi:10.1371/journal.pcbi.1003010. We have also demonstrated how this
new protocol would enable de novo selective sequencing and assembly of large,
highly-repetitive genomes. Here we address the problem of decoding pooled
sequenced data obtained from such a protocol. Our algorithm employs a
synergistic combination of ideas from compressed sensing and the decoding of
error-correcting codes. Experimental results on synthetic data for the rice
genome and real data for the barley genome show that our novel decoding
algorithm enables significantly higher quality assemblies than the previous
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7811</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7811</id><created>2013-07-30</created><authors><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author><author><keyname>Kuosmanen</keyname><forenames>Anna</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>M&#xe4;kinen</keyname><forenames>Veli</forenames></author></authors><title>A Novel Combinatorial Method for Estimating Transcript Expression with
  RNA-Seq: Bounding the Number of Paths</title><categories>q-bio.QM cs.CE cs.DS</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RNA-Seq technology offers new high-throughput ways for transcript
identification and quantification based on short reads, and has recently
attracted great interest. The problem is usually modeled by a weighted splicing
graph whose nodes stand for exons and whose edges stand for split alignments to
the exons. The task consists of finding a number of paths, together with their
expression levels, which optimally explain the coverages of the graph under
various fitness functions, such least sum of squares. In (Tomescu et al.
RECOMB-seq 2013) we showed that under general fitness functions, if we allow a
polynomially bounded number of paths in an optimal solution, this problem can
be solved in polynomial time by a reduction to a min-cost flow program. In this
paper we further refine this problem by asking for a bounded number k of paths
that optimally explain the splicing graph. This problem becomes NP-hard in the
strong sense, but we give a fast combinatorial algorithm based on dynamic
programming for it. In order to obtain a practical tool, we implement three
optimizations and heuristics, which achieve better performance on real data,
and similar or better performance on simulated data, than state-of-the-art
tools Cufflinks, IsoLasso and SLIDE. Our tool, called Traph, is available at
http://www.cs.helsinki.fi/gsa/traph/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7813</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7813</id><created>2013-07-30</created><authors><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author><author><keyname>Lacroix</keyname><forenames>Vincent</forenames></author><author><keyname>Sagot</keyname><forenames>Marie-France</forenames></author></authors><title>A polynomial delay algorithm for the enumeration of bubbles with length
  constraints in directed graphs and its application to the detection of
  alternative splicing in RNA-seq data</title><categories>q-bio.QM cs.CE cs.DS</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for enumerating bubbles with length constraints in
directed graphs. This problem arises in transcriptomics, where the question is
to identify all alternative splicing events present in a sample of mRNAs
sequenced by RNA-seq. This is the first polynomial-delay algorithm for this
problem and we show that in practice, it is faster than previous approaches.
This enables us to deal with larger instances and therefore to discover novel
alternative splicing events, especially long ones, that were previously
overseen using existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7814</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7814</id><created>2013-07-30</created><authors><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author></authors><title>Wireless Message Dissemination via Selective Relay over Bluetooth
  (MDSRoB)</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a wireless message dissemination method designed with no
need to trust other users. This method utilizes modern wireless adaptors
ability to broadcast device name and identification information. Using the
scanning features built into Bluetooth and Wifi, messages can be exchanged via
their device names. This paper outlines a method of interchanging multiple
messages to discoverable and nondiscoverable devices using a user defined
scanning interval method along with a response based system. By selectively
relaying messages each user is in control of their involvement in the ad-hoc
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7820</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7820</id><created>2013-07-30</created><authors><author><keyname>Venkatachalam</keyname><forenames>Balaji</forenames></author><author><keyname>Gusfield</keyname><forenames>Dan</forenames></author><author><keyname>Frid</keyname><forenames>Yelena</forenames></author></authors><title>Faster Algorithms for RNA-folding using the Four-Russians method</title><categories>q-bio.QM cs.CE cs.DS</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013). Editor's note: abstract was
  shortened to comply with arxiv requirements. Full abstract in PDF</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secondary structure that maximizes the number of non-crossing matchings
between complimentary bases of an RNA sequence of length n can be computed in
O(n^3) time using Nussinov's dynamic programming algorithm. The Four-Russians
method is a technique that will reduce the running time for certain dynamic
programming algorithms by a multiplicative factor after a preprocessing step
where solutions to all smaller subproblems of a fixed size are exhaustively
enumerated and solved. Frid and Gusfield designed an O(\frac{n^3}{\log n})
algorithm for RNA folding using the Four-Russians technique. In their algorithm
the preprocessing is interleaved with the algorithm computation. (Algo. Mol.
Biol., 2010).
  We simplify the algorithm and the analysis by doing the preprocessing once
prior to the algorithm computation. We call this the two-vector method. We also
show variants where instead of exhaustive preprocessing, we only solve the
subproblems encountered in the main algorithm once and memoize the results. We
give a simple proof of correctness and explore the practical advantages over
the earlier method. The Nussinov algorithm admits an O(n^2) time parallel
algorithm. We show a parallel algorithm using the two-vector idea that improves
the time bound to O(\frac{n^2}{log n}).
  We discuss the organization of the data structures to exploit coalesced
memory access for fast running times. The ideas to organize the data structures
also help in improving the running time of the serial algorithms. For sequences
of length up to 6000 bases the parallel algorithm takes only about 2.5 seconds
and the two-vector serial method takes about 57 seconds on a desktop and 15
seconds on a server. Among the serial algorithms, the two-vector and memoized
versions are faster than the Frid-Gusfield algorithm by a factor of 3, and are
faster than Nussinov by up to a factor of 20.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7821</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7821</id><created>2013-07-30</created><updated>2013-08-06</updated><authors><author><keyname>Jansson</keyname><forenames>Jesper</forenames></author><author><keyname>Shen</keyname><forenames>Chuanqi</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author></authors><title>Algorithms for the Majority Rule (+) Consensus Tree and the Frequency
  Difference Consensus Tree</title><categories>cs.DS cs.CE q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two new deterministic algorithms for constructing
consensus trees. Given an input of k phylogenetic trees with identical leaf
label sets and n leaves each, the first algorithm constructs the majority rule
(+) consensus tree in O(kn) time, which is optimal since the input size is
Omega(kn), and the second one constructs the frequency difference consensus
tree in min(O(kn^2), O(kn (k+log^2 n))) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7822</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7822</id><created>2013-07-30</created><authors><author><keyname>Deng</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Rongqing</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Truthful Mechanisms for Secure Communication in Wireless Cooperative
  System</title><categories>cs.CR cs.GT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To ensure security in data transmission is one of the most important issues
for wireless relay networks, and physical layer security is an attractive
alternative solution to address this issue. In this paper, we consider a
cooperative network, consisting of one source node, one destination node, one
eavesdropper node, and a number of relay nodes. Specifically, the source may
select several relays to help forward the signal to the corresponding
destination to achieve the best security performance. However, the relays may
have the incentive not to report their true private channel information in
order to get more chances to be selected and gain more payoff from the source.
We propose a Vickey-Clark-Grove (VCG) based mechanism and an
Arrow-d'Aspremont-Gerard-Varet (AGV) based mechanism into the investigated
relay network to solve this cheating problem. In these two different
mechanisms, we design different &quot;transfer payment&quot; functions to the payoff of
each selected relay and prove that each relay gets its maximum (expected)
payoff when it truthfully reveals its private channel information to the
source. And then, an optimal secrecy rate of the network can be achieved. After
discussing and comparing the VCG and AGV mechanisms, we prove that the AGV
mechanism can achieve all of the basic qualifications (incentive compatibility,
individual rationality and budget balance) for our system. Moreover, we discuss
the optimal quantity of relays that the source node should select. Simulation
results verify efficiency and fairness of the VCG and AGV mechanisms, and
consolidate these conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7824</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7824</id><created>2013-07-30</created><authors><author><keyname>B&#xf6;cker</keyname><forenames>Sebastian</forenames></author><author><keyname>Canzar</keyname><forenames>Stefan</forenames></author><author><keyname>Klau</keyname><forenames>Gunnar W.</forenames></author></authors><title>The generalized Robinson-Foulds metric</title><categories>cs.DS cs.CE q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Robinson-Foulds (RF) metric is arguably the most widely used measure of
phylogenetic tree similarity, despite its well-known shortcomings: For example,
moving a single taxon in a tree can result in a tree that has maximum distance
to the original one; but the two trees are identical if we remove the single
taxon. To this end, we propose a natural extension of the RF metric that does
not simply count identical clades but instead, also takes similar clades into
consideration. In contrast to previous approaches, our model requires the
matching between clades to respect the structure of the two trees, a property
that the classical RF metric exhibits, too. We show that computing this
generalized RF metric is, unfortunately, NP-hard. We then present a simple
Integer Linear Program for its computation, and evaluate it by an
all-against-all comparison of 100 trees from a benchmark data set. We find that
matchings that respect the tree structure differ significantly from those that
do not, underlining the importance of this natural condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7825</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7825</id><created>2013-07-30</created><authors><author><keyname>Tsirogiannis</keyname><forenames>Constantinos</forenames></author><author><keyname>Sandel</keyname><forenames>Brody</forenames></author></authors><title>Computing the Skewness of the Phylogenetic Mean Pairwise Distance in
  Linear Time</title><categories>q-bio.QM cs.CE cs.DS</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phylogenetic Mean Pairwise Distance (MPD) is one of the most popular
measures for computing the phylogenetic distance between a given group of
species. More specifically, for a phylogenetic tree T and for a set of species
R represented by a subset of the leaf nodes of T, the MPD of R is equal to the
average cost of all possible simple paths in T that connect pairs of nodes in
R.
  Among other phylogenetic measures, the MPD is used as a tool for deciding if
the species of a given group R are closely related. To do this, it is important
to compute not only the value of the MPD for this group but also the
expectation, the variance, and the skewness of this metric. Although efficient
algorithms have been developed for computing the expectation and the variance
the MPD, there has been no approach so far for computing the skewness of this
measure.
  In the present work we describe how to compute the skewness of the MPD on a
tree T optimally, in Theta(n) time; here n is the size of the tree T. So far
this is the first result that leads to an exact, let alone efficient,
computation of the skewness for any popular phylogenetic distance measure.
Moreover, we show how we can compute in Theta(n) time several interesting
quantities in T that can be possibly used as building blocks for computing
efficiently the skewness of other phylogenetic measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7828</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7828</id><created>2013-07-30</created><authors><author><keyname>Vakati</keyname><forenames>Sudheer</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>Characterizing Compatibility and Agreement of Unrooted Trees via Cuts in
  Graphs</title><categories>cs.DM cs.CE q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding whether there is a single tree -a supertree- that summarizes the
evolutionary information in a collection of unrooted trees is a fundamental
problem in phylogenetics. We consider two versions of this question: agreement
and compatibility. In the first, the supertree is required to reflect precisely
the relationships among the species exhibited by the input trees. In the
second, the supertree can be more refined than the input trees.
  Tree compatibility can be characterized in terms of the existence of a
specific kind of triangulation in a structure known as the display graph.
Alternatively, it can be characterized as a chordal graph sandwich problem in a
structure known as the edge label intersection graph. Here, we show that the
latter characterization yields a natural characterization of compatibility in
terms of minimal cuts in the display graph, which is closely related to
compatibility of splits. We then derive a characterization for agreement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7831</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7831</id><created>2013-07-30</created><authors><author><keyname>Wieseke</keyname><forenames>Nicolas</forenames></author><author><keyname>Bernt</keyname><forenames>Matthias</forenames></author><author><keyname>Middendorf</keyname><forenames>Martin</forenames></author></authors><title>Unifying Parsimonious Tree Reconciliation</title><categories>q-bio.QM cs.CE cs.DS q-bio.PE</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution is a process that is influenced by various environmental factors,
e.g. the interactions between different species, genes, and biogeographical
properties. Hence, it is interesting to study the combined evolutionary history
of multiple species, their genes, and the environment they live in. A common
approach to address this research problem is to describe each individual
evolution as a phylogenetic tree and construct a tree reconciliation which is
parsimonious with respect to a given event model. Unfortunately, most of the
previous approaches are designed only either for host-parasite systems, for
gene tree/species tree reconciliation, or biogeography. Hence, a method is
desirable, which addresses the general problem of mapping phylogenetic trees
and covering all varieties of coevolving systems, including e.g., predator-prey
and symbiotic relationships. To overcome this gap, we introduce a generalized
cophylogenetic event model considering the combinatorial complete set of local
coevolutionary events. We give a dynamic programming based heuristic for
solving the maximum parsimony reconciliation problem in time O(n^2), for two
phylogenies each with at most n leaves. Furthermore, we present an exact
branch-and-bound algorithm which uses the results from the dynamic programming
heuristic for discarding partial reconciliations. The approach has been
implemented as a Java application which is freely available from
http://pacosy.informatik.uni-leipzig.de/coresym.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7833</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7833</id><created>2013-07-30</created><authors><author><keyname>Trivedi</keyname><forenames>Animesh Kr</forenames></author><author><keyname>Kapoor</keyname><forenames>Rishi</forenames></author><author><keyname>Arora</keyname><forenames>Rajan</forenames></author><author><keyname>Sanyal</keyname><forenames>Sudip</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>RISM -- Reputation Based Intrusion Detection System for Mobile Ad hoc
  Networks</title><categories>cs.CR cs.NI</categories><comments>4 pages, 3 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:1006.1956; and with arXiv:1005.4023 by other authors</comments><journal-ref>3rd International Conference on Computers and Devices for
  Communication (CODEC-06) Institute of Radio Physics and Electronics,
  University of Calcutta, December 18-20, 2006, pp. 234-237</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a combination of an Intrusion Detection System with a
routing protocol to strengthen the defense of a Mobile Ad hoc Network. Our
system is Socially Inspired, since we use the new paradigm of Reputation
inherited from human behavior. The proposed IDS also has a unique
characteristic of being Semi-distributed, since it neither distributes its
Observation results globally nor keeps them entirely locally; however, managing
to communicate this vital information without accretion of the network traffic.
This innovative approach also avoids void assumptions and complex calculations
for calculating and maintaining trust values used to estimate the reliability
of other nodes observations. A robust Path Manager and Monitor system and
Redemption and Fading concepts are other salient features of this design. The
design has shown to outperform normal DSR in terms of Packet Delivery Ratio and
Routing Overhead even when up to half of nodes in the network behave as
malicious.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7836</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7836</id><created>2013-07-30</created><updated>2015-02-24</updated><authors><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>LIP6, Syst&#xe8;mes Polynomiaux</affiliation></author><author><keyname>Schost</keyname><forenames>Eric</forenames></author></authors><title>A nearly optimal algorithm for deciding connectivity queries in smooth
  and bounded real algebraic sets</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A roadmap for a semi-algebraic set $S$ is a curve which has a non-empty and
connected intersection with all connected components of $S$. Hence, this kind
of object, introduced by Canny, can be used to answer connectivity queries
(with applications, for instance, to motion planning) but has also become of
central importance in effective real algebraic geometry, since it is used in
many higher-level algorithms. For a long time, the best known complexity result
for computing roadmaps, due to Basu, Pollack and Roy, was $s^{d+1} D^{O(n^2)}$,
where the input is given by $s$ polynomials of degree $D$ in $n$ variables,
with $d \le n$ the dimension of an associated geometric object. In 2011, we
introduced new proof techniques for establishing connectivity results in real
algebraic sets. This gave us more freedom for the design of algorithms
computing roadmaps and led us to a first probabilistic roadmap algorithm for
smooth and bounded real hypersurfaces running in time $(nD)^{O(n^{1.5})}$. With
Basu and Roy, we then obtained a deterministic algorithm for general real
algebraic sets running in time $D^{O(n^{1.5})}$. Recently, Basu and Roy
improved this result to obtain an algorithm computing a roadmap of degree
polynomial in $n^{n\log^2(n)} D^{n\log(n)}$, in time polynomial in
$n^{n\log^3(n)} D^{n\log^2(n)}$; this is close to the expected optimal $D^n$.
In this paper, we provide a probabilistic algorithm which computes roadmaps for
smooth and bounded real algebraic sets such that the output size and the
running time are polynomial in $(nD)^{n\log(n)}$. More precisely, the running
time of the algorithm is essentially subquadratic in the output size. Even
under these extra assumptions, it is the first roadmap algorithm with output
size and running time polynomial in $(nD)^{n\log(n)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7838</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7838</id><created>2013-07-30</created><updated>2014-01-25</updated><authors><author><keyname>Jung</keyname><forenames>Sang Yeob</forenames></author><author><keyname>Yu</keyname><forenames>Seung Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Asymmetric-valued Spectrum Auction and Competition in Wireless Broadband
  Services</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study bidding and pricing competition between two spiteful mobile network
operators (MNOs) with considering their existing spectrum holdings. Given
asymmetric-valued spectrum blocks are auctioned off to them via a first-price
sealed-bid auction, we investigate the interactions between two spiteful MNOs
and users as a three-stage dynamic game and characterize the dynamic game's
equilibria. We show an asymmetric pricing structure and different market share
between two spiteful MNOs. Perhaps counter-intuitively, our results show that
the MNO who acquires the less-valued spectrum block always lowers his service
price despite providing double-speed LTE service to users. We also show that
the MNO who acquires the high-valued spectrum block, despite charing a higher
price, still achieves more market share than the other MNO. We further show
that the competition between two MNOs leads to some loss of their revenues. By
investigating a cross-over point at which the MNOs' profits are switched, it
serves as the benchmark of practical auction designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7840</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7840</id><created>2013-07-30</created><authors><author><keyname>Zanetti</keyname><forenames>Jo&#xe3;o Paulo Pereira</forenames></author><author><keyname>Biller</keyname><forenames>Priscila</forenames></author><author><keyname>Meidanis</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>On the Matrix Median Problem</title><categories>q-bio.QM cs.CE cs.DM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Genome Median Problem is an important problem in phylogenetic
reconstruction under rearrangement models. It can be stated as follows: given
three genomes, find a fourth that minimizes the sum of the pairwise
rearrangement distances between it and the three input genomes. Recently,
Feijao and Meidanis extended the algebraic theory for genome rearrangement to
allow for linear chromosomes, thus yielding a new rearrangement model (the
algebraic model), very close to the celebrated DCJ model. In this paper, we
study the genome median problem under the algebraic model, whose complexity is
currently open, proposing a more general form of the problem, the matrix median
problem. It is known that, for any metric distance, at least one of the corners
is a 4/3-approximation of the median. Our results allow us to compute up to
three additional matrix median candidates, all of them with approximation
ratios at least as good as the best corner, when the input matrices come from
genomes. From the application point of view, it is usually more interesting to
locate medians farther from the corners. We also show a fourth median candidate
that gives better results in cases we tried. However, we do not have proven
bounds for this fourth candidate yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7842</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7842</id><created>2013-07-30</created><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Fertin</keyname><forenames>Guillaume</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>A Fixed-Parameter Algorithm for Minimum Common String Partition with Few
  Duplications</title><categories>cs.DS cs.CE q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the study of genome rearrangements, the NP-hard Minimum Common
String Partition problems asks, given two strings, to split both strings into
an identical set of blocks. We consider an extension of this problem to
unbalanced strings, so that some elements may not be covered by any block. We
present an efficient fixed-parameter algorithm for the parameters number k of
blocks and maximum occurrence d of a letter in either string. We then evaluate
this algorithm on bacteria genomes and synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7848</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7848</id><created>2013-07-30</created><authors><author><keyname>Paletta</keyname><forenames>Lucas</forenames></author><author><keyname>Santner</keyname><forenames>Katrin</forenames></author><author><keyname>Fritz</keyname><forenames>Gerald</forenames></author></authors><title>An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human
  Attention</title><categories>cs.CV</categories><report-no>ISACS/2013/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work describes a computer vision system that enables pervasive mapping
and monitoring of human attention. The key contribution is that our methodology
enables full 3D recovery of the gaze pointer, human view frustum and associated
human centered measurements directly into an automatically computed 3D model in
real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D
modeling, localization and fully automated annotation of ROIs (regions of
interest) within the acquired 3D model. This innovative methodology will open
new avenues for attention studies in real world environments, bringing new
potential into automated processing for human factors technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7849</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7849</id><created>2013-07-30</created><authors><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>CNRS - &#xc9;cole Polytechnique</affiliation></author><author><keyname>Paolini</keyname><forenames>Luca</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author></authors><title>Proceedings Sixth Workshop on Intersection Types and Related Systems</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.4.1, D.3.1</acm-class><journal-ref>EPTCS 121, 2013</journal-ref><doi>10.4204/EPTCS.121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Sixth Workshop on Intersection
Types and Related Systems (ITRS 2012). The workshop was held in Dubrovnik
(Croatia) on June 29th, 2012, affiliated to Twenty-Seventh Annual ACM/IEEE
Symposium on Logic in Computer Science (LICS 2012). The ITRS workshop aims to
bring together researchers working on both the theory and practical
applications of systems based on intersection types and related approaches
(e.g., union types, refinement types, behavioural types, recursive types, and
so on).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7851</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7851</id><created>2013-07-30</created><authors><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Xu</keyname><forenames>Hao</forenames></author><author><keyname>Hua</keyname><forenames>Xian-Sheng</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Hybrid Affinity Propagation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we address a problem of managing tagged images with hybrid
summarization. We formulate this problem as finding a few image exemplars to
represent the image set semantically and visually, and solve it in a hybrid way
by exploiting both visual and textual information associated with images. We
propose a novel approach, called homogeneous and heterogeneous message
propagation ($\text{H}^\text{2}\text{MP}$). Similar to the affinity propagation
(AP) approach, $\text{H}^\text{2}\text{MP}$ reduce the conventional
\emph{vector} message propagation to \emph{scalar} message propagation to make
the algorithm more efficient. Beyond AP that can only handle homogeneous data,
$\text{H}^\text{2}\text{MP}$ generalizes it to exploit extra heterogeneous
relations and the generalization is non-trivial as the reduction to scalar
messages from vector messages is more challenging. The main advantages of our
approach lie in 1) that $\text{H}^\text{2}\text{MP}$ exploits visual similarity
and in addition the useful information from the associated tags, including the
associations relation between images and tags and the relations within tags,
and 2) that the summary is both visually and semantically satisfactory. In
addition, our approach can also present a textual summary to a tagged image
collection, which can be used to automatically generate a textual description.
The experimental results demonstrate the effectiveness and efficiency of the
roposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7852</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7852</id><created>2013-07-30</created><authors><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Zeng</keyname><forenames>Gang</forenames></author><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author><author><keyname>Gan</keyname><forenames>Rui</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Scalable $k$-NN graph construction</title><categories>cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The $k$-NN graph has played a central role in increasingly popular
data-driven techniques for various learning and vision tasks; yet, finding an
efficient and effective way to construct $k$-NN graphs remains a challenge,
especially for large-scale high-dimensional data. In this paper, we propose a
new approach to construct approximate $k$-NN graphs with emphasis in:
efficiency and accuracy. We hierarchically and randomly divide the data points
into subsets and build an exact neighborhood graph over each subset, achieving
a base approximate neighborhood graph; we then repeat this process for several
times to generate multiple neighborhood graphs, which are combined to yield a
more accurate approximate neighborhood graph. Furthermore, we propose a
neighborhood propagation scheme to further enhance the accuracy. We show both
theoretical and empirical accuracy and efficiency of our approach to $k$-NN
graph construction and demonstrate significant speed-up in dealing with large
scale visual data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7858</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7858</id><created>2013-07-30</created><authors><author><keyname>Malinina</keyname><forenames>Natalia</forenames></author></authors><title>The chromatic class and the chromatic number of the planar conjugated
  triangulation</title><categories>cs.DM</categories><comments>16 p., 15 fig</comments><msc-class>05Cxx (primary), 68Rxx (secondary)</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This material is dedicated to the estimation of the chromatic number and
chromatic class of the conjugated triangulation (first conversion) and also of
the second conversion of the planar triangulation. Also this paper introduces
some new hypotheses, which are equivalent to Four Color Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7867</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7867</id><created>2013-07-30</created><updated>2014-07-14</updated><authors><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Emmett</keyname><forenames>Matthew</forenames></author><author><keyname>Bolten</keyname><forenames>Matthias</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>A space-time parallel solver for the three-dimensional heat equation</title><categories>cs.NA cs.DC math.NA</categories><comments>10 pages</comments><journal-ref>Advances in Parallel Computing 25, IOS Press, pages 263 - 272,
  2014</journal-ref><doi>10.3233/978-1-61499-381-0-263</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a combination of the time-parallel &quot;parallel full
approximation scheme in space and time&quot; (PFASST) with a parallel multigrid
method (PMG) in space, resulting in a mesh-based solver for the
three-dimensional heat equation with a uniquely high degree of efficient
concurrency. Parallel scaling tests are reported on the Cray XE6 machine &quot;Monte
Rosa&quot; on up to 16,384 cores and on the IBM Blue Gene/Q system &quot;JUQUEEN&quot; on up
to 65,536 cores. The efficacy of the combined spatial- and temporal
parallelization is shown by demonstrating that using PFASST in addition to PMG
significantly extends the strong-scaling limit. Implications of using spatial
coarsening strategies in PFASST's multi-level hierarchy in large-scale parallel
simulations are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7887</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7887</id><created>2013-07-30</created><updated>2013-12-30</updated><authors><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Fast Algorithms for Refined Parameterized Telescoping in Difference
  Fields</title><categories>cs.SC</categories><comments>In the updated versions typos are removed and illustrative examples
  are inluced</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameterized telescoping (including telescoping and creative telescoping)
and refined versions of it play a central role in the research area of symbolic
summation. Karr introduced 1981 $\Pi\Sigma$-fields, a general class of
difference fields, that enables one to consider this problem for indefinite
nested sums and products covering as special cases, e.g., the
($q$--)hypergeometric case and their mixed versions. This survey article
presents the available algorithms in the framework of $\Pi\Sigma$-extensions
and elaborates new results concerning efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7895</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7895</id><created>2013-07-30</created><authors><author><keyname>Avdakovic</keyname><forenames>Samir</forenames></author><author><keyname>Nuhanovic</keyname><forenames>Amir</forenames></author><author><keyname>Kusljugic</keyname><forenames>Mirza</forenames></author><author><keyname>Becirovic</keyname><forenames>Elvisa</forenames></author></authors><title>Wavelet Analysis of Dynamic Behaviors of the Large Interconnected Power
  System</title><categories>cs.SY</categories><journal-ref>Inter. Jour. of Scientific &amp; Engineering Research., vol. 3, pp.
  1-5, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the simulation of the disturbance propagation through a large
power system is performed on the WSCC 127 bus test system. The signal frequency
analysis from several parts of the power system is performed by applying the
Wavelet Transform (WT). The results show that this approach provides the system
operators with some useful information regarding the identification of the
power system low-frequency electromechanical oscillations, the identification
of the coherent groups of generators and the insight into the speed retardation
of some parts of the power system. The ability to localize the disturbance is
based on the disturbance propagation through the power system and the
time-frequency analysis performed by using the WT is presented along with
detailed physical interpretation of the used approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7897</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7897</id><created>2013-07-30</created><authors><author><keyname>Omerhodzic</keyname><forenames>Ibrahim</forenames></author><author><keyname>Avdakovic</keyname><forenames>Samir</forenames></author><author><keyname>Nuhanovic</keyname><forenames>Amir</forenames></author><author><keyname>Dizdarevic</keyname><forenames>Kemal</forenames></author></authors><title>Energy Distribution of EEG Signals: EEG Signal Wavelet-Neural Network
  Classifier</title><categories>cs.NE q-bio.NC</categories><journal-ref>World Academy of Science, Engineering and Technology, 61,
  1190-1195, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a wavelet-based neural network (WNN) classifier for
recognizing EEG signals is implemented and tested under three sets EEG signals
(healthy subjects, patients with epilepsy and patients with epileptic syndrome
during the seizure). First, the Discrete Wavelet Transform (DWT) with the
Multi-Resolution Analysis (MRA) is applied to decompose EEG signal at
resolution levels of the components of the EEG signal (delta, theta, alpha,
beta and gamma) and the Parsevals theorem are employed to extract the
percentage distribution of energy features of the EEG signal at different
resolution levels. Second, the neural network (NN) classifies these extracted
features to identify the EEGs type according to the percentage distribution of
energy features. The performance of the proposed algorithm has been evaluated
using in total 300 EEG signals. The results showed that the proposed classifier
has the ability of recognizing and classifying EEG signals efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7925</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7925</id><created>2013-07-30</created><authors><author><keyname>Onodera</keyname><forenames>Taku</forenames></author><author><keyname>Sadakane</keyname><forenames>Kunihiko</forenames></author><author><keyname>Shibuya</keyname><forenames>Tetsuo</forenames></author></authors><title>Detecting Superbubbles in Assembly Graphs</title><categories>cs.DS cs.CE cs.DM q-bio.QM</categories><comments>Peer-reviewed and presented as part of the 13th Workshop on
  Algorithms in Bioinformatics (WABI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new concept of a subgraph class called a superbubble for
analyzing assembly graphs, and propose an efficient algorithm for detecting it.
Most assembly algorithms utilize assembly graphs like the de Bruijn graph or
the overlap graph constructed from reads. From these graphs, many assembly
algorithms first detect simple local graph structures (motifs), such as tips
and bubbles, mainly to find sequencing errors. These motifs are easy to detect,
but they are sometimes too simple to deal with more complex errors. The
superbubble is an extension of the bubble, which is also important for
analyzing assembly graphs. Though superbubbles are much more complex than
ordinary bubbles, we show that they can be efficiently enumerated. We propose
an average-case linear time algorithm (i.e., O(n+m) for a graph with n vertices
and m edges) for graphs with a reasonable model, though the worst-case time
complexity of our algorithm is quadratic (i.e., O(n(n+m))). Moreover, the
algorithm is practically very fast: Our experiments show that our algorithm
runs in reasonable time with a single CPU core even against a very large graph
of a whole human genome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7943</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7943</id><created>2013-07-30</created><authors><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Zhou</keyname><forenames>Runlin</forenames></author><author><keyname>Zhu</keyname><forenames>Chunge</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Shi</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lixin</forenames></author></authors><title>The Implications of Diverse Applications and Scalable Data Sets in
  Benchmarking Big Data Systems</title><categories>cs.PF</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now we live in an era of big data, and big data applications are becoming
more and more pervasive. How to benchmark data center computer systems running
big data applications (in short big data systems) is a hot topic. In this
paper, we focus on measuring the performance impacts of diverse applications
and scalable volumes of data sets on big data systems. For four typical data
analysis applications---an important class of big data applications, we find
two major results through experiments: first, the data scale has a significant
impact on the performance of big data systems, so we must provide scalable
volumes of data sets in big data benchmarks. Second, for the four applications,
even all of them use the simple algorithms, the performance trends are
different with increasing data scales, and hence we must consider not only
variety of data sets but also variety of applications in benchmarking big data
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7948</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7948</id><created>2013-07-30</created><authors><author><keyname>Kuljus</keyname><forenames>Kristi</forenames></author><author><keyname>Lember</keyname><forenames>J&#xfc;ri</forenames></author></authors><title>On the accuracy of the Viterbi alignment</title><categories>stat.ME cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a hidden Markov model, the underlying Markov chain is usually hidden.
Often, the maximum likelihood alignment (Viterbi alignment) is used as its
estimate. Although having the biggest likelihood, the Viterbi alignment can
behave very untypically by passing states that are at most unexpected. To avoid
such situations, the Viterbi alignment can be modified by forcing it not to
pass these states. In this article, an iterative procedure for improving the
Viterbi alignment is proposed and studied. The iterative approach is compared
with a simple bunch approach where a number of states with low probability are
all replaced at the same time. It can be seen that the iterative way of
adjusting the Viterbi alignment is more efficient and it has several advantages
over the bunch approach. The same iterative algorithm for improving the Viterbi
alignment can be used in the case of peeping, that is when it is possible to
reveal hidden states. In addition, lower bounds for classification
probabilities of the Viterbi alignment under different conditions on the model
parameters are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7951</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7951</id><created>2013-07-30</created><authors><author><keyname>Ninagawa</keyname><forenames>Shigeru</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Genaro J.</forenames></author></authors><title>Complexity Analysis in Cyclic Tag System Emulated by Rule 110</title><categories>cs.OH</categories><comments>AUTOMATA 2013: 19th International Workshop on Cellular Automata and
  Discrete Complex Systems, Universit\&quot;at Giessen, Germany, September 17-19,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that elementary cellular automaton rule 110 is capable of
supporting universal computation by emulating cyclic tag system. Since the
whole information necessary to perform computation is stored in the
configuration, it is reasonable to investigate the complexity of configuration
for the analysis of computing process. In this research we employed Lempel-Ziv
complexity as a measure of complexity and calculated it during the evolution of
emulating cyclic tag system by rule 110. As a result, we observed the stepwise
decline of complexity during the evolution. That is caused by the
transformation from table data to moving data and the elimination of table data
by a rejector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7970</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7970</id><created>2013-06-30</created><updated>2015-03-03</updated><authors><author><keyname>Charles</keyname><forenames>Adam S.</forenames></author><author><keyname>Yap</keyname><forenames>Han Lun</forenames></author><author><keyname>Rozell</keyname><forenames>Christopher J.</forenames></author></authors><title>Short Term Memory Capacity in Networks via the Restricted Isometry
  Property</title><categories>cs.IT cs.NE math.IT</categories><comments>50 pages, 5 figures</comments><journal-ref>A.S. Charles, H.L. Yap, and C.J. Rozell. Short term network memory
  capacity via the restricted isometry property. Neural Computation, 26(6),
  June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cortical networks are hypothesized to rely on transient network activity to
support short term memory (STM). In this paper we study the capacity of
randomly connected recurrent linear networks for performing STM when the input
signals are approximately sparse in some basis. We leverage results from
compressed sensing to provide rigorous non asymptotic recovery guarantees,
quantifying the impact of the input sparsity level, the input sparsity basis,
and the network characteristics on the system capacity. Our analysis
demonstrates that network memory capacities can scale superlinearly with the
number of nodes, and in some situations can achieve STM capacities that are
much larger than the network size. We provide perfect recovery guarantees for
finite sequences and recovery bounds for infinite sequences. The latter
analysis predicts that network STM systems may have an optimal recovery length
that balances errors due to omission and recall mistakes. Furthermore, we show
that the conditions yielding optimal STM capacity can be embodied in several
network topologies, including networks with sparse or dense connectivities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7973</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7973</id><created>2013-07-30</created><authors><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Yakhnenko</keyname><forenames>Oksana</forenames></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames></author></authors><title>Connecting Language and Knowledge Bases with Embedding Models for
  Relation Extraction</title><categories>cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach for relation extraction from free text
which is trained to jointly use information from the text and from existing
knowledge. Our model is based on two scoring functions that operate by learning
low-dimensional embeddings of words and of entities and relationships from a
knowledge base. We empirically show on New York Times articles aligned with
Freebase relations that our approach is able to efficiently use the extra
information provided by a large subset of Freebase data (4M entities, 23k
relationships) to improve over existing methods that rely on text features
alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7974</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7974</id><created>2013-07-29</created><authors><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Zhou</keyname><forenames>Jiazhen</forenames></author><author><keyname>Xu</keyname><forenames>Hao</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author><author><keyname>Hua</keyname><forenames>Xian-Sheng</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Image Tag Refinement by Regularized Latent Dirichlet Allocation</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tagging is nowadays the most prevalent and practical way to make images
searchable. However, in reality many manually-assigned tags are irrelevant to
image content and hence are not reliable for applications. A lot of recent
efforts have been conducted to refine image tags. In this paper, we propose to
do tag refinement from the angle of topic modeling and present a novel
graphical model, regularized Latent Dirichlet Allocation (rLDA). In the
proposed approach, tag similarity and tag relevance are jointly estimated in an
iterative manner, so that they can benefit from each other, and the multi-wise
relationships among tags are explored. Moreover, both the statistics of tags
and visual affinities of images in the corpus are explored to help topic
modeling. We also analyze the superiority of our approach from the deep
structure perspective. The experiments on tag ranking and image retrieval
demonstrate the advantages of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7976</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7976</id><created>2013-07-30</created><updated>2013-07-31</updated><authors><author><keyname>Dolev</keyname><forenames>Danny</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author></authors><title>Node-Initiated Byzantine Consensus Without a Common Clock</title><categories>cs.DC</categories><comments>19 pages, no figures; under submission to SODA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of the literature on consensus assumes that protocols are
jointly started at all nodes of the distributed system. We show how to remove
this problematic assumption in semi-synchronous systems, where messages delays
and relative drifts of local clocks may vary arbitrarily within known bounds.
Our framework is self-stabilizing and efficient both in terms of communication
and time; more concretely, compared to a synchronous start in a synchronous
model of a non-self-stabilizing protocol, we achieve a constant-factor increase
in the time and communicated bits to complete an instance, plus an additive
communication overhead of O(n log n) broadcasted bits per time unit and node.
The latter can be further reduced, at an additive increase in time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7981</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7981</id><created>2013-07-30</created><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author><author><keyname>Doddington</keyname><forenames>George</forenames></author></authors><title>Likelihood-ratio calibration using prior-weighted proper scoring rules</title><categories>stat.ML cs.LG</categories><comments>Accepted, Interspeech 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior-weighted logistic regression has become a standard tool for calibration
in speaker recognition. Logistic regression is the optimization of the expected
value of the logarithmic scoring rule. We generalize this via a parametric
family of proper scoring rules. Our theoretical analysis shows how different
members of this family induce different relative weightings over a spectrum of
applications of which the decision thresholds range from low to high. Special
attention is given to the interaction between prior weighting and proper
scoring rule parameters. Experiments on NIST SRE'12 suggest that for
applications with low false-alarm rate requirements, scoring rules tailored to
emphasize higher score thresholds may give better accuracy than logistic
regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7982</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7982</id><created>2013-07-30</created><authors><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Myers</keyname><forenames>Daniel</forenames></author></authors><title>Flavor Pairing in Medieval European Cuisine: A Study in Cooking with
  Dirty Data</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>IJCAI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important part of cooking with computers is using statistical methods to
create new, flavorful ingredient combinations. The flavor pairing hypothesis
states that culinary ingredients with common chemical flavor components combine
well to produce pleasant dishes. It has been recently shown that this design
principle is a basis for modern Western cuisine and is reversed for Asian
cuisine.
  Such data-driven analysis compares the chemistry of ingredients to ingredient
sets found in recipes. However, analytics-based generation of novel flavor
profiles can only be as good as the underlying chemical and recipe data.
Incomplete, inaccurate, and irrelevant data may degrade flavor pairing
inferences. Chemical data on flavor compounds is incomplete due to the nature
of the experiments that must be conducted to obtain it. Recipe data may have
issues due to text parsing errors, imprecision in textual descriptions of
ingredients, and the fact that the same ingredient may be known by different
names in different recipes. Moreover, the process of matching ingredients in
chemical data and recipe data may be fraught with mistakes. Much of the
`dirtiness' of the data cannot be cleansed even with manual curation.
  In this work, we collect a new data set of recipes from Medieval Europe
before the Columbian Exchange and investigate the flavor pairing hypothesis
historically. To investigate the role of data incompleteness and error as part
of this hypothesis testing, we use two separate chemical compound data sets
with different levels of cleanliness. Notably, the different data sets give
conflicting conclusions about the flavor pairing hypothesis in Medieval Europe.
As a contribution towards social science, we obtain inferences about the
evolution of culinary arts when many new ingredients are suddenly made
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7993</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7993</id><created>2013-07-30</created><authors><author><keyname>Wang</keyname><forenames>Weiguang</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Sharp Threshold for Multivariate Multi-Response Linear Regression via
  Block Regularized Lasso</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a multivariate multi-response (MVMR) linear
regression problem, which contains multiple linear regression models with
differently distributed design matrices, and different regression and output
vectors. The goal is to recover the support union of all regression vectors
using $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary
conditions on sample complexity \emph{as a sharp threshold} to guarantee
successful recovery of the support union. Namely, if the sample size is above
the threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support
union; and if the sample size is below the threshold, $l_1/l_2$-regularized
Lasso fails to recover the support union. In particular, the threshold
precisely captures the impact of the sparsity of regression vectors and the
statistical properties of the design matrices on sample complexity. Therefore,
the threshold function also captures the advantages of joint support union
recovery using multi-task Lasso over individual support recovery using
single-task Lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.7994</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.7994</id><created>2013-07-30</created><authors><author><keyname>Kahl</keyname><forenames>Thomas</forenames></author></authors><title>The homology graph of a higher dimensional automaton</title><categories>math.AT cs.FL</categories><comments>The preliminaries section is taken from arXiv:1303.2003v1</comments><msc-class>55N99, 55U10, 68Q45, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher dimensional automata, i.e. labelled precubical sets, model concurrent
systems. We introduce the homology graph of an HDA, which is a directed graph
whose nodes are the homology classes of the HDA. We show that the homology
graph is invariant under homeomorphic abstraction, i.e. under weak morphisms
that are homeomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8007</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8007</id><created>2013-07-30</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Cai</keyname><forenames>Minglai</forenames></author><author><keyname>Deppe</keyname><forenames>Christian</forenames></author></authors><title>Classical-Quantum Arbitrarily Varying Wiretap Channel - A Capacity
  Formula with Ahlswede Dichotomy - Resources</title><categories>cs.IT math.IT quant-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We establish Ahlswede Dichotomy for arbitrarily varying classical-quantum
wiretap channels. This means that either the deterministic secrecy capacity of
an arbitrarily varying classical-quantum wiretap channel is zero or it equals
its randomness assisted secrecy capacity. We analyze the secrecy capacity of
arbitrarily varying classical-quantum wiretap channels when the sender and the
receiver use various resources. It turns out that having randomness, common
randomness, and correlation as resources are very helpful for achieving a
positive deterministic secrecy capacity of arbitrarily varying
classical-quantum wiretap channels. We prove the phenomenon ?super-activation?
for arbitrarily varying classical-quantum wiretap channels, i.e., if we use two
arbitrarily varying classical-quantum wiretap channels, both with zero
deterministic secrecy capacity together,they allow perfect secure transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8012</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8012</id><created>2013-07-30</created><authors><author><keyname>Lyon</keyname><forenames>R. J.</forenames></author><author><keyname>Brooke</keyname><forenames>J. M.</forenames></author><author><keyname>Knowles</keyname><forenames>J. D.</forenames></author><author><keyname>Stappers</keyname><forenames>B. W.</forenames></author></authors><title>A Study on Classification in Imbalanced and Partially-Labelled Data
  Streams</title><categories>astro-ph.IM cs.LG</categories><comments>6 Pages, 2 figures, to be published in Proceedings 2013 IEEE
  International Conference on Systems, Man, and Cybernetics (SMC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The domain of radio astronomy is currently facing significant computational
challenges, foremost amongst which are those posed by the development of the
world's largest radio telescope, the Square Kilometre Array (SKA). Preliminary
specifications for this instrument suggest that the final design will
incorporate between 2000 and 3000 individual 15 metre receiving dishes, which
together can be expected to produce a data rate of many TB/s. Given such a high
data rate, it becomes crucial to consider how this information will be
processed and stored to maximise its scientific utility. In this paper, we
consider one possible data processing scenario for the SKA, for the purposes of
an all-sky pulsar survey. In particular we treat the selection of promising
signals from the SKA processing pipeline as a data stream classification
problem. We consider the feasibility of classifying signals that arrive via an
unlabelled and heavily class imbalanced data stream, using currently available
algorithms and frameworks. Our results indicate that existing stream learners
exhibit unacceptably low recall on real astronomical data when used in standard
configuration; however, good false positive performance and comparable accuracy
to static learners, suggests they have definite potential as an on-line
solution to this particular big data challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8013</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8013</id><created>2013-07-30</created><authors><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lixin</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author></authors><title>Characterizing Data Analysis Workloads in Data Centers</title><categories>cs.PF</categories><comments>11 pages, 12 figures, IISWC2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the amount of data explodes rapidly, more and more corporations are using
data centers to make effective decisions and gain a competitive edge. Data
analysis applications play a significant role in data centers, and hence it has
became increasingly important to understand their behaviors in order to further
improve the performance of data center computer systems. In this paper, after
investigating three most important application domains in terms of page views
and daily visitors, we choose eleven representative data analysis workloads and
characterize their micro-architectural characteristics by using hardware
performance counters, in order to understand the impacts and implications of
data analysis workloads on the systems equipped with modern superscalar
out-of-order processors. Our study on the workloads reveals that data analysis
applications share many inherent characteristics, which place them in a
different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,
including traditional server workloads (SPECweb2005) and scale-out service
workloads (four among six benchmarks in CloudSuite), and accordingly we give
several recommendations for architecture and system optimizations. On the basis
of our workload characterization work, we released a benchmark suite named
DCBench for typical datacenter workloads, including data analysis and service
workloads, with an open-source license on our project home page on
http://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing
architecture and small-to-medium scale system researches for datacenter
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8029</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8029</id><created>2013-07-30</created><authors><author><keyname>Bouhoula</keyname><forenames>Adel</forenames></author><author><keyname>Ida</keyname><forenames>Tetsuo</forenames></author><author><keyname>Kamareddine</keyname><forenames>Fairouz</forenames></author></authors><title>Proceedings Fourth International Symposium on Symbolic Computation in
  Software Science</title><categories>cs.SC cs.LO</categories><proxy>EPTCS</proxy><acm-class>D.1, D.2, D.3, F.3, F.4,</acm-class><journal-ref>EPTCS 122, 2013</journal-ref><doi>10.4204/EPTCS.122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic computation is the science of computing with symbolic objects
(terms, formulae, programs, algebraic objects, geometrical objects, etc).
Powerful symbolic algorithms have been developed during the past decades and
have played an influential role in theorem proving, automated reasoning,
software verification, model checking, rewriting, formalisation of mathematics,
network security, Groebner bases, characteristic sets, etc.
  The international Symposium on &quot;Symbolic Computation in Software Science&quot; is
the fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the
Research Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,
SCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal
workshops that bring together researchers from: a) SCORE (Symbolic Computation
Research Group) at the University of Tsukuba, Japan, b) Theorema Group at the
Research Institute for Symbolic Computation, Johannes Kepler University Linz,
Austria, c) SSFG (Software Science Foundation Group) at Kyoto University,
Japan, and d) Sup'Com (Higher School of Communication of Tunis) at the
University of Carthage, Tunisia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8037</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8037</id><created>2013-07-30</created><updated>2013-11-12</updated><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author></authors><title>A Rational Convex Program for Linear Arrow-Debreu Markets</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new, flow-type convex program describing equilibrium solutions to
linear Arrow-Debreu markets. Whereas convex formulations were previously known
[Nenakov, Primak 83; Jain 07; Cornet '89], our program exhibits several new
features. It gives a simple necessary and sufficient condition and a concise
proof of the existence and rationality of equilibria, settling an open question
raised by Vazirani. As a consequence we also obtain a simple new proof of
Mertens's result that the equilibrium prices form a convex polyhedral set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8040</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8040</id><created>2013-07-30</created><updated>2013-08-14</updated><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Stabilization of Nonlinear Delay Systems Using Approximate Predictors
  and High-Gain Observers</title><categories>math.OC cs.SY</categories><comments>14 pages, 3 figures, contains the technical proofs of a paper which
  is going to appear in Automatica. arXiv admin note: substantial text overlap
  with arXiv:1108.4499</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a solution to the heretofore open problem of stabilization of
systems with arbitrarily long delays at the input and output of a nonlinear
system using output feedback only. The solution is global, employs the
predictor approach over the period that combines the input and output delays,
addresses nonlinear systems with sampled measurements and with control applied
using a zero-order hold, and requires that the sampling/holding periods be
sufficiently short, though not necessarily constant. Our approach considers a
class of globally Lipschitz strict-feedback systems with disturbances and
employs an appropriately constructed successive approximation of the predictor
map, a high-gain sampled-data observer, and a linear stabilizing feedback for
the delay-free system. The obtained results guarantee robustness to
perturbations of the sampling schedule and different sampling and holding
periods are considered. The approach is specialized to linear systems, where
the predictor is available explicitly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8043</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8043</id><created>2013-07-30</created><authors><author><keyname>Mandal</keyname><forenames>Susmita</forenames></author></authors><title>A Review on Secured Money Transaction with Fingerprint Technique in ATM
  System</title><categories>cs.CY</categories><comments>04 pages, 04 figures, IJCSN Journal, Volume 2, Issue 4, August 2013</comments><report-no>IJCSN-2013-2-4-05</report-no><journal-ref>International Journal of Computer Science and Network , Volume 2,
  Issue 4, August 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present day, the requirement of securing electronic cash flow system
are increasing significantly. Todays life is so busy that spending a valuable
second cost so much. In, such situation if money flow is possible swiftly by
just one swipe it would be a great relief. Biometric based authentication can
be a new approach to satisfy user needs by replacing password-based
authentication. Among all biometric techniques fingerprinting is the oldest and
secured methodology practised till date. In the proposed system user can
transact money by placing his/her thumb imprint on new proposed ATM card. This
new system will smoothen the transaction with security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8049</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8049</id><created>2013-07-30</created><authors><author><keyname>Pan</keyname><forenames>Xinghao</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Optimistic Concurrency Control for Distributed Unsupervised Learning</title><categories>cs.LG cs.AI cs.DC</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on distributed machine learning algorithms has focused primarily on
one of two extremes - algorithms that obey strict concurrency constraints or
algorithms that obey few or no such constraints. We consider an intermediate
alternative in which algorithms optimistically assume that conflicts are
unlikely and if conflicts do arise a conflict-resolution protocol is invoked.
We view this &quot;optimistic concurrency control&quot; paradigm as particularly
appropriate for large-scale machine learning algorithms, particularly in the
unsupervised setting. We demonstrate our approach in three problem areas:
clustering, feature learning and online facility location. We evaluate our
methods via large-scale experiments in a cluster computing environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8057</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8057</id><created>2013-07-30</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Mercer</keyname><forenames>Robert E.</forenames></author></authors><title>Extracting Connected Concepts from Biomedical Texts using Fog Index</title><categories>cs.CL cs.IR</categories><comments>12th Conference of the Pacific Association for Computational
  Linguistics (PACLING 2011), Kuala Lumpur, Malaysia, July 19-21, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish Fog Index (FI) as a text filter to locate the
sentences in texts that contain connected biomedical concepts of interest. To
do so, we have used 24 random papers each containing four pairs of connected
concepts. For each pair, we categorize sentences based on whether they contain
both, any or none of the concepts. We then use FI to measure difficulty of the
sentences of each category and find that sentences containing both of the
concepts have low readability. We rank sentences of a text according to their
FI and select 30 percent of the most difficult sentences. We use an association
matrix to track the most frequent pairs of concepts in them. This matrix
reports that the first filter produces some pairs that hold almost no
connections. To remove these unwanted pairs, we use the Equally Weighted
Harmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a
second filter. Experimental results demonstrate the effectiveness of our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8060</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8060</id><created>2013-07-30</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author></authors><title>Extracting Information-rich Part of Texts using Text Denoising</title><categories>cs.IR cs.CL</categories><comments>26th Canadian Conference on Artificial Intelligence (CAI-2013),
  Regina, Canada, May 29-31, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to report on a novel text reduction technique,
called Text Denoising, that highlights information-rich content when processing
a large volume of text data, especially from the biomedical domain. The core
feature of the technique, the text readability index, embodies the hypothesis
that complex text is more information-rich than the rest. When applied on tasks
like biomedical relation bearing text extraction, keyphrase indexing and
extracting sentences describing protein interactions, it is evident that the
reduced set of text produced by text denoising is more information-rich than
the rest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8067</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8067</id><created>2013-07-30</created><authors><author><keyname>Kelly</keyname><forenames>Mat</forenames></author><author><keyname>Brunelle</keyname><forenames>Justin F.</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>On the Change in Archivability of Websites Over Time</title><categories>cs.DL</categories><comments>12 pages, 8 figures, Theory and Practice of Digital Libraries (TPDL)
  2013, Valletta, Malta</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As web technologies evolve, web archivists work to keep up so that our
digital history is preserved. Recent advances in web technologies have
introduced client-side executed scripts that load data without a referential
identifier or that require user interaction (e.g., content loading when the
page has scrolled). These advances have made automating methods for capturing
web pages more difficult. Because of the evolving schemes of publishing web
pages along with the progressive capability of web preservation tools, the
archivability of pages on the web has varied over time. In this paper we show
that the archivability of a web page can be deduced from the type of page being
archived, which aligns with that page's accessibility in respect to dynamic
content. We show concrete examples of when these technologies were introduced
by referencing mementos of pages that have persisted through a long evolution
of available technologies. Identifying these reasons for the inability of these
web pages to be archived in the past in respect to accessibility serves as a
guide for ensuring that content that has longevity is published using good
practice methods that make it available for preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8083</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8083</id><created>2013-07-30</created><authors><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Kozat</keyname><forenames>Ulas C.</forenames></author></authors><title>TOFEC: Achieving Optimal Throughput-Delay Trade-off of Cloud Storage
  Using Erasure Codes</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our paper presents solutions using erasure coding, parallel connections to
storage cloud and limited chunking (i.e., dividing the object into a few
smaller segments) together to significantly improve the delay performance of
uploading and downloading data in and out of cloud storage.
  TOFEC is a strategy that helps front-end proxy adapt to level of workload by
treating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring
admission control. Under light workloads, TOFEC creates more smaller chunks and
uses more parallel connections per file, minimizing service delay. Under heavy
workloads, TOFEC automatically reduces the level of chunking (fewer chunks with
increased size) and uses fewer parallel connections to reduce overhead,
resulting in higher throughput and preventing queueing delay. Our trace-driven
simulation results show that TOFEC's adaptation mechanism converges to an
appropriate code that provides the optimal delay-throughput trade-off without
reducing system capacity. Compared to a non-adaptive strategy optimized for
throughput, TOFEC delivers 2.5x lower latency under light workloads; compared
to a non-adaptive strategy optimized for latency, TOFEC can scale to support
over 3x as many requests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8084</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8084</id><created>2013-07-29</created><authors><author><keyname>Zhang</keyname><forenames>Shiqi</forenames></author><author><keyname>Sridharan</keyname><forenames>Mohan</forenames></author></authors><title>Combining Answer Set Programming and POMDPs for Knowledge Representation
  and Reasoning on Mobile Robots</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For widespread deployment in domains characterized by partial observability,
non-deterministic actions and unforeseen changes, robots need to adapt sensing,
processing and interaction with humans to the tasks at hand. While robots
typically cannot process all sensor inputs or operate without substantial
domain knowledge, it is a challenge to provide accurate domain knowledge and
humans may not have the time and expertise to provide elaborate and accurate
feedback. The architecture described in this paper combines declarative
programming and probabilistic reasoning to address these challenges, enabling
robots to: (a) represent and reason with incomplete domain knowledge, resolving
ambiguities and revising existing knowledge using sensor inputs and minimal
human feedback; and (b) probabilistically model the uncertainty in sensor input
processing and navigation. Specifically, Answer Set Programming (ASP), a
declarative programming paradigm, is combined with hierarchical partially
observable Markov decision processes (POMDPs), using domain knowledge to revise
probabilistic beliefs, and using positive and negative observations for early
termination of tasks that can no longer be pursued. All algorithms are
evaluated in simulation and on mobile robots locating target objects in indoor
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8104</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8104</id><created>2013-07-30</created><authors><author><keyname>Stowe</keyname><forenames>Matt</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Neural Network Capacity for Multilevel Inputs</title><categories>cs.NE</categories><comments>24 pages,17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8112</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8112</id><created>2013-07-29</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author></authors><title>Visualization and Analysis of Geographical Crime Patterns Using Formal
  Concept Analysis</title><categories>cs.CY</categories><comments>6 pages. International Journal of Remote Sensing &amp; Geoscience
  (IJRSG), 2013. arXiv admin note: substantial text overlap with
  arXiv:1307.7788</comments><journal-ref>International Journal of Science, Engineering and Technology
  Research, 2.1 (2013): pp: 220-225. Web. 15 Jan. 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There are challenges faced in today's world in terms of crime analysis when
it comes to graphical visualization of crime patterns. Geographical
representation of crime scenes and crime types become very important in
gathering intelligence about crimes. This provides a very dynamic and easy way
of monitoring criminal activities and analyzing them as well as producing
effective countermeasures and preventive measures in solving them. But we need
effective computer tools and intelligent systems that are automated to analyze
and interpret criminal data in real time effectively and efficiently. These
current computer systems should have the capability of providing intelligence
from raw data and creating a visual graph which will make it easy for new
concepts to be built and generated from crime data in order to solve,
understand and analyze crime patterns easily. This paper proposes a new method
of visualizing and analyzing crime patterns based on geographical crime data by
using Formal Concept Analysis, or Galois Lattices, a data analysis technique
grounded on Lattice Theory and Propositional Calculus. This method considered
the set of common and distinct attributes of crimes in such a way that
categorization are done based on related crime types. This will help in
building a more defined and conceptual systems for analysis of geographical
crime data that can easily be visualized and intelligently analyzed by computer
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8136</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8136</id><created>2013-07-30</created><authors><author><keyname>Kent</keyname><forenames>Brian P.</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Verstynen</keyname><forenames>Timothy</forenames></author></authors><title>DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering</title><categories>stat.ME cs.LG stat.ML</categories><comments>28 pages, 9 figures, for associated software see
  https://github.com/CoAxLab/DeBaCl</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The level set tree approach of Hartigan (1975) provides a probabilistically
based and highly interpretable encoding of the clustering behavior of a
dataset. By representing the hierarchy of data modes as a dendrogram of the
level sets of a density estimator, this approach offers many advantages for
exploratory analysis and clustering, especially for complex and
high-dimensional data. Several R packages exist for level set tree estimation,
but their practical usefulness is limited by computational inefficiency,
absence of interactive graphical capabilities and, from a theoretical
perspective, reliance on asymptotic approximations. To make it easier for
practitioners to capture the advantages of level set trees, we have written the
Python package DeBaCl for DEnsity-BAsed CLustering. In this article we
illustrate how DeBaCl's level set tree estimates can be used for difficult
clustering tasks and interactive graphical data analysis. The package is
intended to promote the practical use of level set trees through improvements
in computational efficiency and a high degree of user customization. In
addition, the flexible algorithms implemented in DeBaCl enjoy finite sample
accuracy, as demonstrated in recent literature on density clustering. Finally,
we show the level set tree framework can be easily extended to deal with
functional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8139</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8139</id><created>2013-07-30</created><authors><author><keyname>Ezra</keyname><forenames>Esther</forenames></author></authors><title>A Size-Sensitive Discrepancy Bound for Set Systems of Bounded Primal
  Shatter Dimension</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X,\S)$ be a set system on an $n$-point set $X$. The \emph{discrepancy}
of $\S$ is defined as the minimum of the largest deviation from an even split,
over all subsets of $S \in \S$ and two-colorings $\chi$ on $X$. We consider the
scenario where, for any subset $X' \subseteq X$ of size $m \le n$ and for any
parameter $1 \le k \le m$, the number of restrictions of the sets of $\S$ to
$X'$ of size at most $k$ is only $O(m^{d_1} k^{d-d_1})$, for fixed integers $d
&gt; 0$ and $1 \le d_1 \le d$ (this generalizes the standard notion of
\emph{bounded primal shatter dimension} when $d_1 = d$). In this case we show
that there exists a coloring $\chi$ with discrepancy bound $O^{*}(|S|^{1/2 -
d_1/(2d)} n^{(d_1 - 1)/(2d)})$, for each $S \in \S$, where $O^{*}(\cdot)$ hides
a polylogarithmic factor in $n$. This bound is tight up to a polylogarithmic
factor \cite{Mat-95, Mat-99} and the corresponding coloring $\chi$ can be
computed in expected polynomial time using the very recent machinery of Lovett
and Meka for constructive discrepancy minimization \cite{LM-12}. Our bound
improves and generalizes the bounds obtained from the machinery of Har-Peled
and Sharir \cite{HS-11} (and the follow-up work in \cite{SZ-12}) for points and
halfspaces in $d$-space for $d \ge 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8167</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8167</id><created>2013-07-30</created><authors><author><keyname>Alam</keyname><forenames>Jahrul</forenames></author><author><keyname>Walsh</keyname><forenames>Raymond</forenames></author><author><keyname>Hossain</keyname><forenames>Alamgir</forenames></author><author><keyname>Rose</keyname><forenames>Andrew</forenames></author></authors><title>Discretization of the Poisson equation using the interpolating scaling
  function with applications</title><categories>physics.flu-dyn cs.NA math.NA</categories><comments>29 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dyadic translations of the interpolating scaling function generate a basis
that can be used to approximate functions and develop a multiresolution
methodology for constructing smooth surfaces or curves. Many wavelet methods
for solving par- tial differential equations are also derived from the
interpolating scaling function. However, little is done for developing a higher
order numerical discretization methodology using the scaling function. In this
article, we have employed an iterative interpolation scheme for the
construction of scaling functions in a two- dimensional mesh that is a finite
collection of rectangles. We have studied the development of a weighted
residual collocation method for approximating partial derivatives. We show that
the discretization error is controlled by the order of the scaling function.
The potential of this novel technique has been verified with some
representative examples of the Poisson equation. We have extended the technique
for solving nonlinear advection-diffusion equations, and simulated a shear
driven flow in a square cavity at CFL = 2.5 (Courant Friedrichs Lewy) and Re =
1 000 (Reynolds number). Agreement with the reference solution at a large CFL =
2.5 explores the potential of this development for advection dominated
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8172</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8172</id><created>2013-07-30</created><authors><author><keyname>Rekabi</keyname><forenames>Firas Al</forenames></author><author><keyname>Sheikh</keyname><forenames>Asim El</forenames></author></authors><title>Improved Median Polish Kriging for Simulation Metamodeling</title><categories>cs.OH</categories><journal-ref>International Journal of Information and Communication Technology
  Research, Volume 3 No. 5, pp. 159-165, May 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In simulation, Median Polish Kriging is a technique used to predict
unobserved data points in two-dimensional space. The linear behavior of the
traditional Median Polish Kriging in the estimation of the mean function in a
high grid makes the interpolation of O(1) which has a low order in the
prediction and that leads to a high prediction error. Therefore, an improvement
in the estimation of the mean function has been introduced using Biharmonic
spline interpolation and the new technique has been called Improved Median
Polish Kriging (IMPK). The IMPK has been applied to the standard coal-ash data
in two-dimension. The novel method gave much better results according to the
cross validation results that were obtained when compared with the traditional
Median Polish Kriging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8179</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8179</id><created>2013-07-30</created><authors><author><keyname>Kester</keyname><forenames>Quist-Aphetsi</forenames></author><author><keyname>Kayode</keyname><forenames>Ajibade Ibrahim</forenames></author></authors><title>Using SOA with Web Services for effective data integration of Enterprise
  Pharmaceutical Information Systems</title><categories>cs.SE</categories><comments>11 pages paper. Submitted paper. arXiv admin note: text overlap with
  arXiv:1204.0179, arXiv:1307.7790 by other authors</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering(IJARCSSE)Vol 3 Issue 6 pp1-8. 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Medicines exist for combating illnesses and drug stores provide an outlet for
consumers to gain access to these drugs. However, due to various factors such
as cost of production, accessibility and legal issues as well as distribution
factors, availability of these drugs at any location cannot be guaranteed at
all times. There is need for individuals and organizations such as hospitals to
be able to locate required drugs within given geographical vicinity. This is of
immense importance especially during emergencies, travel, and in cases where
the drugs are uncommon. This research work is aimed at solving this problem by
designing a system that integrates all drugstores. The integration of the drug
stores will be based on SOA concepts with web services via a central service
bus. The database systems of the drug stores will be integrated via a service
bus such that drugs can easily be searched for and the results will be
displayed based on its availability. Drugs can easily be searched for within
geographically distributed pharmaceutical databases as well as consumption of
drugs with relation to geographical locations can easily be monitored and
tracked. This will make it easy for health institutions to research on drug
consumption patterns across geographical areas and also control their usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8182</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8182</id><created>2013-07-30</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author><author><keyname>Buffet</keyname><forenames>Olivier</forenames><affiliation>INRIA</affiliation></author><author><keyname>Hoffmann</keyname><forenames>Joerg</forenames><affiliation>Saarland University</affiliation></author></authors><title>POMDPs Make Better Hackers: Accounting for Uncertainty in Penetration
  Testing</title><categories>cs.AI cs.CR</categories><comments>Twenty-Sixth Conference on Artificial Intelligence (AAAI-12),
  Toronto, Canada</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Penetration Testing is a methodology for assessing network security, by
generating and executing possible hacking attacks. Doing so automatically
allows for regular and systematic testing. A key question is how to generate
the attacks. This is naturally formulated as planning under uncertainty, i.e.,
under incomplete knowledge about the network configuration. Previous work uses
classical planning, and requires costly pre-processes reducing this uncertainty
by extensive application of scanning methods. By contrast, we herein model the
attack planning problem in terms of partially observable Markov decision
processes (POMDP). This allows to reason about the knowledge available, and to
intelligently employ scanning actions as part of the attack. As one would
expect, this accurate solution does not scale. We devise a method that relies
on POMDPs to find good attacks on individual machines, which are then composed
into an attack on the network as a whole. This decomposition exploits network
structure to the extent possible, making targeted approximations (only) where
needed. Evaluating this method on a suitably adapted industrial test suite, we
demonstrate its effectiveness in both runtime and solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8186</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8186</id><created>2013-07-30</created><authors><author><keyname>Calvo</keyname><forenames>Aureliano</forenames><affiliation>Core Security Technologies</affiliation></author><author><keyname>Futoransky</keyname><forenames>Ariel</forenames><affiliation>Core Security Technologies</affiliation></author><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author></authors><title>An Oblivious Password Cracking Server</title><categories>cs.CR</categories><comments>WSegI at 41st JAIIO, La Plata, Argentina</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Building a password cracking server that preserves the privacy of the queries
made to the server is a problem that has not yet been solved. Such a server
could acquire practical relevance in the future: for instance, the tables used
to crack the passwords could be calculated, stored and hosted in
cloud-computing services, and could be queried from devices with limited
computing power.
  In this paper we present a method to preserve the confidentiality of a
password cracker---wherein the tables used to crack the passwords are stored by
a third party---by combining Hellman tables and Private Information Retrieval
(PIR) protocols. We provide the technical details of this method, analyze its
complexity, and show the experimental results obtained with our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8187</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8187</id><created>2013-07-30</created><updated>2013-10-06</updated><authors><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Towards Minimax Online Learning with Unknown Time Horizon</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online learning when the time horizon is unknown. We apply a
minimax analysis, beginning with the fixed horizon case, and then moving on to
two unknown-horizon settings, one that assumes the horizon is chosen randomly
according to some known distribution, and the other which allows the adversary
full control over the horizon. For the random horizon setting with restricted
losses, we derive a fully optimal minimax algorithm. And for the adversarial
horizon setting, we prove a nontrivial lower bound which shows that the
adversary obtains strictly more power than when the horizon is fixed and known.
Based on the minimax solution of the random horizon setting, we then propose a
new adaptive algorithm which &quot;pretends&quot; that the horizon is drawn from a
distribution from a special family, but no matter how the actual horizon is
chosen, the worst-case regret is of the optimal rate. Furthermore, our
algorithm can be combined and applied in many ways, for instance, to online
convex optimization, follow the perturbed leader, exponential weights algorithm
and first order bounds. Experiments show that our algorithm outperforms many
other existing algorithms in an online linear optimization setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8191</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8191</id><created>2013-07-30</created><authors><author><keyname>Syaprina</keyname></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Sopiah</keyname><forenames>Nyimas</forenames></author></authors><title>Sistem Informasi Penjualan Dan Perbaikan Komputer (Studi Kasus: CV
  Computer Plus Palembang)</title><categories>cs.OH</categories><comments>Syaprina; Abdillah, L.A.; Sopiah, N., &quot;Sistem Informasi penjualan dan
  perbaikan komputer,&quot; Jurnal Ilmiah MATRIK, vol. 10, pp. 113-124, 2008</comments><journal-ref>MATRIK. 10 (2008) 113-124</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this research is to develop an Information System of Selling
and Services using Microsoft Visual Basic and Microsoft Access for it database.
The benefits of this research is to help CV Computer Plus in selling and
services data processing everyday. To develop this IS is used 5 (five) steps:
1) Planning, 2) Analysis, 3) Design, 4) Implementation, and 5) Evaluation. The
Information System can record the selling and services data, it also prepared
usefull reports. By using this IS, CV Computer Plus can operate their selling
and services efficiency and effectively. In the future it can be upgraded for
network application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8192</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8192</id><created>2013-07-30</created><updated>2014-01-15</updated><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>Okamoto</keyname><forenames>Takuma</forenames></author><author><keyname>Tatsu</keyname><forenames>Yuichi</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author><author><keyname>Yamato</keyname><forenames>Masahide</forenames></author></authors><title>Morpion Solitaire 5D: a new upper bound of 121 on the maximum score</title><categories>cs.DM</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morpion Solitaire is a pencil-and-paper game for a single player. A move in
this game consists of putting a cross at a lattice point and then drawing a
line segment that passes through exactly five consecutive crosses. The
objective is to make as many moves as possible, starting from a standard
initial configuration of crosses. For one of the variants of this game, called
5D, we prove an upper bound of 121 on the number of moves. This is done by
introducing line-based analysis, and improves the known upper bound of 138
obtained by potential-based analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8198</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8198</id><created>2013-07-30</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Sensing as a Service Model for Smart Cities Supported by Internet of
  Things</title><categories>cs.CY</categories><comments>Transactions on Emerging Telecommunications Technologies 2014
  (Accepted for Publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The world population is growing at a rapid pace. Towns and cities are
accommodating half of the world's population thereby creating tremendous
pressure on every aspect of urban living. Cities are known to have large
concentration of resources and facilities. Such environments attract people
from rural areas. However, unprecedented attraction has now become an
overwhelming issue for city governance and politics. The enormous pressure
towards efficient city management has triggered various Smart City initiatives
by both government and private sector businesses to invest in ICT to find
sustainable solutions to the growing issues. The Internet of Things (IoT) has
also gained significant attention over the past decade. IoT envisions to
connect billions of sensors to the Internet and expects to use them for
efficient and effective resource management in Smart Cities. Today
infrastructure, platforms, and software applications are offered as services
using cloud technologies. In this paper, we explore the concept of sensing as a
service and how it fits with the Internet of Things. Our objective is to
investigate the concept of sensing as a service model in technological,
economical, and social perspectives and identify the major open challenges and
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8199</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8199</id><created>2013-07-30</created><updated>2013-11-29</updated><authors><author><keyname>Nam</keyname><forenames>Sung Sik</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Technical Report: An MGF-based Unified Framework to Determine the Joint
  Statistics of Partial Sums of Ordered i.n.d. Random Variables</title><categories>cs.IT cs.PF math.IT</categories><doi>10.1109/TSP.2014.2326624</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint statistics of partial sums of ordered random variables (RVs) are
often needed for the accurate performance characterization of a wide variety of
wireless communication systems. A unified analytical framework to determine the
joint statistics of partial sums of ordered independent and identically
distributed (i.i.d.) random variables was recently presented. However, the
identical distribution assumption may not be valid in several real-world
applications. With this motivation in mind, we consider in this paper the more
general case in which the random variables are independent but not necessarily
identically distributed (i.n.d.). More specifically, we extend the previous
analysis and introduce a new more general unified analytical framework to
determine the joint statistics of partial sums of ordered i.n.d. RVs. Our
mathematical formalism is illustrated with an application on the exact
performance analysis of the capture probability of generalized selection
combining (GSC)-based RAKE receivers operating over frequency-selective fading
channels with a non-uniform power delay profile. We also discussed a couple of
other sample applications of the generic results presented in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8201</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8201</id><created>2013-07-30</created><authors><author><keyname>Pernas</keyname><forenames>Jaume</forenames></author><author><keyname>Gastony</keyname><forenames>Bernat</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Pujol</keyname><forenames>Jaume</forenames></author></authors><title>Non-homogeneous Two-Rack Model for Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>ISIT 2013. arXiv admin note: text overlap with arXiv:1004.0785 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the traditional two-rack distributed storage system (DSS) model, due to
the assumption that the storage capacity of each node is the same, the minimum
bandwidth regenerating (MBR) point becomes infeasible. In this paper, we design
a new non-homogeneous two-rack model by proposing a generalization of the
threshold function used to compute the tradeoff curve. We prove that by having
the nodes in the rack with higher regenerating bandwidth stores more
information, all the points on the tradeoff curve, including the MBR point,
become feasible. Finally, we show how the non-homogeneous two-rack model
outperforms the traditional model in the tradeoff curve between the storage per
node and the repair bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8202</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8202</id><created>2013-07-30</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames><affiliation>Imperial College London, London, England</affiliation></author><author><keyname>Barbanera</keyname><forenames>Franco</forenames><affiliation>Universita` di Catania, Catania, Italy</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>Universita` di Torino, Torino, Italy</affiliation></author></authors><title>Characterisation of Strongly Normalising lambda-mu-Terms</title><categories>cs.LO</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 121, 2013, pp. 1-17</journal-ref><doi>10.4204/EPTCS.121.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a characterisation of strongly normalising terms of the
lambda-mu-calculus by means of a type system that uses intersection and product
types. The presence of the latter and a restricted use of the type omega enable
us to represent the particular notion of continuation used in the literature
for the definition of semantics for the lambda-mu-calculus. This makes it
possible to lift the well-known characterisation property for
strongly-normalising lambda-terms - that uses intersection types - to the
lambda-mu-calculus. From this result an alternative proof of strong
normalisation for terms typeable in Parigot's propositional logical system
follows, by means of an interpretation of that system into ours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8203</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8203</id><created>2013-07-30</created><authors><author><keyname>D&#xfc;dder</keyname><forenames>Boris</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Garbe</keyname><forenames>Oliver</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Martens</keyname><forenames>Moritz</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Rehof</keyname><forenames>Jakob</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Urzyczyn</keyname><forenames>Pawe&#x142;</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Using Inhabitation in Bounded Combinatory Logic with Intersection Types
  for Composition Synthesis</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 121, 2013, pp. 18-34</journal-ref><doi>10.4204/EPTCS.121.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe ongoing work on a framework for automatic composition synthesis
from a repository of software components. This work is based on combinatory
logic with intersection types. The idea is that components are modeled as typed
combinators, and an algorithm for inhabitation {\textemdash} is there a
combinatory term e with type tau relative to an environment Gamma?
{\textemdash} can be used to synthesize compositions. Here, Gamma represents
the repository in the form of typed combinators, tau specifies the synthesis
goal, and e is the synthesized program. We illustrate our approach by examples,
including an application to synthesis from GUI-components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8204</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8204</id><created>2013-07-30</created><authors><author><keyname>Dunfield</keyname><forenames>Joshua</forenames></author></authors><title>Annotations for Intersection Typechecking</title><categories>cs.PL</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 121, 2013, pp. 35-47</journal-ref><doi>10.4204/EPTCS.121.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In functional programming languages, the classic form of annotation is a
single type constraint on a term. Intersection types add complications: a
single term may have to be checked several times against different types, in
different contexts, requiring annotation with several types. Moreover, it is
useful (in some systems, necessary) to indicate the context in which each such
type is to be used.
  This paper explores the technical design space of annotations in systems with
intersection types. Earlier work (Dunfield and Pfenning 2004) introduced
contextual typing annotations, which we now tease apart into more elementary
mechanisms: a &quot;right hand&quot; annotation (the standard form), a &quot;left hand&quot;
annotation (the context in which a right-hand annotation is to be used), a
merge that allows for multiple annotations, and an existential binder for index
variables. The most novel element is the left-hand annotation, which guards
terms (and right-hand annotations) with a judgment that must follow from the
current context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8205</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8205</id><created>2013-07-30</created><authors><author><keyname>De Benedetti</keyname><forenames>Erika</forenames><affiliation>Universit&#xe0; degli Studi di Torino</affiliation></author><author><keyname>Della Rocca</keyname><forenames>Simona Ronchi</forenames><affiliation>Universit&#xe0; degli Studi di Torino</affiliation></author></authors><title>Bounding normalization time through intersection types</title><categories>cs.LO</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 121, 2013, pp. 48-57</journal-ref><doi>10.4204/EPTCS.121.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-idempotent intersection types are used in order to give a bound of the
length of the normalization beta-reduction sequence of a lambda term: namely,
the bound is expressed as a function of the size of the term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8206</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8206</id><created>2013-07-30</created><authors><author><keyname>Coppo</keyname><forenames>Mario</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Margaria</keyname><forenames>Ines</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Zacchi</keyname><forenames>Maddalena</forenames><affiliation>Universita' di Torino</affiliation></author></authors><title>Toward Isomorphism of Intersection and Union types</title><categories>cs.LO</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><acm-class>F.4.1, Lambda calculus and related systems</acm-class><journal-ref>EPTCS 121, 2013, pp. 58-80</journal-ref><doi>10.1017/S0960129515000304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates type isomorphism in a lambda-calculus with
intersection and union types. It is known that in lambda-calculus, the
isomorphism between two types is realised by a pair of terms inverse one each
other. Notably, invertible terms are linear terms of a particular shape, called
finite hereditary permutators. Typing properties of finite hereditary
permutators are then studied in a relevant type inference system with
intersection and union types for linear terms. In particular, an isomorphism
preserving reduction between types is defined. Type reduction is confluent and
terminating, and induces a notion of normal form of types. The properties of
normal types are a crucial step toward the complete characterisation of type
isomorphism. The main results of this paper are, on one hand, the fact that two
types with the same normal form are isomorphic, on the other hand, the
characterisation of the isomorphism between types in normal form, modulo
isomorphism of arrow types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8207</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8207</id><created>2013-07-30</created><authors><author><keyname>Ancona</keyname><forenames>Davide</forenames></author><author><keyname>Giannini</keyname><forenames>Paola</forenames></author><author><keyname>Zucca</keyname><forenames>Elena</forenames></author></authors><title>Reconciling positional and nominal binding</title><categories>cs.PL cs.LO cs.SE</categories><comments>In Proceedings ITRS 2012, arXiv:1307.7849</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 121, 2013, pp. 81-93</journal-ref><doi>10.4204/EPTCS.121.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an extension of the simply-typed lambda calculus where two
different binding mechanisms, by position and by name, nicely coexist. In the
former, as in standard lambda calculus, the matching between parameter and
argument is done on a positional basis, hence alpha-equivalence holds, whereas
in the latter it is done on a nominal basis. The two mechanisms also
respectively correspond to static binding, where the existence and type
compatibility of the argument are checked at compile-time, and dynamic binding,
where they are checked at run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8208</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8208</id><created>2013-07-30</created><authors><author><keyname>Elleuch</keyname><forenames>Maissa</forenames><affiliation>Sfax University, Sfax, Tunisia</affiliation></author><author><keyname>Hasan</keyname><forenames>Osman</forenames><affiliation>Concordia University, Montreal, Canada</affiliation></author><author><keyname>Tahar</keyname><forenames>Sofi&#xe8;ne</forenames><affiliation>Concordia University, Montreal, Canada</affiliation></author><author><keyname>Abid</keyname><forenames>Mohamed</forenames><affiliation>Sfax University, Sfax, Tunisia</affiliation></author></authors><title>Formal Probabilistic Analysis of a Wireless Sensor Network for Forest
  Fire Detection</title><categories>cs.LO cs.NI</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 1-9</journal-ref><doi>10.4204/EPTCS.122.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) have been widely explored for forest fire
detection, which is considered a fatal threat throughout the world. Energy
conservation of sensor nodes is one of the biggest challenges in this context
and random scheduling is frequently applied to overcome that. The performance
analysis of these random scheduling approaches is traditionally done by
paper-and-pencil proof methods or simulation. These traditional techniques
cannot ascertain 100% accuracy, and thus are not suitable for analyzing a
safety-critical application like forest fire detection using WSNs. In this
paper, we propose to overcome this limitation by applying formal probabilistic
analysis using theorem proving to verify scheduling performance of a real-world
WSN for forest fire detection using a k-set randomized algorithm as an energy
saving mechanism. In particular, we formally verify the expected values of
coverage intensity, the upper bound on the total number of disjoint subsets,
for a given coverage intensity, and the lower bound on the total number of
nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8209</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8209</id><created>2013-07-30</created><authors><author><keyname>Shil</keyname><forenames>Assia Ben</forenames><affiliation>LIP2 Laboratory, Faculty of Sciences of Tunis, Tunisia</affiliation></author><author><keyname>Blibech</keyname><forenames>Kaouther</forenames><affiliation>LIP2 Laboratory, Faculty of Sciences of Tunis, Tunisia</affiliation></author><author><keyname>Robbana</keyname><forenames>Riadh</forenames><affiliation>LIP2 Laboratory, Faculty of Sciences of Tunis, Tunisia</affiliation></author><author><keyname>Neji</keyname><forenames>Wafa</forenames><affiliation>LIP2 Laboratory, Faculty of Sciences of Tunis, Tunisia</affiliation></author></authors><title>A New PVSS Scheme with a Simple Encryption Function</title><categories>cs.CR</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029. This PVSS scheme was
  proposed to be used to provide a distributed Timestamping scheme</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 11-22</journal-ref><doi>10.4204/EPTCS.122.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Publicly Verifiable Secret Sharing (PVSS) scheme allows anyone to verify
the validity of the shares computed and distributed by a dealer. The idea of
PVSS was introduced by Stadler in [18] where he presented a PVSS scheme based
on Discrete Logarithm. Later, several PVSS schemes were proposed. In [2],
Behnad and Eghlidos present an interesting PVSS scheme with explicit membership
and disputation processes. In this paper, we present a new PVSS having the
advantage of being simpler while offering the same features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8210</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8210</id><created>2013-07-30</created><authors><author><keyname>Ghabri</keyname><forenames>Hatem</forenames><affiliation>Sup Com, Tunis</affiliation></author><author><keyname>Maatoug</keyname><forenames>Ghazi</forenames><affiliation>Sup Com, Tunis</affiliation></author><author><keyname>Rusinowitch</keyname><forenames>Michael</forenames><affiliation>INRIA Nancy Grand Est</affiliation></author></authors><title>Compiling symbolic attacks to protocol implementation tests</title><categories>cs.CR cs.SE</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 39-49</journal-ref><doi>10.4204/EPTCS.122.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently efficient model-checking tools have been developed to find flaws in
security protocols specifications. These flaws can be interpreted as potential
attacks scenarios but the feasability of these scenarios need to be confirmed
at the implementation level. However, bridging the gap between an abstract
attack scenario derived from a specification and a penetration test on real
implementations of a protocol is still an open issue. This work investigates an
architecture for automatically generating abstract attacks and converting them
to concrete tests on protocol implementations. In particular we aim to improve
previously proposed blackbox testing methods in order to discover automatically
new attacks and vulnerabilities. As a proof of concept we have experimented our
proposed architecture to detect a renegotiation vulnerability on some
implementations of SSL/TLS, a protocol widely used for securing electronic
transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8211</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8211</id><created>2013-07-30</created><authors><author><keyname>Chaabani</keyname><forenames>Mohamed</forenames><affiliation>LIMOSE, University of Boumerdes, Boumerdes, Algeria</affiliation></author><author><keyname>Mezghiche</keyname><forenames>Mohamed</forenames><affiliation>LIMOSE, University of Boumerdes, Boumerdes, Algeria</affiliation></author><author><keyname>Strecker</keyname><forenames>Martin</forenames><affiliation>IRIT</affiliation></author></authors><title>Formal verification of a proof procedure for the description logic ALC</title><categories>cs.LO</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1</acm-class><journal-ref>EPTCS 122, 2013, pp. 51-61</journal-ref><doi>10.4204/EPTCS.122.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description Logics (DLs) are a family of languages used for the
representation and reasoning on the knowledge of an application domain, in a
structured and formal manner. In order to achieve this objective, several
provers, such as RACER and FaCT++, have been implemented, but these provers
themselves have not been yet certified. In order to ensure the soundness of
derivations in these DLs, it is necessary to formally verify the deductions
applied by these reasoners. Formal methods offer powerful tools for the
specification and verification of proof procedures, among them there are
methods for proving properties such as soundness, completeness and termination
of a proof procedure. In this paper, we present the definition of a proof
procedure for the Description Logic ALC, based on a semantic tableau method. We
ensure validity of our prover by proving its soundness, completeness and
termination properties using Isabelle proof assistant. The proof proceeds in
two phases, first by establishing these properties on an abstract level, and
then by instantiating them for an implementation based on lists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8212</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8212</id><created>2013-07-30</created><authors><author><keyname>Lounas</keyname><forenames>Razika</forenames><affiliation>University of Boumerdes, Algeria</affiliation></author><author><keyname>Mezghiche</keyname><forenames>Mohamed</forenames><affiliation>University of Boumerdes, Algeria</affiliation></author><author><keyname>Lanet</keyname><forenames>Jean-Louis</forenames><affiliation>Universityof Limoges, France</affiliation></author></authors><title>Towards a General Framework for Formal Reasoning about Java Bytecode
  Transformation</title><categories>cs.LO cs.PL</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 63-73</journal-ref><doi>10.4204/EPTCS.122.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program transformation has gained a wide interest since it is used for
several purposes: altering semantics of a program, adding features to a program
or performing optimizations. In this paper we focus on program transformations
at the bytecode level. Because these transformations may introduce errors, our
goal is to provide a formal way to verify the update and establish its
correctness. The formal framework presented includes a definition of a formal
semantics of updates which is the base of a static verification and a scheme
based on Hoare triples and weakest precondition calculus to reason about
behavioral aspects in bytecode transformation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8213</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8213</id><created>2013-07-30</created><authors><author><keyname>Abbasi</keyname><forenames>Naeem</forenames><affiliation>ECE Department, Concordia University, Montreal, Canada</affiliation></author><author><keyname>Hasan</keyname><forenames>Osman</forenames><affiliation>ECE Department, Concordia University, Montreal, Canada</affiliation></author><author><keyname>Tahar</keyname><forenames>Sofi&#xe8;ne</forenames><affiliation>ECE Department, Concordia University, Montreal, Canada</affiliation></author></authors><title>Formal Analysis of Soft Errors using Theorem Proving</title><categories>cs.LO</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029, Hardware -&gt; Dynamic
  memory; Transient errors and upsets; Safety critical systems; Security and
  privacy -&gt; Logic and verification; Theory of computation -&gt; Automated
  reasoning; Computer systems organization -&gt; Reliability</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 75-84</journal-ref><doi>10.4204/EPTCS.122.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling and analysis of soft errors in electronic circuits has traditionally
been done using computer simulations. Computer simulations cannot guarantee
correctness of analysis because they utilize approximate real number
representations and pseudo random numbers in the analysis and thus are not well
suited for analyzing safety-critical applications. In this paper, we present a
higher-order logic theorem proving based method for modeling and analysis of
soft errors in electronic circuits. Our developed infrastructure includes
formalized continuous random variable pairs, their Cumulative Distribution
Function (CDF) properties and independent standard uniform and Gaussian random
variables. We illustrate the usefulness of our approach by modeling and
analyzing soft errors in commonly used dynamic random access memory sense
amplifier circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8214</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8214</id><created>2013-07-30</created><authors><author><keyname>Henaien</keyname><forenames>Amira</forenames><affiliation>LITA, Universit&#xe9; de Lorraine, Ile du Saulcy, Metz, France and Higher School of Communication of Tunis</affiliation></author><author><keyname>Stratulat</keyname><forenames>Sorin</forenames><affiliation>LITA, Universit&#xe9; de Lorraine, Ile du Saulcy, France</affiliation></author></authors><title>Performing Implicit Induction Reasoning with Certifying Proof
  Environments</title><categories>cs.LO</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><acm-class>F.4.1; I.2.3; I.2.4</acm-class><journal-ref>EPTCS 122, 2013, pp. 97-108</journal-ref><doi>10.4204/EPTCS.122.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Largely adopted by proof assistants, the conventional induction methods based
on explicit induction schemas are non-reductive and local, at schema level. On
the other hand, the implicit induction methods used by automated theorem
provers allow for lazy and mutual induction reasoning. In this paper, we
present a new tactic for the Coq proof assistant able to perform automatically
implicit induction reasoning. By using an automatic black-box approach,
conjectures intended to be manually proved by the certifying proof environment
that integrates Coq are proved instead by the Spike implicit induction theorem
prover. The resulting proofs are translated afterwards into certified Coq
scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8225</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8225</id><created>2013-07-31</created><authors><author><keyname>Kapri</keyname><forenames>Deepti</forenames></author><author><keyname>Madaan</keyname><forenames>Rosy</forenames></author><author><keyname>Sharma</keyname><forenames>A. K</forenames></author><author><keyname>Dixit</keyname><forenames>Ashutosh</forenames></author></authors><title>A Novel Architecture for Relevant Blog Page Identifcation</title><categories>cs.IR cs.CL</categories><comments>13 Pages. International Journal of Computer Engineering and
  Applications, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blogs are undoubtedly the richest source of information available in
cyberspace. Blogs can be of various natures i.e. personal blogs which contain
posts on mixed issues or blogs can be domain specific which contains posts on
particular topics, this is the reason, they offer wide variety of relevant
information which is often focused. A general search engine gives back a huge
collection of web pages which may or may not give correct answers, as web is
the repository of information of all kinds and a user has to go through various
documents before he gets what he was originally looking for, which is a very
time consuming process. So, the search can be made more focused and accurate if
it is limited to blogosphere instead of web pages. The reason being that the
blogs are more focused in terms of information. So, User will only get related
blogs in response to his query. These results will be then ranked according to
our proposed method and are finally presented in front of user in descending
order
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8228</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8228</id><created>2013-07-31</created><authors><author><keyname>Salim</keyname><forenames>Abu</forenames></author><author><keyname>Tiwari</keyname><forenames>Rajesh Kumar</forenames></author><author><keyname>Tripathi</keyname><forenames>Sachin</forenames></author></authors><title>Addressing Security Challenges in Cloud Computing</title><categories>cs.DC</categories><comments>13 pages. International Journal of Computer Engineering and
  Applications,April June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a new computing paradigm which allows sharing of resources
on remote server such as hardware, network, storage using internet and provides
the way through which application, computing power, computing infrastructure
can be delivered to the user as a service. Cloud computing unique attribute
promise cost effective Information Technology Solution (IT Solution) to the
user. All computing needs are provided by the Cloud Service Provider (CSP) and
they can be increased or decreased dynamically as required by the user. As data
and Application are located at the server and may be beyond geographical
boundary, this leads a number of concern from the user prospective. The
objective of this paper is to explore the key issues of cloud computing which
is delaying its adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8230</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8230</id><created>2013-07-31</created><authors><author><keyname>Ramaiyan</keyname><forenames>Venkatesh</forenames></author></authors><title>An Information Theoretic Point of View to Contention Resolution</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a slotted wireless network in an infrastructure setup with a base
station (or an access point) and N users. The wireless channel gain between the
base station and the users is assumed to be i.i.d., and the base station seeks
to schedule the user with the highest channel gain in every slot (opportunistic
scheduling). We assume that the identity of the user with the highest channel
gain is resolved using a series of contention slots and with feedback from the
base station. In this setup, we formulate the contention resolution problem for
opportunistic scheduling as identifying a random threshold (channel gain) that
separates the best channel from the other samples. We show that the average
delay to resolve contention is related to the entropy of the random threshold.
We illustrate our formulation by studying the opportunistic splitting algorithm
(OSA) for i.i.d. wireless channel [9]. We note that the thresholds of OSA
correspond to a maximal probability allocation scheme. We conjecture that
maximal probability allocation is an entropy minimizing strategy and a delay
minimizing strategy for i.i.d. wireless channel. Finally, we discuss the
applicability of this framework for few other network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8232</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8232</id><created>2013-07-31</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Nesic</keyname><forenames>Dragan</forenames></author></authors><title>Maximum-Hands-Off Control and L1 Optimality</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>2013 IEEE 52nd Annual Conference on Decision and Control (CDC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a new paradigm of control, called a
maximum-hands-off control. A hands-off control is defined as a control that has
a much shorter support than the horizon length. The maximum-hands-off control
is the minimum-support (or sparsest) control among all admissible controls. We
first prove that a solution to an L1-optimal control problem gives a
maximum-hands-off control, and vice versa. This result rationalizes the use of
L1 optimality in computing a maximum-hands-off control. The solution has in
general the &quot;bang-off-bang&quot; property, and hence the control may be
discontinuous. We then propose an L1/L2-optimal control to obtain a continuous
hands-off control. Examples are shown to illustrate the effectiveness of the
proposed control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8233</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8233</id><created>2013-07-31</created><authors><author><keyname>T&#xfc;nnermann</keyname><forenames>Jan</forenames></author><author><keyname>Hennig</keyname><forenames>Markus</forenames></author><author><keyname>Silbernagel</keyname><forenames>Michael</forenames></author><author><keyname>Mertsching</keyname><forenames>B&#xe4;rbel</forenames></author></authors><title>A Prototyping Environment for Integrated Artificial Attention Systems</title><categories>cs.CV</categories><report-no>ISACS/2013/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial visual attention systems aim to support technical systems in
visual tasks by applying the concepts of selective attention observed in humans
and other animals. Such systems are typically evaluated against ground truth
obtained from human gaze-data or manually annotated test images. When applied
to robotics, the systems are required to be adaptable to the target system.
Here, we describe a flexible environment based on a robotic middleware layer
allowing the development and testing of attention-guided vision systems. In
such a framework, the systems can be tested with input from various sources,
different attention algorithms at the core, and diverse subsequent tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8239</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8239</id><created>2013-07-31</created><authors><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Barth</keyname><forenames>Andreas</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Detecting the historical roots of research fields by reference
  publication year spectroscopy (RPYS)</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the quantitative method named &quot;reference publication year
spectroscopy&quot; (RPYS). With this method one can determine the historical roots
of research fields and quantify their impact on current research. RPYS is based
on the analysis of the frequency with which references are cited in the
publications of a specific research field in terms of the publication years of
these cited references. The origins show up in the form of more or less
pronounced peaks mostly caused by individual publications which are cited
particularly frequently. In this study, we use research on graphene and on
solar cells to illustrate how RPYS functions, and what results it can deliver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8240</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8240</id><created>2013-07-31</created><updated>2016-02-28</updated><authors><author><keyname>Sharma</keyname><forenames>Abhay</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>On Finding a Subset of Healthy Individuals from a Large Population</title><categories>cs.IT math.IT</categories><comments>32 pages, 2 figures, 3 tables, revised version of a paper submitted
  to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive mutual information based upper and lower bounds on
the number of nonadaptive group tests required to identify a given number of
&quot;non defective&quot; items from a large population containing a small number of
&quot;defective&quot; items. We show that a reduction in the number of tests is
achievable compared to the approach of first identifying all the defective
items and then picking the required number of non-defective items from the
complement set. In the asymptotic regime with the population size $N
\rightarrow \infty$, to identify $L$ non-defective items out of a population
containing $K$ defective items, when the tests are reliable, our results show
that $\frac{C_s K}{1-o(1)} (\Phi(\alpha_0, \beta_0) + o(1))$ measurements are
sufficient, where $C_s$ is a constant independent of $N, K$ and $L$, and
$\Phi(\alpha_0, \beta_0)$ is a bounded function of $\alpha_0 \triangleq
\lim_{N\rightarrow \infty} \frac{L}{N-K}$ and $\beta_0 \triangleq
\lim_{N\rightarrow \infty} \frac{K} {N-K}$. Further, in the nonadaptive group
testing setup, we obtain rigorous upper and lower bounds on the number of tests
under both dilution and additive noise models. Our results are derived using a
general sparse signal model, by virtue of which, they are also applicable to
other important sparse signal based applications such as compressive sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8242</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8242</id><created>2013-07-31</created><updated>2013-12-03</updated><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author></authors><title>Sparse Packetized Predictive Control for Networked Control over Erasure
  Channels</title><categories>cs.SY math.OC</categories><comments>IEEE Transactions on Automatic Control, Volume 59 (2014), Issue 7
  (July) (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study feedback control over erasure channels with packet-dropouts. To
achieve robustness with respect to packet-dropouts, the controller transmits
data packets containing plant input predictions, which minimize a finite
horizon cost function. To reduce the data size of packets, we propose to adopt
sparsity-promoting optimizations, namely, ell-1-ell-2 and ell-2-constrained
ell-0 optimizations, for which efficient algorithms exist. We derive sufficient
conditions on design parameters, which guarantee (practical) stability of the
resulting feedback control systems when the number of consecutive
packet-dropouts is bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8250</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8250</id><created>2013-07-31</created><updated>2013-11-13</updated><authors><author><keyname>Kuehn</keyname><forenames>Christian</forenames></author><author><keyname>Martens</keyname><forenames>Erik A.</forenames></author><author><keyname>Romero</keyname><forenames>Daniel</forenames></author></authors><title>Critical Transitions in Social Network Activity</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>14 pages, 5 figures, revised and extended version</comments><journal-ref>Journal of Complex Networks, Vol. 2, No. 2, pp. 141-152, 2014</journal-ref><doi>10.1093/comnet/cnt022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large variety of complex systems in ecology, climate science, biomedicine
and engineering have been observed to exhibit tipping points, where the
internal dynamical state of the system abruptly changes. For example, such
critical transitions may result in the sudden change of ecological environments
and climate conditions. Data and models suggest that detectable warning signs
may precede some of these drastic events. This view is also corroborated by
abstract mathematical theory for generic bifurcations in stochastic multi-scale
systems. Whether the stochastic scaling laws used as warning signs are also
present in social networks that anticipate a-priori {\it unknown} events in
society is an exciting open problem, to which at present only highly
speculative answers can be given. Here, we instead provide a first step towards
tackling this formidable question by focusing on a-priori {\it known} events
and analyzing a social network data set with a focus on classical variance and
autocorrelation warning signs. Our results thus pertain to one absolutely
fundamental question: Can the stochastic warning signs known from other areas
also be detected in large-scale social network data? We answer this question
affirmatively as we find that several a-priori known events are preceded by
variance and autocorrelation growth. Our findings thus clearly establish the
necessary starting point to further investigate the relation between abstract
mathematical theory and various classes of critical transitions in social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8256</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8256</id><created>2013-07-31</created><authors><author><keyname>Kumar</keyname><forenames>Priyanka</forenames></author><author><keyname>Peri</keyname><forenames>Sathya</forenames></author></authors><title>Multi-Version Conflict Notion</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.6315,
  arXiv:1305.6624</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the useful notion of Multi-Version Conflict notion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8257</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8257</id><created>2013-07-31</created><authors><author><keyname>Femminella</keyname><forenames>Mauro</forenames></author><author><keyname>Giacinti</keyname><forenames>Francesco</forenames></author><author><keyname>Reali</keyname><forenames>Gianluca</forenames></author></authors><title>Enhancing Java Call Control with Media Server Control functions</title><categories>cs.NI</categories><comments>Accepted for publication in IEEE Communications Magazine, Design and
  Implementation (D&amp;I) Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel abstraction layer for application service
implementation compliant with the Java Call Control (JCC) specifications. It
simplifies creation of multimedia services using the Session Initiation
Protocol (SIP) and the Media Gateway Control Protocol (MGCP). In order to show
its effectiveness, we have implemented a JCC Resource Adaptor for a JAIN
Service Logic Execution Environment (JSLEE), using the Mobicents application
server, which is the only existing open source JSLEE implementation.
Experimental results, obtained by implementing a complex VoIP service, show
both a significant simplification of service implementation and improved
performance over legacy solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8268</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8268</id><created>2013-07-31</created><updated>2013-12-15</updated><authors><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author><author><keyname>Pratap</keyname><forenames>Rameshwar</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>Helly-Type Theorems in Property Testing</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Helly's theorem is a fundamental result in discrete geometry, describing the
ways in which convex sets intersect with each other. If $S$ is a set of $n$
points in $R^d$, we say that $S$ is $(k,G)$-clusterable if it can be
partitioned into $k$ clusters (subsets) such that each cluster can be contained
in a translated copy of a geometric object $G$. In this paper, as an
application of Helly's theorem, by taking a constant size sample from $S$, we
present a testing algorithm for $(k,G)$-clustering, i.e., to distinguish
between two cases: when $S$ is $(k,G)$-clusterable, and when it is
$\epsilon$-far from being $(k,G)$-clusterable. A set $S$ is $\epsilon$-far
$(0&lt;\epsilon\leq1)$ from being $(k,G)$-clusterable if at least $\epsilon n$
points need to be removed from $S$ to make it $(k,G)$-clusterable. We solve
this problem for $k=1$ and when $G$ is a symmetric convex object. For $k&gt;1$, we
solve a weaker version of this problem. Finally, as an application of our
testing result, in clustering with outliers, we show that one can find the
approximate clusters by querying a constant size sample, with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8269</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8269</id><created>2013-07-31</created><authors><author><keyname>Abiteboul</keyname><forenames>Serge</forenames></author><author><keyname>Antoine</keyname><forenames>&#xc9;milien</forenames></author><author><keyname>Miklau</keyname><forenames>Gerome</forenames></author><author><keyname>Stoyanovich</keyname><forenames>Julia</forenames></author><author><keyname>Moffitt</keyname><forenames>Vera Zaychik</forenames></author></authors><title>Introducing Access Control in Webdamlog</title><categories>cs.DB</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We survey recent work on the specification of an access control mechanism in
a collaborative environment. The work is presented in the context of the
WebdamLog language, an extension of datalog to a distributed context. We
discuss a fine-grained access control mechanism for intentional data based on
provenance as well as a control mechanism for delegation, i.e., for deploying
rules at remote peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8276</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8276</id><created>2013-07-31</created><authors><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Bernaschi</keyname><forenames>Massimo</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Bisson</keyname><forenames>Mauro</forenames></author><author><keyname>Fatica</keyname><forenames>Massimiliano</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Mastrostefano</keyname><forenames>Enrico</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Rossetti</keyname><forenames>Davide</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>GPU peer-to-peer techniques applied to a cluster interconnect</title><categories>physics.comp-ph cs.DC</categories><comments>paper accepted to CASS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern GPUs support special protocols to exchange data directly across the
PCI Express bus. While these protocols could be used to reduce GPU data
transmission times, basically by avoiding staging to host memory, they require
specific hardware features which are not available on current generation
network adapters. In this paper we describe the architectural modifications
required to implement peer-to-peer access to NVIDIA Fermi- and Kepler-class
GPUs on an FPGA-based cluster interconnect. Besides, the current software
implementation, which integrates this feature by minimally extending the RDMA
programming model, is discussed, as well as some issues raised while employing
it in a higher level API like MPI. Finally, the current limits of the technique
are studied by analyzing the performance improvements on low-level benchmarks
and on two GPU-accelerated applications, showing when and how they seem to
benefit from the GPU peer-to-peer method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8279</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8279</id><created>2013-07-31</created><authors><author><keyname>Nabizadeh</keyname><forenames>Somayeh</forenames></author><author><keyname>Rezvanian</keyname><forenames>Alireza</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Tracking Extrema in Dynamic Environment using Multi-Swarm Cellular PSO
  with Local Search</title><categories>cs.AI cs.NE</categories><comments>8 pages, 3 figures</comments><journal-ref>int j electron inform 1 (2012) 29-37</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world phenomena can be modelled as dynamic optimization problems.
In such cases, the environment problem changes dynamically and therefore,
conventional methods are not capable of dealing with such problems. In this
paper, a novel multi-swarm cellular particle swarm optimization algorithm is
proposed by clustering and local search. In the proposed algorithm, the search
space is partitioned into cells, while the particles identify changes in the
search space and form clusters to create sub-swarms. Then a local search is
applied to improve the solutions in the each cell. Simulation results for
static standard benchmarks and dynamic environments show superiority of the
proposed method over other alternative approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8281</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8281</id><created>2013-07-31</created><updated>2014-05-07</updated><authors><author><keyname>Greuet</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6, LM-Versailles, LIFL</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author></authors><title>Probabilistic Algorithm for Polynomial Optimization over a Real
  Algebraic Set</title><categories>cs.SC math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f, f_1, \ldots, f_\nV$ be polynomials with rational coefficients in the
indeterminates $\bfX=X_1, \ldots, X_n$ of maximum degree $D$ and $V$ be the set
of common complex solutions of $\F=(f_1,\ldots, f_\nV)$. We give an algorithm
which, up to some regularity assumptions on $\F$, computes an exact
representation of the global infimum $f^\star=\inf_{x\in V\cap\R^n} f\Par{x}$,
i.e. a univariate polynomial vanishing at $f^\star$ and an isolating interval
for $f^\star$. Furthermore, this algorithm decides whether $f^\star$ is reached
and if so, it returns $x^\star\in V\cap\R^n$ such that
$f\Par{x^\star}=f^\star$. This algorithm is probabilistic. It makes use of the
notion of polar varieties. Its complexity is essentially cubic in $\Par{\nV
D}^n$ and linear in the complexity of evaluating the input. This fits within
the best known deterministic complexity class $D^{O(n)}$. We report on some
practical experiments of a first implementation that is available as a Maple
package. It appears that it can tackle global optimization problems that were
unreachable by previous exact algorithms and can manage instances that are hard
to solve with purely numeric techniques. As far as we know, even under the
extra genericity assumptions on the input, it is the first probabilistic
algorithm that combines practical efficiency with good control of complexity
for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8300</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8300</id><created>2013-07-31</created><authors><author><keyname>Turner</keyname><forenames>Katharine</forenames></author></authors><title>Means and medians of sets of persistence diagrams</title><categories>math.ST cs.CG math.AT math.MG stat.TH</categories><comments>23 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The persistence diagram is the fundamental object in topological data
analysis. It inherits the stochastic variability of the data we use as input.
As such we need to understand how to perform statistics on the space of
persistence diagrams. This paper looks at the space of persistence diagrams
under a variety of different metrics which are analogous to $L^p$ metrics on
the space of functions. Using these metrics we can form different cost
functions defining different central tendencies and their corresponding
measures of variability. This gives us the natural definitions of both the mean
and median of a finite number of persistence diagrams. We give a
characterization of the mean and the median of an odd number of persistence
diagrams. Although we have examples of the mean not being unique nor continuous
we prove that generically the mean of sets of persistence diagrams with
finitely many off diagonal points is unique. In comparison the sets of
persistence diagrams with finitely many off diagonal points which do not have a
unique median is of positive measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8305</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8305</id><created>2013-07-31</created><authors><author><keyname>Glasmachers</keyname><forenames>Tobias</forenames></author></authors><title>The Planning-ahead SMO Algorithm</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sequential minimal optimization (SMO) algorithm and variants thereof are
the de facto standard method for solving large quadratic programs for support
vector machine (SVM) training. In this paper we propose a simple yet powerful
modification. The main emphasis is on an algorithm improving the SMO step size
by planning-ahead. The theoretical analysis ensures its convergence to the
optimum. Experiments involving a large number of datasets were carried out to
demonstrate the superiority of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8319</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8319</id><created>2013-07-31</created><authors><author><keyname>Zhbannikov</keyname><forenames>Ilya Y.</forenames></author><author><keyname>Donohoe</keyname><forenames>Gregory W.</forenames></author></authors><title>Allocating the chains of consecutive additions for optimal fixed-point
  data path synthesis</title><categories>cs.AR</categories><journal-ref>2012 IEEE 55th International Midwest Symposium on Circuits and
  Systems (MWSCAS)</journal-ref><doi>10.1109/MWSCAS.2012.6292184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimization of computational errors in the fixed-point data path is often
difficult task. Many signal processing algorithms use chains of consecutive
additions. The analyzing technique that can be applied to fixed-point data path
synthesis has been proposed. This technique takes advantage of allocating the
chains of consecutive additions in order to predict growing width of the data
path and minimize the design complexity and computational errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8320</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8320</id><created>2013-07-31</created><updated>2014-02-28</updated><authors><author><keyname>Wimalajeewa</keyname><forenames>Thankshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>OMP Based Joint Sparsity Pattern Recovery Under Communication
  Constraints</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1211.6719</comments><doi>10.1109/TSP.2014.2343947</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of joint sparsity pattern recovery based on low
dimensional multiple measurement vectors (MMVs) in resource constrained
distributed networks. We assume that distributed nodes observe sparse signals
which share the same sparsity pattern and each node obtains measurements via a
low dimensional linear operator. When the measurements are collected at
distributed nodes in a communication network, it is often required that joint
sparse recovery be performed under inherent resource constraints such as
communication bandwidth and transmit/processing power.
  We present two approaches to take the communication constraints into account
while performing common sparsity pattern recovery. First, we explore the use of
a shared multiple access channel (MAC) in forwarding observations residing at
each node to a fusion center. With MAC, while the bandwidth requirement does
not depend on the number of nodes, the fusion center has access to only a
linear combination of the observations. We discuss the conditions under which
the common sparsity pattern can be estimated reliably. Second, we develop two
collaborative algorithms based on Orthogonal Matching Pursuit (OMP), to jointly
estimate the common sparsity pattern in a decentralized manner with a low
communication overhead. In the proposed algorithms, each node exploits
collaboration among neighboring nodes by sharing a small amount of information
for fusion at different stages in estimating the indices of the true support in
a greedy manner. Efficiency and effectiveness of the proposed algorithms are
demonstrated via simulations along with a comparison with the most related
existing algorithms considering the trade-off between the performance gain and
the communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8322</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8322</id><created>2013-07-31</created><authors><author><keyname>Abassi</keyname><forenames>Ryma</forenames><affiliation>University of Carthage, sup'com</affiliation></author><author><keyname>Fatmi</keyname><forenames>Sihem Guemara El</forenames><affiliation>University of Carthage, sup'com</affiliation></author></authors><title>Delegation Management Modeling in a Security Policy based Environment</title><categories>cs.CR</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 122, 2013, pp. 85-95</journal-ref><doi>10.4204/EPTCS.122.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security Policies (SP) constitute the core of communication networks
protection infrastructures. It offers a set of rules allowing differentiating
between legitimate actions and prohibited ones and consequently, associates
each entity in the network with a set of permissions and privileges. Moreover,
in today's technological society and to allow applications perpetuity,
communication networks must support the collaboration between entities to face
up any unavailability or flinching. This collaboration must be governed by
security mechanisms according to the established permissions and privileges.
Delegation is a common practice that is used to simplify the sharing of
responsibilities and privileges. The delegation process in a SP environment can
be implanted through the use of adequate formalisms and modeling. The main
contribution of this paper is then, the proposition of a generic and formal
modeling of delegation process. This modeling is based on three steps composing
the delegation life cycle: negotiation used for delegation initiation,
verification of the SP respect while delegating and revocation of an
established delegation. Hence, we propose to deal with each step according to
the main delegation characteristics and extend them by some new specificities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8327</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8327</id><created>2013-07-31</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Song</keyname><forenames>Eva C.</forenames></author></authors><title>The Likelihood Encoder for Source Coding</title><categories>cs.IT math.IT</categories><comments>ITW Sept. 2013, invited, 2 pages, uses IEEEtran.cls</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The likelihood encoder with a random codebook is demonstrated as an effective
tool for source coding. Coupled with a soft covering lemma (associated with
channel resolvability), likelihood encoders yield simple achievability proofs
for known results, such as rate-distortion theory. They also produce a
tractable analysis for secure rate-distortion theory and strong coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8335</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8335</id><created>2013-07-30</created><authors><author><keyname>Asabere</keyname><forenames>Nana Yaw</forenames></author><author><keyname>Gyamfi</keyname><forenames>Nana Kwame</forenames></author></authors><title>AIDSS-HR: An Automated Intelligent Decision Support System for Enhancing
  the Performance of Employees</title><categories>cs.CY</categories><comments>07 pages, 04 figures, IJCSN Journal Volume 2, Issue 4, August 2013</comments><report-no>IJCSN-2013-2-4-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of employees in an organization is a very important issue for
effective delivery and output. Various performance management systems with the
aid of Information and Communication Technology (ICT) are currently being used
by companies. Such systems are most of the time connected and accessible
through to the internet / www. Through review of relevant literature and a
system development methodology, this paper proposes an Automated Intelligent
Decision Support Human Resource (AIDSS-HR) system that seeks to control and
manage employee activities by tracking the number of years a staff has been at
post, keeping inventory on logistics, analyzing appraisal reports of an
individual staff and invoking real time prompts devoid of false alarm. The
implementation of AIDSS-HR will improve the performance management of employees
and benefit the organization, employees and developing nations as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8371</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8371</id><created>2013-07-31</created><updated>2014-03-07</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Long</keyname><forenames>Philip M.</forenames></author></authors><title>The Power of Localization for Efficiently Learning Linear Separators
  with Noise</title><categories>cs.LG cs.CC cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach for designing computationally efficient learning
algorithms that are tolerant to noise. We demonstrate the effectiveness of our
approach by designing algorithms with improved noise tolerance guarantees for
learning linear separators.
  We consider the malicious noise model of Valiant and the adversarial label
noise model of Kearns, Schapire, and Sellie. For malicious noise, where the
adversary can corrupt an $\eta$ of fraction both the label part and the feature
part, we provide a polynomial-time algorithm for learning linear separators in
$\Re^d$ under the uniform distribution with near information-theoretic optimal
noise tolerance of $\eta = \Omega(\epsilon)$. We also get similar improvements
for the adversarial label noise model. We obtain similar results for more
general classes of distributions including isotropic log-concave distributions.
  In addition, our algorithms achieve a label complexity whose dependence on
the error parameter $\epsilon$ is {\em exponentially better} than that of any
passive algorithm. This provides the first polynomial-time active learning
algorithm for learning linear separators in the presence of adversarial label
noise, as well as the first analysis of active learning under the malicious
noise model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8385</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8385</id><created>2013-07-31</created><authors><author><keyname>Sharma</keyname><forenames>Adity</forenames></author><author><keyname>Agarwal</keyname><forenames>Anoo</forenames></author><author><keyname>Kumar</keyname><forenames>Vinay</forenames></author></authors><title>A simple technique for steganography</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new technique for data hiding in digital image is proposed in this paper.
Steganography is a well known technique for hiding data in an image, but
generally the format of image plays a pivotal role in it, and the scheme is
format dependent. In this paper we will discuss a new technique where
irrespective of the format of image, we can easily hide a large amount of data
without deteriorating the quality of the image. The data to be hidden is
enciphered with the help of a secret key. This enciphered data is then embedded
at the end of the image. The enciphered data bits are extracted and then
deciphered with the help of same key used for encryption. Simulation results
show that Image Quality Measures of this proposed scheme are better than the
conventional LSB replacing technique. The proposed method is simple and is easy
to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8389</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8389</id><created>2013-07-31</created><authors><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author><author><keyname>Have</keyname><forenames>Christian Theil</forenames></author></authors><title>Proceedings of the 13th International Colloquium on Implementation of
  Constraint and LOgic Programming Systems</title><categories>cs.PL</categories><comments>Proceedings of the 13th International Colloquium on Implementation of
  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August
  25, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 13th International Colloquium on
Implementation of Constraint and LOgic Programming Systems (CICLOPS 2013), held
in Istanbul, Turkey during August 25, 2013. CICLOPS is a well established line
of workshops, traditionally co-located with ICLP, that aims at discussing and
exchanging experience on the design, implementation, and optimization of
constraint and logic programming systems, and other systems based on logic as a
means of expressing computations. This year, CICLOPS received 8 paper
submissions. Each submission was reviewed by at least 3 Program Committee
members and, at the end, 6 papers were accepted for presentation at the
workshop. We would like to thank the ICLP organizers for their support, the
EasyChair conference management system for making the life of the program
chairs easier and arxiv.org for providing permanent hosting. Thanks should go
also to the authors of all submitted papers for their contribution to make
CICLOPS alive and to the participants for making the event a meeting point for
a fruitful exchange of ideas and feedback on recent developments. Finally, we
want to express our gratitude to the Program Committee members, as the
symposium would not have been possible without their dedicated work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8401</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8401</id><created>2013-07-31</created><authors><author><keyname>Zhbannikov</keyname><forenames>Ilya Y.</forenames></author><author><keyname>Donohoe</keyname><forenames>Gregory W.</forenames></author></authors><title>FpSynt: a fixed-point datapath synthesis tool for embedded systems</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital mobile systems must function with low power, small size and weight,
and low cost. High-performance desktop microprocessors, with built-in floating
point hardware, are not suitable in these cases. For embedded systems, it can
be advantageous to implement these calculations with fixed point arithmetic
instead. We present an automated fixed-point data path synthesis tool FpSynt
for designing embedded applications in fixed-point domain with sufficient
accuracy for most applications. FpSynt is available under the GNU General
Public License from the following GitHub repository:
http://github.com/izhbannikov/FPSYNT
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8403</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8403</id><created>2013-07-31</created><updated>2013-11-19</updated><authors><author><keyname>Dadoun</keyname><forenames>Benjamin</forenames></author><author><keyname>Neininger</keyname><forenames>Ralph</forenames></author></authors><title>A statistical view on exchanges in Quickselect</title><categories>math.PR cs.DM</categories><comments>Theorem 4.4 revised; accepted for publication in Analytic
  Algorithmics and Combinatorics (ANALCO14)</comments><msc-class>Primary 60F05, 68P10, secondary 60C05, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the number of key exchanges required by Hoare's FIND
algorithm (also called Quickselect) when operating on a uniformly distributed
random permutation and selecting an independent uniformly distributed rank.
After normalization we give a limit theorem where the limit law is a perpetuity
characterized by a recursive distributional equation. To make the limit theorem
usable for statistical methods and statistical experiments we provide an
explicit rate of convergence in the Kolmogorov--Smirnov metric, a numerical
table of the limit law's distribution function and an algorithm for exact
simulation from the limit distribution. We also investigate the limit law's
density. This case study provides a program applicable to other cost measures,
alternative models for the rank selected and more balanced choices of the pivot
element such as median-of-$2t+1$ versions of Quickselect as well as further
variations of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8405</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8405</id><created>2013-07-31</created><authors><author><keyname>Wang</keyname><forenames>Zixuan</forenames></author><author><keyname>Yan</keyname><forenames>Jinyun</forenames></author></authors><title>Who and Where: People and Location Co-Clustering</title><categories>cs.CV</categories><comments>2013 IEEE International Conference on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the clustering problem on images where each image
contains patches in people and location domains. We exploit the correlation
between people and location domains, and proposed a semi-supervised
co-clustering algorithm to cluster images. Our algorithm updates the
correlation links at the runtime, and produces clustering in both domains
simultaneously. We conduct experiments in a manually collected dataset and a
Flickr dataset. The result shows that the such correlation improves the
clustering performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8409</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8409</id><created>2013-07-31</created><updated>2014-03-24</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Jovanovic</keyname><forenames>Miodrag</forenames><affiliation>FT R\&amp;D</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames><affiliation>FT R\&amp;D</affiliation></author></authors><title>How user throughput depends on the traffic demand in large cellular
  networks</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><journal-ref>WiOpt - SpaSWiN (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Little's law allows to express the mean user throughput in any region of the
network as the ratio of the mean traffic demand to the steady-state mean number
of users in this region. Corresponding statistics are usually collected in
operational networks for each cell. Using ergodic arguments and Palm theoretic
formalism, we show that the global mean user throughput in the network is equal
to the ratio of these two means in the steady state of the &quot;typical cell&quot;.
Here, both means account for double averaging: over time and network geometry,
and can be related to the per-surface traffic demand, base-station density and
the spatial distribution of the SINR. This latter accounts for network
irregularities, shadowing and idling cells via cell-load equations. We validate
our approach comparing analytical and simulation results for Poisson network
model to real-network cell-measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8410</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8410</id><created>2013-07-31</created><authors><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Rocquencourt, LINCS</affiliation></author><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Singh</keyname><forenames>Chandramani</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Analysis of a Proportionally Fair and Locally Adaptive spatial Aloha in
  Poisson Networks</title><categories>cs.NI cs.IT math.IT math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proportionally fair sharing of the capacity of a Poisson network using
Spatial-Aloha leads to closed-form performance expressions in two extreme
cases: (1) the case without topology information, where the analysis boils down
to a parametric optimization problem leveraging stochastic geometry; (2) the
case with full network topology information, which was recently solved using
shot-noise techniques. We show that there exists a continuum of adaptive
controls between these two extremes, based on local stopping sets, which can
also be analyzed in closed form. We also show that these control schemes are
implementable, in contrast to the full information case which is not. As local
information increases, the performance levels of these schemes are shown to get
arbitrarily close to those of the full information scheme. The analytical
results are combined with discrete event simulation to provide a detailed
evaluation of the performance of this class of medium access controls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8425</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8425</id><created>2013-07-31</created><updated>2014-09-28</updated><authors><author><keyname>Fomin</keyname><forenames>Sergey</forenames></author><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Koshevoy</keyname><forenames>Gleb</forenames></author></authors><title>Subtraction-free complexity, cluster transformations, and spanning trees</title><categories>math.CO cs.CC</categories><comments>30 pages. Version 4: Section 8 edited. Version 3: Section 8 is new.
  Version 2: title changed; Section 7 is new; comparison with the Jerrum-Snir
  lower bound added</comments><msc-class>68Q25, 05E05, 13F60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subtraction-free computational complexity is the version of arithmetic
circuit complexity that allows only three operations: addition, multiplication,
and division.
  We use cluster transformations to design efficient subtraction-free
algorithms for computing Schur functions and their skew, double, and
supersymmetric analogues, thereby generalizing earlier results by P. Koev.
  We develop such algorithms for computing generating functions of spanning
trees, both directed and undirected. A comparison to the lower bound due to M.
Jerrum and M. Snir shows that in subtraction-free computations, &quot;division can
be exponentially powerful.&quot;
  Finally, we give a simple example where the gap between ordinary and
subtraction-free complexity is exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.8430</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.8430</id><created>2013-07-31</created><authors><author><keyname>Conroy</keyname><forenames>Bryan R.</forenames></author><author><keyname>Walz</keyname><forenames>Jennifer M.</forenames></author><author><keyname>Cheung</keyname><forenames>Brian</forenames></author><author><keyname>Sajda</keyname><forenames>Paul</forenames></author></authors><title>Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ)</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm for simultaneously training sparse
generalized linear models across many related problems, which may arise from
bootstrapping, cross-validation and nonparametric permutation testing. Our
approach leverages the redundancies across problems to obtain significant
computational improvements relative to solving the problems sequentially by a
conventional algorithm. We demonstrate our fast simultaneous training of
generalized linear models (FaSTGLZ) algorithm on a number of real-world
datasets, and we run otherwise computationally intensive bootstrapping and
permutation test analyses that are typically necessary for obtaining
statistically rigorous classification results and meaningful interpretation.
Code is freely available at http://liinc.bme.columbia.edu/fastglz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0002</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0002</id><created>2013-07-31</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author></authors><title>Packetized Predictive Control for Rate-Limited Networks via Sparse
  Representation</title><categories>cs.SY math.OC</categories><comments>9 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1307.8242</comments><journal-ref>Proceedings 51th IEEE Conference of Decision and Control (CDC),
  pp. 1362-1367, Dec. 2012</journal-ref><doi>10.1109/CDC.2012.6426647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a networked control architecture for linear time-invariant plants in
which an unreliable data-rate limited network is placed between the controller
and the plant input. The distinguishing aspect of the situation at hand is that
an unreliable data-rate limited network is placed between controller and the
plant input. To achieve robustness with respect to dropouts, the controller
transmits data packets containing plant input predictions, which minimize a
finite horizon cost function. In our formulation, we design sparse packets for
rate-limited networks, by adopting an an ell-0 optimization, which can be
effectively solved by an orthogonal matching pursuit method. Our formulation
ensures asymptotic stability of the control loop in the presence of bounded
packet dropouts. Simulation results indicate that the proposed controller
provides sparse control packets, thereby giving bit-rate reductions for the
case of memoryless scalar coding schemes when compared to the use of, more
common, quadratic cost functions, as in linear quadratic (LQ) control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0029</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0029</id><created>2013-07-31</created><authors><author><keyname>Nepusz</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Vicsek</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Hierarchical self-organization of non-cooperating individuals</title><categories>physics.soc-ph cs.SI physics.bio-ph</categories><comments>Supplementary videos are to be found at
  http://hal.elte.hu/~nepusz/research/supplementary/hierarchy/</comments><doi>10.1371/journal.pone.0081449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchy is one of the most conspicuous features of numerous natural,
technological and social systems. The underlying structures are typically
complex and their most relevant organizational principle is the ordering of the
ties among the units they are made of according to a network displaying
hierarchical features. In spite of the abundant presence of hierarchy no
quantitative theoretical interpretation of the origins of a multi-level,
knowledge-based social network exists. Here we introduce an approach which is
capable of reproducing the emergence of a multi-levelled network structure
based on the plausible assumption that the individuals (representing the nodes
of the network) can make the right estimate about the state of their changing
environment to a varying degree. Our model accounts for a fundamental feature
of knowledge-based organizations: the less capable individuals tend to follow
those who are better at solving the problems they all face. We find that
relatively simple rules lead to hierarchical self-organization and the specific
structures we obtain possess the two, perhaps most important features of
complex systems: a simultaneous presence of adaptability and stability. In
addition, the performance (success score) of the emerging networks is
significantly higher than the average expected score of the individuals without
letting them copy the decisions of the others. The results of our calculations
are in agreement with a related experiment and can be useful from the point of
designing the optimal conditions for constructing a given complex social
structure as well as understanding the hierarchical organization of such
biological structures of major importance as the regulatory pathways or the
dynamics of neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0037</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0037</id><created>2013-07-31</created><updated>2014-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Ryan K.</forenames></author><author><keyname>Gasparri</keyname><forenames>Andrea</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Route Swarm: Wireless Network Optimization through Mobility</title><categories>cs.SY cs.MA cs.NI cs.RO math.OC</categories><comments>9 pages, 4 figures, submitted to the IEEE International Conference on
  Intelligent Robots and Systems (IROS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we demonstrate a novel hybrid architecture for coordinating
networked robots in sensing and information routing applications. The proposed
INformation and Sensing driven PhysIcally REconfigurable robotic network
(INSPIRE), consists of a Physical Control Plane (PCP) which commands agent
position, and an Information Control Plane (ICP) which regulates information
flow towards communication/sensing objectives. We describe an instantiation
where a mobile robotic network is dynamically reconfigured to ensure high
quality routes between static wireless nodes, which act as source/destination
pairs for information flow. The ICP commands the robots towards evenly
distributed inter-flow allocations, with intra-flow configurations that
maximize route quality. The PCP then guides the robots via potential-based
control to reconfigure according to ICP commands. This formulation, deemed
Route Swarm, decouples information flow and physical control, generating a
feedback between routing and sensing needs and robotic configuration. We
demonstrate our propositions through simulation under a realistic wireless
network regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0041</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0041</id><created>2013-07-31</created><updated>2014-07-16</updated><authors><author><keyname>Tanbourgi</keyname><forenames>Ralph</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>A Tractable Model for Non-Coherent Joint-Transmission Base Station
  Cooperation</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2014.2340860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tractable model for analyzing non-coherent joint
transmission base station (BS) cooperation, taking into account the irregular
BS deployment typically encountered in practice. Besides cellular-network
specific aspects such as BS density, channel fading, average path loss and
interference, the model also captures relevant cooperation mechanisms including
user-centric BS clustering and channel-dependent cooperation activation. The
locations of all BSs are modeled by a Poisson point process. Using tools from
stochastic geometry, the signal-to-interference-plus-noise ratio
($\mathtt{SINR}$) distribution with cooperation is precisely characterized in a
generality-preserving form. The result is then applied to practical design
problems of recent interest. We find that increasing the network-wide BS
density improves the $\mathtt{SINR}$, while the gains increase with the path
loss exponent. For pilot-based channel estimation, the average spectral
efficiency saturates at cluster sizes of around $7$ BSs for typical values,
irrespective of backhaul quality. Finally, it is shown that intra-cluster
frequency reuse is favorable in moderately loaded cells with generous
cooperation activation, while intra-cluster coordinated scheduling may be
better in lightly loaded cells with conservative cooperation activation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0047</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0047</id><created>2013-07-31</created><authors><author><keyname>Galas</keyname><forenames>David J.</forenames></author><author><keyname>Sakhanenko</keyname><forenames>Nikita A.</forenames></author><author><keyname>Keller</keyname><forenames>Benjamin</forenames></author></authors><title>On Lattices and the Dualities of Information Measures</title><categories>cs.IT math.IT q-bio.QM</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measures of dependence among variables, and measures of information content
and shared information have become valuable tools of multi-variable data
analysis. Information measures, like marginal entropies, mutual and
multi-information, have a number of significant advantages over more standard
statistical methods, like their reduced sensitivity to sampling limitations
than statistical estimates of probability densities. There are also interesting
applications of these measures to the theory of complexity and to statistical
mechanics. Their mathematical properties and relationships are therefore of
interest at several levels.
  Of the interesting relationships between common information measures, perhaps
none are more intriguing and as elegant as the duality relationships based on
Mobius inversions. These inversions are directly related to the lattices
(posets) that describe these sets of variables and their multi-variable
measures. In this paper we describe extensions of the duality previously noted
by Bell to a range of measures, and show how the structure of the lattice
determines fundamental relationships of these functions. Our major result is a
set of interlinked duality relations among marginal entropies, interaction
information, and conditional interaction information. The implications of these
results include a flexible range of alternative formulations of
information-based measures, and a new set of sum rules that arise from
path-independent sums on the lattice. Our motivation is to advance the
fundamental integration of this set of ideas and relations, and to show
explicitly the ways in which all these measures are interrelated through
lattice properties. These ideas can be useful in constructing theories of
complexity, descriptions of large scale stochastic processes and systems, and
in devising algorithms and approximations for computations in multi-variable
data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0056</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0056</id><created>2013-07-31</created><authors><author><keyname>Zhang</keyname><forenames>Kaiwen</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author></authors><title>SDN-like: The Next Generation of Pub/Sub</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Networking (SDN) has raised the boundaries of cloud
computing by offering unparalleled levels of control and flexibility to system
administrators over their virtualized environments. To properly embrace this
new era of SDN-driven network architectures, the research community must not
only consider the impact of SDN over the protocol stack, but also on its
overlying networked applications. In this big ideas paper, we study the impact
of SDN on the design of future message-oriented middleware, specifically
pub/sub systems. We argue that key concepts put forth by SDN can be applied in
a meaningful fashion to the next generation of pub/sub systems. First, pub/sub
can adopt a logically centralized controller model for maintenance, monitoring,
and control of the overlay network. We establish a parallel with existing work
on centralized pub/sub routing and discuss how the logically centralized
controller model can be implemented in a distributed manner. Second, we
investigate the separation of the control and data plane, which is integral to
SDN, which can be adopted to raise the level of decoupling in pub/sub. We
introduce a new model of pub/sub which separates the traditional publisher and
subscriber roles into flow regulators and producer/consumers of data. We then
present use cases that benefit from this approach and study the impact of
decoupling for performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0066</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0066</id><created>2013-07-31</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Drawing Arrangement Graphs In Small Grids, Or How To Play Planarity</title><categories>cs.CG</categories><comments>12 pages, 8 figures. To appear at 21st Int. Symp. Graph Drawing,
  Bordeaux, 2013</comments><acm-class>F.2.2</acm-class><journal-ref>J. Graph Algorithms &amp; Applications 18(2): 211-231, 2014</journal-ref><doi>10.7155/jgaa.00319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a linear-time algorithm that finds a planar drawing of every
graph of a simple line or pseudoline arrangement within a grid of area
O(n^{7/6}). No known input causes our algorithm to use area
\Omega(n^{1+\epsilon}) for any \epsilon&gt;0; finding such an input would
represent significant progress on the famous k-set problem from discrete
geometry. Drawing line arrangement graphs is the main task in the Planarity
puzzle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0068</identifier>
 <datestamp>2013-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0068</id><created>2013-07-31</created><authors><author><keyname>Christ</keyname><forenames>Michael</forenames></author><author><keyname>Demmel</keyname><forenames>James</forenames></author><author><keyname>Knight</keyname><forenames>Nicholas</forenames></author><author><keyname>Scanlon</keyname><forenames>Thomas</forenames></author><author><keyname>Yelick</keyname><forenames>Katherine</forenames></author></authors><title>Communication lower bounds and optimal algorithms for programs that
  reference arrays -- Part 1</title><categories>math.CA cs.CC cs.DS</categories><msc-class>68W40, 26D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The movement of data (communication) between levels of a memory hierarchy, or
between parallel processors on a network, can greatly dominate the cost of
computation, so algorithms that minimize communication are of interest.
Motivated by this, attainable lower bounds for the amount of communication
required by algorithms were established by several groups for a variety of
algorithms, including matrix computations. Prior work of
Ballard-Demmel-Holtz-Schwartz relied on a geometric inequality of Loomis and
Whitney for this purpose. In this paper the general theory of discrete
multilinear Holder-Brascamp-Lieb (HBL) inequalities is used to establish
communication lower bounds for a much wider class of algorithms. In some cases,
algorithms are presented which attain these lower bounds.
  Several contributions are made to the theory of HBL inequalities proper. The
optimal constant in such an inequality for torsion-free Abelian groups is shown
to equal one whenever it is finite. Bennett-Carbery-Christ-Tao had
characterized the tuples of exponents for which such an inequality is valid as
the convex polyhedron defined by a certain finite list of inequalities. The
problem of constructing an algorithm to decide whether a given inequality is on
this list, is shown to be equivalent to Hilbert's Tenth Problem over the
rationals, which remains open. Nonetheless, an algorithm which computes the
polyhedron itself is constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0075</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0075</id><created>2013-07-31</created><updated>2013-08-05</updated><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author></authors><title>Polynomial-Phase Signal Direction-Finding &amp; Source-Tracking with an
  Acoustic Vector Sensor</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new ESPRIT-based algorithm is proposed to estimate the direction-of-arrival
of an arbitrary degree polynomial-phase signal with a single acoustic vector
sensor. The proposed approach requires neither a priori knowledge of the
polynomial-phase signal's coefficients nor a priori knowledge of the
polynomial-phase signal's frequency-spectrum. A pre-processing technique is
also proposed to incorporate the single-forgetting-factor algorithm and
multiple-forgetting-factor adaptive tracking algorithm to track a
polynomial-phase signal using one acoustic vector sensor. Simulation results
verify the efficacy of the proposed direction finding and source tracking
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0083</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0083</id><created>2013-07-31</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Baochun</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Dominant Resource Fairness in Cloud Computing Systems with Heterogeneous
  Servers</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the multi-resource allocation problem in cloud computing systems
where the resource pool is constructed from a large number of heterogeneous
servers, representing different points in the configuration space of resources
such as processing, memory, and storage. We design a multi-resource allocation
mechanism, called DRFH, that generalizes the notion of Dominant Resource
Fairness (DRF) from a single server to multiple heterogeneous servers. DRFH
provides a number of highly desirable properties. With DRFH, no user prefers
the allocation of another user; no one can improve its allocation without
decreasing that of the others; and more importantly, no user has an incentive
to lie about its resource demand. As a direct application, we design a simple
heuristic that implements DRFH in real-world systems. Large-scale simulations
driven by Google cluster traces show that DRFH significantly outperforms the
traditional slot-based scheduler, leading to much higher resource utilization
with substantially shorter job completion times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0085</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0085</id><created>2013-07-31</created><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>An Improved Parameterized Algorithm for the Independent Feedback Vertex
  Set Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new parameterized algorithm for the {\sc
Independent Feedback Vertex Set} (IFVS) problem. Given a graph $G=(V,E)$, the
goal of the problem is to determine whether there exists a vertex subset
$F\subseteq V$ such that $V-F$ induces a forest in $G$ and $F$ is an
independent set. We show that there exists a parameterized algorithm that can
determine whether a graph contains an IFVS of size $k$ or not in time
$O(4^kn^{2})$. To our best knowledge, this result improves the known upper
bound for this problem, which is $O^{*}(5^{k}n^{O(1)})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0090</identifier>
 <datestamp>2013-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0090</id><created>2013-08-01</created><authors><author><keyname>James</keyname><forenames>A. P.</forenames></author><author><keyname>Francis</keyname><forenames>L. R. V. J.</forenames></author><author><keyname>Kumar</keyname><forenames>D.</forenames></author></authors><title>Resistive Threshold Logic</title><categories>cs.ET cs.AR</categories><comments>Memristors, Brain inspired logic circuits. IEEE Transactions on Very
  Large Scale Integration (VLSI) Systems, 2013</comments><doi>10.1109/TVLSI.2012.2232946</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We report a resistance based threshold logic family useful for mimicking
brain like large variable logic functions in VLSI. A universal Boolean logic
cell based on an analog resistive divider and threshold logic circuit is
presented. The resistive divider is implemented using memristors and provides
output voltage as a summation of weighted product of input voltages. The output
of resistive divider is converted into a binary value by a threshold operation
implemented by CMOS inverter and/or Opamp. An universal cell structure is
presented to decrease the overall implementation complexity and number of
components. When the number of input variables become very high, the proposed
cell offers advantages of smaller area and design simplicity in comparison with
CMOS based logic circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0094</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0094</id><created>2013-08-01</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Li</keyname><forenames>Jiangyuan</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Improving Physical Layer Secrecy Using Full-Duplex Jamming Receivers</title><categories>cs.IT math.IT</categories><comments>This manuscript will appear in the IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies secrecy rate optimization in a wireless network with a
single-antenna source, a multi-antenna destination and a multi-antenna
eavesdropper. This is an unfavorable scenario for secrecy performance as the
system is interference-limited. In the literature, assuming that the receiver
operates in half duplex (HD) mode, the aforementioned problem has been
addressed via use of cooperating nodes who act as jammers to confound the
eavesdropper. This paper investigates an alternative solution, which assumes
the availability of a full duplex (FD) receiver. In particular, while receiving
data, the receiver transmits jamming noise to degrade the eavesdropper channel.
The proposed self-protection scheme eliminates the need for external helpers
and provides system robustness. For the case in which global channel state
information is available, we aim to design the optimal jamming covariance
matrix that maximizes the secrecy rate and mitigates loop interference
associated with the FD operation. We consider both fixed and optimal linear
receiver design at the destination, and show that the optimal jamming
covariance matrix is rank-1, and can be found via an efficient 1-D search. For
the case in which only statistical information on the eavesdropper channel is
available, the optimal power allocation is studied in terms of ergodic and
outage secrecy rates. Simulation results verify the analysis and demonstrate
substantial performance gain over conventional HD operation at the destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0099</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0099</id><created>2013-08-01</created><authors><author><keyname>Teymoori</keyname><forenames>Fatemeh</forenames></author><author><keyname>Nabizadeh</keyname><forenames>Hamid</forenames></author><author><keyname>Teymoori</keyname><forenames>Farzaneh</forenames></author></authors><title>A new approach in position-based routing protocol using learning
  automata for vanets in city scenario</title><categories>cs.NI</categories><comments>9 pages, 6 figures, journal, International Journal of Ambient System
  and Application(IJASA), June 2013, Volume 1, Number 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main issues in Vehicular Ad-hoc NETworks (VANETs) is providing a
reliable and efficient routing in urban scenarios with regard to the high
vehicle mobility and presence of radio obstacle. In this paper, we propose a
Position-Based routing protocol using Learning Automata (PBLA). In addition,
PBLA uses the traffic information for enhancing learning. As we know, a main
characteristic of learning is increasing performance over time. We exploit this
characteristic to decreasing use of traffic information. Initially, PBLA make
effort to finding best and shortest path to mobile destination using traffic
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0102</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0102</id><created>2013-08-01</created><authors><author><keyname>Choi</keyname><forenames>Han-Lim</forenames></author></authors><title>Mutual Information-Based Planning for Informative Windowed Forecasting
  of Continuous-Time Linear Systems</title><categories>cs.SY cs.IT math.IT</categories><comments>9 pages, 3 figures, submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents expression of mutual information that defines the
information gain in planning of sensing resources, when the goal is to reduce
the forecast uncertainty of some quantities of interest and the system dynamics
is described as a continuous-time linear system. The method extends the
smoother approach in [5] to handle more general notion of verification entity -
continuous sequence of variables over some finite time window in the future.
The expression of mutual information for this windowed forecasting case is
derived and quantified, taking advantage of underlying conditional independence
structure and utilizing the fixed-interval smoothing formula with correlated
noises. Two numerical examples on (a) simplified weather forecasting with
moving verification paths, and (b) sensor network scheduling for tracking of
multiple moving targets are considered for validation of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0104</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0104</id><created>2013-08-01</created><authors><author><keyname>Gaurav</keyname><forenames>Dinesh Dileep</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>A Fast Eigen Solution for Homogeneous Quadratic Minimization with at
  most Three Constraints</title><categories>math.NA cs.IT math.IT</categories><comments>15 pages, The same content without appendices is accepted and is to
  be published in IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an eigenvalue based technique to solve the Homogeneous Quadratic
Constrained Quadratic Programming problem (HQCQP) with at most 3 constraints
which arise in many signal processing problems. Semi-Definite Relaxation (SDR)
is the only known approach and is computationally intensive. We study the
performance of the proposed fast eigen approach through simulations in the
context of MIMO relays and show that the solution converges to the solution
obtained using the SDR approach with significant reduction in complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0109</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0109</id><created>2013-08-01</created><updated>2014-07-07</updated><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Optimal Receiver Design for Diffusive Molecular Communication With Flow
  and Additive Noise</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures, 1 appendix. To appear in IEEE Transactions on
  NanoBioscience (submitted July 31, 2013, revised June 18, 2014, accepted July
  7, 2014)</comments><doi>10.1109/TNB.2014.2337239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we perform receiver design for a diffusive molecular
communication environment. Our model includes flow in any direction, sources of
information molecules in addition to the transmitter, and enzymes in the
propagation environment to mitigate intersymbol interference. We characterize
the mutual information between receiver observations to show how often
independent observations can be made. We derive the maximum likelihood sequence
detector to provide a lower bound on the bit error probability. We propose the
family of weighted sum detectors for more practical implementation and derive
their expected bit error probability. Under certain conditions, the performance
of the optimal weighted sum detector is shown to be equivalent to a matched
filter. Receiver simulation results show the tradeoff in detector complexity
versus achievable bit error probability, and that a slow flow in any direction
can improve the performance of a weighted sum detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0120</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0120</id><created>2013-08-01</created><authors><author><keyname>Asvadi</keyname><forenames>Reza</forenames></author><author><keyname>Matsumoto</keyname><forenames>Tad</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>Joint Distributed Source-Channel Decoding for LDPC-Coded Binary Markov
  Sources</title><categories>cs.IT math.IT</categories><comments>accepted to present in PIMRC-2013, London</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel joint decoding technique for distributed source-channel
(DSC) coded systems for transmission of correlated binary Markov sources over
additive white Gaussian noise (AWGN) channels. In the proposed scheme,
relatively short-length, low-density parity-check (LDPC) codes are
independently used to encode the bit sequences of each source. To reconstruct
the original bit sequence, a joint source-channel decoding (JSCD) technique is
proposed which exploits the knowledge of both temporal and source correlations.
The JSCD technique is composed of two stages, which are iteratively performed.
First, a sum-product (SP) decoder is serially concatenated with a BCJR decoder,
where the knowledge of source memory is utilized during {\em local (horizontal)
iterations}. Then, the estimate of correlation between the sources is used to
update the concatenated decoder during {\em global (vertical) iterations}.
Therefore, the correlation of the sources is assumed as side information in the
subsequent global iteration of each concatenated decoder. From the simulation
results of frame/bit error rate (FER/BER), we note that significant gains are
achieved by the proposed decoding scheme with respect to the case where the
correlation knowledge is not completely utilized at the decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0136</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0136</id><created>2013-08-01</created><authors><author><keyname>Kornyushkin</keyname><forenames>A.</forenames></author></authors><title>X- problem of value three</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The X-problem of number 3 for one dimension and related observations are
discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0143</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0143</id><created>2013-08-01</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Near-optimal phase retrieval of sparse vectors</title><categories>cs.IT math.FA math.IT</categories><journal-ref>Wavelets and Sparsity XV, Proceedings of SPIE Optics+Photonics,
  Session 14, 2013</journal-ref><doi>10.1117/12.2024355</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many areas of imaging science, it is difficult to measure the phase of
linear measurements. As such, one often wishes to reconstruct a signal from
intensity measurements, that is, perform phase retrieval. In several
applications the signal in question is believed to be sparse. In this paper, we
use ideas from the recently developed polarization method for phase retrieval
and provide an algorithm that is guaranteed to recover a sparse signal from a
number of phaseless linear measurements that scales linearly with the sparsity
of the signal (up to logarithmic factors). This is particularly remarkable
since it is known that a certain popular class of convex methods is not able to
perform recovery unless the number of measurements scales with the square of
the sparsity of the signal. This is a shorter version of a more complete
publication that will appear elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0148</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0148</id><created>2013-08-01</created><authors><author><keyname>Demirel</keyname><forenames>Omer</forenames></author><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author></authors><title>Balancing indivisible real-valued loads in arbitrary networks</title><categories>cs.DC</categories><comments>22 pages, 5 figures</comments><acm-class>G.2.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In parallel computing, a problem is divided into a set of smaller tasks that
are distributed across multiple processing elements. Balancing the load of the
processing elements is key to achieving good performance and scalability. If
the computational costs of the individual tasks vary over time in an
unpredictable way, dynamic load balancing aims at migrating them between
processing elements so as to maintain load balance. During dynamic load
balancing, the tasks amount to indivisible work packets with a real-valued
cost. For this case of indivisible, real- valued loads, we analyze the
balancing circuit model, a local dynamic load-balancing scheme that does not
require global communication. We extend previous analyses to the present case
and provide a probabilistic bound for the achievable load balance. Based on an
analogy with the offline balls-into-bins problem, we further propose a novel
algorithm for dynamic balancing of indivisible, real-valued loads. We benchmark
the proposed algorithm in numerical experiments and compare it with the
classical greedy algorithm, both in terms of solution quality and communication
cost. We find that the increased communication cost of the proposed algorithm
is compensated by a higher solution quality, leading on average to about an
order of magnitude gain in overall performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0158</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0158</id><created>2013-08-01</created><authors><author><keyname>Grust</keyname><forenames>Torsten</forenames></author><author><keyname>Ulrich</keyname><forenames>Alexander</forenames></author></authors><title>First-Class Functions for First-Order Database Engines</title><categories>cs.DB cs.PL</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><acm-class>H.2.3; D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Query Defunctionalization which enables off-the-shelf first-order
database engines to process queries over first-class functions. Support for
first-class functions is characterized by the ability to treat functions like
regular data items that can be constructed at query runtime, passed to or
returned from other (higher-order) functions, assigned to variables, and stored
in persistent data structures. Query defunctionalization is a non-invasive
approach that transforms such function-centric queries into the data-centric
operations implemented by common query processors. Experiments with XQuery and
PL/SQL database systems demonstrate that first-order database engines can
faithfully and efficiently support the expressive &quot;functions as data&quot; paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0168</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0168</id><created>2013-08-01</created><authors><author><keyname>Leigh</keyname><forenames>Graham E.</forenames></author></authors><title>Conservativity for theories of compositional truth via cut elimination</title><categories>math.LO cs.LO</categories><comments>18 pages</comments><msc-class>03F05</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a cut elimination argument that witnesses the conservativity of
the compositional axioms for truth (without the extended induction axiom) over
any theory interpreting a weak subsystem of arithmetic. In doing so we also fix
a critical error in Halbach's original presentation. Our methods show that the
admission of these axioms determines a hyper-exponential reduction in the size
of derivations of truth-free statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0173</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0173</id><created>2013-08-01</created><updated>2013-08-02</updated><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Parter</keyname><forenames>Merav</forenames></author></authors><title>Braess's Paradox in Wireless Networks: The Danger of Improved Technology</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When comparing new wireless technologies, it is common to consider the effect
that they have on the capacity of the network (defined as the maximum number of
simultaneously satisfiable links). For example, it has been shown that giving
receivers the ability to do interference cancellation, or allowing transmitters
to use power control, never decreases the capacity and can in certain cases
increase it by $\Omega(\log (\Delta \cdot P_{\max}))$, where $\Delta$ is the
ratio of the longest link length to the smallest transmitter-receiver distance
and $P_{\max}$ is the maximum transmission power. But there is no reason to
expect the optimal capacity to be realized in practice, particularly since
maximizing the capacity is known to be NP-hard. In reality, we would expect
links to behave as self-interested agents, and thus when introducing a new
technology it makes more sense to compare the values reached at game-theoretic
equilibria than the optimum values.
  In this paper we initiate this line of work by comparing various notions of
equilibria (particularly Nash equilibria and no-regret behavior) when using a
supposedly &quot;better&quot; technology. We show a version of Braess's Paradox for all
of them: in certain networks, upgrading technology can actually make the
equilibria \emph{worse}, despite an increase in the capacity. We construct
instances where this decrease is a constant factor for power control,
interference cancellation, and improvements in the SINR threshold ($\beta$),
and is $\Omega(\log \Delta)$ when power control is combined with interference
cancellation. However, we show that these examples are basically tight: the
decrease is at most O(1) for power control, interference cancellation, and
improved $\beta$, and is at most $O(\log \Delta)$ when power control is
combined with interference cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0174</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0174</id><created>2013-08-01</created><authors><author><keyname>Ko&#xe7;</keyname><forenames>Yakup</forenames></author><author><keyname>Verma</keyname><forenames>Trivik</forenames></author><author><keyname>Araujo</keyname><forenames>Nuno A. M.</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author></authors><title>MATCASC: A tool to analyse cascading line outages in power grids</title><categories>physics.soc-ph cs.SY</categories><journal-ref>IEEE International Workshop on Intelligent Energy Systems (IWIES),
  2013, pp. 143-148</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blackouts in power grids typically result from cascading failures. The key
importance of the electric power grid to society encourages further research
into sustaining power system reliability and developing new methods to manage
the risks of cascading blackouts. Adequate software tools are required to
better analyze, understand, and assess the consequences of the cascading
failures. This paper presents MATCASC, an open source MATLAB based tool to
analyse cascading failures in power grids. Cascading effects due to line
overload outages are considered. The applicability of the MATCASC tool is
demonstrated by assessing the robustness of IEEE test systems and real-world
power grids with respect to cascading failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0178</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0178</id><created>2013-08-01</created><updated>2015-07-19</updated><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Coded Caching with Nonuniform Demands</title><categories>cs.IT cs.NI math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network consisting of a file server connected through a shared
link to a number of users, each equipped with a cache. Knowing the popularity
distribution of the files, the goal is to optimally populate the caches such as
to minimize the expected load of the shared link. For a single cache, it is
well known that storing the most popular files is optimal in this setting.
However, we show here that this is no longer the case for multiple caches.
Indeed, caching only the most popular files can be highly suboptimal. Instead,
a fundamentally different approach is needed, in which the cache contents are
used as side information for coded communication over the shared link. We
propose such a coded caching scheme and prove that it is close to optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0180</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0180</id><created>2013-08-01</created><authors><author><keyname>Egri</keyname><forenames>Laszlo</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Larose</keyname><forenames>Benoit</forenames></author><author><keyname>Rafiey</keyname><forenames>Arash</forenames></author></authors><title>Space complexity of list H-colouring: a dichotomy</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dichotomy Conjecture for constraint satisfaction problems (CSPs) states
that every CSP is in P or is NP-complete (Feder-Vardi, 1993). It has been
verified for conservative problems (also known as list homomorphism problems)
by A. Bulatov (2003). We augment this result by showing that for digraph
templates H, every conservative CSP, denoted LHOM(H), is solvable in logspace
or is hard for NL. More precisely, we introduce a digraph structure we call a
circular N, and prove the following dichotomy: if H contains no circular N then
LHOM(H) admits a logspace algorithm, and otherwise LHOM(H) is hard for NL. Our
algorithm operates by reducing the lists in a complex manner based on a novel
decomposition of an auxiliary digraph, combined with repeated applications of
Reingold's algorithm for undirected reachability (2005). We also prove an
algebraic version of this dichotomy: the digraphs without a circular N are
precisely those that admit a finite chain of polymorphisms satisfying the
Hagemann-Mitschke identities. This confirms a conjecture of Larose and Tesson
(2007) for LHOM(H). Moreover, we show that the presence of a circular N can be
decided in time polynomial in the size of H.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0181</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0181</id><created>2013-08-01</created><updated>2014-09-17</updated><authors><author><keyname>Place</keyname><forenames>Thomas</forenames><affiliation>University Bordeaux, France</affiliation></author><author><keyname>van Rooijen</keyname><forenames>Lorijn</forenames><affiliation>University Bordeaux, France</affiliation></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames><affiliation>University Bordeaux, France</affiliation></author></authors><title>On Separation by Locally Testable and Locally Threshold Testable
  Languages</title><categories>cs.FL</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  18, 2014) lmcs:1163</journal-ref><doi>10.2168/LMCS-10(3:24)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A separator for two languages is a third language containing the first one
and disjoint from the second one. We investigate the following decision
problem: given two regular input languages, decide whether there exists a
locally testable (resp. a locally threshold testable) separator. In both cases,
we design a decision procedure based on the occurrence of special patterns in
automata accepting the input languages. We prove that the problem is
computationally harder than deciding membership. The correctness proof of the
algorithm yields a stronger result, namely a description of a possible
separator. Finally, we discuss the same problem for context-free input
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0183</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0183</id><created>2013-08-01</created><authors><author><keyname>Unsworth</keyname><forenames>Chris</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>An n-ary Constraint for the Stable Marriage Problem</title><categories>cs.DS cs.AI</categories><comments>7 pages. The Fifth Workshop on Modelling and Solving Problems with
  Constraints, held at the 19th International Joint Conference on Artificial
  Intelligence (IJCAI 2005)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an n-ary constraint for the stable marriage problem. This
constraint acts between two sets of integer variables where the domains of
those variables represent preferences. Our constraint enforces stability and
disallows bigamy. For a stable marriage instance with $n$ men and $n$ women we
require only one of these constraints, and the complexity of enforcing
arc-consistency is $O(n^2)$ which is optimal in the size of input. Our
computational studies show that our n-ary constraint is significantly faster
and more space efficient than the encodings presented in \cite{cp01}. We also
introduce a new problem to the constraint community, the sex-equal stable
marriage problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0187</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0187</id><created>2013-07-31</created><updated>2014-12-23</updated><authors><author><keyname>Pasteris</keyname><forenames>Stephen</forenames></author></authors><title>A Time and Space Efficient Junction Tree Architecture</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The junction tree algorithm is a way of computing marginals of boolean
multivariate probability distributions that factorise over sets of random
variables. The junction tree algorithm first constructs a tree called a
junction tree who's vertices are sets of random variables. The algorithm then
performs a generalised version of belief propagation on the junction tree. The
Shafer-Shenoy and Hugin architectures are two ways to perform this belief
propagation that tradeoff time and space complexities in different ways: Hugin
propagation is at least as fast as Shafer-Shenoy propagation and in the cases
that we have large vertices of high degree is significantly faster. However,
this speed increase comes at the cost of an increased space complexity. This
paper first introduces a simple novel architecture, ARCH-1, which has the best
of both worlds: the speed of Hugin propagation and the low space requirements
of Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, is
then introduced which has, up to a factor only linear in the maximum
cardinality of any vertex, time and space complexities at least as good as
ARCH-1 and in the cases that we have large vertices of high degree is
significantly faster than ARCH-1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0189</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0189</id><created>2013-08-01</created><updated>2015-03-04</updated><authors><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Asymptotically near-optimal RRT for fast, high-quality, motion planning</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Lower Bound Tree-RRT (LBT-RRT), a single-query sampling-based
algorithm that is asymptotically near-optimal. Namely, the solution extracted
from LBT-RRT converges to a solution that is within an approximation factor of
1+epsilon of the optimal solution. Our algorithm allows for a continuous
interpolation between the fast RRT algorithm and the asymptotically optimal
RRT* and RRG algorithms. When the approximation factor is 1 (i.e., no
approximation is allowed), LBT-RRT behaves like RRG. When the approximation
factor is unbounded, LBT-RRT behaves like RRT. In between, LBT-RRT is shown to
produce paths that have higher quality than RRT would produce and run faster
than RRT* would run. This is done by maintaining a tree which is a sub-graph of
the RRG roadmap and a second, auxiliary graph, which we call the lower-bound
graph. The combination of the two roadmaps, which is faster to maintain than
the roadmap maintained by RRT*, efficiently guarantees asymptotic
near-optimality. We suggest to use LBT-RRT for high-quality, anytime motion
planning. We demonstrate the performance of the algorithm for scenarios ranging
from 3 to 12 degrees of freedom and show that even for small approximation
factors, the algorithm produces high-quality solutions (comparable to RRG and
RRT*) with little running-time overhead when compared to RRT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0209</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0209</id><created>2013-07-30</created><authors><author><keyname>Al-Akhras</keyname><forenames>Salim Ismail</forenames><affiliation>Concordia University</affiliation></author><author><keyname>Tahar</keyname><forenames>Sofi&#xe8;ne</forenames><affiliation>Concordia University</affiliation></author><author><keyname>Nicolescu</keyname><forenames>Gabriela</forenames><affiliation>Ecole Polytechnique de Montreal</affiliation></author><author><keyname>Langevin</keyname><forenames>Michel</forenames><affiliation>STMicroelectronics Inc.</affiliation></author><author><keyname>Paulin</keyname><forenames>Pierre</forenames><affiliation>STMicroelectronics Inc.</affiliation></author></authors><title>On the Verification of a WiMax Design Using Symbolic Simulation</title><categories>cs.LO</categories><comments>In Proceedings SCSS 2012, arXiv:1307.8029</comments><proxy>EPTCS</proxy><acm-class>B.4.4; B.4.5; B.5.2</acm-class><journal-ref>EPTCS 122, 2013, pp. 23-37</journal-ref><doi>10.4204/EPTCS.122.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In top-down multi-level design methodologies, design descriptions at higher
levels of abstraction are incrementally refined to the final realizations.
Simulation based techniques have traditionally been used to verify that such
model refinements do not change the design functionality. Unfortunately, with
computer simulations it is not possible to completely check that a design
transformation is correct in a reasonable amount of time, as the number of test
patterns required to do so increase exponentially with the number of system
state variables. In this paper, we propose a methodology for the verification
of conformance of models generated at higher levels of abstraction in the
design process to the design specifications. We model the system behavior using
sequence of recurrence equations. We then use symbolic simulation together with
equivalence checking and property checking techniques for design verification.
Using our proposed method, we have verified the equivalence of three WiMax
system models at different levels of design abstraction, and the correctness of
various system properties on those models. Our symbolic modeling and
verification experiments show that the proposed verification methodology
provides performance advantage over its numerical counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0211</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0211</id><created>2013-08-01</created><updated>2013-08-17</updated><authors><author><keyname>Shayda</keyname><forenames>Dara O</forenames></author></authors><title>On Kolmogorov Complexity of Random Very Long Braided Words</title><categories>cs.CC</categories><comments>Additional code for v2 included in the references. Multi-pass
  experiments included and some test code to make sure the code is correct. The
  Many-Pass reductions are shown to have Gamma/Poisson distribution which has
  been a great excitement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any positive word comprised of random sequence of tokens form a finite
alphabet can be reduced (without change of length) using an appropriate size
Braid group relationships. Surprisingly the Braid relations dramatically reduce
the Kolmogorov Complexity of the original random word and do so in distinct
bands of (rate of change) values with gaps in between. Distribution of these
bands are estimated and empirical statistics collected by actually coding
approximations to the Kolmogorov Complexity (in Mathematica 9.0).
Lempel-Ziv-Welch lossless compression algorithm techniques used to estimate the
distribution for gaped bands. Evidence provided that such distributions of
reduction in Kolmogorov Complexity based upon Braid groups are universal i.e.
they can model more general algebraic structures other than Braid groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0219</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0219</id><created>2013-08-01</created><updated>2014-11-11</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Instruction sequence expressions for the secure hash algorithm SHA-256</title><categories>cs.PL cs.CR</categories><comments>14 pages; several minor errors corrected; counting error corrected;
  instruction sequence fault repaired; misunderstanding cleared up; a minor
  error corrected. arXiv admin note: text overlap with arXiv:1301.3297</comments><acm-class>E.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secure hash function SHA-256 is a function on bit strings. This means
that its restriction to the bit strings of any given length can be computed by
a finite instruction sequence that contains only instructions to set and get
the content of Boolean registers, forward jump instructions, and a termination
instruction. We describe such instruction sequences for the restrictions to bit
strings of the different possible lengths by means of uniform terms from an
algebraic theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0223</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0223</id><created>2013-08-01</created><authors><author><keyname>Anouari</keyname><forenames>Tarik</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>Performance Analysis of VoIP Traffic in WiMAX using various Service
  Classes</title><categories>cs.NI</categories><journal-ref>International.Journal.Computer.Applications. 52-20 (2012) 975-8887</journal-ref><doi>10.5120/8319-1956</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Worldwide Interoperability for Microwave Access (WiMAX) is currently one of
the hottest technologies in wireless, it is a standard-based on the IEEE 802.16
wireless technology that provides high throughput broadband connections over
long distance, which supports Point to MultiPoint (PMP) broadband wireless
access. In parallel, Voice over Internet Protocol is a promising new technology
which provides access to voice communication over internet protocol based
network, it becomes an alternative to public switched telephone networks due to
its capability of transmission of voice as packets over IP networks. Therefore
VoIP is largely intolerant of delay and hence it needs a high priority
transmission. In this paper we investigate the performances of the most common
VoIP codecs, which are G711, G7231 and G729 over a WiMAX network using various
service classes and NOAH as a transport protocol. To analyze the QoS
parameters, the popular network simulator ns2 was used. Various parameters that
determine QoS of real life usage scenarios and traffic flows of applications is
analyzed. The objective is to compare different types of service classes with
respect to the QoS parameters, such as, throughput, average jitter and average
delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0227</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0227</id><created>2013-08-01</created><updated>2014-04-01</updated><authors><author><keyname>Amadini</keyname><forenames>Roberto</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author></authors><title>An Enhanced Features Extractor for a Portfolio of Constraint Solvers</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that a single arbitrarily efficient solver can be
significantly outperformed by a portfolio of possibly slower on-average
solvers. The solver selection is usually done by means of (un)supervised
learning techniques which exploit features extracted from the problem
specification. In this paper we present an useful and flexible framework that
is able to extract an extensive set of features from a Constraint
(Satisfaction/Optimization) Problem defined in possibly different modeling
languages: MiniZinc, FlatZinc or XCSP. We also report some empirical results
showing that the performances that can be obtained using these features are
effective and competitive with state of the art CSP portfolio techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0237</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0237</id><created>2013-08-01</created><authors><author><keyname>Margetts</keyname><forenames>Helen Z.</forenames></author><author><keyname>John</keyname><forenames>Peter</forenames></author><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Reissfelder</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Leadership without Leaders? Starters and Followers in Online Collective
  Action</title><categories>cs.CY cs.HC</categories><doi>10.1111/1467-9248.12075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has been ascribed a prominent role in collective action,
particularly with widespread use of social media. But most mobilisations fail.
We investigate the characteristics of those few mobilisations that succeed and
hypothesise that the presence of 'starters' with low thresholds for joining
will determine whether a mobilisation achieves success, as suggested by
threshold models. We use experimental data from public good games to identify
personality types associated with willingness to start in collective action. We
find a significant association between both extraversion and internal locus of
control, and willingness to start, while agreeableness is associated with a
tendency to follow. Rounds without at least a minimum level of extraversion
among the participants are unlikely to be funded, providing some support for
the hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0239</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0239</id><created>2013-08-01</created><updated>2014-08-14</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Margetts</keyname><forenames>Helen</forenames></author></authors><title>Modeling the Rise in Internet-based Petitions</title><categories>physics.soc-ph cs.CY cs.HC cs.SI physics.data-an</categories><comments>Submitted to EPJ Data Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary collective action, much of which involves social media and other
Internet-based platforms, leaves a digital imprint which may be harvested to
better understand the dynamics of mobilization. Petition signing is an example
of collective action which has gained in popularity with rising use of social
media and provides such data for the whole population of petition signatories
for a given platform. This paper tracks the growth curves of all 20,000
petitions to the UK government over 18 months, analyzing the rate of growth and
outreach mechanism. Previous research has suggested the importance of the first
day to the ultimate success of a petition, but has not examined early growth
within that day, made possible here through hourly resolution in the data. The
analysis shows that the vast majority of petitions do not achieve any measure
of success; over 99 percent fail to get the 10,000 signatures required for an
official response and only 0.1 percent attain the 100,000 required for a
parliamentary debate. We analyze the data through a multiplicative process
model framework to explain the heterogeneous growth of signatures at the
population level. We define and measure an average outreach factor for
petitions and show that it decays very fast (reducing to 0.1% after 10 hours).
After 24 hours, a petition's fate is virtually set. The findings seem to
challenge conventional analyses of collective action from economics and
political science, where the production function has been assumed to follow an
S-shaped curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0247</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0247</id><created>2013-08-01</created><updated>2014-06-02</updated><authors><author><keyname>Eguchi</keyname><forenames>Naohi</forenames></author></authors><title>Predicative Lexicographic Path Orders: An Application of Term Rewriting
  to the Region of Primitive Recursive Functions</title><categories>math.LO cs.LO</categories><comments>Technical Report</comments><acm-class>F.4.1; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel termination order the {\em predicative
lexicographic path order} (PLPO for short), a syntactic restriction of the
lexicographic path order. As well as lexicographic path orders, several
non-trivial primitive recursive equations, e.g., primitive recursion with
parameter substitution, unnested multiple recursion, or simple nested
recursion, can be oriented with PLPOs. It can be shown that the PLPO however
only induces primitive recursive upper bounds on derivation lengths of
compatible rewrite systems. This yields an alternative proof of a classical
fact that the class of primitive recursive functions is closed under those
non-trivial primitive recursive equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0256</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0256</id><created>2013-08-01</created><authors><author><keyname>Paul</keyname><forenames>Norbert</forenames></author></authors><title>Applications of continuous functions in topological CAD data</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most CAD or other spatial data models, in particular boundary representation
models, are called &quot;topological&quot; and represent spatial data by a structured
collection of &quot;topological primitives&quot; like edges, vertices, faces, and
volumes. These then represent spatial objects in geo-information- (GIS) or CAD
systems or in building information models (BIM). Volume objects may then either
be represented by their 2D boundary or by a dedicated 3D-element, the &quot;solid&quot;.
The latter may share common boundary elements with other solids, just as
2D-polygon topologies in GIS share common boundary edges. Despite the frequent
reference to &quot;topology&quot; in publications on spatial modelling the formal link
between mathematical topology and these &quot;topological&quot; models is hardly
described in the literature. Such link, for example, cannot be established by
the often cited nine-intersections model which is too elementary for that
purpose. Mathematically, the link between spatial data and the modelled &quot;real
world&quot; entities is established by a chain of &quot;continuous functions&quot; - a very
important topological notion, yet often overlooked by spatial data modellers.
This article investigates how spatial data can actually be considered
topological spaces, how continuous functions between them are defined, and how
CAD systems can make use of them. Having found examples of applications of
continuity in CAD data models it turns out that of continuity has much
practical relevance for CAD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0267</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0267</id><created>2013-08-01</created><updated>2013-09-22</updated><authors><author><keyname>Sin'ya</keyname><forenames>Ryoma</forenames></author></authors><title>Text Compression using Abstract Numeration System on a Regular Language</title><categories>cs.FL</categories><comments>An extended abstract of the accepted paper for JSSST Journal
  &quot;Computer Software&quot; (Japanese, available at
  http://www.shudo.is.titech.ac.jp/members/sinya)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An abstract numeration system (ANS) is a numeration system that provides a
one-to-one correspondence between the natural numbers and a regular language.
In this paper, we define an ANS-based compression as an extension of this
correspondence. In addition, we show the following results: 1) an average
compression ratio is computable from a language, 2) an ANS-based compression
runs in sublinear time with respect to the length of the input string, and 3)
an ANS-based compression can be extended to block-based compression using a
factorial language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0268</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0268</id><created>2013-07-31</created><authors><author><keyname>Ravara</keyname><forenames>Ant&#xf3;nio</forenames><affiliation>New University of Lisbon, Portugal</affiliation></author><author><keyname>Silva</keyname><forenames>Josep</forenames><affiliation>Universidad Polit&#xe9;cnica de Valencia</affiliation></author></authors><title>Proceedings 9th International Workshop on Automated Specification and
  Verification of Web Systems</title><categories>cs.LO cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 123, 2013</journal-ref><doi>10.4204/EPTCS.123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the accepted papers of the 9th International Workshop on
Automated Specification and Verification of Web Systems (WWV'13), which took
place in Florence, Italy, on June 6, as a satellite event of the 8th
International Federated Conferences on Distributed Computing Techniques
(DisCoTec 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0271</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0271</id><created>2013-08-01</created><updated>2015-09-12</updated><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Compositional Dictionaries for Domain Adaptive Face Recognition</title><categories>cs.CV</categories><comments>Transactions on Image Processing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dictionary learning approach to compensate for the
transformation of faces due to changes in view point, illumination, resolution,
etc. The key idea of our approach is to force domain-invariant sparse coding,
i.e., design a consistent sparse representation of the same face in different
domains. In this way, classifiers trained on the sparse codes in the source
domain consisting of frontal faces for example can be applied to the target
domain (consisting of faces in different poses, illumination conditions, etc)
without much loss in recognition accuracy. The approach is to first learn a
domain base dictionary, and then describe each domain shift (identity, pose,
illumination) using a sparse representation over the base dictionary. The
dictionary adapted to each domain is expressed as sparse linear combinations of
the base dictionary. In the context of face recognition, with the proposed
compositional dictionary approach, a face image can be decomposed into sparse
representations for a given subject, pose and illumination respectively. This
approach has three advantages: first, the extracted sparse representation for a
subject is consistent across domains and enables pose and illumination
insensitive face recognition. Second, sparse representations for pose and
illumination can subsequently be used to estimate the pose and illumination
condition of a face image. Finally, by composing sparse representations for
subject and the different domains, we can also perform pose alignment and
illumination normalization. Extensive experiments using two public face
datasets are presented to demonstrate the effectiveness of our approach for
face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0273</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0273</id><created>2013-08-01</created><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Learning Robust Subspace Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low-rank transformation-learning framework to robustify subspace
clustering. Many high-dimensional data, such as face images and motion
sequences, lie in a union of low-dimensional subspaces. The subspace clustering
problem has been extensively studied in the literature to partition such
high-dimensional data into clusters corresponding to their underlying
low-dimensional subspaces. However, low-dimensional intrinsic structures are
often violated for real-world observations, as they can be corrupted by errors
or deviate from ideal models. We propose to address this by learning a linear
transformation on subspaces using matrix rank, via its convex surrogate nuclear
norm, as the optimization criteria. The learned linear transformation restores
a low-rank structure for data from the same subspace, and, at the same time,
forces a high-rank structure for data from different subspaces. In this way, we
reduce variations within the subspaces, and increase separations between the
subspaces for more accurate subspace clustering. This proposed learned robust
subspace clustering framework significantly enhances the performance of
existing subspace clustering methods. To exploit the low-rank structures of the
transformed subspaces, we further introduce a subspace clustering technique,
called Robust Sparse Subspace Clustering, which efficiently combines robust PCA
with sparse modeling. We also discuss the online learning of the
transformation, and learning of the transformation while simultaneously
reducing the data dimensionality. Extensive experiments using public datasets
are presented, showing that the proposed approach significantly outperforms
state-of-the-art subspace clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0275</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0275</id><created>2013-08-01</created><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Chen</keyname><forenames>Ching-Hui</forenames></author></authors><title>Domain-invariant Face Recognition using Learned Low-rank Transformation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a low-rank transformation approach to compensate for face
variations due to changes in visual domains, such as pose and illumination. The
key idea is to learn discriminative linear transformations for face images
using matrix rank as the optimization criteria. The learned linear
transformations restore a shared low-rank structure for faces from the same
subject, and, at the same time, force a high-rank structure for faces from
different subjects. In this way, among the transformed faces, we reduce
variations caused by domain changes within the classes, and increase
separations between the classes for better face recognition across domains.
Extensive experiments using public datasets are presented to demonstrate the
effectiveness of our approach for face recognition across domains. The
potential of the approach for feature extraction in generic object recognition
and coded aperture design are discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0290</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0290</id><created>2013-08-01</created><authors><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Jiang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Sparse Dictionary-based Attributes for Action Recognition and
  Summarization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for dictionary learning of action attributes via
information maximization. We unify the class distribution and appearance
information into an objective function for learning a sparse dictionary of
action attributes. The objective function maximizes the mutual information
between what has been learned and what remains to be learned in terms of
appearance information and class distribution for each dictionary atom. We
propose a Gaussian Process (GP) model for sparse representation to optimize the
dictionary objective function. The sparse coding property allows a kernel with
compact support in GP to realize a very efficient dictionary learning process.
Hence we can describe an action video by a set of compact and discriminative
action attributes. More importantly, we can recognize modeled action categories
in a sparse feature space, which can be generalized to unseen and unmodeled
action categories. Experimental results demonstrate the effectiveness of our
approach in action recognition and summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0299</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0299</id><created>2013-08-01</created><authors><author><keyname>Borba</keyname><forenames>Leonardo</forenames></author><author><keyname>Ritt</keyname><forenames>Marcus</forenames></author></authors><title>Exact and Heuristic Methods for the Assembly Line Worker Assignment and
  Balancing Problem</title><categories>cs.AI cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traditional assembly lines, it is reasonable to assume that task execution
times are the same for each worker. However, in sheltered work centres for
disabled this assumption is not valid: some workers may execute some tasks
considerably slower or even be incapable of executing them. Worker
heterogeneity leads to a problem called the assembly line worker assignment and
balancing problem (ALWABP). For a fixed number of workers the problem is to
maximize the production rate of an assembly line by assigning workers to
stations and tasks to workers, while satisfying precedence constraints between
the tasks. This paper introduces new heuristic and exact methods to solve this
problem. We present a new MIP model, propose a novel heuristic algorithm based
on beam search, as well as a task-oriented branch-and-bound procedure which
uses new reduction rules and lower bounds for solving the problem. Extensive
computational tests on a large set of instances show that these methods are
effective and improve over existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0309</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0309</id><created>2013-08-01</created><updated>2014-11-04</updated><authors><author><keyname>Grabowicz</keyname><forenames>Przemyslaw A.</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Fast filtering and animation of large dynamic networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>6 figures, 2 tables</comments><journal-ref>EPJ Data Science, Volume 3, Issue 1, 2014</journal-ref><doi>10.1140/epjds/s13688-014-0027-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and visualizing what are the most relevant changes in an evolving
network is an open challenge in several domains. We present a fast algorithm
that filters subsets of the strongest nodes and edges representing an evolving
weighted graph and visualize it by either creating a movie, or by streaming it
to an interactive network visualization tool. The algorithm is an approximation
of exponential sliding time-window that scales linearly with the number of
interactions. We compare the algorithm against rectangular and exponential
sliding time-window methods. Our network filtering algorithm: i) captures
persistent trends in the structure of dynamic weighted networks, ii) smoothens
transitions between the snapshots of dynamic network, and iii) uses limited
memory and processor time. The algorithm is publicly available as open-source
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0311</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0311</id><created>2013-07-31</created><updated>2013-12-13</updated><authors><author><keyname>Bagdasaryan</keyname><forenames>Armen</forenames></author></authors><title>On the partition of R^n by hyperplanes</title><categories>cs.DM math.CO</categories><comments>This paper has been withdrawn by the author because, as the author
  became aware, some results are not new</comments><msc-class>52C35, 51M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The partitioning of space by hyperplanes in the context of discrete
classification problem is considered. We obtain some relations for the number
of partitions and establish a recurrence relation for the maximal number of
partitions of R^n by m hyperplanes. We rederive an explicit formula for the
number of components into which the space can be partitioned by m hyperplanes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0315</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0315</id><created>2013-08-01</created><authors><author><keyname>Chakroun</keyname><forenames>Mohamed</forenames></author><author><keyname>Wali</keyname><forenames>Ali</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>MAS for video objects segmentation and tracking based on active contours
  and SURF descriptor</title><categories>cs.MM cs.CV</categories><comments>6 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 2, No 3, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, video segmentation and tracking is an important
challenging issue. In this paper, we describe a new video sequences
segmentation and tracking algorithm based on MAS &quot;multi-agent systems&quot; and SURF
&quot;Speeded Up Robust Features&quot;. Our approach consists in modelling a multi-agent
system for segmenting the first image from a video sequence and tracking
objects in the video sequences. The used agents are supervisor and explorator
agents, they are communicating between them and they inspire in their behavior
from active contours approaches. The tracking of objects is based on SURF
descriptors &quot;Speed Up Robust Features&quot;. We used the DIMA platform and &quot;API
Ateji PX&quot; (an extension of the Java language to facilitate parallel programming
on heterogeneous architectures) to implement this algorithm. The experimental
results indicate that the proposed algorithm is more robust and faster than
previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0322</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0322</id><created>2013-08-01</created><authors><author><keyname>Gekas</keyname><forenames>John</forenames></author></authors><title>Social Data Mining through Distributed Mobile Sensing</title><categories>cs.HC cs.SI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a distributed framework for collecting and
analyzing environmental and location data recorded by human users (carriers)
with the use of portable sensors. We demonstrate the data mining analysis
potential among the recorded environmental and location variables, as well as
the potential for classification analysis of human activities. We recognize
that the success of such an experimental framework relies on the adoption rate
by its candidate user network; thus, we have built our experimental prototype
on top of hardware equipment already embedded within the potential users'
everyday routine - i.e. hardware sensors installed on modern mobile phones.
Finally, we present preliminary analysis results on our collected data sample,
as well as potential further work directions and proposed use case scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0356</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0356</id><created>2013-08-01</created><authors><author><keyname>Fekri-Ershad</keyname><forenames>Shervan</forenames></author><author><keyname>Tajalizadeh</keyname><forenames>Hadi</forenames></author><author><keyname>Jafari</keyname><forenames>Shahram</forenames></author></authors><title>Design and Development of an Expert System to Help Head of University
  Departments</title><categories>cs.AI cs.LG</categories><comments>4 pages, 2 figures, 2 tables</comments><journal-ref>International Journal of Science and Modern Engineering (IJISME),
  ISSN: 2319-6386, Volume-1, Issue-2, January 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the basic tasks which is responded for head of each university
department, is employing lecturers based on some default factors such as
experience, evidences, qualifies and etc. In this respect, to help the heads,
some automatic systems have been proposed until now using machine learning
methods, decision support systems (DSS) and etc. According to advantages and
disadvantages of the previous methods, a full automatic system is designed in
this paper using expert systems. The proposed system is included two main
steps. In the first one, the human expert's knowledge is designed as decision
trees. The second step is included an expert system which is evaluated using
extracted rules of these decision trees. Also, to improve the quality of the
proposed system, a majority voting algorithm is proposed as post processing
step to choose the best lecturer which satisfied more expert's decision trees
for each course. The results are shown that the designed system average
accuracy is 78.88. Low computational complexity, simplicity to program and are
some of other advantages of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0365</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0365</id><created>2013-08-01</created><authors><author><keyname>Aldea</keyname><forenames>Emanuel</forenames></author><author><keyname>Kiyani</keyname><forenames>Khurom H.</forenames></author></authors><title>Hybrid Focal Stereo Networks for Pattern Analysis in Homogeneous Scenes</title><categories>cs.CV</categories><comments>13 pages, 6 figures, submitted to Machine Vision and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of multiple camera calibration in the
presence of a homogeneous scene, and without the possibility of employing
calibration object based methods. The proposed solution exploits salient
features present in a larger field of view, but instead of employing active
vision we replace the cameras with stereo rigs featuring a long focal analysis
camera, as well as a short focal registration camera. Thus, we are able to
propose an accurate solution which does not require intrinsic variation models
as in the case of zooming cameras. Moreover, the availability of the two views
simultaneously in each rig allows for pose re-estimation between rigs as often
as necessary. The algorithm has been successfully validated in an indoor
setting, as well as on a difficult scene featuring a highly dense pilgrim crowd
in Makkah.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0371</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0371</id><created>2013-08-01</created><updated>2013-12-01</updated><authors><author><keyname>Graham</keyname><forenames>Benjamin</forenames></author></authors><title>Sparse arrays of signatures for online character recognition</title><categories>cs.CV cs.NE</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mathematics the signature of a path is a collection of iterated integrals,
commonly used for solving differential equations. We show that the path
signature, used as a set of features for consumption by a convolutional neural
network (CNN), improves the accuracy of online character recognition---that is
the task of reading characters represented as a collection of paths. Using
datasets of letters, numbers, Assamese and Chinese characters, we show that the
first, second, and even the third iterated integrals contain useful information
for consumption by a CNN.
  On the CASIA-OLHWDB1.1 3755 Chinese character dataset, our approach gave a
test error of 3.58%, compared with 5.61% for a traditional CNN [Ciresan et
al.]. A CNN trained on the CASIA-OLHWDB1.0-1.2 datasets won the ICDAR2013
Online Isolated Chinese Character recognition competition.
  Computationally, we have developed a sparse CNN implementation that make it
practical to train CNNs with many layers of max-pooling. Extending the MNIST
dataset by translations, our sparse CNN gets a test error of 0.31%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0372</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0372</id><created>2013-08-01</created><authors><author><keyname>Al-Ameen</keyname><forenames>Mahdi Nasrullah</forenames></author></authors><title>An Intelligent Fire Alert System using Wireless Mobile Communication</title><categories>cs.HC</categories><comments>10 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The system has come to light through the way of inspiration to develop a
compact system, based on the fundamental ideas of safety, security and control.
Once this system is installed to operation specifying temperature and smoke
threshold, in case of any emergency situation due to increasing temperature
and/or smoke at place surpassing the threshold, the system immediately sends
automatic alert-notifications to the users, concerned with the situations. The
user gets total control over the system through mobile SMS, even from the
distant location, that to change the threshold, turn on/off the feature of
sending 'alert notification' and also to reset the system after the emergency
situation is overcome. Before executing any command (through SMS) from the
user, the system asks for the preset password to verify an authorized user. The
security issues have been considered with utter attention in this system to
ensure its applicability in industries and business organizations, where
security is an important concern. Hence, the fundamental ideas of safety,
security and control have been entirely ensured through the system, which have
definitely worked as the gear moving factor to look for a new dimension of an
'Intelligent Fire Alert System'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0375</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0375</id><created>2013-08-01</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Zhao</keyname><forenames>Xin</forenames></author></authors><title>A New 3D Geometric Approach to Focus and Context Lens Effect Simulation</title><categories>cs.GR</categories><comments>Poster for I3D</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel methodology based on geometric approach to simulate
magnification lens effects. Our aim is to promote new applications of powerful
geometric modeling techniques in visual computing. Conventional image
processing/visualization methods are computed in two dimensional space (2D). We
examine this conventional 2D manipulation from a completely innovative
perspective of 3D geometric processing. Compared with conventional optical lens
design, 3D geometric method are much more capable of preserving shape features
and minimizing distortion. We magnify an area of interest to better visualize
the interior details, while keeping the rest of area without perceivable
distortion. We flatten the mesh back into 2D space for viewing, and further
applications in the screen space. In both steps, we devise an iterative
deformation scheme to minimize distortion around both focus and context region,
while avoiding the noncontinuous transition region between the focus and
context areas. Particularly, our method allows the user to flexibly modify the
ROI shapes to accommodate complex feature. The user can also easily specify a
spectrum of metrics for different visual effects. Various experimental results
demonstrate the effectiveness, robustness, and efficiency of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0376</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0376</id><created>2013-08-01</created><authors><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Oikawa</keyname><forenames>Minoru</forenames></author><author><keyname>Takada</keyname><forenames>Naoki</forenames></author><author><keyname>Okada</keyname><forenames>Naohisa</forenames></author><author><keyname>Endo</keyname><forenames>Yutaka</forenames></author><author><keyname>Hirayama</keyname><forenames>Ryuji</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyoshi</forenames></author></authors><title>Calculation reduction method for color computer-generated hologram using
  color space conversion</title><categories>physics.optics cs.GR</categories><doi>10.1117/1.OE.53.2.024108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a calculation reduction method for color computer-generated
holograms (CGHs) using color space conversion. Color CGHs are generally
calculated on RGB space. In this paper, we calculate color CGHs in other color
spaces: for example, YCbCr color space. In YCbCr color space, a RGB image is
converted to the luminance component (Y), blue-difference chroma (Cb) and
red-difference chroma (Cr) components. In terms of the human eye, although the
negligible difference of the luminance component is well-recognized, the
difference of the other components is not. In this method, the luminance
component is normal sampled and the chroma components are down-sampled. The
down-sampling allows us to accelerate the calculation of the color CGHs. We
compute diffraction calculations from the components, and then we convert the
diffracted results in YCbCr color space to RGB color space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0384</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0384</id><created>2013-08-01</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Martin</keyname><forenames>Clyde F.</forenames></author></authors><title>L1-Optimal Splines for Outlier Rejection</title><categories>cs.SY cs.IT math.IT math.OC stat.AP</categories><comments>Submitted to the 59th World Statistics Congress (WSC), Aug. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider control theoretic splines with L1 optimization
for rejecting outliers in data. Control theoretic splines are either
interpolating or smoothing splines, depending on a cost function with a
constraint defined by linear differential equations. Control theoretic splines
are effective for Gaussian noise in data since the estimation is based on L2
optimization. However, in practice, there may be outliers in data, which may
occur with vanishingly small probability under the Gaussian assumption of
noise, to which L2-optimized spline regression may be very sensitive. To
achieve robustness against outliers, we propose to use L1 optimality, which is
also used in support vector regression. A numerical example shows the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0385</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0385</id><created>2013-08-01</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Optimal Discretization of Analog Filters via Sampled-Data H-infinity
  Control Theory</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>submitted to the 2013 IEEE Multi-Conference on Systems and Control
  (MSC 2013), Aug. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose optimal discretization of analog filters (or
controllers) based on the theory of sampled-data H-infinity control. We
formulate the discretization problem as minimization of the H-infinity norm of
the error system between a (delayed) target analog filter and a digital system
including an ideal sampler, a zero-order hold, and a digital filter. The
problem is reduced to discrete-time H-infinity optimization via the fast
sample/hold approximation method. We also extend the proposed method to
multirate systems. Feedback controller discretization by the proposed method is
discussed with respect to stability. Numerical examples show the effectiveness
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0388</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0388</id><created>2013-08-01</created><authors><author><keyname>Cesari</keyname><forenames>Luca</forenames><affiliation>Universit&#xe0; di Pisa / Universit&#xe0; degli Studi di Firenze, Italy</affiliation></author><author><keyname>Pugliese</keyname><forenames>Rosario</forenames><affiliation>Universit&#xe0; degli Studi di Firenze, Italy</affiliation></author><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames><affiliation>IMT Advanced Studies Lucca, Italy</affiliation></author></authors><title>Blind-date Conversation Joining</title><categories>cs.SE cs.PL</categories><comments>In Proceedings WWV 2013, arXiv:1308.0268</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 123, 2013, pp. 3-18</journal-ref><doi>10.4204/EPTCS.123.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on a form of joining conversations among multiple parties in
service-oriented applications where a client may asynchronously join an
existing conversation without need to know in advance any information about it.
More specifically, we show how the correlation mechanism provided by
orchestration languages enables a form of conversation joining that is
completely transparent to clients and that we call 'blind-date joining'. We
provide an implementation of this strategy by using the standard orchestration
language WS-BPEL. We then present its formal semantics by resorting to COWS, a
process calculus specifically designed for modelling service-oriented
applications. We illustrate our approach by means of a simple, but realistic,
case study from the online games domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0389</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0389</id><created>2013-08-01</created><authors><author><keyname>Ciobanu</keyname><forenames>Gabriel</forenames><affiliation>Romanian Academy, Iasi</affiliation></author><author><keyname>Horne</keyname><forenames>Ross</forenames><affiliation>Romanian Academy, Iasi</affiliation></author><author><keyname>Sassone</keyname><forenames>Vladimiro</forenames><affiliation>University of Southampton</affiliation></author></authors><title>Local Type Checking for Linked Data Consumers</title><categories>cs.PL cs.DB</categories><comments>In Proceedings WWV 2013, arXiv:1308.0268</comments><proxy>EPTCS</proxy><acm-class>D.3.3</acm-class><journal-ref>EPTCS 123, 2013, pp. 19-33</journal-ref><doi>10.4204/EPTCS.123.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web of Linked Data is the cumulation of over a decade of work by the Web
standards community in their effort to make data more Web-like. We provide an
introduction to the Web of Linked Data from the perspective of a Web developer
that would like to build an application using Linked Data. We identify a
weakness in the development stack as being a lack of domain specific scripting
languages for designing background processes that consume Linked Data. To
address this weakness, we design a scripting language with a simple but
appropriate type system. In our proposed architecture some data is consumed
from sources outside of the control of the system and some data is held
locally. Stronger type assumptions can be made about the local data than
external data, hence our type system mixes static and dynamic typing.
Throughout, we relate our work to the W3C recommendations that drive Linked
Data, so our syntax is accessible to Web developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0390</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0390</id><created>2013-08-01</created><authors><author><keyname>Lanese</keyname><forenames>Ivan</forenames><affiliation>Focus Team, University of Bologna/INRIA, Italy</affiliation></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames><affiliation>IT University of Copenhagen, Denmark</affiliation></author><author><keyname>Zavattaro</keyname><forenames>Gianluigi</forenames><affiliation>Focus Team, University of Bologna/INRIA, Italy</affiliation></author></authors><title>Amending Choreographies</title><categories>cs.PL cs.DC cs.LO</categories><comments>In Proceedings WWV 2013, arXiv:1308.0268</comments><proxy>EPTCS</proxy><acm-class>D.1.3; D.3.3; F.3.2</acm-class><journal-ref>EPTCS 123, 2013, pp. 34-48</journal-ref><doi>10.4204/EPTCS.123.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreographies are global descriptions of system behaviors, from which the
local behavior of each endpoint entity can be obtained automatically through
projection. To guarantee that its projection is correct, i.e. it has the same
behaviors of the original choreography, a choreography usually has to respect
some coherency conditions. This restricts the set of choreographies that can be
projected.
  In this paper, we present a transformation for amending choreographies that
do not respect common syntactic conditions for projection correctness.
Specifically, our transformation automatically reduces the amount of
concurrency, and it infers and adds hidden communications that make the
resulting choreography respect the desired conditions, while preserving its
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0391</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0391</id><created>2013-08-01</created><authors><author><keyname>Smith</keyname><forenames>James</forenames></author></authors><title>Proving Properties of Rich Internet Applications</title><categories>cs.NI cs.LO</categories><comments>In Proceedings WWV 2013, arXiv:1308.0268</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 123, 2013, pp. 49-63</journal-ref><doi>10.4204/EPTCS.123.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce application layer specifications, which allow us to reason about
the state and transactions of rich Internet applications. We define variants of
the state/event based logic UCTL* along with two example applications to
demonstrate this approach, and then look at a distributed, rich Internet
application, proving properties about the information it stores and
disseminates. Our approach enables us to justify proofs about abstract
properties that are preserved in the face of concurrent, networked inputs by
proofs about concrete properties in an Internet setting. We conclude that our
approach makes it possible to reason about the programs and protocols that
comprise the Internet's application layer with reliability and generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0403</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0403</id><created>2013-08-02</created><updated>2014-04-02</updated><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Cheng</keyname><forenames>Zhanpeng</forenames></author><author><keyname>Devanny</keyname><forenames>William E.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Superpatterns and Universal Point Sets</title><categories>cs.CG math.CO</categories><comments>GD 2013 special issue of JGAA</comments><journal-ref>J. Graph Algorithms &amp; Applications 18(2): 177-209, 2014</journal-ref><doi>10.7155/jgaa.00318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An old open problem in graph drawing asks for the size of a universal point
set, a set of points that can be used as vertices for straight-line drawings of
all n-vertex planar graphs. We connect this problem to the theory of
permutation patterns, where another open problem concerns the size of
superpatterns, permutations that contain all patterns of a given size. We
generalize superpatterns to classes of permutations determined by forbidden
patterns, and we construct superpatterns of size n^2/4 + Theta(n) for the
213-avoiding permutations, half the size of known superpatterns for
unconstrained permutations. We use our superpatterns to construct universal
point sets of size n^2/4 - Theta(n), smaller than the previous bound by a 9/16
factor. We prove that every proper subclass of the 213-avoiding permutations
has superpatterns of size O(n log^O(1) n), which we use to prove that the
planar graphs of bounded pathwidth have near-linear universal point sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0419</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0419</id><created>2013-08-02</created><updated>2014-05-08</updated><authors><author><keyname>Wu</keyname><forenames>Fuzhang</forenames></author><author><keyname>Yan</keyname><forenames>Dong-Ming</forenames></author><author><keyname>Dong</keyname><forenames>Weiming</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author></authors><title>Inverse Procedural Modeling of Facade Layouts</title><categories>cs.GR</categories><comments>10</comments><acm-class>I.3.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the following research problem: How can we generate
a meaningful split grammar that explains a given facade layout? To evaluate if
a grammar is meaningful, we propose a cost function based on the description
length and minimize this cost using an approximate dynamic programming
framework. Our evaluation indicates that our framework extracts meaningful
split grammars that are competitive with those of expert users, while some
users and all competing automatic solutions are less successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0428</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0428</id><created>2013-08-02</created><authors><author><keyname>Hetzl</keyname><forenames>Stefan</forenames></author><author><keyname>Weller</keyname><forenames>Daniel</forenames></author></authors><title>Expansion Trees with Cut</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herbrand's theorem is one of the most fundamental insights in logic. From the
syntactic point of view it suggests a compact representation of proofs in
classical first- and higher-order logic by recording the information which
instances have been chosen for which quantifiers, known in the literature as
expansion trees.
  Such a representation is inherently analytic and hence corresponds to a
cut-free sequent calculus proof. Recently several extensions of such proof
representations to proofs with cut have been proposed. These extensions are
based on graphical formalisms similar to proof nets and are limited to prenex
formulas.
  In this paper we present a new approach that directly extends expansion trees
by cuts and covers also non-prenex formulas. We describe a cut-elimination
procedure for our expansion trees with cut that is based on the natural
reduction steps. We prove that it is weakly normalizing using methods from the
epsilon-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0435</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0435</id><created>2013-08-02</created><authors><author><keyname>Razafindradina</keyname><forenames>Henri Bruno</forenames></author><author><keyname>Razafindrakoto</keyname><forenames>Nicolas Raft</forenames></author><author><keyname>Randriamitantsoa</keyname><forenames>Paul Auguste</forenames></author></authors><title>Improved Watermarking Scheme Using Discrete Cosine Transform and Schur
  Decomposition</title><categories>cs.MM cs.CR</categories><journal-ref>IJCSN International Journal of Computer Science and Network,
  Volume 2, Issue 4, August 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Watermarking is a technique which consists in introducing a brand, the name
or the logo of the author, in an image in order to protect it against illegal
copy. The capacity of the existing watermark channel is often limited. We
propose in this paper a new robust method which consists in adding the
triangular matrix of the mark obtained after the Schur decomposition to the DCT
transform of the host image. The unitary matrix acts as secret key for the
extraction of the mark. Unlike most watermarking algorithms, the host image and
the mark have the same size. The results show that our method is robust against
attack techniques as : JPEG compression, colors reducing, adding noise,
filtering, cropping, low rotations, and histogram spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0437</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0437</id><created>2013-08-02</created><authors><author><keyname>Rakotondraina</keyname><forenames>Tahina Ez&#xe9;chiel</forenames></author><author><keyname>Razafindradina</keyname><forenames>Henri Bruno</forenames></author></authors><title>Authentication System Securing Index of Image using SVD and ECC</title><categories>cs.CR</categories><journal-ref>IJCSN International Journal of Computer Science and Network, Vol
  2, Issue 1, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach to securing information stored in a
database. It has three components including: an operation for indexing images
using Singular Value Decomposition (SVD), which will constitute the reference
images, asymmetric encryption operation using Elliptic Curve Cryptosystem
(ECC), aiming to make confidential these reference images stored and a
technique for comparing these images to a query image using the Euclidian
Distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0455</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0455</id><created>2013-08-02</created><authors><author><keyname>Zhou</keyname><forenames>Shenglong</forenames></author><author><keyname>Kong</keyname><forenames>Lingchen</forenames></author><author><keyname>Luo</keyname><forenames>Ziyan</forenames></author><author><keyname>Xiu</keyname><forenames>Naihua</forenames></author></authors><title>New RIC Bounds via l_q-minimization with 0&lt;q&lt;=1 in Compressed Sensing</title><categories>cs.IT math.IT math.OC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The restricted isometry constants (RICs) play an important role in exact
recovery theory of sparse signals via l_q(0&lt;q&lt;=1) relaxations in compressed
sensing. Recently, Cai and Zhang[6] have achieved a sharp bound
\delta_tk&lt;\sqrt{1-1/t} for t&gt;=4/3 to guarantee the exact recovery of k sparse
signals through the l_1 minimization. This paper aims to establish new RICs
bounds via l_q(0&lt;q&lt;=1) relaxation. Based on a key inequality on l_q norm, we
show that (i) the exact recovery can be succeeded via l_{1/2} and l_1
minimizations if \delta_tk&lt;\sqrt{1-1/t} for any t&gt;1, (ii)several sufficient
conditions can be derived, such as for any 0&lt;q&lt;1/2, \delta_2k&lt;0.5547 when k&gt;=2,
for any 1/2&lt;q&lt;1, \delta_2k&lt;0.6782 when k&gt;=1, (iii) the bound on \delta_k is
given as well for any 0&lt;q&lt;=1, especially for q=1/2,1, we obtain \delta_k&lt;1/3
when k(&gt;=2) is even or \delta_k&lt;0.3203 when k(&gt;=3) is odd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0482</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0482</id><created>2013-08-02</created><updated>2013-11-22</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Muciaccia</keyname><forenames>Gabriele</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Parameterized Complexity of k-Chinese Postman Problem</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem called the $k$-Chinese Postman Problem
($k$-CPP): given a connected edge-weighted graph $G$ and integers $p$ and $k$,
decide whether there are at least $k$ closed walks such that every edge of $G$
is contained in at least one of them and the total weight of the edges in the
walks is at most $p$? The problem $k$-CPP is NP-complete, and van Bevern et al.
(to appear) and Sorge (2013) asked whether the $k$-CPP is fixed-parameter
tractable when parameterized by $k$. We prove that the $k$-CPP is indeed
fixed-parameter tractable. In fact, we prove a stronger result: the problem
admits a kernel with $O(k^2\log k)$ vertices. We prove that the directed
version of $k$-CPP is NP-complete and ask whether the directed version is
fixed-parameter tractable when parameterized by $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0484</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0484</id><created>2013-08-02</created><updated>2013-08-15</updated><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Kaul</keyname><forenames>Manohar</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author></authors><title>Using Incomplete Information for Complete Weight Annotation of Road
  Networks -- Extended Version</title><categories>cs.LG cs.DB</categories><comments>This is an extended version of &quot;Using Incomplete Information for
  Complete Weight Annotation of Road Networks,&quot; which is accepted for
  publication in IEEE TKDE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are witnessing increasing interests in the effective use of road networks.
For example, to enable effective vehicle routing, weighted-graph models of
transportation networks are used, where the weight of an edge captures some
cost associated with traversing the edge, e.g., greenhouse gas (GHG) emissions
or travel time. It is a precondition to using a graph model for routing that
all edges have weights. Weights that capture travel times and GHG emissions can
be extracted from GPS trajectory data collected from the network. However, GPS
trajectory data typically lack the coverage needed to assign weights to all
edges. This paper formulates and addresses the problem of annotating all edges
in a road network with travel cost based weights from a set of trips in the
network that cover only a small fraction of the edges, each with an associated
ground-truth travel cost. A general framework is proposed to solve the problem.
Specifically, the problem is modeled as a regression problem and solved by
minimizing a judiciously designed objective function that takes into account
the topology of the road network. In particular, the use of weighted PageRank
values of edges is explored for assigning appropriate weights to all edges, and
the property of directional adjacency of edges is also taken into account to
assign weights. Empirical studies with weights capturing travel time and GHG
emissions on two road networks (Skagen, Denmark, and North Jutland, Denmark)
offer insight into the design properties of the proposed techniques and offer
evidence that the techniques are effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0490</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0490</id><created>2013-08-02</created><updated>2013-12-23</updated><authors><author><keyname>Crismani</keyname><forenames>Alessandro</forenames></author><author><keyname>Schilcher</keyname><forenames>Udo</forenames></author><author><keyname>Brandner</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Toumpis</keyname><forenames>Stavros</forenames></author><author><keyname>Bettstetter</keyname><forenames>Christian</forenames></author></authors><title>Cooperative Relaying in Wireless Networks under Spatially and Temporally
  Correlated Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the packet delivery performance of wireless cooperative relaying
in an interference limited scenario, by means of stochastic geometry. We show
that the temporal and spatial characteristics of interference play a
significant role in shaping the system performance. In particular, when
interference has high temporal and spatial dependence, the packet delivery
probability increases for harsh communication conditions but decreases for good
communication conditions. We also show that these properties of interference
affect the optimal positions of relays. Hence, when studying cooperative
protocols one should carefully account for interference dynamics for obtaining
meaningful results. We also discuss different detection strategies at the
destination, namely selection combining and maximal ratio combining. We find
that maximal ratio combining is effective only when relays are close to the
source. The benefits of maximal ratio combining vanish when relays move toward
the destination, which is the scenario with the maximum packet delivery
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0497</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0497</id><created>2013-07-31</created><authors><author><keyname>Cattabriga</keyname><forenames>Paola</forenames></author></authors><title>A note on T\&quot;uring's 1936</title><categories>cs.CC</categories><comments>4 pages, for more information see
  http://paolacattabriga.wordpress.com/</comments><msc-class>03D10, 68Qxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  T\&quot;uring's argument that there can be no machine computing the diagonal on
the enumeration of the computable sequences is not a demonstration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0502</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0502</id><created>2013-08-02</created><authors><author><keyname>Cheney</keyname><forenames>James</forenames></author></authors><title>Static Enforceability of XPath-Based Access Control Policies</title><categories>cs.DB cs.CR cs.LO</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of extending XML databases with fine-grained,
high-level access control policies specified using XPath expressions. Most
prior work checks individual updates dynamically, which is expensive (requiring
worst-case execution time proportional to the size of the database). On the
other hand, static enforcement can be performed without accessing the database
but may be incomplete, in the sense that it may forbid accesses that dynamic
enforcement would allow. We introduce topological characterizations of XPath
fragments in order to study the problem of determining when an access control
policy can be enforced statically without loss of precision. We introduce the
notion of fair policies that are statically enforceable, and study the
complexity of determining fairness and of static enforcement itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0503</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0503</id><created>2013-08-01</created><authors><author><keyname>Henry</keyname><forenames>Ehirim</forenames></author></authors><title>The Effects Of Computerizing Banking Operations</title><categories>cs.CY</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Computerizing banking operation has a far-reaching consequences,both
positively and negatively.But here,i have been able to deal with both effects
and proffer solutions on the best way to go about computerizing banking
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0514</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0514</id><created>2013-08-02</created><authors><author><keyname>Scherzinger</keyname><forenames>Stefanie</forenames></author><author><keyname>Klettke</keyname><forenames>Meike</forenames></author><author><keyname>St&#xf6;rl</keyname><forenames>Uta</forenames></author></authors><title>Managing Schema Evolution in NoSQL Data Stores</title><categories>cs.DB cs.PL</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL data stores are commonly schema-less, providing no means for globally
defining or managing the schema. While this offers great flexibility in early
stages of application development, developers soon can experience the heavy
burden of dealing with increasingly heterogeneous data. This paper targets
schema evolution for NoSQL data stores, the complex task of adapting and
changing the implicit structure of the data stored. We discuss the
recommendations of the developer community on handling schema changes, and
introduce a simple, declarative schema evolution language. With our language,
software developers and architects can systematically manage the evolution of
their production data and perform typical schema maintenance tasks. We further
provide a holistic NoSQL database programming language to define the semantics
of our schema evolution language. Our solution does not require any
modifications to the NoSQL data store, treating the data store as a black box.
Thus, we want to address application developers that use NoSQL systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0517</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0517</id><created>2013-08-02</created><updated>2014-09-01</updated><authors><author><keyname>Indiveri</keyname><forenames>Giovanni</forenames></author><author><keyname>Parlangeli</keyname><forenames>Gianfranco</forenames></author></authors><title>Further results on the observability analysis and observer design for
  single range localization in 3D</title><categories>cs.RO</categories><comments>PRIN project MARIS: Marine Autonomous Robotics for InterventionS,
  call of year 2010-2011, prot. 2010FBLHRJ. The version dated 31st August 2014
  has only minor changes (eg. spelling) with respect to the previous version.
  One bibliographical entry has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of single range based observability analysis and observer design
for the kinematics model of a 3D vehicle subject to a constant unknown drift
velocity is addressed. The proposed method departs from alternative solutions
to the problem and leads to the definition of a linear time invariant state
equation with a linear time varying output that can be used to globally solve
the original nonlinear state estimation problem with a standard Kalman filter.
Simple necessary and sufficient observability conditions are derived. Numerical
simulation examples are described to illustrate the performance of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0518</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0518</id><created>2013-08-02</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author></authors><title>Sparsely-Packetized Predictive Control by Orthogonal Matching Pursuit</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>3-page extended abstract for MTNS 2012 with 3 figures</comments><msc-class>37N35, 47N70, 49J15, 49M20, 93C41, 93D20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study packetized predictive control, known to be robust against packet
dropouts in networked systems. To obtain sparse packets for rate-limited
networks, we design control packets via an L0 optimization, which can be
effectively solved by orthogonal matching pursuit. Our formulation ensures
asymptotic stability of the control loop in the presence of bounded packet
dropouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0544</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0544</id><created>2013-08-02</created><authors><author><keyname>Fitzsimmons</keyname><forenames>Zack</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>Control in the Presence of Manipulators: Cooperative and Competitive
  Cases</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control and manipulation are two of the most studied types of attacks on
elections. In this paper, we study the complexity of control attacks on
elections in which there are manipulators. We study both the case where the
&quot;chair&quot; who is seeking to control the election is allied with the manipulators,
and the case where the manipulators seek to thwart the chair. In the latter
case, we see that the order of play substantially influences the complexity. We
prove upper bounds, holding over every election system with a polynomial-time
winner problem, for all standard control cases, and some of these bounds are at
the second or third level of the polynomial hierarchy, and we provide matching
lower bounds to prove these tight. Nonetheless, for important natural systems
the complexity can be much lower. We prove that for approval and plurality
elections, the complexity of even competitive clashes between a controller and
manipulators falls far below those high bounds, even as low as polynomial time.
Yet we for a Borda-voting case show that such clashes raise the complexity
unless NP = coNP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0555</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0555</id><created>2013-08-01</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author></authors><title>On Two Conversion Methods of Decimal-to-Binary</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decimal-to-binary conversion is important to modern binary computers. The
classical method to solve this problem is based on division operation. In this
paper, we investigate a decimal-to-binary conversion method based on addition
operation. The method is very easily implemented by software. The cost analysis
shows that the latter is more preferable than the classical method. Thus the
current Input/Output translation hardware to convert between the internal digit
pairs and the external standard BCD codes can be reasonably removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0568</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0568</id><created>2013-08-02</created><authors><author><keyname>Awad</keyname><forenames>M A</forenames></author><author><keyname>Rashad</keyname><forenames>M Z</forenames></author><author><keyname>Elsoud</keyname><forenames>M A</forenames></author><author><keyname>El-dosuky</keyname><forenames>M A</forenames></author></authors><title>Visualization of Job Scheduling in Grid Computers</title><categories>cs.DC</categories><journal-ref>International Journal of Computer Applications 74(8):37-40, July
  2013</journal-ref><doi>10.5120/12908-0036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the hot problems in grid computing is job scheduling. It is known that
the job scheduling is NP-complete, and thus the use of heuristics is the de
facto approach to deal with this practice in its difficulty. The proposed is an
imagination to fish swarm, job dispatcher and Visualization gridsim to execute
some jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0577</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0577</id><created>2013-08-02</created><authors><author><keyname>Orman</keyname><forenames>G&#xfc;nce Keziban</forenames></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author></authors><title>Towards realistic artificial benchmark for community detection
  algorithms evaluation</title><categories>cs.SI physics.soc-ph</categories><journal-ref>International Journal of Web Based Communities, 9(3):349-370, 2013</journal-ref><doi>10.1504/IJWBC.2013.054908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing the partitioning performance of community detection algorithms is
one of the most important issues in complex network analysis. Artificially
generated networks are often used as benchmarks for this purpose. However,
previous studies showed their level of realism have a significant effect on the
algorithms performance. In this study, we adopt a thorough experimental
approach to tackle this problem and investigate this effect. To assess the
level of realism, we use consensual network topological properties. Based on
the LFR method, the most realistic generative method to date, we propose two
alternative random models to replace the Configuration Model originally used in
this algorithm, in order to increase its realism. Experimental results show
both modifications allow generating collections of community-structured
artificial networks whose topological properties are closer to those
encountered in real-world networks. Moreover, the results obtained with eleven
popular community identification algorithms on these benchmarks show their
performance decrease on more realistic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0580</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0580</id><created>2013-08-02</created><updated>2013-12-10</updated><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Siap</keyname><forenames>Irfan</forenames></author></authors><title>New extremal binary self-dual codes of length 68 from quadratic residue
  codes over f_2+uf_2+u^2f_2</title><categories>cs.IT math.IT</categories><comments>Under review</comments><msc-class>94B05, 94B99</msc-class><journal-ref>Finite Fileds and Their Applications Volume 29 2014</journal-ref><doi>10.1016/j.ffa.2014.04.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, quadratic reside codes over the ring F2 +uF2 +u^2F2 with u^3 =
u are considered. A duality and distance preserving Gray map from F2 + uF2 +
u^2F2 to (F_2)^3 is defined. By using quadratic double circulant, quadratic
bordered double circulant constructions and their extensions self- dual codes
of different lengths are obtained. As Gray images of these codes and their
extensions, a substantial number of new extremal self-dual binary codes are
found. More precisely, thirty two new extremal binary self-dual codes of length
68, 363 Type I codes of parameters [72; 36; 12], a Type II [72; 36; 12] code
and a Type II [96; 48; 16] code with new weight enumerators are obtained
through these constructions. The results are tabulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0583</identifier>
 <datestamp>2013-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0583</id><created>2013-08-02</created><updated>2013-09-25</updated><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author><author><keyname>Jain</keyname><forenames>Mitesh</forenames></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author></authors><title>Verification of Sequential Circuits by Tests-As-Proofs Paradigm</title><categories>cs.LO</categories><acm-class>B.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an algorithm for detection of bugs in sequential circuits. This
algorithm is incomplete i.e. its failure to find a bug breaking a property P
does not imply that P holds. The appeal of incomplete algorithms is that they
scale better than their complete counterparts. However, to make an incomplete
algorithm effective one needs to guarantee that the probability of finding a
bug is reasonably high. We try to achieve such effectiveness by employing the
Test-As-Proofs (TAP) paradigm. In our TAP based approach, a counterexample is
built as a sequence of states extracted from proofs that some local variations
of property P hold. This increases the probability that a) a representative set
of states is examined and that b) the considered states are relevant to
property P.
  We describe an algorithm of test generation based on the TAP paradigm and
give preliminary experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0585</identifier>
 <datestamp>2013-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0585</id><created>2013-08-02</created><updated>2013-09-04</updated><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Bhuvan</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Sivasubramaniam</keyname><forenames>Anand</forenames></author></authors><title>Data Center Cost Optimization Via Workload Modulation Under Real-World
  Electricity Pricing</title><categories>cs.SY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate optimization problems to study how data centers might modulate
their power demands for cost-effective operation taking into account three key
complex features exhibited by real-world electricity pricing schemes: (i)
time-varying prices (e.g., time-of-day pricing, spot pricing, or higher energy
prices during coincident peaks) and (ii) separate charge for peak power
consumption. Our focus is on demand modulation at the granularity of an entire
data center or a large part of it. For computational tractability reasons, we
work with a fluid model for power demands which we imagine can be modulated
using two abstract knobs of demand dropping and demand delaying (each with its
associated penalties or costs). Given many data center workloads and electric
prices can be effectively predicted using statistical modeling techniques, we
devise a stochastic dynamic program (SDP) that can leverage such predictive
models. Since the SDP can be computationally infeasible in many real platforms,
we devise approximations for it. We also devise fully online algorithms that
might be useful for scenarios with poor power demand or utility price
predictability. For one of our online algorithms, we prove a competitive ratio
of 2-1/n. Finally, using empirical evaluation with both real-world and
synthetic power demands and real-world prices, we demonstrate the efficacy of
our techniques. As two salient empirically-gained insights: (i) demand delaying
is more effective than demand dropping regarding to peak shaving (e.g., 10.74%
cost saving with only delaying vs. 1.45% with only dropping for Google
workload) and (ii) workloads tend to have different cost saving potential under
various electricity tariffs (e.g., 16.97% cost saving under peak-based tariff
vs. 1.55% under time-varying pricing tariff for Facebook workload).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0586</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0586</id><created>2013-07-30</created><authors><author><keyname>Coogan</keyname><forenames>Samuel</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author></authors><title>A note on norm-based Lyapunov functions via contraction analysis</title><categories>math.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well know that for globally contractive autonomous systems, there
exists a unique equilibrium and the distance to the equilibrium evaluated along
any trajectory decreases exponentially with time. We show that, additionally,
the magnitude of the velocity evaluated along any trajectory decreases
exponentially, thus giving an alternative choice of Lyapunov function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0608</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0608</id><created>2013-08-02</created><authors><author><keyname>Razafindradina</keyname><forenames>Henri Bruno</forenames></author><author><keyname>Randriamitantsoa</keyname><forenames>Paul Auguste</forenames></author><author><keyname>Razafindrakoto</keyname><forenames>Nicolas Raft</forenames></author></authors><title>Compression d'images par SVD et sur-approximation des composantes de
  chrominance</title><categories>cs.IT math.IT</categories><comments>CARI 2010 Proceedings, Yamoussoukro, pp 253-260</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a new scheme of colour image compression related to singular
values matrix approximation. The image has to be converted in luminance /
chrominance space before being processed like JPEG standard 4 : 2 : 0. Our
approach is first based on a chrominance sub-sampling, then an over estimation
of its singular values. Instead of keeping only the k first singular values for
the 3 components R, G and B, we hold k first coefficients for the Y component
and only k' (k' &lt;= k) coefficients for 2 components Cb and Cr. Results show
that for 512 x 512 pixels that, from k = 40 corresponding in an average
distortion of 30 dB and a ratio of 15 : 1, the restored image has good quality.
The algorithm allows a significant speed gain by sub-sampling too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0625</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0625</id><created>2013-08-02</created><authors><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>TCP-Aware Backpressure Routing and Scheduling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we explore the performance of backpressure routing and
scheduling for TCP flows over wireless networks. TCP and backpressure are not
compatible due to a mismatch between the congestion control mechanism of TCP
and the queue size based routing and scheduling of the backpressure framework.
We propose a TCP-aware backpressure routing and scheduling that takes into
account the behavior of TCP flows. TCP-aware backpressure (i) provides
throughput optimality guarantees in the Lyapunov optimization framework, (ii)
gracefully combines TCP and backpressure without making any changes to the TCP
protocol, (iii) improves the throughput of TCP flows significantly, and (iv)
provides fairness across competing TCP flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0626</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0626</id><created>2013-08-02</created><authors><author><keyname>Saks</keyname><forenames>M.</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Estimating the longest increasing sequence in polylogarithmic time</title><categories>cs.DS cs.DM</categories><comments>Full version of FOCS 2010 paper</comments><acm-class>F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the length of the longest increasing subsequence (LIS) is a classic
algorithmic problem. Let $n$ denote the size of the array. Simple $O(n\log n)$
algorithms are known for this problem. We develop a polylogarithmic time
randomized algorithm that for any constant $\delta &gt; 0$, estimates the length
of the LIS of an array to within an additive error of $\delta n$. More
precisely, the running time of the algorithm is $(\log n)^c
(1/\delta)^{O(1/\delta)}$ where the exponent $c$ is independent of $\delta$.
Previously, the best known polylogarithmic time algorithms could only achieve
an additive $n/2$ approximation. With a suitable choice of parameters, our
algorithm also gives, for any fixed $\tau&gt;0$, a multiplicative
$(1+\tau)$-approximation to the distance to monotonicity $\varepsilon_f$ (the
fraction of entries not in the LIS), whose running time is polynomial in
$\log(n)$ and $1/varepsilon_f$. The best previously known algorithm could only
guarantee an approximation within a factor (arbitrarily close to) 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0632</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0632</id><created>2013-08-02</created><authors><author><keyname>Ma</keyname><forenames>Rick</forenames></author><author><keyname>Cheng</keyname><forenames>Samuel</forenames></author></authors><title>Zero-error Slepian-Wolf Coding of Confined Correlated Sources with
  Deviation Symmetry</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use linear codes to study zero-error Slepian-Wolf coding of
a set of sources with deviation symmetry, where the sources are generalization
of the Hamming sources over an arbitrary field. We extend our previous codes,
Generalized Hamming Codes for Multiple Sources, to Matrix Partition Codes and
use the latter to efficiently compress the target sources. We further show that
every perfect or linear-optimal code is a Matrix Partition Code. We also
present some conditions when Matrix Partition Codes are perfect and/or
linear-optimal. Detail discussions of Matrix Partition Codes on Hamming sources
are given at last as examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0650</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0650</id><created>2013-08-02</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Frequency Domain Min-Max Optimization of Noise-Shaping Delta-Sigma
  Modulators</title><categories>cs.IT cs.SY math.IT math.OC</categories><journal-ref>IEEE Transactions on Signal Processing, Vol. 60, No. 6, pp.
  2828-2839, 2012</journal-ref><doi>10.1109/TSP.2012.2188522</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a min-max design of noise-shaping delta-sigma modulators.
We first characterize the all stabilizing loop-filters for a linearized
modulator model. By this characterization, we formulate the design problem of
lowpass, bandpass, and multi-band modulators as minimization of the maximum
magnitude of the noise transfer function (NTF) in fixed frequency band(s). We
show that this optimization minimizes the worst-case reconstruction error, and
hence improves the SNR (signal-to-noise ratio) of the modulator. The
optimization is reduced to an optimization with a linear matrix inequality
(LMI) via the generalized KYP (Kalman-Yakubovich-Popov) lemma. The obtained NTF
is an FIR (finite-impulse-response) filter, which is favorable in view of
implementation. We also derive a stability condition for the nonlinear model of
delta-sigma modulators with general quantizers including uniform ones. This
condition is described as an H-infinity norm condition, which is reduced to an
LMI via the KYP lemma. Design examples show advantages of our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0656</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0656</id><created>2013-08-02</created><updated>2014-06-16</updated><authors><author><keyname>Fu</keyname><forenames>Yupeng</forenames></author><author><keyname>Ong</keyname><forenames>Kian Win</forenames></author><author><keyname>Papakonstantinou</keyname><forenames>Yannis</forenames></author></authors><title>Declarative Ajax Web Applications through SQL++ on a Unified Application
  State</title><categories>cs.DB cs.SE</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing even a conceptually simple web application requires an
inordinate amount of time. FORWARD addresses three problems that reduce
developer productivity: (a) Impedance mismatch across the multiple languages
used at different tiers of the application architecture. (b) Distributed data
access across the multiple data sources of the application (SQL database, user
input of the browser page, session data in the application server, etc). (c)
Asynchronous, incremental modification of the pages, as performed by Ajax
actions.
  FORWARD belongs to a novel family of web application frameworks that attack
impedance mismatch by offering a single unifying language. FORWARD's language
is SQL++, a minimally extended SQL. FORWARD's architecture is based on two
novel cornerstones: (a) A Unified Application State (UAS), which is a virtual
database over the multiple data sources. The UAS is accessed via distributed
SQL++ queries, therefore resolving the distributed data access problem. (b)
Declarative page specifications, which treat the data displayed by pages as
rendered SQL++ page queries. The resulting pages are automatically
incrementally modified by FORWARD. User input on the page becomes part of the
UAS.
  We show that SQL++ captures the semi-structured nature of web pages and
subsumes the data models of two important data sources of the UAS: SQL
databases and JavaScript components. We show that simple markup is sufficient
for creating Ajax displays and for modeling user input on the page as UAS data
sources. Finally, we discuss the page specification syntax and semantics that
are needed in order to avoid race conditions and conflicts between the user
input and the automated Ajax page modifications.
  FORWARD has been used in the development of eight commercial and academic
applications. An alpha-release web-based IDE (itself built in FORWARD) enables
development in the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0658</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0658</id><created>2013-08-03</created><authors><author><keyname>Ren</keyname><forenames>Jimmy SJ.</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Jiawei</forenames></author><author><keyname>Liao</keyname><forenames>Stephen Shaoyi</forenames></author></authors><title>Exploring The Contribution of Unlabeled Data in Financial Sentiment
  Analysis</title><categories>cs.CL cs.LG</categories><comments>Appeared in The 27th AAAI Conference on Artificial Intelligence
  (AAAI-13); Proceedings of AAAI-13 (AAAI Press 2013) pp. 1149-1155</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of its applications in various industries, sentiment
analysis by using publicly available web data has become an active research
area in text classification during these years. It is argued by researchers
that semi-supervised learning is an effective approach to this problem since it
is capable to mitigate the manual labeling effort which is usually expensive
and time-consuming. However, there was a long-term debate on the effectiveness
of unlabeled data in text classification. This was partially caused by the fact
that many assumptions in theoretic analysis often do not hold in practice. We
argue that this problem may be further understood by adding an additional
dimension in the experiment. This allows us to address this problem in the
perspective of bias and variance in a broader view. We show that the well-known
performance degradation issue caused by unlabeled data can be reproduced as a
subset of the whole scenario. We argue that if the bias-variance trade-off is
to be better balanced by a more effective feature selection method unlabeled
data is very likely to boost the classification performance. We then propose a
feature selection framework in which labeled and unlabeled training samples are
both considered. We discuss its potential in achieving such a balance. Besides,
the application in financial sentiment analysis is chosen because it not only
exemplifies an important application, the data possesses better illustrative
power as well. The implications of this study in text classification and
financial sentiment analysis are both discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0661</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0661</id><created>2013-08-03</created><authors><author><keyname>Atda&#x11f;</keyname><forenames>Samet</forenames></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames></author></authors><title>A Comparison of Named Entity Recognition Tools Applied to Biographical
  Texts</title><categories>cs.IR cs.CL</categories><journal-ref>2nd International Conference on Systems and Computer Science,
  Villeneuve d'Ascq (FR), 228-233, 2013</journal-ref><doi>10.1109/IcConSCS.2013.6632052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named entity recognition (NER) is a popular domain of natural language
processing. For this reason, many tools exist to perform this task. Amongst
other points, they differ in the processing method they rely upon, the entity
types they can detect, the nature of the text they can handle, and their
input/output formats. This makes it difficult for a user to select an
appropriate NER tool for a specific situation. In this article, we try to
answer this question in the context of biographic texts. For this matter, we
first constitute a new corpus by annotating Wikipedia articles. We then select
publicly available, well known and free for research NER tools for comparison:
Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply
them to our corpus, assess their performances and compare them. When
considering overall performances, a clear hierarchy emerges: Stanford has the
best results, followed by LingPipe, Illionois and OpenCalais. However, a more
detailed evaluation performed relatively to entity types and article categories
highlights the fact their performances are diversely influenced by those
factors. This complementarity opens an interesting perspective regarding the
combination of these individual tools in order to improve performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0678</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0678</id><created>2013-08-03</created><authors><author><keyname>Masood</keyname><forenames>Syed Haani</forenames></author></authors><title>Performance comparison of IEEE 802.11g and IEEE 802.11n in the presence
  of interference from 802.15.4 networks</title><categories>cs.NI cs.PF</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare the packet error rate (PER) and maximum throughput
of IEEE 802.11n and IEEE 802.11g under interference from IEEE 802.15.4 by using
MATLAB to simulate the IEEE PHY for 802.11n and 802.11g networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0679</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0679</id><created>2013-08-03</created><authors><author><keyname>Li</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Xingming</forenames></author><author><keyname>Liu</keyname><forenames>Quansheng</forenames></author></authors><title>Image Integrity Authentication Scheme Based On Fixed Point Theory</title><categories>cs.CR cs.MM</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on fixed point theory, this paper proposes a new scheme for image
integrity authentication, which is different from Digital Signature and Fragile
Watermarking. A realization of the new scheme is given based on Gaussian
Convolution and Deconvolution (GCD) functions. For a given image, if it is
invariant under a GCD function, we call it GCD fixed point image. An existence
theorem of fixed points for GCD functions is proved and an iterative algorithm
is presented for finding fixed points. Experiments show that GCD fixed point
images perform well in transparence, fragility, security and tampering
localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0682</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0682</id><created>2013-08-03</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>A Survey on Security and Privacy Protocols for Cognitive Wireless Sensor
  Networks</title><categories>cs.CR cs.NI</categories><comments>31 pages, 4 figures, 2 tables. The paper is accepted for publication
  in Journal of Information and Network Security, Vol 1, Issue 1, August 2013.
  This paper is an extended version of author's already existing paper in arXiv
  archive with identifier arXiv:1302.2253. There are some text overlap with
  this current paper with the previous version arXiv:1302.2253</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks have emerged as an important and new area in
wireless and mobile computing research because of their numerous potential
applications that range from indoor deployment scenarios in home and office to
outdoor deployment in adversary's territory in tactical battleground. Since in
many WSN applications, lives and livelihoods may depend on the timeliness and
correctness of sensor data obtained from dispersed sensor nodes, these networks
must be secured to prevent any possible attacks that may be launched on them.
Security is, therefore, an important issue in WSNs. However, this issue becomes
even more critical in cognitive wireless sensor networks, a type of WSN in
which the sensor nodes have the capabilities of changing their transmission and
reception parameters according to the radio environment under which they
operate in order to achieve reliable and efficient communication and optimum
utilization of the network resources. This survey paper presents a
comprehensive discussion on various security issues in CWSNs by identifying
numerous security threats in these networks and defense mechanisms to counter
these vulnerabilities. Various types of attacks on CWSNs are categorized under
different classes based on their natures and tragets, and corresponding to each
attack class, appropriate security mechanisms are presented. The paper also
identifies some open problems in this emerging area of wireless networking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0683</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0683</id><created>2013-08-03</created><authors><author><keyname>Graham</keyname><forenames>Mark</forenames></author><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Gaffney</keyname><forenames>Devin</forenames></author></authors><title>Where in the World are You? Geolocation and Language Identification in
  Twitter</title><categories>cs.CY cs.SI</categories><doi>10.1080/00330124.2014.907699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The movements of ideas and content between locations and languages are
unquestionably crucial concerns to researchers of the information age, and
Twitter has emerged as a central, global platform on which hundreds of millions
of people share knowledge and information. A variety of research has attempted
to harvest locational and linguistic metadata from tweets in order to
understand important questions related to the 300 million tweets that flow
through the platform each day. However, much of this work is carried out with
only limited understandings of how best to work with the spatial and linguistic
contexts in which the information was produced. Furthermore, standard,
well-accepted practices have yet to emerge. As such, this paper studies the
reliability of key methods used to determine language and location of content
in Twitter. It compares three automated language identification packages to
Twitter's user interface language setting and to a human coding of languages in
order to identify common sources of disagreement. The paper also demonstrates
that in many cases user-entered profile locations differ from the physical
locations users are actually tweeting from. As such, these open-ended,
user-generated, profile locations cannot be used as useful proxies for the
physical locations from which information is published to Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0686</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0686</id><created>2013-08-03</created><updated>2013-08-22</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>As-You-Go Deployment of a Wireless Network with On-Line Measurements and
  Backtracking</title><categories>cs.NI</categories><comments>16 pages; 6 figures; submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are motivated by the need, in some applications, for impromptu or
as-you-go deployment of wireless sensor networks. A person walks along a line,
making link quality measurements with the previous relay at equally spaced
locations, and deploys relays at some of these locations, so as to connect a
sensor placed on the line with a sink at the start of the line. In this paper,
we extend our earlier work on the problem (see [1]) to incorporate two new
aspects: (i) inclusion of path outage in the deployment objective, and (ii)
permitting the deployment agent to make measurements over several consecutive
steps before selecting a placement location among them (which we call
backtracking). We consider a light traffic regime, and formulate the problem as
a Markov decision process. Placement algorithms are obtained for two cases: (i)
the distance to the source is geometrically distributed with known mean, and
(ii) the average cost per step case. We motivate the per-step cost function in
terms of several known forwarding protocols for sleep-wake cycling wireless
sensor networks. We obtain the structures of the optimal policies for the
various formulations, and provide some sensitivity results about the policies
and the optimal values. We then provide a numerical study of the algorithms,
thus providing insights into the advantage of backtracking, and a comparison
with simple heuristic placement policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0689</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0689</id><created>2013-08-03</created><updated>2013-09-23</updated><authors><author><keyname>Borgstr&#xf6;m</keyname><forenames>Johannes</forenames><affiliation>Uppsala University, Uppsala, Sweden</affiliation></author><author><keyname>Gordon</keyname><forenames>Andrew D</forenames><affiliation>Microsoft Research, Cambridge, UK</affiliation></author><author><keyname>Greenberg</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania, Philadelphia, PA, USA</affiliation></author><author><keyname>Margetson</keyname><forenames>James</forenames><affiliation>Microsoft Research, Cambridge, UK</affiliation></author><author><keyname>Van Gael</keyname><forenames>Jurgen</forenames><affiliation>Microsoft FUSE Labs, Cambridge, UK</affiliation></author></authors><title>Measure Transformer Semantics for Bayesian Machine Learning</title><categories>cs.LO cs.AI cs.PL</categories><comments>An abridged version of this paper appears in the proceedings of the
  20th European Symposium on Programming (ESOP'11), part of ETAPS 2011</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  9, 2013) lmcs:815</journal-ref><doi>10.2168/LMCS-9(3:11)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bayesian approach to machine learning amounts to computing posterior
distributions of random variables from a probabilistic model of how the
variables are related (that is, a prior distribution) and a set of observations
of variables. There is a trend in machine learning towards expressing Bayesian
models as probabilistic programs. As a foundation for this kind of programming,
we propose a core functional calculus with primitives for sampling prior
distributions and observing variables. We define measure-transformer
combinators inspired by theorems in measure theory, and use these to give a
rigorous semantics to our core calculus. The original features of our semantics
include its support for discrete, continuous, and hybrid measures, and, in
particular, for observations of zero-probability events. We compile our core
language to a small imperative language that is processed by an existing
inference engine for factor graphs, which are data structures that enable many
efficient inference algorithms. This allows efficient approximate inference of
posterior marginal distributions, treating thousands of observations per second
for large instances of realistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0690</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0690</id><created>2013-08-03</created><authors><author><keyname>Dutta</keyname><forenames>P. K.</forenames></author><author><keyname>Naskar</keyname><forenames>M. K.</forenames></author><author><keyname>Mishra</keyname><forenames>O. P.</forenames></author></authors><title>Impact of two-level fuzzy cluster head selection model for wireless
  sensor network: An Energy efficient approach in remote monitoring scenarios</title><categories>cs.NI</categories><comments>10 pages,International Journal of Information Processing, 7(2),
  69-81, 2013,ISSN : 0973-8215</comments><journal-ref>International Journal of Information Processing, 7(2), 69-81,
  2013,ISSN : 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust application of wireless sensor networks has increased during the
past decade due to the potential use of wireless nodes in transmission of
information by decreasing latency for surveillance and monitoring. The study
proposes an Energy Efficient Dynamic Scenario (EEDS) for cluster head
allocation for optimum balance in the energy consumption of the whole network
that will prolong the lifetime of the network in an efficient manner. In this
paper, a two-level fuzzy logic is proposed in choosing cluster head based on
node localization and network traffic. In the upper decision making level
called global level of qualification leads to better performance of the
inference system based on all the above six fuzzy parameters for establishing
an energy efficient network model. We develop an algorithm to calculate energy
across the network if the source and destination is known. We evaluate the cost
and benefit of the data fusion, in order to adaptively adjust whether fusion
shall be performed for minimizing the total energy consumption when energy
efficient node scheduling migrates from a particular node to another node.
Simulation results show that EEDS gives the best performance with respect to
network life time density and residual energy of the node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0698</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0698</id><created>2013-08-03</created><authors><author><keyname>Booshehri</keyname><forenames>Meisam</forenames></author><author><keyname>Malekpour</keyname><forenames>Abbas</forenames></author><author><keyname>Luksch</keyname><forenames>Peter</forenames></author></authors><title>An Improving Method for Loop Unrolling</title><categories>cs.PL cs.OS</categories><comments>4 pages, International Journal of Computer Science and Information
  Security</comments><msc-class>68Nxx</msc-class><acm-class>D.3.4</acm-class><journal-ref>International Journal of Computer Science and Information
  Security, Vol. 11, No. 5, pp. 73-76 , 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we review main ideas mentioned in several other papers which
talk about optimization techniques used by compilers. Here we focus on loop
unrolling technique and its effect on power consumption, energy usage and also
its impact on program speed up by achieving ILP (Instruction-level
parallelism). Concentrating on superscalar processors, we discuss the idea of
generalized loop unrolling presented by J.C. Hang and T. Leng and then we
present a new method to traverse a linked list to get a better result of loop
unrolling in that case. After that we mention the results of some experiments
carried out on a Pentium 4 processor (as an instance of super scalar
architecture). Furthermore, the results of some other experiments on
supercomputer (the Alliat FX/2800 System) containing superscalar node
processors would be mentioned. These experiments show that loop unrolling has a
slight measurable effect on energy usage as well as power consumption. But it
could be an effective way for program speed up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0701</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0701</id><created>2013-08-03</created><authors><author><keyname>Booshehri</keyname><forenames>Meisam</forenames></author><author><keyname>Malekpour</keyname><forenames>Abbas</forenames></author><author><keyname>Luksch</keyname><forenames>Peter</forenames></author><author><keyname>Zamanifar</keyname><forenames>Kamran</forenames></author><author><keyname>Shariatmadari</keyname><forenames>Shahdad</forenames></author></authors><title>Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text</title><categories>cs.IR cs.CL</categories><comments>9 pages, International Journal of Computer Science and Information
  Security</comments><msc-class>68Txx</msc-class><acm-class>I.2.6</acm-class><journal-ref>IJCSIS, 11(5), 64-72</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this position paper we present a new approach for discovering some special
classes of assertional knowledge in the text by using large RDF repositories,
resulting in the extraction of new non-taxonomic ontological relations. Also we
use inductive reasoning beside our approach to make it outperform. Then, we
prepare a case study by applying our approach on sample data and illustrate the
soundness of our proposed approach. Moreover in our point of view current LOD
cloud is not a suitable base for our proposal in all informational domains.
Therefore we figure out some directions based on prior works to enrich datasets
of Linked Data by using web mining. The result of such enrichment can be reused
for further relation extraction and ontology enrichment from unstructured free
text documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0702</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0702</id><created>2013-08-03</created><authors><author><keyname>Potapov</keyname><forenames>Alexey</forenames></author><author><keyname>Rodionov</keyname><forenames>Sergey</forenames></author></authors><title>Universal Empathy and Ethical Bias for Artificial General Intelligence</title><categories>cs.AI</categories><comments>AGI Impacts conference 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rational agents are usually built to maximize rewards. However, AGI agents
can find undesirable ways of maximizing any prior reward function. Therefore
value learning is crucial for safe AGI. We assume that generalized states of
the world are valuable - not rewards themselves, and propose an extension of
AIXI, in which rewards are used only to bootstrap hierarchical value learning.
The modified AIXI agent is considered in the multi-agent environment, where
other agents can be either humans or other &quot;mature&quot; agents, which values should
be revealed and adopted by the &quot;infant&quot; AGI agent. General framework for
designing such empathic agent with ethical bias is proposed also as an
extension of the universal intelligence model. Moreover, we perform experiments
in the simple Markov environment, which demonstrate feasibility of our approach
to value learning in safe AGI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0723</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0723</id><created>2013-08-03</created><updated>2013-11-05</updated><authors><author><keyname>Gauvin</keyname><forenames>Laetitia</forenames></author><author><keyname>Panisson</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author></authors><title>Detecting the community structure and activity patterns of temporal
  networks: a non-negative tensor factorization approach</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PLoS ONE 9(1): e86028 (2014)</journal-ref><doi>10.1371/journal.pone.0086028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of temporal network data is calling for more
research on extracting and characterizing mesoscopic structures in temporal
networks and on relating such structure to specific functions or properties of
the system. An outstanding challenge is the extension of the results achieved
for static networks to time-varying networks, where the topological structure
of the system and the temporal activity patterns of its components are
intertwined. Here we investigate the use of a latent factor decomposition
technique, non-negative tensor factorization, to extract the community-activity
structure of temporal networks. The method is intrinsically temporal and allows
to simultaneously identify communities and to track their activity over time.
We represent the time-varying adjacency matrix of a temporal network as a
three-way tensor and approximate this tensor as a sum of terms that can be
interpreted as communities of nodes with an associated activity time series. We
summarize known computational techniques for tensor decomposition and discuss
some quality metrics that can be used to tune the complexity of the factorized
representation. We subsequently apply tensor factorization to a temporal
network for which a ground truth is available for both the community structure
and the temporal activity patterns. The data we use describe the social
interactions of students in a school, the associations between students and
school classes, and the spatio-temporal trajectories of students over time. We
show that non-negative tensor factorization is capable of recovering the class
structure with high accuracy. In particular, the extracted tensor components
can be validated either as known school classes, or in terms of correlated
activity patterns, i.e., of spatial and temporal coincidences that are
determined by the known school activity schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0725</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0725</id><created>2013-08-03</created><authors><author><keyname>Acharjya</keyname><forenames>Debi Prasanna</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debarati</forenames></author></authors><title>A Rough Computing based Performance Evaluation Approach for Educational
  Institutions</title><categories>cs.CY cs.AI</categories><comments>18 pages</comments><journal-ref>International Journal of Software Engineering and Its
  Applications, Vol. 7, No. 4, July, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance evaluation of various organizations especially educational
institutions is a very important area of research and needs to be cultivated
more. In this paper, we propose a performance evaluation for educational
institutions using rough set on fuzzy approximation spaces with ordering rules
and information entropy. In order to measure the performance of educational
institutions, we construct an evaluation index system. Rough set on fuzzy
approximation spaces with ordering is applied to explore the evaluation index
data of each level. Furthermore, the concept of information entropy is used to
determine the weighting coefficients of evaluation indexes. Also, we find the
most important indexes that influence the weighting coefficients. The proposed
approach is validated and shows the practical viability. Moreover, the proposed
approach can be applicable to any organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0726</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0726</id><created>2013-08-03</created><authors><author><keyname>Shore</keyname><forenames>Jesse</forenames></author><author><keyname>Chu</keyname><forenames>Catherine J.</forenames></author><author><keyname>Bianchi</keyname><forenames>Matt T.</forenames></author></authors><title>Power Laws and Fragility in Flow Networks</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.PE</categories><comments>Social Networks, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What makes economic and ecological networks so unlike other highly skewed
networks in their tendency toward turbulence and collapse? Here, we explore the
consequences of a defining feature of these networks: their nodes are tied
together by flow. We show that flow networks tend to the power law degree
distribution (PLDD) due to a self-reinforcing process involving position within
the global network structure, and thus present the first random graph model for
PLDDs that does not depend on a rich-get-richer function of nodal degree. We
also show that in contrast to non-flow networks, PLDD flow networks are
dramatically more vulnerable to catastrophic failure than non-PLDD flow
networks, a finding with potential explanatory power in our age of resource-
and financial-interdependence and turbulence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0729</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0729</id><created>2013-08-03</created><authors><author><keyname>Program</keyname><forenames>The Univalent Foundations</forenames></author></authors><title>Homotopy Type Theory: Univalent Foundations of Mathematics</title><categories>math.LO cs.PL math.AT math.CT</categories><comments>465 pages. arXiv v1: first-edition-257-g5561b73, formatted for online
  reading. The most recent version, copies formatted for printing, and bound
  copies, are available at http://homotopytypetheory.org/book/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Homotopy type theory is a new branch of mathematics, based on a recently
discovered connection between homotopy theory and type theory, which brings new
ideas into the very foundation of mathematics. On the one hand, Voevodsky's
subtle and beautiful &quot;univalence axiom&quot; implies that isomorphic structures can
be identified. On the other hand, &quot;higher inductive types&quot; provide direct,
logical descriptions of some of the basic spaces and constructions of homotopy
theory. Both are impossible to capture directly in classical set-theoretic
foundations, but when combined in homotopy type theory, they permit an entirely
new kind of &quot;logic of homotopy types&quot;. This suggests a new conception of
foundations of mathematics, with intrinsic homotopical content, an &quot;invariant&quot;
conception of the objects of mathematics -- and convenient machine
implementations, which can serve as a practical aid to the working
mathematician. This book is intended as a first systematic exposition of the
basics of the resulting &quot;Univalent Foundations&quot; program, and a collection of
examples of this new style of reasoning -- but without requiring the reader to
know or learn any formal logic, or to use any computer proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0743</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0743</id><created>2013-08-03</created><authors><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Brandt-Pearce</keyname><forenames>Maite</forenames></author></authors><title>High-Speed Visible Light Indoor Networks Based on Optical Orthogonal
  Codes and Combinatorial Designs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnecting devices in an indoor environment using the illumination
system and white light emitting diodes (LED) requires adaptive networking
techniques that can provide network access for multiple users. Two techniques
based on multilevel signaling and optical orthogonal codes (OOC) are explored
in this paper in order to provide simultaneous multiple access in an indoor
multiuser network. Balanced incomplete block designs (BIBD) are used to
construct multilevel symbols for M-ary signaling. Using these multilevel
symbols we are able to control the optical peak to average power ratio (PAPR)
in the system, and hereby control the dimming level. In the first technique,
the M-ary data of each user is first encoded using the OOC codeword that is
assigned to that user, and then it is fed into a BIBD encoder to generate a
multilevel signal. The second multiple access method uses sub-sets of a BIBD
code to apply multilevel expurgated pulse-position modulation (MEPPM) to the
data of each user. While the first approach has a larger Hamming distance
between the symbols of each user, the latter can provide higher bit-rates for
users in VLC systems with bandwidth-limited LEDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0749</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0749</id><created>2013-08-03</created><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Accuracy of simple, initials-based methods for author name
  disambiguation</title><categories>cs.DL</categories><comments>In press in Journal of Informetrics</comments><doi>10.1016/j.joi.2013.06.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a number of solutions that perform unsupervised name disambiguation
based on the similarity of bibliographic records or common co-authorship
patterns. Whether the use of these advanced methods, which are often difficult
to implement, is warranted depends on whether the accuracy of the most basic
disambiguation methods, which only use the author's last name and initials, is
sufficient for a particular purpose. We derive realistic estimates for the
accuracy of simple, initials-based methods using simulated bibliographic
datasets in which the true identities of authors are known. Based on the
simulations in five diverse disciplines we find that the first initial method
already correctly identifies 97% of authors. An alternative simple method,
which takes all initials into account, is typically two times less accurate,
except in certain datasets that can be identified by applying a simple
criterion. Finally, we introduce a new name-based method that combines the
features of first initial and all initials methods by implicitly taking into
account the last name frequency and the size of the dataset. This hybrid method
reduces the fraction of incorrectly identified authors by 10-30% over the first
initial method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0761</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0761</id><created>2013-08-03</created><authors><author><keyname>Semenov</keyname><forenames>Alexander</forenames></author><author><keyname>Zaikin</keyname><forenames>Oleg</forenames></author></authors><title>On estimating total time to solve SAT in distributed computing
  environments: Application to the SAT@home project</title><categories>cs.AI cs.CR cs.DC cs.DS</categories><comments>This paper was submitted to SAT-2013 conference. Its materials were
  reported in a poster session (the paper in its full variant was not
  accepted). 16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to estimate the total time required to solve SAT
in distributed environments via partitioning approach. It is based on the
observation that for some simple forms of problem partitioning one can use the
Monte Carlo approach to estimate the time required to solve an original
problem. The method proposed is based on an algorithm for searching for
partitioning with an optimal solving time estimation. We applied this method to
estimate the time required to perform logical cryptanalysis of the widely known
stream ciphers A5/1 and Bivium. The paper also describes a volunteer computing
project SAT@home aimed at solving hard combinatorial problems reduced to SAT.
In this project during several months there were solved 10 problems of logical
cryptanalysis of the A5/1 cipher thatcould not be solved using known rainbow
tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0762</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0762</id><created>2013-08-03</created><authors><author><keyname>Wang</keyname><forenames>Bing</forenames></author><author><keyname>Ruchikachorn</keyname><forenames>Puripant</forenames></author><author><keyname>Mueller</keyname><forenames>Klaus</forenames></author></authors><title>SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-dimensional data visualization has been attracting much attention. To
fully test related software and algorithms, researchers require a diverse pool
of data with known and desired features. Test data do not always provide this,
or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What
You Get). Its embodiment, Sketch Pad ND, is a tool that allows users to
generate high-dimensional data in the same interface they also use for
visualization. This provides for an immersive and direct data generation
activity, and furthermore it also enables users to interactively edit and clean
existing high-dimensional data from possible artifacts. Sketch Pad ND offers
two visualization paradigms, one based on parallel coordinates and the other
based on a relatively new framework using an N-D polygon to navigate in
high-dimensional space. The first interface allows users to draw arbitrary
profiles of probability density functions along each dimension axis and sketch
shapes for data density and connections between adjacent dimensions. The second
interface embraces the idea of sculpting. Users can carve data at arbitrary
orientations and refine them wherever necessary. This guarantees the data
generated is truly high-dimensional. We demonstrate our tool's usefulness in
real data visualization scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0768</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0768</id><created>2013-08-03</created><authors><author><keyname>Sabek</keyname><forenames>Ibrahim</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN
  Localization System</title><categories>cs.NI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-free (DF) localization is an emerging technology that allows the
detection and tracking of entities that do not carry any devices nor
participate actively in the localization process. Typically, DF systems require
a large number of transmitters and receivers to achieve acceptable accuracy,
which is not available in many scenarios such as homes and small businesses. In
this paper, we introduce MonoStream as an accurate single-stream DF
localization system that leverages the rich Channel State Information (CSI) as
well as MIMO information from the physical layer to provide accurate DF
localization with only one stream. To boost its accuracy and attain low
computational requirements, MonoStream models the DF localization problem as an
object recognition problem and uses a novel set of CSI-context features and
techniques with proven accuracy and efficiency. Experimental evaluation in two
typical testbeds, with a side-by-side comparison with the state-of-the-art,
shows that MonoStream can achieve an accuracy of 0.95m with at least 26%
enhancement in median distance error using a single stream only. This
enhancement in accuracy comes with an efficient execution of less than 23ms per
location update on a typical laptop. This highlights the potential of
MonoStream usage for real-time DF tracking applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0769</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0769</id><created>2013-08-03</created><authors><author><keyname>Ishihara</keyname><forenames>Yasunori</forenames></author><author><keyname>Suzuki</keyname><forenames>Nobutaka</forenames></author><author><keyname>Hashimoto</keyname><forenames>Kenji</forenames></author><author><keyname>Shimizu</keyname><forenames>Shogo</forenames></author><author><keyname>Fujiwara</keyname><forenames>Toru</forenames></author></authors><title>XPath Satisfiability with Parent Axes or Qualifiers Is Tractable under
  Many of Real-World DTDs</title><categories>cs.DB</categories><comments>Proceedings of the 14th International Symposium on Database
  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at finding a subclass of DTDs that covers many of the
real-world DTDs while offering a polynomial-time complexity for deciding the
XPath satisfiability problem. In our previous work, we proposed RW-DTDs, which
cover most of the real-world DTDs (26 out of 27 real-world DTDs and 1406 out of
1407 DTD rules). However, under RW-DTDs, XPath satisfiability with only child,
descendant-or-self, and sibling axes is tractable. In this paper, we propose
MRW-DTDs, which are slightly smaller than RW-DTDs but have tractability on
XPath satisfiability with parent axes or qualifiers. MRW-DTDs are a proper
superclass of duplicate-free DTDs proposed by Montazerian et al., and cover 24
out of the 27 real-world DTDs and 1403 out of the 1407 DTD rules. Under
MRW-DTDs, we show that XPath satisfiability problems with (1) child, parent,
and sibling axes, and (2) child and sibling axes and qualifiers are both
tractable, which are known to be intractable under RW-DTDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0776</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0776</id><created>2013-08-04</created><updated>2015-06-16</updated><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier
  and Derandomization</title><categories>cs.DS</categories><comments>A preliminary version was presented at the 2013 IEEE 54th Annual
  Symposium on Foundations of Computer Science (FOCS 2013)</comments><acm-class>F.2.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study dynamic $(1+\epsilon)$-approximation algorithms for the all-pairs
shortest paths problem in unweighted undirected $n$-node $m$-edge graphs under
edge deletions. The fastest algorithm for this problem is a randomized
algorithm with a total update time of $\tilde O(mn/\epsilon)$ and constant
query time by Roditty and Zwick [FOCS 2004]. The fastest deterministic
algorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total
update time of $O(mn^2)$ and constant query time. We improve these results as
follows: (1) We present an algorithm with a total update time of $\tilde
O(n^{5/2}/\epsilon)$ and constant query time that has an additive error of $2$
in addition to the $1+\epsilon$ multiplicative error. This beats the previous
$\tilde O(mn/\epsilon)$ time when $m=\Omega(n^{3/2})$. Note that the additive
error is unavoidable since, even in the static case, an $O(n^{3-\delta})$-time
(a so-called truly subcubic) combinatorial algorithm with $1+\epsilon$
multiplicative error cannot have an additive error less than $2-\epsilon$,
unless we make a major breakthrough for Boolean matrix multiplication [Dor et
al. FOCS 1996] and many other long-standing problems [Vassilevska Williams and
Williams FOCS 2010]. The algorithm can also be turned into a
$(2+\epsilon)$-approximation algorithm (without an additive error) with the
same time guarantees, improving the recent $(3+\epsilon)$-approximation
algorithm with $\tilde O(n^{5/2+O(\sqrt{\log{(1/\epsilon)}/\log n})})$ running
time of Bernstein and Roditty [SODA 2011] in terms of both approximation and
time guarantees. (2) We present a deterministic algorithm with a total update
time of $\tilde O(mn/\epsilon)$ and a query time of $O(\log\log n)$. The
algorithm has a multiplicative error of $1+\epsilon$ and gives the first
improved deterministic algorithm since 1981. It also answers an open question
raised by Bernstein [STOC 2013].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0777</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0777</id><created>2013-08-04</created><updated>2014-12-03</updated><authors><author><keyname>Wilson</keyname><forenames>James D.</forenames></author><author><keyname>Wang</keyname><forenames>Simi</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author><author><keyname>Bhamidi</keyname><forenames>Shankar</forenames></author><author><keyname>Nobel</keyname><forenames>Andrew B.</forenames></author></authors><title>A testing based extraction algorithm for identifying significant
  communities in networks</title><categories>cs.SI physics.soc-ph stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOAS760 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS760</report-no><journal-ref>Annals of Applied Statistics 2014, Vol. 8, No. 3, 1853-1891</journal-ref><doi>10.1214/14-AOAS760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common and important problem arising in the study of networks is how to
divide the vertices of a given network into one or more groups, called
communities, in such a way that vertices of the same community are more
interconnected than vertices belonging to different ones. We propose and
investigate a testing based community detection procedure called Extraction of
Statistically Significant Communities (ESSC). The ESSC procedure is based on
$p$-values for the strength of connection between a single vertex and a set of
vertices under a reference distribution derived from a conditional
configuration network model. The procedure automatically selects both the
number of communities in the network and their size. Moreover, ESSC can handle
overlapping communities and, unlike the majority of existing methods,
identifies &quot;background&quot; vertices that do not belong to a well-defined
community. The method has only one parameter, which controls the stringency of
the hypothesis tests. We investigate the performance and potential use of ESSC
and compare it with a number of existing methods, through a validation study
using four real network data sets. In addition, we carry out a simulation study
to assess the effectiveness of ESSC in networks with various types of community
structure, including networks with overlapping communities and those with
background vertices. These results suggest that ESSC is an effective
exploratory tool for the discovery of relevant community structure in complex
network systems. Data and software are available at
\urlhttp://www.unc.edu/~jameswd/research.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0786</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0786</id><created>2013-08-04</created><authors><author><keyname>Masood</keyname><forenames>Syed Haani</forenames></author><author><keyname>Raza</keyname><forenames>Syed Ali</forenames></author><author><keyname>Coates</keyname><forenames>Mark</forenames></author></authors><title>Content Distribution Strategies in Opportunistic Networks</title><categories>cs.SI cs.NI physics.soc-ph</categories><comments>63 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a mechanism for content distribution through
opportunistic contacts between subscribers. A subset of subscribers in the
network are seeded with the content. The remaining subscribers obtain the
information through opportunistic contact with a user carrying the updated
content. We study how the rate of content retrieval by subscribers is affected
by the number of initial seeders in the network. We also study the rate of
content retrieval by the subscribers under coding strategies (Network Coding,
Erasure Coding) and under Flooding, Epidemic Routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0795</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0795</id><created>2013-08-04</created><authors><author><keyname>Zafar</keyname><forenames>Salman</forenames></author></authors><title>The Economic and Sustainability Future of Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global data traffic is expected to grow exponentially in the next few years
with video and smartphone applications driving data growth. Many mobile network
providers in the UK have either deployed or planning to deploy 4th generation
Long-Term-Evolution (LTE) mobile technology as the solution to meet capacity
demands. This study evaluates the technological improvements in 4G LTE in
comparison to 3G High Speed Packet Access (HSPA) and further conducts a
techno-economic analysis using primary researched tariff data to determine
network operator profitability and mobile tariff strategy to meet user demand.
To ensure holistic analysis, the study also considers the environmental impacts
of LTE by determining the annual carbon emission for a network operator. The
study results shows LTE will prove profitable; however a trade-off has to be
made by network operators between meeting consumer tariff demands or increasing
profitability. Analysis also shows a 63% reduced in carbon emissions is
possible with migration to 4G services with implication of further financial
benefits for network operators as a result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0797</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0797</id><created>2013-08-04</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>H-Infinity-Optimal Fractional Delay Filters</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>To appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2265678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional delay filters are digital filters to delay discrete-time signals
by a fraction of the sampling period. Since the delay is fractional, the
intersample behavior of the original analog signal becomes crucial. In contrast
to the conventional designs based on the Shannon sampling theorem with the
band-limiting hypothesis, the present paper proposes a new approach based on
the modern sampled-data H-infinity optimization that aims at restoring the
intersample behavior beyond the Nyquist frequency. By using the lifting
transform or continuous-time blocking the design problem is equivalently
reduced to a discrete-time H-infinity optimization, which can be effectively
solved by numerical computation softwares. Moreover, a closed-form solution is
obtained under an assumption on the original analog signals. Design examples
are given to illustrate the advantage of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0799</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0799</id><created>2013-08-04</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Matsuda</keyname><forenames>Takahiro</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author></authors><title>Compressive Sampling for Remote Control Systems</title><categories>cs.SY cs.IT math.IT math.OC</categories><journal-ref>IEICE Transactions on Fundamentals, Vol. E95-A, No. 4, pp.
  713-722, Apr. 2012</journal-ref><doi>10.1587/transfun.E95.A.713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In remote control, efficient compression or representation of control signals
is essential to send them through rate-limited channels. For this purpose, we
propose an approach of sparse control signal representation using the
compressive sampling technique. The problem of obtaining sparse representation
is formulated by cardinality-constrained L2 optimization of the control
performance, which is reducible to L1-L2 optimization. The low rate random
sampling employed in the proposed method based on the compressive sampling, in
addition to the fact that the L1-L2 optimization can be effectively solved by a
fast iteration method, enables us to generate the sparse control signal with
reduced computational complexity, which is preferable in remote control systems
where computation delays seriously degrade the performance. We give a
theoretical result for control performance analysis based on the notion of
restricted isometry property (RIP). An example is shown to illustrate the
effectiveness of the proposed approach via numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0801</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0801</id><created>2013-08-04</created><updated>2016-02-01</updated><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author><author><keyname>Parida</keyname><forenames>Laxmi</forenames></author></authors><title>Spectral Sequences, Exact Couples and Persistent Homology of filtrations</title><categories>math.AT cs.CG</categories><comments>12 pages. Changes in the grading according to suggestion of the
  referee. One new reference added. Final version to appear in Expositiones
  Mathematicae</comments><msc-class>55T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the relationship between a very classical algebraic
object associated to a filtration of spaces, namely a spectral sequence
introduced by Leray in the 1940's, and a more recently invented object that has
found many applications -- namely, its persistent homology groups. We show the
existence of a long exact sequence of groups linking these two objects and
using it derive formulas expressing the dimensions of each individual groups of
one object in terms of the dimensions of the groups in the other object. The
main tool used to mediate between these objects is the notion of exact couples
first introduced by Massey in 1952.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0807</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0807</id><created>2013-08-04</created><authors><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author><author><keyname>Kern-Isberner</keyname><forenames>Gabriele</forenames></author></authors><title>Stratified Labelings for Abstract Argumentation</title><categories>cs.AI</categories><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce stratified labelings as a novel semantical approach to abstract
argumentation frameworks. Compared to standard labelings, stratified labelings
provide a more fine-grained assessment of the controversiality of arguments
using ranks instead of the usual labels in, out, and undecided. We relate the
framework of stratified labelings to conditional logic and, in particular, to
the System Z ranking functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0813</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0813</id><created>2013-08-04</created><updated>2015-08-05</updated><authors><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Multi-agent Systems with Compasses</title><categories>cs.MA</categories><comments>SIAM Journal on Control and Optimization, In press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates agreement protocols over cooperative and
cooperative--antagonistic multi-agent networks with coupled continuous-time
nonlinear dynamics. To guarantee convergence for such systems, it is common in
the literature to assume that the vector field of each agent is pointing inside
the convex hull formed by the states of the agent and its neighbors, given that
the relative states between each agent and its neighbors are available. This
convexity condition is relaxed in this paper, as we show that it is enough that
the vector field belongs to a strict tangent cone based on a local supporting
hyperrectangle. The new condition has the natural physical interpretation of
requiring shared reference directions in addition to the available local
relative states. Such shared reference directions can be further interpreted as
if each agent holds a magnetic compass indicating the orientations of a global
frame. It is proven that the cooperative multi-agent system achieves
exponential state agreement if and only if the time-varying interaction graph
is uniformly jointly quasi-strongly connected. Cooperative--antagonistic
multi-agent systems are also considered. For these systems, the relation has a
negative sign for arcs corresponding to antagonistic interactions. State
agreement may not be achieved, but instead it is shown that all the agents'
states asymptotically converge, and their limits agree componentwise in
absolute values if and in general only if the time-varying interaction graph is
uniformly jointly strongly connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0818</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0818</id><created>2013-08-04</created><authors><author><keyname>Y&#x131;lmaz</keyname><forenames>Onur</forenames></author></authors><title>Effects of Individual Success on Globally Distributed Team Performance</title><categories>cs.SE</categories><comments>5 pages, 8 figures, research paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Necessity of different competencies with high level of knowledge makes it
inevitable that software development is a team work. With the today's
technology, teams can communicate both synchronously and asynchronously using
different online collaboration tools throughout the world. Researches indicate
that there are many factors that affect the team success and in this paper,
effect of individual success on globally distributed team performance will be
analyzed. Student team projects undertaken by other researchers will be used to
analyze collected data and conclusions will be drawn for further analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0824</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0824</id><created>2013-08-04</created><authors><author><keyname>Bhadauria</keyname><forenames>Rohit</forenames></author><author><keyname>Borgohain</keyname><forenames>Rajdeep</forenames></author><author><keyname>Biswas</keyname><forenames>Abirlal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Secure Authentication of Cloud Data Mining API</title><categories>cs.CR</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a revolutionary concept that has brought a paradigm shift
in the IT world. This has made it possible to manage and run businesses without
even setting up an IT infrastructure. It offers multi-fold benefits to the
users moving to a cloud, while posing unknown security and privacy issues. User
authentication is one such growing concern and is greatly needed in order to
ensure privacy and security in a cloud computing environment. This paper
discusses the security at different levels viz. network, application and
virtualization, in a cloud computing environment. A security framework based on
one-time pass key mechanism has been proposed. The uniqueness of the proposed
security protocol lies in the fact, that it provides security to both the
service providers as well the users in a highly conflicting cloud environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0827</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0827</id><created>2013-08-04</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Dvo&#x159;&#xe1;k</keyname><forenames>Zden&#x11b;k</forenames></author><author><keyname>Klimo&#x161;ov&#xe1;</keyname><forenames>Tereza</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author></authors><title>Immersion in four-edge-connected graphs</title><categories>math.CO cs.DM</categories><comments>11 pages</comments><msc-class>05C83, 05C75</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fix g&gt;1. Every graph of large enough tree-width contains a g x g grid as a
minor; but here we prove that every four-edge-connected graph of large enough
tree-width contains a g x g grid as an immersion (and hence contains any fixed
graph with maximum degree at most four as an immersion). This result has a
number of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0833</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0833</id><created>2013-08-04</created><authors><author><keyname>Fillinger</keyname><forenames>Maximilian</forenames></author></authors><title>Data Structures in Classical and Quantum Computing</title><categories>quant-ph cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey summarizes several results about quantum computing related to
(mostly static) data structures. First, we describe classical data structures
for the set membership and the predecessor search problems: Perfect Hash tables
for set membership by Fredman, Koml\'{o}s and Szemer\'{e}di and a data
structure by Beame and Fich for predecessor search. We also prove results about
their space complexity (how many bits are required) and time complexity (how
many bits have to be read to answer a query). After that, we turn our attention
to classical data structures with quantum access. In the quantum access model,
data is stored in classical bits, but they can be accessed in a quantum way: We
may read several bits in superposition for unit cost. We give proofs for lower
bounds in this setting that show that the classical data structures from the
first section are, in some sense, asymptotically optimal - even in the quantum
model. In fact, these proofs are simpler and give stronger results than
previous proofs for the classical model of computation. The lower bound for set
membership was proved by Radhakrishnan, Sen and Venkatesh and the result for
the predecessor problem by Sen and Venkatesh. Finally, we examine fully quantum
data structures. Instead of encoding the data in classical bits, we now encode
it in qubits. We allow any unitary operation or measurement in order to answer
queries. We describe one data structure by de Wolf for the set membership
problem and also a general framework using fully quantum data structures in
quantum walks by Jeffery, Kothari and Magniez.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0840</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0840</id><created>2013-08-04</created><authors><author><keyname>Paul</keyname><forenames>Goutam</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author><author><keyname>Chandak</keyname><forenames>Chander</forenames></author></authors><title>Designing Parity Preserving Reversible Circuits</title><categories>cs.AR cs.ET</categories><msc-class>94C10, 81P68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making a reversible circuit fault-tolerant is much more difficult than
classical circuit and there have been only a few works in the area of
parity-preserving reversible logic design. Moreover, all of these designs are
ad hoc, based on some pre-defined parity preserving reversible gates as
building blocks. In this paper, we for the first time propose a novel and
systematic approach towards parity preserving reversible circuits design. We
provide some related theoretical results and give two algorithms, one from
reversible specification to parity preserving reversible specification and
another from irreversible specification to parity preserving reversible
specification. We also evaluate the effectiveness of our approach by extensive
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0841</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0841</id><created>2013-08-04</created><updated>2013-12-08</updated><authors><author><keyname>Zhao</keyname><forenames>Jian</forenames></author><author><keyname>Wu</keyname><forenames>Chuan</forenames></author><author><keyname>Li</keyname><forenames>Zongpeng</forenames></author></authors><title>Cost Minimization in Multiple IaaS Clouds: A Double Auction Approach</title><categories>cs.NI cs.GT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IaaS clouds invest substantial capital in operating their data centers.
Reducing the cost of resource provisioning, is their forever pursuing goal.
Computing resource trading among multiple IaaS clouds provide a potential for
IaaS clouds to utilize cheaper resources to fulfill their jobs, by exploiting
the diversities of different clouds' workloads and operational costs. In this
paper, we focus on studying the IaaS clouds' cost reduction through computing
resource trading among multiple IaaS clouds. We formulate the global cost
minimization problem among multiple IaaS clouds under cooperative scenario
where each individual cloud's workload and cost information is known. Taking
into consideration jobs with disparate lengths, a non-preemptive approximation
algorithm for leftover job migration and new job scheduling is designed. Given
to the selfishness of individual clouds, we further design a randomized double
auction mechanism to elicit clouds' truthful bidding for buying or selling
virtual machines. We evaluate our algorithms using trace-driven simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0843</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0843</id><created>2013-08-04</created><updated>2013-10-01</updated><authors><author><keyname>Avetisyan</keyname><forenames>A.</forenames><affiliation>Boston University, Boston, USA</affiliation></author><author><keyname>Bhattacharya</keyname><forenames>S.</forenames><affiliation>Brown University, Providence, USA</affiliation></author><author><keyname>Narain</keyname><forenames>M.</forenames><affiliation>Brown University, Providence, USA</affiliation></author><author><keyname>Padhi</keyname><forenames>S.</forenames><affiliation>University of California, San Diego, USA</affiliation></author><author><keyname>Hirschauer</keyname><forenames>J.</forenames><affiliation>Fermi National Accelerator Lab, Batavia, USA</affiliation></author><author><keyname>Levshina</keyname><forenames>T.</forenames><affiliation>Fermi National Accelerator Lab, Batavia, USA</affiliation></author><author><keyname>McBride</keyname><forenames>P.</forenames><affiliation>Fermi National Accelerator Lab, Batavia, USA</affiliation></author><author><keyname>Sehgal</keyname><forenames>C.</forenames><affiliation>Fermi National Accelerator Lab, Batavia, USA</affiliation></author><author><keyname>Slyz</keyname><forenames>M.</forenames><affiliation>Fermi National Accelerator Lab, Batavia, USA</affiliation></author><author><keyname>Rynge</keyname><forenames>M.</forenames><affiliation>Information Sciences Institute, Marina del Rey, USA</affiliation></author><author><keyname>Malik</keyname><forenames>S.</forenames><affiliation>University of Nebraska, Lincoln, USA</affiliation></author><author><keyname>Stupak</keyname><forenames>J.</forenames><suffix>III</suffix><affiliation>Purdue University Calumet, Hammond, USA</affiliation></author></authors><title>Snowmass Energy Frontier Simulations using the Open Science Grid (A
  Snowmass 2013 whitepaper)</title><categories>hep-ex cs.DC</categories><report-no>SNOW13-00168</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Snowmass is a US long-term planning study for the high-energy community by
the American Physical Society's Division of Particles and Fields. For its
simulation studies, opportunistic resources are harnessed using the Open
Science Grid infrastructure. Late binding grid technology, GlideinWMS, was used
for distributed scheduling of the simulation jobs across many sites mainly in
the US. The pilot infrastructure also uses the Parrot mechanism to dynamically
access CvmFS in order to ascertain a homogeneous environment across the nodes.
This report presents the resource usage and the storage model used for
simulating large statistics Standard Model backgrounds needed for Snowmass
Energy Frontier studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0850</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0850</id><created>2013-08-04</created><updated>2014-06-05</updated><authors><author><keyname>Graves</keyname><forenames>Alex</forenames></author></authors><title>Generating Sequences With Recurrent Neural Networks</title><categories>cs.NE cs.CL</categories><comments>Thanks to Peng Liu and Sergey Zyrianov for various corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how Long Short-term Memory recurrent neural networks can be
used to generate complex sequences with long-range structure, simply by
predicting one data point at a time. The approach is demonstrated for text
(where the data are discrete) and online handwriting (where the data are
real-valued). It is then extended to handwriting synthesis by allowing the
network to condition its predictions on a text sequence. The resulting system
is able to generate highly realistic cursive handwriting in a wide variety of
styles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0864</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0864</id><created>2013-08-04</created><authors><author><keyname>Geng</keyname><forenames>Yanfeng</forenames></author><author><keyname>Cassandras</keyname><forenames>Christos G.</forenames></author></authors><title>Quasi-dynamic Traffic Light Control for a Single Intersection</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1204.1910</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the traffic light control problem for a single intersection by
viewing it as a stochastic hybrid system and developing a Stochastic Flow Model
(SFM) for it. We adopt a quasi-dynamic control policy based on partial state
information defined by detecting whether vehicle backlog is above or below a
certain threshold, without the need to observe an exact vehicle count. The
policy is parameterized by green and red cycle lengths which depend on this
partial state information. Using Infinitesimal Perturbation Analysis (IPA), we
derive online gradient estimators of an average traffic congestion metric with
respect to these controllable green and red cycle lengths when the vehicle
backlog is above or below the threshold. The estimators are used to iteratively
adjust light cycle lengths so as to improve performance and, in conjunction
with a standard gradient-based algorithm, to seek optimal values which adapt to
changing traffic conditions. Simulation results are included to illustrate the
approach and quantify the benefits of quasidynamic traffic light control over
earlier static approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0867</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0867</id><created>2013-08-04</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author></authors><title>A Survey of Spline-based Volumetric Data Modeling Framework and Its
  Applications</title><categories>cs.GR</categories><comments>Master Thesis, Computer Science Department, Stony Brook University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid advances in 3D scanning and acquisition techniques have given rise
to the explosive increase of volumetric digital models in recent years. This
dissertation systematically trailblazes a novel volumetric modeling framework
to represent 3D solids. The need to explore more efficient and robust 3D
modeling framework has gained the prominence. Although the traditional surface
representation (e.g., triangle mesh) has many attractive properties, it is
incapable of expressing the interior space and materials. Such a serious
drawback overshadows many potential modeling and analysis applications.
Consequently volumetric modeling techniques become the well-known solution to
this problem. Nevertheless, many unsolved research issues remain when
developing an efficient modeling paradigm for existing 3D models: complex
geometry (fine details and extreme concaveness), arbitrary topology,
heterogenous materials, large-scale data storage and processing, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0869</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0869</id><created>2013-08-04</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author></authors><title>A Spline-based Volumetric Data Modeling Framework and Its Applications</title><categories>cs.GR</categories><comments>Ph.D thesis, Computer Science Department, Stony Brook University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this dissertation, we concentrate on the challenging research issue of
developing a spline-based modeling framework, which converts the conventional
data (e.g., surface meshes) to tensor-product trivariate splines. This
methodology can represent both boundary/volumetric geometry and real volumetric
physical attributes in a compact and continuous fashion. The regular
tensor-product structure enables our new developed methods to be embedded into
the industry standard seamlessly. These properties make our techniques highly
preferable in many physically-based applications including mechanical analysis,
shape deformation and editing, virtual surgery training, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0870</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0870</id><created>2013-08-04</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>On Interference Networks over Finite Fields</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework to study linear deterministic interference networks
over finite fields. Unlike the popular linear deterministic models introduced
to study Gaussian networks, we consider networks where the channel coefficients
are general scalars over some extension field $\FF_{p^m}$ (scalar $m$-th
extension-field models), $m \times m$ diagonal matrices over $\FF_p$
($m$-symbol extension ground-field models), and $m \times m$ general
non-singular matrices (MIMO ground field models). We use the companion matrix
representation of the extension field to convert $m$-th extension scalar models
into MIMO ground-field models where the channel matrices have special algebraic
structure. For such models, we consider the $2 \times 2 \times 2$ topology
(two-hops two-flow) and the 3-user interference network topology. We derive
achievability results and feasibility conditions for certain schemes based on
the Precoding-Based Network Alignment (PBNA) approach, where intermediate nodes
use random linear network coding (i.e., propagate random linear combinations of
their incoming messages) and non-trivial precoding/decoding is performed only
at the network edges, at the sources and destinations. Furthermore, we apply
this approach to the scalar $2\times 2\times 2$ complex Gaussian IC with fixed
channel coefficients, and show two competitive schemes outperforming other
known approaches at any SNR, where we combine finite-field linear
precoding/decoding with lattice coding and the Compute and Forward approach at
the signal level. As a side result, we also show significant advantages of
vector linear network coding both in terms of feasibility probability (with
random coding coefficients) and in terms of coding latency, with respect to
standard scalar linear network coding, in PBNA schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0890</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0890</id><created>2013-08-05</created><authors><author><keyname>Saikia</keyname><forenames>Parimita</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>Head Gesture Recognition using Optical Flow based Classification with
  Reinforcement of GMM based Background Subtraction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a technique of real time head gesture recognition
system. The method includes Gaussian mixture model (GMM) accompanied by optical
flow algorithm which provided us the required information regarding head
movement. The proposed model can be implemented in various control system. We
are also presenting the result and implementation of both mentioned method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0892</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0892</id><created>2013-08-05</created><authors><author><keyname>Zolotykh</keyname><forenames>Nikolai Yu.</forenames></author></authors><title>Alcuin's Propositiones de Civitatibus: the Earliest Packing Problems</title><categories>math.HO cs.DM math.CO</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider three problems about cities from Alcuin's_Propositiones ad
acuendos juvenes_. These problems can be considered as the earliest packing
problems difficult also for modern state-of-the-art packing algorithms. We
discuss the Alcuin's solutions and give the known (to the author) best
solutions to these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0897</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0897</id><created>2013-08-05</created><authors><author><keyname>A</keyname><forenames>Kowcika</forenames></author><author><keyname>Maheswari</keyname><forenames>Uma</forenames></author><author><keyname>T</keyname><forenames>Geetha</forenames><suffix>V</suffix></author></authors><title>Context Specific Event Model For News Articles</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new context based event indexing and event ranking model for
News Articles. The context event clusters formed from the UNL Graphs uses the
modified scoring scheme for segmenting events which is followed by clustering
of events. From the context clusters obtained three models are developed-
Identification of Main and Sub events; Event Indexing and Event Ranking. Based
on the properties considered from the UNL Graphs for the modified scoring main
events and sub events associated with main-events are identified. The temporal
details obtained from the context cluster are stored using hashmap data
structure. The temporal details are place-where the event took; person-who
involved in that event; time-when the event took place. Based on the
information collected from the context clusters three indices are generated-
Time index, Person index, and Place index. This index gives complete details
about every event obtained from context clusters. A new scoring scheme is
introduced for ranking the events. The scoring scheme for event ranking gives
weight-age based on the priority level of the events. The priority level
includes the occurrence of the event in the title of the document, event
frequency, and inverse document frequency of the events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0900</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0900</id><created>2013-08-05</created><updated>2013-10-29</updated><authors><author><keyname>Lee</keyname><forenames>Donny</forenames></author></authors><title>Trading USDCHF filtered by Gold dynamics via HMM coupling</title><categories>stat.ML cs.LG</categories><comments>Abridge version titled &quot;The profitable, hidden and Markovian couple
  of Swiss and gold&quot; in Nov '13 issue of Futures. Read it online at
  http://www.futuresmag.com/2013/10/21/the-profitable-hidden-and-markovian-couple-of-swi</comments><msc-class>91G99, 60J22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a USDCHF trading strategy using the dynamics of gold as a filter.
Our strategy involves modelling both USDCHF and gold using a coupled hidden
Markov model (CHMM). The observations will be indicators, RSI and CCI, which
will be used as triggers for our trading signals. Upon decoding the model in
each iteration, we can get the next most probable state and the next most
probable observation. Hopefully by taking advantage of intermarket analysis and
the Markov property implicit in the model, trading with these most probable
values will produce profitable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0907</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0907</id><created>2013-08-05</created><authors><author><keyname>Vaya</keyname><forenames>Shailesh</forenames></author></authors><title>The complexity of resolving conflicts on MAC</title><categories>cs.DS</categories><comments>Xerox internal report 27th July; 7 pages</comments><acm-class>F.2.0; G.0; G.2; C.2.5; C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental problem of multiple stations competing to
transmit on a multiple access channel (MAC). We are given $n$ stations out of
which at most $d$ are active and intend to transmit a message to other stations
using MAC. All stations are assumed to be synchronized according to a time
clock. If $l$ stations node transmit in the same round, then the MAC provides
the feedback whether $l=0$, $l=2$ (collision occurred) or $l=1$. When $l=1$,
then a single station is indeed able to successfully transmit a message, which
is received by all other nodes. For the above problem the active stations have
to schedule their transmissions so that they can singly, transmit their
messages on MAC, based only on the feedback received from the MAC in previous
round.
  For the above problem it was shown in [Greenberg, Winograd, {\em A Lower
bound on the Time Needed in the Worst Case to Resolve Conflicts
Deterministically in Multiple Access Channels}, Journal of ACM 1985] that every
deterministic adaptive algorithm should take $\Omega(d (\lg n)/(\lg d))$ rounds
in the worst case. The fastest known deterministic adaptive algorithm requires
$O(d \lg n)$ rounds. The gap between the upper and lower bound is $O(\lg d)$
round. It is substantial for most values of $d$: When $d = $ constant and $d
\in O(n^{\epsilon})$ (for any constant $\epsilon \leq 1$, the lower bound is
respectively $O(\lg n)$ and O(n), which is trivial in both cases. Nevertheless,
the above lower bound is interesting indeed when $d \in$ poly($\lg n$). In this
work, we present a novel counting argument to prove a tight lower bound of
$\Omega(d \lg n)$ rounds for all deterministic, adaptive algorithms, closing
this long standing open question.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0916</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0916</id><created>2013-08-05</created><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author></authors><title>Convergence Rates of Distributed Nesterov-like Gradient Methods on
  Random Networks</title><categories>cs.IT math.IT</categories><comments>journal; submitted for publication on May 11, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed optimization in random networks where N nodes
cooperatively minimize the sum \sum_{i=1}^N f_i(x) of their individual convex
costs. Existing literature proposes distributed gradient-like methods that are
computationally cheap and resilient to link failures, but have slow convergence
rates. In this paper, we propose accelerated distributed gradient methods that:
1) are resilient to link failures; 2) computationally cheap; and 3) improve
convergence rates over other gradient methods. We model the network by a
sequence of independent, identically distributed random matrices {W(k)} drawn
from the set of symmetric, stochastic matrices with positive diagonals. The
network is connected on average and the cost functions are convex,
differentiable, with Lipschitz continuous and bounded gradients. We design two
distributed Nesterov-like gradient methods that modify the D-NG and D-NC
methods that we proposed for static networks. We prove their convergence rates
in terms of the expected optimality gap at the cost function. Let k and K be
the number of per-node gradient evaluations and per-node communications,
respectively. Then the modified D-NG achieves rates O(log k/k) and O(\log K/K),
and the modified D-NC rates O(1/k^2) and O(1/K^{2-\xi}), where \xi&gt;0 is
arbitrarily small. For comparison, the standard distributed gradient method
cannot do better than \Omega(1/k^{2/3}) and \Omega(1/K^{2/3}), on the same
class of cost functions (even for static networks). Simulation examples
illustrate our analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0938</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0938</id><created>2013-08-05</created><authors><author><keyname>Schill</keyname><forenames>Mischael</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Handling Parallelism in a Concurrency Model</title><categories>cs.SE</categories><comments>MUSEPAT 2013</comments><journal-ref>Proceedings of the 2013 International Conference on Multicore
  Software Engineering, Performance, and Tools (MUSEPAT'13), volume 8063 of
  Lecture Notes in Computer Science, pages 37-48. Springer, 2013</journal-ref><doi>10.1007/978-3-642-39955-8_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming models for concurrency are optimized for dealing with
nondeterminism, for example to handle asynchronously arriving events. To shield
the developer from data race errors effectively, such models may prevent shared
access to data altogether. However, this restriction also makes them unsuitable
for applications that require data parallelism. We present a library-based
approach for permitting parallel access to arrays while preserving the safety
guarantees of the original model. When applied to SCOOP, an object-oriented
concurrency model, the approach exhibits a negligible performance overhead
compared to ordinary threaded implementations of two parallel benchmark
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0959</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0959</id><created>2013-08-05</created><updated>2016-01-08</updated><authors><author><keyname>Pari</keyname><forenames>Seyed Mohammad Asghari</forenames></author><author><keyname>Froushani</keyname><forenames>Mohammad Hassan Lotfi</forenames></author><author><keyname>Noormohammadpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author><author><keyname>Katz</keyname><forenames>Marcos</forenames></author></authors><title>An Incentive-Based Energy-Efficient Management Framework for Service
  Discovery in Device-to-Device Communications</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the authors due to an error in the
  model assumption section and also in the simulation section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cooperative arrangement of Device-to-Device (D2D) communications-enabled
devices for sharing distributed resources brings many new opportunities. In
this paper, we use the term &quot;Mobile Cloud&quot; for representing a mobile
peer-to-peer network where devices themselves are resource providers. One of
the most important problems required to be addressed before real implementation
of such mobile clouds is adopting a proper service discovery scheme. In this
paper, we propose a task-based self-organizing model as our management
framework for mobile clouds. In the proposed model, the service discovery task
is handled by a node selected as the leader. In order to select the most
qualified node as the leader, we propose a leader selection mechanism which is
based on multi-player first-price sealed-bid auction. Incentives in the form of
credit transfer from service discovery requesting nodes to the leader are
considered to encourage nodes to assume the leadership role. Furthermore, we
propose a secure transaction model between service discovery requesting nodes
and the leader for each service discovery. The simulation results demonstrate
that our proposed leader-based service discovery model balances the energy
consumption of cloud participants, and increases the overall lifetime of mobile
clouds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0971</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0971</id><created>2013-08-05</created><authors><author><keyname>Malliaros</keyname><forenames>Fragkiskos D.</forenames></author><author><keyname>Vazirgiannis</keyname><forenames>Michalis</forenames></author></authors><title>Clustering and Community Detection in Directed Networks: A Survey</title><categories>cs.SI cs.IR physics.bio-ph physics.comp-ph physics.soc-ph</categories><comments>86 pages, 17 figures. Physics Reports Journal (To Appear)</comments><doi>10.1016/j.physrep.2013.08.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks (or graphs) appear as dominant structures in diverse domains,
including sociology, biology, neuroscience and computer science. In most of the
aforementioned cases graphs are directed - in the sense that there is
directionality on the edges, making the semantics of the edges non symmetric.
An interesting feature that real networks present is the clustering or
community structure property, under which the graph topology is organized into
modules commonly called communities or clusters. The essence here is that nodes
of the same community are highly similar while on the contrary, nodes across
communities present low similarity. Revealing the underlying community
structure of directed complex networks has become a crucial and
interdisciplinary topic with a plethora of applications. Therefore, naturally
there is a recent wealth of research production in the area of mining directed
graphs - with clustering being the primary method and tool for community
detection and evaluation. The goal of this paper is to offer an in-depth review
of the methods presented so far for clustering directed networks along with the
relevant necessary methodological background and also related applications. The
survey commences by offering a concise review of the fundamental concepts and
methodological base on which graph clustering algorithms capitalize on. Then we
present the relevant work along two orthogonal classifications. The first one
is mostly concerned with the methodological principles of the clustering
algorithms, while the second one approaches the methods from the viewpoint
regarding the properties of a good cluster in a directed network. Further, we
present methods and metrics for evaluating graph clustering results,
demonstrate interesting application domains and provide promising future
research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0979</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0979</id><created>2013-08-05</created><updated>2014-08-26</updated><authors><author><keyname>Naghizadeh</keyname><forenames>Parinaz</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Closing the Price of Anarchy Gap in the Interdependent Security Game</title><categories>cs.GT cs.CR</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reliability and security of a user in an interconnected system depends on
all users' collective effort in security. Consequently, investments in security
technologies by strategic users is typically modeled as a public good problem,
known as the Interdependent Security (IDS) game. The equilibria for such games
are often inefficient, as selfish users free-ride on positive externalities of
others' contributions. In this paper, we present a mechanism that implements
the socially optimal equilibrium in an IDS game through a message exchange
process, in which users submit proposals about the security investment and
tax/price profiles of one another. This mechanism is different from existing
solutions in that (1) it results in socially optimal levels of investment,
closing the Price of Anarchy gap in the IDS game, (2) it is applicable to a
general model of user interdependencies. We further consider the issue of
individual rationality, often a trivial condition to satisfy in many resource
allocation problems, and argue that with positive externality, the incentive to
stay out and free-ride on others' investment can make individual rationality
much harder to satisfy in designing a mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0990</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0990</id><created>2013-08-05</created><authors><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Vojnovic</keyname><forenames>Milan</forenames></author></authors><title>Incentives and Efficiency in Uncertain Collaborative Environments</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider collaborative systems where users make contributions across
multiple available projects and are rewarded for their contributions in
individual projects according to a local sharing of the value produced. This
serves as a model of online social computing systems such as online Q&amp;A forums
and of credit sharing in scientific co-authorship settings. We show that the
maximum feasible produced value can be well approximated by simple local
sharing rules where users are approximately rewarded in proportion to their
marginal contributions and that this holds even under incomplete information
about the player's abilities and effort constraints. For natural instances we
show almost 95% optimality at equilibrium. When players incur a cost for their
effort, we identify a threshold phenomenon: the efficiency is a constant
fraction of the optimal when the cost is strictly convex and decreases with the
number of players if the cost is linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.0994</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.0994</id><created>2013-08-05</created><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>Cluster expansion and the boxdot conjecture</title><categories>math.LO cs.LO</categories><comments>9 pages</comments><msc-class>03B45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The boxdot conjecture asserts that every normal modal logic that faithfully
interprets T by the well-known boxdot translation is in fact included in T. We
confirm that the conjecture is true. More generally, we present a simple
semantic condition on modal logics $L_0$ which ensures that the largest logic
where $L_0$ embeds faithfully by the boxdot translation is $L_0$ itself. In
particular, this natural generalization of the boxdot conjecture holds for S4,
S5, and KTB in place of T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1004</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1004</id><created>2013-08-05</created><updated>2013-10-02</updated><authors><author><keyname>Dehghan</keyname><forenames>Azad</forenames></author></authors><title>Boundary identification of events in clinical named entity recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of named entity recognition in the medical/clinical domain has
gained increasing attention do to its vital role in a wide range of clinical
decision support applications. The identification of complete and correct term
span is vital for further knowledge synthesis (e.g., coding/mapping concepts
thesauruses and classification standards). This paper investigates boundary
adjustment by sequence labeling representations models and post-processing
techniques in the problem of clinical named entity recognition (recognition of
clinical events). Using current state-of-the-art sequence labeling algorithm
(conditional random fields), we show experimentally that sequence labeling
representation and post-processing can be significantly helpful in strict
boundary identification of clinical events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1006</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1006</id><created>2013-08-05</created><authors><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Fast Semidifferential-based Submodular Function Optimization</title><categories>cs.DS cs.DM cs.LG</categories><comments>This work appeared in Proc. International Conference of Machine
  Learning (ICML, 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a practical and powerful new framework for both unconstrained and
constrained submodular function optimization based on discrete
semidifferentials (sub- and super-differentials). The resulting algorithms,
which repeatedly compute and then efficiently optimize submodular
semigradients, offer new and generalize many old methods for submodular
optimization. Our approach, moreover, takes steps towards providing a unifying
paradigm applicable to both submodular min- imization and maximization,
problems that historically have been treated quite distinctly. The practicality
of our algorithms is important since interest in submodularity, owing to its
natural and wide applicability, has recently been in ascendance within machine
learning. We analyze theoretical properties of our algorithms for minimization
and maximization, and show that many state-of-the-art maximization algorithms
are special cases. Lastly, we complement our theoretical analyses with
supporting empirical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1009</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1009</id><created>2013-08-05</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Samorodnitsky</keyname><forenames>Gennady</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author></authors><title>Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels</title><categories>cs.LG cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of stable random projections is popular for efficiently computing
the Lp distances in high dimension (where 0&lt;p&lt;=2), using small space. Because
it adopts nonadaptive linear projections, this method is naturally suitable
when the data are collected in a dynamic streaming fashion (i.e., turnstile
data streams). In this paper, we propose to use only the signs of the projected
data and analyze the probability of collision (i.e., when the two signs
differ). We derive a bound of the collision probability which is exact when p=2
and becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,
Cauchy random projections), we show that the probability of collision can be
accurately approximated as functions of the chi-square similarity. For example,
when the (un-normalized) data are binary, the maximum approximation error of
the collision probability is smaller than 0.0192. In text and vision
applications, the chi-square similarity is a popular measure for nonnegative
data when the features are generated from histograms. Our experiments confirm
that the proposed method is promising for large-scale learning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1028</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1028</id><created>2013-07-22</created><authors><author><keyname>Garcia</keyname><forenames>Cristobal J.</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Gluesing</keyname><forenames>Julia</forenames></author><author><keyname>Lassenius</keyname><forenames>Casper</forenames></author><author><keyname>Miller</keyname><forenames>Christine</forenames></author><author><keyname>Paasivaara</keyname><forenames>Maria</forenames></author><author><keyname>Riopelle</keyname><forenames>Ken</forenames></author></authors><title>Proceedings of the 4th International Conference on Collaborative
  Innovation Networks COINs13, Santiago de Chile, August 11-13, 2013</title><categories>cs.SI cs.HC</categories><comments>17 papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Where science, design, business and art meet, COINs13 looks at the emerging
forces behind the phenomena of open-source, creative, entrepreneurial and
social movements. COINs13 combines a wide range of interdisciplinary fields
such as social network analysis, group dynamics, design and visualization,
information systems, collective action and the psychology and sociality of
collaboration. The COINs13 conference theme is Learning from the Swarm. The
papers in this volume explore what is relevant with regard to the innovative
powers of creative and civic swarms, what are the observable qualities of
virtual collaboration and mobilization, and how does the quest for global
cooperation affect local networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1031</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1031</id><created>2013-08-05</created><authors><author><keyname>Lohrmann</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Warneke</keyname><forenames>Daniel</forenames></author><author><keyname>Kao</keyname><forenames>Odej</forenames></author></authors><title>Nephele Streaming: Stream Processing Under QoS Constraints At Scale</title><categories>cs.DC</categories><comments>Journal of Cluster Computing, 2013</comments><doi>10.1007/s10586-013-0281-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to process large numbers of continuous data streams in a
near-real-time fashion has become a crucial prerequisite for many scientific
and industrial use cases in recent years. While the individual data streams are
usually trivial to process, their aggregated data volumes easily exceed the
scalability of traditional stream processing systems. At the same time,
massively-parallel data processing systems like MapReduce or Dryad currently
enjoy a tremendous popularity for data-intensive applications and have proven
to scale to large numbers of nodes. Many of these systems also provide
streaming capabilities. However, unlike traditional stream processors, these
systems have disregarded QoS requirements of prospective stream processing
applications so far. In this paper we address this gap. First, we analyze
common design principles of today's parallel data processing frameworks and
identify those principles that provide degrees of freedom in trading off the
QoS goals latency and throughput. Second, we propose a highly distributed
scheme which allows these frameworks to detect violations of user-defined QoS
constraints and optimize the job execution without manual interaction. As a
proof of concept, we implemented our approach for our massively-parallel data
processing framework Nephele and evaluated its effectiveness through a
comparison with Hadoop Online. For an example streaming application from the
multimedia domain running on a cluster of 200 nodes, our approach improves the
processing latency by a factor of at least 13 while preserving high data
throughput when needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1034</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1034</id><created>2013-08-02</created><updated>2013-11-02</updated><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Rochel</keyname><forenames>Jan</forenames></author></authors><title>Term Graph Representations for Cyclic Lambda-Terms</title><categories>cs.LO cs.PL</categories><comments>35 pages. report extending proceedings article on arXiv:1302.6338
  (changes with respect to version v2: added section 8, modified Proposition
  2.4, added Remark 2.5, added Corollary 7.11, modified figures in the
  conclusion)</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study various representations for cyclic lambda-terms as higher-order or
as first-order term graphs. We focus on the relation between
`lambda-higher-order term graphs' (lambda-ho-term-graphs), which are
first-order term graphs endowed with a well-behaved scope function, and their
representations as `lambda-term-graphs', which are plain first-order term
graphs with scope-delimiter vertices that meet certain scoping requirements.
Specifically we tackle the question: Which class of first-order term graphs
admits a faithful embedding of lambda-ho-term-graphs in the sense that: (i) the
homomorphism-based sharing-order on lambda-ho-term-graphs is preserved and
reflected, and (ii) the image of the embedding corresponds closely to a natural
class (of lambda-term-graphs) that is closed under homomorphism?
  We systematically examine whether a number of classes of lambda-term-graphs
have this property, and we find a particular class of lambda-term-graphs that
satisfies this criterion. Term graphs of this class are built from application,
abstraction, variable, and scope-delimiter vertices, and have the
characteristic feature that the latter two kinds of vertices have back-links to
the corresponding abstraction.
  This result puts a handle on the concept of subterm sharing for higher-order
term graphs, both theoretically and algorithmically: We obtain an easily
implementable method for obtaining the maximally shared form of
lambda-ho-term-graphs. Also, we open up the possibility to pull back properties
from first-order term graphs to lambda-ho-term-graphs. In fact we prove this
for the property of the sharing-order successors of a given term graph to be a
complete lattice with respect to the sharing order.
  This report extends the paper with the same title
(http://arxiv.org/abs/1302.6338v1) in the proceedings of the workshop TERMGRAPH
2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1041</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1041</id><created>2013-08-05</created><authors><author><keyname>White</keyname><forenames>David</forenames></author></authors><title>Traversals of Infinite Graphs with Random Local Orientations</title><categories>cs.DM</categories><comments>This is my masters thesis from Wesleyan University. Currently my
  advisor and I are selecting a journal where we will submit a shorter version.
  We plan to split this work into two papers: one for the case of infinite
  graphs and one for the finite case (which is not fully treated here)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of a &quot;random basic walk&quot; on an infinite graph, give
numerous examples, list potential applications, and provide detailed
comparisons between the random basic walk and existing generalizations of
simple random walks. We define analogues in the setting of random basic walks
of the notions of recurrence and transience in the theory of simple random
walks, and we study the question of which graphs have a cycling random basic
walk and which a transient random basic walk.
  We prove that cycles of arbitrary length are possible in any regular graph,
but that they are unlikely. We give upper bounds on the expected number of
vertices a random basic walk will visit on the infinite graphs studied and on
their finite analogues of sufficiently large size. We then study random basic
walks on complete graphs, and prove that the class of complete graphs has
random basic walks asymptotically visit a constant fraction of the nodes. We
end with numerous conjectures and problems for future study, as well as ideas
for how to approach these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1042</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1042</id><created>2013-08-02</created><authors><author><keyname>Monu</keyname><forenames>Kafui</forenames></author><author><keyname>Ralph</keyname><forenames>Paul</forenames></author></authors><title>Beyond Gamification: Implications of Purposeful Games for the
  Information Systems Discipline</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Gamification is an emerging design principle for information systems where
game design elements are applied to non-game contexts. IS researchers have
suggested that the IS discipline must study this area but there are other
applications such as serious games, and simulations that also use games in
non-game contexts. Specifically, the management field has been using games and
simulations for years and these applications are now being supported by
information systems. We propose in this paper that we must think beyond
gamification, towards other uses of games in non-gaming contexts, which we call
purposeful gaming.
  In this paper we identify how the IS discipline can adapt to purposeful
gaming. Specifically, we show how IT artifacts, IS design, and IS theories can
be used in the purposeful gaming area. We also provide three conceptual
dimensions of purposeful gaming that can aid IS practitioners and researchers
to classify and understand purposeful games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1049</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1049</id><created>2013-08-05</created><authors><author><keyname>Kianercy</keyname><forenames>Ardeshir</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Coevolutionary networks of reinforcement-learning agents</title><categories>cs.MA cs.LG nlin.AO</categories><journal-ref>Phys. Rev. E 88, 012815 (2013)</journal-ref><doi>10.1103/PhysRevE.88.012815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model of network formation in repeated games where the
players adapt their strategies and network ties simultaneously using a simple
reinforcement-learning scheme. It is demonstrated that the coevolutionary
dynamics of such systems can be described via coupled replicator equations. We
provide a comprehensive analysis for three-player two-action games, which is
the minimum system size with nontrivial structural dynamics. In particular, we
characterize the Nash equilibria (NE) in such games and examine the local
stability of the rest points corresponding to those equilibria. We also study
general n-player networks via both simulations and analytical methods and find
that in the absence of exploration, the stable equilibria consist of star
motifs as the main building blocks of the network. Furthermore, in all stable
equilibria the agents play pure strategies, even when the game allows mixed NE.
Finally, we study the impact of exploration on learning outcomes, and observe
that there is a critical exploration rate above which the symmetric and
uniformly connected network topology becomes stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1056</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1056</id><created>2013-08-05</created><authors><author><keyname>Li</keyname><forenames>Tiancheng</forenames></author></authors><title>A Gap between Simulation and Practice for Recursive Filters: On the
  State Transition Noise</title><categories>cs.DM cs.SY</categories><comments>This is a short technical note pointing out a common unfair treatment
  of the transition noise in the discrete simulation modeling for recursive
  filters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to evaluate and compare different recursive filters, simulation is a
common tool and numerous simulation models are widely used as 'benchmark'. In
the simulation, the continuous time dynamic system is converted into a
discrete-time recursive system. As a result of this, the state indeed evolves
by Markov transitions in the simulation rather than in continuous time. One
significant issue involved with modeling of the system from practice to
simulation is that the simulation parameter, particularly e.g. the state Markov
transition noise, needs to match the iteration period of the filter. Otherwise,
the simulation performance may be far from the truth. Unfortunately, quite
commonly different-speed filters are evaluated and compared under the same
simulation model with the same state transition noise for simplicity regardless
of their real sampling periods. Here the note primarily aims at clarifying this
problem and point out that it is very necessary to use a proper simulation
noise that matches the filter's speed for evaluation and comparison under the
same simulation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1066</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1066</id><created>2013-08-05</created><authors><author><keyname>Shrager</keyname><forenames>Jeff</forenames></author></authors><title>Theoretical Issues for Global Cumulative Treatment Analysis (GCTA)</title><categories>stat.AP cs.LG</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive trials are now mainstream science. Recently, researchers have taken
the adaptive trial concept to its natural conclusion, proposing what we call
&quot;Global Cumulative Treatment Analysis&quot; (GCTA). Similar to the adaptive trial,
decision making and data collection and analysis in the GCTA are continuous and
integrated, and treatments are ranked in accord with the statistics of this
information, combined with what offers the most information gain. Where GCTA
differs from an adaptive trial, or, for that matter, from any trial design, is
that all patients are implicitly participants in the GCTA process, regardless
of whether they are formally enrolled in a trial. This paper discusses some of
the theoretical and practical issues that arise in the design of a GCTA, along
with some preliminary thoughts on how they might be approached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1068</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1068</id><created>2013-08-05</created><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Egri</keyname><forenames>Laszlo</forenames></author><author><keyname>Marx</keyname><forenames>Daniel</forenames></author></authors><title>List H-Coloring a Graph by Removing Few Vertices</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the deletion version of the list homomorphism problem, we are given graphs
G and H, a list L(v) that is a subset of V(H) for each vertex v of G, and an
integer k. The task is to decide whether there exists a subset W of V(G) of
size at most k such that there is a homomorphism from G \ W to H respecting the
lists. We show that DL-Hom(H), parameterized by k and |H|, is fixed-parameter
tractable for any (P6, C6)-free bipartite graph H; already for this restricted
class of graphs, the problem generalizes Vertex Cover, Odd Cycle Transversal,
and Vertex Multiway Cut parameterized by the size of the cutset and the number
of terminals. We conjecture that DL-Hom(H) is fixed-parameter tractable for the
class of graphs H for which the list homomorphism problem (without deletions)
is polynomial-time solvable; by a result of Feder, Hell and Huang (1999), a
graph H belongs to this class precisely if it is a bipartite graph whose
complement is a circular arc graph. We show that this conjecture is equivalent
to the fixed-parameter tractability of a single fairly natural satisfiability
problem, Clause Deletion Chain-SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1084</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1084</id><created>2013-08-05</created><updated>2014-04-22</updated><authors><author><keyname>Bradonji&#x107;</keyname><forenames>Milan</forenames></author><author><keyname>Perkins</keyname><forenames>Will</forenames></author></authors><title>On Sharp Thresholds in Random Geometric Graphs</title><categories>math.PR cs.CC cs.DM math.CO</categories><comments>Replaces an earlier version with the title &quot;Thresholds for Random
  Geometric k-SAT&quot;</comments><doi>10.4230/LIPIcs.APPROX-RANDOM.2014.500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a characterization of vertex-monotone properties with sharp
thresholds in a Poisson random geometric graph or hypergraph. As an application
we show that a geometric model of random k-SAT exhibits a sharp threshold for
satisfiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1118</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1118</id><created>2013-08-05</created><authors><author><keyname>Liao</keyname><forenames>Guoqiong</forenames></author><author><keyname>Zhao</keyname><forenames>Yuchen</forenames></author><author><keyname>Xie</keyname><forenames>Sihong</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Latent Networks Fusion based Model for Event Recommendation in Offline
  Ephemeral Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Full version of ACM CIKM2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing amount of mobile social media, offline ephemeral social
networks (OffESNs) are receiving more and more attentions. Offline ephemeral
social networks (OffESNs) are the networks created ad-hoc at a specific
location for a specific purpose and lasting for short period of time, relying
on mobile social media such as Radio Frequency Identification (RFID) and
Bluetooth devices. The primary purpose of people in the OffESNs is to acquire
and share information via attending prescheduled events. Event Recommendation
over this kind of networks can facilitate attendees on selecting the
prescheduled events and organizers on making resource planning. However,
because of lack of users preference and rating information, as well as explicit
social relations, both rating based traditional recommendation methods and
social-trust based recommendation methods can no longer work well to recommend
events in the OffESNs. To address the challenges such as how to derive users
latent preferences and social relations and how to fuse the latent information
in a unified model, we first construct two heterogeneous interaction social
networks, an event participation network and a physical proximity network.
Then, we use them to derive users latent preferences and latent networks on
social relations, including like-minded peers, co-attendees and friends.
Finally, we propose an LNF (Latent Networks Fusion) model under a pairwise
factor graph to infer event attendance probabilities for recommendation.
Experiments on an RFID-based real conference dataset have demonstrated the
effectiveness of the proposed model compared with typical solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1126</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1126</id><created>2013-08-05</created><authors><author><keyname>Lakshman</keyname><forenames>H.</forenames></author><author><keyname>Lim</keyname><forenames>W. -Q</forenames></author><author><keyname>Schwarz</keyname><forenames>H.</forenames></author><author><keyname>Marpe</keyname><forenames>D.</forenames></author><author><keyname>Kutyniok</keyname><forenames>G.</forenames></author><author><keyname>Wiegand</keyname><forenames>T.</forenames></author></authors><title>Image interpolation using Shearlet based iterative refinement</title><categories>cs.CV</categories><msc-class>94A08 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an image interpolation algorithm exploiting sparse
representation for natural images. It involves three main steps: (a) obtaining
an initial estimate of the high resolution image using linear methods like FIR
filtering, (b) promoting sparsity in a selected dictionary through iterative
thresholding, and (c) extracting high frequency information from the
approximation to refine the initial estimate. For the sparse modeling, a
shearlet dictionary is chosen to yield a multiscale directional representation.
The proposed algorithm is compared to several state-of-the-art methods to
assess its objective as well as subjective performance. Compared to the cubic
spline interpolation method, an average PSNR gain of around 0.8 dB is observed
over a dataset of 200 images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1138</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1138</id><created>2013-08-05</created><updated>2015-04-27</updated><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames></author><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>The Vectorial $\lambda$-Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a type system for the linear-algebraic $\lambda$-calculus. The
type system accounts for the linear-algebraic aspects of this extension of
$\lambda$-calculus: it is able to statically describe the linear combinations
of terms that will be obtained when reducing the programs. This gives rise to
an original type theory where types, in the same way as terms, can be
superposed into linear combinations. We prove that the resulting typed
$\lambda$-calculus is strongly normalising and features weak subject reduction.
Finally, we show how to naturally encode matrices and vectors in this typed
calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1144</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1144</id><created>2013-08-05</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Performance Limits and Practical Decoding of Interleaved Reed-Solomon
  Polar Concatenated Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications. arXiv admin note:
  substantial text overlap with arXiv:1301.7491</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scheme for concatenating the recently invented polar codes with non-binary
MDS codes, as Reed-Solomon codes, is considered. By concatenating binary polar
codes with interleaved Reed-Solomon codes, we prove that the proposed
concatenation scheme captures the capacity-achieving property of polar codes,
while having a significantly better error-decay rate. We show that for any
$\epsilon &gt; 0$, and total frame length $N$, the parameters of the scheme can be
set such that the frame error probability is less than $2^{-N^{1-\epsilon}}$,
while the scheme is still capacity achieving. This improves upon
$2^{-N^{0.5-\epsilon}}$, the frame error probability of Arikan's polar codes.
The proposed concatenated polar codes and Arikan's polar codes are also
compared for transmission over channels with erasure bursts. We provide a
sufficient condition on the length of erasure burst which guarantees failure of
the polar decoder. On the other hand, it is shown that the parameters of the
concatenated polar code can be set in such a way that the capacity-achieving
properties of polar codes are preserved. We also propose decoding algorithms
for concatenated polar codes, which significantly improve the error-rate
performance at finite block lengths while preserving the low decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1147</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1147</id><created>2013-08-05</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author></authors><title>Empirical Entropy, Minimax Regret and Minimax Risk</title><categories>math.ST cs.LG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the random design regression model with square loss. We propose a
method that aggregates empirical minimizers (ERM) over appropriately chosen
random subsets and reduces to ERM in the extreme case, and we establish sharp
oracle inequalities for its risk. We show that, under the $\epsilon^{-p}$
growth of the empirical $\epsilon$-entropy, the excess risk of the proposed
method attains the rate $n^{-\frac{2}{2+p}}$ for $p\in(0,2]$ and $n^{-1/p}$ for
$p&gt; 2$ where $n$ is the sample size. Furthermore, for $p\in(0,2]$, the excess
risk rate matches the behavior of the minimax risk of function estimation in
regression problems under the well-specified model. This yields a conclusion
that the rates of statistical estimation in well-specified models (minimax
risk) and in misspecified models (minimax regret) are equivalent in the regime
$p\in(0,2]$. In other words, for $p\in(0,2]$ the problem of statistical
learning enjoys the same minimax rate as the problem of statistical estimation.
On the contrary, for $p&gt;2$ we show that the rates of the minimax regret are, in
general, slower than for the minimax risk. Our oracle inequalities also imply
the $v\log(n/v)/n$ rates for Vapnik-Chervonenkis type classes of dimension $v$
without the usual convexity assumption on the class; we show that these rates
are optimal. Finally, for a slightly modified method, we derive a bound on the
excess risk of $s$-sparse convex aggregation improving that of (Lounici 07) and
providing the optimal rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1150</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1150</id><created>2013-08-05</created><authors><author><keyname>Wali</keyname><forenames>Ali</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>Multimodal Approach for Video Surveillance Indexing and Retrieval</title><categories>cs.MM cs.CV</categories><comments>7 pages</comments><journal-ref>Journal of Intelligent Computing, Volume: 1, Issue: 4 (December
  2010), Page: 165-175</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an overview of a multimodal system to indexing and
searching video sequence by the content that has been developed within the
REGIMVid project. A large part of our system has been developed as part of
TRECVideo evaluation. The MAVSIR platform provides High-level feature
extraction from audio-visual content and concept/event-based video retrieval.
We illustrate the architecture of the system as well as provide an overview of
the descriptors supported to date. Then we demonstrate the usefulness of the
toolbox in the context of feature extraction, concepts/events learning and
retrieval in large collections of video surveillance dataset. The results are
encouraging as we are able to get good results on several event categories,
while for all events we have gained valuable insights and experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1158</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1158</id><created>2013-08-05</created><authors><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Paasivaara</keyname><forenames>Maria</forenames></author></authors><title>COINs change leaders - Lessons Learned from a Distributed Course</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the communication network of 50 students from five
universities in three countries participating in a joint course on
Collaborative Innovation Networks (COINs). Students formed ten teams.
Interaction variables calculated from the e-mail archive of individual team
members predict the level of creativity of the team. Oscillating leadership,
where members switch between central and peripheral roles is the best predictor
of creativity, it is complemented by the variance in the amount of sending or
receiving information, and by answering quickly, and positive language. We
verify our automatically generated creativity metrics with interviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1160</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1160</id><created>2013-08-05</created><authors><author><keyname>Frick</keyname><forenames>Karin</forenames></author><author><keyname>Guertler</keyname><forenames>Detlef</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author></authors><title>Coolhunting for the World's Thought Leaders</title><categories>cs.SI cs.SY physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which thinkers are we guided by? A novel &quot;Thought Leader Map&quot; shows the
select group of people with real influence who are setting the trends in the
market for ideas. The influencers in philosophy, sociology, economics, and the
&quot;hard sciences&quot; have been identified by a Delphi process, asking 50 thought
leaders to name their peers. The importance of the influencers is calculated by
constructing a co-occurrence network in the Blogosphere. Our main insight is
that the era of the great authorities seems to be over. Major thought leaders
are rare - the picture is composed of many specialists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1162</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1162</id><created>2013-08-05</created><authors><author><keyname>Hybbeneth</keyname><forenames>Stori</forenames></author><author><keyname>Brunnberg</keyname><forenames>Dirk</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author></authors><title>Increasing Knowledge Worker Efficiency through a &quot;Virtual Mirror&quot; of the
  Social Network</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a case study describing the combination of manual
survey-based and e-mail-based social network analysis. The goal of the project
was to increase collaboration efficiency in a team of consultants of a major
high tech manufacturer. By analyzing the social network of a team of 42
consultants and comparing it with their utilization as the dependent variable,
their efficiency in working together was improved in various way by bridging
structure holes and eliminating bottlenecks, reducing stress for overburdened
individuals, connecting isolated individuals and identifying the best network
structures for high utilization and increased job satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1164</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1164</id><created>2013-08-05</created><authors><author><keyname>Brunberg</keyname><forenames>Dirk</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Giacomelli</keyname><forenames>Gianni</forenames></author></authors><title>Predicting Client Satisfaction through (E-Mail) Network Analysis: The
  Communication Score Card</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study seeks to better understand the network characteristics of client
support teams by analyzing the teams' e-mail communication networks and
comparing it to client organization's satisfaction. In collaboration with a
large service provider we studied the impact of network properties on the
satisfaction of client organizations. In particular, we found that social
network metrics correlate with client satisfaction as measured by Net Promoter
Score (NPS). A Communication Score Card is suggested as a dashboard to
continuously measure client satisfaction, illustrating that data-driven
analysis might help improving service providers' service quality management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1166</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1166</id><created>2013-08-05</created><authors><author><keyname>Futterer</keyname><forenames>Tobias</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Malhotra</keyname><forenames>Tushar</forenames></author><author><keyname>Mfula</keyname><forenames>Harrison</forenames></author><author><keyname>Packmohr</keyname><forenames>Karsten</forenames></author><author><keyname>Schultheiss</keyname><forenames>Stefan</forenames></author></authors><title>WikiPulse - A News-Portal Based on Wikipedia</title><categories>cs.IR cs.DL cs.SI physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More and more user-generated content is complementing conventional
journalism. While we don't think that CNN or New York Times and its
professional journalists will disappear anytime soon, formidable competition is
emerging through humble Wikipedia editors. In earlier work (Becker 2012), we
found that entertainment and sports news appeared on average about two hours
earlier on Wikipedia than on CNN and Reuters online. In this project we build a
news-reader that automatically identifies late-breaking news among the most
recent Wikipedia articles and then displays it on a dedicated Web site.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1168</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1168</id><created>2013-08-05</created><authors><author><keyname>Lotrecchiano</keyname><forenames>Gaetano R.</forenames></author></authors><title>Role and Discipline Relationships in a Transdisciplinary Biomedical
  Team: Structuration, Values Override and Context Scaffolding</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though accepted that &quot;team science&quot; is needed to tackle and conquer the
health problems that are plaguing our society significant empirical evidence of
team mechanisms and functional dynamics is still lacking in abundance. Through
grounded methods the relationship between scientific disciplines and team roles
was observed in a United States National Institutes of Health-funded (NIH)
research consortium. Interviews and the Organizational Culture Assessment
Instrument (OCAI) were employed.. Findings show strong role and discipline
idiosyncrasies that when viewed separately provide different insights into team
functioning and change receptivity. When considered simultaneously,
value-latent characteristics emerged showing self-perceived contributions to
the team. This micro/meso analysis suggests that individual participation in
team level interactions can inform the structuration of roles and disciplines
in an attempt to tackle macro level problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1175</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1175</id><created>2013-08-05</created><authors><author><keyname>Furukawazono</keyname><forenames>Tomoki</forenames></author><author><keyname>Seshimo</keyname><forenames>Shota</forenames></author><author><keyname>Muramatsu</keyname><forenames>Daiki</forenames></author><author><keyname>Iba</keyname><forenames>Takashi</forenames></author></authors><title>Designing a Pattern Language For Surviving Earthquakes</title><categories>cs.CY</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed the Survival Language, a pattern language to
support survival when a catastrophic earthquake occurs. This proposal comes
from the problem that the tragedies of earthquakes are repeated, because
knowledge and wisdom on how to prepare for an earthquake and what to do during
an earthquake have not been passed down sufficiently. This paper presented the
four patterns of the Survival Language: &quot;Daily Use of Reserves,&quot; &quot;Life over
Furniture,&quot; &quot;Evacuation Initiator,&quot; and &quot;Kick Signal.&quot; In addition, we
described that the Survival Language is created and used by collaborations
because a pattern language has been a tool for collaboration since it was
presented by Christopher Alexander.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1176</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1176</id><created>2013-08-05</created><authors><author><keyname>Fabrega</keyname><forenames>Jorge</forenames></author><author><keyname>Sajuria</keyname><forenames>Javier</forenames></author></authors><title>The Emergence of Political Discourse on Digital Networks: The Case of
  the Occupy Movement</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does political discourse spread in digital networks? Can we empirically
test if certain conceptual frames of social movements have a correlate on their
online discussion networks? Through an analysis of the Twitter data from the
Occupy movement, this paper describes the formation of political discourse over
time. Building on a previous set of concepts - derived from theoretical
discussions about the movement and its roots - we analyse the data to observe
when those concepts start to appear within the networks, who are those Twitter
users responsible for them, and what are the patterns through which those
concepts spread. Preliminary evidence shows that, although there are some signs
of opportunistic behaviour among activists, most of them are central nodes from
the onset of the network, and shape the discussions across time. These central
activists do not only start the conversations around given frames, but also
sustain over time and become key members of the network. From here, we aim to
provide a thorough account of the &quot;travel&quot; of political discourse, and the
correlate of online conversational networks with theoretical accounts of the
movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1178</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1178</id><created>2013-08-05</created><authors><author><keyname>Iba</keyname><forenames>Takashi</forenames></author></authors><title>Pattern Languages as Media for the Creative Society</title><categories>cs.CY</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes new languages for basic skills in the Creative Society,
where people create their own goods, tools, concepts, knowledge, and mechanisms
with their own hands: the skills of learning, presentation, and collaboration.
These languages are written as a pattern language, which is a way of describing
the tacit practical knowledge. In this paper, a new type of pattern languages
are proposed as &quot;Pattern Languages 3.0&quot; and three examples are introduced:
Learning Patterns, Collaboration Patterns, and Presentation Patterns. By
analyzing the functions with the social systems theory and the creative systems
theory, pattern languages are considered as communication media and discovery
media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1187</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1187</id><created>2013-08-06</created><authors><author><keyname>Soltani-Farani</keyname><forenames>Ali</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Hosseini</keyname><forenames>Seyyed Abbas</forenames></author></authors><title>Spatial-Aware Dictionary Learning for Hyperspectral Image Classification</title><categories>cs.CV cs.LG</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a structured dictionary-based model for hyperspectral
data that incorporates both spectral and contextual characteristics of a
spectral sample, with the goal of hyperspectral image classification. The idea
is to partition the pixels of a hyperspectral image into a number of spatial
neighborhoods called contextual groups and to model each pixel with a linear
combination of a few dictionary elements learned from the data. Since pixels
inside a contextual group are often made up of the same materials, their linear
combinations are constrained to use common elements from the dictionary. To
this end, dictionary learning is carried out with a joint sparse regularizer to
induce a common sparsity pattern in the sparse coefficients of each contextual
group. The sparse coefficients are then used for classification using a linear
SVM. Experimental results on a number of real hyperspectral images confirm the
effectiveness of the proposed representation for hyperspectral image
classification. Moreover, experiments with simulated multispectral data show
that the proposed model is capable of finding representations that may
effectively be used for classification of multispectral-resolution samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1199</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1199</id><created>2013-08-06</created><authors><author><keyname>Vichare</keyname><forenames>Abhijat</forenames></author></authors><title>Intensional view of General Single Processor Operating Systems</title><categories>cs.OS</categories><comments>17 pages, 3 figures. Condensed and improved version of
  http://arxiv.org/abs/1112.4451 for submission to SOSP'13. arXiv admin note:
  substantial text overlap with arXiv:1112.4451</comments><acm-class>D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating systems are currently viewed ostensively. As a result they mean
different things to different people. The ostensive character makes it is hard
to understand OSes formally. An intensional view can enable better formal work,
and also offer constructive support for some important problems, e.g. OS
architecture. This work argues for an intensional view of operating systems. It
proposes to overcome the current ostensive view by defining an OS based on
formal models of computation, and also introduces some principles. Together
these are used to develop a framework of algorithms of single processor OS
structure using an approach similar to function level programming. In this
abridged paper we illustrate the essential approach, discuss some advantages
and limitations and point out some future possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1201</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1201</id><created>2013-08-06</created><updated>2014-03-03</updated><authors><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>Zampieri</keyname><forenames>Sandro</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Controllability Metrics, Limitations and Algorithms for Complex Networks</title><categories>cs.SY math.OC physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of controlling complex networks, that is, the
joint problem of selecting a set of control nodes and of designing a control
input to steer a network to a target state. For this problem (i) we propose a
metric to quantify the difficulty of the control problem as a function of the
required control energy, (ii) we derive bounds based on the system dynamics
(network topology and weights) to characterize the tradeoff between the control
energy and the number of control nodes, and (iii) we propose an open-loop
control strategy with performance guarantees. In our strategy we select control
nodes by relying on network partitioning, and we design the control input by
leveraging optimal and distributed control techniques. Our findings show
several control limitations and properties. For instance, for Schur stable and
symmetric networks: (i) if the number of control nodes is constant, then the
control energy increases exponentially with the number of network nodes, (ii)
if the number of control nodes is a fixed fraction of the network nodes, then
certain networks can be controlled with constant energy independently of the
network dimension, and (iii) clustered networks may be easier to control
because, for sufficiently many control nodes, the control energy depends only
on the controllability properties of the clusters and on their coupling
strength. We validate our results with examples from power networks, social
networks, and epidemics spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1204</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1204</id><created>2013-08-06</created><authors><author><keyname>Eggert</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author><author><keyname>Wilke</keyname><forenames>Thomas</forenames></author></authors><title>Complexity and Unwinding for Intransitive Noninterference</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers several definitions of information flow security for
intransitive policies from the point of view of the complexity of verifying
whether a finite-state system is secure. The results are as follows. Checking
(i) P-security (Goguen and Meseguer), (ii) IP-security (Haigh and Young), and
(iii) TA-security (van der Meyden) are all in PTIME, while checking TO-security
(van der Meyden) is undecidable, as is checking ITO-security (van der Meyden).
The most important ingredients in the proofs of the PTIME upper bounds are new
characterizations of the respective security notions, which also lead to new
unwinding proof techniques that are shown to be sound and complete for these
notions of security, and enable the algorithms to return simple
counter-examples demonstrating insecurity. Our results for IP-security improve
a previous doubly exponential bound of Hadj-Alouane et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1206</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1206</id><created>2013-08-06</created><authors><author><keyname>Prashanth</keyname><forenames>B. U. V</forenames></author><author><keyname>Pandurangaiah</keyname><forenames>Y.</forenames></author></authors><title>Generation of Secret Key for Physical Layer to Evaluate Channel
  Characteristics in Wireless Communications</title><categories>cs.CR</categories><comments>5 pages, 2 figures, proceedings of International Conference ERCICA
  2013 pp: 251-255,Published by Elsevier Ltd</comments><report-no>ISBN:9789351071020</report-no><journal-ref>Proceedings of International Conference- ERCICA 2013 pp:
  251-255,Published by Elsevier Ltd</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript aims to generate a secret key for a PHY layer to evaluate the
channel characteristics in wireless communications. An algorithmic approach is
adopted for multimedia encryption to generate a secret key between two entities
communicating with each other in a real time simulation environment such as
MATLAB. Two classes of PHY key generation schemes are analyzed for designing
the algorithm such as received-signal-strength-based and channel- phase-based
protocols. We present a performance comparison of them in terms of key
disagreement probability, key generation rate, key bit randomness, scalability,
and implementation issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1212</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1212</id><created>2013-08-06</created><authors><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Online Algorithms for Basestation Allocation</title><categories>cs.NI cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of {\it online algorithms} for assigning mobile users to basestations
is considered with the objective of maximizing the sum-rate, when all users
associated to any one basestation equally share each basestation's resources.
Each user on its arrival reveals the rates it can obtain if connected to each
of the basestations, and the problem is to assign each user to any one
basestation irrevocably so that the sum-rate is maximized at the end of all
user arrivals, without knowing the future user arrival or rate information or
its statistics at each user arrival. Online algorithms with constant factor
loss in comparison to offline algorithms (that know both the user arrival and
user rates profile in advance) are derived. The proposed online algorithms are
motivated from the famous online k-secretary problem and online maximum weight
matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1214</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1214</id><created>2013-08-06</created><authors><author><keyname>Alam</keyname><forenames>Sk. Shariful</forenames></author><author><keyname>Marcenaro</keyname><forenames>Lucio</forenames></author><author><keyname>Regazzoni</keyname><forenames>Carlo</forenames></author></authors><title>Opportunistic Spectrum Sensing and Transmissions</title><categories>cs.NI cs.ET</categories><journal-ref>Cognitive Radio and Interference Management: Technology and
  Strategy. IGI Global, 2013. 1-28</journal-ref><doi>10.4018/978-1-4666-2005-6.ch001</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays, cognitive radio is one of the most promising paradigms in the arena
of wireless communications, as it aims at the proficient use of radio
resources. Proper utilization of the radio spectrum requires dynamic spectrum
accessing. To this end, spectrum sensing is undoubtedly necessary. In this
chapter, various approaches for dynamic spectrum access scheme are presented,
together with a survey of spectrum sensing methodologies for cognitive radio.
Moreover, the challenges are analyzed that are associated with spectrum sensing
and dynamic spectrum access techniques. Sensing beacon transmitted from
different cognitive terminals creates significant interference to the primary
users if proper precautions have not be not taken into consideration.
Consequently, cognitive radio transmitter power control will be finally
addressed to analyze energy efficiency aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1224</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1224</id><created>2013-08-06</created><authors><author><keyname>Stupar</keyname><forenames>Aleksandar</forenames></author><author><keyname>Michel</keyname><forenames>Sebastian</forenames></author></authors><title>Benchmarking Soundtrack Recommendation Systems with SRBench</title><categories>cs.IR cs.MM</categories><comments>Extended version of the CIKM 2013 paper: SRbench-A Benchmark for
  Soundtrack Recommendation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a benchmark to evaluate the retrieval performance of soundtrack
recommendation systems is proposed. Such systems aim at finding songs that are
played as background music for a given set of images. The proposed benchmark is
based on preference judgments, where relevance is considered a continuous
ordinal variable and judgments are collected for pairs of songs with respect to
a query (i.e., set of images). To capture a wide variety of songs and images,
we use a large space of possible music genres, different emotions expressed
through music, and various query-image themes. The benchmark consists of two
types of relevance assessments: (i) judgments obtained from a user study, that
serve as a &quot;gold standard&quot; for (ii) relevance judgments gathered through
Amazon's Mechanical Turk. We report on an analysis of relevance judgments based
on different levels of user agreement and investigate the performance of two
state-of-the-art soundtrack recommendation systems using the proposed
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1228</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1228</id><created>2013-08-06</created><updated>2013-09-20</updated><authors><author><keyname>Winter</keyname><forenames>Joost</forenames><affiliation>CWI</affiliation></author><author><keyname>Rutten</keyname><forenames>Jan J. M.</forenames><affiliation>CWI/Radboud University Nijmegen</affiliation></author><author><keyname>Bonsangue</keyname><forenames>Marcello M.</forenames><affiliation>Leiden University/CWI</affiliation></author></authors><title>Coalgebraic Characterizations of Context-Free Languages</title><categories>cs.LO</categories><comments>39 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  13, 2013) lmcs:739</journal-ref><doi>10.2168/LMCS-9(3:14)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we provide three coalgebraic characterizations of the class
of context-free languages, each based on the idea of adding coalgebraic
structure to an existing algebraic structure by specifying output-derivative
pairs. Final coalgebra semantics then gives an interpretation function into the
final coalgebra of all languages with the usual output and derivative
operations. The first characterization is based on systems, where each
derivative is given as a finite language over the set of nonterminals; the
second characterization on systems where derivatives are given as elements of a
term-algebra; and the third characterization is based on adding coalgebraic
structure to a class of closed (unique) fixed point expressions. We prove
equivalences between these characterizations, discuss the generalization from
languages to formal power series, as well as the relationship to the
generalized powerset construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1246</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1246</id><created>2013-08-06</created><updated>2015-11-23</updated><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Seo</keyname><forenames>Jeongyoon</forenames></author><author><keyname>Kang</keyname><forenames>Daeseong</forenames></author></authors><title>Bounded-Choice Statements for User Interaction in Imperative and
  Object-Oriented Programming</title><categories>cs.PL</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding versatile interactions to imperative programming -- C, Java and
Android -- is an essential task. Unfortunately, existing languages provide only
limited constructs for user interaction. These constructs are usually in the
form of $unbounded$ quantification. For example, existing languages can take
the keyboard input from the user only via the $read(x)/scan(x)$ construct. Note
that the value of $x$ is unbounded in the sense that $x$ can have any value.
This construct is thus not useful for applications with bounded inputs. To
support bounded choices, we propose new bounded-choice statements for user
interation. Each input device (the keyboard, the mouse, the touch, $...$)
naturally requires a new bounded-choice statement. To make things simple,
however, we focus on a bounded-choice statement for keyboard -- kchoose -- to
allow for more controlled and more guided participation from the user. It is
straightforward to adjust our idea to other input devices. We illustrate our
idea via Java(BI), an extension of the core Java with a new bounded-choice
statement for the keyboard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1247</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1247</id><created>2013-08-06</created><authors><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Rappoccio</keyname><forenames>Salvatore</forenames></author><author><keyname>Stenson</keyname><forenames>Kevin</forenames></author><author><keyname>Wittich</keyname><forenames>Peter</forenames></author></authors><title>The Need for an R&amp;D and Upgrade Program for CMS Software and Computing</title><categories>hep-ex cs.DC physics.ins-det</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the next ten years, the physics reach of the Large Hadron Collider (LHC)
at the European Organization for Nuclear Research (CERN) will be greatly
extended through increases in the instantaneous luminosity of the accelerator
and large increases in the amount of collected data. Due to changes in the way
Moore's Law computing performance gains have been realized in the past decade,
an aggressive program of R&amp;D is needed to ensure that the computing capability
of CMS will be up to the task of collecting and analyzing this data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1257</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1257</id><created>2013-08-06</created><authors><author><keyname>Ba&#xf1;os</keyname><forenames>Raquel A.</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Gonz&#xe1;lez-Bail&#xf3;n</keyname><forenames>Sandra</forenames></author></authors><title>Diffusion Dynamics with Changing Network Composition</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 6 figures, 2 tables</comments><doi>10.3390/e15114553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze information diffusion using empirical data that tracks online
communication around two instances of mass political mobilization, including
the year that lapsed in-between the protests. We compare the global properties
of the topological and dynamic networks through which communication took place
as well as local changes in network composition. We show that changes in
network structure underlie aggregated differences on how information diffused:
an increase in network hierarchy is accompanied by a reduction in the average
size of cascades. The increasing hierarchy affects not only the underlying
communication topology but also the more dynamic structure of information
exchange; the increase is especially noticeable amongst certain categories of
nodes (or users). This suggests that the relationship between the structure of
networks and their function in diffusing information is not as straightforward
as some theoretical models of diffusion in networks imply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1259</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1259</id><created>2013-08-06</created><authors><author><keyname>Karimi</keyname><forenames>Mehdi</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author></authors><title>On Characterization of Elementary Trapping Sets of Variable-Regular LDPC
  Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the graphical structure of elementary trapping sets
(ETS) of variable-regular low-density parity-check (LDPC) codes. ETSs are known
to be the main cause of error floor in LDPC coding schemes. For the set of LDPC
codes with a given variable node degree $d_l$ and girth $g$, we identify all
the non-isomorphic structures of an arbitrary class of $(a,b)$ ETSs, where $a$
is the number of variable nodes and $b$ is the number of odd-degree check nodes
in the induced subgraph of the ETS. Our study leads to a simple
characterization of dominant classes of ETSs (those with relatively small
values of $a$ and $b$) based on short cycles in the Tanner graph of the code.
For such classes of ETSs, we prove that any set ${\cal S}$ in the class is a
layered superset (LSS) of a short cycle, where the term &quot;layered&quot; is used to
indicate that there is a nested sequence of ETSs that starts from the cycle and
grows, one variable node at a time, to generate ${\cal S}$. This
characterization corresponds to a simple search algorithm that starts from the
short cycles of the graph and finds all the ETSs with LSS property in a
guaranteed fashion. Specific results on the structure of ETSs are presented for
$d_l = 3, 4, 5, 6$, $g = 6, 8$ and $a, b \leq 10$ in this paper. The results of
this paper can be used for the error floor analysis and for the design of LDPC
codes with low error floors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1262</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1262</id><created>2013-08-06</created><authors><author><keyname>Marinho</keyname><forenames>Eraldo Pereira</forenames></author></authors><title>Pattern recognition issues on anisotropic smoothed particle
  hydrodynamics</title><categories>cs.AI cs.CG physics.comp-ph</categories><comments>Submitted to the International Conference on Mathematical Modeling in
  Physical Sciences - 2013</comments><msc-class>68T05, 68T30, 68U20 (Primary), 65C60, 05C42, 74E10, 85-08
  (Secondary)</msc-class><acm-class>I.2.6; I.2.11; I.6.1</acm-class><journal-ref>2014 J. Phys.: Conf. Ser. 490 012063</journal-ref><doi>10.1088/1742-6596/490/1/012063</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This is a preliminary theoretical discussion on the computational
requirements of the state of the art smoothed particle hydrodynamics (SPH) from
the optics of pattern recognition and artificial intelligence. It is pointed
out in the present paper that, when including anisotropy detection to improve
resolution on shock layer, SPH is a very peculiar case of unsupervised machine
learning. On the other hand, the free particle nature of SPH opens an
opportunity for artificial intelligence to study particles as agents acting in
a collaborative framework in which the timed outcomes of a fluid simulation
forms a large knowledge base, which might be very attractive in computational
astrophysics phenomenological problems like self-propagating star formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1279</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1279</id><created>2013-08-06</created><updated>2014-10-30</updated><authors><author><keyname>Brown</keyname><forenames>Russell A.</forenames></author></authors><title>Barycentric Coordinates as Interpolants</title><categories>cs.GR</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barycentric coordinates are frequently used as interpolants to shade computer
graphics images. A simple equation transforms barycentric coordinates from
screen space into eye space in order to undo the perspective transformation and
permit accurate interpolative shading of texture maps. This technique is
amenable to computation using a block-normalized integer representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1281</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1281</id><created>2013-08-06</created><updated>2013-08-11</updated><authors><author><keyname>Matsuzuka</keyname><forenames>Ko</forenames></author><author><keyname>Isaku</keyname><forenames>Taichi</forenames></author><author><keyname>Nishina</keyname><forenames>Satoshi</forenames></author><author><keyname>Iba</keyname><forenames>Takashi</forenames></author></authors><title>Global Life Patterns: A Methodology for Designing a Personal Global Life</title><categories>cs.CY</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the Global Life Patterns, a methodology to support
people to maintain individuality and design their own actions in respect to the
&quot;globalizing&quot; society - a Global Life. Today, globalization requires each
person to maintain individuality in respect to Globalization. Because methods
to acquire such ability are tacit, a clarified methodology to live a Global
Life is needed.This methodology is based on the format of a Pattern Language -
specifically, this methodology is in the format of the Pattern Language 3.0
(Iba, 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1284</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1284</id><created>2013-08-06</created><authors><author><keyname>Marisca</keyname><forenames>Eduardo</forenames></author></authors><title>The Networks Are Out There: Building Cultural and Economic Resilience
  Through Informal Communities of Practice</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the possibilities offered by informal communities of
practice to operate as &quot;prototyping spaces&quot; for innovation in the context of
developing economies. It begins by looking at the concept of &quot;economic
complexity&quot; and how it is useful in both guiding the priorities and evaluating
the challenges developing economies face when attempting to drive growth and
build measures of resilience, and raises the question of how these economies
can both introduce higher complexity activities while compensating for
latecomer disadvantages versus more complex economies. It then examines in
detail the case of the Twin Eagles Group, a Peruvian video game development
community in the 1990s, and how they reverse engineered technologies and global
practices to pursue their creative objectives. Based on this case, it concludes
by laying out some of the methodological challenges associated to researching
this sort of community because of the multi-sited nature of their activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1287</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1287</id><created>2013-08-06</created><authors><author><keyname>Oloritun</keyname><forenames>Rahman</forenames><affiliation>Sandy</affiliation></author><author><keyname>Alex</keyname><affiliation>Sandy</affiliation></author><author><keyname>Pentland</keyname></author><author><keyname>Khayal</keyname><forenames>Inas</forenames></author></authors><title>Dynamics of Human Social Networks: People, Time, Relationships, and
  Places</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of advanced social interaction sensing technologies provides
fine grained data for social network analysis. Although traditional methods of
gathering social network data may be subject to human ability to recall social
details, people's rating of their closeness to persons in their network is
important. This study assesses the relationship amidst closeness ratings,
sensed interactions and shared places of recreation. The study found that
people tend to give high closeness ratings to people with whom they spend more
time. Shared places are correlated to social closeness ratings but not to
length of interactions. The results of this study highlight the importance of
sensed interactions and closeness ratings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1292</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1292</id><created>2013-08-06</created><authors><author><keyname>Wells</keyname><forenames>Elysia</forenames></author></authors><title>Science Fiction as a Worldwide Phenomenon: A Study of International
  Creation, Consumption and Dissemination</title><categories>cs.DL cs.CL cs.SI physics.soc-ph</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the international nature of science fiction. The focus of
this research is to determine whether science fiction is primarily English
speaking and Western or global; being created and consumed by people in
non-Western, non-English speaking countries? Science fiction's international
presence was found in three ways, by network analysis, by examining a online
retailer and with a survey. Condor, a program developed by GalaxyAdvisors was
used to determine if science fiction is being talked about by non-English
speakers. An analysis of the international Amazon.com websites was done to
discover if it was being consumed worldwide. A survey was also conducted to see
if people had experience with science fiction. All three research methods
revealed similar results. Science fiction was found to be international, with
science fiction creators originating in different countries and writing in a
host of different languages. English and non-English science fiction was being
created and consumed all over the world, not just in the English speaking West.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1303</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1303</id><created>2013-08-05</created><authors><author><keyname>Rajan</keyname><forenames>Arokia Paul</forenames></author><author><keyname>Shanmugapriyaa</keyname></author></authors><title>Evolution of Cloud Storage as Cloud Computing Infrastructure Service</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprises are driving towards less cost, more availability, agility,
managed risk - all of which is accelerated towards Cloud Computing. Cloud is
not a particular product, but a way of delivering IT services that are
consumable on demand, elastic to scale up and down as needed, and follow a
pay-for-usage model. Out of the three common types of cloud computing service
models, Infrastructure as a Service (IaaS) is a service model that provides
servers, computing power, network bandwidth and Storage capacity, as a service
to their subscribers. Cloud can relate to many things but without the
fundamental storage pieces, which is provided as a service namely Cloud
Storage, none of the other applications is possible. This paper introduces
Cloud Storage, which covers the key technologies in cloud computing and Cloud
Storage, management insights about cloud computing, different types of cloud
services, driving forces of cloud computing and cloud storage, advantages and
challenges of cloud storage and concludes by pinpointing few challenges to be
addressed by the cloud storage providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1324</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1324</id><created>2013-08-06</created><updated>2014-03-03</updated><authors><author><keyname>Beros</keyname><forenames>Achilles A.</forenames></author></authors><title>A DNC function that computes no effectively bi-immune set</title><categories>math.LO cs.LO</categories><comments>7 pages, 1 figures</comments><msc-class>03D28</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Diagonally Non-Computable Functions and Bi-Immunity, Carl Jockusch and
Andrew Lewis proved that every DNC function computes a bi-immune set. They
asked whether every DNC function computes an effectively bi-immune set. We
construct a DNC function that computes no effectively bi-immune set, thereby
answering their question in the negative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1336</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1336</id><created>2013-08-06</created><authors><author><keyname>Vogt</keyname><forenames>Hendrik</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Secret-key generation from wireless channels: Mind the reflections</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret-key generation in a wireless environment exploiting the randomness and
reciprocity of the channel gains is considered. A new channel model is proposed
which takes into account the effect of reflections (or re-radiations) from
receive antenna elements, thus capturing an physical property of practical
antennas. It turns out that the reflections have a deteriorating effect on the
achievable secret-key rate between the legitimate nodes at high
signal-to-noise-power-ratio (SNR). The insights provide guidelines in the
design and operation of communication systems using the properties of the
wireless channel to prevent eavesdropping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1343</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1343</id><created>2013-08-06</created><authors><author><keyname>Schnetter</keyname><forenames>Erik</forenames></author></authors><title>Performance and Optimization Abstractions for Large Scale Heterogeneous
  Systems in the Cactus/Chemora Framework</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe a set of lower-level abstractions to improve performance on
modern large scale heterogeneous systems. These provide portable access to
system- and hardware-dependent features, automatically apply dynamic
optimizations at run time, and target stencil-based codes used in finite
differencing, finite volume, or block-structured adaptive mesh refinement
codes.
  These abstractions include a novel data structure to manage refinement
information for block-structured adaptive mesh refinement, an iterator
mechanism to efficiently traverse multi-dimensional arrays in stencil-based
codes, and a portable API and implementation for explicit SIMD vectorization.
  These abstractions can either be employed manually, or be targeted by
automated code generation, or be used via support libraries by compilers during
code generation. The implementations described below are available in the
Cactus framework, and are used e.g. in the Einstein Toolkit for relativistic
astrophysics simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1351</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1351</id><created>2013-08-06</created><updated>2015-04-12</updated><authors><author><keyname>Issac</keyname><forenames>Davis</forenames></author><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author></authors><title>An $O^*(1.0821^n)$-Time Algorithm for Computing Maximum Independent Set
  in Graphs with Bounded Degree 3</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $O^*(1.0821^n)$-time, polynomial space algorithm for computing
Maximum Independent Set in graphs with bounded degree 3. This improves all the
previous running time bounds known for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1358</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1358</id><created>2013-08-06</created><authors><author><keyname>Vieira</keyname><forenames>Gustavo M. D.</forenames></author><author><keyname>Buzato</keyname><forenames>Luiz E.</forenames></author></authors><title>The Performance of Paxos and Fast Paxos</title><categories>cs.DC</categories><comments>14 pages, published in the Proc. of the 27th Brazilian Symposium on
  Computer Networks, Recife, Brazil, May 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paxos and Fast Paxos are optimal consensus algorithms that are simple and
elegant, while suitable for efficient implementation. In this paper, we compare
the performance of both algorithms in failure-free and failure-prone runs using
Treplica, a general replication toolkit that implements these algorithms in a
modular and efficient manner. We have found that Paxos outperforms Fast Paxos
for small number of replicas and that collisions are not the cause of this
performance difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1365</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1365</id><created>2013-08-06</created><authors><author><keyname>Avil&#xe9;s</keyname><forenames>Mauricio Gonz&#xe1;lez</forenames></author><author><keyname>Avil&#xe9;s</keyname><forenames>Jos&#xe9; Juan Gonz&#xe1;lez</forenames></author></authors><title>Mathematical model of concentrating solar cooker</title><categories>cs.CE</categories><comments>Keywords: Solar cooker, thermal model; mathematical model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this work is to obtain a mathematical model consistent
with the thermal behavior of concentrating solar cookers, such as
Jorhejpataranskua. We also want to simulate different conditions respect to the
parameters involved of several materials for its construction and efficiency.
The model is expressed in terms of a coupled nonlinear system of differential
equations which are solved using Mathematica 8. The results obtained by our
model are compared with measurements of solar cooker in field testing
operation. We obtained good results in agreement with experimental data.
Moreover, the simulation results are used by calculating cooking power and
standardized cooking power of solar cooker for different parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1374</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1374</id><created>2013-08-06</created><authors><author><keyname>Oh</keyname><forenames>Hyuntaek</forenames></author></authors><title>Bayesian ensemble learning for image denoising</title><categories>cs.CV</categories><comments>computer vision and image understanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural images are often affected by random noise and image denoising has
long been a central topic in Computer Vision. Many algorithms have been
introduced to remove the noise from the natural images, such as Gaussian,
Wiener filtering and wavelet thresholding. However, many of these algorithms
remove the fine edges and make them blur. Recently, many promising denoising
algorithms have been introduced such as Non-local Means, Fields of Experts, and
BM3D. In this paper, we explore Bayesian method of ensemble learning for image
denoising. Ensemble methods seek to combine multiple different algorithms to
retain the strengths of all methods and the weaknesses of none. Bayesian
ensemble models are Non-local Means and Fields of Experts, the very successful
recent algorithms. The Non-local Means presumes that the image contains an
extensive amount of self-similarity. The approach of the Fields of Experts
model extends traditional Markov Random Field model by learning potential
functions over extended pixel neighborhoods. The two models are implemented and
image denoising is performed on natural images. The experimental results
obtained are used to compare with the single algorithm and discuss the ensemble
learning and their approaches. Comparing to the results of Non-local Means and
Fields of Experts, Ensemble learning showed improvement nearly 1dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1382</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1382</id><created>2013-08-06</created><authors><author><keyname>Deng</keyname><forenames>Xiaotie</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul</forenames></author><author><keyname>Sun</keyname><forenames>Yang</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author><author><keyname>Zhang</keyname><forenames>Jinshan</forenames></author></authors><title>Pricing Ad Slots with Consecutive Multi-unit Demand</title><categories>cs.GT</categories><comments>27pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the optimal pricing problem for a model of the rich media
advertisement market, as well as other related applications. In this market,
there are multiple buyers (advertisers), and items (slots) that are arranged in
a line such as a banner on a website. Each buyer desires a particular number of
{\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on
the ad only) while each slot $j$ has a quality $q_j$ (dependent on the position
only such as click-through rate in position auctions). Hence, the valuation of
the buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and
the prices in order to maximize the total revenue of the market maker.
  A key difference from the traditional position auction is the advertiser's
requirement of a fixed number of consecutive slots. Consecutive slots may be
needed for a large size rich media ad. We study three major pricing mechanisms,
the Bayesian pricing model, the maximum revenue market equilibrium model and an
envy-free solution model. Under the Bayesian model, we design a polynomial time
computable truthful mechanism which is optimum in revenue. For the market
equilibrium paradigm, we find a polynomial time algorithm to obtain the maximum
revenue market equilibrium solution. In envy-free settings, an optimal solution
is presented when the buyers have the same demand for the number of consecutive
slots. We conduct a simulation that compares the revenues from the above
schemes and gives convincing results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1385</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1385</id><created>2013-08-06</created><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Efficient Algorithms for Privately Releasing Marginals via Convex
  Relaxations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a database of $n$ people, each represented by a bit-string of length
$d$ corresponding to the setting of $d$ binary attributes. A $k$-way marginal
query is specified by a subset $S$ of $k$ attributes, and a $|S|$-dimensional
binary vector $\beta$ specifying their values. The result for this query is a
count of the number of people in the database whose attribute vector restricted
to $S$ agrees with $\beta$.
  Privately releasing approximate answers to a set of $k$-way marginal queries
is one of the most important and well-motivated problems in differential
privacy. Information theoretically, the error complexity of marginal queries is
well-understood: the per-query additive error is known to be at least
$\Omega(\min\{\sqrt{n},d^{\frac{k}{2}}\})$ and at most
$\tilde{O}(\min\{\sqrt{n} d^{1/4},d^{\frac{k}{2}}\})$. However, no polynomial
time algorithm with error complexity as low as the information theoretic upper
bound is known for small $n$. In this work we present a polynomial time
algorithm that, for any distribution on marginal queries, achieves average
error at most $\tilde{O}(\sqrt{n} d^{\frac{\lceil k/2 \rceil}{4}})$. This error
bound is as good as the best known information theoretic upper bounds for
$k=2$. This bound is an improvement over previous work on efficiently releasing
marginals when $k$ is small and when error $o(n)$ is desirable. Using private
boosting we are also able to give nearly matching worst-case error bounds.
  Our algorithms are based on the geometric techniques of Nikolov, Talwar, and
Zhang. The main new ingredients are convex relaxations and careful use of the
Frank-Wolfe algorithm for constrained convex minimization. To design our
relaxations, we rely on the Grothendieck inequality from functional analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1389</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1389</id><created>2013-08-06</created><authors><author><keyname>Tian</keyname><forenames>Ye</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Degrees of Freedom for the MIMO Multi-way Relay Channel</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the degrees of freedom (DoF) of the L-cluster, K-user
MIMO multi-way relay channel, where users in each cluster wish to exchange
messages within the cluster, and they can only communicate through the relay. A
novel DoF upper bound is derived by providing users with carefully designed
genie information. Achievable DoF is identified using signal space alignment
and multiple-access transmission. For the two-cluster MIMO multi-way relay
channel with two users in each cluster, DoF is established for the general case
when users and the relay have arbitrary number of antennas, and it is shown
that the DoF upper bound can be achieved using signal space alignment or
multiple-access transmission, or a combination of both. The result is then
generalized to the three user case. For the L-cluster K-user MIMO multi-way
relay channel in the symmetric setting, conditions under which the DoF upper
bound can be achieved are established. In addition to being shown to be tight
in a variety of scenarios of interests of the multi-way relay channel, the
newly derived upperbound also establishes the optimality of several previously
established achievable DoF results for multiuser relay channels that are
special cases of the multi-way relay channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1391</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1391</id><created>2013-08-06</created><updated>2013-08-10</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Scalar Reconciliation for Gaussian Modulation of Two-Way
  Continuous-Variable Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>42 pages, 20 figures, 1 table; minor typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-way continuous-variable quantum key distribution (CVQKD) systems
allow higher key rates and improved transmission distances over standard
telecommunication networks in comparison to the one-way CVQKD protocols. To
exploit the real potential of two-way CVQKD systems a robust reconciliation
technique is needed. It is currently unavailable, which makes it impossible to
reach the real performance of a two-way CVQKD system. The reconciliation
process of correlated Gaussian variables is a complex problem that requires
either tomography in the physical layer that is intractable in a practical
scenario, or high-cost calculations in the multidimensional spherical space
with strict dimensional limitations. To avoid these issues, we propose an
efficient logical layer-based reconciliation method for two-way CVQKD to
extract binary information from correlated Gaussian variables. We demonstrate
that by operating on the raw-data level, the noise of the quantum channel can
be corrected in the scalar space and the reconciliation can be extended to
arbitrary high dimensions. We prove that the error probability of scalar
reconciliation is zero in any practical CVQKD scenario, and provides
unconditional security. The results allow to significantly improve the
currently available key rates and transmission distances of two-way CVQKD. The
proposed scalar reconciliation can also be applied in one-way systems as well,
to replace the existing reconciliation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1394</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1394</id><created>2013-08-06</created><updated>2013-09-29</updated><authors><author><keyname>Uppman</keyname><forenames>Hannes</forenames></author></authors><title>Computational Complexity of the Minimum Cost Homomorphism Problem on
  Three-Element Domains</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the computational complexity of the (extended) minimum
cost homomorphism problem (Min-Cost-Hom) as a function of a constraint
language, i.e. a set of constraint relations and cost functions that are
allowed to appear in instances. A wide range of natural combinatorial
optimisation problems can be expressed as Min-Cost-Homs and a classification of
their complexity would be highly desirable, both from a direct, applied point
of view as well as from a theoretical perspective.
  Min-Cost-Hom can be understood either as a flexible optimisation version of
the constraint satisfaction problem (CSP) or a restriction of the
(general-valued) valued constraint satisfaction problem (VCSP). Other
optimisation versions of CSPs such as the minimum solution problem (Min-Sol)
and the minimum ones problem (Min-Ones) are special cases of Min-Cost-Hom.
  The study of VCSPs has recently seen remarkable progress. A complete
classification for the complexity of finite-valued languages on arbitrary
finite domains has been obtained Thapper and Zivny [STOC'13]. However,
understanding the complexity of languages that are not finite-valued appears to
be more difficult. Min-Cost-Hom allows us to study problematic languages of
this type without having to deal with with the full generality of the VCSP. A
recent classification for the complexity of three-element Min-Sol, Uppman
[ICALP'13], takes a step in this direction. In this paper we extend this result
considerably by determining the complexity of three-element Min-Cost-Hom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1418</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1418</id><created>2013-08-06</created><authors><author><keyname>Nwana</keyname><forenames>Amandianeze O</forenames></author><author><keyname>Avestimehr</keyname><forenames>Salman</forenames></author><author><keyname>Chen</keyname><forenames>Tsuhan</forenames></author></authors><title>A Latent Social Approach to YouTube Popularity Prediction</title><categories>cs.SI cs.MM cs.NI physics.soc-ph</categories><doi>10.1109/GLOCOM.2013.6831554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current works on Information Centric Networking assume the spectrum of
caching strategies under the Least Recently/ Frequently Used (LRFU) scheme as
the de-facto standard, due to the ease of implementation and easier analysis of
such strategies. In this paper we predict the popularity distribution of
YouTube videos within a campus network. We explore two broad approaches in
predicting the popularity of videos in the network: consensus approaches based
on aggregate behavior in the network, and social approaches based on the
information diffusion over an implicit network. We measure the performance of
our approaches under a simple caching framework by picking the k most popular
videos according to our predicted distribution and calculating the hit rate on
the cache. We develop our approach by first incorporating video inter-arrival
time (based on the power-law distribution governing the transmission time
between two receivers of the same message in scale-free networks) to the
baseline (LRFU), then combining with an information diffusion model over the
inferred latent social graph that governs diffusion of videos in the network.
We apply techniques from latent social network inference to learn the sharing
probabilities between users in the network and apply a virus propagation model
borrowed from mathematical epidemiology to estimate the number of times a video
will be accessed in the future. Our approach gives rise to a 14% hit rate
improvement over the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1419</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1419</id><created>2013-08-06</created><authors><author><keyname>Navarro</keyname><forenames>Cristobal A.</forenames></author><author><keyname>Hitschfeld</keyname><forenames>Nancy</forenames></author></authors><title>Improving the GPU space of computation under triangular domain problems</title><categories>cs.DC cs.AR</categories><comments>6 pages, 9 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a stage in the GPU computing pipeline where a grid of thread-blocks
is mapped to the problem domain. Normally, this grid is a k-dimensional
bounding box that covers a k-dimensional problem no matter its shape. Threads
that fall inside the problem domain perform computations, otherwise they are
discarded at runtime. For problems with non-square geometry, this is not always
the best idea because part of the space of computation is executed without any
practical use. Two- dimensional triangular domain problems, alias td-problems,
are a particular case of interest. Problems such as the Euclidean distance map,
LU decomposition, collision detection and simula- tions over triangular tiled
domains are all td-problems and they appear frequently in many areas of
science. In this work, we propose an improved GPU mapping function g(lambda),
that maps any lambda block to a unique location (i, j) in the triangular
domain. The mapping is based on the properties of the lower triangular matrix
and it works at a block level, thus not compromising thread organization within
a block. The theoretical improvement from using g(lambda) is upper bounded as I
&lt; 2 and the number of wasted blocks is reduced from O(n^2) to O(n). We compare
our strategy with other proposed methods; the upper-triangular mapping (UTM),
the rectangular box (RB) and the recursive partition (REC). Our experimental
results on Nvidias Kepler GPU architecture show that g(lambda) is between 12%
and 15% faster than the bounding box (BB) strategy. When compared to the other
strategies, our mapping runs significantly faster than UTM and it is as fast as
RB in practical use, with the advantage that thread organization is not
compromised, as in RB. This work also contributes at presenting, for the first
time, a fair comparison of all existing strategies running the same experiments
under the same hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1440</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1440</id><created>2013-08-06</created><authors><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Budav&#xe1;ri</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Li</keyname><forenames>Nolan</forenames></author></authors><title>Graywulf: A platform for federated scientific databases and services</title><categories>cs.DB</categories><comments>SSDBM 2013 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many fields of science rely on relational database management systems to
analyze, publish and share data. Since RDBMS are originally designed for, and
their development directions are primarily driven by, business use cases they
often lack features very important for scientific applications. Horizontal
scalability is probably the most important missing feature which makes it
challenging to adapt traditional relational database systems to the ever
growing data sizes. Due to the limited support of array data types and metadata
management, successful application of RDBMS in science usually requires the
development of custom extensions. While some of these extensions are specific
to the field of science, the majority of them could easily be generalized and
reused in other disciplines. With the Graywulf project we intend to target
several goals. We are building a generic platform that offers reusable
components for efficient storage, transformation, statistical analysis and
presentation of scientific data stored in Microsoft SQL Server. Graywulf also
addresses the distributed computational issues arising from current RDBMS
technologies. The current version supports load balancing of simple queries and
parallel execution of partitioned queries over a set of mirrored databases.
Uniform user access to the data is provided through a web based query interface
and a data surface for software clients. Queries are formulated in a slightly
modified syntax of SQL that offers a transparent view of the distributed data.
The software library consists of several components that can be reused to
develop complex scientific data warehouses: a system registry, administration
tools to manage entire database server clusters, a sophisticated workflow
execution framework, and a SQL parser library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1443</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1443</id><created>2013-08-06</created><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author></authors><title>Category of asynchronous systems and polygonal morphisms</title><categories>cs.LO math.CT</categories><comments>18 pages</comments><msc-class>18A35, 18A40, 18B20, 68Q85</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A weak asynchronous system is a trace monoid with a partial action on a set.
A polygonal morphism between weak asynchronous systems commutes with the
actions and preserves the independence of events. We prove that the category of
weak asynchronous systems and polygonal morphisms has all limits and colimits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1464</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1464</id><created>2013-08-06</created><authors><author><keyname>Terrel</keyname><forenames>Andy R.</forenames></author><author><keyname>Mandli</keyname><forenames>Kyle T.</forenames></author></authors><title>ManyClaw: Slicing and dicing Riemann solvers for next generation highly
  parallel architectures</title><categories>cs.CE cs.MS</categories><comments>TACC-Intel Symposium on Highly Parallel Architectures. 2012</comments><acm-class>C.1.4; D.1.3; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation computer architectures will include order of magnitude more
intra-node parallelism; however, many application programmers have a difficult
time keeping their codes current with the state-of-the-art machines. In this
context, we analyze Hyperbolic PDE solvers, which are used in the solution of
many important applications in science and engineering. We present ManyClaw, a
project intended to explore the exploitation of intra-node parallelism in
hyperbolic PDE solvers via the Clawpack software package for solving hyperbolic
PDEs. Our goal is to separate the low level parallelism and the physical
equations thus providing users the capability to leverage intra-node
parallelism without explicitly writing code to take advantage of newer
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1466</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1466</id><created>2013-08-06</created><updated>2013-08-19</updated><authors><author><keyname>Bhorkar</keyname><forenames>Abhijeet</forenames></author><author><keyname>Bhanage</keyname><forenames>Gautam</forenames></author></authors><title>Interference Reduction in High Density WLAN Deployments using antenna
  Selection</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a novel, robust scheme for high density WLAN
deployments. This scheme uses well known selection diversity at the
transmitter. We show that our scheme increases the number of simultaneous
transmissions at any given time without excessive overhead (compared to other
schemes such as Multi-user MIMO). Furthermore, this scheme can be easily
implemented using existing standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1471</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1471</id><created>2013-08-07</created><authors><author><keyname>Rajan</keyname><forenames>R. Arokia Paul</forenames></author><author><keyname>Francis</keyname><forenames>F. Sagayaraj</forenames></author></authors><title>Application of Inventory Management Principles for Efficient Data
  Placement in Storage Networks</title><categories>cs.DB</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  6, No 2, November 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principles and strategies found in material management are comparable and
analogue with the data management. This paper concentrates on the conversion of
product inventory management principles into data inventory management
principles. Efforts were made to enumerate various impacting parameters that
would be appropriate to consider if any data inventory model could be plotted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1472</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1472</id><created>2013-08-07</created><authors><author><keyname>Burstedde</keyname><forenames>Carsten</forenames></author><author><keyname>Calhoun</keyname><forenames>Donna</forenames></author><author><keyname>Mandli</keyname><forenames>Kyle</forenames></author><author><keyname>Terrel</keyname><forenames>Andy R.</forenames></author></authors><title>ForestClaw: Hybrid forest-of-octrees AMR for hyperbolic conservation
  laws</title><categories>cs.MS</categories><comments>submitted to International Conference on Parallel Computing -
  ParCo2013</comments><acm-class>G.4; G.1.8; D.1.3</acm-class><journal-ref>Adv. Par. Comp. 25 (2014) 253-262</journal-ref><doi>10.3233/978-1-61499-381-0-253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new hybrid paradigm for parallel adaptive mesh refinement (AMR)
that combines the scalability and lightweight architecture of tree-based AMR
with the computational efficiency of patch-based solvers for hyperbolic
conservation laws. The key idea is to interpret each leaf of the AMR hierarchy
as one uniform compute patch in $\sR^d$ with $m^d$ degrees of freedom, where
$m$ is customarily between 8 and 32. Thus, computation on each patch can be
optimized for speed, while we inherit the flexibility of adaptive meshes. In
our work we choose to integrate with the p4est AMR library since it allows us
to compose the mesh from multiple mapped octrees and enables the cubed sphere
and other nontrivial multiblock geometries. We describe aspects of the parallel
implementation and close with scalings for both MPI-only and OpenMP/MPI hybrid
runs, where the largest MPI run executes on 16,384 CPU cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1481</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1481</id><created>2013-08-07</created><updated>2015-04-28</updated><authors><author><keyname>Ethier</keyname><forenames>S. N.</forenames></author><author><keyname>Lee</keyname><forenames>Jiyeon</forenames></author></authors><title>The evolution of the game of baccarat</title><categories>math.OC cs.GT</categories><comments>13 pages, 0 figures; change in title and emphasis in v2</comments><msc-class>91A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game of baccarat has evolved from a parlor game played by French
aristocrats in the first half of the 19th century to a casino game that
generated over US\$41 billion in revenue for the casinos of Macau in 2013. The
parlor game was originally a three-person zero-sum game. Later in the 19th
century it was simplified to a two-person zero-sum game. Early in the 20th
century the parlor game became a casino game, no longer zero-sum. In the mid
20th century, the strategic casino game became a nonstrategic game, with
players competing against the house instead of against each other. We argue
that this evolution was motivated by both economic and game-theoretic
considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1482</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1482</id><created>2013-08-07</created><authors><author><keyname>Rezvaniana</keyname><forenames>Saba</forenames></author><author><keyname>Towhidkhah</keyname><forenames>Farzad</forenames></author><author><keyname>Ghahramani</keyname><forenames>Nematollah</forenames></author><author><keyname>Rezvanian</keyname><forenames>Alireza</forenames></author></authors><title>Increasing Robustness of the Anesthesia Process from Difference
  Patient's Delay Using a State-Space Model Predictive Controller</title><categories>cs.SY</categories><comments>5 pages, 4 figures, Journal</comments><journal-ref>Procedia Engineering 15 (2011) 928-932</journal-ref><doi>10.1016/j.proeng.2011.08.171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of anesthesia is nonlinear with time delay and also there are
some constraints which have to be considered in calculating administrative drug
dosage. We present an Extended Kalman Filter (EKF) observer to estimate drug
concentration in the patient's body and use this estimation in a state-space
based Model of Predictive Controller (MPC) for controlling the depth of
anesthesia. Bispectral Index (BIS) is used as a patient consciousness index and
propofol as an anesthetic agent. Performance evaluations of the proposed
controller, the results have been compared with those of a MPC controller. The
results demonstrate that state-space MPC including the EKF estimator for
controlling the anesthesia process can significantly increase the robustness in
encountering patients' delay deviations in comparison with the MPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1484</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1484</id><created>2013-08-07</created><authors><author><keyname>Nabizadeh</keyname><forenames>Somayeh</forenames></author><author><keyname>Rezvanian</keyname><forenames>Alireza</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammd Reza</forenames></author></authors><title>A Multi-Swarm Cellular PSO based on Clonal Selection Algorithm in
  Dynamic Environments</title><categories>cs.NE cs.AI</categories><comments>5 pages, 3 figures, conference paper</comments><journal-ref>2012 International Conference on Informatics, Electronics &amp; Vision
  (ICIEV 2012) 482-486</journal-ref><doi>10.1109/ICIEV.2012.6317524</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world problems are dynamic optimization problems. In this case, the
optima in the environment change dynamically. Therefore, traditional
optimization algorithms disable to track and find optima. In this paper, a new
multi-swarm cellular particle swarm optimization based on clonal selection
algorithm (CPSOC) is proposed for dynamic environments. In the proposed
algorithm, the search space is partitioned into cells by a cellular automaton.
Clustered particles in each cell, which make a sub-swarm, are evolved by the
particle swarm optimization and clonal selection algorithm. Experimental
results on Moving Peaks Benchmark demonstrate the superiority of the CPSOC its
popular methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1503</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1503</id><created>2013-08-07</created><authors><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>ALOHA Random Access that Operates as a Rateless Code</title><categories>cs.IT math.IT</categories><comments>Revised version submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various applications of wireless Machine-to-Machine (M2M) communications have
rekindled the research interest in random access protocols, suitable to support
a large number of connected devices. Slotted ALOHA and its derivatives
represent a simple solution for distributed random access in wireless networks.
Recently, a framed version of slotted ALOHA gained renewed interest due to the
incorporation of successive interference cancellation (SIC) in the scheme,
which resulted in substantially higher throughputs. Based on similar principles
and inspired by the rateless coding paradigm, a frameless approach for
distributed random access in slotted ALOHA framework is described in this
paper. The proposed approach shares an operational analogy with rateless
coding, expressed both through the user access strategy and the adaptive length
of the contention period, with the objective to end the contention when the
instantaneous throughput is maximized. The paper presents the related analysis,
providing heuristic criteria for terminating the contention period and showing
that very high throughputs can be achieved, even for a low number for
contending users. The demonstrated results potentially have more direct
practical implications compared to the approaches for coded random access that
lead to high throughputs only asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1507</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1507</id><created>2013-08-07</created><authors><author><keyname>Ostapov</keyname><forenames>Yuriy</forenames></author></authors><title>Logical analysis of natural language semantics to solve the problem of
  computer understanding</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An object--oriented approach to create a natural language understanding
system is considered. The understanding program is a formal system built on the
base of predicative calculus. Horn's clauses are used as well--formed formulas.
An inference is based on the principle of resolution. Sentences of natural
language are represented in the view of typical predicate set. These predicates
describe physical objects and processes, abstract objects, categories and
semantic relations between objects. Predicates for concrete assertions are
saved in a database. To describe the semantics of classes for physical objects,
abstract concepts and processes, a knowledge base is applied. The proposed
representation of natural language sentences is a semantic net. Nodes of such
net are typical predicates. This approach is perspective as, firstly, such
typification of nodes facilitates essentially forming of processing algorithms
and object descriptions, secondly, the effectiveness of algorithms is increased
(particularly for the great number of nodes), thirdly, to describe the
semantics of words, encyclopedic knowledge is used, and this permits
essentially to extend the class of solved problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1509</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1509</id><created>2013-08-07</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Martin</keyname><forenames>Clyde F.</forenames></author></authors><title>Monotone Smoothing Splines Using General Linear Systems</title><categories>cs.SY cs.IT math.IT math.OC stat.AP</categories><journal-ref>Asian Journal of Control, Vol. 5, No. 2, pp. 461-468, Mar. 2013</journal-ref><doi>10.1002/asjc.557</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method is proposed to solve the problem of monotone
smoothing splines using general linear systems. This problem, also called
monotone control theoretic splines, has been solved only when the curve
generator is modeled by the second-order integrator, but not for other cases.
The difficulty in the problem is that the monotonicity constraint should be
satisfied over an interval which has the cardinality of the continuum. To solve
this problem, we first formulate the problem as a semi-infinite quadratic
programming, and then we adopt a discretization technique to obtain a
finite-dimensional quadratic programming problem. It is shown that the solution
of the finite-dimensional problem always satisfies the infinite-dimensional
monotonicity constraint. It is also proved that the approximated solution
converges to the exact solution as the discretization grid-size tends to zero.
An example is presented to show the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1533</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1533</id><created>2013-08-07</created><authors><author><keyname>Jiang</keyname><forenames>Bin</forenames></author><author><keyname>Duan</keyname><forenames>Yingying</forenames></author><author><keyname>Lu</keyname><forenames>Feng</forenames></author><author><keyname>Yang</keyname><forenames>Tinghong</forenames></author><author><keyname>Zhao</keyname><forenames>Jing</forenames></author></authors><title>Topological Structure of Urban Street Networks from the Perspective of
  Degree Correlations</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>15 pages, 8 figures, and 3 tables</comments><journal-ref>Environment and Planning B: Planning and Design, 41(5), 813-828,
  2014</journal-ref><doi>10.1068/b39110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex networks demonstrate a phenomenon of striking degree
correlations, i.e., a node tends to link to other nodes with similar (or
dissimilar) degrees. From the perspective of degree correlations, this paper
attempts to characterize topological structures of urban street networks. We
adopted six urban street networks (three European and three North American),
and converted them into network topologies in which nodes and edges
respectively represent individual streets and street intersections, and
compared the network topologies to three reference network topologies
(biological, technological, and social). The urban street network topologies
(with the exception of Manhattan) showed a consistent pattern that distinctly
differs from the three reference networks. The topologies of urban street
networks lack striking degree correlations in general. Through reshuffling the
network topologies towards for example maximum or minimum degree correlations
while retaining the initial degree distributions, we found that all the
surrogate topologies of the urban street networks, as well as the reference
ones, tended to deviate from small world properties. This implies that the
initial degree correlations do not have any positive or negative effect on the
networks' performance or functions.
  Keywords: Scale free, small world, rewiring, rich club effect, reshuffle, and
complex networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1536</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1536</id><created>2013-08-07</created><updated>2013-08-08</updated><authors><author><keyname>Beliakov</keyname><forenames>Gleb</forenames></author><author><keyname>Matiyasevich</keyname><forenames>Yuri</forenames></author></authors><title>A Parallel Algorithm for Calculation of Large Determinants with High
  Accuracy for GPUs and MPI clusters</title><categories>cs.DC cs.MS cs.NA math.NA math.NT</categories><msc-class>65F40, 68W10, 11M26</msc-class><acm-class>D.1.3; G.1.0; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel algorithm for calculating very large determinants with
arbitrary precision on computer clusters. This algorithm minimises data
movements between the nodes and computes not only the determinant but also all
minors corresponding to a particular row or column at a little extra cost, and
also the determinants and minors of all submatrices in the top left corner at
no extra cost. We implemented the algorithm in arbitrary precision arithmetic,
suitable for very ill conditioned matrices, and empirically estimated the loss
of precision. The algorithm was applied to studies of Riemann's zeta function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1539</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1539</id><created>2013-08-07</created><authors><author><keyname>Alsouri</keyname><forenames>Sami</forenames></author><author><keyname>Feller</keyname><forenames>Thomas</forenames></author><author><keyname>Malipatlolla</keyname><forenames>Sunil</forenames></author><author><keyname>Katzenbeisser</keyname><forenames>Stefan</forenames></author></authors><title>Hardware-based Security for Virtual Trusted Platform Modules</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual Trusted Platform modules (TPMs) were proposed as a software-based
alternative to the hardware-based TPMs to allow the use of their cryptographic
functionalities in scenarios where multiple TPMs are required in a single
platform, such as in virtualized environments. However, virtualizing TPMs,
especially virutalizing the Platform Configuration Registers (PCRs), strikes
against one of the core principles of Trusted Computing, namely the need for a
hardware-based root of trust. In this paper we show how strength of
hardware-based security can be gained in virtual PCRs by binding them to their
corresponding hardware PCRs. We propose two approaches for such a binding. For
this purpose, the first variant uses binary hash trees, whereas the other
variant uses incremental hashing. In addition, we present an FPGA-based
implementation of both variants and evaluate their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1552</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1552</id><created>2013-08-07</created><authors><author><keyname>Antonoyiannakis</keyname><forenames>Manolis</forenames></author></authors><title>Acceptance Rates in Physical Review Letters: No Seasonal Bias</title><categories>physics.soc-ph cs.DL</categories><comments>Scheduled to appear in Learned Publishing</comments><journal-ref>Learned Publishing 27, 53 (2014)</journal-ref><doi>10.1087/20140108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are editorial decisions biased? A recent discussion in Learned Publishing has
focused on one aspect of potential bias in editorial decisions, namely seasonal
(e.g., monthly) variations in acceptance rates of research journals. In this
letter, we contribute to the discussion by analyzing data from Physical Review
Letters (PRL), a journal published by the American Physical Society. We studied
the 190,106 papers submitted to PRL from January 1990 until September 2012. No
statistically significant variations were found in the monthly acceptance
rates. We conclude that the time of year that the authors of a paper submit
their work to PRL has no effect on the fate of the paper through the review
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1554</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1554</id><created>2013-08-07</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author></authors><title>The Wisdom of Citing Scientists</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>Accepted for publication in the Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Brief Communication discusses the benefits of citation analysis in
research evaluation based on Galton's &quot;Wisdom of Crowds&quot; (1907). Citations are
based on the assessment of many which is why they can be ascribed a certain
amount of accuracy. However, we show that citations are incomplete assessments
and that one cannot assume that a high number of citations correlate with a
high level of usefulness. Only when one knows that a rarely cited paper has
been widely read is it possible to say (strictly speaking) that it was
obviously of little use for further research. Using a comparison with 'like'
data, we try to determine that cited reference analysis allows a more
meaningful analysis of bibliometric data than times-cited analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1556</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1556</id><created>2013-08-07</created><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>On the Independent Set and Common Subgraph Problems in Random Graphs</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we develop efficient exact and approximate algorithms for
computing a maximum independent set in random graphs. In a random graph $G$,
each pair of vertices are joined by an edge with a probability $p$, where $p$
is a constant between $0$ and $1$. We show that, a maximum independent set in a
random graph that contains $n$ vertices can be computed in expected computation
time $2^{O(\log_{2}^{2}{n})}$. Using techniques based on enumeration, we
develop an algorithm that can find a largest common subgraph in two random
graphs in $n$ and $m$ vertices ($m \leq n$) in expected computation time
$2^{O(n^{\frac{1}{2}}\log_{2}^{\frac{5}{3}}{n})}$. In addition, we show that,
with high probability, the parameterized independent set problem is fixed
parameter tractable in random graphs and the maximum independent set in a
random graph in $n$ vertices can be approximated within a ratio of
$\frac{2n}{2^{\sqrt{\log_{2}{n}}}}$ in expected polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1568</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1568</id><created>2013-08-07</created><authors><author><keyname>Urakov</keyname></author><author><keyname>Timeryaev</keyname></author></authors><title>All-Pairs Shortest Paths Algorithm for High-dimensional Sparse Graphs</title><categories>cs.DS</categories><comments>8 pages, 3 figures, 2 tables. A more detailed text on Russian:
  http://www.lib.tsu.ru/mminfo/000349342/19/image/19-084.pdf</comments><msc-class>05C12</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here the All-pairs shortest path problem on weighted undirected sparse graphs
is being considered. For the problem considered, we propose ``disassembly and
assembly of a graph'' algorithm which uses a solution of the problem on a
small-dimensional graph to obtain the solution for the given graph. The
proposed algorithm has been compared to one of the fastest classic algorithms
on data from an open public source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1575</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1575</id><created>2013-08-07</created><updated>2014-11-14</updated><authors><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>The Parameterised Complexity of Counting Connected Subgraphs and Graph
  Motifs</title><categories>cs.CC math.CO</categories><comments>Final version, to appear in JCSS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a class of parameterised counting problems on graphs, p-#Induced
Subgraph With Property(\Phi), which generalises a number of problems which have
previously been studied. This paper focusses on the case in which \Phi defines
a family of graphs whose edge-minimal elements all have bounded treewidth; this
includes the special case in which \Phi describes the property of being
connected. We show that exactly counting the number of connected induced
k-vertex subgraphs in an n-vertex graph is #W[1]-hard, but on the other hand
there exists an FPTRAS for the problem; more generally, we show that there
exists an FPTRAS for p-#Induced Subgraph With Property(\Phi) whenever \Phi is
monotone and all the minimal graphs satisfying \Phi have bounded treewidth. We
then apply these results to a counting version of the Graph Motif problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1590</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1590</id><created>2013-08-07</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author></authors><title>Sparse Representations for Packetized Predictive Networked Control</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1307.8242</comments><journal-ref>IFAC 18th World Congress, pp. 84-89, Aug. 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a networked control architecture for LTI plant models with a
scalar input. Communication from controller to actuator is over an unreliable
network which introduces packet dropouts. To achieve robustness against
dropouts, we adopt a packetized predictive control paradigm wherein each
control packet transmitted contains tentative future plant input values. The
novelty of our approach is that we seek that the control packets transmitted be
sparse. For that purpose, we adapt tools from the area of compressed sensing
and propose to design the control packets via on-line minimization of a
suitable L1/L2 cost function. We then show how to choose parameters of the cost
function to ensure that the resultant closed loop system be practically stable,
provided the maximum number of consecutive packet dropouts is bounded. A
numerical example illustrates that sparsity reduces bit-rates, thereby making
our proposal suited to control over unreliable and bit-rate limited networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1600</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1600</id><created>2013-08-07</created><updated>2013-08-27</updated><authors><author><keyname>Filmus</keyname><forenames>Yuval</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Universal codes of the natural numbers</title><categories>cs.LO cs.IT math.IT</categories><comments>11 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (August 29,
  2013) lmcs:975</journal-ref><doi>10.2168/LMCS-9(3:7)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code of the natural numbers is a uniquely-decodable binary code of the
natural numbers with non-decreasing codeword lengths, which satisfies Kraft's
inequality tightly. We define a natural partial order on the set of codes, and
show how to construct effectively a code better than a given sequence of codes,
in a certain precise sense. As an application, we prove that the existence of a
scale of codes (a well-ordered set of codes which contains a code better than
any given code) is independent of ZFC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1602</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1602</id><created>2013-08-07</created><authors><author><keyname>Poss</keyname><forenames>Raphael 'kena'</forenames></author></authors><title>Optimizing for confidence - Costs and opportunities at the frontier
  between abstraction and reality</title><categories>cs.CY</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is there a relationship between computing costs and the confidence people
place in the behavior of computing systems? What are the tuning knobs one can
use to optimize systems for human confidence instead of correctness in purely
abstract models? This report explores these questions by reviewing the
mechanisms by which people build confidence in the match between the physical
world behavior of machines and their abstract intuition of this behavior
according to models or programming language semantics. We highlight in
particular that a bottom-up approach relies on arbitrary trust in the accuracy
of I/O devices, and that there exists clear cost trade-offs in the use of I/O
devices in computing systems. We also show various methods which alleviate the
need to trust I/O devices arbitrarily and instead build confidence
incrementally &quot;from the outside&quot; by considering systems as black box entities.
We highlight cases where these approaches can reach a given confidence level at
a lower cost than bottom-up approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1603</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1603</id><created>2013-08-07</created><updated>2013-08-08</updated><authors><author><keyname>Volz</keyname><forenames>Dietmar</forenames></author></authors><title>A Note on Topology Preservation in Classification, and the Construction
  of a Universal Neuron Grid</title><categories>cs.NE cs.AI nlin.AO stat.ML</categories><msc-class>92F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1605</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1605</id><created>2013-08-07</created><authors><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Schaub</keyname><forenames>Michael T.</forenames></author><author><keyname>Yaliraki</keyname><forenames>Sophia N.</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>The stability of a graph partition: A dynamics-based framework for
  community detection</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>3 figures; published as book chapter</comments><journal-ref>Dynamics On and Of Complex Networks, Volume 2, pp 221-242,
  Springer 2013</journal-ref><doi>10.1007/978-1-4614-6729-8_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen a surge of interest in the analysis of complex
networks, facilitated by the availability of relational data and the
increasingly powerful computational resources that can be employed for their
analysis. Naturally, the study of real-world systems leads to highly complex
networks and a current challenge is to extract intelligible, simplified
descriptions from the network in terms of relevant subgraphs, which can provide
insight into the structure and function of the overall system.
  Sparked by seminal work by Newman and Girvan, an interesting line of research
has been devoted to investigating modular community structure in networks,
revitalising the classic problem of graph partitioning.
  However, modular or community structure in networks has notoriously evaded
rigorous definition. The most accepted notion of community is perhaps that of a
group of elements which exhibit a stronger level of interaction within
themselves than with the elements outside the community. This concept has
resulted in a plethora of computational methods and heuristics for community
detection. Nevertheless a firm theoretical understanding of most of these
methods, in terms of how they operate and what they are supposed to detect, is
still lacking to date.
  Here, we will develop a dynamical perspective towards community detection
enabling us to define a measure named the stability of a graph partition. It
will be shown that a number of previously ad-hoc defined heuristics for
community detection can be seen as particular cases of our method providing us
with a dynamic reinterpretation of those measures. Our dynamics-based approach
thus serves as a unifying framework to gain a deeper understanding of different
aspects and problems associated with community detection and allows us to
propose new dynamically-inspired criteria for community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1606</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1606</id><created>2013-08-06</created><authors><author><keyname>Popleteev</keyname><forenames>Andrei</forenames></author><author><keyname>Osmani</keyname><forenames>Venet</forenames></author><author><keyname>Mayora</keyname><forenames>Oscar</forenames></author></authors><title>Investigation of indoor localization with ambient FM radio stations</title><categories>cs.NI</categories><comments>10th IEEE Pervasive Computing and Communication conference, PerCom
  2012, pp. 171 - 179</comments><doi>10.1109/PerCom.2012.6199864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization plays an essential role in many ubiquitous computing
applications. While the outdoor location-aware services based on GPS are
becoming increasingly popular, their proliferation to indoor environments is
limited due to the lack of widely available indoor localization systems. The
de-facto standard for indoor positioning is based on Wi-Fi and while other
localization alternatives exist, they either require expensive hardware or
provide a low accuracy. This paper presents an investigation into localization
system that leverages signals of broadcasting FM radio stations. The FM
stations provide a worldwide coverage, while FM tuners are readily available in
many mobile devices. The experimental results show that FM radio can be used
for indoor localization, while providing longer battery life than Wi-Fi, making
FM an alternative to consider for positioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1609</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1609</id><created>2013-08-07</created><authors><author><keyname>Swannack</keyname><forenames>Charles H.</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author></authors><title>Geometric Relationships Between Gaussian and Modulo-Lattice Error
  Exponents</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice coding and decoding have been shown to achieve the capacity of the
additive white Gaussian noise (AWGN) channel. This was accomplished using a
minimum mean-square error scaling and randomization to transform the AWGN
channel into a modulo-lattice additive noise channel of the same capacity. It
has been further shown that when operating at rates below capacity but above
the critical rate of the channel, there exists a rate-dependent scaling such
that the associated modulo-lattice channel attains the error exponent of the
AWGN channel. A geometric explanation for this result is developed. In
particular, it is shown how the geometry of typical error events for the
modulo-lattice channel coincides with that of a spherical code for the AWGN
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1611</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1611</id><created>2013-08-06</created><authors><author><keyname>Kadotani</keyname><forenames>Megumi</forenames></author><author><keyname>Matsumoto</keyname><forenames>Aya</forenames></author><author><keyname>Shibuya</keyname><forenames>Takafumi</forenames></author><author><keyname>Lee</keyname><forenames>Younjae</forenames></author><author><keyname>Watanabe</keyname><forenames>Saori</forenames></author><author><keyname>Iba</keyname><forenames>Takashi</forenames></author></authors><title>Pattern Language for Good Old Future From Japanese Culture</title><categories>cs.CY</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having developed greatly over millennium under its culture, the ancient
buildings and old town atmospheres maintain a quality of comfort. However,
people only appreciate the &quot;good old&quot; quality and do not think further about
the rational reasons why they feel comfort in it. This keeps them from creating
their own things and models with good old quality, relying on the imported
western thinking and methods as a result of modernization. Since people are now
struggling under the imbalanced and complex society, we believe that support is
needed for generating things and frameworks with good old quality in modern
time situations and scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1612</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1612</id><created>2013-08-06</created><authors><author><keyname>Matsuzawa</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Tohyama</keyname><forenames>Sayaka</forenames></author><author><keyname>Sakai</keyname><forenames>Sanshiro</forenames></author></authors><title>The Course Design To Develop Meta-Cognitive Skills for Collaborative
  Learning Through Tool-Assisted Discourse Analysis</title><categories>cs.CY</categories><comments>Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)</comments><report-no>coins13/2013/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the course design titled &quot;Learning Management&quot; of which
the goal is to &quot;learn collaborative learning&quot; for a first-year undergraduate
student. The objective of the class design is to help transform the student's
belief of learning from a passive, individual model to an active, collaborative
model which is supported by the concept of &quot;Knowledge Building&quot; or
&quot;Constructive Interaction&quot;. We conducted an empirical study where the students
analyzed their own discourse with KBDeX which is the software they used to
assist their analysis in the experimental group whereas the students in the
control group reflected their project activities in their own way. We examined
the transformation of their beliefs through the qualitative analysis of their
reports after the course. The results showed that the design led to the
transforming of their learning beliefs from &quot;just experiences of the
participation of the collaborative learning&quot; to &quot;active contribution for
collaborative knowledge creation&quot;. The course succeeded in changing the
students' preferences about collaborative learning from negative to positive as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1640</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1640</id><created>2013-08-07</created><updated>2013-11-15</updated><authors><author><keyname>Chillara</keyname><forenames>Suryajith</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Partha</forenames></author></authors><title>Depth-4 Lower Bounds, Determinantal Complexity : A Unified Approach</title><categories>cs.CC</categories><comments>Extension of the previous upload</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tavenas has recently proved that any n^{O(1)}-variate and degree n polynomial
in VP can be computed by a depth-4 circuit of size 2^{O(\sqrt{n}\log n)}. So to
prove VP not equal to VNP, it is sufficient to show that an explicit polynomial
in VNP of degree n requires 2^{\omega(\sqrt{n}\log n)} size depth-4 circuits.
Soon after Tavenas's result, for two different explicit polynomials, depth-4
circuit size lower bounds of 2^{\Omega(\sqrt{n}\log n)} have been proved Kayal
et al. and Fournier et al. In particular, using combinatorial design Kayal et
al.\ construct an explicit polynomial in VNP that requires depth-4 circuits of
size 2^{\Omega(\sqrt{n}\log n)} and Fournier et al.\ show that iterated matrix
multiplication polynomial (which is in VP) also requires 2^{\Omega(\sqrt{n}\log
n)} size depth-4 circuits.
  In this paper, we identify a simple combinatorial property such that any
polynomial f that satisfies the property would achieve similar circuit size
lower bound for depth-4 circuits. In particular, it does not matter whether f
is in VP or in VNP. As a result, we get a very simple unified lower bound
analysis for the above mentioned polynomials.
  Another goal of this paper is to compare between our current knowledge of
depth-4 circuit size lower bounds and determinantal complexity lower bounds. We
prove the that the determinantal complexity of iterated matrix multiplication
polynomial is \Omega(dn) where d is the number of matrices and n is the
dimension of the matrices. So for d=n, we get that the iterated matrix
multiplication polynomial achieves the current best known lower bounds in both
fronts: depth-4 circuit size and determinantal complexity. To the best of our
knowledge, a \Theta(n) bound for the determinantal complexity for the iterated
matrix multiplication polynomial was known only for constant d&gt;1 by Jansen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1643</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1643</id><created>2013-08-07</created><authors><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>Improved Lower Bounds for Testing Triangle-freeness in Boolean Functions
  via Fast Matrix Multiplication</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the query complexity for testing linear-invariant properties
has been a central open problem in the study of algebraic property testing.
Triangle-freeness in Boolean functions is a simple property whose testing
complexity is unknown. Three Boolean functions $f_1$, $f_2$ and $f_3:
\mathbb{F}_2^k \to \{0, 1\}$ are said to be triangle free if there is no $x, y
\in \mathbb{F}_2^k$ such that $f_1(x) = f_2(y) = f_3(x + y) = 1$. This property
is known to be strongly testable (Green 2005), but the number of queries needed
is upper-bounded only by a tower of twos whose height is polynomial in $1 /
\epsislon$, where $\epsislon$ is the distance between the tested function
triple and triangle-freeness, i.e., the minimum fraction of function values
that need to be modified to make the triple triangle free. A lower bound of $(1
/ \epsilon)^{2.423}$ for any one-sided tester was given by Bhattacharyya and
Xie (2010). In this work we improve this bound to $(1 / \epsilon)^{6.619}$.
Interestingly, we prove this by way of a combinatorial construction called
\emph{uniquely solvable puzzles} that was at the heart of Coppersmith and
Winograd's renowned matrix multiplication algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1688</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1688</id><created>2013-08-05</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>The Number Theoretic Hilbert Transform</title><categories>cs.CR cs.IT math.IT math.NT</categories><comments>11 pages</comments><journal-ref>Circuits, Systems and Signal Processing 33 (2014) 2539-2548</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general expression for a number-theoretic Hilbert
transform (NHT). The transformations preserve the circulant nature of the
discrete Hilbert transform (DHT) matrix together with alternating values in
each row being zero and non-zero. Specific examples for 4-point, 6-point, and
8-point NHT are provided. The NHT transformation can be used as a primitive to
create cryptographically useful scrambling transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1700</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1700</id><created>2013-08-07</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author></authors><title>A simple combinatorial interpretation of certain generalized Bell and
  Stirling numbers</title><categories>cs.DM math.CO</categories><journal-ref>Discrete Mathematics 318 (2014) 53-57</journal-ref><doi>10.1016/j.disc.2013.11.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a series of papers, P. Blasiak et al. developed a wide-ranging
generalization of Bell numbers (and of Stirling numbers of the second kind)
that appears to be relevant to the so-called Boson normal ordering problem.
They provided a recurrence and, more recently, also offered a (fairly complex)
combinatorial interpretation of these numbers. We show that by restricting the
numbers somewhat (but still widely generalizing Bell and Stirling numbers), one
can supply a much more natural combinatorial interpretation. In fact, we offer
two different such interpretations, one in terms of graph colourings and
another one in terms of certain labelled Eulerian digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1725</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1725</id><created>2013-08-07</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ahlen</keyname><forenames>Anders</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>State Estimation over Sensor Networks with Correlated Wireless Fading
  Channels</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>13 pages, 6 figures</comments><msc-class>93E11, 93E15</msc-class><journal-ref>IEEE Transactions on Automatic Control, vol. 58, no. 3, pp.
  581-593, March 2013</journal-ref><doi>10.1109/TAC.2012.2212515</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic stability for centralized time-varying Kalman filtering over a
wireles ssensor network with correlated fading channels is studied. On their
route to the gateway, sensor packets, possibly aggregated with measurements
from several nodes, may be dropped because of fading links. To study this
situation, we introduce a network state process, which describes a finite set
of configurations of the radio environment. The network state characterizes the
channel gain distributions of the links, which are allowed to be correlated
between each other. Temporal correlations of channel gains are modeled by
allowing the network state process to form a (semi-)Markov chain. We establish
sufficient conditions that ensure the Kalman filter to be exponentially
bounded. In the one-sensor case, this new stability condition is shown to
include previous results obtained in the literature as special cases. The
results also hold when using power and bit-rate control policies, where the
transmission power and bit-rate of each node are nonlinear mapping of the
network state and channel gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1733</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1733</id><created>2013-08-07</created><authors><author><keyname>Annamalai</keyname><forenames>Muthiah</forenames></author></authors><title>Invitation to Ezhil: A Tamil Programming Language for Early
  Computer-Science Education</title><categories>cs.CY cs.PL</categories><comments>11 pages, 5 insets, 4 figures; accepted to Tamil Internet Conference,
  2013, TI-2013, Malaysia</comments><acm-class>D.3.0; D.3.2; K.3.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ezhil is a Tamil programming language with support for imperative
programming, with mixed use of Tamil and English identifiers and
function-names. Ezhil programing system is targeted toward the K-12 (junior
high-school) level Tamil speaking students, as an early introduction to
thinking like a computer-scientist. We believe this 'numeracy' knowledge is
easily transferred over from a native language (Tamil) to the pervasive English
language programming systems, in Java, dot-Net, Ruby or Python. Ezhil is an
effort to improve access to computing in the 21st Century.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1744</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1744</id><created>2013-08-07</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Ahlen</keyname><forenames>Anders</forenames></author><author><keyname>Jurado</keyname><forenames>Isabel</forenames></author></authors><title>Adaptive Controller Placement for Wireless Sensor-Actuator Networks with
  Erasure Channels</title><categories>cs.SY math.OC</categories><comments>10 pages, 8 figures, to be published in Automatica</comments><msc-class>93C41</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor-actuator networks offer flexibility for control design. One
novel element which may arise in networks with multiple nodes is that the role
of some nodes does not need to be fixed. In particular, there is no need to
pre-allocate which nodes assume controller functions and which ones merely
relay data. We present a flexible architecture for networked control using
multiple nodes connected in series over analog erasure channels without
acknowledgments. The control architecture proposed adapts to changes in network
conditions, by allowing the role played by individual nodes to depend upon
transmission outcomes. We adopt stochastic models for transmission outcomes and
characterize the distribution of controller location and the covariance of
system states. Simulation results illustrate that the proposed architecture has
the potential to give better performance than limiting control calculations to
be carried out at a fixed node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1745</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1745</id><created>2013-08-07</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author><author><keyname>Ahlen</keyname><forenames>Anders</forenames></author></authors><title>Power Control and Coding Formulation for State Estimation with Wireless
  Sensors</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>15 pages, 12 figures</comments><msc-class>93E11, 94A29</msc-class><doi>10.1109/TCST.2013.2253464</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technological advances have made wireless sensors cheap and reliable enough
to be brought into industrial use. A major challenge arises from the fact that
wireless channels introduce random packet dropouts. Power control and coding
are key enabling technologies in wireless communications to ensure efficient
communications. In the present work, we examine the role of power control and
coding for Kalman filtering over wireless correlated channels. Two estimation
architectures are considered: In the first, the sensors send their measurements
directly to a single gateway. In the second scheme, wireless relay nodes
provide additional links. The gateway decides on the coding scheme and the
transmitter power levels of the wireless nodes. The decision process is carried
out on-line and adapts to varying channel conditions in order to improve the
trade-off between state estimation accuracy and energy expenditure. In
combination with predictive power control, we investigate the use of
multiple-description coding, zero-error coding and network coding and provide
sufficient conditions for the expectation of the estimation error covariance
matrix to be bounded. Numerical results suggest that the proposed method may
lead to energy savings of around 50 %, when compared to an alternative scheme,
wherein transmission power levels and bit-rates are governed by simple logic.
In particular, zero-error coding is preferable at time instances with high
channel gains, whereas multiple-description coding is superior for time
instances with low gains. When channels between the sensors and the gateway are
in deep fades, network coding improves estimation accuracy significantly
without sacrificing energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1746</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1746</id><created>2013-08-07</created><updated>2013-11-26</updated><authors><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Online Decision Making in Crowdsourcing Markets: Theoretical Challenges
  (Position Paper)</title><categories>cs.SI cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, crowdsourcing has emerged as a cheap and efficient
method of obtaining solutions to simple tasks that are difficult for computers
to solve but possible for humans. The popularity and promise of crowdsourcing
markets has led to both empirical and theoretical research on the design of
algorithms to optimize various aspects of these markets, such as the pricing
and assignment of tasks. Much of the existing theoretical work on crowdsourcing
markets has focused on problems that fall into the broad category of online
decision making; task requesters or the crowdsourcing platform itself make
repeated decisions about prices to set, workers to filter out, problems to
assign to specific workers, or other things. Often these decisions are complex,
requiring algorithms that learn about the distribution of available tasks or
workers over time and take into account the strategic (or sometimes irrational)
behavior of workers.
  As human computation grows into its own field, the time is ripe to address
these challenges in a principled way. However, it appears very difficult to
capture all pertinent aspects of crowdsourcing markets in a single coherent
model. In this paper, we reflect on the modeling issues that inhibit
theoretical research on online decision making for crowdsourcing, and identify
some steps forward. This paper grew out of the authors' own frustration with
these issues, and we hope it will encourage the community to attempt to
understand, debate, and ultimately address them.
  The authors welcome feedback for future revisions of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1747</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1747</id><created>2013-08-08</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author></authors><title>Sequence-based Anytime Control</title><categories>math.OC cs.SY</categories><comments>14 pages</comments><msc-class>93C10, 93E15</msc-class><journal-ref>IEEE Trans. Automat. Contr., vol. 58, pp. 377-390, Feb. 2013</journal-ref><doi>10.1109/TAC.2012.2209977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two related anytime algorithms for control of nonlinear systems
when the processing resources available are time-varying. The basic idea is to
calculate tentative control input sequences for as many time steps into the
future as allowed by the available processing resources at every time step.
This serves to compensate for the time steps when the processor is not
available to perform any control calculations. Using a stochastic Lyapunov
function based approach, we analyze the stability of the resulting closed loop
system for the cases when the processor availability can be modeled as an
independent and identically distributed sequence and via an underlying Markov
chain. Numerical simulations indicate that the increase in performance due to
the proposed algorithms can be significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1761</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1761</id><created>2013-08-08</created><authors><author><keyname>Zewail</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Mohasseb</keyname><forenames>Y.</forenames></author><author><keyname>Nafie</keyname><forenames>M.</forenames></author><author><keyname>Gamal</keyname><forenames>H. EL</forenames></author></authors><title>The Deterministic Capacity of Relay Networks with Relay Private Messages</title><categories>cs.IT math.IT</categories><comments>3 figures, accepted at ITW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity region of a deterministic 4-node network, where 3 nodes
can only communicate via the fourth one. However, the fourth node is not merely
a relay since it can exchange private messages with all other nodes. This
situation resembles the case where a base station relays messages between users
and delivers messages between the backbone system and the users. We assume an
asymmetric scenario where the channel between any two nodes is not reciprocal.
First, an upper bound on the capacity region is obtained based on the notion of
single sided genie. Subsequently, we construct an achievable scheme that
achieves this upper bound using a superposition of broadcasting node 4 messages
and an achievable &quot;detour&quot; scheme for a reduced 3-user relay network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1762</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1762</id><created>2013-08-08</created><updated>2014-10-09</updated><authors><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Srivastava</keyname><forenames>Piyush</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Spatial mixing and approximation algorithms for graphs with bounded
  connective constant</title><categories>cs.DM cs.DS math.PR</categories><comments>26 pages. In October 2014, this paper was superseded by
  arxiv:1410.2595. Before that, an extended abstract of this paper appeared in
  Proc. IEEE Symposium on the Foundations of Computer Science (FOCS), 2013, pp.
  300-309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hard core model in statistical physics is a probability distribution on
independent sets in a graph in which the weight of any independent set I is
proportional to lambda^(|I|), where lambda &gt; 0 is the vertex activity. We show
that there is an intimate connection between the connective constant of a graph
and the phenomenon of strong spatial mixing (decay of correlations) for the
hard core model; specifically, we prove that the hard core model with vertex
activity lambda &lt; lambda_c(Delta + 1) exhibits strong spatial mixing on any
graph of connective constant Delta, irrespective of its maximum degree, and
hence derive an FPTAS for the partition function of the hard core model on such
graphs. Here lambda_c(d) := d^d/(d-1)^(d+1) is the critical activity for the
uniqueness of the Gibbs measure of the hard core model on the infinite d-ary
tree. As an application, we show that the partition function can be efficiently
approximated with high probability on graphs drawn from the random graph model
G(n,d/n) for all lambda &lt; e/d, even though the maximum degree of such graphs is
unbounded with high probability.
  We also improve upon Weitz's bounds for strong spatial mixing on bounded
degree graphs (Weitz, 2006) by providing a computationally simple method which
uses known estimates of the connective constant of a lattice to obtain bounds
on the vertex activities lambda for which the hard core model on the lattice
exhibits strong spatial mixing. Using this framework, we improve upon these
bounds for several lattices including the Cartesian lattice in dimensions 3 and
higher.
  Our techniques also allow us to relate the threshold for the uniqueness of
the Gibbs measure on a general tree to its branching factor (Lyons, 1989).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1763</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1763</id><created>2013-08-08</created><authors><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author><author><keyname>Tyagi</keyname><forenames>Vipin</forenames></author></authors><title>Linear Network Coding on Multi-Mesh of Trees (MMT) using All to All
  Broadcast (AAB)</title><categories>cs.DC</categories><comments>10 pages, 15 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 1, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce linear network coding on parallel architecture for multi-source
finite acyclic network. In this problem, different messages in diverse time
periods are broadcast and every nonsource node in the network decodes and
encodes the message based on further communication.We wish to minimize the
communication steps and time complexity involved in transfer of data from
node-to-node during parallel communication.We have used Multi-Mesh of Trees
(MMT) topology for implementing network coding. To envisage our result, we use
all-to-all broadcast as communication algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1767</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1767</id><created>2013-08-08</created><authors><author><keyname>Angius</keyname><forenames>Fabio</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Gerla</keyname><forenames>Mario</forenames></author><author><keyname>Pau</keyname><forenames>Giovanni</forenames></author></authors><title>WARP: A ICN architecture for social data</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network companies maintain complete visibility and ownership of the
data they store. However users should be able to maintain full control over
their content. For this purpose, we propose WARP, an architecture based upon
Information-Centric Networking (ICN) designs, which expands the scope of the
ICN architecture beyond media distribution, to provide data control in social
networks. The benefit of our solution lies in the lightweight nature of the
protocol and in its layered design. With WARP, data distribution and access
policies are enforced on the user side. Data can still be replicated in an ICN
fashion but we introduce control channels, named \textit{thread updates}, which
ensures that the access to the data is always updated to the latest control
policy. WARP decentralizes the social network but still offers APIs so that
social network providers can build products and business models on top of WARP.
Social applications run directly on the user's device and store their data on
the user's \textit{butler} that takes care of encryption and distribution.
Moreover, users can still rely on third parties to have high-availability
without renouncing their privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1776</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1776</id><created>2013-08-08</created><authors><author><keyname>Schreck</keyname><forenames>Berit</forenames></author><author><keyname>K&#xe4;mpf</keyname><forenames>Mirko</forenames></author><author><keyname>Kantelhardt</keyname><forenames>Jan W.</forenames></author><author><keyname>Motzkau</keyname><forenames>Holger</forenames></author></authors><title>Comparing the usage of global and local Wikipedias with focus on Swedish
  Wikipedia</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report summarizes the results of a short-term student research project
focused on the usage of Swedish Wikipedia. It is trying to answer the following
question: To what extent (and why) do people from non-English language
communities use the English Wikipedia instead of the one in their local
language? Article access time series and article edit time series from major
Wikipedias including Swedish Wikipedia are analyzed with various tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1779</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1779</id><created>2013-08-08</created><updated>2013-09-02</updated><authors><author><keyname>Caminati</keyname><forenames>Marco B.</forenames></author><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Rowat</keyname><forenames>Colin</forenames></author></authors><title>Proving soundness of combinatorial Vickrey auctions and generating
  verified executable code</title><categories>cs.GT cs.CE cs.LO</categories><msc-class>91-04, 68T15, 03B35, 91B26, 03B70, 03B15</msc-class><acm-class>J.4; D.2.4; F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using mechanised reasoning we prove that combinatorial Vickrey auctions are
soundly specified in that they associate a unique outcome (allocation and
transfers) to any valid input (bids). Having done so, we auto-generate verified
executable code from the formally defined auction. This removes a source of
error in implementing the auction design. We intend to use formal methods to
verify new auction designs. Here, our contribution is to introduce and
demonstrate the use of formal methods for auction verification in the familiar
setting of a well-known auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1780</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1780</id><created>2013-08-08</created><updated>2013-10-15</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Johansson</keyname><forenames>Moa</forenames></author><author><keyname>Maclean</keyname><forenames>Ewen</forenames></author></authors><title>Proof-Pattern Recognition and Lemma Discovery in ACL2</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel technique for combining statistical machine learning for
proof-pattern recognition with symbolic methods for lemma discovery. The
resulting tool, ACL2(ml), gathers proof statistics and uses statistical
pattern-recognition to pre-processes data from libraries, and then suggests
auxiliary lemmas in new proofs by analogy with already seen examples. This
paper presents the implementation of ACL2(ml) alongside theoretical
descriptions of the proof-pattern recognition and lemma discovery methods
involved in it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1792</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1792</id><created>2013-08-08</created><authors><author><keyname>Aharon</keyname><forenames>Michal</forenames></author><author><keyname>Aizenberg</keyname><forenames>Natalie</forenames></author><author><keyname>Bortnikov</keyname><forenames>Edward</forenames></author><author><keyname>Lempel</keyname><forenames>Ronny</forenames></author><author><keyname>Adadi</keyname><forenames>Roi</forenames></author><author><keyname>Benyamini</keyname><forenames>Tomer</forenames></author><author><keyname>Levin</keyname><forenames>Liron</forenames></author><author><keyname>Roth</keyname><forenames>Ran</forenames></author><author><keyname>Serfaty</keyname><forenames>Ohad</forenames></author></authors><title>OFF-Set: One-pass Factorization of Feature Sets for Online
  Recommendation in Persistent Cold Start Settings</title><categories>cs.LG cs.IR</categories><comments>8 pages, 3 figures, a shorter version is supposed to be published in
  RecSys13</comments><acm-class>H.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most challenging recommendation tasks is recommending to a new,
previously unseen user. This is known as the 'user cold start' problem.
Assuming certain features or attributes of users are known, one approach for
handling new users is to initially model them based on their features.
  Motivated by an ad targeting application, this paper describes an extreme
online recommendation setting where the cold start problem is perpetual. Every
user is encountered by the system just once, receives a recommendation, and
either consumes or ignores it, registering a binary reward.
  We introduce One-pass Factorization of Feature Sets, OFF-Set, a novel
recommendation algorithm based on Latent Factor analysis, which models users by
mapping their features to a latent space. Furthermore, OFF-Set is able to model
non-linear interactions between pairs of features. OFF-Set is designed for
purely online recommendation, performing lightweight updates of its model per
each recommendation-reward observation. We evaluate OFF-Set against several
state of the art baselines, and demonstrate its superiority on real
ad-targeting data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1797</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1797</id><created>2013-08-08</created><updated>2013-08-16</updated><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Introduction to Management Information system</title><categories>cs.OH</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Management Information System (MIS) is a systematic organization and
presentation of information that is generally required by the management of an
organization for taking better decisions for the organization. The MIS data may
be derived from various units of the organization or from other sources.
However it is very difficult to say the exact structure of MIS as the structure
and goals of different types of organizations are different. Hence both the
data and structure of MIS is dependent on the type of organization and often
customized to the specific requirement of the management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1801</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1801</id><created>2013-08-08</created><authors><author><keyname>Tamouk</keyname><forenames>Jamshid</forenames></author><author><keyname>Lotfi</keyname><forenames>Nasser</forenames></author><author><keyname>Farmanbar</keyname><forenames>Mina</forenames></author></authors><title>Satellite image classification methods and Landsat 5TM bands</title><categories>cs.CV astro-ph.IM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to find the most accurate classification method among
parallelepiped, minimum distance and chain methods. Moreover, this study also
challenges to find the suitable combination of bands, which can lead to better
results in case combinations of bands occur. After comparing these three
methods, the chain method over perform the other methods with 79% overall
accuracy. Hence, it is more accurate than minimum distance with 67% and
parallelepiped with 65%. On the other hand, based on bands features, and also
by combining several researchers' findings, a table was created which includes
the main objects on the land and the suitable combination of the bands for
accurately detecting of landcover objects. During this process, it was observed
that band 4 (out of 7 bands of Landsat 5TM) is the band, which can be used for
increasing the accuracy of the combined bands in detecting objects on the land.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1802</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1802</id><created>2013-08-08</created><updated>2014-06-04</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author></authors><title>Editing to a Connected Graph of Given Degrees</title><categories>cs.DS cs.DM math.CO</categories><comments>Some proofs are simplified and typos are corrected in this version</comments><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of edge editing or modification problems is to change a given graph
by adding and deleting of a small number of edges in order to satisfy a certain
property. We consider the Edge Editing to a Connected Graph of Given Degrees
problem that asks for a graph G, non-negative integers d,k and a function
\delta:V(G)-&gt;{1,...,d}, whether it is possible to obtain a connected graph G'
from G such that the degree of v is \delta(v) for any vertex v by at most k
edge editing operations. As the problem is NP-complete even if \delta(v)=2, we
are interested in the parameterized complexity and show that Edge Editing to a
Connected Graph of Given Degrees admits a polynomial kernel when parameterized
by d+k. For the special case \delta(v)=d, i.e., when the aim is to obtain a
connected d-regular graph, the problem is shown to be fixed parameter tractable
when parameterized by k only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1806</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1806</id><created>2013-08-08</created><authors><author><keyname>Mittal</keyname><forenames>Gaurav</forenames></author><author><keyname>Kesswani</keyname><forenames>Dr. Nishtha</forenames></author><author><keyname>Goswami</keyname><forenames>Kuldeep</forenames></author></authors><title>A Survey of Current Trends in Distributed, Grid and Cloud Computing</title><categories>cs.DC</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Studies in Computers, Science &amp;
  Engineering (IJASCSE Vol 2, Issue 3, 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through the 1990s to 2012 the internet changed the world of computing
drastically. It started its journey with parallel computing after it advanced
to distributed computing and further to grid computing. And in present scenario
it creates a new world which is pronounced as a Cloud Computing [1]. These all
three terms have different meanings. Cloud computing is based on backward
computing schemes like cluster computing, distributed computing, grid computing
and utility computing. The basic concept of cloud computing is virtualization.
It provides virtual hardware and software resources to various requesting
programs. This paper gives a detailed description about cluster computing, grid
computing and cloud computing and gives an insight of some implementations of
the same. We try to list the inspirations for the advent of all these
technologies. We also account for some present scenario faults of grid
computing and also discuss new cloud computing projects which are being managed
by the Government of India for learning. The paper also reviews the existing
work and covers (analytically), to some extent, some innovative ideas that can
be implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1809</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1809</id><created>2013-08-08</created><authors><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author><author><keyname>Salem</keyname><forenames>Amer O. Abu</forenames></author></authors><title>An Indoor Fingerprinting Localization Approach for ZigBee Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>13 pages</comments><journal-ref>European Journal of Scientific Research ISSN 1450-216X / 1450-202X
  Vol. 105 No 2 July, 2013, pp.190-202</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location tracking systems are increasingly becoming the focus of research in
the field of Wireless Sensor Network (WSN). Received Signal Strength
(RSS)-based localization systems are at the forefront of tracking research
applications. Radio location fingerprinting is one of the most promising indoor
positioning approaches due to its powerful in terms of accuracy and cost.
However, fingerprinting systems require the collection of a large number of
reference points in the tracking area to achieve reasonable localization
accuracy. In this paper, we propose a fingerprinting localization approach
based on a RSS technique. The proposed system does not require gathering a
large number of reference points and offers good localization accuracy indoors.
The implemented approach is based on dividing the tracking area into subareas
and assigning a unique feature to each subarea through ranging the RSS values
from different reference points. In order to test the proposed system's
efficiency, a number of real experiments have been conducted using Jennic
sensor nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1817</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1817</id><created>2013-08-08</created><authors><author><keyname>Saari</keyname><forenames>Pasi</forenames></author><author><keyname>Eerola</keyname><forenames>Tuomas</forenames></author></authors><title>Semantic Computing of Moods Based on Tags in Social Media of Music</title><categories>cs.MM cs.IR cs.SI</categories><comments>Preprint, 14 pages</comments><journal-ref>IEEE Transactions on Knowledge and Data Engineering, 2013</journal-ref><doi>10.1109/TKDE.2013.128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social tags inherent in online music services such as Last.fm provide a rich
source of information on musical moods. The abundance of social tags makes this
data highly beneficial for developing techniques to manage and retrieve mood
information, and enables study of the relationships between music content and
mood representations with data substantially larger than that available for
conventional emotion research. However, no systematic assessment has been done
on the accuracy of social tags and derived semantic models at capturing mood
information in music. We propose a novel technique called Affective Circumplex
Transformation (ACT) for representing the moods of music tracks in an
interpretable and robust fashion based on semantic computing of social tags and
research in emotion modeling. We validate the technique by predicting listener
ratings of moods in music tracks, and compare the results to prediction with
the Vector Space Model (VSM), Singular Value Decomposition (SVD), Nonnegative
Matrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA).
The results show that ACT consistently outperforms the baseline techniques, and
its performance is robust against a low number of track-level mood tags. The
results give validity and analytical insights for harnessing millions of music
tracks and associated mood data available through social tags in application
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1820</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1820</id><created>2013-08-08</created><updated>2014-03-31</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author></authors><title>Parameterized Algorithms for Load Coloring Problem</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One way to state the Load Coloring Problem (LCP) is as follows. Let $G=(V,E)$
be graph and let $f:V\rightarrow \{{\rm red}, {\rm blue}\}$ be a 2-coloring. An
edge $e\in E$ is called red (blue) if both end-vertices of $e$ are red (blue).
For a 2-coloring $f$, let $r'_f$ and $b'_f$ be the number of red and blue edges
and let $\mu_f(G)=\min\{r'_f,b'_f\}$. Let $\mu(G)$ be the maximum of $\mu_f(G)$
over all 2-colorings.
  We introduce the parameterized problem $k$-LCP of deciding whether $\mu(G)\ge
k$, where $k$ is the parameter. We prove that this problem admits a kernel with
at most $7k$. Ahuja et al. (2007) proved that one can find an optimal
2-coloring on trees in polynomial time. We generalize this by showing that an
optimal 2-coloring on graphs with tree decomposition of width $t$ can be found
in time $O^*(2^t)$. We also show that either $G$ is a Yes-instance of $k$-LCP
or the treewidth of $G$ is at most $2k$. Thus, $k$-LCP can be solved in time
$O^*(4^k).$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1827</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1827</id><created>2013-08-08</created><updated>2014-06-24</updated><authors><author><keyname>Ishteva</keyname><forenames>Mariya</forenames></author><author><keyname>Usevich</keyname><forenames>Konstantin</forenames></author><author><keyname>Markovsky</keyname><forenames>Ivan</forenames></author></authors><title>Factorization approach to structured low-rank approximation with
  applications</title><categories>math.NA cs.NA</categories><comments>Accepted for publication in SIAM Journal on Matrix Analysis and
  Applications (SIMAX)</comments><msc-class>15A23, 15A83, 65F99, 93B30, 37M10, 37N30, 11A05, 15A69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating an affinely structured matrix, for
example a Hankel matrix, by a low-rank matrix with the same structure. This
problem occurs in system identification, signal processing and computer
algebra, among others. We impose the low-rank by modeling the approximation as
a product of two factors with reduced dimension. The structure of the low-rank
model is enforced by introducing a penalty term in the objective function. The
proposed local optimization algorithm is able to solve the weighted structured
low-rank approximation problem, as well as to deal with the cases of missing or
fixed elements. In contrast to approaches based on kernel representations (in
linear algebraic sense), the proposed algorithm is designed to address the case
of small targeted rank. We compare it to existing approaches on numerical
examples of system identification, approximate greatest common divisor problem,
and symmetric tensor decomposition and demonstrate its consistently good
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1832</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1832</id><created>2013-08-08</created><authors><author><keyname>Kliemann</keyname><forenames>Lasse</forenames></author></authors><title>The Price of Anarchy in Bilateral Network Formation in an Adversary
  Model</title><categories>cs.GT</categories><comments>6th International Symposium on Algorithmic Game Theory (SAGT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study network formation with the bilateral link formation rule (Jackson
and Wolinsky 1996) with $n$ players and link cost $\alpha&gt;0$. After the network
is built, an adversary randomly destroys one link according to a certain
probability distribution. Cost for player $v$ incorporates the expected number
of players to which $v$ will become disconnected. This model was previously
studied for unilateral link formation (K. 2011).
  We prove existence of pairwise Nash equilibria under moderate assumptions on
the adversary and $n\geq 9$. As the main result, we prove bounds on the price
of anarchy for two special adversaries: one destroys a link chosen uniformly at
random, while the other destroys a link that causes a maximum number of player
pairs to be separated. We prove bounds tight up to constants, namely $O(1)$ for
one adversary (if $\alpha&gt;1/2$), and $\Theta(n)$ for the other (if $\alpha&gt;2$
considered constant and $n \geq 9$). The latter is the worst that can happen
for any adversary in this model (if $\alpha=\Omega(1)$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1838</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1838</id><created>2013-08-08</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Peters</keyname><forenames>Isabella</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy R.</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author></authors><title>Tweeting biomedicine: an analysis of tweets and citations in the
  biomedical literature</title><categories>cs.DL</categories><comments>22 pages, 4 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data collected by social media platforms have recently been introduced as a
new source for indicators to help measure the impact of scholarly research in
ways that are complementary to traditional citation-based indicators. Data
generated from social media activities related to scholarly content can be used
to reflect broad types of impact. This paper aims to provide systematic
evidence regarding how often Twitter is used to diffuse journal articles in the
biomedical and life sciences. The analysis is based on a set of 1.4 million
documents covered by both PubMed and Web of Science (WoS) and published between
2010 and 2012. The number of tweets containing links to these documents was
analyzed to evaluate the degree to which certain journals, disciplines, and
specialties were represented on Twitter. It is shown that, with less than 10%
of PubMed articles mentioned on Twitter, its uptake is low in general. The
relationship between tweets and WoS citations was examined for each document at
the level of journals and specialties. The results show that tweeting behavior
varies between journals and specialties and correlations between tweets and
citations are low, implying that impact metrics based on tweets are different
from those based on citations. A framework utilizing the coverage of articles
and the correlation between Twitter mentions and citations is proposed to
facilitate the evaluation of novel social-media based metrics and to shed light
on the question in how far the number of tweets is a valid metric to measure
research impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1839</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1839</id><created>2013-08-08</created><authors><author><keyname>Seyedebrahimi</keyname><forenames>Mirghiasaldin</forenames></author><author><keyname>Bailey</keyname><forenames>Colin</forenames></author><author><keyname>Peng</keyname><forenames>Xiao-Hong</forenames></author></authors><title>Model and Performance of a No-Reference Quality Assessment Metric for
  Video Streaming</title><categories>cs.MM</categories><comments>To appear in IEEE Transactions on Circuits and Systems for Video
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video streaming via TCP networks has become a popular and highly demanded
service, but its quality assessment in both objective and subjective terms has
not been properly addressed. In this paper, based on statistical analysis a
full analytic model of a no-reference objective metric, namely Pause Intensity,
for video quality assessment is presented. The model characterizes the video
playout buffer behavior in connection with the network performance (throughput)
and the video playout rate. This allows for instant quality measurement and
control without requiring a reference video. Pause intensity specifically
addresses the need for assessing the quality issue in terms of the continuity
in the playout of TCP streaming videos, which cannot be properly measured by
other objective metrics such as PSNR, SSIM and buffer underrun or pause
frequency. The performance of the analytical model is rigidly verified by
simulation results and subjective tests using a range of video clips. It is
demonstrated that pause intensity is closely correlated with viewer opinion
scores regardless of the vastly different composition of individual elements,
such as pause duration and pause frequency which jointly constitute this new
quality metric. It is also shown that the correlation performance of pause
intensity is consistent and content independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1846</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1846</id><created>2013-08-08</created><authors><author><keyname>Astoul</keyname><forenames>Anthony</forenames></author><author><keyname>Filliter</keyname><forenames>Christopher</forenames></author><author><keyname>Mason</keyname><forenames>Eric</forenames></author><author><keyname>Rau-Chaplin</keyname><forenames>Andrew</forenames></author><author><keyname>Shridhar</keyname><forenames>Kunal</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Varshney</keyname><forenames>Naman</forenames></author></authors><title>Developing and Testing the Automated Post-Event Earthquake Loss
  Estimation and Visualisation (APE-ELEV) Technique</title><categories>cs.CY physics.geo-ph</categories><comments>Bulletin of Earthquake Engineering, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automated, real-time, multiple sensor data source relying and globally
applicable earthquake loss model and visualiser is desirable for post-event
earthquake analysis. To achieve this there is a need to support rapid data
ingestion, loss estimation and integration of data from multiple data sources
and rapid visualisation at multiple geographic levels. In this paper, the
design and development of the Automated Post-Event Earthquake Loss Estimation
and Visualisation (APE-ELEV) system for real-time estimation and visualisation
of insured losses incurred due to earthquakes is presented. A model for
estimating ground up and net of facultative losses due to earthquakes in near
real-time is implemented. Since post-event data is often available immediately
from multiple disparate sources, a geo-browser is employed to facilitate the
visualisation and integration of earthquake hazard, exposure and loss data. The
feasibility of APE-ELEV is demonstrated using a test case earthquake that
occurred in Tohoku, Japan (2011). The APE-ELEV model is further validated for
ten global earthquakes using industry loss data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1847</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1847</id><created>2013-08-08</created><updated>2013-08-16</updated><authors><author><keyname>Nguyen</keyname><forenames>Vu Dung</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>The Royal Birth of 2013: Analysing and Visualising Public Sentiment in
  the UK Using Twitter</title><categories>cs.CL cs.IR cs.SI physics.soc-ph</categories><comments>http://www.blessonv.com/research/publicsentiment/ 9 pages. Submitted
  to IEEE BigData 2013: Workshop on Big Humanities, October 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of information retrieved from microblogging services such as Twitter
can provide valuable insight into public sentiment in a geographic region. This
insight can be enriched by visualising information in its geographic context.
Two underlying approaches for sentiment analysis are dictionary-based and
machine learning. The former is popular for public sentiment analysis, and the
latter has found limited use for aggregating public sentiment from Twitter
data. The research presented in this paper aims to extend the machine learning
approach for aggregating public sentiment. To this end, a framework for
analysing and visualising public sentiment from a Twitter corpus is developed.
A dictionary-based approach and a machine learning approach are implemented
within the framework and compared using one UK case study, namely the royal
birth of 2013. The case study validates the feasibility of the framework for
analysis and rapid visualisation. One observation is that there is good
correlation between the results produced by the popular dictionary-based
approach and the machine learning approach when large volumes of tweets are
analysed. However, for rapid analysis to be possible faster methods need to be
developed using big data techniques and parallel methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1857</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1857</id><created>2013-08-08</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Pollyanna</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author></authors><title>PANAS-t: A Pychometric Scale for Measuring Sentiments on Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online social networks have become a major communication platform, where
people share their thoughts and opinions about any topic real-time. The short
text updates people post in these network contain emotions and moods, which
when measured collectively can unveil the public mood at population level and
have exciting implications for businesses, governments, and societies.
Therefore, there is an urgent need for developing solid methods for accurately
measuring moods from large-scale social media data. In this paper, we propose
PANAS-t, which measures sentiments from short text updates in Twitter based on
a well-established psychometric scale, PANAS (Positive and Negative Affect
Schedule). We test the efficacy of PANAS-t over 10 real notable events drawn
from 1.8 billion tweets and demonstrate that it can efficiently capture the
expected sentiments of a wide variety of issues spanning tragedies, technology
releases, political debates, and healthcare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1860</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1860</id><created>2013-08-08</created><updated>2013-11-09</updated><authors><author><keyname>Cioaca</keyname><forenames>Alexandru</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>An Optimization Framework to Improve 4D-Var Data Assimilation System
  Performance</title><categories>cs.CE</categories><report-no>CSL-TR-4-2013</report-no><doi>10.1016/j.jcp.2014.07.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a computational framework for optimizing the parameters
of data assimilation systems in order to improve their performance. The
approach formulates a continuous meta-optimization problem for parameters; the
meta-optimization is constrained by the original data assimilation problem. The
numerical solution process employs adjoint models and iterative solvers. The
proposed framework is applied to optimize observation values, data weighting
coefficients, and the location of sensors for a test problem. The ability to
optimize a distributed measurement network is crucial for cutting down
operating costs and detecting malfunctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1876</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1876</id><created>2013-08-08</created><authors><author><keyname>Zandi</keyname><forenames>Ehsan</forenames></author><author><keyname>Dartmann</keyname><forenames>Guido</forenames></author><author><keyname>Ascheid</keyname><forenames>Gerd</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>A Non-Alternating Algorithm for Joint BS-RS Precoding Design in Two-Way
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>submitted at IEEE TVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative relay systems have become an active area of research during
recent years since they help cellular networks to enhance data rate and
coverage. In this paper we develop a method to jointly optimize precoding
matrices for amplify-and-forward relay station and base station. Our objective
is to increase max-min SINR fairness within co-channel users in a cell. The
main achievement of this work is avoiding any tedious alternating optimization
for joint design of RS/BS precoders, in order to save complexity. Moreover, no
convex solver is required in this method. RS precoding is done by transforming
the underlying non-convex problem into a system of nonlinear equations which is
then solved using Levenberg-Marquardt algorithm. This method for RS precoder
design is guaranteed to converge to a local optimum. For the BS precoder a
low-complexity iterative method is proposed. The efficiency of the joint
optimization method is verified by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1887</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1887</id><created>2013-08-08</created><updated>2013-08-12</updated><authors><author><keyname>Cook</keyname><forenames>John</forenames></author><author><keyname>Primmer</keyname><forenames>Robert</forenames></author><author><keyname>de Kwant</keyname><forenames>Ab</forenames></author></authors><title>Comparing cost and performance of replication and erasure coding</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures, 5 forumulas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data storage systems are more reliable than their individual components. In
order to build highly reliable systems out of less reliable parts, systems
introduce redundancy. In replicated systems, objects are simply copied several
times with each copy residing on a different physical device. While such an
approach is simple and direct, more elaborate approaches such as erasure coding
can achieve equivalent levels of data protection while using less redundancy.
This report examines the trade-offs in cost and performance between replicated
and erasure encoded storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1888</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1888</id><created>2013-08-08</created><authors><author><keyname>Hutter</keyname><forenames>Dieter</forenames></author><author><keyname>Monroy</keyname><forenames>Raul</forenames></author></authors><title>Strand-Based Approach to Patch Security Protocols</title><categories>cs.CR</categories><comments>36 pages</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a mechanism that aims to speed up the development
cycle of security protocols, by adding automated aid for diagnosis and repair.
Our mechanism relies on existing verification tools analyzing intermediate
protocols and synthesizing potential attacks if the protocol is flawed. The
analysis of these attacks (including type flaw attacks) pinpoints the source of
the failure and controls the synthesis of appropriate patches to the protocol.
Using strand spaces, we have developed general guidelines for protocol repair,
and captured them into formal requirements on (sets of) protocol steps. For
each requirement, there is a collection of rules that transform a set of
protocol steps violating the requirement into a set conforming it. We have
implemented our mechanism into a tool, called SHRIMP. We have successfully
tested SHRIMP on numerous faulty protocols, all of which were successfully
repaired, fully automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1889</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1889</id><created>2013-08-08</created><authors><author><keyname>Seiler</keyname><forenames>Peter</forenames></author></authors><title>SOSOPT: A Toolbox for Polynomial Optimization</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SOSOPT is a Matlab toolbox for formulating and solving Sum-of-Squares (SOS)
polynomial optimizations. This document briefly describes the use and
functionality of this toolbox. Section 1 introduces the problem formulations
for SOS tests, SOS feasibility problems, SOS optimizations, and generalized SOS
problems. Section 2 reviews the SOSOPT toolbox for solving these optimizations.
This section includes information on toolbox installation, formulating
constraints, solving SOS optimizations, and setting optimization options.
Finally, Section 3 briefly reviews the connections between SOS optimizations
and semide?nite programs (SDPs). It is the connection to SDPs that enables SOS
optimizations to be solved in an efficient manner
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1901</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1901</id><created>2013-08-08</created><authors><author><keyname>Primmer</keyname><forenames>Robert</forenames></author></authors><title>Distributed Object Store Principles of Operation The Case for
  Intelligent Storage</title><categories>cs.SE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at the growth of distributed object stores (DOS) and
examine the underlying mechanisms that guide their use and development. Our
focus is on the fundamental principles of operation that define this class of
system, how it has evolved, and where it is heading as new markets expand
beyond the use originally presented. We conclude by speculating about how
object stores as a class must evolve to meet the more demanding requirements of
future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1911</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1911</id><created>2013-08-08</created><authors><author><keyname>Aggarwal</keyname><forenames>Saurabh</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Social optimum in Social Groups with Give-and-Take criterion</title><categories>cs.NI cs.DS</categories><comments>Submitted for review to INFOCOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a &quot;Social Group&quot; of networked nodes, seeking a &quot;universe&quot; of
segments. Each node has subset of the universe, and access to an expensive
resource for downloading data. Alternatively, nodes can also acquire the
universe by exchanging segments among themselves, at low cost, using a local
network interface. While local exchanges ensure minimum cost, &quot;free riders&quot; in
the group can exploit the system. To prohibit free riding, we propose the
&quot;Give-and-Take&quot; criterion, where exchange is allowed if each node has segments
unavailable with the other. Under this criterion, we consider the problem of
maximizing the aggregate cardinality of the nodes' segment sets. First, we
present a randomized algorithm, whose analysis yields a lower bound on the
expected aggregate cardinality, as well as an approximation ratio of 1/4 under
some conditions. Four other algorithms are presented and analyzed. We identify
conditions under which some of these algorithms are optimal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1940</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1940</id><created>2013-08-08</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Tamas</keyname><forenames>Wani W.</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Balu</keyname><forenames>Aur&#xe9;lia</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author><author><keyname>Notton</keyname><forenames>Gilles</forenames><affiliation>SPE</affiliation></author></authors><title>Time series modeling with pruned multi-layer perceptron and 2-stage
  damped least-squares method</title><categories>cs.NE</categories><proxy>ccsd</proxy><journal-ref>International Conference on Mathematical Modeling in Physical
  Sciences, IC-MSQUARE, Czech Republic (2013)</journal-ref><doi>10.1088/1742-6596/490/1/012040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks
often used in TS modeling and forecasting. Because of its &quot;black box&quot; aspect,
many researchers refuse to use it. Moreover, the optimization (often based on
the exhaustive approach where &quot;all&quot; configurations are tested) and learning
phases of this artificial intelligence tool (often based on the
Levenberg-Marquardt algorithm; LMA) are weaknesses of this approach
(exhaustively and local minima). These two tasks must be repeated depending on
the knowledge of each new problem studied, making the process, long, laborious
and not systematically robust. In this paper a pruning process is proposed.
This method allows, during the training phase, to carry out an inputs selecting
method activating (or not) inter-nodes connections in order to verify if
forecasting is improved. We propose to use iteratively the popular damped
least-squares method to activate inputs and neurons. A first pass is applied to
10% of the learning sample to determine weights significantly different from 0
and delete other. Then a classical batch process based on LMA is used with the
new MLP. The validation is done using 25 measured meteorological TS and
cross-comparing the prediction results of the classical LMA and the 2-stage
LMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1941</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1941</id><created>2013-08-08</created><updated>2013-09-19</updated><authors><author><keyname>Estrada</keyname><forenames>Octavio Andr&#xe9;s Gonz&#xe1;lez</forenames><affiliation>IMAM</affiliation></author><author><keyname>Garc&#xed;a</keyname><forenames>Juan Jos&#xe9; R&#xf3;denas</forenames><affiliation>DIMM</affiliation></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane</forenames><affiliation>CITV</affiliation></author><author><keyname>Nadal</keyname><forenames>E.</forenames><affiliation>CITV</affiliation></author><author><keyname>Kerfriden</keyname><forenames>Pierre</forenames><affiliation>CITV</affiliation></author><author><keyname>Fuenmayor</keyname><forenames>F. J.</forenames><affiliation>CITV</affiliation></author></authors><title>Locally equilibrated stress recovery for goal oriented error estimation
  in the extended finite element method</title><categories>math.NA cs.NA physics.class-ph</categories><comments>arXiv admin note: text overlap with arXiv:1209.3102</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal oriented error estimation and adaptive procedures are essential for the
accurate and efficient evaluation of numerical simulations that involve complex
domains. By locally improving the approximation quality we can solve expensive
problems which could result intractable otherwise. Here, we present an error
estimation technique for enriched finite element approximations that is based
on an equilibrated recovery technique, which considers the stress intensity
factor as the quantity of interest. The locally equilibrated superconvergent
patch recovery is used to obtain enhanced stress fields for the primal and dual
problems defined to evaluate the error estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1946</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1946</id><created>2013-08-07</created><authors><author><keyname>Bartoli</keyname><forenames>A.</forenames></author><author><keyname>Medvet</keyname><forenames>E.</forenames></author></authors><title>Citation Counts and Evaluation of Researchers in the Internet Age</title><categories>cs.DL cs.CY</categories><comments>4 pages, 2 figures, 3 tables</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bibliometric measures derived from citation counts are increasingly being
used as a research evaluation tool. Their strengths and weaknesses have been
widely analyzed in the literature and are often subject of vigorous debate. We
believe there are a few fundamental issues related to the impact of the web
that are not taken into account with the importance they deserve. We focus on
evaluation of researchers, but several of our arguments may be applied also to
evaluation of research institutions as well as of journals and conferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1947</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1947</id><created>2013-08-08</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Interdependent network reciprocity in evolutionary games</title><categories>physics.soc-ph cs.GT cs.SI q-bio.PE</categories><comments>7 two-column pages, 6 figures; accepted for publication in Scientific
  Reports</comments><journal-ref>Sci. Rep. 3 (2013) 1183</journal-ref><doi>10.1038/srep01183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides the structure of interactions within networks, also the interactions
between networks are of the outmost importance. We therefore study the outcome
of the public goods game on two interdependent networks that are connected by
means of a utility function, which determines how payoffs on both networks
jointly influence the success of players in each individual network. We show
that an unbiased coupling allows the spontaneous emergence of interdependent
network reciprocity, which is capable to maintain healthy levels of public
cooperation even in extremely adverse conditions. The mechanism, however,
requires simultaneous formation of correlated cooperator clusters on both
networks. If this does not emerge or if the coordination process is disturbed,
network reciprocity fails, resulting in the total collapse of cooperation.
Network interdependence can thus be exploited effectively to promote
cooperation past the limits imposed by isolated networks, but only if the
coordination between the interdependent networks is not disturbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1963</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1963</id><created>2013-08-08</created><authors><author><keyname>Keranidis</keyname><forenames>Stratos</forenames></author><author><keyname>Kazdaridis</keyname><forenames>Giannis</forenames></author><author><keyname>Makris</keyname><forenames>Nikos</forenames></author><author><keyname>Korakis</keyname><forenames>Thanasis</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Evolution of IEEE 802.11 compatible standards and impact on Energy
  Consumption</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, the IEEE 802.11 has emerged as the most popular
protocol in the wireless domain. Since the release of the first standard
version, several amendments have been introduced in an effort to improve its
throughput performance, with the most recent one being the IEEE 802.11n
extension. In this work, we present detailed experimentally obtained
measurements that evaluate the energy efficiency of the base standard in
comparison with the latest 802.11n version. Moreover, we investigate the impact
of various MAC layer enhancements, both vendor specific and standard compliant
ones, on the energy consumption of wireless transceivers and total nodes as
well. Results obtained under a wide range of settings, indicate that the latest
standard enables reduction of energy expenditure, by more than 75%, when
combined with innovative frame aggregation mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1968</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1968</id><created>2013-08-08</created><authors><author><keyname>Rahimian</keyname><forenames>M. Amin</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Detection and Isolation of Link Failures under the Agreement Protocol</title><categories>cs.SY cs.SI math.DS math.OC</categories><comments>6 pages, 3 figures, IEEE Conference on Decision and Control, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a property of the multi-agent consensus dynamics that relates
the failure of links in the network to jump discontinuities in the derivatives
of the output responses of the nodes is derived and verified analytically. At
the next step, an algorithm for sensor placement is proposed, which would
enable the designer to detect and isolate any link failures across the network
based on the observed jump discontinuities in the derivatives of the responses
of a subset of nodes. These results are explained through elaborative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1971</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1971</id><created>2013-08-08</created><authors><author><keyname>Zhu</keyname><forenames>Ji</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Tree dynamics for peer-to-peer streaming</title><categories>cs.DS cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an asynchronous distributed algorithm to manage multiple
trees for peer-to-peer streaming in a flow level model. It is assumed that
videos are cut into substreams, with or without source coding, to be
distributed to all nodes. The algorithm guarantees that each node receives
sufficiently many substreams within delay logarithmic in the number of peers.
The algorithm works by constantly updating the topology so that each substream
is distributed through trees to as many nodes as possible without interference.
Competition among trees for limited upload capacity is managed so that both
coverage and balance are achieved. The algorithm is robust in that it
efficiently eliminates cycles and maintains tree structures in a distributed
way. The algorithm favors nodes with higher degree, so it not only works for
live streaming and video on demand, but also in the case a few nodes with large
degree act as servers and other nodes act as clients.
  A proof of convergence of the algorithm is given assuming instantaneous
update of depth information, and for the case of a single tree it is shown that
the convergence time is stochastically tightly bounded by a small constant
times the log of the number of nodes. These theoretical results are
complemented by simulations showing that the algorithm works well even when
most assumptions for the theoretical tractability do not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1975</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1975</id><created>2013-08-08</created><updated>2013-08-19</updated><authors><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Predicting protein contact map using evolutionary and physical
  constraints by integer programming (extended version)</title><categories>q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML</categories><comments>14 pages, 13 figures, 10 tables</comments><journal-ref>Bioinformatics (2013) 29 (13): i266-i273</journal-ref><doi>10.1093/bioinformatics/btt211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation. Protein contact map describes the pairwise spatial and functional
relationship of residues in a protein and contains key information for protein
3D structure prediction. Although studied extensively, it remains very
challenging to predict contact map using only sequence information. Most
existing methods predict the contact map matrix element-by-element, ignoring
correlation among contacts and physical feasibility of the whole contact map. A
couple of recent methods predict contact map based upon residue co-evolution,
taking into consideration contact correlation and enforcing a sparsity
restraint, but these methods require a very large number of sequence homologs
for the protein under consideration and the resultant contact map may be still
physically unfavorable.
  Results. This paper presents a novel method PhyCMAP for contact map
prediction, integrating both evolutionary and physical restraints by machine
learning and integer linear programming (ILP). The evolutionary restraints
include sequence profile, residue co-evolution and context-specific statistical
potential. The physical restraints specify more concrete relationship among
contacts than the sparsity restraint. As such, our method greatly reduces the
solution space of the contact map matrix and thus, significantly improves
prediction accuracy. Experimental results confirm that PhyCMAP outperforms
currently popular methods no matter how many sequence homologs are available
for the protein under consideration. PhyCMAP can predict contacts within
minutes after PSIBLAST search for sequence homologs is done, much faster than
the two recent methods PSICOV and EvFold.
  See http://raptorx.uchicago.edu for the web server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1978</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1978</id><created>2013-08-08</created><authors><author><keyname>Kowalski</keyname><forenames>Jakub</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>A New Heuristic Synchronizing Algorithm</title><categories>cs.FL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new heuristic algorithm finding reset words. The algorithm
called CutOff-IBFS is based on a simple idea of inverse breadth-first-search in
the power automaton. We perform an experimental investigation of effectiveness
compared to other algorithms existing in literature, which yields that our
method generally finds a shorter word in an average case and works well in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1981</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1981</id><created>2013-08-08</created><updated>2014-03-13</updated><authors><author><keyname>Mitra</keyname><forenames>Kaushik</forenames></author><author><keyname>Cossairt</keyname><forenames>Oliver</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author></authors><title>A Framework for the Analysis of Computational Imaging Systems with
  Practical Applications</title><categories>cs.CV</categories><acm-class>I.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, a number of Computational Imaging (CI) systems have
been proposed for tasks such as motion deblurring, defocus deblurring and
multispectral imaging. These techniques increase the amount of light reaching
the sensor via multiplexing and then undo the deleterious effects of
multiplexing by appropriate reconstruction algorithms. Given the widespread
appeal and the considerable enthusiasm generated by these techniques, a
detailed performance analysis of the benefits conferred by this approach is
important.
  Unfortunately, a detailed analysis of CI has proven to be a challenging
problem because performance depends equally on three components: (1) the
optical multiplexing, (2) the noise characteristics of the sensor, and (3) the
reconstruction algorithm. A few recent papers have performed analysis taking
multiplexing and noise characteristics into account. However, analysis of CI
systems under state-of-the-art reconstruction algorithms, most of which exploit
signal prior models, has proven to be unwieldy. In this paper, we present a
comprehensive analysis framework incorporating all three components.
  In order to perform this analysis, we model the signal priors using a
Gaussian Mixture Model (GMM). A GMM prior confers two unique characteristics.
Firstly, GMM satisfies the universal approximation property which says that any
prior density function can be approximated to any fidelity using a GMM with
appropriate number of mixtures. Secondly, a GMM prior lends itself to
analytical tractability allowing us to derive simple expressions for the
`minimum mean square error' (MMSE), which we use as a metric to characterize
the performance of CI systems. We use our framework to analyze several
previously proposed CI techniques, giving conclusive answer to the question:
`How much performance gain is due to use of a signal prior and how much is due
to multiplexing?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1986</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1986</id><created>2013-08-08</created><authors><author><keyname>Irving</keyname><forenames>Geoffrey</forenames></author><author><keyname>Green</keyname><forenames>Forrest</forenames></author></authors><title>A deterministic pseudorandom perturbation scheme for arbitrary
  polynomial predicates</title><categories>cs.CG</categories><comments>15 pages, 2 figures</comments><msc-class>68U05</msc-class><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a symbolic perturbation scheme for arbitrary polynomial geometric
predicates which combines the benefits of Emiris and Canny's simple randomized
linear perturbation scheme with Yap's multiple infinitesimal scheme for general
predicates. Like the randomized scheme, our method accepts black box polynomial
functions as input. For nonmaliciously chosen predicates, our method is as fast
as the linear scheme, scaling reasonably with the degree of the polynomial even
for fully degenerate input. Like Yap's scheme, the computed sign is
deterministic, never requiring an algorithmic restart (assuming a high quality
pseudorandom generator), and works for arbitrary predicates with no knowledge
of their structure. We also apply our technique to exactly or nearly exactly
rounded constructions that work correctly for degenerate input, using
l'Hopital's rule to compute the necessary singular limits. We provide an open
source prototype implementation including example algorithms for Delaunay
triangulation and Boolean operations on polygons and circular arcs in the
plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.1995</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.1995</id><created>2013-08-08</created><updated>2013-10-28</updated><authors><author><keyname>Lin</keyname><forenames>Shuyang</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Predicting Trends in Social Networks via Dynamic Activeness Model</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, a shorter version published in CIKM 2013</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the effect of word-of-the-mouth, trends in social networks are now
playing a significant role in shaping people's lives. Predicting dynamic trends
is an important problem with many useful applications. There are three dynamic
characteristics of a trend that should be captured by a trend model: intensity,
coverage and duration. However, existing approaches on the information
diffusion are not capable of capturing these three characteristics. In this
paper, we study the problem of predicting dynamic trends in social networks. We
first define related concepts to quantify the dynamic characteristics of trends
in social networks, and formalize the problem of trend prediction. We then
propose a Dynamic Activeness (DA) model based on the novel concept of
activeness, and design a trend prediction algorithm using the DA model. Due to
the use of stacking principle, we are able to make the prediction algorithm
very efficient. We examine the prediction algorithm on a number of real social
network datasets, and show that it is more accurate than state-of-the-art
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2002</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2002</id><created>2013-08-08</created><updated>2014-03-29</updated><authors><author><keyname>Qin</keyname><forenames>Peng</forenames></author><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Huang</keyname><forenames>Benxiong</forenames></author><author><keyname>Xu</keyname><forenames>Guan</forenames></author></authors><title>Taking A Free Ride for Routing Topology Inference in Peer-to-Peer
  Networks</title><categories>cs.NI</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A Peer-to-Peer (P2P) network can boost its performance if peers are provided
with underlying network-layer routing topology. The task of inferring the
network-layer routing topology and link performance from an end host to a set
of other hosts is termed as network tomography, and it normally requires host
computers to send probing messages. We design a passive network tomography
method that does not requires any probing messages and takes a free ride over
data flows in P2P networks. It infers routing topology based on end-to-end
delay correlation estimation (DCE) without requiring any synchronization or
cooperation from the intermediate routers. We implement and test our method in
the real world Internet environment and achieved the accuracy of 92% in
topology recovery. We also perform extensive simulation in OMNet++ to evaluate
its performance over large scale networks, showing that its topology recovery
accuracy is about 95% for large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2003</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2003</id><created>2013-08-08</created><authors><author><keyname>Avci</keyname><forenames>Serhat Nazim</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Link Failure Recovery over Very Large Arbitrary Networks: The Case of
  Coding</title><categories>cs.NI</categories><comments>To be submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding-based link failure recovery techniques provide near-hitless
recovery and offer high capacity efficiency. Diversity coding is the first
technique to incorporate coding in this field and is easy to implement over
small arbitrary networks. However, its capacity efficiency is restricted by its
systematic coding and high design complexity even though its design complexity
is lower than the other coding-based recovery techniques. Alternative
techniques mitigate some of these limitations, but they are difficult to
implement over arbitrary networks. In this paper, we propose a simple column
generation-based design algorithm and a novel advanced diversity coding
technique to achieve near-hitless recovery over arbitrary networks. The design
framework consists of two parts: a main problem and subproblem. Main problem is
realized with Linear Programming (LP) and Integer Linear Programming (ILP),
whereas the subproblem can be realized with different methods. The simulation
results suggest that both the novel coding structure and the novel design
algorithm lead to higher capacity efficiency for near-hitless recovery. The
novel design algorithm simplifies the capacity placement problem which enables
implementing diversity coding-based techniques on very large arbitrary
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2013</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2013</id><created>2013-08-08</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Min-Max Design of FIR Digital Filters by Semidefinite Programming</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>MATLAB codes are available at
  http://www-ics.acs.i.kyoto-u.ac.jp/mat/fdf/</comments><journal-ref>Applications of Digital Signal Processing, pp.~193-210, InTech,
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we consider two problems: FIR (Finite Impulse Response)
approximation of IIR (Infinite Impulse Response) filters and inverse FIR
filtering of FIR or IIR filters. By means of Kalman-Yakubovich-Popov (KYP)
lemma and its generalization (GKYP), the problems are reduced to semidefinite
programming described in linear matrix inequalities (LMIs). MATLAB codes for
these design methods are given. An design example shows the effectiveness of
these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2015</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2015</id><created>2013-08-08</created><updated>2013-08-12</updated><authors><author><keyname>Malik</keyname><forenames>Nishant</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author></authors><title>Role of social environment and social clustering in spread of opinions
  in co-evolving networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1063/1.4833995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking a pragmatic approach to the processes involved in the phenomena of
collective opinion formation, we investigate two specific modifications to the
co-evolving network voter model of opinion formation, studied by Holme and
Newman [1]. First, we replace the rewiring probability parameter by a
distribution of probability of accepting or rejecting opinions between
individuals, accounting for the asymmetric influences in relationships among
individuals in a social group. Second, we modify the rewiring step by a
path-length-based preference for rewiring that reinforces local clustering. We
have investigated the influences of these modifications on the outcomes of the
simulations of this model. We found that varying the shape of the distribution
of probability of accepting or rejecting opinions can lead to the emergence of
two qualitatively distinct final states, one having several isolated connected
components each in internal consensus leading to the existence of diverse set
of opinions and the other having one single dominant connected component with
each node within it having the same opinion. Furthermore, and more importantly,
we found that the initial clustering in network can also induce similar
transitions. Our investigation also brings forward that these transitions are
governed by a weak and complex dependence on system size. We found that the
networks in the final states of the model have rich structural properties
including the small world property for some parameter regimes. [1] P. Holme and
M. Newman, Phys. Rev. E 74, 056108 (2006).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2027</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2027</id><created>2013-08-08</created><authors><author><keyname>Huang</keyname><forenames>Tao</forenames></author><author><keyname>Fan</keyname><forenames>Yi-Zheng</forenames></author><author><keyname>Zhu</keyname><forenames>Ming</forenames></author></authors><title>Symmetric Toeplitz-Structured Compressed Sensing Matrices</title><categories>cs.IT math.IT</categories><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to construct a suitable measurement matrix is still an open question in
compressed sensing. A significant part of the recent work is that the
measurement matrices are not completely random on the entries but exhibit
considerable structure. In this paper, we proved that the symmetric Toeplitz
matrix and its transforms can be used as measurement matrix and recovery signal
with high probability. Compared with random matrices (e.g. Gaussian and
Bernullio matrices) and some structured matrices (e.g. Toeplitz and circulant
matrices), we need to generate fewer independent entries to obtain the
measurement matrix while the effectiveness of recovery does not get worse.
Furthermore, the signal can be recovered more efficiently by the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2055</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2055</id><created>2013-08-09</created><updated>2013-08-22</updated><authors><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Morales</keyname><forenames>Jose</forenames></author></authors><title>Proceedings of the 23rd Workshop on Logic-based methods in Programming
  Environments (WLPE 2013)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the 23rd Workshop on Logic-based
Methods in Programming Environments (WLPE 2013), which was held in Istanbul,
Turkey, on August 24 &amp; 25 2013 as a satellite event of the 29th International
Conference on Logic Programming, (ICLP 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2058</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2058</id><created>2013-08-09</created><authors><author><keyname>Patel</keyname><forenames>Ishan</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>RBioCloud: A Light-weight Framework for Bioconductor and R-based Jobs on
  the Cloud</title><categories>cs.DC cs.CE cs.PF cs.SE</categories><comments>Webpage: http://www.rbiocloud.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale ad hoc analytics of genomic data is popular using the
R-programming language supported by 671 software packages provided by
Bioconductor. More recently, analytical jobs are benefitting from on-demand
computing and storage, their scalability and their low maintenance cost, all of
which are offered by the cloud. While Biologists and Bioinformaticists can take
an analytical job and execute it on their personal workstations, it remains
challenging to seamlessly execute the job on the cloud infrastructure without
extensive knowledge of the cloud dashboard. How analytical jobs can not only
with minimum effort be executed on the cloud, but also how both the resources
and data required by the job can be managed is explored in this paper. An
open-source light-weight framework for executing R-scripts using Bioconductor
packages, referred to as `RBioCloud', is designed and developed. RBioCloud
offers a set of simple command-line tools for managing the cloud resources, the
data and the execution of the job. Three biological test cases validate the
feasibility of RBioCloud. The framework is publicly available from
http://www.rbiocloud.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2063</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2063</id><created>2013-08-09</created><authors><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Khargonekar</keyname><forenames>Pramod P.</forenames></author></authors><title>Signal Reconstruction via H-infinity Sampled-Data Control Theory: Beyond
  the Shannon Paradigm</title><categories>cs.IT cs.SY math.IT math.OC</categories><journal-ref>IEEE Transactions on Signal Processing, Vol. 60, No. 2, pp.
  613-625, Feb. 2012</journal-ref><doi>10.1109/TSP.2011.2175223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method for signal reconstruction by leveraging
sampled-data control theory. We formulate the signal reconstruction problem in
terms of an analog performance optimization problem using a stable
discrete-time filter. The proposed H-infinity performance criterion naturally
takes intersample behavior into account, reflecting the energy distributions of
the signal. We present methods for computing optimal solutions which are
guaranteed to be stable and causal. Detailed comparisons to alternative methods
are provided. We discuss some applications in sound and image reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2066</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2066</id><created>2013-08-09</created><authors><author><keyname>Bahl</keyname><forenames>Aman</forenames></author><author><keyname>Baltzer</keyname><forenames>Oliver</forenames></author><author><keyname>Rau-Chaplin</keyname><forenames>Andrew</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author></authors><title>Parallel Simulations for Analysing Portfolios of Catastrophic Event Risk</title><categories>cs.DC cs.CE cs.PF</categories><comments>Proceedings of the Workshop at the International Conference for High
  Performance Computing, Networking, Storage and Analysis (SC), 2012, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of the analytical pipeline of a modern quantitative
insurance/reinsurance company is a stochastic simulation technique for
portfolio risk analysis and pricing process referred to as Aggregate Analysis.
Support for the computation of risk measures including Probable Maximum Loss
(PML) and the Tail Value at Risk (TVAR) for a variety of types of complex
property catastrophe insurance contracts including Cat eXcess of Loss (XL), or
Per-Occurrence XL, and Aggregate XL, and contracts that combine these measures
is obtained in Aggregate Analysis.
  In this paper, we explore parallel methods for aggregate risk analysis. A
parallel aggregate risk analysis algorithm and an engine based on the algorithm
is proposed. This engine is implemented in C and OpenMP for multi-core CPUs and
in C and CUDA for many-core GPUs. Performance analysis of the algorithm
indicates that GPUs offer an alternative HPC solution for aggregate risk
analysis that is cost effective. The optimised algorithm on the GPU performs a
1 million trial aggregate simulation with 1000 catastrophic events per trial on
a typical exposure set and contract structure in just over 20 seconds which is
approximately 15x times faster than the sequential counterpart. This can
sufficiently support the real-time pricing scenario in which an underwriter
analyses different contractual terms and pricing while discussing a deal with a
client over the phone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2069</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2069</id><created>2013-08-09</created><authors><author><keyname>Paajanen</keyname><forenames>Pirita</forenames></author></authors><title>Finite p-groups, entropy vectors and the Ingleton inequality for
  nilpotent groups</title><categories>cs.IT math.GR math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the capacity/entropy region of finite, directed,
acyclic, multiple-sources, multiple-sinks network by means of group theory and
entropy vectors coming from groups. There is a one-to-one correspondence
between the entropy vector of a collection of n random variables and a certain
group-characterizable vector obtained from a finite group and n of its
subgroups. We are looking at nilpotent group characterizable entropy vectors
and show that they are all also Abelian group characterizable, and hence they
satisfy the Ingleton inequality. It is known that not all entropic vectors can
be obtained from Abelian groups, so our result implies that in order to get
more exotic entropic vectors, one has to go at least to soluble groups or
larger nilpotency classes. The result also implies that Ingleton inequality is
satisfied by nilpotent groups of bounded class, depending on the order of the
group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2101</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2101</id><created>2013-08-09</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Imrich</keyname><forenames>Wilfried</forenames></author><author><keyname>Kupka</keyname><forenames>Tomas</forenames></author></authors><title>Fast Recognition of Partial Star Products and Quasi Cartesian Products</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the fast computation of a relation $\R$ on the
edge set of connected graphs that plays a decisive role in the recognition of
approximate Cartesian products, the weak reconstruction of Cartesian products,
and the recognition of Cartesian graph bundles with a triangle free basis.
  A special case of $\R$ is the relation $\delta^\ast$, whose convex closure
yields the product relation $\sigma$ that induces the prime factor
decomposition of connected graphs with respect to the Cartesian product. For
the construction of $\R$ so-called Partial Star Products are of particular
interest. Several special data structures are used that allow to compute
Partial Star Products in constant time. These computations are tuned to the
recognition of approximate graph products, but also lead to a linear time
algorithm for the computation of $\delta^\ast$ for graphs with maximum bounded
degree.
  Furthermore, we define \emph{quasi Cartesian products} as graphs with
non-trivial $\delta^\ast$. We provide several examples, and show that quasi
Cartesian products can be recognized in linear time for graphs with bounded
maximum degree. Finally, we note that quasi products can be recognized in
sublinear time with a parallelized algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2116</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2116</id><created>2013-08-09</created><updated>2014-06-01</updated><authors><author><keyname>K&#xfc;hlwein</keyname><forenames>Daniel</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>MaLeS: A Framework for Automatic Tuning of Automated Theorem Provers</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MaLeS is an automatic tuning framework for automated theorem provers. It
provides solutions for both the strategy finding as well as the strategy
scheduling problem. This paper describes the tool and the methods used in it,
and evaluates its performance on three automated theorem provers: E, LEO-II and
Satallax. An evaluation on a subset of the TPTP library problems shows that on
average a MaLeS-tuned prover solves 8.67% more problems than the prover with
its default settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2119</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2119</id><created>2013-08-09</created><authors><author><keyname>Keane</keyname><forenames>Mark</forenames></author></authors><title>Deconstructing analogy</title><categories>cs.AI</categories><comments>Published Chapter in Book from Conference; CogSc-12: ILCLI
  International Workshop on Cognitive Science. Universidad del Pais Vasco
  Press: San Sebastian, Spain. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analogy has been shown to be important in many key cognitive abilities,
including learning, problem solving, creativity and language change. For
cognitive models of analogy, the fundamental computational question is how its
inherent complexity (its NP-hardness) is solved by the human cognitive system.
Indeed, different models of analogical processing can be categorized by the
simplification strategies they adopt to make this computational problem more
tractable. In this paper, I deconstruct several of these models in terms of the
simplification-strategies they use; a deconstruction that provides some
interesting perspectives on the relative differences between them. Later, I
consider whether any of these computational simplifications reflect the actual
strategies used by people and sketch a new cognitive model that tries to
present a closer fit to the psychological evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2122</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2122</id><created>2013-08-09</created><updated>2014-06-25</updated><authors><author><keyname>Allamigeon</keyname><forenames>Xavier</forenames></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Katz</keyname><forenames>Ricardo D.</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author></authors><title>Tropical Fourier-Motzkin elimination, with an application to real-time
  verification</title><categories>math.CO cs.LO math.OC</categories><comments>29 pages, 8 figures</comments><msc-class>14T05, 52A01, 52B55</msc-class><journal-ref>International Journal of Algebra and Computation, 24(5) :569-607,
  2014</journal-ref><doi>10.1142/S0218196714500258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalization of tropical polyhedra able to express both
strict and non-strict inequalities. Such inequalities are handled by means of a
semiring of germs (encoding infinitesimal perturbations). We develop a tropical
analogue of Fourier-Motzkin elimination from which we derive geometrical
properties of these polyhedra. In particular, we show that they coincide with
the tropically convex union of (non-necessarily closed) cells that are convex
both classically and tropically. We also prove that the redundant inequalities
produced when performing successive elimination steps can be dynamically
deleted by reduction to mean payoff game problems. As a complement, we provide
a coarser (polynomial time) deletion procedure which is enough to arrive at a
simply exponential bound for the total execution time. These algorithms are
illustrated by an application to real-time systems (reachability analysis of
timed automata).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2124</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2124</id><created>2013-08-09</created><authors><author><keyname>Terekhov</keyname><forenames>Alexander V.</forenames></author><author><keyname>O'Regan</keyname><forenames>J. Kevin</forenames></author></authors><title>Space as an invention of biological organisms</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of the nature of space around us has occupied thinkers since the
dawn of humanity, with scientists and philosophers today implicitly assuming
that space is something that exists objectively. Here we show that this does
not have to be the case: the notion of space could emerge when biological
organisms seek an economic representation of their sensorimotor flow. The
emergence of spatial notions does not necessitate the existence of real
physical space, but only requires the presence of sensorimotor invariants
called `compensable' sensory changes. We show mathematically and then in
simulations that na\&quot;ive agents making no assumptions about the existence of
space are able to learn these invariants and to build the abstract notion that
physicists call rigid displacement, which is independent of what is being
displaced. Rigid displacements may underly perception of space as an unchanging
medium within which objects are described by their relative positions. Our
findings suggest that the question of the nature of space, currently exclusive
to philosophy and physics, should also be addressed from the standpoint of
neuroscience and artificial intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2140</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2140</id><created>2013-08-09</created><updated>2013-11-06</updated><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Axioms for Centrality</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a social network, which of its nodes are more central? This question
has been asked many times in sociology, psychology and computer science, and a
whole plethora of centrality measures (a.k.a. centrality indices, or rankings)
were proposed to account for the importance of the nodes of a network. In this
paper, we try to provide a mathematically sound survey of the most important
classic centrality measures known from the literature and propose an axiomatic
approach to establish whether they are actually doing what they have been
designed for. Our axioms suggest some simple, basic properties that a
centrality measure should exhibit.
  Surprisingly, only a new simple measure based on distances, harmonic
centrality, turns out to satisfy all axioms; essentially, harmonic centrality
is a correction to Bavelas's classic closeness centrality designed to take
unreachable nodes into account in a natural way.
  As a sanity check, we examine in turn each measure under the lens of
information retrieval, leveraging state-of-the-art knowledge in the discipline
to measure the effectiveness of the various indices in locating web pages that
are relevant to a query. While there are some examples of this comparisons in
the literature, here for the first time we take into consideration centrality
measures based on distances, such as closeness, in an information-retrieval
setting. The results match closely the data we gathered using our axiomatic
approach.
  Our results suggest that centrality measures based on distances, which have
been neglected in information retrieval in favour of spectral centrality
measures in the last years, are actually of very high quality; moreover,
harmonic centrality pops up as an excellent general-purpose centrality index
for arbitrary directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2142</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2142</id><created>2013-08-09</created><updated>2014-01-29</updated><authors><author><keyname>Wieringa</keyname><forenames>Siert</forenames></author></authors><title>Some notes on model rotation</title><categories>cs.LO</categories><comments>v1: Fixed a bug in the note after lemma 2.3 v2: Uploaded by accident
  (modified but incorrect definition of MES). v3: Fixed definition of MES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model rotation is an efficient technique for improving MUS finding
algorithms. In previous work we have studied model rotation as an algorithm
that traverses a graph which is induced by the input formula. This document
introduces the notion of blocked edges, which are edges in this graph that can
never be traversed. We show the existence of irredundant CNF formulas in which
some clauses are unreachable by model rotation. Additionally, we proof a
conjecture by Belov, Lynce and Marques-Silva.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2144</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2144</id><created>2013-08-09</created><updated>2013-08-12</updated><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>In-Core Computation of Geometric Centralities with HyperBall: A Hundred
  Billion Nodes and Beyond</title><categories>cs.DS cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a social network, which of its nodes are more central? This question
has been asked many times in sociology, psychology and computer science, and a
whole plethora of centrality measures (a.k.a. centrality indices, or rankings)
were proposed to account for the importance of the nodes of a network. In this
paper, we approach the problem of computing geometric centralities, such as
closeness and harmonic centrality, on very large graphs; traditionally this
task requires an all-pairs shortest-path computation in the exact case, or a
number of breadth-first traversals for approximated computations, but these
techniques yield very weak statistical guarantees on highly disconnected
graphs. We rather assume that the graph is accessed in a semi-streaming
fashion, that is, that adjacency lists are scanned almost sequentially, and
that a very small amount of memory (in the order of a dozen bytes) per node is
available in core memory. We leverage the newly discovered algorithms based on
HyperLogLog counters, making it possible to approximate a number of geometric
centralities at a very high speed and with high accuracy. While the application
of similar algorithms for the approximation of closeness was attempted in the
MapReduce framework, our exploitation of HyperLogLog counters reduces
exponentially the memory footprint, paving the way for in-core processing of
networks with a hundred billion nodes using &quot;just&quot; 2TiB of RAM. Moreover, the
computations we describe are inherently parallelizable, and scale linearly with
the number of available cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2147</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2147</id><created>2013-08-09</created><authors><author><keyname>Hendler</keyname><forenames>Danny</forenames></author><author><keyname>Naiman</keyname><forenames>Alex</forenames></author><author><keyname>Peluso</keyname><forenames>Sebastiano</forenames></author><author><keyname>Quaglia</keyname><forenames>Francesco</forenames></author><author><keyname>Romano</keyname><forenames>Paolo</forenames></author><author><keyname>Suissa</keyname><forenames>Adi</forenames></author></authors><title>Exploiting Locality in Lease-Based Replicated Transactional Memory via
  Task Migration</title><categories>cs.DB cs.DC</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Lilac-TM, the first locality-aware Distributed Software
Transactional Memory (DSTM) implementation. Lilac-TM is a fully decentralized
lease-based replicated DSTM. It employs a novel self- optimizing lease
circulation scheme based on the idea of dynamically determining whether to
migrate transactions to the nodes that own the leases required for their
validation, or to demand the acquisition of these leases by the node that
originated the transaction. Our experimental evaluation establishes that
Lilac-TM provides significant performance gains for distributed workloads
exhibiting data locality, while typically incurring no overhead for non-data
local workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2149</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2149</id><created>2013-08-09</created><updated>2014-06-25</updated><authors><author><keyname>Farmer</keyname><forenames>William M.</forenames></author><author><keyname>Larjani</keyname><forenames>Pouya</forenames></author></authors><title>Frameworks for Reasoning about Syntax that Utilize Quotation and
  Evaluation</title><categories>cs.LO</categories><comments>This research was supported by NSERC</comments><msc-class>03B70 (Primary)</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is often useful, if not necessary, to reason about the syntactic structure
of an expression in an interpreted language (i.e., a language with a
semantics). This paper introduces a mathematical structure called a syntax
framework that is intended to be an abstract model of a system for reasoning
about the syntax of an interpreted language. Like many concrete systems for
reasoning about syntax, a syntax framework contains a mapping of expressions in
the interpreted language to syntactic values that represent the syntactic
structures of the expressions; a language for reasoning about the syntactic
values; a mechanism called quotation to refer to the syntactic value of an
expression; and a mechanism called evaluation to refer to the value of the
expression represented by a syntactic value. A syntax framework provides a
basis for integrating reasoning about the syntax of the expressions with
reasoning about what the expressions mean. The notion of a syntax framework is
used to discuss how quotation and evaluation can be built into a language and
to define what quasiquotation is. Several examples of syntax frameworks are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2166</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2166</id><created>2013-08-09</created><authors><author><keyname>Tangwongsan</keyname><forenames>Kanat</forenames></author><author><keyname>Pavan</keyname><forenames>A.</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author></authors><title>Parallel Triangle Counting in Massive Streaming Graphs</title><categories>cs.DB cs.DC cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of triangles in a graph is a fundamental metric, used in social
network analysis, link classification and recommendation, and more. Driven by
these applications and the trend that modern graph datasets are both large and
dynamic, we present the design and implementation of a fast and cache-efficient
parallel algorithm for estimating the number of triangles in a massive
undirected graph whose edges arrive as a stream. It brings together the
benefits of streaming algorithms and parallel algorithms. By building on the
streaming algorithms framework, the algorithm has a small memory footprint. By
leveraging the paralell cache-oblivious framework, it makes efficient use of
the memory hierarchy of modern multicore machines without needing to know its
specific parameters. We prove theoretical bounds on accuracy, memory access
cost, and parallel runtime complexity, as well as showing empirically that the
algorithm yields accurate results and substantial speedups compared to an
optimized sequential implementation.
  (This is an expanded version of a CIKM'13 paper of the same title.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2188</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2188</id><created>2013-07-30</created><authors><author><keyname>Wang</keyname><forenames>Hongwei</forenames></author><author><keyname>Yu</keyname><forenames>F. Richard</forenames></author><author><keyname>Zhu</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Tao</forenames></author><author><keyname>Ning</keyname><forenames>Bin</forenames></author></authors><title>Finite-State Markov Modeling of Leaky Waveguide Channels in
  Communication-based Train Control (CBTC) Systems</title><categories>cs.DM cs.IT math.IT</categories><comments>3 pages, 4 figures, letter. arXiv admin note: text overlap with
  arXiv:1307.7807</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Leaky waveguide has been adopted in communication based train control (CBTC)
systems, as it can significantly enhance railway network efficiency, safety and
capacity. Since CBTC systems have high requirements for the train ground
communications, modeling the leaky waveguide channels is very important to
design the wireless networks and evaluate the performance of CBTC systems. In
the letter, we develop a finite-state Markov channel (FSMC) model for leaky
waveguide channels in CBTC systems based on real field channel measurements
obtained from a business operating subway line. The proposed FSMC channel model
takes train locations into account to have a more accurate channel model. The
overall leaky waveguide is divided into intervals, and an FSMC model is applied
in each interval. The accuracy of the proposed FSMC model is illustrated by the
simulation results generated from the model and the real field measurement
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1308.2196</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1308.2196</id><created>2013-08-08</created><authors><author><keyname>Hsu</keyname><forenames>Hsiu-Chen</forenames></author><author><keyname>Lo</keyname><forenames>Rong-Chin</forenames></author></authors><title>A New Mattress Development Based on Pressure Sensors for Body-contouring
  Uniform Support</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For getting good sleep quality, an improved approach of new mattress
development based on the pressure sensors for body-contouring uniform support
is proposed in this paper. This method solved the problems of innerspring
mattresses that cannot allow body-contouring uniform support, and foam
mattresses that cannot provide everybody equal comfort from the same mattress.
By the buried pressure sensor array and actuator array in foam layer of a
mattress, both are connected to a controller to generate the pressure
distribution mapping of a human body on the mattress, then from the data of
this mapping, some of the actuators are driven up or down by the controller to
generate a body-contouring uniform support. By the aid of mathematical
morphology algorithms, user can also choose a different support mode by another
wireless controller with touch-screen to accommodate personal favorite firmness
of the mattress and to take his tensed mood and pressure off with good sleep
until daylight. Moreover, some other homecare functions, such as temperature
measurement, sleep on posture correction and fall down prevention, can approach
by additional hardware and software as user requirement in the future.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="48000" completeListSize="102538">1122234|49001</resumptionToken>
</ListRecords>
</OAI-PMH>
