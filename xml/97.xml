<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:06:07Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|96001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212016</id><created>2002-12-09</created><updated>2004-03-23</updated><authors><author><keyname>Riege</keyname><forenames>Tobias</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Complexity of the Exact Domatic Number Problem and of the Exact Conveyor
  Flow Shop Problem</title><categories>cs.CC</categories><comments>37 pages; 4 figures</comments><acm-class>F.2.2;F.1.3</acm-class><abstract>  We prove that the exact versions of the domatic number problem are complete
for the levels of the boolean hierarchy over NP. The domatic number problem,
which arises in the area of computer networks, is the problem of partitioning a
given graph into a maximum number of disjoint dominating sets. This number is
called the domatic number of the graph. We prove that the problem of
determining whether or not the domatic number of a given graph is {\em exactly}
one of k given values is complete for the 2k-th level of the boolean hierarchy
over NP. In particular, for k = 1, it is DP-complete to determine whether or
not the domatic number of a given graph equals exactly a given integer. Note
that DP is the second level of the boolean hierarchy over NP. We obtain similar
results for the exact versions of generalized dominating set problems and of
the conveyor flow shop problem. Our reductions apply Wagner's conditions
sufficient to prove hardness for the levels of the boolean hierarchy over NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212017</id><created>2002-12-09</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author><author><keyname>Haesevoets</keyname><forenames>Sofie</forenames></author><author><keyname>Kuijpers</keyname><forenames>Bart</forenames></author><author><keyname>Revesz</keyname><forenames>Peter</forenames></author></authors><title>Classes of Spatiotemporal Objects and Their Closure Properties</title><categories>cs.DB</categories><comments>27 pages, 4 figures</comments><acm-class>H.2.8</acm-class><abstract>  We present a data model for spatio-temporal databases. In this model
spatio-temporal data is represented as a finite union of objects described by
means of a spatial reference object, a temporal object and a geometric
transformation function that determines the change or movement of the reference
object in time.
  We define a number of practically relevant classes of spatio-temporal
objects, and give complete results concerning closure under Boolean set
operators for these classes. Since only few classes are closed under all set
operators, we suggest an extension of the model, which leads to better closure
properties, and therefore increased practical applicability. We also discuss a
normal form for this extended data model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212018</id><created>2002-12-10</created><authors><author><keyname>Lecomte</keyname><forenames>P.</forenames></author><author><keyname>Rigo</keyname><forenames>M.</forenames></author></authors><title>Real numbers having ultimately periodic representations in abstract
  numeration systems</title><categories>cs.CC cs.CL</categories><comments>22 pages, 10 figures</comments><acm-class>F.4.1; F.4.3</acm-class><abstract>  Using a genealogically ordered infinite regular language, we know how to
represent an interval of R. Numbers having an ultimately periodic
representation play a special role in classical numeration systems. The aim of
this paper is to characterize the numbers having an ultimately periodic
representation in generalized systems built on a regular language. The
syntactical properties of these words are also investigated. Finally, we show
the equivalence of the classical &quot;theta&quot;-expansions with our generalized
representations in some special case related to a Pisot number &quot;theta&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212019</id><created>2002-12-10</created><authors><author><keyname>Becker</keyname><forenames>Joerg D.</forenames></author></authors><title>Thinking, Learning, and Autonomous Problem Solving</title><categories>cs.NE</categories><comments>9 pages, 4 figures</comments><acm-class>H.1.1; I.2.0; I.2.2</acm-class><abstract>  Ever increasing computational power will require methods for automatic
programming. We present an alternative to genetic programming, based on a
general model of thinking and learning. The advantage is that evolution takes
place in the space of constructs and can thus exploit the mathematical
structures of this space. The model is formalized, and a macro language is
presented which allows for a formal yet intuitive description of the problem
under consideration. A prototype has been developed to implement the scheme in
PERL. This method will lead to a concentration on the analysis of problems, to
a more rapid prototyping, to the treatment of new problem classes, and to the
investigation of philosophical problems. We see fields of application in
nonlinear differential equations, pattern recognition, robotics, model
building, and animated pictures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212020</id><created>2002-12-10</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Learning Algorithms for Keyphrase Extraction</title><categories>cs.LG cs.CL cs.IR</categories><comments>46 pages</comments><report-no>NRC-44105</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><journal-ref>Information Retrieval, (2000), 2 (4), 303-336</journal-ref><abstract>  Many academic journals ask their authors to provide a list of about five to
fifteen keywords, to appear on the first page of each article. Since these key
words are often phrases of two or more words, we prefer to call them
keyphrases. There is a wide variety of tasks for which keyphrases are useful,
as we discuss in this paper. We approach the problem of automatically
extracting keyphrases from text as a supervised learning task. We treat a
document as a set of phrases, which the learning algorithm must learn to
classify as positive or negative examples of keyphrases. Our first set of
experiments applies the C4.5 decision tree induction algorithm to this learning
task. We evaluate the performance of nine different configurations of C4.5. The
second set of experiments applies the GenEx algorithm to the task. We developed
the GenEx algorithm specifically for automatically extracting keyphrases from
text. The experimental results support the claim that a custom-designed
algorithm (GenEx), incorporating specialized procedural domain knowledge, can
generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective
human evaluation of the keyphrases generated by Extractor suggests that about
80% of the keyphrases are acceptable to human readers. This level of
performance should be satisfactory for a wide variety of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212021</id><created>2002-12-10</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>A Simple Model of Unbounded Evolutionary Versatility as a Largest-Scale
  Trend in Organismal Evolution</title><categories>cs.NE cs.CE q-bio.PE</categories><comments>32 pages</comments><report-no>NRC-43672</report-no><acm-class>I.6.3; I.6.8; J.3</acm-class><journal-ref>Artificial Life, (2000), 6, 109-128</journal-ref><abstract>  The idea that there are any large-scale trends in the evolution of biological
organisms is highly controversial. It is commonly believed, for example, that
there is a large-scale trend in evolution towards increasing complexity, but
empirical and theoretical arguments undermine this belief. Natural selection
results in organisms that are well adapted to their local environments, but it
is not clear how local adaptation can produce a global trend. In this paper, I
present a simple computational model, in which local adaptation to a randomly
changing environment results in a global trend towards increasing evolutionary
versatility. In this model, for evolutionary versatility to increase without
bound, the environment must be highly dynamic. The model also shows that
unbounded evolutionary versatility implies an accelerating evolutionary pace. I
believe that unbounded increase in evolutionary versatility is a large-scale
trend in evolution. I discuss some of the testable predictions about organismal
evolution that are suggested by the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212022</id><created>2002-12-10</created><authors><author><keyname>Hsiang</keyname><forenames>Tien-Ruey</forenames></author><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Bender</keyname><forenames>Michael</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author></authors><title>Algorithms for Rapidly Dispersing Robot Swarms in Unknown Environments</title><categories>cs.RO</categories><comments>17 pages, 4 figures, Latex, to appear in Workshop on Algorithmic
  Foundations of Robotics, 2002</comments><acm-class>I.2.9</acm-class><abstract>  We develop and analyze algorithms for dispersing a swarm of primitive robots
in an unknown environment, R. The primary objective is to minimize the
makespan, that is, the time to fill the entire region. An environment is
composed of pixels that form a connected subset of the integer grid.
 There is at most one robot per pixel and robots move horizontally or
vertically at unit speed. Robots enter R by means of k&gt;=1 door pixels
 Robots are primitive finite automata, only having local communication, local
sensors, and a constant-sized memory.
 We first give algorithms for the single-door case (i.e., k=1), analyzing the
algorithms both theoretically and experimentally. We prove that our algorithms
have optimal makespan 2A-1, where A is the area of R.
 We next give an algorithm for the multi-door case (k&gt;1), based on a
wall-following version of the leader-follower strategy. We prove that our
strategy is O(log(k+1))-competitive, and that this bound is tight for our
strategy and other related strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212023</id><created>2002-12-10</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>How to Shift Bias: Lessons from the Baldwin Effect</title><categories>cs.LG cs.NE</categories><comments>36 pages</comments><report-no>NRC-40180</report-no><acm-class>I.2.6; I.2.8</acm-class><journal-ref>Evolutionary Computation, (1996), 4 (3), 271-295</journal-ref><abstract>  An inductive learning algorithm takes a set of data as input and generates a
hypothesis as output. A set of data is typically consistent with an infinite
number of hypotheses; therefore, there must be factors other than the data that
determine the output of the learning algorithm. In machine learning, these
other factors are called the bias of the learner. Classical learning algorithms
have a fixed bias, implicit in their design. Recently developed learning
algorithms dynamically adjust their bias as they search for a hypothesis.
Algorithms that shift bias in this manner are not as well understood as
classical algorithms. In this paper, we show that the Baldwin effect has
implications for the design and analysis of bias shifting algorithms. The
Baldwin effect was proposed in 1896, to explain how phenomena that might appear
to require Lamarckian evolution (inheritance of acquired characteristics) can
arise from purely Darwinian evolution. Hinton and Nowlan presented a
computational model of the Baldwin effect in 1987. We explore a variation on
their model, which we constructed explicitly to illustrate the lessons that the
Baldwin effect has for research in bias shifting algorithms. The main lesson is
that it appears that a good strategy for shift of bias in a learning algorithm
is to begin with a weak bias and gradually shift to a strong bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212024</id><created>2002-12-10</created><authors><author><keyname>Clark</keyname><forenames>Alexander</forenames></author></authors><title>Unsupervised Language Acquisition: Theory and Practice</title><categories>cs.CL cs.LG</categories><comments>D. Phil., 196 pages</comments><acm-class>I.2.6; I.2.7</acm-class><journal-ref>D. Phil., School of Cognitive and Computing Sciences, University
  of Sussex, 2001</journal-ref><abstract>  In this thesis I present various algorithms for the unsupervised machine
learning of aspects of natural languages using a variety of statistical models.
The scientific object of the work is to examine the validity of the so-called
Argument from the Poverty of the Stimulus advanced in favour of the proposition
that humans have language-specific innate knowledge. I start by examining an a
priori argument based on Gold's theorem, that purports to prove that natural
languages cannot be learned, and some formal issues related to the choice of
statistical grammars rather than symbolic grammars. I present three novel
algorithms for learning various parts of natural languages: first, an algorithm
for the induction of syntactic categories from unlabelled text using
distributional information, that can deal with ambiguous and rare words;
secondly, a set of algorithms for learning morphological processes in a variety
of languages, including languages such as Arabic with non-concatenative
morphology; thirdly an algorithm for the unsupervised induction of a
context-free grammar from tagged text. I carefully examine the interaction
between the various components, and show how these algorithms can form the
basis for a empiricist model of language acquisition. I therefore conclude that
the Argument from the Poverty of the Stimulus is unsupported by the evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212025</id><created>2002-12-10</created><authors><author><keyname>Szita</keyname><forenames>Istvan</forenames></author><author><keyname>Takacs</keyname><forenames>Balint</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Searching for Plannable Domains can Speed up Reinforcement Learning</title><categories>cs.AI</categories><acm-class>I.2.8</acm-class><abstract>  Reinforcement learning (RL) involves sequential decision making in uncertain
environments. The aim of the decision-making agent is to maximize the benefit
of acting in its environment over an extended period of time. Finding an
optimal policy in RL may be very slow. To speed up learning, one often used
solution is the integration of planning, for example, Sutton's Dyna algorithm,
or various other methods using macro-actions.
  Here we suggest to separate plannable, i.e., close to deterministic parts of
the world, and focus planning efforts in this domain. A novel reinforcement
learning method called plannable RL (pRL) is proposed here. pRL builds a simple
model, which is used to search for macro actions. The simplicity of the model
makes planning computationally inexpensive. It is shown that pRL finds an
optimal policy, and that plannable macro actions found by pRL are near-optimal.
In turn, it is unnecessary to try large numbers of macro actions, which enables
fast learning. The utility of pRL is demonstrated by computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212026</id><created>2002-12-11</created><authors><author><keyname>Payet</keyname><forenames>Etienne</forenames></author><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author></authors><title>A Generalization of the Lifting Lemma for Logic Programming</title><categories>cs.LO</categories><comments>25 pages, submitted to TOCL</comments><acm-class>D.1.6</acm-class><abstract>  Since the seminal work of J. A. Robinson on resolution, many lifting lemmas
for simplifying proofs of completeness of resolution have been proposed in the
literature. In the logic programming framework, they may also help to detect
some infinite derivations while proving goals under the SLD-resolution. In this
paper, we first generalize a version of the lifting lemma, by extending the
relation &quot;is more general than&quot; so that it takes into account only some
arguments of the atoms. The other arguments, which we call neutral arguments,
are disregarded. Then we propose two syntactic conditions of increasing power
for identifying neutral arguments from mere inspection of the text of a logic
program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212027</id><created>2002-12-11</created><authors><author><keyname>Monerat</keyname><forenames>G. A.</forenames></author><author><keyname>Silva</keyname><forenames>E. V. Correa</forenames></author><author><keyname>Cyrino</keyname><forenames>A. G.</forenames></author></authors><title>Qualitative Study of a Robot Arm as a Hamiltonian System</title><categories>cs.RO</categories><comments>15 pages, 5 figures</comments><acm-class>I.2.9</acm-class><abstract>  A double pendulum subject to external torques is used as a model to study the
stability of a planar manipulator with two links and two rotational driven
joints. The hamiltonian equations of motion and the fixed points (stationary
solutions) in phase space are determined. Under suitable conditions, the
presence of constant torques does not change the number of fixed points, and
preserves the topology of orbits in their linear neighborhoods; two equivalent
invariant manifolds are observed, each corresponding to a saddle-center fixed
point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212028</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Technical Note: Bias and the Quantification of Stability</title><categories>cs.LG cs.CV</categories><comments>14 pages</comments><report-no>NRC-38313</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>Machine Learning, (1995), 20, 23-33</journal-ref><abstract>  Research on bias in machine learning algorithms has generally been concerned
with the impact of bias on predictive accuracy. We believe that there are other
factors that should also play a role in the evaluation of bias. One such factor
is the stability of the algorithm; in other words, the repeatability of the
results. If we obtain two sets of data from the same phenomenon, with the same
underlying probability distribution, then we would like our learning algorithm
to induce approximately the same concepts from both sets of data. This paper
introduces a method for quantifying stability, based on a measure of the
agreement between concepts. We also discuss the relationships among stability,
predictive accuracy, and bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212029</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>A Theory of Cross-Validation Error</title><categories>cs.LG cs.CV</categories><comments>48 pages</comments><report-no>NRC-35072</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>Journal of Experimental and Theoretical Artificial Intelligence,
  (1994), 6, 361-391</journal-ref><abstract>  This paper presents a theory of error in cross-validation testing of
algorithms for predicting real-valued attributes. The theory justifies the
claim that predicting real-valued attributes requires balancing the conflicting
demands of simplicity and accuracy. Furthermore, the theory indicates precisely
how these conflicting demands must be balanced, in order to minimize
cross-validation error. A general theory is presented, then it is developed in
detail for linear regression and instance-based learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212030</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Theoretical Analyses of Cross-Validation Error and Voting in
  Instance-Based Learning</title><categories>cs.LG cs.CV</categories><comments>48 pages</comments><report-no>NRC-35073</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>Journal of Experimental and Theoretical Artificial Intelligence,
  (1994), 6, 331-360</journal-ref><abstract>  This paper begins with a general theory of error in cross-validation testing
of algorithms for supervised learning from examples. It is assumed that the
examples are described by attribute-value pairs, where the values are symbolic.
Cross-validation requires a set of training examples and a set of testing
examples. The value of the attribute that is to be predicted is known to the
learner in the training set, but unknown in the testing set. The theory
demonstrates that cross-validation error has two components: error on the
training set (inaccuracy) and sensitivity to noise (instability). This general
theory is then applied to voting in instance-based learning. Given an example
in the testing set, a typical instance-based learning algorithm predicts the
designated attribute by voting among the k nearest neighbors (the k most
similar examples) to the testing example in the training set. Voting is
intended to increase the stability (resistance to noise) of instance-based
learning, but a theoretical analysis shows that there are circumstances in
which voting can be destabilizing. The theory suggests ways to minimize
cross-validation error, by insuring that voting is stable and does not
adversely affect accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212031</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Halasz</keyname><forenames>Michael</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Contextual Normalization Applied to Aircraft Gas Turbine Engine
  Diagnosis</title><categories>cs.LG cs.CE cs.CV</categories><comments>45 pages</comments><report-no>NRC-35028</report-no><acm-class>I.2.6; I.5.4; J.2</acm-class><journal-ref>Journal of Applied Intelligence, (1993), 3, 109-129</journal-ref><abstract>  Diagnosing faults in aircraft gas turbine engines is a complex problem. It
involves several tasks, including rapid and accurate interpretation of patterns
in engine sensor data. We have investigated contextual normalization for the
development of a software tool to help engine repair technicians with
interpretation of sensor data. Contextual normalization is a new strategy for
employing machine learning. It handles variation in data that is due to
contextual factors, rather than the health of the engine. It does this by
normalizing the data in a context-sensitive manner. This learning strategy was
developed and tested using 242 observations of an aircraft gas turbine engine
in a test cell, where each observation consists of roughly 12,000 numbers,
gathered over a 12 second interval. There were eight classes of observations:
seven deliberately implanted classes of faults and a healthy class. We compared
two approaches to implementing our learning strategy: linear regression and
instance-based learning. We have three main results. (1) For the given problem,
instance-based learning works better than linear regression. (2) For this
problem, contextual normalization works better than other common forms of
normalization. (3) The algorithms described here can be the basis for a useful
software tool for assisting technicians with the interpretation of sensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212032</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised
  Classification of Reviews</title><categories>cs.LG cs.CL cs.IR</categories><comments>8 pages</comments><report-no>NRC-44946</report-no><acm-class>I.2.6; I.2.7; H.3.1; H.3.3</acm-class><journal-ref>Proceedings of the 40th Annual Meeting of the Association for
  Computational Linguistics, (2002), Philadelphia, Pennsylvania, 417-424</journal-ref><abstract>  This paper presents a simple unsupervised learning algorithm for classifying
reviews as recommended (thumbs up) or not recommended (thumbs down). The
classification of a review is predicted by the average semantic orientation of
the phrases in the review that contain adjectives or adverbs. A phrase has a
positive semantic orientation when it has good associations (e.g., &quot;subtle
nuances&quot;) and a negative semantic orientation when it has bad associations
(e.g., &quot;very cavalier&quot;). In this paper, the semantic orientation of a phrase is
calculated as the mutual information between the given phrase and the word
&quot;excellent&quot; minus the mutual information between the given phrase and the word
&quot;poor&quot;. A review is classified as recommended if the average semantic
orientation of its phrases is positive. The algorithm achieves an average
accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four
different domains (reviews of automobiles, banks, movies, and travel
destinations). The accuracy ranges from 84% for automobile reviews to 66% for
movie reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212033</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL</title><categories>cs.LG cs.CL cs.IR</categories><comments>12 pages</comments><report-no>NRC-44893</report-no><acm-class>I.2.6; I.2.7; H.3.1; H.3.3</acm-class><journal-ref>Proceedings of the Twelfth European Conference on Machine
  Learning, (2001), Freiburg, Germany, 491-502</journal-ref><abstract>  This paper presents a simple unsupervised learning algorithm for recognizing
synonyms, based on statistical data acquired by querying a Web search engine.
The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and
Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR
is empirically evaluated using 80 synonym test questions from the Test of
English as a Foreign Language (TOEFL) and 50 synonym test questions from a
collection of tests for students of English as a Second Language (ESL). On both
tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent
Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL
questions. The paper discusses potential applications of the new unsupervised
learning algorithm and some implications of the results for LSA and LSI (Latent
Semantic Indexing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212034</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Types of Cost in Inductive Concept Learning</title><categories>cs.LG cs.CV</categories><comments>7 pages</comments><report-no>NRC-43671</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>Workshop on Cost-Sensitive Learning at the Seventeenth
  International Conference on Machine Learning, (2000), Stanford University,
  California, 15-21</journal-ref><abstract>  Inductive concept learning is the task of learning to assign cases to a
discrete set of classes. In real-world applications of concept learning, there
are many different types of cost involved. The majority of the machine learning
literature ignores all types of cost (unless accuracy is interpreted as a type
of cost measure). A few papers have investigated the cost of misclassification
errors. Very few papers have examined the many other types of cost. In this
paper, we attempt to create a taxonomy of the different types of cost that are
involved in inductive concept learning. This taxonomy may help to organize the
literature on cost-sensitive learning. We hope that it will inspire researchers
to investigate all types of cost in inductive concept learning in more depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212035</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Exploiting Context When Learning to Classify</title><categories>cs.LG cs.CV</categories><comments>6 pages</comments><report-no>NRC-35058</report-no><acm-class>I.2.6; I.5.2; I.5.4</acm-class><journal-ref>Proceedings of the European Conference on Machine Learning,
  Vienna, Austria, (1993), 402-407</journal-ref><abstract>  This paper addresses the problem of classifying observations when features
are context-sensitive, specifically when the testing set involves a context
that is different from the training set. The paper begins with a precise
definition of the problem, then general strategies are presented for enhancing
the performance of classification algorithms on this type of problem. These
strategies are tested on two domains. The first domain is the diagnosis of gas
turbine engines. The problem is to diagnose a faulty engine in one context,
such as warm weather, when the fault has previously been seen only in another
context, such as cold weather. The second domain is speech recognition. The
problem is to recognize words spoken by a new speaker, not represented in the
training set. For both domains, exploiting context results in substantially
more accurate classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212036</id><created>2002-12-11</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Myths and Legends of the Baldwin Effect</title><categories>cs.LG cs.NE</categories><comments>8 pages</comments><report-no>NRC-39220</report-no><acm-class>I.2.6; I.2.8</acm-class><journal-ref>13th International Conference on Machine Learning, Workshop on
  Evolutionary Computation and Machine Learning, Bari, Italy, (1996), 135-142</journal-ref><abstract>  This position paper argues that the Baldwin effect is widely misunderstood by
the evolutionary computation community. The misunderstandings appear to fall
into two general categories. Firstly, it is commonly believed that the Baldwin
effect is concerned with the synergy that results when there is an evolving
population of learning individuals. This is only half of the story. The full
story is more complicated and more interesting. The Baldwin effect is concerned
with the costs and benefits of lifetime learning by individuals in an evolving
population. Several researchers have focussed exclusively on the benefits, but
there is much to be gained from attention to the costs. This paper explains the
two sides of the story and enumerates ten of the costs and benefits of lifetime
learning by individuals in an evolving population. Secondly, there is a cluster
of misunderstandings about the relationship between the Baldwin effect and
Lamarckian inheritance of acquired characteristics. The Baldwin effect is not
Lamarckian. A Lamarckian algorithm is not better for most evolutionary
computing problems than a Baldwinian algorithm. Finally, Lamarckian inheritance
is not a better model of memetic (cultural) evolution than the Baldwin effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212037</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>The Management of Context-Sensitive Features: A Review of Strategies</title><categories>cs.LG cs.CV</categories><comments>7 pages</comments><report-no>NRC-39221</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>13th International Conference on Machine Learning, Workshop on
  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 60-66</journal-ref><abstract>  In this paper, we review five heuristic strategies for handling
context-sensitive features in supervised machine learning from examples. We
discuss two methods for recovering lost (implicit) contextual information. We
mention some evidence that hybrid strategies can have a synergetic effect. We
then show how the work of several machine learning researchers fits into this
framework. While we do not claim that these strategies exhaust the
possibilities, it appears that the framework includes all of the techniques
that can be found in the published literature on contextsensitive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212038</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>The Identification of Context-Sensitive Features: A Formal Definition of
  Context for Concept Learning</title><categories>cs.LG cs.CV</categories><comments>7 pages</comments><report-no>NRC-39222</report-no><acm-class>I.2.6; I.5.2</acm-class><journal-ref>13th International Conference on Machine Learning, Workshop on
  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 53-59</journal-ref><abstract>  A large body of research in machine learning is concerned with supervised
learning from examples. The examples are typically represented as vectors in a
multi-dimensional feature space (also known as attribute-value descriptions). A
teacher partitions a set of training examples into a finite number of classes.
The task of the learning algorithm is to induce a concept from the training
examples. In this paper, we formally distinguish three types of features:
primary, contextual, and irrelevant features. We also formally define what it
means for one feature to be context-sensitive to another feature.
Context-sensitive features complicate the task of the learner and potentially
impair the learner's performance. Our formal definitions make it possible for a
learner to automatically identify context-sensitive features. After
context-sensitive features have been identified, there are several strategies
that the learner can employ for managing the features; however, a discussion of
these strategies is outside of the scope of this paper. The formal definitions
presented here correct a flaw in previously proposed definitions. We discuss
the relationship between our work and a formal definition of relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212039</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Low Size-Complexity Inductive Logic Programming: The East-West Challenge
  Considered as a Problem in Cost-Sensitive Classification</title><categories>cs.LG cs.NE</categories><comments>17 pages</comments><report-no>NRC-39164</report-no><acm-class>I.2.6; I.2.8</acm-class><journal-ref>Proceedings of the Fifth International Inductive Logic Programming
  Workshop, Leuven, Belgium, (1995), 247-263</journal-ref><abstract>  The Inductive Logic Programming community has considered proof-complexity and
model-complexity, but, until recently, size-complexity has received little
attention. Recently a challenge was issued &quot;to the international computing
community&quot; to discover low size-complexity Prolog programs for classifying
trains. The challenge was based on a problem first proposed by Ryszard
Michalski, 20 years ago. We interpreted the challenge as a problem in
cost-sensitive classification and we applied a recently developed
cost-sensitive classifier to the competition. Our algorithm was relatively
successful (we won a prize). This paper presents our algorithm and analyzes the
results of the competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212040</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Data Engineering for the Analysis of Semiconductor Manufacturing Data</title><categories>cs.LG cs.CE cs.CV</categories><comments>10 pages</comments><report-no>NRC-39163</report-no><acm-class>I.2.6; I.5.2; I.5.4; J.2</acm-class><journal-ref>Proceedings of the IJCAI-95 Workshop on Data Engineering for
  Inductive Learning, Montreal, Quebec, (1995), 50-59</journal-ref><abstract>  We have analyzed manufacturing data from several different semiconductor
manufacturing plants, using decision tree induction software called Q-YIELD.
The software generates rules for predicting when a given product should be
rejected. The rules are intended to help the process engineers improve the
yield of the product, by helping them to discover the causes of rejection.
Experience with Q-YIELD has taught us the importance of data engineering --
preprocessing the data to enable or facilitate decision tree induction. This
paper discusses some of the data engineering problems we have encountered with
semiconductor manufacturing data. The paper deals with two broad classes of
problems: engineering the features in a feature vector representation and
engineering the definition of the target concept (the classes). Manufacturing
process data present special problems for feature engineering, since the data
have multiple levels of granularity (detail, resolution). Engineering the
target concept is important, due to our focus on understanding the past, as
opposed to the more common focus in machine learning on predicting the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212041</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Robust Classification with Context-Sensitive Features</title><categories>cs.LG cs.CV</categories><comments>9 pages</comments><report-no>NRC-35074</report-no><acm-class>I.2.6; I.5.2; I.5.4</acm-class><journal-ref>Proceedings of the Sixth International Conference on Industrial
  and Engineering Applications of Artificial Intelligence and Expert Systems,
  Edinburgh, Scotland, (1993), 268-276</journal-ref><abstract>  This paper addresses the problem of classifying observations when features
are context-sensitive, especially when the testing set involves a context that
is different from the training set. The paper begins with a precise definition
of the problem, then general strategies are presented for enhancing the
performance of classification algorithms on this type of problem. These
strategies are tested on three domains. The first domain is the diagnosis of
gas turbine engines. The problem is to diagnose a faulty engine in one context,
such as warm weather, when the fault has previously been seen only in another
context, such as cold weather. The second domain is speech recognition. The
context is given by the identity of the speaker. The problem is to recognize
words spoken by a new speaker, not represented in the training set. The third
domain is medical prognosis. The problem is to predict whether a patient with
hepatitis will live or die. The context is the age of the patient. For all
three domains, exploiting context results in substantially more accurate
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212042</id><created>2002-12-12</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Increasing Evolvability Considered as a Large-Scale Trend in Evolution</title><categories>cs.NE cs.CE q-bio.PE</categories><comments>4 pages</comments><report-no>NRC-43583</report-no><acm-class>I.6.3; I.6.8; J.3</acm-class><journal-ref>Proceedings of the 1999 Genetic and Evolutionary Computation
  Conference Workshop Program, (1999), 43-46</journal-ref><abstract>  Evolvability is the capacity to evolve. This paper introduces a simple
computational model of evolvability and demonstrates that, under certain
conditions, evolvability can increase indefinitely, even when there is no
direct selection for evolvability. The model shows that increasing evolvability
implies an accelerating evolutionary pace. It is suggested that the conditions
for indefinitely increasing evolvability are satisfied in biological and
cultural evolution. We claim that increasing evolvability is a large-scale
trend in evolution. This hypothesis leads to testable predictions about
biological and cultural evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212043</id><created>2002-12-13</created><authors><author><keyname>Gu</keyname><forenames>Xianfeng</forenames></author><author><keyname>Yau</keyname><forenames>Shing-Tung</forenames></author></authors><title>Computing Conformal Structure of Surfaces</title><categories>cs.GR cs.CG</categories><comments>14 pages, 3 figures, simplified version, full version upon request</comments><acm-class>I.3.5;F.2.2;G.2.m</acm-class><abstract>  This paper solves the problem of computing conformal structures of general
2-manifolds represented as triangle meshes. We compute conformal structures in
the following way: first compute homology bases from simplicial complex
structures, then construct dual cohomology bases and diffuse them to harmonic
1-forms. Next, we construct bases of holomorphic differentials. We then obtain
period matrices by integrating holomorphic differentials along homology bases.
We also study the global conformal mapping between genus zero surfaces and
spheres, and between general meshes and planes. Our method of computing
conformal structures can be applied to tackle fundamental problems in computer
aid design and computer graphics, such as geometry classification and
identification, and surface global parametrization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212044</id><created>2002-12-16</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author><author><keyname>Rohe</keyname><forenames>Andre</forenames></author><author><keyname>Tietze</keyname><forenames>Walter</forenames></author></authors><title>Solving a &quot;Hard&quot; Problem to Approximate an &quot;Easy&quot; One: Heuristics for
  Maximum Matchings and Maximum Traveling Salesman Problems</title><categories>cs.DS</categories><comments>20 pages, 14 figures, Latex, to appear in Journal of Experimental
  Algorithms, 2002</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Journal of Experimental Algorithms, 7 (2002), article 11.</journal-ref><abstract>  We consider geometric instances of the Maximum Weighted Matching Problem
(MWMP) and the Maximum Traveling Salesman Problem (MTSP) with up to 3,000,000
vertices. Making use of a geometric duality relationship between MWMP, MTSP,
and the Fermat-Weber-Problem (FWP), we develop a heuristic approach that yields
in near-linear time solutions as well as upper bounds. Using various
computational tools, we get solutions within considerably less than 1% of the
optimum.
  An interesting feature of our approach is that, even though an FWP is hard to
compute in theory and Edmonds' algorithm for maximum weighted matching yields a
polynomial solution for the MWMP, the practical behavior is just the opposite,
and we can solve the FWP with high accuracy in order to find a good heuristic
solution for the MWMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212045</id><created>2002-12-16</created><authors><author><keyname>Almeida</keyname><forenames>Rodrigo B.</forenames></author><author><keyname>Almeida</keyname><forenames>Virgilio A. F.</forenames></author></authors><title>Local Community Identification through User Access Patterns</title><categories>cs.IR cs.HC</categories><comments>11 pages, 2 figures, 2 tables, submitted to WWW2003 for evaluation</comments><acm-class>I.5.3;H.1.2;J.4</acm-class><abstract>  Community identification algorithms have been used to enhance the quality of
the services perceived by its users. Although algorithms for community have a
widespread use in the Web, their application to portals or specific subsets of
the Web has not been much studied. In this paper, we propose a technique for
local community identification that takes into account user access behavior
derived from access logs of servers in the Web. The technique takes a departure
from the existing community algorithms since it changes the focus of in terest,
moving from authors to users. Our approach does not use relations imposed by
authors (e.g. hyperlinks in the case of Web pages). It uses information derived
from user accesses to a service in order to infer relationships. The
communities identified are of great interest to content providers since they
can be used to improve quality of their services. We also propose an evaluation
methodology for analyzing the results obtained by the algorithm. We present two
case studies based on actual data from two services: an online bookstore and an
online radio. The case of the online radio is particularly relevant, because it
emphasizes the contribution of the proposed algorithm to find out communities
in an environment (i.e., streaming media service) without links, that represent
the relations imposed by authors (e.g. hyperlinks in the case of Web pages).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212046</id><created>2002-12-17</created><authors><author><keyname>Dickerson</keyname><forenames>Matthew</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Meng</keyname><forenames>Jeremy</forenames></author></authors><title>Confluent Drawings: Visualizing Non-planar Diagrams in a Planar Way</title><categories>cs.CG cs.SE</categories><comments>10 pages, 18 figures</comments><acm-class>D.2.2</acm-class><journal-ref>J. Graph Algorithms and Applications (special issue for GD'03)
  9(1):31-52, 2005.</journal-ref><abstract>  In this paper, we introduce a new approach for drawing diagrams that have
applications in software visualization. Our approach is to use a technique we
call confluent drawing for visualizing non-planar diagrams in a planar way.
This approach allows us to draw, in a crossing-free manner, graphs--such as
software interaction diagrams--that would normally have many crossings. The
main idea of this approach is quite simple: we allow groups of edges to be
merged together and drawn as &quot;tracks&quot; (similar to train tracks). Producing such
confluent diagrams automatically from a graph with many crossings is quite
challenging, however, so we offer two heuristic algorithms to test if a
non-planar graph can be drawn efficiently in a confluent way. In addition, we
identify several large classes of graphs that can be completely categorized as
being either confluently drawable or confluently non-drawable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212047</id><created>2002-12-18</created><updated>2005-03-13</updated><authors><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author></authors><title>On local equilibrium equations for clustering states</title><categories>cs.CC cond-mat.dis-nn cs.DS</categories><comments>9 pages</comments><acm-class>G.3, G.2.1</acm-class><abstract>  In this note we show that local equilibrium equations (the generalization of
the TAP equations or of the belief propagation equations) do have solutions in
the colorable phase of the coloring problem. The same results extend to other
optimization problems where the solutions has cost zero (e.g.
K-satisfiability). On a random graph the solutions of the local equilibrium
equations are associated to clusters of configurations (clustering states). On
a random graph the local equilibrium equations have solutions almost everywhere
in the uncolored phase; in this case we have to introduce the concept
quasi-solution of the local equilibrium equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212048</id><created>2002-12-19</created><authors><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author><author><keyname>Visser</keyname><forenames>Joost</forenames></author></authors><title>Strategic polymorphism requires just two combinators!</title><categories>cs.PL</categories><comments>A preliminary version of this paper was presented at IFL 2002, and
  included in the informal preproceedings of the workshop</comments><acm-class>D.1.1; D.3.3; I.1.3</acm-class><abstract>  In previous work, we introduced the notion of functional strategies:
first-class generic functions that can traverse terms of any type while mixing
uniform and type-specific behaviour. Functional strategies transpose the notion
of term rewriting strategies (with coverage of traversal) to the functional
programming paradigm. Meanwhile, a number of Haskell-based models and
combinator suites were proposed to support generic programming with functional
strategies.
  In the present paper, we provide a compact and matured reconstruction of
functional strategies. We capture strategic polymorphism by just two primitive
combinators. This is done without commitment to a specific functional language.
We analyse the design space for implementational models of functional
strategies. For completeness, we also provide an operational reference model
for implementing functional strategies (in Haskell). We demonstrate the
generality of our approach by reconstructing representative fragments of the
Strafunski library for functional strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212049</id><created>2002-12-20</created><authors><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>An Ehrenfeucht-Fraisse Game Approach to Collapse Results in Database
  Theory</title><categories>cs.LO cs.DB</categories><comments>70 pages, 9 figures</comments><acm-class>F.4.1; H.2.3</acm-class><abstract>  We present a new Ehrenfeucht-Fraisse game approach to collapse results in
database theory and we show that, in principle, this approach suffices to prove
every natural generic collapse result. Following this approach we can deal with
certain infinite databases where previous, highly involved methods fail. We
prove the natural generic collapse for Z-embeddable databases over any linearly
ordered context structure with arbitrary monadic predicates, and for
N-embeddable databases over the context structure (R,&lt;,+,Mon_Q,Groups). Here,
N, Z, R, denote the sets of natural numbers, integers, and real numbers,
respectively. Groups is the collection of all subgroups of (R,+) that contain
Z, and Mon_Q is the collection of all subsets of a particular infinite subset Q
of N. Restricting the complexity of the formulas that may be used to formulate
queries to Boolean combinations of purely existential first-order formulas, we
even obtain the collapse for N-embeddable databases over any linearly ordered
context structure with arbitrary predicates. Finally, we develop the notion of
N-representable databases, which is a natural generalization of the classical
notion of finitely representable databases. We show that natural generic
collapse results for N-embeddable databases can be lifted to the larger class
of N-representable databases. To obtain, in particular, the collapse result for
(N,&lt;,+,Mon_Q), we explicitly construct a winning strategy for the duplicator in
the presence of the built-in addition relation +. This, as a side product, also
leads to an Ehrenfeucht-Fraisse game proof of the theorem of Ginsburg and
Spanier, stating that the spectra of FO(&lt;,+)-sentences are semi-linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212050</id><created>2002-12-21</created><updated>2003-06-21</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Open Problems from CCCG 2002</title><categories>cs.CG cs.DM</categories><comments>10 problems, 4 pages. Minor updates in 2nd version. To appear in the
  Proceedings of the Canadian Computational Geometry Conference, August 2003</comments><acm-class>F.2.2</acm-class><abstract>  A list of the problems presented on August 12, 2002 at the open-problem
session of the 14th Canadian Conference on Computational Geometry held in
Lethbridge, Alberta, Canada.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212051</id><created>2002-12-23</created><authors><author><keyname>Dogac</keyname><forenames>Asuman</forenames></author><author><keyname>Laleci</keyname><forenames>Gokce</forenames></author><author><keyname>Kabak</keyname><forenames>Yildiray</forenames></author><author><keyname>Cingil</keyname><forenames>Ibrahim</forenames></author></authors><title>ExploitingWeb Service Semantics: Taxonomies vs. Ontologies</title><categories>cs.DB</categories><comments>7 pages</comments><acm-class>D</acm-class><journal-ref>IEEE Data Engineering Bulletin, Vol. 25, No. 4, December 2002</journal-ref><abstract>  Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212052</id><created>2002-12-25</created><authors><author><keyname>Dogac</keyname><forenames>Asuman</forenames></author><author><keyname>Cingil</keyname><forenames>Ibrahim</forenames></author><author><keyname>Laleci</keyname><forenames>Gokce</forenames></author><author><keyname>Kabak</keyname><forenames>Yildiray</forenames></author></authors><title>Improving the Functionality of UDDI Registries through Web Service
  Semantics</title><categories>cs.DB</categories><acm-class>D</acm-class><journal-ref>3rd VLDB Workshop on Technologies for E-Services (TES-02), Hong
  Kong, China, August 23-24, 2002</journal-ref><abstract>  In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212053</id><created>2002-12-28</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Merging Locally Correct Knowledge Bases: A Preliminary Report</title><categories>cs.AI cs.LO</categories><acm-class>I.2.4, F.4.1</acm-class><abstract>  Belief integration methods are often aimed at deriving a single and
consistent knowledge base that retains as much as possible of the knowledge
bases to integrate. The rationale behind this approach is the minimal change
principle: the result of the integration process should differ as less as
possible from the knowledge bases to integrate. We show that this principle can
be reformulated in terms of a more general model of belief revision, based on
the assumption that inconsistency is due to the mistakes the knowledge bases
contain. Current belief revision strategies are based on a specific kind of
mistakes, which however does not include all possible ones. Some alternative
possibilities are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212054</id><created>2002-12-29</created><authors><author><keyname>Lin</keyname><forenames>Ching-Chi</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author><author><keyname>Sun</keyname><forenames>I-Fan</forenames></author></authors><title>Improved Compact Visibility Representation of Planar Graph via
  Schnyder's Realizer</title><categories>cs.DS cs.CG</categories><comments>11 pages, 6 figures, the preliminary version of this paper is to
  appear in Proceedings of the 20th Annual Symposium on Theoretical Aspects of
  Computer Science (STACS), Berlin, Germany, 2003</comments><acm-class>F.2.2; B.7.2; E.1; G.2.2; I.3.6</acm-class><journal-ref>SIAM Journal on Discrete Math, 18(1):19-29, 2004</journal-ref><doi>10.1137/S0895480103420744</doi><abstract>  Let $G$ be an $n$-node planar graph. In a visibility representation of $G$,
each node of $G$ is represented by a horizontal line segment such that the line
segments representing any two adjacent nodes of $G$ are vertically visible to
each other. In the present paper we give the best known compact visibility
representation of $G$. Given a canonical ordering of the triangulated $G$, our
algorithm draws the graph incrementally in a greedy manner. We show that one of
three canonical orderings obtained from Schnyder's realizer for the
triangulated $G$ yields a visibility representation of $G$ no wider than
$\frac{22n-40}{15}$. Our easy-to-implement O(n)-time algorithm bypasses the
complicated subroutines for four-connected components and four-block trees
required by the best previously known algorithm of Kant. Our result provides a
negative answer to Kant's open question about whether $\frac{3n-6}{2}$ is a
worst-case lower bound on the required width. Also, if $G$ has no degree-three
(respectively, degree-five) internal node, then our visibility representation
for $G$ is no wider than $\frac{4n-9}{3}$ (respectively, $\frac{4n-7}{3}$).
Moreover, if $G$ is four-connected, then our visibility representation for $G$
is no wider than $n-1$, matching the best known result of Kant and He. As a
by-product, we obtain a much simpler proof for a corollary of Wagner's Theorem
on realizers, due to Bonichon, Sa\&quot;{e}c, and Mosbah.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212055</id><created>2002-11-30</created><authors><author><keyname>Goldwasser</keyname><forenames>Shafi</forenames></author></authors><title>Mathematical foundations of modern cryptography: computational
  complexity perspective</title><categories>cs.CR cs.CC</categories><report-no>ICM-2002</report-no><msc-class>68Qxx, 11xx</msc-class><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 1, 245--272</journal-ref><abstract>  Theoretical computer science has found fertile ground in many areas of
mathematics. The approach has been to consider classical problems through the
prism of computational complexity, where the number of basic computational
steps taken to solve a problem is the crucial qualitative parameter. This new
approach has led to a sequence of advances, in setting and solving new
mathematical challenges as well as in harnessing discrete mathematics to the
task of solving real-world problems.
  In this talk, I will survey the development of modern cryptography -- the
mathematics behind secret communications and protocols -- in this light. I will
describe the complexity theoretic foundations underlying the cryptographic
tasks of encryption, pseudo-randomness number generators and functions, zero
knowledge interactive proofs, and multi-party secure protocols. I will attempt
to highlight the paradigms and proof techniques which unify these foundations,
and which have made their way into the mainstream of complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212056</id><created>2002-11-30</created><authors><author><keyname>Goldwasser</keyname><forenames>Shafi</forenames></author></authors><title>On the Work of Madhu Sudan: the 2002 Nevalinna Prize Winner</title><categories>cs.CC</categories><report-no>ICM-2002</report-no><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 1, 105--115</journal-ref><abstract>  Madhu Sudan's work spans many areas of computer science theory including
computational complexity theory, the design of efficient algorithms,
algorithmic coding theory, and the theory of program checking and correcting.
  Two results of Sudan stand out in the impact they have had on the mathematics
of computation. The first work shows a probabilistic characterization of the
class NP -- those sets for which short and easily checkable proofs of
membership exist, and demonstrates consequences of this characterization to
classifying the complexity of approximation problems. The second work shows a
polynomial time algorithm for list decoding the Reed Solomon error correcting
codes.
  This short note will be devoted to describing Sudan's work on
probabilistically checkable proofs -- the so called {\it PCP theorem} and its
implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301001</id><created>2003-01-01</created><authors><author><keyname>Chernov</keyname><forenames>N.</forenames></author><author><keyname>Lesort</keyname><forenames>C.</forenames></author></authors><title>Least squares fitting of circles and lines</title><categories>cs.CV</categories><comments>26 pages, 14 figures, submitted</comments><acm-class>I.4.8; I.5.1; I.2.10; G.1.2; G.3</acm-class><abstract>  We study theoretical and computational aspects of the least squares fit (LSF)
of circles and circular arcs. First we discuss the existence and uniqueness of
LSF and various parametrization schemes. Then we evaluate several popular
circle fitting algorithms and propose a new one that surpasses the existing
methods in reliability. We also discuss and compare direct (algebraic) circle
fits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301002</id><created>2003-01-06</created><authors><author><keyname>Everitt</keyname><forenames>Cass</forenames></author><author><keyname>Kilgard</keyname><forenames>Mark J.</forenames></author></authors><title>Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated
  Rendering</title><categories>cs.GR cs.CG</categories><comments>8 pages, 5 figures</comments><acm-class>I.3.6; I.3.1</acm-class><abstract>  Twenty-five years ago, Crow published the shadow volume approach for
determining shadowed regions in a scene. A decade ago, Heidmann described a
hardware-accelerated stencil buffer-based shadow volume algorithm.
  Unfortunately hardware-accelerated stenciled shadow volume techniques have
not been widely adopted by 3D games and applications due in large part to the
lack of robustness of described techniques. This situation persists despite
widely available hardware support. Specifically what has been lacking is a
technique that robustly handles various &quot;hard&quot; situations created by near or
far plane clipping of shadow volumes.
  We describe a robust, artifact-free technique for hardware-accelerated
rendering of stenciled shadow volumes. Assuming existing hardware, we resolve
the issues otherwise caused by shadow volume near and far plane clipping
through a combination of (1) placing the conventional far clip plane &quot;at
infinity&quot;, (2) rasterization with infinite shadow volume polygons via
homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth
clamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves
existing depth precision by not requiring the far plane to be placed at
infinity. We also propose two-sided stencil testing to improve the efficiency
of rendering stenciled shadow volumes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301003</id><created>2003-01-07</created><authors><author><keyname>Eleftheriadis</keyname><forenames>Alexandros</forenames></author><author><keyname>Hong</keyname><forenames>Danny</forenames></author></authors><title>Flavor: A Language for Media Representation</title><categories>cs.PL</categories><comments>20 pages and 15 figures</comments><acm-class>D.3.4; D.3.0</acm-class><abstract>  Flavor (Formal Language for Audio-Visual Object Representation) has been
created as a language for describing coded multimedia bitstreams in a formal
way so that the code for reading and writing bitstreams can be automatically
generated. It is an extension of C++ and Java, in which the typing system
incorporates bitstream representation semantics. This allows describing in a
single place both the in-memory representation of data as well as their
bitstream-level (compressed) representation. Flavor also comes with a
translator that automatically generates standard C++ or Java code from the
Flavor source code so that direct access to compressed multimedia information
by application developers can be achieved with essentially zero programming.
Flavor has gone through many enhancements and this paper fully describes the
latest version of the language and the translator. The software has been made
into an open source project as of Version 4.1, and the latest downloadable
Flavor package is available at http://flavor.sourceforge.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301004</id><created>2003-01-08</created><updated>2003-02-04</updated><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>Near Quadratic Matrix Multiplication Modulo Composites</title><categories>cs.CC cs.DM</categories><comments>Prelimanary version, 6 pages</comments><acm-class>F.1.3; F.2.1; G.1.3</acm-class><abstract>  We show how one can use non-prime-power, composite moduli for computing
representations of the product of two $n\times n$ matrices using only
$n^{2+o(1)}$ multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301005</id><created>2003-01-09</created><authors><author><keyname>Ganesh</keyname><forenames>R.</forenames></author><author><keyname>Kaushik</keyname><forenames>B.</forenames></author><author><keyname>Sadhu</keyname><forenames>R.</forenames></author></authors><title>Modelling Delay Jitter in Voice over IP</title><categories>cs.PF</categories><acm-class>C.2.2;C.2.1</acm-class><abstract>  It has been suggested in voice over IP that an appropriate choice of the
distribution used in modeling the delay jitters, can improve the play-out
algorithm. In this paper, we propose a tool using which, one can determine, at
a given instance, which distribution model best explains the jitter
distribution. This is done using Expectation Maximization, to choose amongst
possible distribution models which include, the i.i.d exponential distribution,
the gamma distribution etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301006</id><created>2003-01-09</created><authors><author><keyname>Takacs</keyname><forenames>Balint</forenames></author><author><keyname>Szita</keyname><forenames>Istvan</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Temporal plannability by variance of the episode length</title><categories>cs.AI</categories><acm-class>I.2.8</acm-class><abstract>  Optimization of decision problems in stochastic environments is usually
concerned with maximizing the probability of achieving the goal and minimizing
the expected episode length. For interacting agents in time-critical
applications, learning of the possibility of scheduling of subtasks (events) or
the full task is an additional relevant issue. Besides, there exist highly
stochastic problems where the actual trajectories show great variety from
episode to episode, but completing the task takes almost the same amount of
time. The identification of sub-problems of this nature may promote e.g.,
planning, scheduling and segmenting Markov decision processes. In this work,
formulae for the average duration as well as the standard deviation of the
duration of events are derived. The emerging Bellman-type equation is a simple
extension of Sobel's work (1982). Methods of dynamic programming as well as
methods of reinforcement learning can be applied for our extension. Computer
demonstration on a toy problem serve to highlight the principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301007</id><created>2003-01-09</created><authors><author><keyname>Szita</keyname><forenames>Istvan</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Kalman filter control in the reinforcement learning framework</title><categories>cs.LG cs.AI</categories><comments>4 pages</comments><acm-class>I.2.6; I.2.8</acm-class><abstract>  There is a growing interest in using Kalman-filter models in brain modelling.
In turn, it is of considerable importance to make Kalman-filters amenable for
reinforcement learning. In the usual formulation of optimal control it is
computed off-line by solving a backward recursion. In this technical note we
show that slight modification of the linear-quadratic-Gaussian Kalman-filter
model allows the on-line estimation of optimal control and makes the bridge to
reinforcement learning. Moreover, the learning rule for value estimation
assumes a Hebbian form weighted by the error of the value estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301008</id><created>2003-01-09</created><updated>2003-05-01</updated><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames><affiliation>Artificial Intelligence Institute, Dresden University of Technology</affiliation></author><author><keyname>Wendt</keyname><forenames>Matthias</forenames><affiliation>Artificial Intelligence Institute, Dresden University of Technology</affiliation></author></authors><title>Formal Concept Analysis and Resolution in Algebraic Domains</title><categories>cs.LO cs.AI</categories><comments>14 pages. We have rewritten the old version according to the
  suggestions of some referees. The results are the same. The presentation is
  completely different</comments><acm-class>F.4.1;I.2.3;I.2.4</acm-class><abstract>  We relate two formerly independent areas: Formal concept analysis and logic
of domains. We will establish a correspondene between contextual attribute
logic on formal contexts resp. concept lattices and a clausal logic on coherent
algebraic cpos. We show how to identify the notion of formal concept in the
domain theoretic setting. In particular, we show that a special instance of the
resolution rule from the domain logic coincides with the concept closure
operator from formal concept analysis. The results shed light on the use of
contexts and domains for knowledge representation and reasoning purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301009</id><created>2003-01-13</created><authors><author><keyname>Zheng</keyname><forenames>Qingguo</forenames></author></authors><title>A Script Language for Data Integration in Database</title><categories>cs.DB</categories><comments>9 pages</comments><report-no>Report:9-1-1999</report-no><acm-class>H.2.5</acm-class><abstract>  A Script Language in this paper is designed to transform the original data
into the target data by the computing formula. The Script Language can be
translated into the corresponding SQL Language, and the computation is finally
implemented by the first type of dynamic SQL. The Script Language has the
operations of insert, update, delete, union, intersect, and minus for the table
in the database.The Script Language is edited by a text file and you can easily
modify the computing formula in the text file to deal with the situations when
the computing formula have been changed. So you only need modify the text of
the script language, but needn't change the programs that have complied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301010</id><created>2003-01-14</created><updated>2003-01-15</updated><authors><author><keyname>Wang</keyname><forenames>Kewen</forenames></author><author><keyname>Zhou</keyname><forenames>Lizhu</forenames></author></authors><title>Comparisons and Computation of Well-founded Semantics for Disjunctive
  Logic Programs</title><categories>cs.AI</categories><comments>31 pages</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  Much work has been done on extending the well-founded semantics to general
disjunctive logic programs and various approaches have been proposed. However,
these semantics are different from each other and no consensus is reached about
which semantics is the most intended. In this paper we look at disjunctive
well-founded reasoning from different angles. We show that there is an
intuitive form of the well-founded reasoning in disjunctive logic programming
which can be characterized by slightly modifying some exisitng approaches to
defining disjunctive well-founded semantics, including program transformations,
argumentation, unfounded sets (and resolution-like procedure). We also provide
a bottom-up procedure for this semantics. The significance of our work is not
only in clarifying the relationship among different approaches, but also shed
some light on what is an intended well-founded semantics for disjunctive logic
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301011</id><created>2003-01-14</created><authors><author><keyname>O'Donnell</keyname><forenames>Michael J.</forenames></author></authors><title>Open Network Handles Implemented in DNS</title><categories>cs.NI</categories><comments>Internet Draft draft-odonnell-onhs-imp-dns-00.txt, 5 September 2002</comments><acm-class>C.2.6</acm-class><abstract>  An Open Network Handle System (ONHS) provides an intermediate level of
service between IP numbers and domain names. A handle adheres permanently to an
owner, who may assign and reassign it to different addresses at will. But a
handle is a number, carrying no significance in natural language. Any user
desiring a handle may generate one from a public key. This memo describes a
simple implementation of an Open Network Handle System using the security
extensions to the Domain Name System (DNSSEC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301012</id><created>2003-01-15</created><authors><author><keyname>Nikolenko</keyname><forenames>Sergey I.</forenames></author></authors><title>Hard satisfiable formulas for DPLL-type algorithms</title><categories>cs.CC</categories><comments>9 pages</comments><acm-class>F.2.2</acm-class><abstract>  We address lower bounds on the time complexity of algorithms solving the
propositional satisfiability problem. Namely, we consider two DPLL-type
algorithms, enhanced with the unit clause and pure literal heuristics.
Exponential lower bounds for solving satisfiability on provably satisfiable
formulas are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301013</id><created>2003-01-15</created><authors><author><keyname>Kautz</keyname><forenames>S. M.</forenames></author></authors><title>Independence Properties of Algorithmically Random Sequences</title><categories>cs.CC</categories><comments>20 pages</comments><acm-class>F.1.3</acm-class><abstract>  A bounded Kolmogorov-Loveland selection rule is an adaptive strategy for
recursively selecting a subsequence of an infinite binary sequence; such a
subsequence may be interpreted as the query sequence of a time-bounded Turing
machine. In this paper we show that if A is an algorithmically random sequence,
A_0 is selected from A via a bounded Kolmogorov-Loveland selection rule, and
A_1 denotes the sequence of nonselected bits of A, then A_1 is independent of
A_0; that is, A_1 is algorithmically random relative to A_0. This result has
been used by Kautz and Miltersen [1] to show that relative to a random oracle,
NP does not have p-measure zero (in the sense of Lutz [2]).
  [1] S. M. Kautz and P. B. Miltersen. Relative to a random oracle, NP is not
small. Journal of Computer and System Sciences, 53:235-250, 1996.
  [2] J. H. Lutz. Almost everywhere high nonuniform complexity. Journal of
Computer and System Sciences, 44:220-258, 1992.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301014</id><created>2003-01-16</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Convergence and Loss Bounds for Bayesian Sequence Prediction</title><categories>cs.LG cs.AI math.PR</categories><comments>8 twocolumn pages</comments><report-no>IDSIA-09-01</report-no><acm-class>E.4;I.2.6;G.3</acm-class><journal-ref>IEEE Transactions on Information Theory, 49:8 (2003) 2061--2067</journal-ref><abstract>  The probability of observing $x_t$ at time $t$, given past observations
$x_1...x_{t-1}$ can be computed with Bayes' rule if the true generating
distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. If $\mu$ is
unknown, but known to belong to a class $M$ one can base ones prediction on the
Bayes mix $\xi$ defined as a weighted sum of distributions $\nu\in M$. Various
convergence results of the mixture posterior $\xi_t$ to the true posterior
$\mu_t$ are presented. In particular a new (elementary) derivation of the
convergence $\xi_t/\mu_t\to 1$ is provided, which additionally gives the rate
of convergence. A general sequence predictor is allowed to choose an action
$y_t$ based on $x_1...x_{t-1}$ and receives loss $\ell_{x_t y_t}$ if $x_t$ is
the next symbol of the sequence. No assumptions are made on the structure of
$\ell$ (apart from being bounded) and $M$. The Bayes-optimal prediction scheme
$\Lambda_\xi$ based on mixture $\xi$ and the Bayes-optimal informed prediction
scheme $\Lambda_\mu$ are defined and the total loss $L_\xi$ of $\Lambda_\xi$ is
bounded in terms of the total loss $L_\mu$ of $\Lambda_\mu$. It is shown that
$L_\xi$ is bounded for bounded $L_\mu$ and $L_\xi/L_\mu\to 1$ for $L_\mu\to
\infty$. Convergence of the instantaneous losses are also proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301015</id><created>2003-01-16</created><authors><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author></authors><title>Some remarks on the survey decimation algorithm for K-satisfiability</title><categories>cs.CC cond-mat.dis-nn cs.DS</categories><acm-class>G.3, G.2.1</acm-class><abstract>  In this note we study the convergence of the survey decimation algorithm. An
analytic formula for the reduction of the complexity during the decimation is
derived. The limit of the converge of the algorithm are estimated in the random
case: interesting phenomena appear near the boundary of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301016</id><created>2003-01-16</created><authors><author><keyname>Buergisser</keyname><forenames>Peter</forenames></author><author><keyname>Lotz</keyname><forenames>Martin</forenames></author></authors><title>Lower Bounds on the Bounded Coefficient Complexity of Bilinear Maps</title><categories>cs.CC</categories><comments>19 pages</comments><acm-class>F1.1; F2.1; I.1.2</acm-class><abstract>  We prove lower bounds of order $n\log n$ for both the problem to multiply
polynomials of degree $n$, and to divide polynomials with remainder, in the
model of bounded coefficient arithmetic circuits over the complex numbers.
These lower bounds are optimal up to order of magnitude. The proof uses a
recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix
multiplication. It reduces the linear problem to multiply a random circulant
matrix with a vector to the bilinear problem of cyclic convolution. We treat
the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp.
305-306, 1973] in a unitarily invariant way. This establishes a new lower bound
on the bounded coefficient complexity of linear forms in terms of the singular
values of the corresponding matrix. In addition, we extend these lower bounds
for linear and bilinear maps to a model of circuits that allows a restricted
number of unbounded scalar multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301017</id><created>2003-01-19</created><authors><author><keyname>Vincent</keyname><forenames>Millist W.</forenames></author><author><keyname>Liu</keyname><forenames>Jixue</forenames></author></authors><title>Completeness and Decidability Properties for Functional Dependencies in
  XML</title><categories>cs.DB</categories><acm-class>H.2.1</acm-class><abstract>  XML is of great importance in information storage and retrieval because of
its recent emergence as a standard for data representation and interchange on
the Internet. However XML provides little semantic content and as a result
several papers have addressed the topic of how to improve the semantic
expressiveness of XML. Among the most important of these approaches has been
that of defining integrity constraints in XML. In a companion paper we defined
strong functional dependencies in XML(XFDs). We also presented a set of axioms
for reasoning about the implication of XFDs and showed that the axiom system is
sound for arbitrary XFDs. In this paper we prove that the axioms are also
complete for unary XFDs (XFDs with a single path on the l.h.s.). The second
contribution of the paper is to prove that the implication problem for unary
XFDs is decidable and to provide a linear time algorithm for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301018</id><created>2003-01-20</created><authors><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Novel Runtime Systems Support for Adaptive Compositional Modeling on the
  Grid</title><categories>cs.CE cs.DC</categories><acm-class>D.4.1; I.6</acm-class><abstract>  Grid infrastructures and computing environments have progressed significantly
in the past few years. The vision of truly seamless Grid usage relies on
runtime systems support that is cognizant of the operational issues underlying
grid computations and, at the same time, is flexible enough to accommodate
diverse application scenarios. This paper addresses the twin aspects of Grid
infrastructure and application support through a novel combination of two
computational technologies: Weaves - a source-language independent parallel
runtime compositional framework that operates through reverse-analysis of
compiled object files, and runtime recommender systems that aid in dynamic
knowledge-based application composition. Domain-specific adaptivity is
exploited through a novel compositional system that supports runtime
recommendation of code modules and a sophisticated checkpointing and runtime
migration solution that can be transparently deployed over Grid
infrastructures. A core set of &quot;adaptivity schemas&quot; are provided as templates
for adaptive composition of large-scale scientific computations. Implementation
issues, motivating application contexts, and preliminary results are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301019</id><created>2003-01-21</created><authors><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Smoothed Analysis of Interior-Point Algorithms: Termination</title><categories>cs.DS</categories><comments>to be presented at the 2003 International Symposium on Mathematical
  Programming</comments><acm-class>F.2.1; G.1.6</acm-class><abstract>  We perform a smoothed analysis of the termination phase of an interior-point
method. By combining this analysis with the smoothed analysis of Renegar's
interior-point algorithm by Dunagan, Spielman and Teng, we show that the
smoothed complexity of an interior-point algorithm for linear programming is $O
(m^{3} \log (m/\sigma))$. In contrast, the best known bound on the worst-case
complexity of linear programming is $O (m^{3} L)$, where $L$ could be as large
as $m$. We include an introduction to smoothed analysis and a tutorial on proof
techniques that have been useful in smoothed analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301020</id><created>2003-01-21</created><authors><author><keyname>Dusart</keyname><forenames>P.</forenames></author><author><keyname>Letourneux</keyname><forenames>G.</forenames></author><author><keyname>Vivolo</keyname><forenames>O.</forenames></author></authors><title>Differential Fault Analysis on A.E.S</title><categories>cs.CR</categories><comments>10 pages</comments><acm-class>F.2.1</acm-class><abstract>  We explain how a differential fault analysis (DFA) works on AES 128, 192 or
256 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301021</id><created>2003-01-21</created><updated>2003-03-23</updated><authors><author><keyname>Lins</keyname><forenames>Lauro</forenames></author><author><keyname>Lins</keyname><forenames>Sostenes</forenames></author><author><keyname>Melo</keyname><forenames>Silvio</forenames></author></authors><title>PHORMA: Perfectly Hashable Order Restricted Multidimensional Arrays</title><categories>cs.DS</categories><comments>12 pages, 4 figures, 2 tables. Revised version. Submitted to Discrete
  Applied Mathematics</comments><acm-class>E.2;E.1</acm-class><abstract>  In this paper we propose a simple and efficient data structure yielding a
perfect hashing of quite general arrays. The data structure is named phorma,
which is an acronym for perfectly hashable order restricted multidimensional
array.
  Keywords: Perfect hash function, Digraph, Implicit enumeration,
Nijenhuis-Wilf combinatorial family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301022</id><created>2003-01-22</created><updated>2003-02-28</updated><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Ponomarenko</keyname><forenames>Ilia</forenames></author></authors><title>Homomorphic public-key cryptosystems and encrypting boolean circuits</title><categories>cs.CR</categories><acm-class>E.3.3</acm-class><abstract>  In this paper homomorphic cryptosystems are designed for the first time over
any finite group. Applying Barrington's construction we produce for any boolean
circuit of the logarithmic depth its encrypted simulation of a polynomial size
over an appropriate finitely generated group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301023</id><created>2003-01-23</created><authors><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Wang</keyname><forenames>Kewen</forenames></author></authors><title>A semantic framework for preference handling in answer set programming</title><categories>cs.AI</categories><comments>39 pages. To appear in Theory and Practice of Logic Programming</comments><acm-class>I.2.3; D.1.6</acm-class><abstract>  We provide a semantic framework for preference handling in answer set
programming. To this end, we introduce preference preserving consequence
operators. The resulting fixpoint characterizations provide us with a uniform
semantic framework for characterizing preference handling in existing
approaches. Although our approach is extensible to other semantics by means of
an alternating fixpoint theory, we focus here on the elaboration of preferences
under answer set semantics. Alternatively, we show how these approaches can be
characterized by the concept of order preservation. These uniform semantic
characterizations provide us with new insights about interrelationships and
moreover about ways of implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301024</id><created>2003-01-23</created><updated>2003-01-23</updated><authors><author><keyname>Brylinski</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Brylinski</keyname><forenames>Ranee</forenames></author></authors><title>Complexity and Completeness of Immanants</title><categories>cs.CC math.CO</categories><comments>10 pages, Latex</comments><acm-class>F.1.3 ; F.2.3</acm-class><abstract>  Immanants are polynomial functions of n by n matrices attached to irreducible
characters of the symmetric group S_n, or equivalently to Young diagrams of
size n. Immanants include determinants and permanents as extreme cases. Valiant
proved that computation of permanents is a complete problem in his algebraic
model of NP theory, i.e., it is VNP-complete. We prove that computation of
immanants is VNP-complete if the immanants are attached to a family of diagrams
whose separation is $\Omega(n^\delta)$ for some $\delta&gt;0$. We define the
separation of a diagram to be the largest number of overhanging boxes contained
in a single row. Our theorem proves a conjecture of Buergisser for a large
variety of families, and in particular we recover with new proofs his
VNP-completeness results for hooks and rectangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301025</id><created>2003-01-24</created><authors><author><keyname>Lins</keyname><forenames>Lauro</forenames></author><author><keyname>Lins</keyname><forenames>Sostenes</forenames></author><author><keyname>Melo</keyname><forenames>Silvio</forenames></author></authors><title>PHORMA: Perfectly Hashed Order Restricted Multidimensional Array</title><categories>cs.DM</categories><comments>12 pages, 4 figures</comments><acm-class>E.2</acm-class><abstract>  In this paper we propose a simple and efficient strategy to obtain a data
structure generator to accomplish a perfect hash of quite general order
restricted multidimensional arrays named {\em phormas}. The constructor of such
objects gets two parameters as input: an n-vector a of non negative integers
and a boolean function B on the types of order restrictions on the coordinates
of the valid n-vectors bounded by a. At compiler time, the phorma constructor
builds, from the pair a,B, a digraph G(a,B) with a single source s and a single
sink t such that the st-paths are in 1-1 correspondence with the members of the
B-restricted a-bounded array A(a,B). Besides perfectly hashing A(a,B), G(a,B)
is an instance of an NW-family. This permits other useful computational tasks
on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301026</id><created>2003-01-24</created><authors><author><keyname>Beeson</keyname><forenames>Michael</forenames></author><author><keyname>Veroff</keyname><forenames>Robert</forenames></author><author><keyname>Wos</keyname><forenames>Larry</forenames></author></authors><title>Double-Negation Elimination in Some Propositional Logics</title><categories>cs.LO</categories><comments>32 pages, no figures</comments><report-no>Preprint ANL/MCS-P1014-1202</report-no><acm-class>F.4.1</acm-class><abstract>  This article answers two questions (posed in the literature), each concerning
the guaranteed existence of proofs free of double negation. A proof is free of
double negation if none of its deduced steps contains a term of the form
n(n(t)) for some term t, where n denotes negation. The first question asks for
conditions on the hypotheses that, if satisfied, guarantee the existence of a
double-negation-free proof when the conclusion is free of double negation. The
second question asks about the existence of an axiom system for classical
propositional calculus whose use, for theorems with a conclusion free of double
negation, guarantees the existence of a double-negation-free proof. After
giving conditions that answer the first question, we answer the second question
by focusing on the Lukasiewicz three-axiom system. We then extend our studies
to infinite-valued sentential calculus and to intuitionistic logic and
generalize the notion of being double-negation free. The double-negation proofs
of interest rely exclusively on the inference rule condensed detachment, a rule
that combines modus ponens with an appropriately general rule of substitution.
The automated reasoning program OTTER played an indispensable role in this
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301027</id><created>2003-01-24</created><updated>2003-02-01</updated><authors><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author></authors><title>A comparison of four approaches to the calculation of conservation laws</title><categories>cs.SC math-ph math.MP</categories><comments>updated source code download link</comments><acm-class>I.1.2</acm-class><journal-ref>Euro. Jnl of Applied Mathematics, 13, part 2 (2002) 129-152</journal-ref><abstract>  The paper compares computational aspects of four approaches to compute
conservation laws of single differential equations (DEs) or systems of them,
ODEs and PDEs. The only restriction, required by two of the four corresponding
computer algebra programs, is that each DE has to be solvable for a leading
derivative. Extra constraints for the conservation laws can be specified.
Examples include new conservation laws that are non-polynomial in the
functions, that have an explicit variable dependence and families of
conservation laws involving arbitrary functions. The following equations are
investigated in examples: Ito, Liouville, Burgers, Kadomtsev-Petviashvili,
Karney-Sen-Chu-Verheest, Boussinesq, Tzetzeica, Benney.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301028</id><created>2003-01-28</created><authors><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author></authors><title>The integration of systems of linear PDEs using conservation laws of
  syzygies</title><categories>cs.SC math.AP</categories><comments>26 pages</comments><acm-class>I.1.2</acm-class><journal-ref>J. of Symb. Comp. 35, no 5 (2003) 499-526.</journal-ref><abstract>  A new integration technique is presented for systems of linear partial
differential equations (PDEs) for which syzygies can be formulated that obey
conservation laws. These syzygies come for free as a by-product of the
differential Groebner Basis computation. Compared with the more obvious way of
integrating a single equation and substituting the result in other equations
the new technique integrates more than one equation at once and therefore
introduces temporarily fewer new functions of integration that in addition
depend on fewer variables. Especially for high order PDE systems in many
variables the conventional integration technique may lead to an explosion of
the number of functions of integration which is avoided with the new method. A
further benefit is that redundant free functions in the solution are either
prevented or that their number is at least reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301029</id><created>2003-01-28</created><authors><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author></authors><title>Size reduction and partial decoupling of systems of equations</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><journal-ref>J. Symb. Comp. 33, no 3 (2002) 367-383</journal-ref><abstract>  A method is presented that reduces the number of terms of systems of linear
equations (algebraic, ordinary and partial differential equations). As a
byproduct these systems have a tendency to become partially decoupled and are
more likely to be factorizable or integrable. A variation of this method is
applicable to non-linear systems. Modifications to improve efficiency are given
and examples are shown. This procedure can be used in connection with the
computation of the radical of a differential ideal (differential Groebner
basis).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301030</id><created>2003-01-28</created><updated>2003-08-06</updated><authors><author><keyname>Greenberg</keyname><forenames>Ronald I.</forenames></author></authors><title>Bounds on the Number of Longest Common Subsequences</title><categories>cs.DM cs.DS</categories><comments>13 pages. Corrected typos, corrected operation of hyperlinks,
  improved presentation</comments><acm-class>G.2.1</acm-class><abstract>  This paper performs the analysis necessary to bound the running time of
known, efficient algorithms for generating all longest common subsequences.
That is, we bound the running time as a function of input size for algorithms
with time essentially proportional to the output size. This paper considers
both the case of computing all distinct LCSs and the case of computing all LCS
embeddings. Also included is an analysis of how much better the efficient
algorithms are than the standard method of generating LCS embeddings. A full
analysis is carried out with running times measured as a function of the total
number of input characters, and much of the analysis is also provided for cases
in which the two input sequences are of the same specified length or of two
independently specified lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301031</id><created>2003-01-28</created><authors><author><keyname>Keahey</keyname><forenames>K.</forenames></author><author><keyname>Welch</keyname><forenames>V.</forenames></author></authors><title>Fine-Grain Authorization for Resource Management in the Grid Environment</title><categories>cs.CR cs.DC</categories><comments>7 pages</comments><report-no>ANL/MCS-P991-0802</report-no><acm-class>D.2.1; D.2.2</acm-class><abstract>  In this document we describe our work-in-progress for enabling fine-grain
authorization of resource management. In particular, we address the needs of
Virtual Organizations (VOs) to enforce their own policies in addition to those
of the resource owners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301032</id><created>2003-01-28</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>Subclassing errors, OOP, and practically checkable rules to prevent them</title><categories>cs.PL cs.SE</categories><comments>10 pages, 1 LaTeX file; accompanying C++ and Haskell code and
  compilation instructions</comments><acm-class>F.3.3; D.1.5; D.1.1; D.2.3; D.2.4; D.2.11</acm-class><journal-ref>Proc. &quot;Monterey Workshop 2001: Engineering Automation for Software
  Intensive System Integration,&quot; sponsored by ONR/AFOSR/ARO/DARPA. June 18-22,
  2001, Monterey, CA. -- pp. 33-42</journal-ref><abstract>  This paper considers an example of Object-Oriented Programming (OOP) leading
to subtle errors that break separation of interface and implementations. A
comprehensive principle that guards against such errors is undecidable. The
paper introduces a set of mechanically verifiable rules that prevent these
insidious problems. Although the rules seem restrictive, they are powerful and
expressive, as we show on several familiar examples. The rules contradict both
the spirit and the letter of the OOP. The present examples as well as available
theoretical and experimental results pose a question if OOP is conducive to
software development at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301033</id><created>2003-01-28</created><authors><author><keyname>Keahey</keyname><forenames>K.</forenames></author><author><keyname>Fredian</keyname><forenames>T.</forenames></author><author><keyname>Peng</keyname><forenames>Q.</forenames></author><author><keyname>Schissel</keyname><forenames>D. P.</forenames></author><author><keyname>Thompson</keyname><forenames>M.</forenames></author><author><keyname>Foster</keyname><forenames>I.</forenames></author><author><keyname>Greenwald</keyname><forenames>M.</forenames></author><author><keyname>McCune</keyname><forenames>D.</forenames></author></authors><title>Computational Grids in Action: The Natinal Fusion Collaboratory</title><categories>cs.DC</categories><acm-class>D.2.2; J.2</acm-class><journal-ref>Future Generation Computer Systems 18 (2002), 1005-1015</journal-ref><abstract>  The National Fusion Collaboratory (NFC) project was created to advance
scientific understanding and innovation in magnetic fusion research by enabling
more efficient use of existing experimental facilities through more effective
integration of experiment, theory, and modeling. To achieve this objective, NFC
introduced the concept of &quot;network services&quot;, which build on top of
computational Grids, and provide Fusion codes, together with their maintenance
and hardware resources as a service to the community. This mode of operation
requires the development of new authorization and enforcement capabilities. In
addition, the nature of Fusion experiments places strident quality of service
requirements on codes run during the experimental cycle. In this paper, we
describe Grid computing requirements of the Fusion community, and present our
first experiments in meeting those requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301034</id><created>2003-01-29</created><authors><author><keyname>Greenberg</keyname><forenames>Ronald I.</forenames></author></authors><title>Computing the Number of Longest Common Subsequences</title><categories>cs.DS cs.DM</categories><comments>3 pages, LaTeX</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  This note provides very simple, efficient algorithms for computing the number
of distinct longest common subsequences of two input strings and for computing
the number of LCS embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301035</id><created>2003-01-30</created><authors><author><keyname>Brodsky</keyname><forenames>Alex</forenames></author><author><keyname>Pedersen</keyname><forenames>Jan B.</forenames></author><author><keyname>Wagner</keyname><forenames>Alan</forenames></author></authors><title>On the Complexity of Buffer Allocation in Message Passing Systems</title><categories>cs.DC</categories><comments>35 Pages, lots of figures</comments><acm-class>D.1.3;D.4.2;D.4.4;F.1.2;F.1.3</acm-class><abstract>  Message passing programs commonly use buffers to avoid unnecessary
synchronizations and to improve performance by overlapping communication with
computation. Unfortunately, using buffers makes the program no longer portable,
potentially unable to complete on systems without a sufficient number of
buffers. Effective buffer use entails that the minimum number needed for a safe
execution be allocated.
  We explore a variety of problems related to buffer allocation for safe and
efficient execution of message passing programs. We show that determining the
minimum number of buffers or verifying a buffer assignment are intractable
problems. However, we give a polynomial time algorithm to determine the minimum
number of buffers needed to allow for asynchronous execution. We extend these
results to several different buffering schemes, which in some cases make the
problems tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0301036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0301036</id><created>2003-01-31</created><updated>2003-06-09</updated><authors><author><keyname>Francoeur</keyname><forenames>Joe</forenames></author></authors><title>Algorithms using Java for Spreadsheet Dependent Cell Recomputation</title><categories>cs.DS cs.DM</categories><comments>23 pages, 4 figures, 2 tables</comments><acm-class>E.1;E.2;F.2.2</acm-class><abstract>  Java implementations of algorithms used by spreadsheets to automatically
recompute the set of cells dependent on a changed cell are described using a
mathematical model for spreadsheets based on graph theory. These solutions
comprise part of a Java API that allows a client application to read, modify,
and maintain spreadsheet data without using the spreadsheet application program
that produced it. Features of the Java language that successfully improve the
running time performance of the algorithms are also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302001</id><created>2003-02-01</created><updated>2003-11-11</updated><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>Many Hard Examples in Exact Phase Transitions with Application to
  Generating Hard Satisfiable Instances</title><categories>cs.CC cond-mat.stat-mech cs.AI cs.DM</categories><comments>19 pages, corrected mistakes in Theorems 5 and 6</comments><acm-class>F.2.2; I.2.8</acm-class><abstract>  This paper first analyzes the resolution complexity of two random CSP models
(i.e. Model RB/RD) for which we can establish the existence of phase
transitions and identify the threshold points exactly. By encoding CSPs into
CNF formulas, it is proved that almost all instances of Model RB/RD have no
tree-like resolution proofs of less than exponential size. Thus, we not only
introduce new families of CNF formulas hard for resolution, which is a central
task of Proof-Complexity theory, but also propose models with both many hard
instances and exact phase transitions. Then, the implications of such models
are addressed. It is shown both theoretically and experimentally that an
application of Model RB/RD might be in the generation of hard satisfiable
instances, which is not only of practical importance but also related to some
open problems in cryptography such as generating one-way functions.
Subsequently, a further theoretical support for the generation method is shown
by establishing exponential lower bounds on the complexity of solving random
satisfiable and forced satisfiable instances of RB/RD near the threshold.
Finally, conclusions are presented, as well as a detailed comparison of Model
RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively,
exhibit three different kinds of phase transition behavior in NP-complete
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302002</id><created>2003-02-01</created><authors><author><keyname>Pratola</keyname><forenames>Matthew</forenames></author><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author></authors><title>Optimizing GoTools' Search Heuristics using Genetic Algorithms</title><categories>cs.NE</categories><comments>23 pages, to appear in Journal of ICGA</comments><acm-class>I.2.1</acm-class><abstract>  GoTools is a program which solves life &amp; death problems in the game of Go.
This paper describes experiments using a Genetic Algorithm to optimize
heuristic weights used by GoTools' tree-search. The complete set of heuristic
weights is composed of different subgroups, each of which can be optimized with
a suitable fitness function. As a useful side product, an MPI interface for
FreePascal was implemented to allow the use of a parallelized fitness function
running on a Beowulf cluster. The aim of this exercise is to optimize the
current version of GoTools, and to make tools available in preparation of an
extension of GoTools for solving open boundary life &amp; death problems, which
will introduce more heuristic parameters to be fine tuned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302003</id><created>2003-02-03</created><authors><author><keyname>Cocco</keyname><forenames>Simona</forenames></author><author><keyname>Monasson</keyname><forenames>Remi</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author></authors><title>Approximate analysis of search algorithms with &quot;physical&quot; methods</title><categories>cs.CC cond-mat.stat-mech</categories><comments>28 pages, 23 figures</comments><acm-class>F.2.2</acm-class><abstract>  An overview of some methods of statistical physics applied to the analysis of
algorithms for optimization problems (satisfiability of Boolean constraints,
vertex cover of graphs, decoding, ...) with distributions of random inputs is
proposed. Two types of algorithms are analyzed: complete procedures with
backtracking (Davis-Putnam-Loveland-Logeman algorithm) and incomplete, local
search procedures (gradient descent, random walksat, ...). The study of
complete algorithms makes use of physical concepts such as phase transitions,
dynamical renormalization flow, growth processes, ... As for local search
procedures, the connection between computational complexity and the structure
of the cost function landscape is questioned, with emphasis on the notion of
metastability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302004</id><created>2003-02-03</created><authors><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author></authors><title>Unique Pattern Matching in Strings</title><categories>cs.PL cs.DB</categories><comments>22 pages</comments><acm-class>D.3.3; F.1.1; H.2.3; H.3.3; H.2.4; I.5.5</acm-class><abstract>  Regular expression patterns are a key feature of document processing
languages like Perl and XDuce. It is in this context that the first and longest
match policies have been proposed to disambiguate the pattern matching process.
We formally define a matching semantics with these policies and show that the
generally accepted method of simulating longest match by first match and
recursion is incorrect. We continue by solving the associated type inference
problem, which consists in calculating for every subexpression the set of words
the subexpression can still match when these policies are in effect, and show
how this algorithm can be used to efficiently implement the matching process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302005</id><created>2003-02-03</created><authors><author><keyname>Choi</keyname><forenames>Vicky</forenames></author><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author></authors><title>Barnacle: An Assembly Algorithm for Clone-based Sequences of Whole
  Genomes</title><categories>cs.DS cs.DM q-bio</categories><comments>13 pages, 10 figures</comments><acm-class>G.4; G2.3</acm-class><abstract>  We propose an assembly algorithm {\sc Barnacle} for sequences generated by
the clone-based approach. We illustrate our approach by assembling the human
genome. Our novel method abandons the original physical-mapping-first
framework. As we show, {\sc Barnacle} more effectively resolves conflicts due
to repeated sequences. The latter is the main difficulty of the sequence
assembly problem. Inaddition, we are able to detect inconsistencies in the
underlying data. We present and compare our results on the December 2001 freeze
of the public working draft of the human genome with NCBI's assembly (Build
28).
  The assembly of December 2001 freeze of the public working draft generated by
{\sc Barnacle} and the source code of {\sc Barnacle} are available at
(http://www.cs.rutgers.edu/~vchoi).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302006</id><created>2003-02-05</created><authors><author><keyname>Yu</keyname><forenames>Jia</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Grid Market Directory: A Web Services based Grid Service Publication
  Directory</title><categories>cs.DC</categories><comments>Technical Report, Grid Computing and Distributed Systems Lab,
  University of Melbourne, Jan 2003</comments><acm-class>C.2.4</acm-class><abstract>  As Grids are emerging as the next generation service-oriented computing
platforms, they need to support Grid economy that helps in the management of
supply and demand for resources and offers an economic incentive for Grid
resource providers. To enable this Grid economy, a market-like Grid environment
including an infrastructure that supports the publication of services and their
discovery is needed. As part of the Gridbus project, we proposed and have
developed a Grid Market Directory (GMD) that serves as a registry for
high-level service publication and discovery in Virtual Organisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302007</id><created>2003-02-05</created><authors><author><keyname>Placek</keyname><forenames>Martin</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>G-Monitor: Gridbus web portal for monitoring and steering application
  execution on global grids</title><categories>cs.DC</categories><comments>Technical Report, Grid Computing and Distributed Systems Lab, Dept.
  of Computer Science and Software Engineer, The University of Melbourne,
  Australia</comments><acm-class>C.2.4</acm-class><abstract>  Grids are experiencing a rapid growth in their application and along with
this there is a requirement for a portal which is easy to use and scalable. We
have responded to this requirement by developing an easy to use, scalable,
web-based portal called G-Monitor. This paper proposes a generic architecture
for a web portal into a grid environment and discusses our implementation and
its application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302008</id><created>2003-02-05</created><authors><author><keyname>Burq</keyname><forenames>Shoib</forenames></author><author><keyname>Melnikoff</keyname><forenames>Steve</forenames></author><author><keyname>Branson</keyname><forenames>Kim</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Visual Environment for Rapid Composition of Parameter-Sweep Applications
  for Distributed Processing on Global Grids</title><categories>cs.DC</categories><comments>Technical Report, Grid Computing and Distributed Systems Lab, Dept.
  of Computer Science and Software Engineering, The University of Melbourne,
  Australia</comments><acm-class>C.2.4</acm-class><abstract>  Computational Grids are emerging as a platform for next-generation parallel
and distributed computing. Large-scale parametric studies and parameter sweep
applications find a natural place in the Grid?s distribution model. There is
little or no communication between jobs. The task of parallelizing and
distributing existing applications is conceptually trivial. These properties of
parametric studies make it an ideal place to start developing integrated
development environments (IDEs) for rapidly Grid-enabling applications.
However, the availability of IDEs for scientists to Grid-enable their
applications, without the need of developing them as parallel applications
explicitly, is still lacking. This paper presents a Java based IDE called
Visual Parametric Tool (VPT), developed as part of the Gridbus project, for
rapid creation of parameter sweep (data parallel/SPMD) applications. It
supports automatic creation of parameter script and parameterisation of the
input data files, which is compatible with the Nimrod-G parameter specification
language. The usefulness of VPT is demonstrated by a case study on composition
of molecular docking application as a parameter sweep application. Such
applications can be deployed on clusters using the Nimrod/enFuzion system and
on global Grids using the Nimrod-G grid resource broker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302009</id><created>2003-02-06</created><authors><author><keyname>Brodnik</keyname><forenames>Andrej</forenames><affiliation>IMFM, Ljubljana, Slovenia</affiliation><affiliation>University of Technology, Lulea, Sweden</affiliation></author><author><keyname>Nilsson</keyname><forenames>Andreas</forenames><affiliation>University of Technology, Lulea, Sweden</affiliation></author></authors><title>Data Structure for a Time-Based Bandwidth Reservations Problem</title><categories>cs.DS cs.NI</categories><acm-class>E.1; C.2.3</acm-class><abstract>  We discuss a problem of handling resource reservations. The resource can be
reserved for some time, it can be freed or it can be queried what is the
largest amount of reserved resource during a time interval. We show that the
problem has a lower bound of $\Omega(\log n)$ per operation on average and we
give a matching upper bound algorithm. Our solution also solves a dynamic
version of the related problems of a prefix sum and a partial sum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302010</id><created>2003-02-06</created><authors><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Authenticated Append-only Skip Lists</title><categories>cs.CR cs.DC</categories><comments>24 pages</comments><acm-class>K.6.5; D.4.6; C.2.4</acm-class><abstract>  In this work we describe, design and analyze the security of a
tamper-evident, append-only data structure for maintaining secure data
sequences in a loosely coupled distributed system where individual system
components may be mutually distrustful. The resulting data structure, called an
Authenticated Append-Only Skip List (AASL), allows its maintainers to produce
one-way digests of the entire data sequence, which they can publish to others
as a commitment on the contents and order of the sequence. The maintainer can
produce efficiently succinct proofs that authenticate a particular datum in a
particular position of the data sequence against a published digest. AASLs are
secure against tampering even by malicious data structure maintainers. First,
we show that a maintainer cannot ``invent'' and authenticate data elements for
the AASL after he has committed to the structure. Second, he cannot equivocate
by being able to prove conflicting facts about a particular position of the
data sequence. This is the case even when the data sequence grows with time and
its maintainer publishes successive commitments at times of his own choosing.
  AASLs can be invaluable in reasoning about the integrity of system logs
maintained by untrusted components of a loosely-coupled distributed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302011</id><created>2003-02-10</created><updated>2003-10-17</updated><authors><author><keyname>Dunagan</keyname><forenames>John</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Smoothed Analysis of Interior-Point Algorithms: Condition Number</title><categories>cs.DS cs.NA</categories><comments>Fixed it up quite a bit</comments><acm-class>F.2.1; G.1.6</acm-class><abstract>  We show that the smoothed complexity of the logarithm of Renegar's condition
number is O(log (n/sigma)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302012</id><created>2003-02-10</created><updated>2003-11-27</updated><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>The New AI: General &amp; Sound &amp; Relevant for Physics</title><categories>cs.AI cs.LG quant-ph</categories><comments>23 pages, updated refs, added Goedel machine overview, corrected
  computing history timeline. To appear in B. Goertzel and C. Pennachin, eds.:
  Artificial General Intelligence</comments><report-no>TR IDSIA-04-03</report-no><acm-class>I.2</acm-class><abstract>  Most traditional artificial intelligence (AI) systems of the past 50 years
are either very limited, or based on heuristics, or both. The new millennium,
however, has brought substantial progress in the field of theoretically optimal
and practically feasible algorithms for prediction, search, inductive inference
based on Occam's razor, problem solving, decision making, and reinforcement
learning in environments of a very general type. Since inductive inference is
at the heart of all inductive sciences, some of the results are relevant not
only for AI and computer science but also for physics, provoking nontraditional
predictions based on Zuse's thesis of the computer-generated universe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302013</id><created>2003-02-12</created><authors><author><keyname>Kilgard</keyname><forenames>Mark J.</forenames></author></authors><title>Cg in Two Pages</title><categories>cs.GR cs.PL</categories><comments>2 pages</comments><acm-class>I.3.6; C.1.3</acm-class><abstract>  Cg is a language for programming GPUs. This paper describes Cg briefly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302014</id><created>2003-02-12</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Sriram</keyname><forenames>V.</forenames></author><author><keyname>Krishna</keyname><forenames>A. Vamshi</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author><author><keyname>Bendre</keyname><forenames>S. M.</forenames></author></authors><title>An Algorithm for Aligning Sentences in Bilingual Corpora Using Lexical
  Information</title><categories>cs.CL</categories><comments>10 pages, 5 figures, Conference : International Conference on Natural
  Language Processing ' 2002, Mumbai</comments><acm-class>I.2.7</acm-class><abstract>  In this paper we describe an algorithm for aligning sentences with their
translations in a bilingual corpus using lexical information of the languages.
Existing efficient algorithms ignore word identities and consider only the
sentence lengths (Brown, 1991; Gale and Church, 1993). For a sentence in the
source language text, the proposed algorithm picks the most likely translation
from the target language text using lexical information and certain heuristics.
It does not do statistical analysis using sentence lengths. The algorithm is
language independent. It also aids in detecting addition and deletion of text
in translations. The algorithm gives comparable results with the existing
algorithms in most of the cases while it does better in cases where statistical
algorithms do not give good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302015</id><created>2003-02-12</created><authors><author><keyname>Wolff</keyname><forenames>J. G.</forenames></author></authors><title>Unsupervised Learning in a Framework of Information Compression by
  Multiple Alignment, Unification and Search</title><categories>cs.AI cs.LG</categories><comments>39 pages, 1 JPEG figure</comments><acm-class>I.2.4; I.2.6; I.2.7</acm-class><abstract>  This paper describes a novel approach to unsupervised learning that has been
developed within a framework of &quot;information compression by multiple alignment,
unification and search&quot; (ICMAUS), designed to integrate learning with other AI
functions such as parsing and production of language, fuzzy pattern
recognition, probabilistic and exact forms of reasoning, and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302016</id><created>2003-02-12</created><authors><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>Data-sharing relationships in the Web</title><categories>cs.NI cond-mat</categories><report-no>University of Chicago TR-2003-01</report-no><acm-class>C.2.3</acm-class><abstract>  We propose a novel structure, the data-sharing graph, for characterizing
sharing patterns in large-scale data distribution systems. We analyze this
structure in two such systems and uncover small-world patterns for data-sharing
relationships. Using the data-sharing graph for system characterization has
potential both for basic science, because we can identify new structures
emerging in real, dynamic networks; and for system design, because we can
exploit these structures when designing data location and delivery mechanisms.
We conjecture that similar patterns arise in other large-scale systems and that
these patterns can be exploited for mechanism design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302017</id><created>2003-02-12</created><authors><author><keyname>O'Donnell</keyname><forenames>Michael J.</forenames></author></authors><title>A Proposal to Separate Handles from Names on the Internet</title><categories>cs.NI</categories><acm-class>C.2.3</acm-class><abstract>  Networked communications inherently depend on the ability of the sender of a
message to indicate through some token how the message should be delivered to a
particular recipient. The tokens that refer messages to recipients are
variously known as routes, addresses,handles, and names} ordered by their
relative nearness to network topology vs. human meaning. All four sorts of
token refer in some way to a recipient, but they are controlled by different
authorities and their meanings depend on different contextual parameters.
  Today's global Internet employs dynamically determined routes, IP addresses,
and domain names. Domain names combine the functions of handles and names. The
high value of domain names as names leads to substantial social and legal
dispute about their assignment, degrading their value as handles. The time has
come to provide a distinct open network handle system (ONHS), using handles
that are not meaningful in natural language and are therefore not subject to
the disputes surrounding the use of names.
  A handle service may be deployed easily as a handle domain within the current
Domain Name System. In order to minimize the administrative load, and maximize
their own autonomy, netizens may use public-key cryptography to assign their
own handles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302018</id><created>2003-02-12</created><authors><author><keyname>Hoong</keyname><forenames>Ding Choon</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Guided Google: A Meta Search Engine and its Implementation using the
  Google Distributed Web Services</title><categories>cs.DC</categories><comments>Technical Report, Grid Computing and Distributed Systems (GRIDS) Lab,
  Dept. of Computer Science and Software Engineering, The University of
  Melbourne, Australia</comments><acm-class>C.2.1</acm-class><abstract>  With the advent of the Internet, search engines have begun sprouting like
mushrooms after a rainfall. Only in recent years, have developers become more
innovative, and came up with guided searching facilities online. The goals of
these applications are to help ease and guide the searching efforts of a novice
web user toward their desired objectives. A number of implementations of such
services are emerging. This paper proposes a guided meta-search engine, called
&quot;Guided Google&quot;, as it serves as an interface to the actual Google.com search
engine, using the Google Web Services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302019</id><created>2003-02-12</created><authors><author><keyname>Buyya</keyname><forenames>R.</forenames></author><author><keyname>Date</keyname><forenames>S.</forenames></author><author><keyname>Mizuno-Matsumoto</keyname><forenames>Y.</forenames></author><author><keyname>Venugopal</keyname><forenames>S.</forenames></author><author><keyname>Abramson</keyname><forenames>D.</forenames></author></authors><title>Economic and On Demand Brain Activity Analysis on Global Grids</title><categories>cs.DC</categories><comments>Technical Report, Grid Computing and Distributed Systems (GRIDS) Lab,
  Dept. of Computer Science and Software Engineering, The University of
  Melbourne, Australia</comments><acm-class>C.2.1, J.3</acm-class><abstract>  The lack of computational power within an organization for analyzing
scientific data, and the distribution of knowledge (by scientists) and
technologies (advanced scientific devices) are two major problems commonly
observed in scientific disciplines. One such scientific discipline is brain
science. The analysis of brain activity data gathered from the MEG
(Magnetoencephalography) instrument is an important research topic in medical
science since it helps doctors in identifying symptoms of diseases. The data
needs to be analyzed exhaustively to efficiently diagnose and analyze brain
functions and requires access to large-scale computational resources. The
potential platform for solving such resource intensive applications is the
Grid. This paper describes a MEG data analysis system developed by us,
leveraging Grid technologies, primarily Nimrod-G, Gridbus, and Globus. This
paper explains the application of economy-based grid scheduling algorithms to
the problem domain for on-demand processing of analysis jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302020</id><created>2003-02-13</created><authors><author><keyname>Harwood</keyname><forenames>Aaron</forenames></author></authors><title>Analytical formulations of Peer-to-Peer Connection Efficiency</title><categories>cs.DC cs.AR cs.NI</categories><comments>9 pages</comments><acm-class>B.4.3; B.4.4; C.2.4</acm-class><abstract>  Use of Peer-to-Peer (P2P) service networks introduces a new communication
paradigm because peers are both clients and servers and so each peer may
provide/request services to/from other peers. Empirical studies of P2P networks
have been undertaken and reveal useful characteristics. However there is to
date little analytical work to describe P2P networks with respect to their
communication paradigm and their interconnections. This paper provides an
analytical formulation and optimisation of peer connection efficiency, in terms
of minimising the fraction of wasted connection time. Peer connection
efficiency is analysed for both a uni- and multi-connected peer. Given this
fundamental optimisation, the paper optimises the number of connections that
peers should make use of as a function of network load, in terms of minimising
the total queue size that requests in the P2P network experience. The results
of this paper provide a basis for engineering high performance P2P
interconnection networks. The optimisations are useful for reducing bandwidth
and power consumption, e.g. in the case of peers being mobile devices with a
limited power supply. Also these results could be used to determine when a
(virtual) circuit should be switched to support a connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302021</id><created>2003-02-14</created><authors><author><keyname>Simons</keyname><forenames>Gary</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Building an Open Language Archives Community on the OAI Foundation</title><categories>cs.CL cs.DL</categories><comments>12 pages</comments><acm-class>H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5</acm-class><journal-ref>Library Hi Tech 21(2), 2003</journal-ref><abstract>  The Open Language Archives Community (OLAC) is an international partnership
of institutions and individuals who are creating a worldwide virtual library of
language resources. The Dublin Core (DC) Element Set and the OAI Protocol have
provided a solid foundation for the OLAC framework. However, we need more
precision in community-specific aspects of resource description than is offered
by DC. Furthermore, many of the institutions and individuals who might
participate in OLAC do not have the technical resources to support the OAI
protocol. This paper presents our solutions to these two problems. To address
the first, we have developed an extensible application profile for language
resource metadata. To address the second, we have implemented Vida (the virtual
data provider) and Viser (the virtual service provider), which permit community
members to provide data and services without having to implement the OAI
protocol. These solutions are generic and could be adopted by other specialized
subcommunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302022</id><created>2003-02-15</created><updated>2003-06-22</updated><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Diamadi</keyname><forenames>Zoe</forenames></author><author><keyname>Shah</keyname><forenames>Gauri</forenames></author></authors><title>Fault-tolerant routing in peer-to-peer systems</title><categories>cs.DS cs.DC</categories><comments>Full version of PODC 2002 paper. New version corrects missing
  conditioning in Lemma 9 and some related details in the proof of Theorem 10,
  with no changes to main results</comments><acm-class>F.2.2; C.2.4; E.1</acm-class><abstract>  We consider the problem of designing an overlay network and routing mechanism
that permits finding resources efficiently in a peer-to-peer system. We argue
that many existing approaches to this problem can be modeled as the
construction of a random graph embedded in a metric space whose points
represent resource identifiers, where the probability of a connection between
two nodes depends only on the distance between them in the metric space. We
study the performance of a peer-to-peer system where nodes are embedded at grid
points in a simple metric space: a one-dimensional real line. We prove upper
and lower bounds on the message complexity of locating particular resources in
such a system, under a variety of assumptions about failures of either nodes or
the connections between them. Our lower bounds in particular show that the use
of inverse power-law distributions in routing, as suggested by Kleinberg
(1999), is close to optimal. We also give efficient heuristics to dynamically
maintain such a system as new nodes arrive and old nodes depart. Finally, we
give experimental results that suggest promising directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302023</id><created>2003-02-16</created><authors><author><keyname>Haubold</keyname><forenames>Alexander</forenames></author><author><keyname>Kender</keyname><forenames>John R.</forenames></author></authors><title>Segmentation, Indexing, and Visualization of Extended Instructional
  Videos</title><categories>cs.IR cs.CV</categories><comments>8 pages, 13 figures</comments><acm-class>H.3.1;H.3.3;I.4.8;I.5.3</acm-class><abstract>  We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Given a
series of key frames from the video, we generate a condensed view of the data
by clustering frames according to media type and visual similarities. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); the classification
accuracy exceeds 96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302024</id><created>2003-02-16</created><updated>2003-08-30</updated><authors><author><keyname>Haubold</keyname><forenames>Alexander</forenames></author><author><keyname>Kender</keyname><forenames>John R.</forenames></author></authors><title>Analysis and Interface for Instructional Video</title><categories>cs.IR cs.CV</categories><comments>4 pages, 8 figures, ICME 2003</comments><acm-class>H.3.1;H.3.3;I.4.8;I.5.3</acm-class><journal-ref>Proceedings of 2003 IEEE International Conference on Multimedia &amp;
  Expo, Volume II, pages 705-708, July 2003</journal-ref><abstract>  We present a new method for segmenting, and a new user interface for indexing
and visualizing, the semantic content of extended instructional videos. Using
various visual filters, key frames are first assigned a media type (board,
class, computer, illustration, podium, and sheet). Key frames of media type
board and sheet are then clustered based on contents via an algorithm with
near-linear cost. A novel user interface, the result of two user studies,
displays related topics using icons linked topologically, allowing users to
quickly locate semantically related portions of the video. We analyze the
accuracy of the segmentation tool on 17 instructional videos, each of which is
from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302025</id><created>2003-02-18</created><updated>2003-05-23</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Jakobsson</keyname><forenames>Markus</forenames></author><author><keyname>Lipmaa</keyname><forenames>Helger</forenames></author></authors><title>Cryptographic Randomized Response Techniques</title><categories>cs.CC cs.CR cs.CY quant-ph</categories><comments>21 pages</comments><acm-class>D.4.6</acm-class><abstract>  We develop cryptographically secure techniques to guarantee unconditional
privacy for respondents to polls. Our constructions are efficient and
practical, and are shown not to allow cheating respondents to affect the
``tally'' by more than their own vote -- which will be given the exact same
weight as that of other respondents. We demonstrate solutions to this problem
based on both traditional cryptographic techniques and quantum cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302026</id><created>2003-02-19</created><authors><author><keyname>Myrnyy</keyname><forenames>Volodymyr</forenames><affiliation>Brandenburg University of Technology, Cottbus, Germany</affiliation></author></authors><title>Recursive function templates as a solution of linear algebra expressions
  in C++</title><categories>cs.MS cs.PL</categories><comments>Latex2e, 8 pages, 3 figures</comments><acm-class>G.4; I.1.2; I.1.3</acm-class><abstract>  The article deals with a kind of recursive function templates in C++, where
the recursion is realized corresponding template parameters to achieve better
computational performance. Some specialization of these template functions ends
the recursion and can be implemented using optimized hardware dependent or
independent routines. The method is applied in addition to the known expression
templates technique to solve linear algebra expressions with the help of the
BLAS library. The whole implementation produces a new library, which keeps
object-oriented benefits and has a higher computational speed represented in
the tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302027</id><created>2003-02-19</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Sullivan</keyname><forenames>John M.</forenames></author><author><keyname>Ungor</keyname><forenames>Alper</forenames></author></authors><title>Tiling space and slabs with acute tetrahedra</title><categories>cs.CG math.MG</categories><comments>20 pages; 17 figures; 1 table; see also
  http://www.cs.duke.edu/~ungor/abstracts/acute_tiling.html</comments><acm-class>F.2.2</acm-class><journal-ref>Computational Geometry Theory &amp; Applications 27(3):237-255, 2004</journal-ref><doi>10.1016/j.comgeo.2003.11.003</doi><abstract>  We show it is possible to tile three-dimensional space using only tetrahedra
with acute dihedral angles. We present several constructions to achieve this,
including one in which all dihedral angles are less than $77.08^\circ$, and
another which tiles a slab in space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302028</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302028</id><created>2003-02-19</created><authors><author><keyname>Brodsky</keyname><forenames>Alex</forenames></author><author><keyname>Pippenger</keyname><forenames>Nicholas</forenames></author></authors><title>The Boolean Functions Computed by Random Boolean Formulas OR How to Grow
  the Right Function</title><categories>cs.DM cs.CC</categories><acm-class>F.1.1;F.1.2;G.2.1;G.3</acm-class><abstract>  Among their many uses, growth processes (probabilistic amplification), were
used for constructing reliable networks from unreliable components, and
deriving complexity bounds of various classes of functions. Hence, determining
the initial conditions for such processes is an important and challenging
problem. In this paper we characterize growth processes by their initial
conditions and derive conditions under which results such as Valiant's (1984)
hold. First, we completely characterize growth processes that use linear
connectives. Second, by extending Savick\'y's (1990) analysis, via
``Restriction Lemmas'', we characterize growth processes that use monotone
connectives, and show that our technique is applicable to growth processes that
use other connectives as well. Additionally, we obtain explicit bounds on the
convergence rates of several growth processes, including the growth process
studied by Savick\'y (1990).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302029</id><created>2003-02-19</created><authors><author><keyname>Garcia</keyname><forenames>Alejandro Javier</forenames></author><author><keyname>Simari</keyname><forenames>Guillermo Ricardo</forenames></author></authors><title>Defeasible Logic Programming: An Argumentative Approach</title><categories>cs.AI</categories><comments>43 pages, to appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>I.2.3</acm-class><abstract>  The work reported here introduces Defeasible Logic Programming (DeLP), a
formalism that combines results of Logic Programming and Defeasible
Argumentation. DeLP provides the possibility of representing information in the
form of weak rules in a declarative manner, and a defeasible argumentation
inference mechanism for warranting the entailed conclusions.
  In DeLP an argumentation formalism will be used for deciding between
contradictory goals. Queries will be supported by arguments that could be
defeated by other arguments. A query q will succeed when there is an argument A
for q that is warranted, ie, the argument A that supports q is found undefeated
by a warrant procedure that implements a dialectical analysis.
  The defeasible argumentation basis of DeLP allows to build applications that
deal with incomplete and contradictory information in dynamic domains. Thus,
the resulting approach is suitable for representing agent's knowledge and for
providing an argumentation based reasoning mechanism to agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302030</identifier>
 <datestamp>2007-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302030</id><created>2003-02-20</created><updated>2004-04-19</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>The traveling salesman problem for cubic graphs</title><categories>cs.DS</categories><comments>20 pages, 8 figures. A preliminary version of this paper appeared at
  the 8th Worksh. Algorithms and Data Structures, LNCS 2748, Springer-Verlag,
  2003, pp. 307-318. This version generalizes an algorithm from the previous
  version, to generate all cycles instead of counting them. It also includes a
  derandomized version of the degree-four algorithm and an implementation of
  the cycle listing algorithm</comments><acm-class>F.2.2</acm-class><journal-ref>J. Graph Algorithms and Applications 11(1):61-81, 2007</journal-ref><abstract>  We show how to find a Hamiltonian cycle in a graph of degree at most three
with n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithm
can find the minimum weight Hamiltonian cycle (traveling salesman problem), in
the same time bound. We can also count or list all Hamiltonian cycles in a
degree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the traveling
salesman problem in graphs of degree at most four, by randomized and
deterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n and
O((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specify
a set of forced edges which must be part of any generated cycle. Our cycle
listing algorithm shows that every degree three graph has O(2^{3n/8})
Hamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltonian
cycles per graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302031</id><created>2003-02-20</created><authors><author><keyname>Edelsbrunner</keyname><forenames>Herbert</forenames></author><author><keyname>Ungor</keyname><forenames>Alper</forenames></author></authors><title>Relaxed Scheduling in Dynamic Skin Triangulation</title><categories>cs.CG</categories><comments>17 pages; 7 figures; 3 tables; see also
  http://www.cs.duke.edu/~ungor/abstracts/schedule_skin.html</comments><acm-class>F.2.2</acm-class><abstract>  We introduce relaxed scheduling as a paradigm for mesh maintenance and
demonstrate its applicability to triangulating a skin surface in $\Rspace^3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302032</id><created>2003-02-22</created><authors><author><keyname>Koehn</keyname><forenames>Philipp</forenames></author><author><keyname>Knight</keyname><forenames>Kevin</forenames></author></authors><title>Empirical Methods for Compound Splitting</title><categories>cs.CL</categories><comments>8 pages, 2 figures. Published at EACL 2003</comments><acm-class>I.2.7</acm-class><abstract>  Compounded words are a challenge for NLP applications such as machine
translation (MT). We introduce methods to learn splitting rules from
monolingual and parallel corpora. We evaluate them against a gold standard and
measure their impact on performance of statistical MT systems. Results show
accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a
German-English noun phrase translation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302033</id><created>2003-02-24</created><authors><author><keyname>Fabritius</keyname><forenames>Sampsa</forenames></author><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author><author><keyname>Metz</keyname><forenames>Edu</forenames></author><author><keyname>Ran</keyname><forenames>Alexander</forenames></author></authors><title>Experimental Software Schedulability Estimation For Varied Processor
  Frequencies</title><categories>cs.SE cs.OS</categories><comments>6 pages, 3 figures, published in the Proceedings of the Symposium on
  Software Engineering at 21th IASTED International Multi-Conference on Applied
  Informatics (AI 2003)</comments><acm-class>D.2.8;D.4.1</acm-class><abstract>  This paper describes a new approach to experimentally estimate the
application schedulability for various processor frequencies. We use additional
workload generated by an artificial high priority routine to simulate the
frequency decrease of a processor. Then we estimate the schedulability of
applications at different frequencies. The results of such estimation can be
used to determine the frequencies and control algorithms of dynamic voltage
scaling/dynamic frequency scaling (DVS/DFS) implementations. The paper presents
a general problem description, the proposed schedulability estimation method,
its analysis and evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302034</id><created>2003-02-24</created><updated>2005-10-05</updated><authors><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author></authors><title>Interest Rate Model Calibration Using Semidefinite Programming</title><categories>cs.CE</categories><report-no>CMAPX-491</report-no><acm-class>J.1</acm-class><journal-ref>Applied Mathematical Finance 10(3), pp. 183-213, September 2003</journal-ref><abstract>  We show that, for the purpose of pricing Swaptions, the Swap rate and the
corresponding Forward rates can be considered lognormal under a single
martingale measure. Swaptions can then be priced as options on a basket of
lognormal assets and an approximation formula is derived for such options. This
formula is centered around a Black-Scholes price with an appropriate
volatility, plus a correction term that can be interpreted as the expected
tracking error. The calibration problem can then be solved very efficiently
using semidefinite programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302035</id><created>2003-02-24</created><updated>2005-10-05</updated><authors><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author></authors><title>Risk-Management Methods for the Libor Market Model Using Semidefinite
  Programming</title><categories>cs.CE</categories><acm-class>J.1</acm-class><journal-ref>Journal of Computational Finance 8(4), pp. 77-99, Summer 2005</journal-ref><abstract>  When interest rate dynamics are described by the Libor Market Model as in
BGM97, we show how some essential risk-management results can be obtained from
the dual of the calibration program. In particular, if the objetive is to
maximize another swaption's price, we show that the optimal dual variables
describe a hedging portfolio in the sense of \cite{Avel96}. In the general
case, the local sensitivity of the covariance matrix to all market movement
scenarios can be directly computed from the optimal dual solution. We also show
how semidefinite programming can be used to manage the Gamma exposure of a
portfolio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302036</id><created>2003-02-25</created><updated>2003-09-07</updated><authors><author><keyname>Petrov</keyname><forenames>Evgueni</forenames></author><author><keyname>Monfroy</keyname><forenames>Eric</forenames></author></authors><title>Constraint-based analysis of composite solvers</title><categories>cs.AI</categories><comments>submitted to AI SAC 2004</comments><acm-class>I.2</acm-class><abstract>  Cooperative constraint solving is an area of constraint programming that
studies the interaction between constraint solvers with the aim of discovering
the interaction patterns that amplify the positive qualities of individual
solvers. Automatisation and formalisation of such studies is an important issue
of cooperative constraint solving.
  In this paper we present a constraint-based analysis of composite solvers
that integrates reasoning about the individual solvers and the processed data.
The idea is to approximate this reasoning by resolution of set constraints on
the finite sets representing the predicates that express all the necessary
properties. We illustrate application of our analysis to two important
cooperation patterns: deterministic choice and loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302037</id><created>2003-02-26</created><updated>2003-03-29</updated><authors><author><keyname>Toli</keyname><forenames>Ilia</forenames></author></authors><title>Hidden Polynomial(s) Cryptosystems</title><categories>cs.CR cs.SC</categories><comments>17 pages</comments><acm-class>E.3</acm-class><abstract>  We propose variations of the class of hidden monomial cryptosystems in order
to make it resistant to all known attacks. We use identities built upon a
single bivariate polynomial equation with coefficients in a finite field.
Indeed, it can be replaced by a ``small'' ideal, as well. Throughout, we set up
probabilistic encryption protocols, too. The same ideas extend to digital
signature algorithms, as well. Our schemes work as well on differential fields
of positive characteristic, and elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302038</id><created>2003-02-27</created><authors><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author></authors><title>Tight Logic Programs</title><categories>cs.AI cs.LO</categories><comments>To appear in Special Issue of the Theory and Practice of Logic
  Programming Journal on Answer Set Programming, 2003</comments><acm-class>D.1.6; F.4.1; I.2.3</acm-class><journal-ref>Theory and Practice of Logic Programming, 3(4--5):499--518, 2003.</journal-ref><abstract>  This note is about the relationship between two theories of negation as
failure -- one based on program completion, the other based on stable models,
or answer sets. Francois Fages showed that if a logic program satisfies a
certain syntactic condition, which is now called ``tightness,'' then its stable
models can be characterized as the models of its completion. We extend the
definition of tightness and Fages' theorem to programs with nested expressions
in the bodies of rules, and study tight logic programs containing the
definition of the transitive closure of a predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0302039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0302039</id><created>2003-02-28</created><authors><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Kalman-filtering using local interactions</title><categories>cs.AI</categories><acm-class>I.2.6</acm-class><abstract>  There is a growing interest in using Kalman-filter models for brain
modelling. In turn, it is of considerable importance to represent Kalman-filter
in connectionist forms with local Hebbian learning rules. To our best
knowledge, Kalman-filter has not been given such local representation. It seems
that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a
connectionist representation is presented, which is derived by means of the
recursive prediction error method. We show that this method gives rise to
attractive local learning rules and can adapt the Kalman-gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303001</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303001</id><created>2003-03-01</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author></authors><title>When Crossings Count - Approximating the Minimum Spanning Tree</title><categories>cs.CG</categories><acm-class>I.3.5</acm-class><abstract>  In the first part of the paper, we present an (1+\mu)-approximation algorithm
to the minimum-spanning tree of points in a planar arrangement of lines, where
the metric is the number of crossings between the spanning tree and the lines.
The expected running time is O((n/\mu^5) alpha^3(n) log^5 n), where \mu &gt; 0 is
a prescribed constant.
  In the second part of our paper, we show how to embed such a crossing metric,
into high-dimensions, so that the distances are preserved. As a result, we can
deploy a large collection of subquadratic approximations algorithms \cite
im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric
as a distance function. Applications include matching, clustering,
nearest-neighbor, and furthest-neighbor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303002</id><created>2003-03-05</created><updated>2003-03-21</updated><authors><author><keyname>Maslov</keyname><forenames>V. P.</forenames></author></authors><title>About compression of vocabulary in computer oriented languages</title><categories>cs.CL</categories><comments>Latex, 7 pages</comments><acm-class>B.1.4; H.3.1</acm-class><abstract>  The author uses the entropy of the ideal Bose-Einstein gas to minimize losses
in computer-oriented languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303003</id><created>2003-03-05</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>A first approach for a possible cellular automaton model of fluids
  dynamics</title><categories>cs.CC cs.DC nlin.CG physics.comp-ph</categories><comments>7 pages, 6 figures, Computational Fluidodynamics, Cellular Automata
  model</comments><acm-class>F.1.1</acm-class><abstract>  In this paper I present a first attempt for a possible description of fluids
dynamics by mean of a cellular automata technique. With the use of simple and
elementary rules, based on random behaviour either, the model permits to obtain
the evolution in time for a two-dimensional grid, where one molecule of the
material fluid can ideally place itself on a single geometric square. By mean
of computational simulations, some realistic effects, here showed by use of
digital pictures, have been obtained. In a subsequent step of this work I think
to use a parallel program for a high performances computational simulation, for
increasing the degree of realism of the digital rendering by mean of a
three-dimensional grid too. For the execution of the simulations, numerical
methods of resolution for differential equations have not been used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303004</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303004</id><created>2003-03-06</created><authors><author><keyname>Adam</keyname><forenames>Gh.</forenames></author><author><keyname>Adam</keyname><forenames>S.</forenames></author><author><keyname>Plakida</keyname><forenames>N. M.</forenames></author></authors><title>Reliability Conditions in Quadrature Algorithms</title><categories>cs.NA cs.MS physics.comp-ph</categories><comments>23 pages, 8 figures, 1 table, LaTeX2e, elsart.cls macro added,
  submitted to Computer Physics Communications</comments><report-no>E17-2002-205 (JINR Dubna preprint, sept.2002; preliminary version of
  this paper)</report-no><acm-class>G.4; G.1.4; G.1.0; J.2; D.2.4</acm-class><doi>10.1016/S0010-4655(03)00282-0</doi><abstract>  The detection of insufficiently resolved or ill-conditioned integrand
structures is critical for the reliability assessment of the quadrature rule
outputs. We discuss a method of analysis of the profile of the integrand at the
quadrature knots which allows inferences approaching the theoretical 100% rate
of success, under error estimate sharpening. The proposed procedure is of the
highest interest for the solution of parametric integrals arising in complex
physical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303005</id><created>2003-03-08</created><authors><author><keyname>Ballhausen</keyname><forenames>H.</forenames></author></authors><title>Fair Solution to the Reader-Writer-Problem with Semaphores only</title><categories>cs.DC</categories><comments>2 pages</comments><acm-class>D.1.3</acm-class><abstract>  The reader-writer-problem is a standard problem in concurrent programming. A
resource is shared by several processes which need either inclusive reading or
exclusive writing access. The known solutions to this problem typically involve
a number of global counters and queues. Here a very simple algorithm is
presented which needs only two semaphores for synchronisation and no other
global objects. The approach yields a fair solution without starving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303006</id><created>2003-03-10</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>On the Notion of Cognition</title><categories>cs.AI</categories><comments>6 pages, 2 figures</comments><acm-class>I.2.0</acm-class><abstract>  We discuss philosophical issues concerning the notion of cognition basing
ourselves in experimental results in cognitive sciences, especially in computer
simulations of cognitive systems. There have been debates on the &quot;proper&quot;
approach for studying cognition, but we have realized that all approaches can
be in theory equivalent. Different approaches model different properties of
cognitive systems from different perspectives, so we can only learn from all of
them. We also integrate ideas from several perspectives for enhancing the
notion of cognition, such that it can contain other definitions of cognition as
special cases. This allows us to propose a simple classification of different
types of cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303007</id><created>2003-03-14</created><authors><author><keyname>Victor</keyname><forenames>Kromer</forenames></author></authors><title>Glottochronology and problems of protolanguage reconstruction</title><categories>cs.CL</categories><comments>17 pages, 9 figures. In Russian</comments><acm-class>I.2.7</acm-class><abstract>  A method of languages genealogical trees construction is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303008</id><created>2003-03-14</created><authors><author><keyname>Bolotashvili</keyname><forenames>Givi</forenames></author></authors><title>Solution of the Linear Ordering Problem (NP=P)</title><categories>cs.CC cs.DM</categories><acm-class>F.2.0</acm-class><abstract>  A polynomial algorithm is obtained for the NP-complete linear ordering
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303009</id><created>2003-03-14</created><updated>2004-01-02</updated><authors><author><keyname>Janhunen</keyname><forenames>T.</forenames></author><author><keyname>Niemela</keyname><forenames>I.</forenames></author><author><keyname>Seipel</keyname><forenames>D.</forenames></author><author><keyname>Simons</keyname><forenames>P.</forenames></author><author><keyname>You</keyname><forenames>J.</forenames></author></authors><title>Unfolding Partiality and Disjunctions in Stable Model Semantics</title><categories>cs.AI</categories><comments>49 pages, 4 figures, 1 table</comments><acm-class>I.2.4; F.4.1</acm-class><abstract>  The paper studies an implementation methodology for partial and disjunctive
stable models where partiality and disjunctions are unfolded from a logic
program so that an implementation of stable models for normal
(disjunction-free) programs can be used as the core inference engine. The
unfolding is done in two separate steps. Firstly, it is shown that partial
stable models can be captured by total stable models using a simple linear and
modular program transformation. Hence, reasoning tasks concerning partial
stable models can be solved using an implementation of total stable models.
Disjunctive partial stable models have been lacking implementations which now
become available as the translation handles also the disjunctive case.
Secondly, it is shown how total stable models of disjunctive programs can be
determined by computing stable models for normal programs. Hence, an
implementation of stable models of normal programs can be used as a core engine
for implementing disjunctive programs. The feasibility of the approach is
demonstrated by constructing a system for computing stable models of
disjunctive programs using the smodels system as the core engine. The
performance of the resulting system is compared to that of dlv which is a
state-of-the-art special purpose system for disjunctive programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303010</id><created>2003-03-15</created><authors><author><keyname>Mittal</keyname><forenames>Neeraj</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>Techniques and Applications of Computation Slicing</title><categories>cs.DC cs.SE</categories><comments>50 pages, 14 figures</comments><acm-class>C.2.4; D.4.5; D.2.2</acm-class><abstract>  Writing correct distributed programs is hard. In spite of extensive testing
and debugging, software faults persist even in commercial grade software. Many
distributed systems, especially those employed in safety-critical environments,
should be able to operate properly even in the presence of software faults.
Monitoring the execution of a distributed system, and, on detecting a fault,
initiating the appropriate corrective action is an important way to tolerate
such faults. This gives rise to the predicate detection problem which requires
finding a consistent cut of a given computation that satisfies a given global
predicate, if it exists.
  Detecting a predicate in a computation is, however, an NP-complete problem.
To ameliorate the associated combinatorial explosion problem, we introduce the
notion of computation slice. Formally, the slice of a computation with respect
to a predicate is a (sub)computation with the least number of consistent cuts
that contains all consistent cuts of the computation satisfying the predicate.
To detect a predicate, rather than searching the state-space of the
computation, it is much more efficient to search the state-space of the slice.
  We prove that the slice exists and is uniquely defined for all predicates. We
present efficient slicing algorithms for several useful classes of predicates.
We develop efficient heuristic algorithms for computing an approximate slice
for predicates for which computing the slice is otherwise provably intractable.
Our experimental results show that slicing can lead to an exponential
improvement over existing techniques for predicate detection in terms of time
and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303011</id><created>2003-03-18</created><updated>2004-10-27</updated><authors><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Groote</keyname><forenames>Jan Friso</forenames></author><author><keyname>Hesselink</keyname><forenames>Wim H.</forenames></author></authors><title>Lock-free dynamic hash tables with open addressing</title><categories>cs.DC cs.DS</categories><acm-class>D.1</acm-class><journal-ref>Distributed Computing 17 (2005) 21-42</journal-ref><abstract>  We present an efficient lock-free algorithm for parallel accessible hash
tables with open addressing, which promises more robust performance and
reliability than conventional lock-based implementations. ``Lock-free'' means
that it is guaranteed that always at least one process completes its operation
within a bounded number of steps. For a single processor architecture our
solution is as efficient as sequential hash tables. On a multiprocessor
architecture this is also the case when all processors have comparable speeds.
The algorithm allows processors that have widely different speeds or come to a
halt. It can easily be implemented using C-like languages and requires on
average only constant time for insertion, deletion or accessing of elements.
The algorithm allows the hash tables to grow and shrink when needed.
  Lock-free algorithms are hard to design correctly, even when apparently
straightforward. Ensuring the correctness of the design at the earliest
possible stage is a major challenge in any responsible system development. In
view of the complexity of the algorithm, we turned to the interactive theorem
prover PVS for mechanical support. We employ standard deductive verification
techniques to prove around 200 invariance properties of our algorithm, and
describe how this is achieved with the theorem prover PVS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303012</id><created>2003-03-18</created><authors><author><keyname>Dolgikh</keyname><forenames>Dmitry</forenames></author><author><keyname>Sukhov</keyname><forenames>Andrei</forenames></author></authors><title>The measurements, parameters and construction of Web proxy cache</title><categories>cs.NI</categories><comments>12 pages, 4 figures, 3 tables. Submitted to Compt. Comm</comments><report-no>SSAU-02-13</report-no><acm-class>C.2.1, C.2.4, F.0</acm-class><abstract>  The aim of this paper is an experimental study of cache systems in order to
optimize proxy cache systems and to modernize construction principles. Our
investigations lead to the criteria for the optimal use of storage capacity and
allow the description of the basic effects of the ratio between construction
parts, steady-state performance, optimal size, etc. We want to outline that the
results obtained and the plan of the experiment follow from the theoretical
model. Special consideration is given to the modification of the key formulas
supposed by Wolman at al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303013</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303013</id><created>2003-03-18</created><updated>2003-03-21</updated><authors><author><keyname>Marino</keyname><forenames>Massimo</forenames></author></authors><title>Extending the code generation capabilities of the Together CASE tool to
  support Data Definition languages</title><categories>cs.SE</categories><comments>7 pages, 6 figures, Talk from the 2003 Computing in High Energy and
  Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 2003, PDF</comments><acm-class>D.2.2</acm-class><journal-ref>ECONFC0303241:TUJP004,2003</journal-ref><abstract>  Together is the recommended software development tool in the Atlas
collaboration. The programmatic API, which provides the capability to use and
augment Together's internal functionality, is comprised of three major
components - IDE, RWI and SCI. IDE is a read-only interface used to generate
custom outputs based on the information contained in a Together model. RWI
allows to both extract and write information to a Together model. SCI is the
Source Code Interface, as the name implies it allows to work at the level of
the source code. Together is extended by writing modules (java classes)
extensively making use of the relevant API. We exploited Together extensibility
to add support for the Atlas Dictionary Language. ADL is an extended subset of
OMG IDL. The implemented module (ADLModule) makes Together to support ADL
keywords, enables options and generate ADL object descriptions directly from
UML Class diagrams. The module thoroughly accesses a Together reverse
engineered C++ project - and/or design only class diagrams - and it is general
enough to allow for possibly additional HEP-specific Together tool tailoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303014</id><created>2003-03-18</created><authors><author><keyname>Dolgikh</keyname><forenames>Dmitry</forenames></author><author><keyname>Sukhov</keyname><forenames>Andrei</forenames></author></authors><title>Theoretical study of cache syatems</title><categories>cs.NI</categories><comments>17 pages, 5 figures, 1 tables. submitted to Ther. Com. Sc. A</comments><report-no>SSAU-02-112</report-no><acm-class>C.2.1, F.0, C.2.4</acm-class><abstract>  The aim of this paper is a theoretical study of a cache system in order to
optimize proxy cache systems and to modernize construction principles including
prefetching schemes. Two types of correlations, Zipf-like distribution and
normalizing conditions, play a role of the fundamental laws. A corresponding
system of equations allows to describe the basic effects like ratio between
construction parts, steady-state performance, optimal size, long-term
prefetching, etc. A modification of the fundamental laws leads to the
description of new effects of documents' renewal in the global network. An
internet traffic caching system based on Zipf-like distribution (ZBS) is
invented. The additional module to the cache construction gives an effective
prefetching by lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303015</id><created>2003-03-18</created><authors><author><keyname>Chernov</keyname><forenames>N.</forenames></author><author><keyname>Lesort</keyname><forenames>C.</forenames></author></authors><title>Statistical efficiency of curve fitting algorithms</title><categories>cs.CV</categories><comments>17 pages, 3 figures</comments><acm-class>I.4.8;I.5.1;I.2.10;G.3;G.1.2</acm-class><abstract>  We study the problem of fitting parametrized curves to noisy data. Under
certain assumptions (known as Cartesian and radial functional models), we
derive asymptotic expressions for the bias and the covariance matrix of the
parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower
bound, which he proved for unbiased estimates only, to more general estimates
that include many popular algorithms (most notably, the orthogonal least
squares and algebraic fits). We then show that the gradient-weighted algebraic
fit is statistically efficient and describe all other statistically efficient
algebraic fits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303016</id><created>2003-03-19</created><authors><author><keyname>Duessel</keyname><forenames>Thomas</forenames></author><author><keyname>Eicker</keyname><forenames>Norbert</forenames></author><author><keyname>Isaila</keyname><forenames>Florin</forenames></author><author><keyname>Lippert</keyname><forenames>Thomas</forenames></author><author><keyname>Moschny</keyname><forenames>Thomas</forenames></author><author><keyname>Neff</keyname><forenames>Hartmut</forenames></author><author><keyname>Schilling</keyname><forenames>Klaus</forenames></author><author><keyname>Tichy</keyname><forenames>Walter</forenames></author></authors><title>Fast Parallel I/O on Cluster Computers</title><categories>cs.DC cs.AR hep-lat</categories><comments>22 pages, 10 figures</comments><acm-class>B.4.3; C.1.2; C.2.2; D.4.3</acm-class><abstract>  Today's cluster computers suffer from slow I/O, which slows down
I/O-intensive applications. We show that fast disk I/O can be achieved by
operating a parallel file system over fast networks such as Myrinet or Gigabit
Ethernet.
  In this paper, we demonstrate how the ParaStation3 communication system helps
speed-up the performance of parallel I/O on clusters using the open source
parallel virtual file system (PVFS) as testbed and production system. We will
describe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) located
at Wuppertal University, Germany. Benchmarks on ALiCE achieve
write-performances of up to 1 GB/s from a 32-processor compute-partition to a
32-processor PVFS I/O-partition, outperforming known benchmark results for PVFS
on the same network by more than a factor of 2. Read-performance from
buffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensive
eigenmode problems from lattice quantum chromodynamics, demonstrating stability
and performance of PVFS over Parastation in large-scale production runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303017</id><created>2003-03-19</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>A Neural Network Assembly Memory Model with Maximum-Likelihood Recall
  and Recognition Properties</title><categories>cs.AI cs.IR cs.NE q-bio.NC q-bio.QM</categories><comments>A report presented at the Vth International Congress on Mathematical
  Modeling held in Dubna, Russia on September 30 - October 6, 2002 (Book of
  Abstracts, vol. 2. p. 91), 39 pages, 5 Figures, 1 Table, 70 references</comments><acm-class>I.2; E.4; J.3; J.4</acm-class><abstract>  It has been shown that a neural network model recently proposed to describe
basic memory performance is based on a ternary/binary coding/decoding algorithm
which leads to a new neural network assembly memory model (NNAMM) providing
maximum-likelihood recall/recognition properties and implying a new memory unit
architecture with Hopfield two-layer network, N-channel time gate, auxiliary
reference memory, and two nested feedback loops. For the data coding used,
conditions are found under which a version of Hopfied network implements
maximum-likelihood convolutional decoding algorithm and, simultaneously, linear
statistical classifier of arbitrary binary vectors with respect to Hamming
distance between vector analyzed and reference vector given. In addition to
basic memory performance and etc, the model explicitly describes the dependence
on time of memory trace retrieval, gives a possibility of one-trial learning,
metamemory simulation, generalized knowledge representation, and distinct
description of conscious and unconscious mental processes. It has been shown
that an assembly memory unit may be viewed as a model of a smallest inseparable
part or an 'atom' of consciousness. Some nontraditional neurobiological
backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and
error detector neurons, early precise spike firing, etc) and the model's
application to solve some interdisciplinary problems from different scientific
fields are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303018</id><created>2003-03-20</created><authors><author><keyname>Sidenbladh</keyname><forenames>Hedvig</forenames></author></authors><title>Multi-target particle filtering for the probability hypothesis density</title><categories>cs.AI</categories><comments>Submitted to International Conference on Information Fusion 2003</comments><acm-class>G.3</acm-class><abstract>  When tracking a large number of targets, it is often computationally
expensive to represent the full joint distribution over target states. In cases
where the targets move independently, each target can instead be tracked with a
separate filter. However, this leads to a model-data association problem.
Another approach to solve the problem with computational complexity is to track
only the first moment of the joint distribution, the probability hypothesis
density (PHD). The integral of this distribution over any area S is the
expected number of targets within S. Since no record of object identity is
kept, the model-data association problem is avoided.
  The contribution of this paper is a particle filter implementation of the PHD
filter mentioned above. This PHD particle filter is applied to tracking of
multiple vehicles in terrain, a non-linear tracking problem. Experiments show
that the filter can track a changing number of vehicles robustly, achieving
near-real-time performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303019</id><created>2003-03-20</created><authors><author><keyname>Boigelot</keyname><forenames>Bernard</forenames></author><author><keyname>Jodogne</keyname><forenames>Sebastien</forenames></author><author><keyname>Wolper</keyname><forenames>Pierre</forenames></author></authors><title>An Effective Decision Procedure for Linear Arithmetic with Integer and
  Real Variables</title><categories>cs.LO</categories><comments>20 pages, 6 figures</comments><acm-class>D.2.4; F.1.1; F.4.1; F.4.3</acm-class><abstract>  This paper considers finite-automata based algorithms for handling linear
arithmetic with both real and integer variables. Previous work has shown that
this theory can be dealt with by using finite automata on infinite words, but
this involves some difficult and delicate to implement algorithms. The
contribution of this paper is to show, using topological arguments, that only a
restricted class of automata on infinite words are necessary for handling real
and integer linear arithmetic. This allows the use of substantially simpler
algorithms, which have been successfully implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303020</id><created>2003-03-20</created><authors><author><keyname>Smith</keyname><forenames>Jeffrey B.</forenames></author></authors><title>Complex Systems</title><categories>cs.NI</categories><comments>14 pages</comments><acm-class>H.1.1</acm-class><abstract>  The study of Complex Systems is considered by many to be a new scientific
field, and is distinguished by being a discipline that has applications within
many separate areas of scientific study. The study of Neural Networks, Traffic
Patterns, Artificial Intelligence, Social Systems, and many other scientific
areas can all be considered to fall within the realm of Complex Systems, and
can be studied from this new perspective. The advent of more capable computer
systems has allowed these systems to be simulated and modeled with far greater
ease, and new understanding of computer modeling approaches has allowed the
fledgling science to be studied as never before.
  The preliminary focus of this paper will be to provide a general overview of
the science of Complex Systems, including terminology, definitions, history,
and examples. I will attempt to look at some of the most important trends in
different areas of research, and give a general overview of research methods
that have been used in parallel with computer modeling. Also, I will further
define the areas of the science that concern themselves with computer modeling
and simulation, and I will attempt to make it clear why the science only came
into its own when the proper modeling and simulation tools were finally
available. In addition, although there seems to be general agreement between
different authors and institutes regarding the generalities of the study, there
are some differences in terminology and methodology. I have attempted in this
paper to bring as many elements together as possible, as far as the scope of
the subject is concerned, without losing focus by studying Complex System
techniques that are bound to one particular area of scientific study, unless
that area is that of computer modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303021</id><created>2003-03-21</created><updated>2003-04-02</updated><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>A Development Calculus for Specifications</title><categories>cs.LO cs.PL</categories><comments>14 pages with some minor errors in the original version corrected</comments><acm-class>F.3.1</acm-class><abstract>  A first order inference system, called R-calculus, is defined to develop the
specifications. It is used to eliminate the laws which is not consistent with
the user's requirements. The R-calculus consists of the structural rules, an
axiom, a cut rule, and the rules for logical connectives. Some examples are
given to demonstrate the usage of the R-calculus. The properties about
reachability and completeness of the R-calculus are formally defined and are
proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303022</id><created>2003-03-21</created><authors><author><keyname>Hong</keyname><forenames>Dawei</forenames></author><author><keyname>Birget</keyname><forenames>Jean-Camille</forenames></author><author><keyname>Man</keyname><forenames>Shushuang</forenames></author></authors><title>Probabilistic behavior of hash tables</title><categories>cs.DS cs.DB</categories><acm-class>E.2</acm-class><abstract>  We extend a result of Goldreich and Ron about estimating the collision
probability of a hash function. Their estimate has a polynomial tail. We prove
that when the load factor is greater than a certain constant, the estimator has
a gaussian tail. As an application we find an estimate of an upper bound for
the average search time in hashing with chaining, for a particular user (we
allow the overall key distribution to be different from the key distribution of
a particular user). The estimator has a gaussian tail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303023</id><created>2003-03-22</created><authors><author><keyname>Das</keyname><forenames>A.</forenames></author><author><keyname>Mayer-Kress</keyname><forenames>G.</forenames></author><author><keyname>Gershenson</keyname><forenames>C.</forenames></author><author><keyname>Das</keyname><forenames>P.</forenames></author></authors><title>Conferences with Internet Web-Casting as Binding Events in a Global
  Brain: Example Data From Complexity Digest</title><categories>cs.NI cs.AI</categories><comments>4 pages, 4 figures</comments><acm-class>A.m</acm-class><abstract>  There is likeness of the Internet to human brains which has led to the
metaphor of the world-wide computer network as a `Global Brain'. We consider
conferences as 'binding events' in the Global Brain that can lead to
metacognitive structures on a global scale. One of the critical factors for
that phenomenon to happen (similar to the biological brain) are the time-scales
characteristic for the information exchange. In an electronic newsletter- the
Complexity Digest (ComDig) we include webcasting of audio (mp3) and video (asf)
files from international conferences in the weekly ComDig issues. Here we
present the time variation of the weekly rate of accesses to the conference
files. From those empirical data it appears that the characteristic time-scales
related to access of web-casting files is of the order of a few weeks. This is
at least an order of magnitude shorter than the characteristic time-scales of
peer reviewed publications and conference proceedings. We predict that this
observation will have profound implications on the nature of future conference
proceedings, presumably in electronic form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303024</id><created>2003-03-23</created><authors><author><keyname>Hicks</keyname><forenames>R. Andrew</forenames></author></authors><title>Differential Methods in Catadioptric Sensor Design with Applications to
  Panoramic Imaging</title><categories>cs.CV cs.RO</categories><comments>8 pages, 7 figures</comments><acm-class>I.2.9</acm-class><abstract>  We discuss design techniques for catadioptric sensors that realize given
projections. In general, these problems do not have solutions, but approximate
solutions may often be found that are visually acceptable. There are several
methods to approach this problem, but here we focus on what we call the
``vector field approach''. An application is given where a true panoramic
mirror is derived, i.e. a mirror that yields a cylindrical projection to the
viewer without any digital unwarping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303025</id><created>2003-03-24</created><authors><author><keyname>Cilibrasi</keyname><forenames>Rudi</forenames><affiliation>CWI</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames><affiliation>CWI</affiliation></author></authors><title>Algorithmic Clustering of Music</title><categories>cs.SD cs.LG physics.data-an</categories><comments>17 pages, 11 figures</comments><acm-class>E.4, H.3.1, I.5.3, F.1.3, J.5</acm-class><abstract>  We present a fully automatic method for music classification, based only on
compression of strings that represent the music pieces. The method uses no
background knowledge about music whatsoever: it is completely general and can,
without change, be used in different areas like linguistic classification and
genomics. It is based on an ideal theory of the information content in
individual objects (Kolmogorov complexity), information distance, and a
universal similarity metric. Experiments show that the method distinguishes
reasonably well between various musical genres and can even cluster pieces by
composer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303026</id><created>2003-03-25</created><updated>2003-10-17</updated><authors><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author><author><keyname>Giuli</keyname><forenames>TJ</forenames></author><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author><author><keyname>Muliadi</keyname><forenames>Yanto</forenames></author></authors><title>Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS</title><categories>cs.DC cs.DL</categories><comments>25 Pages, 10 figures. Extended version of conference paper</comments><acm-class>C.2.4; H.3.7; D.4.5</acm-class><abstract>  The LOCKSS project has developed and deployed in a world-wide test a
peer-to-peer system for preserving access to journals and other archival
information published on the Web. It consists of a large number of independent,
low-cost, persistent web caches that cooperate to detect and repair damage to
their content by voting in &quot;opinion polls.&quot; Based on this experience, we
present a design for and simulations of a novel protocol for voting in systems
of this kind. It incorporates rate limitation and intrusion detection to ensure
that even some very powerful adversaries attacking over many years have only a
small probability of causing irrecoverable damage before being detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303027</id><created>2003-03-25</created><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author><author><keyname>Hwang</keyname><forenames>Geng-Dian</forenames></author><author><keyname>Yu</keyname><forenames>Fang</forenames></author></authors><title>Numerical Coverage Estimation for the Symbolic Simulation of Real-Time
  Systems</title><categories>cs.SE cs.SC</categories><comments>27 pages</comments><acm-class>B.1.2</acm-class><abstract>  Three numerical coverage metrics for the symbolic simulation of dense-time
systems and their estimation methods are presented. Special techniques to
derive numerical estimations of dense-time state-spaces have also been
developed. Properties of the metrics are also discussed with respect to four
criteria. Implementation and experiments are then reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303028</id><created>2003-03-26</created><updated>2003-04-01</updated><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>The missing links in the BGP-based AS connectivity maps</title><categories>cs.NI</categories><comments>PAM2003 - The Passive and Active Measurement
  Workshop(http://www.pam2003.org), San Diego, USA, April 2003</comments><acm-class>C.2.1 and C.2.5</acm-class><abstract>  A number of recent studies of the Internet topology at the autonomous systems
level (AS graph) are based on the BGP-based AS connectivity maps (original
maps). The so-called extended maps use additional data sources and contain more
complete pictures of the AS graph. In this paper, we compare an original map,
an extended map and a synthetic map generated by the Barabasi-Albert model. We
examine the recently reported rich-club phenomenon, alternative routing paths
and attack tolerance. We point out that the majority of the missing links of
the original maps are the connecting links between rich nodes (nodes with large
numbers of links) of the extended maps. We show that the missing links are
relevant because links between rich nodes can be crucial for the network
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303029</identifier>
 <datestamp>2008-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303029</id><created>2003-03-26</created><updated>2003-06-18</updated><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>Towards Modelling The Internet Topology - The Interactive Growth Model</title><categories>cs.NI</categories><comments>Tp be appeared in the proceedings of the 18th International
  Teletraffic Congress (ITC18), Berlin, August 2003</comments><acm-class>C.2.1 C.2.5</acm-class><journal-ref>Published in Proc. of the 18th International Teletraffic Congress
  (Elsevier's Teletraffic Science and Engineering series, vol.5a, p.121) 2003
  http://www.elsevier.com/wps/find/bookdescription.cws_home/680828/description</journal-ref><abstract>  The Internet topology at the Autonomous Systems level (AS graph) has a
power--law degree distribution and a tier structure. In this paper, we
introduce the Interactive Growth (IG) model based on the joint growth of new
nodes and new links. This simple and dynamic model compares favorable with
other Internet power--law topology generators because it not only closely
resembles the degree distribution of the AS graph, but also accurately matches
the hierarchical structure, which is measured by the recently reported
rich-club phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303030</id><created>2003-03-26</created><updated>2003-06-30</updated><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>Analyzing and modelling the AS-level Internet topology</title><categories>cs.NI</categories><comments>To be appeared in proceedings of the HET-NETs' 03 (First
  International Working Conference on Performance Modelling and Evaluation of
  Heterogeneous Networks), York, UK, July 2003</comments><acm-class>C.2.1 and C.2.5</acm-class><abstract>  Recently we introduced the rich-club phenomenon as a quantitative metric to
characterize the tier structure of the Autonomous Systems level Internet
topology (AS graph) and we proposed the Interactive Growth (IG) model, which
closely matches the degree distribution and hierarchical structure of the AS
graph and compares favourble with other available Internet power-law topology
generators. Our research was based on the widely used BGP AS graph obtained
from the Oregon BGP routing tables. Researchers argue that Traceroute AS graph,
extracted from the traceroute data collected by the CAIDA's active probing
tool, Skitter, is more complete and reliable. To be prudent, in this paper we
analyze and compare topological structures of Traceroute AS graph and BGP AS
graph. Also we compare with two synthetic Internet topologies generated by the
IG model and the well-known Barabasi-Albert (BA) model. Result shows that both
AS graphs show the rich-club phenomenon and have similar tier structures, which
are closely matched by the IG model, however the BA model does not show the
rich-club phenomenon at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303031</id><created>2003-03-28</created><authors><author><keyname>Di Pierro</keyname><forenames>Massimo</forenames></author></authors><title>A Bird's eye view of Matrix Distributed Processing</title><categories>cs.DC cs.CE cs.DM cs.MS hep-lat physics.comp-ph</categories><acm-class>D.1.3; D.3.2;G.1;G.4;I.6.8</acm-class><journal-ref>Proceedings of the ICCSA 2003 Conference</journal-ref><abstract>  We present Matrix Distributed Processing, a C++ library for fast development
of efficient parallel algorithms. MDP is based on MPI and consists of a
collection of C++ classes and functions such as lattice, site and field. Once
an algorithm is written using these components the algorithm is automatically
parallel and no explicit call to communication functions is required. MDP is
particularly suitable for implementing parallel solvers for multi-dimensional
differential equations and mesh-like problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303032</id><created>2003-03-31</created><authors><author><keyname>Igel</keyname><forenames>Christian</forenames></author><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>Recent Results on No-Free-Lunch Theorems for Optimization</title><categories>cs.NE math.OC nlin.AO</categories><comments>10 pages, LaTeX, see http://www.neuroinformatik.rub.de/PROJECTS/SONN/</comments><acm-class>G.1.6</acm-class><abstract>  The sharpened No-Free-Lunch-theorem (NFL-theorem) states that the performance
of all optimization algorithms averaged over any finite set F of functions is
equal if and only if F is closed under permutation (c.u.p.) and each target
function in F is equally likely. In this paper, we first summarize some
consequences of this theorem, which have been proven recently: The average
number of evaluations needed to find a desirable (e.g., optimal) solution can
be calculated; the number of subsets c.u.p. can be neglected compared to the
overall number of possible subsets; and problem classes relevant in practice
are not likely to be c.u.p. Second, as the main result, the NFL-theorem is
extended. Necessary and sufficient conditions for NFL-results to hold are given
for arbitrary, non-uniform distributions of target functions. This yields the
most general NFL-theorem for optimization presented so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0303033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0303033</id><created>2003-03-30</created><updated>2004-11-21</updated><authors><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author></authors><title>A Digital Preservation Appliance Based on OpenBSD</title><categories>cs.DC cs.DL</categories><comments>12 pages</comments><acm-class>D.4.5</acm-class><journal-ref>Proceedings of BSDcon, 2003</journal-ref><abstract>  The LOCKSS program has developed and deployed in a world-wide test a system
for preserving access to academic journals published on the Web. The
fundamental problem for any digital preservation system is that it must be
affordable for the long term. To reduce the cost of ownership, the LOCKSS
system uses generic PC hardware, open source software, and peer-to-peer
technology. It is packaged as a ``network appliance'', a single-function box
that can be connected to the Internet, configured and left alone to do its job
with minimal monitoring or administration. The first version of this system was
based on a Linux boot floppy. After three years of testing it was replaced by a
second version, based on OpenBSD and booting from CD-ROM.
  We focus in this paper on the design, implementation and deployment of a
network appliance based on an open source operating system. We provide an
overview of the LOCKSS application and describe the experience of deploying and
supporting its first version. We list the requirements we took from this to
drive the design of the second version, describe how we satisfied them in the
OpenBSD environment, and report on the initial
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304001</id><created>2003-03-31</created><authors><author><keyname>Vinson</keyname><forenames>Norman G.</forenames></author></authors><title>Design Guidelines for Landmarks to Support Navigation in Virtual
  Environments</title><categories>cs.HC</categories><comments>9 pages, 1 figure</comments><report-no>NRC 43578</report-no><acm-class>H.5.4; I.6</acm-class><journal-ref>Proceedings of the SIGCHI conference on Human factors in computing
  systems: the CHI is the limit, p.278-285, May 15-20, 1999, Pittsburgh,
  Pennsylvania, United States</journal-ref><abstract>  Unfamiliar, large-scale virtual environments are difficult to navigate. This
paper presents design guidelines to ease navigation in such virtual
environments. The guidelines presented here focus on the design and placement
of landmarks in virtual environments. Moreover, the guidelines are based
primarily on the extensive empirical literature on navigation in the real
world. A rationale for this approach is provided by the similarities between
navigational behavior in real and virtual environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304002</id><created>2003-04-01</created><authors><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Romaine</keyname><forenames>Matthew</forenames></author><author><keyname>Szymanski</keyname><forenames>Margaret H.</forenames></author><author><keyname>Thornton</keyname><forenames>James D.</forenames></author><author><keyname>Wilson</keyname><forenames>Daniel</forenames></author><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author></authors><title>The Mad Hatter&amp;acute;s Cocktail Party: A Social Mobile Audio Space
  Supporting Multiple Simultaneous Conversations</title><categories>cs.HC cs.SD</categories><comments>8 pages</comments><acm-class>H.4.3; H.5.3</acm-class><journal-ref>Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems, Ft.
  Lauderdale, FL, Apr. 2003, 425-432. ACM Press.</journal-ref><doi>10.1145/642611.642686</doi><abstract>  This paper presents a mobile audio space intended for use by gelled social
groups. In face-to-face interactions in such social groups, conversational
floors change frequently, e.g., two participants split off to form a new
conversational floor, a participant moves from one conversational floor to
another, etc. To date, audio spaces have provided little support for such
dynamic regroupings of participants, either requiring that the participants
explicitly specify with whom they wish to talk or simply presenting all
participants as though they are in a single floor. By contrast, the audio space
described here monitors participant behavior to identify conversational floors
as they emerge. The system dynamically modifies the audio delivered to each
participant to enhance the salience of the participants with whom they are
currently conversing. We report a user study of the system, focusing on
conversation analytic results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304003</id><created>2003-04-01</created><updated>2003-05-19</updated><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author><author><keyname>Hwang</keyname><forenames>Geng-Dian</forenames></author><author><keyname>Yu</keyname><forenames>Fang</forenames></author></authors><title>TCTL Inevitability Analysis of Dense-time Systems</title><categories>cs.SC</categories><comments>22 pages</comments><acm-class>B.1.2</acm-class><abstract>  Inevitability properties in branching temporal logics are of the syntax
forall eventually \phi, where \phi is an arbitrary (timed) CTL formula. In the
sense that &quot;good things will happen&quot;, they are parallel to the &quot;liveness&quot;
properties in linear temporal logics. Such inevitability properties in
dense-time logics can be analyzed with greatest fixpoint calculation. We
present algorithms to model-check inevitability properties both with and
without requirement of non-Zeno computations. We discuss a technique for early
decision on greatest fixpoints in the temporal logics, and experiment with the
effect of non-Zeno computations on the evaluation of greatest fixpoints. We
also discuss the TCTL subclass with only universal path quantifiers which
allows for the safe abstraction analysis of inevitability properties. Finally,
we report our implementation and experiments to show the plausibility of our
ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304004</id><created>2003-04-01</created><updated>2004-01-02</updated><authors><author><keyname>Ziegler</keyname><forenames>Martin</forenames></author></authors><title>Quasi-Optimal Arithmetic for Quaternion Polynomials</title><categories>cs.SC</categories><comments>published version (11 pages) plus appendix (2 pages)</comments><acm-class>I.1;F.2.1</acm-class><journal-ref>pp.705-715 in Proc.14th ISAAC (2003), Springer LNCS 2906</journal-ref><abstract>  Fast algorithms for arithmetic on real or complex polynomials are well-known
and have proven to be not only asymptotically efficient but also very
practical. Based on Fast Fourier Transform (FFT), they for instance multiply
two polynomials of degree up to N or multi-evaluate one at N points
simultaneously within quasi-linear time O(N.polylog N). An extension to (and in
fact the mere definition of) polynomials over the skew-field H of quaternions
is promising but still missing. The present work proposes three such
definitions which in the commutative case coincide but for H turn out to
differ, each one satisfying some desirable properties while lacking others. For
each notion we devise algorithms for according arithmetic; these are
quasi-optimal in that their running times match lower complexity bounds up to
polylogarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304005</id><created>2003-04-01</created><authors><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>Quantum Computation and Lattice Problems</title><categories>cs.DS</categories><acm-class>F.2.1</acm-class><abstract>  We present the first explicit connection between quantum computation and
lattice problems. Namely, we show a solution to the Unique Shortest Vector
Problem (SVP) under the assumption that there exists an algorithm that solves
the hidden subgroup problem on the dihedral group by coset sampling. Moreover,
we solve the hidden subgroup problem on the dihedral group by using an average
case subset sum routine. By combining the two results, we get a quantum
reduction from $\Theta(n^{2.5})$-unique-SVP to the average case subset sum
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304006</id><created>2003-04-02</created><authors><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence
  Alignment</title><categories>cs.CL</categories><comments>Proceedings of HLT-NAACL 2003 (Human Language Technology Conference)</comments><acm-class>I.2.7</acm-class><abstract>  We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304007</id><created>2003-04-03</created><authors><author><keyname>Petrovic</keyname><forenames>Slobodan</forenames></author><author><keyname>Alvarez</keyname><forenames>Gonzalo</forenames></author></authors><title>A Method for Clustering Web Attacks Using Edit Distance</title><categories>cs.IR cs.AI cs.CR</categories><comments>10 pages, 2 figures, latex format</comments><acm-class>H.3.3;K.6.5</acm-class><abstract>  Cluster analysis often serves as the initial step in the process of data
classification. In this paper, the problem of clustering different length input
data is considered. The edit distance as the minimum number of elementary edit
operations needed to transform one vector into another is used. A heuristic for
clustering unequal length vectors, analogue to the well known k-means algorithm
is described and analyzed. This heuristic determines cluster centroids
expanding shorter vectors to the lengths of the longest ones in each cluster in
a specific way. It is shown that the time and space complexities of the
heuristic are linear in the number of input vectors. Experimental results on
real data originating from a system for classification of Web attacks are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304008</id><created>2003-04-04</created><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames></author></authors><title>A Physics-Free Introduction to the Quantum Computation Model</title><categories>cs.CC quant-ph</categories><comments>18 pages, 18 figures. Expanded write-up for BEATCS of introductory
  talk given at Dagstuhl Seminar 02421, &quot;Algebraic Methods in Quantum and
  Classical Models of Computation,&quot; October 2002</comments><acm-class>F.1.1; F.1.2; F.1.3</acm-class><journal-ref>Bulletin of the European Association for Theoretical Computer
  Science, 79(Feb 2003), 69-85</journal-ref><abstract>  This article defines and proves basic properties of the standard quantum
circuit model of computation. The model is developed abstractly in close
analogy with (classical) deterministic and probabilistic circuits, without
recourse to any physical concepts or principles. It is intended as a primer for
theoretical computer scientists who do not know--and perhaps do not care to
know--any physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304009</id><created>2003-04-07</created><authors><author><keyname>Daniel</keyname><forenames>Gilles</forenames></author></authors><title>Stochastic Volatility in a Quantitative Model of Stock Market Returns</title><categories>cs.CE</categories><comments>MSc thesis (2002), 83 pages, 15 figures</comments><acm-class>G3</acm-class><abstract>  Standard quantitative models of the stock market predict a log-normal
distribution for stock returns (Bachelier 1900, Osborne 1959), but it is
recognised (Fama 1965) that empirical data, in comparison with a Gaussian,
exhibit leptokurtosis (it has more probability mass in its tails and centre)
and fat tails (probabilities of extreme events are underestimated). Different
attempts to explain this departure from normality have coexisted. In
particular, since one of the strong assumptions of the Gaussian model concerns
the volatility, considered finite and constant, the new models were built on a
non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)
volatility. We investigate in this thesis a very recent model (Dragulescu et
al. 2002) based on a Brownian motion process for the returns, and a stochastic
mean-reverting process for the volatility. In this model, the forward
Kolmogorov equation that governs the time evolution of returns is solved
analytically. We test this new theory against different stock indexes (Dow
Jones Industrial Average, Standard and Poor s and Footsie), over different
periods (from 20 to 105 years). Our aim is to compare this model with the
classical Gaussian and with a simple Neural Network, used as a benchmark. We
perform the usual statistical tests on the kurtosis and tails of the expected
distributions, paying particular attention to the outliers. As claimed by the
authors, the new model outperforms the Gaussian for any time lag, but is
artificially too complex for medium and low frequencies, where the Gaussian is
preferable. Moreover this model is still rejected for high frequencies, at a
0.05 level of significance, due to the kurtosis, incorrectly handled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304010</identifier>
 <datestamp>2010-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304010</id><created>2003-04-08</created><authors><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author><author><keyname>Vishne</keyname><forenames>Uzi</forenames></author></authors><title>Efficient linear feedback shift registers with maximal period</title><categories>cs.CR math.NT</categories><acm-class>11T06, 11T71</acm-class><journal-ref>Finite Fields and their Applications 8 (2002), 256--267</journal-ref><doi>10.1006/ffta.2001.0339</doi><abstract>  We introduce and analyze an efficient family of linear feedback shift
registers (LFSR's) with maximal period. This family is word-oriented and is
suitable for implementation in software, thus provides a solution to a recent
challenge posed in FSE '94. The classical theory of LFSR's is extended to
provide efficient algorithms for generation of irreducible and primitive LFSR's
of this new type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304011</id><created>2003-04-08</created><authors><author><keyname>Anderson</keyname><forenames>Paul</forenames></author><author><keyname>Carvalho</keyname><forenames>Goncalo</forenames></author></authors><title>Embedded Reflection Mapping</title><categories>cs.GR</categories><acm-class>I.3.7</acm-class><abstract>  Environment maps are used to simulate reflections off curved objects. We
present a technique to reflect a user, or a group of users, in a real
environment, onto a virtual object, in a virtual reality application, using the
live video feeds from a set of cameras, in real-time. Our setup can be used in
a variety of environments ranging from outdoor or indoor scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304012</id><created>2003-04-08</created><authors><author><keyname>Buhrman</keyname><forenames>Harry</forenames><affiliation>CWI and University of Amsterdam</affiliation></author><author><keyname>Klauck</keyname><forenames>Hartmut</forenames><affiliation>IAS, Princeton</affiliation></author><author><keyname>Vereshchagin</keyname><forenames>Nikolai</forenames><affiliation>Moscow University</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Individual Communication Complexity</title><categories>cs.CC cs.DC</categories><comments>11 pages, LaTeX</comments><acm-class>F.1; F.2</acm-class><abstract>  We initiate the theory of communication complexity of individual inputs held
by the agents, rather than worst-case or average-case. We consider total,
partial, and partially correct protocols, one-way versus two-way, with and
without help bits. The results are expressed in trems of Kolmogorov complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304013</id><created>2003-04-09</created><authors><author><keyname>Toli</keyname><forenames>Ilia</forenames></author></authors><title>Hidden Polynomial(s) Cryptosystems</title><categories>cs.CR cs.SC</categories><acm-class>E.3</acm-class><abstract>  We propose public-key cryptosystems with public key a system of polynomial
equations, algebraic or differential, and private key a single polynomial or a
small-size ideal. We set up probabilistic encryption, signature, and
signcryption protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304014</id><created>2003-04-10</created><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author><author><keyname>Nascimento</keyname><forenames>Anderson C. A.</forenames></author><author><keyname>Imai</keyname><forenames>Hideki</forenames></author></authors><title>Commitment Capacity of Discrete Memoryless Channels</title><categories>cs.CR quant-ph</categories><comments>20 pages, LaTeX2e</comments><acm-class>E.3;H.1</acm-class><journal-ref>Proc. 9th Cirencester Crypto and Coding Conf., LNCS 2989, pp
  35-51, Springer, Berlin 2003.</journal-ref><abstract>  In extension of the bit commitment task and following work initiated by
Crepeau and Kilian, we introduce and solve the problem of characterising the
optimal rate at which a discrete memoryless channel can be used for bit
commitment. It turns out that the answer is very intuitive: it is the maximum
equivocation of the channel (after removing trivial redundancy), even when
unlimited noiseless bidirectional side communication is allowed.
  By a well-known reduction, this result provides a lower bound on the
channel's capacity for implementing coin tossing, which we conjecture to be an
equality.
  The method of proving this relates the problem to Wyner's wire--tap channel
in an amusing way. We also discuss extensions to quantum channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304015</id><created>2003-04-10</created><authors><author><keyname>Zhang</keyname><forenames>Xuehai</forenames></author><author><keyname>Freschl</keyname><forenames>Jeffrey</forenames></author><author><keyname>Schopf</keyname><forenames>Jennifer M.</forenames></author></authors><title>A Performance Study of Monitoring and Information Services for
  Distributed Systems</title><categories>cs.PF</categories><comments>12 pages, 20 figures</comments><report-no>Preprint ANL/MCS-P1040-0403</report-no><acm-class>H.3.4; C.4</acm-class><abstract>  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304016</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304016</id><created>2003-04-10</created><updated>2011-06-13</updated><authors><author><keyname>Burger</keyname><forenames>J. R.</forenames></author></authors><title>Symmetric and anti-symmetric quantum functions</title><categories>cs.OH quant-ph</categories><comments>Rewrote for clarity; added references to reversible computing;
  removed mention of Simon's functions</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and analyzes symmetric and anti-symmetric quantum
binary functions. Generally, such functions uniquely convert a given
computational basis state into a different basis state, but with either a plus
or a minus sign. Such functions may serve along with a constant function (in a
Deutsch-Jozsa type of algorithm) to provide 2**n deterministic qubit
combinations (for n qubits) instead of just one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304017</id><created>2003-04-10</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author></authors><title>Ground Canonicity</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><abstract>  We explore how different proof orderings induce different notions of
saturation. We relate completion, paramodulation, saturation, redundancy
elimination, and rewrite system reduction to proof orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304018</id><created>2003-04-10</created><updated>2003-07-09</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Quasiconvex Analysis of Backtracking Algorithms</title><categories>cs.DS cs.CG math.CO</categories><comments>12 pages, 2 figures. This revision includes a larger example
  recurrence and reports on a second implementation of the algorithm</comments><acm-class>F.2.2; G.1.6</acm-class><abstract>  We consider a class of multivariate recurrences frequently arising in the
worst case analysis of Davis-Putnam-style exponential time backtracking
algorithms for NP-hard problems. We describe a technique for proving asymptotic
upper bounds on these recurrences, by using a suitable weight function to
reduce the problem to that of solving univariate linear recurrences; show how
to use quasiconvex programming to determine the weight function yielding the
smallest upper bound; and prove that the resulting upper bounds are within a
polynomial factor of the true asymptotics of the recurrence. We develop and
implement a multiple-gradient descent algorithm for the resulting quasiconvex
programs, using a real-number arithmetic package for guaranteed accuracy of the
computed worst case time bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304019</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304019</id><created>2003-04-10</created><authors><author><keyname>Levin</keyname><forenames>David N.</forenames></author></authors><title>Blind Normalization of Speech From Different Channels</title><categories>cs.CL</categories><comments>25 pages, 7 figures</comments><acm-class>I.2.7</acm-class><doi>10.1121/1.1755235</doi><abstract>  We show how to construct a channel-independent representation of speech that
has propagated through a noisy reverberant channel. This is done by blindly
rescaling the cepstral time series by a non-linear function, with the form of
this scale function being determined by previously encountered cepstra from
that channel. The rescaled form of the time series is an invariant property of
it in the following sense: it is unaffected if the time series is transformed
by any time-independent invertible distortion. Because a linear channel with
stationary noise and impulse response transforms cepstra in this way, the new
technique can be used to remove the channel dependence of a cepstral time
series. In experiments, the method achieved greater channel-independence than
cepstral mean normalization, and it was comparable to the combination of
cepstral mean normalization and spectral subtraction, despite the fact that no
measurements of channel noise or reverberations were required (unlike spectral
subtraction).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304020</id><created>2003-04-11</created><updated>2003-04-14</updated><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Sen</keyname><forenames>Pranab</forenames></author></authors><title>A direct sum theorem in communication complexity via message compression</title><categories>cs.CC</categories><comments>21 pages. Full version of a paper to appear at ICALP 2003</comments><acm-class>F.1.2; E.4</acm-class><abstract>  We prove lower bounds for the direct sum problem for two-party bounded error
randomised multiple-round communication protocols. Our proofs use the notion of
information cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yao
and refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our main
technical result is a `compression' theorem saying that, for any probability
distribution $\mu$ over the inputs, a $k$-round private coin bounded error
protocol for a function $f$ with information cost $c$ can be converted into a
$k$-round deterministic protocol for $f$ with bounded distributional error and
communication cost $O(kc)$. We prove this result using a substate theorem about
relative entropy and a rejection sampling argument. Our direct sum result
follows from this `compression' result via elementary information theoretic
arguments.
  We also consider the direct sum problem in quantum communication. Using a
probabilistic argument, we show that messages cannot be compressed in this
manner even if they carry small information. Hence, new techniques may be
necessary to tackle the direct sum problem in quantum communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304021</id><created>2003-04-15</created><authors><author><keyname>Buchholz</keyname><forenames>Peter</forenames></author><author><keyname>Kemper</keyname><forenames>Peter</forenames></author></authors><title>Model Checking for a Class of Weighted Automata</title><categories>cs.LO</categories><comments>24 pages</comments><report-no>University of Dortmund, Department of Computer Science Technical
  report No. 779</report-no><acm-class>D.2.4; F.3.1</acm-class><abstract>  A large number of different model checking approaches has been proposed
during the last decade. The different approaches are applicable to different
model types including untimed, timed, probabilistic and stochastic models. This
paper presents a new framework for model checking techniques which includes
some of the known approaches, but enlarges the class of models for which model
checking can be applied to the general class of weighted automata. The approach
allows an easy adaption of model checking to models which have not been
considered yet for this purpose. Examples for those new model types for which
model checking can be applied are max/plus or min/plus automata which are well
established models to describe different forms of dynamic systems and
optimization problems. In this context, model checking can be used to verify
temporal or quantitative properties of a system. The paper first presents
briefly our class of weighted automata, as a very general model type. Then
Valued Computational Tree Logic (CTL$) is introduced as a natural extension of
the well known branching time logic CTL. Afterwards, algorithms to check a
weighted automaton according to a CTL$ formula are presented. As a last result,
a bisimulation is presented for weighted automata and for CTL$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304022</id><created>2003-04-15</created><authors><author><keyname>Smith</keyname><forenames>Arnold</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Turney</keyname><forenames>Peter</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Ewaschuk</keyname><forenames>Robert</forenames><affiliation>University of Waterloo</affiliation></author></authors><title>Self-Replicating Machines in Continuous Space with Virtual Physics</title><categories>cs.NE cs.CE q-bio.PE</categories><comments>39 pages, Java code available at http://purl.org/net/johnnyvon/</comments><report-no>NRC-44969</report-no><acm-class>I.6.3; I.6.8; J.2; J.3</acm-class><journal-ref>Artificial Life, (2003), 9, 21-40</journal-ref><abstract>  JohnnyVon is an implementation of self-replicating machines in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes Brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary &quot;seed&quot; pattern
is put in a &quot;soup&quot; of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304023</id><created>2003-04-17</created><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Partitioning Regular Polygons into Circular Pieces I: Convex Partitions</title><categories>cs.CG</categories><comments>21 pages, 25 figures</comments><acm-class>F.2.2</acm-class><abstract>  We explore an instance of the question of partitioning a polygon into pieces,
each of which is as ``circular'' as possible, in the sense of having an aspect
ratio close to 1. The aspect ratio of a polygon is the ratio of the diameters
of the smallest circumscribing circle to the largest inscribed disk. The
problem is rich even for partitioning regular polygons into convex pieces, the
focus of this paper. We show that the optimal (most circular) partition for an
equilateral triangle has an infinite number of pieces, with the lower bound
approachable to any accuracy desired by a particular finite partition. For
pentagons and all regular k-gons, k &gt; 5, the unpartitioned polygon is already
optimal. The square presents an interesting intermediate case. Here the
one-piece partition is not optimal, but nor is the trivial lower bound
approachable. We narrow the optimal ratio to an aspect-ratio gap of 0.01082
with several somewhat intricate partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304024</id><created>2003-04-16</created><authors><author><keyname>Victor</keyname><forenames>Kromer</forenames></author></authors><title>Glottochronologic Retrognostic of Language System</title><categories>cs.CL</categories><comments>10 pages, 7 figures. In Russian</comments><acm-class>I.2.7</acm-class><abstract>  A glottochronologic retrognostic of language system is proposed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304025</id><created>2003-04-18</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 44</title><categories>cs.CG</categories><comments>3 pages, 3 figures</comments><acm-class>F.2.2</acm-class><abstract>  The open problem of whether or not every pair of equal-area polygons has a
hinged dissection is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304026</id><created>2003-04-19</created><authors><author><keyname>Dinur</keyname><forenames>Irit</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Khot</keyname><forenames>Subhash</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>A New Multilayered PCP and the Hardness of Hypergraph Vertex Cover</title><categories>cs.CC</categories><acm-class>F.1.3</acm-class><abstract>  Given a $k$-uniform hyper-graph, the E$k$-Vertex-Cover problem is to find the
smallest subset of vertices that intersects every hyper-edge. We present a new
multilayered PCP construction that extends the Raz verifier. This enables us to
prove that E$k$-Vertex-Cover is NP-hard to approximate within factor
$(k-1-\epsilon)$ for any $k \geq 3$ and any $\epsilon&gt;0$. The result is
essentially tight as this problem can be easily approximated within factor $k$.
Our construction makes use of the biased Long-Code and is analyzed using
combinatorial properties of $s$-wise $t$-intersecting families of subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304027</id><created>2003-04-21</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>&quot;I'm sorry Dave, I'm afraid I can't do that&quot;: Linguistics, Statistics,
  and Natural Language Processing circa 2001</title><categories>cs.CL</categories><comments>To appear, National Research Council study on the Fundamentals of
  Computer Science. 7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>In &quot;Computer Science: Reflections on the Field, Reflections from
  the Field&quot; (report of the National Academies' Study on the Fundamentals of
  Computer Science), pp. 111--118, 2004</journal-ref><abstract>  A brief, general-audience overview of the history of natural language
processing, focusing on data-driven approaches.Topics include &quot;Ambiguity and
language analysis&quot;, &quot;Firth things first&quot;, &quot;A 'C' change&quot;, and &quot;The empiricists
strike back&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304028</id><created>2003-04-21</created><authors><author><keyname>Hughes</keyname><forenames>Baden</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Grid-Enabling Natural Language Engineering By Stealth</title><categories>cs.DC cs.CL</categories><acm-class>J.5; D.1; C.2</acm-class><abstract>  We describe a proposal for an extensible, component-based software
architecture for natural language engineering applications. Our model leverages
existing linguistic resource description and discovery mechanisms based on
extended Dublin Core metadata. In addition, the application design is flexible,
allowing disparate components to be combined to suit the overall application
functionality. An application specification language provides abstraction from
the programming environment and allows ease of interface with computational
grids via a broker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304029</id><created>2003-04-22</created><authors><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author></authors><title>An XML based Document Suite</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.1</acm-class><journal-ref>Proceedings of COLING 2002; p. 1278-1282</journal-ref><abstract>  We report about the current state of development of a document suite and its
applications. This collection of tools for the flexible and robust processing
of documents in German is based on the use of XML as unifying formalism for
encoding input and output data as well as process information. It is organized
in modules with limited responsibilities that can easily be combined into
pipelines to solve complex tasks. Strong emphasis is laid on a number of
techniques to deal with lexical and conceptual gaps that are typical when
starting a new application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304030</id><created>2003-04-22</created><authors><author><keyname>Hitchcock</keyname><forenames>John M.</forenames></author></authors><title>Small Spans in Scaled Dimension</title><categories>cs.CC</categories><comments>28 pages</comments><acm-class>F.1.3</acm-class><abstract>  Juedes and Lutz (1995) proved a small span theorem for polynomial-time
many-one reductions in exponential time. This result says that for language A
decidable in exponential time, either the class of languages reducible to A
(the lower span) or the class of problems to which A can be reduced (the upper
span) is small in the sense of resource-bounded measure and, in particular,
that the degree of A is small. Small span theorems have been proven for
increasingly stronger polynomial-time reductions, and a small span theorem for
polynomial-time Turing reductions would imply BPP != EXP. In contrast to the
progress in resource-bounded measure, Ambos-Spies, Merkle, Reimann, and Stephan
(2001) showed that there is no small span theorem for the resource-bounded
dimension of Lutz (2000), even for polynomial-time many-one reductions.
  Resource-bounded scaled dimension, recently introduced by Hitchcock, Lutz,
and Mayordomo (2003), provides rescalings of resource-bounded dimension. We use
scaled dimension to further understand the contrast between measure and
dimension regarding polynomial-time spans and degrees. We strengthen prior
results by showing that the small span theorem holds for polynomial-time
many-one reductions in the -3rd-order scaled dimension, but fails to hold in
the -2nd-order scaled dimension. Our results also hold in exponential space.
  As an application, we show that determining the -2nd- or -1st-order scaled
dimension in ESPACE of the many-one complete languages for E would yield a
proof of P = BPP or P != PSPACE. On the other hand, it is shown unconditionally
that the complete languages for E have -3rd-order scaled dimension 0 in ESPACE
and -2nd- and -1st-order scaled dimension 1 in E.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304031</id><created>2003-04-22</created><authors><author><keyname>Galbi</keyname><forenames>Douglas A.</forenames></author></authors><title>Transforming the Structure of Network Interconnection and Transport</title><categories>cs.CY</categories><acm-class>C.2.1</acm-class><journal-ref>CommLaw Conspectus, v. 8, n. 2 (Summer 2000) pp. 203-18</journal-ref><abstract>  Vibrant development of a network-based economy requires separating investment
in highly location specific local access technology from the development of
standardized, geography-independent, wide-area network services. Thus far
interconnection arrangements and associated regulations have been too closely
tied to the idiosyncratic geographic structure of individual operators'
networks. A key industry challenge is to foster the development of a wide area
lattice of common geographic points of interconnection. Sound regulatory and
anti-trust policy can help address this industry need.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304032</id><created>2003-04-22</created><authors><author><keyname>Galbi</keyname><forenames>Douglas A.</forenames></author></authors><title>Growth in the &quot;New Economy&quot;: U.S. Bandwidth Use and Pricing Across the
  1990s</title><categories>cs.CY</categories><acm-class>C.2.3; C.2.5</acm-class><journal-ref>Telecommunications Policy 25 (2001) 139-154</journal-ref><abstract>  An acceleration in the growth of communications bandwidth in use and a rapid
reduction in bandwidth prices have not accompanied the U.S. economy's strong
performance in the second half of the 1990s. Overall U.S. bandwidth in use has
grown robustly throughout the 1990s, but growth has not significantly
accelerated in the second half of 1990s. Average prices for U.S. bandwidth in
use have fallen little in nominal terms in the second half of the 1990s. Policy
makers and policy analysts should recognize that institutional change, rather
than more competitors of established types, appears to be key to dramatic
improvements in bandwidth growth and prices. Such a development could provide a
significant additional impetus to aggregate growth and productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304033</id><created>2003-04-22</created><authors><author><keyname>Galbi</keyname><forenames>Douglas A.</forenames></author></authors><title>A New Account of Personalization and Effective Communication</title><categories>cs.CY</categories><acm-class>H.1.1; E.4; H.4.3</acm-class><abstract>  To contribute to understanding of information economies of daily life, this
paper explores over the past millennium given names of a large number of
persons. Analysts have long both condemned and praised mass media as a source
of common culture, national unity, or shared symbolic experiences. Names,
however, indicate a large decline in shared symbolic experience over the past
two centuries, a decline that the growth of mass media does not appear to have
affected significantly. Study of names also shows that action and personal
relationships, along with time horizon, are central aspects of effective
communication across a large population. The observed preference for
personalization over the past two centuries and the importance of action and
personal relationships to effective communication are aspects of information
economies that are likely to have continuing significance for industry
developments, economic statistics, and public policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304034</id><created>2003-04-22</created><authors><author><keyname>Galbi</keyname><forenames>Douglas A.</forenames></author></authors><title>Revolutionary Ideas for Radio Regulation</title><categories>cs.CY</categories><acm-class>C.2.1</acm-class><abstract>  Radio technology seems destined to become part of the standard
micro-processor input/output system. But unlike for memory or display systems,
for radio systems government regulation matters a lot. Much discussion of radio
regulation has focused on narrow spectrum management and interference issues.
Reflecting on historical experience and centuries of conversation about
fundamental political choices, persons working with radio technology should
also ponder three questions. First, what is a good separation and balance of
powers in radio regulation? Second, how should radio regulation be
geographically configured? Third, how should radio regulation understand and
respect personal freedom and equality? Working out answering to these questions
involves a general process of shaping good government. This process will be
hugely important for radio regulation, technology, and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304035</id><created>2003-04-23</created><authors><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author></authors><title>Exploiting Sublanguage and Domain Characteristics in a Bootstrapping
  Approach to Lexicon and Ontology Creation</title><categories>cs.CL</categories><acm-class>H.3.1; I.2.7</acm-class><journal-ref>Workshop-Proceedings of the OntoLex 2002 - Ontologies and Lexical
  Knowledge Bases at the LREC 2002, p. 68-73</journal-ref><abstract>  It is very costly to build up lexical resources and domain ontologies.
Especially when confronted with a new application domain lexical gaps and a
poor coverage of domain concepts are a problem for the successful exploitation
of natural language document analysis systems that need and exploit such
knowledge sources. In this paper we report about ongoing experiments with
`bootstrapping techniques' for lexicon and ontology creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304036</id><created>2003-04-23</created><authors><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Xiao</keyname><forenames>Chun</forenames></author></authors><title>An Approach for Resource Sharing in Multilingual NLP</title><categories>cs.CL</categories><comments>poster</comments><acm-class>H3.1; I.2.7</acm-class><journal-ref>STAIRS 2002 - STarting Artificial Intelligence Researchers
  Symposium at the ECAI 2002. Lyon, France. ISBN 158603 259 3. IOS Press
  Amsterdam, p. 123-124</journal-ref><abstract>  In this paper we describe an approach for the analysis of documents in German
and English with a shared pool of resources. For the analysis of German
documents we use a document suite, which supports the user in tasks like
information retrieval and information extraction. The core of the document
suite is based on our tool XDOC. Now we want to exploit these methods for the
analysis of English documents as well. For this aim we need a multilingual
presentation format of the resources. These resources must be transformed into
an unified format, in which we can set additional information about linguistic
characteristics of the language depending on the analyzed documents. In this
paper we describe our approach for such an exchange model for multilingual
resources based on XML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304037</id><created>2003-04-23</created><authors><author><keyname>Vazhkudai</keyname><forenames>Sudharshan</forenames></author><author><keyname>Schopf</keyname><forenames>Jennifer M.</forenames></author></authors><title>Using Regression Techniques to Predict Large Data Transfers</title><categories>cs.DC</categories><comments>29 pages, 11 figures</comments><report-no>Preprint ANL/MCS-P1033-0303</report-no><acm-class>C.2.4</acm-class><abstract>  The recent proliferation of Data Grids and the increasingly common practice
of using resources as distributed data stores provide a convenient environment
for communities of researchers to share, replicate, and manage access to copies
of large datasets. This has led to the question of which replica can be
accessed most efficiently. In such environments, fetching data from one of the
several replica locations requires accurate predictions of end-to-end transfer
times. The answer to this question can depend on many factors, including
physical characteristics of the resources and the load behavior on the CPUs,
networks, and storage devices that are part of the end-to-end data path linking
possible sources and sinks. Our approach combines end-to-end application
throughput observations with network and disk load variations and captures
whole-system performance and variations in load patterns. Our predictions
characterize the effect of load variations of several shared devices (network
and disk) on file transfer times. We develop a suite of univariate and
multivariate predictors that can use multiple data sources to improve the
accuracy of the predictions as well as address Data Grid variations
(availability of data and sporadic nature of transfers). We ran a large set of
data transfer experiments using GridFTP and observed performance predictions
within 15% error for our testbed sites, which is quite promising for a
pragmatic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304038</identifier>
 <datestamp>2008-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304038</id><created>2003-04-28</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author></authors><title>How NP got a new definition: a survey of probabilistically checkable
  proofs</title><categories>cs.CC</categories><msc-class>68Q10, 68Q15, 68Q17, 68Q25</msc-class><acm-class>F.1</acm-class><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 3, 637--648</journal-ref><abstract>  We survey a collective achievement of a group of researchers: the PCP
Theorems. They give new definitions of the class \np, and imply that computing
approximate solutions to many \np-hard problems is itself \np-hard. Techniques
developed to prove them have had many other consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304039</identifier>
 <datestamp>2008-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304039</id><created>2003-04-28</created><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author></authors><title>Approximation thresholds for combinatorial optimization problems</title><categories>cs.CC</categories><msc-class>68Q17, 68W25</msc-class><acm-class>F.1</acm-class><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 3, 649--658</journal-ref><abstract>  An NP-hard combinatorial optimization problem $\Pi$ is said to have an {\em
approximation threshold} if there is some $t$ such that the optimal value of
$\Pi$ can be approximated in polynomial time within a ratio of $t$, and it is
NP-hard to approximate it within a ratio better than $t$. We survey some of the
known approximation threshold results, and discuss the pattern that emerges
from the known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304040</identifier>
 <datestamp>2008-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304040</id><created>2003-04-28</created><authors><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author></authors><title>Hardness as randomness: a survey of universal derandomization</title><categories>cs.CC</categories><msc-class>68Q15, 68Q10, 68Q17, 68W20</msc-class><acm-class>F.1</acm-class><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 3, 659--672</journal-ref><abstract>  We survey recent developments in the study of probabilistic complexity
classes. While the evidence seems to support the conjecture that probabilism
can be deterministically simulated with relatively low overhead, i.e., that
$P=BPP$, it also indicates that this may be a difficult question to resolve. In
fact, proving that probabilistic algorithms have non-trivial deterministic
simulations is basically equivalent to proving circuit lower bounds, either in
the algebraic or Boolean models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304041</identifier>
 <datestamp>2008-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304041</id><created>2003-04-28</created><authors><author><keyname>Raz</keyname><forenames>Ran</forenames></author></authors><title>$P \ne NP$, propositional proof complexity, and resolution lower bounds
  for the weak pigeonhole principle</title><categories>cs.CC</categories><msc-class>68Q15, 68Q17, 03F20, 03D15</msc-class><acm-class>F.1, F.4</acm-class><journal-ref>Proceedings of the ICM, Beijing 2002, vol. 3, 685--696</journal-ref><abstract>  Recent results established exponential lower bounds for the length of any
Resolution proof for the weak pigeonhole principle. More formally, it was
proved that any Resolution proof for the weak pigeonhole principle, with $n$
holes and any number of pigeons, is of length $\Omega(2^{n^{\epsilon}})$, (for
a constant $\epsilon = 1/3$). One corollary is that certain propositional
formulations of the statement $P \ne NP$ do not have short Resolution proofs.
After a short introduction to the problem of $P \ne NP$ and to the research
area of propositional proof complexity, I will discuss the above mentioned
lower bounds for the weak pigeonhole principle and the connections to the
hardness of proving $P \ne NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304042</id><created>2003-04-28</created><updated>2003-04-30</updated><authors><author><keyname>Ben-Hur</keyname><forenames>A.</forenames></author><author><keyname>Roitershtein</keyname><forenames>A.</forenames></author><author><keyname>Siegelmann</keyname><forenames>H.</forenames></author></authors><title>On probabilistic analog automata</title><categories>cs.OH</categories><acm-class>F.1.1; F.1.2</acm-class><abstract>  We consider probabilistic automata on a general state space and study their
computational power. The model is based on the concept of language recognition
by probabilistic automata due to Rabin and models of analog computation in a
noisy environment suggested by Maass and Orponen, and Maass and Sontag. Our
main result is a generalization of Rabin's reduction theorem that implies that
under very mild conditions, the computational power of the automaton is limited
to regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304043</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304043</id><created>2003-04-29</created><authors><author><keyname>Weinzierl</keyname><forenames>Stefan</forenames></author></authors><title>gTybalt - a free computer algebra system</title><categories>cs.SC hep-ph</categories><comments>22 pages, 7 figures</comments><acm-class>I.1.3</acm-class><journal-ref>Comput.Phys.Commun.156:180-198,2004</journal-ref><doi>10.1016/S0010-4655(03)00468-5</doi><abstract>  This article documents the free computer algebra system &quot;gTybalt&quot;. The
program is build on top of other packages, among others GiNaC, TeXmacs and
Root. It offers the possibility of interactive symbolic calculations within the
C++ programming language. Mathematical formulae are visualized using TeX fonts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304044</id><created>2003-04-29</created><authors><author><keyname>Vyalyi</keyname><forenames>M. N.</forenames></author></authors><title>Hardness of approximating the weight enumerator of a binary linear code</title><categories>cs.CC</categories><comments>7 pages</comments><acm-class>F.1.3</acm-class><abstract>  We consider the problem of evaluation of the weight enumerator of a binary
linear code. We show that the exact evaluation is hard for polynomial
hierarchy. More exactly, if WE is an oracle answering the solution of the
evaluation problem then P^WE=P^GapP. Also we consider the approximative
evaluation of the weight enumerator. In the case of approximation with additive
accuracy $2^{\alpha n}$, $\alpha$ is constant the problem is hard in the above
sense. We also prove that approximate evaluation at a single point $e^{\pi
i/4}$ is hard for $0&lt;\al&lt;\al_0\approx0.88$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304045</id><created>2003-04-30</created><authors><author><keyname>Severini</keyname><forenames>Simone</forenames><affiliation>U. Bristol</affiliation></author></authors><title>On a composition of digraphs</title><categories>cs.DM cs.AR cs.NI</categories><comments>6 pages, 1 figure. Abstract accepted for the Third Haifa Workshop on
  Interdisciplinary Applications of Graph Theory, Combinatorics and Algorithms,
  May 27-29, 2003, Haifa, Israel</comments><acm-class>Primary 05C20; Secondary 68R10</acm-class><abstract>  Many &quot;good&quot; topologies for interconnection networks are based on line
digraphs of regular digraphs. These digraphs support unitary matrices. We
propose the property &quot;being the digraph of a unitary matrix&quot; as additional
criterion for the design of new interconnection networks. We define a
composition of digraphs, which we call diagonal union. Diagonal union can be
used to construct digraphs of unitary matrices. We remark that digraphs
obtained via diagonal union are state split graphs, as defined in symbolic
dynamics. Finally, we list some potential directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0304046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0304046</id><created>2003-04-30</created><authors><author><keyname>Montangero</keyname><forenames>Carlo</forenames><affiliation>Dipartimento di Informatica, Universita' di Pisa, Italy</affiliation></author><author><keyname>Semini</keyname><forenames>Laura</forenames><affiliation>Dipartimento di Informatica, Universita' di Pisa, Italy</affiliation></author></authors><title>Distributed States Temporal Logic</title><categories>cs.LO</categories><comments>25 pages, uses xypic</comments><acm-class>F.4.1; F.3.1</acm-class><abstract>  We introduce a temporal logic to reason on global applications in an
asynchronous setting. First, we define the Distributed States Logic (DSL), a
modal logic for localities that embeds the local theories of each component
into a theory of the distributed states of the system. We provide the logic
with a sound and complete axiomatization. The contribution is that it is
possible to reason about properties that involve several components, even in
the absence of a global clock. Then, we define the Distributed States Temporal
Logic (DSTL) by introducing temporal operators a' la Unity. We support our
proposal by working out a pair of examples: a simple secure communication
system, and an algorithm for distributed leader election.
 The motivation for this work is that the existing logics for distributed
systems do not have the right expressive power to reason on the systems
behaviour, when the communication is based on asynchronous message passing. On
the other side, asynchronous communication is the most used abstraction when
modelling global applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305001</id><created>2003-05-01</created><authors><author><keyname>Mahanti</keyname><forenames>Ambuj</forenames></author><author><keyname>Ghose</keyname><forenames>Supriyo</forenames></author><author><keyname>Sadhukhan</keyname><forenames>Samir K.</forenames></author></authors><title>A Framework for Searching AND/OR Graphs with Cycles</title><categories>cs.AI</categories><comments>40 pages, 20 figures, 5 tables</comments><acm-class>I.2.8</acm-class><abstract>  Search in cyclic AND/OR graphs was traditionally known to be an unsolved
problem. In the recent past several important studies have been reported in
this domain. In this paper, we have taken a fresh look at the problem. First, a
new and comprehensive theoretical framework for cyclic AND/OR graphs has been
presented, which was found missing in the recent literature. Based on this
framework, two best-first search algorithms, S1 and S2, have been developed. S1
does uninformed search and is a simple modification of the Bottom-up algorithm
by Martelli and Montanari. S2 performs a heuristically guided search and
replicates the modification in Bottom-up's successors, namely HS and AO*. Both
S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles.
We then present a detailed analysis for the correctness and complexity results
of S1 and S2, using the proposed framework. We have observed through
experiments that S1 and S2 output correct results in all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305002</id><created>2003-05-02</created><authors><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Hybrid Rounding Techniques for Knapsack Problems</title><categories>cs.CC cs.DM cs.DS</categories><comments>19 LaTeX pages</comments><report-no>IDSIA-03-02</report-no><acm-class>F.2</acm-class><journal-ref>Discrete Applied Mathematics, 154:4 (2006) 640-649</journal-ref><doi>10.1016/j.dam.2005.08.004</doi><abstract>  We address the classical knapsack problem and a variant in which an upper
bound is imposed on the number of items that can be selected. We show that
appropriate combinations of rounding techniques yield novel and powerful ways
of rounding. As an application of these techniques, we present a linear-storage
Polynomial Time Approximation Scheme (PTAS) and a Fully Polynomial Time
Approximation Scheme (FPTAS) that compute an approximate solution, of any fixed
accuracy, in linear time. This linear complexity bound gives a substantial
improvement of the best previously known polynomial bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305003</id><created>2003-05-05</created><authors><author><keyname>Nylander</keyname><forenames>Stina</forenames></author><author><keyname>Bylund</keyname><forenames>Markus</forenames></author><author><keyname>Waern</keyname><forenames>Annika</forenames></author></authors><title>The Ubiquitous Interactor - Device Independent Access to Mobile Services</title><categories>cs.HC</categories><acm-class>H.5.2</acm-class><abstract>  The Ubiquitous Interactor (UBI) addresses the problems of design and
development that arise around services that need to be accessed from many
different devices. In UBI, the same service can present itself with different
user interfaces on different devices. This is done by separating interaction
between users and services from presentation. The interaction is kept the same
for all devices, and different presentation information is provided for
different devices. This way, tailored user interfaces for many different
devices can be created without multiplying development and maintenance work. In
this paper we describe the system design of UBI, the system implementation, and
two services implemented for the system: a calendar service and a stockbroker
service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305004</id><created>2003-05-06</created><authors><author><keyname>Sriram</keyname><forenames>V.</forenames></author><author><keyname>Reddy</keyname><forenames>B. Ravi Sekar</forenames></author><author><keyname>Sangal</keyname><forenames>R.</forenames></author></authors><title>Approximate Grammar for Information Extraction</title><categories>cs.CL cs.AI</categories><comments>10 pages, 3 figures, 2 tables, Presented at &quot;International Conference
  on Universal Knowledge and Language, Goa'2002&quot;</comments><acm-class>I.2.7</acm-class><journal-ref>Conference on Universal Knowledge and Language, Goa'2002</journal-ref><abstract>  In this paper, we present the concept of Approximate grammar and how it can
be used to extract information from a documemt. As the structure of
informational strings cannot be defined well in a document, we cannot use the
conventional grammar rules to represent the information. Hence, the need arises
to design an approximate grammar that can be used effectively to accomplish the
task of Information extraction. Approximate grammars are a novel step in this
direction. The rules of an approximate grammar can be given by a user or the
machine can learn the rules from an annotated document. We have performed our
experiments in both the above areas and the results have been impressive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305005</id><created>2003-05-09</created><authors><author><keyname>Franceschini</keyname><forenames>Gianni</forenames></author><author><keyname>Geffert</keyname><forenames>Viliam</forenames></author></authors><title>An In-Place Sorting with O(n log n) Comparisons and O(n) Moves</title><categories>cs.DS cs.CC</categories><acm-class>F.2.2</acm-class><abstract>  We present the first in-place algorithm for sorting an array of size n that
performs, in the worst case, at most O(n log n) element comparisons and O(n)
element transports.
  This solves a long-standing open problem, stated explicitly, e.g., in [J.I.
Munro and V. Raman, Sorting with minimum data movement, J. Algorithms, 13,
374-93, 1992], of whether there exists a sorting algorithm that matches the
asymptotic lower bounds on all computational resources simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305006</id><created>2003-05-12</created><authors><author><keyname>Chen</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I.</forenames></author><author><keyname>Yen</keyname><forenames>Hsu-Chun</forenames></author></authors><title>On the Ramsey Numbers for Bipartite Multigraphs</title><categories>cs.DM</categories><comments>10 pages, 3 figures</comments><acm-class>G.2.2</acm-class><abstract>  A coloring of a complete bipartite graph is shuffle-preserved if it is the
case that assigning a color $c$ to edges $(u, v)$ and $(u', v')$ enforces the
same color assignment for edges $(u, v')$ and $(u',v)$. (In words, the induced
subgraph with respect to color $c$ is complete.) In this paper, we investigate
a variant of the Ramsey problem for the class of complete bipartite
multigraphs. (By a multigraph we mean a graph in which multiple edges, but no
loops, are allowed.) Unlike the conventional m-coloring scheme in Ramsey theory
which imposes a constraint (i.e., $m$) on the total number of colors allowed in
a graph, we introduce a relaxed version called m-local coloring which only
requires that, for every vertex $v$, the number of colors associated with $v$'s
incident edges is bounded by $m$. Note that the number of colors found in a
graph under $m$-local coloring may exceed m. We prove that given any $n \times
n$ complete bipartite multigraph $G$, every shuffle-preserved $m$-local
coloring displays a monochromatic copy of $K_{p,p}$ provided that $2(p-1)(m-1)
&lt; n$. Moreover, the above bound is tight when (i) $m=2$, or (ii) $n=2^k$ and
$m=3\cdot 2^{k-2}$ for every integer $k\geq 2$. As for the lower bound of $p$,
we show that the existence of a monochromatic $K_{p,p}$ is not guaranteed if
$p&gt; \lceil \frac{n}{m} \rceil$. Finally, we give a generalization for
$k$-partite graphs and a method applicable to general graphs. Many conclusions
found in $m$-local coloring can be inferred to similar results of $m$-coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305007</id><created>2003-05-13</created><authors><author><keyname>Johnson</keyname><forenames>C. A.</forenames></author></authors><title>Computing only minimal answers in disjunctive deductive databases</title><categories>cs.LO</categories><comments>48 pages</comments><acm-class>F4.1</acm-class><abstract>  A method is presented for computing minimal answers in disjunctive deductive
databases under the disjunctive stable model semantics. Such answers are
constructed by repeatedly extending partial answers. Our method is complete (in
that every minimal answer can be computed) and does not admit redundancy (in
the sense that every partial answer generated can be extended to a minimal
answer), whence no non-minimal answer is generated. For stratified databases,
the method does not (necessarily) require the computation of models of the
database in their entirety. Compilation is proposed as a tool by which problems
relating to computational efficiency and the non-existence of disjunctive
stable models can be overcome. The extension of our method to other semantics
is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305008</id><created>2003-05-13</created><authors><author><keyname>Kim</keyname><forenames>Gene</forenames></author><author><keyname>Kim</keyname><forenames>MyungHo</forenames></author></authors><title>A Representation of Changes of Images and its Application for
  Developmental Biolology</title><categories>cs.CC cs.MS q-bio</categories><comments>10 pages</comments><acm-class>I.1.2; H.1.1;I.5.0</acm-class><abstract>  In this paper, we consider a series of events observed at spaced time
intervals and present a method of representation of the series. To explain an
idea, by dealing with a set of gene expression data, which could be obtained
from developmental biology, the procedures are sketched with comments in some
details. We mean representation by choosing a proper function, which fits well
with observed data of a series, and turning its characteristics into numbers,
which extract the intrinsic properties of fluctuating data. With help of a
machine learning techniques, this method will give a classification of
developmental biological data as well as any varying data during a certain
period and the classification can be applied for diagnosis of a disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305009</id><created>2003-05-13</created><updated>2003-09-08</updated><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>The Threshold for Random k-SAT is 2^k ln2 - O(k)</title><categories>cs.CC cond-mat.stat-mech cs.DM math.PR</categories><comments>Added figures and explained the intuition behind our approach. Made a
  correction following comments of Chris Calabro</comments><acm-class>F.2.2</acm-class><abstract>  Let F be a random k-SAT formula on n variables, formed by selecting uniformly
and independently m = rn out of all possible k-clauses. It is well-known that
if r&gt;2^k ln 2, then the formula F is unsatisfiable with probability that tends
to 1 as n tends to infinity. We prove that there exists a sequence t_k = O(k)
such that if r &lt; 2^k ln 2 - t_k, then the formula F is satisfiable with
probability that tends to 1 as n tends to infinity.
  Our technique yields an explicit lower bound for the random k-SAT threshold
for every k. For k&gt;3 this improves upon all previously known lower bounds. For
example, when k=10 our lower bound is 704.94 while the upper bound is 708.94.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305010</id><created>2003-05-14</created><updated>2003-07-15</updated><authors><author><keyname>Faulstich</keyname><forenames>Lukas C.</forenames></author></authors><title>The NPC Framework for Building Information Dissemination Networks</title><categories>cs.DL cs.NI</categories><comments>Erroneous title in PostScript version fixed, two figures added</comments><acm-class>H.3.4, H.3.7</acm-class><abstract>  Numerous systems for dissemination, retrieval, and archiving of documents
have been developed in the past. Those systems often focus on one of these
aspects and are hard to extend and combine. Typically, the transmission
protocols, query and filtering languages are fixed as well as the interfaces to
other systems. We rather envisage the seamless establishment of networks among
the providers, repositories and consumers of information, supporting
information retrieval and dissemination while being highly interoperable and
extensible.
  We propose a framework with a single event-based mechanism that unifies
document storage, retrieval, and dissemination. This framework offers complete
openness with respect to document and metadata formats, transmission protocols,
and filtering mechanisms. It specifies a high-level building kit, by which
arbitrary processors for document streams can be incorporated to support the
retrieval, transformation, aggregation and disaggregation of documents. Using
the same kit, interfaces for different transmission protocols can be added
easily to enable the communication with various information sources and
information consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305011</id><created>2003-05-15</created><authors><author><keyname>Coppola</keyname><forenames>Paolo</forenames></author><author><keyname>Martini</keyname><forenames>Simone</forenames></author></authors><title>Optimizing Optimal Reduction: A Type Inference Algorithm for Elementary
  Affine Logic</title><categories>cs.LO</categories><acm-class>F.4.1: D.3</acm-class><journal-ref>ACM Transactions on Computational Logic, vol 7 (2006) pp. 219 -
  260.</journal-ref><abstract>  We present a type inference algorithm for lambda-terms in Elementary Affine
Logic using linear constraints. We prove that the algorithm is correct and
complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305012</id><created>2003-05-15</created><updated>2003-07-11</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Mayer-Kress</keyname><forenames>Gottfried</forenames></author><author><keyname>Das</keyname><forenames>Atin</forenames></author><author><keyname>Das</keyname><forenames>Pritha</forenames></author><author><keyname>Marko</keyname><forenames>Matus</forenames></author></authors><title>Time-scales, Meaning, and Availability of Information in a Global Brain</title><categories>cs.AI cs.CY cs.NI</categories><comments>8 pages, 1 figure</comments><acm-class>C.2.m; H.0; K.4.0</acm-class><abstract>  We note the importance of time-scales, meaning, and availability of
information for the emergence of novel information meta-structures at a global
scale. We discuss previous work in this area and develop future perspectives.
We focus on the transmission of scientific articles and the integration of
traditional conferences with their virtual extensions on the Internet, their
time-scales, and availability. We mention the Semantic Web as an effort for
integrating meaningful information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305013</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>On Nonspecific Evidence</title><categories>cs.AI cs.NE</categories><comments>15 pages, 0 figures</comments><report-no>FOA Report B 20112-2.7</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>International Journal of Intelligent Systems 8(6) (1993) 711-725</journal-ref><abstract>  When simultaneously reasoning with evidences about several different events
it is necessary to separate the evidence according to event. These events
should then be handled independently. However, when propositions of evidences
are weakly specified in the sense that it may not be certain to which event
they are referring, this may not be directly possible. In this paper a
criterion for partitioning evidences into subsets representing events is
established. This criterion, derived from the conflict within each subset,
involves minimising a criterion function for the overall conflict of the
partition. An algorithm based on characteristics of the criterion function and
an iterative optimisation among partitionings of evidences is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305014</id><created>2003-05-16</created><authors><author><keyname>Bergsten</keyname><forenames>Ulla</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Dempster's Rule for Evidence Ordered in a Complete Directed Acyclic Graph</title><categories>cs.AI cs.DM</categories><comments>37 pages, 12 figures</comments><report-no>FOA Report B 20114-2.7</report-no><acm-class>G.2.2; I.2.3; I.4.8</acm-class><journal-ref>International Journal of Approximate Reasoning 9(1) (1993) 37-73</journal-ref><abstract>  For the case of evidence ordered in a complete directed acyclic graph this
paper presents a new algorithm with lower computational complexity for
Dempster's rule than that of step-by-step application of Dempster's rule. In
this problem, every original pair of evidences, has a corresponding evidence
against the simultaneous belief in both propositions. In this case, it is
uncertain whether the propositions of any two evidences are in logical
conflict. The original evidences are associated with the vertices and the
additional evidences are associated with the edges. The original evidences are
ordered, i.e., for every pair of evidences it is determinable which of the two
evidences is the earlier one. We are interested in finding the most probable
completely specified path through the graph, where transitions are possible
only from lower- to higher-ranked vertices. The path is here a representation
for a sequence of states, for instance a sequence of snapshots of a physical
object's track. A completely specified path means that the path includes no
other vertices than those stated in the path representation, as opposed to an
incompletely specified path that may also include other vertices than those
stated. In a hierarchical network of all subsets of the frame, i.e., of all
incompletely specified paths, the original and additional evidences support
subsets that are not disjoint, thus it is not possible to prune the network to
a tree. Instead of propagating belief, the new algorithm reasons about the
logical conditions of a completely specified path through the graph. The new
algorithm is O(|THETA| log |THETA|), compared to O(|THETA| ** log |THETA|) of
the classic brute force algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305015</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Finding a Posterior Domain Probability Distribution by Specifying
  Nonspecific Evidence</title><categories>cs.AI cs.NE</categories><comments>23 pages, 1 figure</comments><report-no>FOA-B-95-00077-3.4-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>International Journal of Uncertainty, Fuzziness and
  Knowledge-Based Systems 3(2) (1995) 163-185</journal-ref><abstract>  This article is an extension of the results of two earlier articles. In [J.
Schubert, On nonspecific evidence, Int. J. Intell. Syst. 8 (1993) 711-725] we
established within Dempster-Shafer theory a criterion function called the
metaconflict function. With this criterion we can partition into subsets a set
of several pieces of evidence with propositions that are weakly specified in
the sense that it may be uncertain to which event a proposition is referring.
In a second article [J. Schubert, Specifying nonspecific evidence, in
Cluster-based specification techniques in Dempster-Shafer theory for an
evidential intelligence analysis of multiple target tracks, Ph.D. Thesis,
TRITA-NA-9410, Royal Institute of Technology, Stockholm, 1994, ISBN
91-7170-801-4] we not only found the most plausible subset for each piece of
evidence, we also found the plausibility for every subset that this piece of
evidence belongs to the subset. In this article we aim to find a posterior
probability distribution regarding the number of subsets. We use the idea that
each piece of evidence in a subset supports the existence of that subset to the
degree that this piece of evidence supports anything at all. From this we can
derive a bpa that is concerned with the question of how many subsets we have.
That bpa can then be combined with a given prior domain probability
distribution in order to obtain the sought-after posterior domain distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305016</id><created>2003-05-16</created><updated>2004-04-26</updated><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author></authors><title>The one-round Voronoi game replayed</title><categories>cs.CG cs.GT</categories><comments>14 pages, 6 figures, Latex; revised for journal version, to appear in
  Computational Geometry: Theory and Applications. Extended abstract version
  appeared in Workshop on Algorithms and Data Structures, Springer Lecture
  Notes in Computer Science, vol.2748, 2003, pp. 150-161</comments><acm-class>F.2.2</acm-class><abstract>  We consider the one-round Voronoi game, where player one (``White'', called
``Wilma'') places a set of n points in a rectangular area of aspect ratio r
&lt;=1, followed by the second player (``Black'', called ``Barney''), who places
the same number of points. Each player wins the fraction of the board closest
to one of his points, and the goal is to win more than half of the total area.
This problem has been studied by Cheong et al., who showed that for large
enough $n$ and r=1, Barney has a strategy that guarantees a fraction of 1/2+a,
for some small fixed a.
  We resolve a number of open problems raised by that paper. In particular, we
give a precise characterization of the outcome of the game for optimal play: We
show that Barney has a winning strategy for n&gt;2 and r&gt;sqrt{2}/n, and for n=2
and r&gt;sqrt{3}/2. Wilma wins in all remaining cases, i.e., for n&gt;=3 and
r&lt;=sqrt{2}/n, for n=2 and r&lt;=sqrt{3}/2, and for n=1. We also discuss complexity
aspects of the game on more general boards, by proving that for a polygon with
holes, it is NP-hard to maximize the area Barney can win against a given set of
points by Wilma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305017</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Cluster-based Specification Techniques in Dempster-Shafer Theory</title><categories>cs.AI cs.NE</categories><comments>10 pages, 1 figure</comments><report-no>FOA-B-95-00078-3.4-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>in Symbolic and Quantitative Approaches to Reasoning and
  Uncertainty, C. Froidevaux and J. Kohlas (Eds.), Proceedings of the European
  Conference on Symbolic and Quantitative Approaches to Reasoning and
  Uncertainty (ECSQARU'95), pp. 395-404, Universite' de Fribourg, Switzerland,
  3-5 July 1995, Springer-Verlag (LNAI 946), Berlin, 1995</journal-ref><abstract>  When reasoning with uncertainty there are many situations where evidences are
not only uncertain but their propositions may also be weakly specified in the
sense that it may not be certain to which event a proposition is referring. It
is then crucial not to combine such evidences in the mistaken belief that they
are referring to the same event. This situation would become manageable if the
evidences could be clustered into subsets representing events that should be
handled separately. In an earlier article we established within Dempster-Shafer
theory a criterion function called the metaconflict function. With this
criterion we can partition a set of evidences into subsets. Each subset
representing a separate event. In this article we will not only find the most
plausible subset for each piece of evidence, we will also find the plausibility
for every subset that the evidence belongs to the subset. Also, when the number
of subsets are uncertain we aim to find a posterior probability distribution
regarding the number of subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305018</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Cluster-based Specification Techniques in Dempster-Shafer Theory for an
  Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)</title><categories>cs.AI cs.NE</categories><comments>4 pages, 1 figure</comments><report-no>FOA-B-95-00079-3.4-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>AI Communications 8(2) (1995) 107-110</journal-ref><abstract>  In Intelligence Analysis it is of vital importance to manage uncertainty.
Intelligence data is almost always uncertain and incomplete, making it
necessary to reason and taking decisions under uncertainty. One way to manage
the uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis
contains five results regarding multiple target tracks and intelligence
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305019</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305019</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>On rho in a Decision-Theoretic Apparatus of Dempster-Shafer Theory</title><categories>cs.AI</categories><comments>16 pages, 2 figures</comments><report-no>FOA-B-95-00097-3.4-SE</report-no><acm-class>I.2.3</acm-class><journal-ref>International Journal of Approximate Reasoning 13(3), 185-200,
  1995</journal-ref><abstract>  Thomas M. Strat has developed a decision-theoretic apparatus for
Dempster-Shafer theory (Decision analysis using belief functions, Intern. J.
Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility
intervals are constructed for different choices. The choice with the highest
expected utility is preferable to others. However, to find the preferred choice
when the expected utility interval of one choice is included in that of
another, it is necessary to interpolate a discerning point in the intervals.
This is done by the parameter rho, defined as the probability that the
ambiguity about the utility of every nonsingleton focal element will turn out
as favorable as possible. If there are several different decision makers, we
might sometimes be more interested in having the highest expected utility among
the decision makers rather than only trying to maximize our own expected
utility regardless of choices made by other decision makers. The preference of
each choice is then determined by the probability of yielding the highest
expected utility. This probability is equal to the maximal interval length of
rho under which an alternative is preferred. We must here take into account not
only the choices already made by other decision makers but also the rational
choices we can assume to be made by later decision makers. In Strats apparatus,
an assumption, unwarranted by the evidence at hand, has to be made about the
value of rho. We demonstrate that no such assumption is necessary. It is
sufficient to assume a uniform probability distribution for rho to be able to
discern the most preferable choice. We discuss when this approach is
justifiable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305020</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Specifying nonspecific evidence</title><categories>cs.AI cs.NE</categories><comments>39 pages, 2 figures</comments><report-no>FOA-B-96-00174-3.4-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>International Journal of Intelligent Systems 11(8), 525-563, 1996</journal-ref><abstract>  In an earlier article [J. Schubert, On nonspecific evidence, Int. J. Intell.
Syst. 8(6), 711-725 (1993)] we established within Dempster-Shafer theory a
criterion function called the metaconflict function. With this criterion we can
partition into subsets a set of several pieces of evidence with propositions
that are weakly specified in the sense that it may be uncertain to which event
a proposition is referring. Each subset in the partitioning is representing a
separate event. The metaconflict function was derived as the plausibility that
the partitioning is correct when viewing the conflict in Dempster's rule within
each subset as a newly constructed piece of metalevel evidence with a
proposition giving support against the entire partitioning. In this article we
extend the results of the previous article. We will not only find the most
plausible subset for each piece of evidence as was done in the earlier article.
In addition we will specify each piece of nonspecific evidence, in the sense
that we find to which events the proposition might be referring, by finding the
plausibility for every subset that this piece of evidence belong to the subset.
In doing this we will automatically receive indication that some evidence might
be false. We will then develop a new methodology to exploit these newly
specified pieces of evidence in a subsequent reasoning process. This will
include methods to discount evidence based on their degree of falsity and on
their degree of credibility due to a partial specification of affiliation, as
well as a refined method to infer the event of each subset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305021</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Creating Prototypes for Fast Classification in Dempster-Shafer
  Clustering</title><categories>cs.AI cs.NE</categories><comments>11 pages, 3 figures</comments><report-no>FOA-B-97-00244-505-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>in Qualitative and Quantitative Practical Reasoning, Proceedings
  of the First International Joint Conference on Qualitative and Quantitative
  Practical Reasoning (ECSQARU-FAPR'97), pp. 525-535, Bad Honnef, Germany, 9-12
  June 1997, Springer-Verlag (LNAI 1244), Berlin, 1997</journal-ref><abstract>  We develop a classification method for incoming pieces of evidence in
Dempster-Shafer theory. This methodology is based on previous work with
clustering and specification of originally nonspecific evidence. This
methodology is here put in order for fast classification of future incoming
pieces of evidence by comparing them with prototypes representing the clusters,
instead of making a full clustering of all evidence. This method has a
computational complexity of O(M * N) for each new piece of evidence, where M is
the maximum number of subsets and N is the number of prototypes chosen for each
subset. That is, a computational complexity independent of the total number of
previously arrived pieces of evidence. The parameters M and N are typically
fixed and domain dependent in any application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305022</id><created>2003-05-16</created><authors><author><keyname>Bergsten</keyname><forenames>Ulla</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author><author><keyname>Svensson</keyname><forenames>Per</forenames></author></authors><title>Applying Data Mining and Machine Learning Techniques to Submarine
  Intelligence Analysis</title><categories>cs.AI cs.DB cs.NE</categories><comments>4 pages, 7 figures</comments><report-no>FOA-B-97-00263-505-SE</report-no><acm-class>H.2.8; H.4.2; I.2.3; I.5.3</acm-class><journal-ref>in Proceedings of the Third International Conference on Knowledge
  Discovery and Data Mining (KDD'97), pp. 127-130, Newport Beach, USA, 14-17
  August 1997, The AAAI Press, Menlo Park, 1997</journal-ref><abstract>  We describe how specialized database technology and data analysis methods
were applied by the Swedish defense to help deal with the violation of Swedish
marine territory by foreign submarine intruders during the Eighties and early
Nineties. Among several approaches tried some yielded interesting information,
although most of the key questions remain unanswered. We conclude with a survey
of belief-function- and genetic-algorithm-based methods which were proposed to
support interpretation of intelligence reports and prediction of future
submarine positions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305023</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Fast Dempster-Shafer clustering using a neural network structure</title><categories>cs.AI cs.NE</categories><comments>8 pages, 7 figures</comments><report-no>FOA-B-98-00355-505-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Proceedings of the Seventh International Conference on
  Information Processing and Management of Uncertainty in Knowledge-based
  Systems (IPMU'98), pp. 1438-1445, Universite' de La Sorbonne, Paris, France,
  6-10 July 1998, Editions EDK, Paris, 1998</journal-ref><abstract>  In this paper we study a problem within Dempster-Shafer theory where 2**n - 1
pieces of evidence are clustered by a neural structure into n clusters. The
clustering is done by minimizing a metaconflict function. Previously we
developed a method based on iterative optimization. However, for large scale
problems we need a method with lower computational complexity. The neural
structure was found to be effective and much faster than iterative optimization
for larger problems. While the growth in metaconflict was faster for the neural
structure compared with iterative optimization in medium sized problems, the
metaconflict per cluster and evidence was moderate. The neural structure was
able to find a global minimum over ten runs for problem sizes up to six
clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305024</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>A neural network and iterative optimization hybrid for Dempster-Shafer
  clustering</title><categories>cs.AI cs.NE</categories><comments>8 pages, 10 figures</comments><report-no>FOA-B-98-00383-505-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Proceedings of EuroFusion98 International Conference on Data
  Fusion (EF'98), M. Bedworth, J. O'Brien (Eds.), pp. 29-36, Great Malvern, UK,
  6-7 October 1998</journal-ref><abstract>  In this paper we extend an earlier result within Dempster-Shafer theory
[&quot;Fast Dempster-Shafer Clustering Using a Neural Network Structure,&quot; in Proc.
Seventh Int. Conf. Information Processing and Management of Uncertainty in
Knowledge-Based Systems (IPMU 98)] where a large number of pieces of evidence
are clustered into subsets by a neural network structure. The clustering is
done by minimizing a metaconflict function. Previously we developed a method
based on iterative optimization. While the neural method had a much lower
computation time than iterative optimization its average clustering performance
was not as good. Here, we develop a hybrid of the two methods. We let the
neural structure do the initial clustering in order to achieve a high
computational performance. Its solution is fed as the initial state to the
iterative optimization in order to improve the clustering performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305025</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Simultaneous Dempster-Shafer clustering and gradual determination of
  number of clusters using a neural network structure</title><categories>cs.AI cs.NE</categories><comments>6 pages, 10 figures</comments><report-no>FOA-B-99-00431-505-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Proceedings of the 1999 Information, Decision and Control
  Conference (IDC'99), pp. 401-406, Adelaide, Australia, 8-10 February 1999,
  IEEE, Piscataway, 1999</journal-ref><abstract>  In this paper we extend an earlier result within Dempster-Shafer theory
[&quot;Fast Dempster-Shafer Clustering Using a Neural Network Structure,&quot; in Proc.
Seventh Int. Conf. Information Processing and Management of Uncertainty in
Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were
clustered into a fixed number of clusters using a neural structure. This was
done by minimizing a metaconflict function. We now develop a method for
simultaneous clustering and determination of number of clusters during
iteration in the neural structure. We let the output signals of neurons
represent the degree to which a pieces of evidence belong to a corresponding
cluster. From these we derive a probability distribution regarding the number
of clusters, which gradually during the iteration is transformed into a
determination of number of clusters. This gradual determination is fed back
into the neural structure at each iteration to influence the clustering
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305026</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Fast Dempster-Shafer clustering using a neural network structure</title><categories>cs.AI cs.NE</categories><comments>12 pages, 6 figures</comments><report-no>FOA-B-99-00504-505-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Information, Uncertainty and Fusion, B. Bouchon-Meunier, R.R.
  Yager, L.A. Zadeh (Eds.), pp. 419-430, Kluwer Academic Publishers, Boston,
  1999</journal-ref><abstract>  In this article we study a problem within Dempster-Shafer theory where 2**n -
1 pieces of evidence are clustered by a neural structure into n clusters. The
clustering is done by minimizing a metaconflict function. Previously we
developed a method based on iterative optimization. However, for large scale
problems we need a method with lower computational complexity. The neural
structure was found to be effective and much faster than iterative optimization
for larger problems. While the growth in metaconflict was faster for the neural
structure compared with iterative optimization in medium sized problems, the
metaconflict per cluster and evidence was moderate. The neural structure was
able to find a global minimum over ten runs for problem sizes up to six
clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305027</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Managing Inconsistent Intelligence</title><categories>cs.AI cs.NE</categories><comments>7 pages, 3 figures</comments><report-no>FOA-B-00-00619-505-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Proceedings of the Third International Conference on
  Information Fusion (FUSION 2000), pp. TuB4/10-16, Paris, France, 10-13 July
  2000, International Society of Information Fusion, Sunnyvale, 2000</journal-ref><abstract>  In this paper we demonstrate that it is possible to manage intelligence in
constant time as a pre-process to information fusion through a series of
processes dealing with issues such as clustering reports, ranking reports with
respect to importance, extraction of prototypes from clusters and immediate
classification of newly arriving intelligence reports. These methods are used
when intelligence reports arrive which concerns different events which should
be handled independently, when it is not known a priori to which event each
intelligence report is related. We use clustering that runs as a back-end
process to partition the intelligence into subsets representing the events, and
in parallel, a fast classification that runs as a front-end process in order to
put the newly arriving intelligence into its correct information fusion
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305028</id><created>2003-05-16</created><authors><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Dempster-Shafer clustering using Potts spin mean field theory</title><categories>cs.AI cs.NE</categories><comments>14 pages, 9 figures</comments><report-no>FOI-S-0027-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>Soft Computing 5(3) (2001) 215-228</journal-ref><abstract>  In this article we investigate a problem within Dempster-Shafer theory where
2**q - 1 pieces of evidence are clustered into q clusters by minimizing a
metaconflict function, or equivalently, by minimizing the sum of weight of
conflict over all clusters. Previously one of us developed a method based on a
Hopfield and Tank model. However, for very large problems we need a method with
lower computational complexity. We demonstrate that the weight of conflict of
evidence can, as an approximation, be linearized and mapped to an
antiferromagnetic Potts Spin model. This facilitates efficient numerical
solution, even for large problem sizes. Optimal or nearly optimal solutions are
found for Dempster-Shafer clustering benchmark tests with a time complexity of
approximately O(N**2 log**2 N). Furthermore, an isomorphism between the
antiferromagnetic Potts spin model and a graph optimization problem is shown.
The graph model has dynamic variables living on the links, which have a priori
probabilities that are directly related to the pairwise conflict between pieces
of evidence. Hence, the relations between three different models are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305029</id><created>2003-05-16</created><authors><author><keyname>Cantwell</keyname><forenames>John</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author><author><keyname>Walter</keyname><forenames>Johan</forenames></author></authors><title>Conflict-based Force Aggregation</title><categories>cs.AI cs.NE</categories><comments>15 pages, 17 figures</comments><report-no>FOI-S-0040-SE</report-no><acm-class>H.4.2; I.2.3; I.2.6; I.5.3; J.7</acm-class><journal-ref>in Cd Proceedings of the Sixth International Command and Control
  Research and Technology Symposium, Track 7, Paper 031, pp. 1-15, Annapolis,
  USA, 19-21 June 2001, US Department of Defence CCRP, Washington, DC, 2001</journal-ref><abstract>  In this paper we present an application where we put together two methods for
clustering and classification into a force aggregation method. Both methods are
based on conflicts between elements. These methods work with different type of
elements (intelligence reports, vehicles, military units) on different
hierarchical levels using specific conflict assessment methods on each level.
We use Dempster-Shafer theory for conflict calculation between elements,
Dempster-Shafer clustering for clustering these elements, and templates for
classification. The result of these processes is a complete force aggregation
on all levels handled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305030</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Reliable Force Aggregation Using a Refined Evidence Specification from
  Dempster-Shafer Clustering</title><categories>cs.AI cs.NE</categories><comments>8 pages, 5 figures</comments><report-no>FOI-S-0050-SE</report-no><acm-class>I.2.3; I.2.6; I.5.3</acm-class><journal-ref>in Proceedings of the Fourth Annual Conference on Information
  Fusion (FUSION 2001), pp. TuB3/15-22, Montreal, Canada, 7-10 August 2001,
  International Society of Information Fusion, Sunnyvale, 2001</journal-ref><abstract>  In this paper we develop methods for selection of templates and use these
templates to recluster an already performed Dempster-Shafer clustering taking
into account intelligence to template fit during the reclustering phase. By
this process the risk of erroneous force aggregation based on some misplace
pieces of evidence from the first clustering process is greatly reduced.
Finally, a more reliable force aggregation is performed using the result of the
second clustering. These steps are taken in order to maintain most of the
excellent computational performance of Dempster-Shafer clustering, while at the
same time improve on the clustering result by including some higher relations
among intelligence reports described by the templates. The new improved
algorithm has a computational complexity of O(n**3 log**2 n) compared to O(n**2
log**2 n) of standard Dempster-Shafer clustering using Potts spin mean field
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305031</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Clustering belief functions based on attracting and conflicting
  metalevel evidence</title><categories>cs.AI cs.NE</categories><comments>8 pages, 3 figures</comments><report-no>FOI-S-0524-SE</report-no><acm-class>I.2.3; I.5.3</acm-class><journal-ref>in Proceedings of the Ninth International Conference on
  Information Processing and Management of Uncertainty in Knowledge-based
  Systems (IPMU'02), pp. 571-578, Annecy, France, 1-5 July 2002</journal-ref><abstract>  In this paper we develop a method for clustering belief functions based on
attracting and conflicting metalevel evidence. Such clustering is done when the
belief functions concern multiple events, and all belief functions are mixed
up. The clustering process is used as the means for separating the belief
functions into subsets that should be handled independently. While the
conflicting metalevel evidence is generated internally from pairwise conflicts
of all belief functions, the attracting metalevel evidence is assumed given by
some external source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305032</id><created>2003-05-16</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Robust Report Level Cluster-to-Track Fusion</title><categories>cs.AI cs.NE</categories><comments>6 pages, 5 figures</comments><report-no>FOI-S-0525-SE</report-no><acm-class>I.2.3; I.2.6; I.4.8; I.5.3</acm-class><journal-ref>in Proceedings of the Fifth International Conference on
  Information Fusion (FUSION 2002), pp. 913-918, Annapolis, USA, 8-11 July
  2002, International Society of Information Fusion, 2002</journal-ref><abstract>  In this paper we develop a method for report level tracking based on
Dempster-Shafer clustering using Potts spin neural networks where clusters of
incoming reports are gradually fused into existing tracks, one cluster for each
track. Incoming reports are put into a cluster and continuous reclustering of
older reports is made in order to obtain maximum association fit within the
cluster and towards the track. Over time, the oldest reports of the cluster
leave the cluster for the fixed track at the same rate as new incoming reports
are put into it. Fusing reports to existing tracks in this fashion allows us to
take account of both existing tracks and the probable future of each track, as
represented by younger reports within the corresponding cluster. This gives us
a robust report-to-track association. Compared to clustering of all available
reports this approach is computationally faster and has a better
report-to-track association than simple step-by-step association.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305033</id><created>2003-05-16</created><authors><author><keyname>Bergsten</keyname><forenames>Ulla</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author><author><keyname>Svensson</keyname><forenames>Per</forenames></author></authors><title>Beslutst\&quot;odssystemet Dezzy - en \&quot;oversikt</title><categories>cs.AI cs.DB</categories><comments>18 pages, 8 figures, in Swedish, with appendix in English</comments><report-no>FOA Report B 20078-2.7</report-no><acm-class>H.4.2; I.2.3</acm-class><journal-ref>in Dokumentation 7 juni av Seminarium och fackutst\&quot;allning om
  samband, sensorer och datorer f\&quot;or ledningssystem till f\&quot;orsvaret
  (MILINF'89), pp. 07B2:19-31, Enk\&quot;oping, June 1989, Telub AB, V\&quot;axj\&quot;o, 1989</journal-ref><abstract>  Within the scope of the three-year ANTI-SUBMARINE WARFARE project of the
National Defence Research Establishment, the INFORMATION SYSTEMS subproject has
developed the demonstration prototype Dezzy for handling and analysis of
intelligence reports concerning foreign underwater activities.
  -----
  Inom ramen f\&quot;or FOA:s tre{\aa}riga huvudprojekt UB{\AA}TSSKYDD har
delprojekt INFORMATIONSSYSTEM utvecklat demonstrationsprototypen Dezzy till ett
beslutsst\&quot;odsystem f\&quot;or hantering och analys av underr\&quot;attelser om
fr\&quot;ammande undervattensverksamhet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305034</id><created>2003-05-17</created><updated>2003-06-26</updated><authors><author><keyname>Toli</keyname><forenames>Ilia</forenames></author></authors><title>Cryptanalysis of HFE</title><categories>cs.CR cs.SC</categories><comments>7 pages. Minor changes expected</comments><acm-class>E.3</acm-class><abstract>  I transform the trapdoor problem of HFE into a linear algebra problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305035</identifier>
 <datestamp>2012-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305035</id><created>2003-05-19</created><updated>2012-01-15</updated><authors><author><keyname>Feinstein</keyname><forenames>Craig Alan</forenames></author></authors><title>The Computational Complexity of Computing the Permanent of a Matrix</title><categories>cs.CC</categories><comments>1 page</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that there is no deterministic and exact algorithm that
computes the permanent of a matrix in polynomial-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305036</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305036</id><created>2003-05-19</created><updated>2011-08-28</updated><authors><author><keyname>Filla</keyname><forenames>Reno</forenames><affiliation>Volvo Wheel Loaders AB</affiliation></author><author><keyname>Palmberg</keyname><forenames>Jan-Ove</forenames><affiliation>Linkoping University</affiliation></author></authors><title>Using Dynamic Simulation in the Development of Construction Machinery</title><categories>cs.CE</categories><comments>17 pages, 5 figures, SICFP'03 conference</comments><acm-class>I.6.3; I.6.5; J.6</acm-class><journal-ref>The Eighth Scandinavian International Conference on Fluid Power
  2003, vol. 1, pp. 651-667</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As in the car industry for quite some time, dynamic simulation of complete
vehicles is being practiced more and more in the development of off-road
machinery. However, specific questions arise due not only to company structure
and size, but especially to the type of product. Tightly coupled, non-linear
subsystems of different domains make prediction and optimisation of the
complete system's dynamic behaviour a challenge. Furthermore, the demand for
versatile machines leads to sometimes contradictory target requirements and can
turn the design process into a hunt for the least painful compromise. This can
be avoided by profound system knowledge, assisted by simulation-driven product
development. This paper gives an overview of joint research into this issue by
Volvo Wheel Loaders and Linkoping University on that matter, lists the results
of a related literature review and introduces the term &quot;operateability&quot;. Rather
than giving detailed answers, the problem space for ongoing and future research
is examined and possible solutions are sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305037</id><created>2003-05-20</created><updated>2003-07-17</updated><authors><author><keyname>Wheeldon</keyname><forenames>Richard</forenames></author><author><keyname>Counsell</keyname><forenames>Steve</forenames></author></authors><title>Power Law Distributions in Class Relationships</title><categories>cs.SE</categories><comments>To appear in proceedings of Third IEEE International Workshop on
  Source Code Analysis and Manipulation. 10 pages, 10 figures. Minor errors
  fixed</comments><acm-class>D.1.5;D.2.8;D.3.3;K.6.2</acm-class><abstract>  Power law distributions have been found in many natural and social phenomena,
and more recently in the source code and run-time characteristics of
Object-Oriented (OO) systems. A power law implies that small values are
extremely common, whereas large values are extremely rare. In this paper, we
identify twelve new power laws relating to the static graph structures of Java
programs. The graph structures analyzed represented different forms of OO
coupling, namely, inheritance, aggregation, interface, parameter type and
return type. Identification of these new laws provide the basis for predicting
likely features of classes in future developments. The research in this paper
ties together work in object-based coupling and World Wide Web structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305038</id><created>2003-05-21</created><authors><author><keyname>Bercich</keyname><forenames>Nancy Hartline</forenames></author></authors><title>The Evolution of the Computerized Database</title><categories>cs.DB</categories><comments>13 pages, including figures and bibliography</comments><acm-class>H.2.m</acm-class><abstract>  Databases, collections of related data, are as old as the written word. A
database can be anything from a homemaker's metal recipe file to a
sophisticated data warehouse. Yet today, when we think of a database we
invariably think of computerized data and their DBMSs (database management
systems). How did we go from organizing our data in a simple metal filing box
or cabinet to storing our data in a sophisticated computerized database? How
did the computerized database evolve?
  This paper defines what we mean by a database. It traces the evolution of the
database, from its start as a non-computerized set of related data, to the, now
standard, computerized RDBMS (relational database management system). Early
computerized storage methods are reviewed including both the ISAM (Indexed
Sequential Access Method) and VSAM (Virtual Storage Access Method) storage
methods. Early database models are explored including the network and
hierarchical database models. Eventually, the relational, object-relational and
object-oriented databases models are discussed. An appendix of diagrams,
including hierarchical occurrence tree, network schema, ER (entity
relationship) and UML (unified modeling language) diagrams, is included to
support the text.
  This paper concludes with an exploration of current and future trends in DBMS
development. It discusses the factors affecting these trends. It delves into
the relationship between DBMSs and the increasingly popular object-oriented
development methodologies. Finally, it speculates on the future of the DBMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305039</id><created>2003-05-23</created><updated>2003-05-25</updated><authors><author><keyname>Harju</keyname><forenames>Tero</forenames></author><author><keyname>Nowotka</keyname><forenames>Dirk</forenames></author></authors><title>Periodicity and Unbordered Words: A Proof of the Extended Duval
  Conjecture</title><categories>cs.DM</categories><comments>16 pages</comments><acm-class>F.4.m</acm-class><abstract>  The relationship between the length of a word and the maximum length of its
unbordered factors is investigated in this paper. Consider a finite word w of
length n. We call a word bordered, if it has a proper prefix which is also a
suffix of that word. Let f(w) denote the maximum length of all unbordered
factors of w, and let p(w) denote the period of w. Clearly, f(w) &lt; p(w)+1.
  We establish that f(w) = p(w), if w has an unbordered prefix of length f(w)
and n &gt; 2f(w)-2. This bound is tight and solves the stronger version of a 21
years old conjecture by Duval. It follows from this result that, in general, n
&gt; 3f(w)-3 implies f(w) = p(w) which gives an improved bound for the question
asked by Ehrenfeucht and Silberger in 1979.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305040</id><created>2003-05-23</created><authors><author><keyname>Heljanko</keyname><forenames>Keijo</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Bounded LTL Model Checking with Stable Models</title><categories>cs.LO cs.AI</categories><comments>32 pages, to appear in Theory and Practice of Logic Programming</comments><acm-class>D.2.4;D.2.2;F.4.1;I.2.4;F.3.1</acm-class><abstract>  In this paper bounded model checking of asynchronous concurrent systems is
introduced as a promising application area for answer set programming. As the
model of asynchronous systems a generalisation of communicating automata,
1-safe Petri nets, are used. It is shown how a 1-safe Petri net and a
requirement on the behaviour of the net can be translated into a logic program
such that the bounded model checking problem for the net can be solved by
computing stable models of the corresponding program. The use of the stable
model semantics leads to compact encodings of bounded reachability and deadlock
detection tasks as well as the more general problem of bounded model checking
of linear temporal logic. Correctness proofs of the devised translations are
given, and some experimental results using the translation and the Smodels
system are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305041</id><created>2003-05-23</created><updated>2003-05-26</updated><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Factorization of Language Models through Backing-Off Lattices</title><categories>cs.CL</categories><comments>7 pages and 1 figure; technical report; added one more related work,
  and removed some errors</comments><acm-class>I.2.7</acm-class><abstract>  Factorization of statistical language models is the task that we resolve the
most discriminative model into factored models and determine a new model by
combining them so as to provide better estimate. Most of previous works mainly
focus on factorizing models of sequential events, each of which allows only one
factorization manner. To enable parallel factorization, which allows a model
event to be resolved in more than one ways at the same time, we propose a
general framework, where we adopt a backing-off lattice to reflect parallel
factorizations and to define the paths along which a model is resolved into
factored models, we use a mixture model to combine parallel paths in the
lattice, and generalize Katz's backing-off method to integrate all the mixture
models got by traversing the entire lattice. Based on this framework, we
formulate two types of model factorizations that are used in natural language
modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305042</id><created>2003-05-23</created><authors><author><keyname>Jakobsson</keyname><forenames>Markus</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Untraceable Email Cluster Bombs: On Agent-Based Distributed Denial of
  Service</title><categories>cs.CY cs.NI</categories><acm-class>K.4.1; K.4.4; C.2.0; H.4.3; H.3.5; I.2.11</acm-class><abstract>  We uncover a vulnerability that allows for an attacker to perform an
email-based attack on selected victims, using only standard scripts and agents.
What differentiates the attack we describe from other, already known forms of
distributed denial of service (DDoS) attacks is that an attacker does not need
to infiltrate the network in any manner -- as is normally required to launch a
DDoS attack. Thus, we see this type of attack as a poor man's DDoS. Not only is
the attack easy to mount, but it is also almost impossible to trace back to the
perpetrator. Along with descriptions of our attack, we demonstrate its
destructive potential with (limited and contained) experimental results. We
illustrate the potential impact of our attack by describing how an attacker can
disable an email account by flooding its inbox; block competition during
on-line auctions; harm competitors with an on-line presence; disrupt phone
service to a given victim; cheat in SMS-based games; disconnect mobile
corporate leaders from their networks; and disrupt electronic elections.
Finally, we propose a set of countermeasures that are light-weight, do not
require modifications to the infrastructure, and can be deployed in a gradual
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305043</identifier>
 <datestamp>2009-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305043</id><created>2003-05-24</created><authors><author><keyname>Gornev</keyname><forenames>Serge</forenames></author></authors><title>Modeling of aerodynamic Space-to-Surface flight with optimal trajectory
  for targeting</title><categories>cs.OH</categories><comments>5 pages</comments><acm-class>I.6.3; I.6.5</acm-class><abstract>  Modeling has been created for a Space-to-Surface system defined for an
optimal trajectory for targeting in terminal phase. The modeling includes
models for simulation atmosphere, speed of sound, aerodynamic flight and
navigation by an infrared system. The modeling simulation includes statistical
analysis of the modeling results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305044</id><created>2003-05-27</created><updated>2004-05-17</updated><authors><author><keyname>de Cooman</keyname><forenames>Gert</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>Updating beliefs with incomplete observations</title><categories>cs.AI</categories><comments>Replaced with extended version</comments><acm-class>I.2.3</acm-class><abstract>  Currently, there is renewed interest in the problem, raised by Shafer in
1985, of updating probabilities when observations are incomplete. This is a
fundamental problem in general, and of particular interest for Bayesian
networks. Recently, Grunwald and Halpern have shown that commonly used updating
strategies fail in this case, except under very special assumptions. In this
paper we propose a new method for updating probabilities with incomplete
observations. Our approach is deliberately conservative: we make no assumptions
about the so-called incompleteness mechanism that associates complete with
incomplete observations. We model our ignorance about this mechanism by a
vacuous lower prevision, a tool from the theory of imprecise probabilities, and
we use only coherence arguments to turn prior into posterior probabilities. In
general, this new approach to updating produces lower and upper posterior
probabilities and expectations, as well as partially determinate decisions.
This is a logical consequence of the existing ignorance about the
incompleteness mechanism. We apply the new approach to the problem of
classification of new evidence in probabilistic expert systems, where it leads
to a new, so-called conservative updating rule. In the special case of Bayesian
networks constructed using expert knowledge, we provide an exact algorithm for
classification based on our updating rule, which has linear-time complexity for
a class of networks wider than polytrees. This result is then extended to the
more general framework of credal networks, where computations are often much
harder than with Bayesian nets. Using an example, we show that our rule appears
to provide a solid basis for reliable updating with incomplete observations,
when no strong assumptions about the incompleteness mechanism are justified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305045</id><created>2003-05-27</created><updated>2004-02-23</updated><authors><author><keyname>Gokden</keyname><forenames>Burc</forenames></author></authors><title>Semiclassical Quantum Computation Solutions to the Count to Infinity
  Problem: A Brief Discussion</title><categories>cs.NI cs.GL cs.OH</categories><comments>5 pages, revtex, draft version, a quick and brief discussion,
  comments and criticisms are welcome; writing errors corrected</comments><acm-class>C.2.2; C.2.1</acm-class><abstract>  In this paper we briefly define distance vector routing algorithms, their
advantages and possible drawbacks. On these possible drawbacks, currently
widely used methods split horizon and poisoned reverse are defined and
compared. The count to infinity problem is specified and it is classified to be
a halting problem and a proposition stating that entangled states used in
quantum computation can be used to handle this problem is examined. Several
solutions to this problem by using entangled states are proposed and a very
brief introduction to entangled states is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305046</id><created>2003-05-27</created><authors><author><keyname>Osorio</keyname><forenames>Mauricio</forenames></author><author><keyname>Navarro</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Arrazola</keyname><forenames>Jose</forenames></author></authors><title>Applications of Intuitionistic Logic in Answer Set Programming</title><categories>cs.LO</categories><comments>30 pages, Under consideration for publication in Theory and Practice
  of Logic Programming</comments><acm-class>F.4.1</acm-class><abstract>  We present some applications of intermediate logics in the field of Answer
Set Programming (ASP). A brief, but comprehensive introduction to the answer
set semantics, intuitionistic and other intermediate logics is given. Some
equivalence notions and their applications are discussed. Some results on
intermediate logics are shown, and applied later to prove properties of answer
sets. A characterization of answer sets for logic programs with nested
expressions is provided in terms of intuitionistic provability, generalizing a
recent result given by Pearce.
  It is known that the answer set semantics for logic programs with nested
expressions may select non-minimal models. Minimal models can be very important
in some applications, therefore we studied them; in particular we obtain a
characterization, in terms of intuitionistic logic, of answer sets which are
also minimal models. We show that the logic G3 characterizes the notion of
strong equivalence between programs under the semantic induced by these models.
Finally we discuss possible applications and consequences of our results. They
clearly state interesting links between ASP and intermediate logics, which
might bring research in these two areas together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305047</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305047</id><created>2003-05-28</created><authors><author><keyname>Baud</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Couturier</keyname><forenames>Ben</forenames></author><author><keyname>Curran</keyname><forenames>Charles</forenames></author><author><keyname>Durand</keyname><forenames>Jean-Damien</forenames></author><author><keyname>Knezo</keyname><forenames>Emil</forenames></author><author><keyname>Occhetti</keyname><forenames>Stefano</forenames></author><author><keyname>Barring</keyname><forenames>Olof</forenames></author></authors><title>CASTOR status and evolution</title><categories>cs.OH</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 2 pages, PDF. PSN TUDT007</comments><acm-class>D.4.2</acm-class><journal-ref>ECONFC0303241:TUDT007,2003</journal-ref><abstract>  In January 1999, CERN began to develop CASTOR (&quot;CERN Advanced STORage
manager&quot;). This Hierarchical Storage Manager targetted at HEP applications has
been in full production at CERN since May 2001. It now contains more than two
Petabyte of data in roughly 9 million files. In 2002, 350 Terabytes of data
were stored for COMPASS at 45 MB/s and a Data Challenge was run for ALICE in
preparation for the LHC startup in 2007 and sustained a data transfer to tape
of 300 MB/s for one week (180 TB). The major functionality improvements were
the support for files larger than 2 GB (in collaboration with IN2P3) and the
development of Grid interfaces to CASTOR: GridFTP and SRM (&quot;Storage Resource
Manager&quot;). An ongoing effort is taking place to copy the existing data from
obsolete media like 9940 A to better cost effective offerings. CASTOR has also
been deployed at several HEP sites with little effort. In 2003, we plan to
continue working on Grid interfaces and to improve performance not only for
Central Data Recording but also for Data Analysis applications where thousands
of processes possibly access the same hot data. This could imply the selection
of another filesystem or the use of replication (hardware or software).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305048</id><created>2003-05-28</created><authors><author><keyname>Kim</keyname><forenames>Gene</forenames></author><author><keyname>Kim</keyname><forenames>MyungHo</forenames></author></authors><title>2D Electrophoresis Gel Image and Diagnosis of a Disease</title><categories>cs.CC cs.CV q-bio.QM</categories><comments>10 pages, DIMACS workshop of Rutgers University, on complexity in
  Biosystems</comments><acm-class>I.5; J.3; I.4.1; I.4.3</acm-class><abstract>  The process of diagnosing a disease from the 2D gel electrophoresis image is
a challenging problem. This is due to technical difficulties of generating
reproducible images with a normalized form and the effect of negative stain. In
this paper, we will discuss a new concept of interpreting the 2D images and
overcoming the aforementioned technical difficulties using mathematical
transformation. The method makes use of 2D gel images of proteins in serums and
we explain a way of representing the images into vectors in order to apply
machine-learning methods, such as the support vector machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305049</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305049</id><created>2003-05-28</created><authors><author><keyname>Bazan</keyname><forenames>Alain</forenames></author><author><keyname>Bouedo</keyname><forenames>Thierry</forenames></author><author><keyname>Ghez</keyname><forenames>Philippe</forenames></author><author><keyname>Marino</keyname><forenames>Massimo</forenames></author><author><keyname>Tull</keyname><forenames>Craig</forenames></author></authors><title>The Athena Data Dictionary and Description Language</title><categories>cs.SE</categories><comments>4 pages, 2 figures</comments><acm-class>D.2.2</acm-class><journal-ref>ECONFC0303241:MOJT010,2003</journal-ref><abstract>  Athena is the ATLAS off-line software framework, based upon the GAUDI
architecture from LHCb. As part of ATLAS' continuing efforts to enhance and
customise the architecture to meet our needs, we have developed a data object
description tool suite and service for Athena. The aim is to provide a set of
tools to describe, manage, integrate and use the Event Data Model at a design
level according to the concepts of the Athena framework (use of patterns,
relationships, ...). Moreover, to ensure stability and reusability this must be
fully independent from the implementation details. After an extensive
investigation into the many options, we have developed a language grammar based
upon a description language (IDL, ODL) to provide support for object
integration in Athena. We have then developed a compiler front end based upon
this language grammar, JavaCC, and a Java Reflection API-like interface. We
have then used these tools to develop several compiler back ends which meet
specific needs in ATLAS such as automatic generation of object converters, and
data object scripting interfaces. We present here details of our work and
experience to date on the Athena Definition Language and Athena Data
Dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305050</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305050</id><created>2003-05-28</created><updated>2003-05-30</updated><authors><author><keyname>Barring</keyname><forenames>Olof</forenames></author></authors><title>Towards automation of computing fabrics using tools from the fabric
  management workpackage of the EU DataGrid project</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages. PSN MODT004</comments><acm-class>K.6.0; K.6.2; K.6.4</acm-class><journal-ref>ECONFC0303241:MODT004,2003</journal-ref><abstract>  The EU DataGrid project workpackage 4 has as an objective to provide the
necessary tools for automating the management of medium size to very large
computing fabrics. At the end of the second project year subsystems for
centralized configuration management (presented at LISA'02) and
performance/exception monitoring have been delivered. This will soon be
augmented with a subsystem for node installation and service configuration,
which is based on existing widely used standards where available (e.g. rpm,
kickstart, init.d scripts) and clean interfaces to OS dependent components
(e.g. base installation and service management). The three subsystems together
allow for centralized management of very large computer farms. Finally, a fault
tolerance system is being developed for tying together the above subsystems to
form a complete framework for automated enterprise computing management by
3Q03. All software developed is open source covered by the EU DataGrid project
license agreements. This article describes the architecture behind the designed
fabric management system and the status of the different developments. It also
covers the experience with an existing tool for automated configuration and
installation that have been adapted and used from the beginning to manage the
EU DataGrid testbed, which is now used for LHC data challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305051</id><created>2003-05-28</created><authors><author><keyname>Berger-Wolf</keyname><forenames>Tanya Y.</forenames></author><author><keyname>Harris</keyname><forenames>Mitchell A.</forenames></author></authors><title>Sharp Bounds for Bandwidth of Clique Products</title><categories>cs.DM</categories><comments>15 pages, 13 figures. Presented at the SIAM Discrete Math 2002
  conference</comments><acm-class>G.2.2</acm-class><abstract>  The bandwidth of a graph is the labeling of vertices with minimum maximum
edge difference. For many graph families this is NP-complete. A classic result
computes the bandwidth for the hypercube. We generalize this result to give
sharp lower bounds for products of cliques. This problem turns out to be
equivalent to one in communication over multiple channels in which channels can
fail and the information sent over those channels is lost. The goal is to
create an encoding that minimizes the difference between the received and the
original information while having as little redundancy as possible. Berger-Wolf
and Reingold [2] have considered the problem for the equal size cliques (or
equal capacity channels). This paper presents a tight lower bound and an
algorithm for constructing the labeling for the product of any number of
arbitrary size cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305052</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305052</id><created>2003-05-29</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>On the Existence and Convergence Computable Universal Priors</title><categories>cs.LG cs.AI cs.CC math.ST stat.TH</categories><comments>13 pages</comments><report-no>IDSIA-05-03</report-no><acm-class>G.3; I.2</acm-class><journal-ref>Proceedings of the 14th International Conference on Algorithmic
  Learning Theory (ALT-2003) 298-312</journal-ref><abstract>  Solomonoff unified Occam's razor and Epicurus' principle of multiple
explanations to one elegant, formal, universal theory of inductive inference,
which initiated the field of algorithmic information theory. His central result
is that the posterior of his universal semimeasure M converges rapidly to the
true sequence generating posterior mu, if the latter is computable. Hence, M is
eligible as a universal predictor in case of unknown mu. We investigate the
existence and convergence of computable universal (semi)measures for a
hierarchy of computability classes: finitely computable, estimable, enumerable,
and approximable. For instance, M is known to be enumerable, but not finitely
computable, and to dominate all enumerable semimeasures. We define seven
classes of (semi)measures based on these four computability concepts. Each
class may or may not contain a (semi)measure which dominates all elements of
another class. The analysis of these 49 cases can be reduced to four basic
cases, two of them being new. The results hold for discrete and continuous
semimeasures. We also investigate more closely the types of convergence,
possibly implied by universality: in difference and in ratio, with probability
1, in mean sum, and for Martin-Loef random sequences. We introduce a
generalized concept of randomness for individual sequences and use it to
exhibit difficulties regarding these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305053</id><created>2003-05-29</created><authors><author><keyname>Hughes</keyname><forenames>Baden</forenames></author></authors><title>Developing Open Data Models for Linguistic Field Data</title><categories>cs.DL cs.CL</categories><acm-class>H.3.7; H.2.4; H.2.1; E.2; D.2.11</acm-class><abstract>  The UQ Flint Archive houses the field notes and elicitation recordings made
by Elwyn Flint in the 1950's and 1960's during extensive linguistic survey work
across Queensland, Australia.
  The process of digitizing the contents of the UQ Flint Archive provides a
number of interesting challenges in the context of EMELD. Firstly, all of the
linguistic data is for languages which are either endangered or extinct, and as
such forms a valuable ethnographic repository. Secondly, the physical format of
the data is itself in danger of decline, and as such digitization is an
important preservation task in the short to medium term. Thirdly, the adoption
of open standards for the encoding and presentation of text and audio data for
linguistic field data, whilst enabling preservation, represents a new field of
research in itself where best practice has yet to be formalised. Fourthly, the
provision of this linguistic data online as a new data source for future
research introduces concerns of data portability and longevity.
  This paper will outline the origins of the data model, the content creation
components, presentation forms based on the data model, data capture tools and
media conversion components. It will also address some of the larger questions
regarding the digitization and annotation of linguistic field work based on
experience gained through work with the Flint Archive contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305054</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305054</id><created>2003-05-29</created><authors><author><keyname>Marzolla</keyname><forenames>M.</forenames></author><author><keyname>Melloni</keyname><forenames>V.</forenames></author></authors><title>A Monitoring System for the BaBar INFN Computing Cluster</title><categories>cs.PF</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, LaTeX, 4 eps figures. PSN
  MOET006</comments><acm-class>B.8.2; C.2.3</acm-class><journal-ref>ECONFC0303241:MOET006,2003</journal-ref><abstract>  Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305055</id><created>2003-05-29</created><authors><author><keyname>Daniel</keyname><forenames>Gilles</forenames></author></authors><title>Goodness-of-fit of the Heston model</title><categories>cs.CE</categories><comments>10 pages, 3 figures, The 9th International Conference of Computing in
  Economics and Finance, Seattle (July 2003)</comments><acm-class>G3</acm-class><abstract>  An analytical formula for the probability distribution of stock-market
returns, derived from the Heston model assuming a mean-reverting stochastic
volatility, was recently proposed by Dragulescu and Yakovenko in Quantitative
Finance 2002. While replicating their results, we found two significant
weaknesses in their method to pre-process the data, which cast a shadow over
the effective goodness-of-fit of the model. We propose a new method, more truly
capturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square
test on the resulting probability distribution. The results raise some
significant questions for large time lags -- 40 to 250 days -- where the
smoothness of the data does not require such a complex model; nevertheless, we
also provide some statistical evidence in favour of the Heston model for small
time lags -- 1 and 5 days -- compared with the traditional Gaussian model
assuming constant volatility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305056</id><created>2003-05-29</created><authors><author><keyname>Bartoldus</keyname><forenames>R.</forenames></author><author><keyname>Dubois-Felsmann</keyname><forenames>G.</forenames></author><author><keyname>Kolomensky</keyname><forenames>Y.</forenames></author><author><keyname>Salnikov</keyname><forenames>A.</forenames></author></authors><title>Configuration Database for BaBar On-line</title><categories>cs.DB cs.IR</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, 4 figures, PDF. PSN MOKT004</comments><report-no>SLAC-PUB-9831</report-no><acm-class>H.2.4; H.2.8</acm-class><abstract>  The configuration database is one of the vital systems in the BaBar on-line
system. It provides services for the different parts of the data acquisition
system and control system, which require run-time parameters. The original
design and implementation of the configuration database played a significant
role in the successful BaBar operations since the beginning of experiment.
Recent additions to the design of the configuration database provide better
means for the management of data and add new tools to simplify main
configuration tasks. We describe the design of the configuration database, its
implementation with the Objectivity/DB object-oriented database, and our
experience collected during the years of operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305057</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305057</id><created>2003-05-29</created><authors><author><keyname>Pomarede</keyname><forenames>D.</forenames></author><author><keyname>Virchaux</keyname><forenames>M.</forenames></author></authors><title>The Persint visualization program for the ATLAS experiment</title><categories>cs.GR</categories><comments>9 pages, 10 figures, proceedings of CHEP2003</comments><acm-class>I.3.0</acm-class><journal-ref>ECONFC0303241:MOLT009,2003</journal-ref><abstract>  The Persint program is designed for the three-dimensional representation of
objects and for the interfacing and access to a variety of independent
applications, in a fully interactive way. Facilities are provided for the
spatial navigation and the definition of the visualization properties, in order
to interactively set the viewing and viewed points, and to obtain the desired
perspective. In parallel, applications may be launched through the use of
dedicated interfaces, such as the interactive reconstruction and display of
physics events. Recent developments have focalized on the interfacing to the
XML ATLAS General Detector Description AGDD, making it a widely used tool for
XML developers. The graphics capabilities of this program were exploited in the
context of the ATLAS 2002 Muon Testbeam where it was used as an online event
display, integrated in the online software framework and participating in the
commissioning and debug of the detector system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305058</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305058</id><created>2003-05-30</created><updated>2003-10-14</updated><authors><author><keyname>Ciaschini</keyname><forenames>V.</forenames></author><author><keyname>Donno</keyname><forenames>F.</forenames></author><author><keyname>Fanfani</keyname><forenames>A.</forenames></author><author><keyname>Fanzago</keyname><forenames>F.</forenames></author><author><keyname>Garbellotto</keyname><forenames>V.</forenames></author><author><keyname>Verlato</keyname><forenames>M.</forenames></author><author><keyname>Vaccarossa</keyname><forenames>L.</forenames></author></authors><title>ATLAS and CMS applications on the WorldGrid testbed</title><categories>cs.DC</categories><comments>Poster paper from the 2003 Computing in High Energy and Nuclear
  Physics (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, PDF. PSN TUCP004;
  added credit to funding agency</comments><acm-class>A0</acm-class><journal-ref>ECONF C0303241:TUCP004,2003</journal-ref><abstract>  WorldGrid is an intercontinental testbed spanning Europe and the US
integrating architecturally different Grid implementations based on the Globus
toolkit. It has been developed in the context of the DataTAG and iVDGL
projects, and successfully demonstrated during the WorldGrid demos at IST2002
(Copenhagen) and SC2002 (Baltimore). Two HEP experiments, ATLAS and CMS,
successful exploited the WorldGrid testbed for executing jobs simulating the
response of their detectors to physics eve nts produced by real collisions
expected at the LHC accelerator starting from 2007. This data intensive
activity has been run since many years on local dedicated computing farms
consisting of hundreds of nodes and Terabytes of disk and tape storage. Within
the WorldGrid testbed, for the first time HEP simulation jobs were submitted
and run indifferently on US and European resources, despite of their underlying
different Grid implementations, and produced data which could be retrieved and
further analysed on the submitting machine, or simply stored on the remote
resources and registered on a Replica Catalogue which made them available to
the Grid for further processing. In this contribution we describe the job
submission from Europe for both ATLAS and CMS applications, performed through
the GENIUS portal operating on top of an EDG User Interface submitting to an
EDG Resource Broker, pointing out the chosen interoperability solutions which
made US and European resources equivalent from the applications point of view,
the data management in the WorldGrid environment, and the CMS specific
production tools which were interfaced to the GENIUS portal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305059</identifier>
 <datestamp>2009-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305059</id><created>2003-05-30</created><authors><author><keyname>Leonardi</keyname><forenames>E.</forenames></author><author><keyname>Schulz</keyname><forenames>M. W.</forenames></author></authors><title>EU DataGRID testbed management and support at CERN</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 7 pages, LaTeX. PSN THCT007</comments><acm-class>C.1.4; C.2.4; D.4.7</acm-class><journal-ref>ECONF C0303241:THCT007,2003</journal-ref><abstract>  In this paper we report on the first two years of running the CERN testbed
site for the EU DataGRID project. The site consists of about 120 dual-processor
PCs distributed over several testbeds used for different purposes: software
development, system integration, and application tests. Activities at the site
included test productions of MonteCarlo data for LHC experiments, tutorials and
demonstrations of GRID technologies, and support for individual users analysis.
This paper focuses on node installation and configuration techniques, service
management, user support in a gridified environment, and includes
considerations on scalability and security issues and comparisons with
&quot;traditional&quot; production systems, as seen from the administrator point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305060</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305060</id><created>2003-05-30</created><authors><author><keyname>Gug</keyname><forenames>Mathias</forenames></author></authors><title>Performance comparison between iSCSI and other hardware and software
  solutions</title><categories>cs.PF</categories><comments>Paper associated to a poster from the 2003 Computing in High Energy
  and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX.
  PSN TUDP001</comments><acm-class>C.4</acm-class><journal-ref>ECONFC0303241:TUDP001,2003</journal-ref><abstract>  We report on our investigations on some technologies that can be used to
build disk servers and networks of disk servers using commodity hardware and
software solutions. It focuses on the performance that can be achieved by these
systems and gives measured figures for different configurations.
  It is divided into two parts : iSCSI and other technologies and hardware and
software RAID solutions.
  The first part studies different technologies that can be used by clients to
access disk servers using a gigabit ethernet network. It covers block access
technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for
different numbers of clients and servers.
  The second part compares a system based on 3ware hardware RAID controllers, a
system using linux software RAID and IDE cards and a system mixing both
hardware RAID and software RAID. Performance measurements for reading and
writing are given for different RAID levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305061</id><created>2003-05-30</created><authors><author><keyname>Horvath</keyname><forenames>Andras</forenames></author><author><keyname>Leonardi</keyname><forenames>Emanuele</forenames></author><author><keyname>Schulz</keyname><forenames>Markus</forenames></author></authors><title>A Secure Infrastructure For System Console and Reset Access</title><categories>cs.DC</categories><comments>Conference for Computing in High Energy and Nuclear Physics
  (CHEP2003), March 24-28, 2003, La Jolla, California</comments><acm-class>K.6.4; C.2.4</acm-class><abstract>  During the last years large farms have been built using commodity hardware.
This hardware lacks components for remote and automated administration.
Products that can be retrofitted to these systems are either costly or
inherently insecure. We present a system based on serial ports and simple
machine controlled relays. We report on experience gained by setting up a
50-machine test environment as well as current work in progress in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305062</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305062</id><created>2003-05-30</created><updated>2003-06-13</updated><authors><author><keyname>Shafi</keyname><forenames>Aamir</forenames></author><author><keyname>Farooq</keyname><forenames>Umer</forenames></author><author><keyname>Kiani</keyname><forenames>Saad</forenames></author><author><keyname>Riaz</keyname><forenames>Maria</forenames></author><author><keyname>Shehzad</keyname><forenames>Anjum</forenames></author><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Legrand</keyname><forenames>Iosif</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author></authors><title>DIAMOnDS - DIstributed Agents for MObile &amp; Dynamic Services</title><categories>cs.DC</categories><comments>7 pages, 4 figures, CHEP03, La Jolla, California, March 24-28, 2003</comments><acm-class>C.2.4</acm-class><journal-ref>ECONFC0303241:THAT003,2003</journal-ref><abstract>  Distributed Services Architecture with support for mobile agents between
services, offer significantly improved communication and computational
flexibility. The uses of agents allow execution of complex operations that
involve large amounts of data to be processed effectively using distributed
resources. The prototype system Distributed Agents for Mobile and Dynamic
Services (DIAMOnDS), allows a service to send agents on its behalf, to other
services, to perform data manipulation and processing. Agents have been
implemented as mobile services that are discovered using the Jini Lookup
mechanism and used by other services for task management and communication.
Agents provide proxies for interaction with other services as well as specific
GUI to monitor and control the agent activity. Thus agents acting on behalf of
one service cooperate with other services to carry out a job, providing
inter-operation of loosely coupled services in a semi-autonomous way. Remote
file system access functionality has been incorporated by the agent framework
and allows services to dynamically share and browse the file system resources
of hosts, running the services. Generic database access functionality has been
implemented in the mobile agent framework that allows performing complex data
mining and processing operations efficiently in distributed system. A basic
data searching agent is also implemented that performs a query based search in
a file system. The testing of the framework was carried out on WAN by moving
Connectivity Test agents between AgentStations in CERN, Switzerland and NUST,
Pakistan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305063</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305063</id><created>2003-05-30</created><updated>2003-06-10</updated><authors><author><keyname>Graham</keyname><forenames>Gregory E.</forenames><affiliation>Fermilab</affiliation></author><author><keyname>Evans</keyname><forenames>Dave</forenames><affiliation>Lancaster University</affiliation></author><author><keyname>Bertram</keyname><forenames>Iain</forenames><affiliation>Lancaster University</affiliation></author></authors><title>McRunjob: A High Energy Physics Workflow Planner for Grid Production
  Processing</title><categories>cs.DC</categories><comments>CHEP 2003 serial number TUCT007</comments><acm-class>A.0; C.2.4</acm-class><journal-ref>ECONFC0303241:TUCT007,2003</journal-ref><abstract>  McRunjob is a powerful grid workflow manager used to manage the generation of
large numbers of production processing jobs in High Energy Physics. In use at
both the DZero and CMS experiments, McRunjob has been used to manage large
Monte Carlo production processing since 1999 and is being extended to uses in
regular production processing for analysis and reconstruction. Described at
CHEP 2001, McRunjob converts core metadata into jobs submittable in a variety
of environments. The powerful core metadata description language includes
methods for converting the metadata into persistent forms, job descriptions,
multi-step workflows, and data provenance information. The language features
allow for structure in the metadata by including full expressions, namespaces,
functional dependencies, site specific parameters in a grid environment, and
ontological definitions. It also has simple control structures for
parallelization of large jobs. McRunjob features a modular design which allows
for easy expansion to new job description languages or new application level
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305064</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305064</id><created>2003-05-30</created><authors><author><keyname>Stancu</keyname><forenames>Stefan</forenames></author><author><keyname>Dobinson</keyname><forenames>Bob</forenames></author><author><keyname>Ciobotaru</keyname><forenames>Matei</forenames></author><author><keyname>Korcyl</keyname><forenames>Krzysztof</forenames></author><author><keyname>Knezo</keyname><forenames>Emil</forenames></author></authors><title>The use of Ethernet in the DataFlow of the ATLAS Trigger &amp; DAQ</title><categories>cs.NI</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, March 2003, 10 pages, LaTeX, 10 eps figures. PSN
  MOGT010</comments><acm-class>C.2.5; C.3</acm-class><journal-ref>ECONFC0303241:MOGT010,2003</journal-ref><abstract>  The article analyzes a proposed network topology for the ATLAS DAQ DataFlow,
and identifies the Ethernet features required for a proper operation of the
network: MAC address table size, switch performance in terms of throughput and
latency, the use of Flow Control, Virtual LANs and Quality of Service. We
investigate these features on some Ethernet switches, and conclude on their
usefulness for the ATLAS DataFlow network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305065</id><created>2003-05-30</created><authors><author><keyname>Hamilton</keyname><forenames>James A.</forenames></author><author><keyname>Dubois-Felsmann</keyname><forenames>Gregory P.</forenames></author><author><keyname>Bartoldus</keyname><forenames>Rainer</forenames></author></authors><title>A Generic Multi-node State Monitoring Subsystem</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages</comments><report-no>SLAC-PUB-9909</report-no><acm-class>C.2.4</acm-class><abstract>  The BaBar online data acquisition (DAQ) system includes approximately fifty
Unix systems that collectively implement the level-three trigger. These systems
all run the same code. Each of these systems has its own state, and this state
is expected to change in response to changes in the overall DAQ system. A
specialized subsystem has been developed to initiate processing on this
collection of systems, and to monitor them both for error conditions and to
ensure that they all follow the same state trajectory within a specifiable
period of time. This subsystem receives start commands from the main DAQ run
control system, and reports major coherent state changes, as well as error
conditions, back to the run control system. This state monitoring subsystem has
the novel feature that it does not know anything about the state machines that
it is monitoring, and hence does not introduce any fundamentally new state
machine into the overall system. This feature makes it trivially applicable to
other multi-node subsystems. Indeed it has already found a second application
beyond the level-three trigger, within the BaBar experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0305066</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0305066</id><created>2003-05-30</created><updated>2003-06-10</updated><authors><author><keyname>Graham</keyname><forenames>Gregory E.</forenames></author><author><keyname>Afaq</keyname><forenames>M. Anzar</forenames></author><author><keyname>Aziz</keyname><forenames>Shafqat</forenames></author><author><keyname>Bauerdick</keyname><forenames>L. A. T.</forenames></author><author><keyname>Ernst</keyname><forenames>Michael</forenames></author><author><keyname>Kaiser</keyname><forenames>Joseph</forenames></author><author><keyname>Ratnikova</keyname><forenames>Natalia</forenames></author><author><keyname>Wenzel</keyname><forenames>Hans</forenames></author><author><keyname>Wu</keyname><forenames>Yujun</forenames></author><author><keyname>Aslakson</keyname><forenames>Erik</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Iqbal</keyname><forenames>Saima</forenames></author><author><keyname>Legrand</keyname><forenames>Iosif</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Singh</keyname><forenames>Suresh</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>Branson</keyname><forenames>James</forenames></author><author><keyname>Fisk</keyname><forenames>Ian</forenames></author><author><keyname>Letts</keyname><forenames>James</forenames></author><author><keyname>Arbree</keyname><forenames>Adam</forenames></author><author><keyname>Avery</keyname><forenames>Paul</forenames></author><author><keyname>Bourilkov</keyname><forenames>Dimitri</forenames></author><author><keyname>Cavanaugh</keyname><forenames>Richard</forenames></author><author><keyname>Rodriguez</keyname><forenames>Jorge</forenames></author><author><keyname>Kategari</keyname><forenames>Suchindra</forenames></author><author><keyname>Couvares</keyname><forenames>Peter</forenames></author><author><keyname>DeSmet</keyname><forenames>Alan</forenames></author><author><keyname>Livny</keyname><forenames>Miron</forenames></author><author><keyname>Roy</keyname><forenames>Alain</forenames></author><author><keyname>Tannenbaum</keyname><forenames>Todd</forenames></author></authors><title>The CMS Integration Grid Testbed</title><categories>cs.DC</categories><comments>CHEP 2003 MOCT010</comments><acm-class>A.0; C.2.4</acm-class><journal-ref>eConfC0303241:MOCT010B,2003</journal-ref><abstract>  The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2
hardware at the following sites: the California Institute of Technology, Fermi
National Accelerator Laboratory, the University of California at San Diego, and
the University of Florida at Gainesville. The IGT runs jobs using the Globus
Toolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is
managed using VO management scripts from the European Data Grid (EDG). Gridwide
monitoring is accomplished using local tools such as Ganglia interfaced into
the Globus Metadata Directory Service (MDS) and the agent based Mona Lisa.
Domain specific software is packaged and installed using the Distrib ution
After Release (DAR) tool of CMS, while middleware under the auspices of the
Virtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us
two month span in Fall of 2002, over 1 million official CMS GEANT based Monte
Carlo events were generated and returned to CERN for analysis while being
demonstrated at SC2002. In this paper, we describe the process that led to one
of the world's first continuously available, functioning grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306001</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306001</id><created>2003-05-30</created><updated>2003-07-14</updated><authors><author><keyname>Steenberg</keyname><forenames>Conrad D.</forenames></author><author><keyname>Aslakson</keyname><forenames>Eric</forenames></author><author><keyname>Bunn</keyname><forenames>Julian J.</forenames></author><author><keyname>Newman</keyname><forenames>Harvey B.</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author></authors><title>Clarens Client and Server Applications</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, no figures, PSN
  TUCT005</comments><acm-class>H.3.4</acm-class><abstract>  Several applications have been implemented with access via the Clarens web
service infrastructure, including virtual organization management, JetMET
physics data analysis using relational databases, and Storage Resource Broker
(SRB) access. This functionality is accessible transparently from Python
scripts, the Root analysis framework and from Java applications and browser
applets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306002</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306002</id><created>2003-05-30</created><updated>2003-07-14</updated><authors><author><keyname>Steenberg</keyname><forenames>Conrad D.</forenames></author><author><keyname>Aslakson</keyname><forenames>Eric</forenames></author><author><keyname>Bunn</keyname><forenames>Julian J.</forenames></author><author><keyname>Newman</keyname><forenames>Harvey B.</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author></authors><title>The Clarens web services architecture</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, LaTeX, 4 figures, PSN
  MONT008</comments><acm-class>H.3.4</acm-class><abstract>  Clarens is a uniquely flexible web services infrastructure providing a
unified access protocol to a diverse set of functions useful to the HEP
community. It uses the standard HTTP protocol combined with application layer,
certificate based authentication to provide single sign-on to individuals,
organizations and hosts, with fine-grained access control to services, files
and virtual organization (VO) management. This contribution describes the
server functionality, while client applications are described in a subsequent
talk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306003</id><created>2003-05-30</created><updated>2003-06-12</updated><authors><author><keyname>Byrom</keyname><forenames>Rob</forenames></author><author><keyname>Coghlan</keyname><forenames>Brian</forenames></author><author><keyname>Cooke</keyname><forenames>Andrew W</forenames></author><author><keyname>Cordenonsi</keyname><forenames>Roney</forenames></author><author><keyname>Cornwall</keyname><forenames>Linda</forenames></author><author><keyname>Datta</keyname><forenames>Ari</forenames></author><author><keyname>Djaoui</keyname><forenames>Abdeslem</forenames></author><author><keyname>Field</keyname><forenames>Laurence</forenames></author><author><keyname>Fisher</keyname><forenames>Steve</forenames></author><author><keyname>Hicks</keyname><forenames>Steve</forenames></author><author><keyname>Kenny</keyname><forenames>Stuart</forenames></author><author><keyname>Magowan</keyname><forenames>James</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>O'Callaghan</keyname><forenames>David</forenames></author><author><keyname>Oevers</keyname><forenames>Manfred</forenames></author><author><keyname>Podhorszki</keyname><forenames>Norbert</forenames></author><author><keyname>Ryan</keyname><forenames>John</forenames></author><author><keyname>Soni</keyname><forenames>Manish</forenames></author><author><keyname>Taylor</keyname><forenames>Paul</forenames></author><author><keyname>Wilson</keyname><forenames>Antony J.</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaomei</forenames></author></authors><title>R-GMA: First results after deployment</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 3 eps figures. PSN
  MOET004</comments><acm-class>H.2.4;H.m</acm-class><abstract>  We describe R-GMA (Relational Grid Monitoring Architecture) which is being
developed within the European DataGrid Project as an Grid Information and
Monitoring System. Is is based on the GMA from GGF, which is a simple
Consumer-Producer model. The special strength of this implementation comes from
the power of the relational model. We offer a global view of the information as
if each VO had one large relational database. We provide a number of different
Producer types with different characteristics; for example some support
streaming of information. We also provide combined Consumer/Producers, which
are able to combine information and republish it. At the heart of the system is
the mediator, which for any query is able to find and connect to the best
Producers to do the job. We are able to invoke MDS info-provider scripts and
publish the resulting information via R-GMA in addition to having some of our
own sensors. APIs are available which allow the user to deploy monitoring and
information services for any application that may be needed in the future. We
have used it both for information about the grid (primarily to find what
services are available at any one time) and for application monitoring. R-GMA
has been deployed in Grid testbeds, we describe the results and experiences of
this deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306004</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306004</id><created>2003-05-30</created><updated>2003-06-13</updated><authors><author><keyname>Alfieri</keyname><forenames>R.</forenames></author><author><keyname>Cecchini</keyname><forenames>R.</forenames></author><author><keyname>Ciaschini</keyname><forenames>V.</forenames></author><author><keyname>dell'Agnello</keyname><forenames>L.</forenames></author><author><keyname>Gianoli</keyname><forenames>A.</forenames></author><author><keyname>Spataro</keyname><forenames>F.</forenames></author><author><keyname>Bonnassieux</keyname><forenames>F.</forenames></author><author><keyname>Broadfoot</keyname><forenames>P.</forenames></author><author><keyname>Lowe</keyname><forenames>G.</forenames></author><author><keyname>Cornwall</keyname><forenames>L.</forenames></author><author><keyname>Jensen</keyname><forenames>J.</forenames></author><author><keyname>Kelsey</keyname><forenames>D.</forenames></author><author><keyname>Frohner</keyname><forenames>A.</forenames></author><author><keyname>Groep</keyname><forenames>D. L.</forenames></author><author><keyname>de Cerff</keyname><forenames>W. Som</forenames></author><author><keyname>Steenbakkers</keyname><forenames>M.</forenames></author><author><keyname>Venekamp</keyname><forenames>G.</forenames></author><author><keyname>Kouril</keyname><forenames>D.</forenames></author><author><keyname>McNab</keyname><forenames>A.</forenames></author><author><keyname>Mulmo</keyname><forenames>O.</forenames></author><author><keyname>Silander</keyname><forenames>M.</forenames></author><author><keyname>Hahkala</keyname><forenames>J.</forenames></author><author><keyname>Lhorentey</keyname><forenames>K.</forenames></author></authors><title>Managing Dynamic User Communities in a Grid of Autonomous Resources</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 7 pages, LaTeX, 5 eps figures. PSN
  TUBT005</comments><acm-class>C.2.4 ; D.4.6</acm-class><journal-ref>ECONF C0303241:TUBT005,2003</journal-ref><abstract>  One of the fundamental concepts in Grid computing is the creation of Virtual
Organizations (VO's): a set of resource consumers and providers that join
forces to solve a common problem. Typical examples of Virtual Organizations
include collaborations formed around the Large Hadron Collider (LHC)
experiments. To date, Grid computing has been applied on a relatively small
scale, linking dozens of users to a dozen resources, and management of these
VO's was a largely manual operation. With the advance of large collaboration,
linking more than 10000 users with a 1000 sites in 150 counties, a
comprehensive, automated management system is required. It should be simple
enough not to deter users, while at the same time ensuring local site autonomy.
The VO Management Service (VOMS), developed by the EU DataGrid and DataTAG
projects[1, 2], is a secured system for managing authorization for users and
resources in virtual organizations. It extends the existing Grid Security
Infrastructure[3] architecture with embedded VO affiliation assertions that can
be independently verified by all VO members and resource providers. Within the
EU DataGrid project, Grid services for job submission, file- and database
access are being equipped with fine- grained authorization systems that take VO
membership into account. These also give resource owners the ability to ensure
site security and enforce local access policies. This paper will describe the
EU DataGrid security architecture, the VO membership service and the local site
enforcement mechanisms Local Centre Authorization Service (LCAS), Local
Credential Mapping Service(LCMAPS) and the Java Trust and Authorization
Manager.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306005</identifier>
 <datestamp>2010-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306005</id><created>2003-05-30</created><authors><author><keyname>Hrivnacova</keyname><forenames>I.</forenames><affiliation>IPN, Orsay, France</affiliation></author><author><keyname>Adamova</keyname><forenames>D.</forenames><affiliation>NPI, ASCR, Rez, Czech Republic</affiliation></author><author><keyname>Berejnoi</keyname><forenames>V.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Brun</keyname><forenames>R.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Carminati</keyname><forenames>F.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Fasso</keyname><forenames>A.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Futo</keyname><forenames>E.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Gheata</keyname><forenames>A.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Caballero</keyname><forenames>I. Gonzalez</forenames><affiliation>IFCA, Santander, Spain</affiliation></author><author><keyname>Morsch</keyname><forenames>A.</forenames><affiliation>CERN, Geneva, Switzerland</affiliation></author><author><keyname>Collaboration</keyname><forenames>for the ALICE</forenames></author></authors><title>The Virtual Monte Carlo</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 6 eps figures. PSN
  THJT006. See http://root.cern.ch/root/vmc/VirtualMC.html</comments><acm-class>D.2.11</acm-class><journal-ref>ECONF C0303241:THJT006,2003</journal-ref><abstract>  The concept of Virtual Monte Carlo (VMC) has been developed by the ALICE
Software Project to allow different Monte Carlo simulation programs to run
without changing the user code, such as the geometry definition, the detector
response simulation or input and output formats. Recently, the VMC classes have
been integrated into the ROOT framework, and the other relevant packages have
been separated from the AliRoot framework and can be used individually by any
other HEP project. The general concept of the VMC and its set of base classes
provided in ROOT will be presented. Existing implementations for Geant3, Geant4
and FLUKA and simple examples of usage will be described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306006</id><created>2003-05-30</created><updated>2003-06-13</updated><authors><author><keyname>Amorim</keyname><forenames>A.</forenames></author><author><keyname>Lima</keyname><forenames>J.</forenames></author><author><keyname>Oliveira</keyname><forenames>C.</forenames></author><author><keyname>Pedro</keyname><forenames>L.</forenames></author><author><keyname>Barros</keyname><forenames>N.</forenames></author></authors><title>Experience with the Open Source based implementation for ATLAS
  Conditions Data Management System</title><categories>cs.DB</categories><comments>8 pages, 4 figures, 3 tables, conference</comments><acm-class>H.2.4; H.2.2; H.3.3</acm-class><abstract>  Conditions Data in high energy physics experiments is frequently seen as
every data needed for reconstruction besides the event data itself. This
includes all sorts of slowly evolving data like detector alignment, calibration
and robustness, and data from detector control system. Also, every Conditions
Data Object is associated with a time interval of validity and a version.
Besides that, quite often is useful to tag collections of Conditions Data
Objects altogether. These issues have already been investigated and a data
model has been proposed and used for different implementations based in
commercial DBMSs, both at CERN and for the BaBar experiment. The special case
of the ATLAS complex trigger that requires online access to calibration and
alignment data poses new challenges that have to be met using a flexible and
customizable solution more in the line of Open Source components. Motivated by
the ATLAS challenges we have developed an alternative implementation, based in
an Open Source RDBMS. Several issues were investigated land will be described
in this paper:
  -The best way to map the conditions data model into the relational database
concept considering what are foreseen as the most frequent queries.
  -The clustering model best suited to address the scalability problem.
-Extensive tests were performed and will be described.
  The very promising results from these tests are attracting the attention from
the HEP community and driving further developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306007</id><created>2003-05-31</created><authors><author><keyname>Avellino</keyname><forenames>G.</forenames></author><author><keyname>Beco</keyname><forenames>S.</forenames></author><author><keyname>Cantalupo</keyname><forenames>B.</forenames></author><author><keyname>Pacini</keyname><forenames>F.</forenames></author><author><keyname>Terracina</keyname><forenames>A.</forenames></author><author><keyname>Maraschini</keyname><forenames>A.</forenames></author><author><keyname>Colling</keyname><forenames>D.</forenames></author><author><keyname>Monforte</keyname><forenames>S.</forenames></author><author><keyname>Pappalardo</keyname><forenames>M.</forenames></author><author><keyname>Salconi</keyname><forenames>L.</forenames></author><author><keyname>Giacomini</keyname><forenames>F.</forenames></author><author><keyname>Ronchieri</keyname><forenames>E.</forenames></author><author><keyname>Kouril</keyname><forenames>D.</forenames></author><author><keyname>Krenek</keyname><forenames>A.</forenames></author><author><keyname>Matyska</keyname><forenames>L.</forenames></author><author><keyname>Mulac</keyname><forenames>M.</forenames></author><author><keyname>Pospisil</keyname><forenames>J.</forenames></author><author><keyname>Ruda</keyname><forenames>M.</forenames></author><author><keyname>Salvet</keyname><forenames>Z.</forenames></author><author><keyname>Sitera</keyname><forenames>J.</forenames></author><author><keyname>Vocu</keyname><forenames>M.</forenames></author><author><keyname>Mezzadri</keyname><forenames>M.</forenames></author><author><keyname>Prelz</keyname><forenames>F.</forenames></author><author><keyname>Gianelle</keyname><forenames>A.</forenames></author><author><keyname>Peluso</keyname><forenames>R.</forenames></author><author><keyname>Sgaravatto</keyname><forenames>M.</forenames></author><author><keyname>Barale</keyname><forenames>S.</forenames></author><author><keyname>Guarise</keyname><forenames>A.</forenames></author><author><keyname>Werbrouck</keyname><forenames>A.</forenames></author></authors><title>The first deployment of workload management services on the EU DataGrid
  Testbed: feedback on design and implementation</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 2 eps figures</comments><acm-class>C.2.4; D.4.1</acm-class><abstract>  Application users have now been experiencing for about a year with the
standardized resource brokering services provided by the 'workload management'
package of the EU DataGrid project (WP1). Understanding, shaping and pushing
the limits of the system has provided valuable feedback on both its design and
implementation. A digest of the lessons, and &quot;better practices&quot;, that were
learned, and that were applied towards the second major release of the
software, is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306008</id><created>2003-05-31</created><updated>2003-06-11</updated><authors><author><keyname>Ceseracciu</keyname><forenames>A.</forenames></author><author><keyname>Piemontese</keyname><forenames>M.</forenames></author><author><keyname>Tehrani</keyname><forenames>F. Safai</forenames></author><author><keyname>Elmer</keyname><forenames>P.</forenames></author><author><keyname>Johnson</keyname><forenames>D.</forenames></author><author><keyname>Pulliam</keyname><forenames>T. M.</forenames></author></authors><title>The new BaBar Data Reconstruction Control System</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 9 pages, LaTeX, 2 eps figures PSN
  TUDT011</comments><report-no>SLAC-PUB-9873</report-no><acm-class>H.2.4</acm-class><abstract>  The BaBar experiment is characterized by extremely high luminosity, a complex
detector, and a huge data volume, with increasing requirements each year. To
fulfill these requirements a new control system has been designed and developed
for the offline data reconstruction system. The new control system described in
this paper provides the performance and flexibility needed to manage a large
number of small computing farms, and takes full benefit of OO design. The
infrastructure is well isolated from the processing layer, it is generic and
flexible, based on a light framework providing message passing and cooperative
multitasking. The system is actively distributed, enforces the separation
between different processing tiers by using different naming domains, and glues
them together by dedicated brokers. It provides a powerful Finite State Machine
framework to describe custom processing models in a simple regular language.
This paper describes this new control system, currently in use at SLAC and
Padova on ~450 CPUs organized in 12 farms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306009</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306009</id><created>2003-05-31</created><authors><author><keyname>Arbree</keyname><forenames>A.</forenames></author><author><keyname>Avery</keyname><forenames>P.</forenames></author><author><keyname>Bourilkov</keyname><forenames>D.</forenames></author><author><keyname>Cavanaugh</keyname><forenames>R.</forenames></author><author><keyname>Graham</keyname><forenames>G.</forenames></author><author><keyname>Katageri</keyname><forenames>S.</forenames></author><author><keyname>Rodriguez</keyname><forenames>J.</forenames></author><author><keyname>Voeckler</keyname><forenames>J.</forenames></author><author><keyname>Wilde</keyname><forenames>M.</forenames></author></authors><title>Virtual Data in CMS Production</title><categories>cs.DC hep-ex</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, PDF. PSN TUAT011</comments><acm-class>H.2.4</acm-class><journal-ref>ECONFC0303241:TUAT011,2003</journal-ref><abstract>  Initial applications of the GriPhyN Chimera Virtual Data System have been
performed within the context of CMS Production of Monte Carlo Simulated Data.
The GriPhyN Chimera system consists of four primary components: 1) a Virtual
Data Language, which is used to describe virtual data products, 2) a Virtual
Data Catalog, which is used to store virtual data entries, 3) an Abstract
Planner, which resolves all dependencies of a particular virtual data product
and forms a location and existence independent plan, 4) a Concrete Planner,
which maps an abstract, logical plan onto concrete, physical grid resources
accounting for staging in/out files and publishing results to a replica
location service. A CMS Workflow Planner, MCRunJob, is used to generate virtual
data products using the Virtual Data Language. Subsequently, a prototype
workflow manager, known as WorkRunner, is used to schedule the instantiation of
virtual data products across a grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306010</id><created>2003-06-02</created><authors><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author><author><keyname>Sarkar</keyname><forenames>Dilip</forenames></author></authors><title>On multiple connectedness of regions visible due to multiple diffuse
  reflections</title><categories>cs.CG cs.DM cs.GR</categories><comments>10 pages, 3 figures</comments><acm-class>F2.2, G2.1</acm-class><abstract>  It is known that the region $V(s)$ of a simple polygon $P$, directly visible
(illuminable) from an internal point $s$, is simply connected. Aronov et al.
\cite{addpp981} established that the region $V_1(s)$ of a simple polygon
visible from an internal point $s$ due to at most one diffuse reflection on the
boundary of the polygon $P$, is also simply connected. In this paper we
establish that the region $V_2(s)$, visible from $s$ due to at most two diffuse
reflections may be multiply connected; we demonstrate the construction of an
$n$-sided simple polygon with a point $s$ inside it so that and the region of
$P$ visible from $s$ after at most two diffuse reflections is multiple
connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306011</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306011</id><created>2003-06-02</created><authors><author><keyname>Stockinger</keyname><forenames>Heinz</forenames></author><author><keyname>Donno</keyname><forenames>Flavia</forenames></author><author><keyname>Laure</keyname><forenames>Erwin</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter</forenames></author><author><keyname>Andronico</keyname><forenames>Giuseppe</forenames></author><author><keyname>Millar</keyname><forenames>Paul</forenames></author></authors><title>Grid Data Management in Action: Experience in Running and Supporting
  Data Management Services in the EU DataGrid Project</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 9 pages, LaTeX, PSN: TUAT007 all
  figures are in the directory &quot;figures&quot;</comments><acm-class>E.0</acm-class><journal-ref>ECONFC0303241:TUAT007,2003</journal-ref><abstract>  In the first phase of the EU DataGrid (EDG) project, a Data Management System
has been implemented and provided for deployment. The components of the current
EDG Testbed are: a prototype of a Replica Manager Service built around the
basic services provided by Globus, a centralised Replica Catalogue to store
information about physical locations of files, and the Grid Data Mirroring
Package (GDMP) that is widely used in various HEP collaborations in Europe and
the US for data mirroring. During this year these services have been refined
and made more robust so that they are fit to be used in a pre-production
environment. Application users have been using this first release of the Data
Management Services for more than a year. In the paper we present the
components and their interaction, our implementation and experience as well as
the feedback received from our user communities. We have resolved not only
issues regarding integration with other EDG service components but also many of
the interoperability issues with components of our partner projects in Europe
and the U.S. The paper concludes with the basic lessons learned during this
operation. These conclusions provide the motivation for the architecture of the
next generation of Data Management Services that will be deployed in EDG during
2003.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306012</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306012</id><created>2003-06-02</created><authors><author><keyname>Hrivnac</keyname><forenames>Julius</forenames></author></authors><title>GraXML - Modular Geometric Modeler</title><categories>cs.GR</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003. PSN THJT009</comments><acm-class>I.2.10; I.3.7</acm-class><journal-ref>ECONFC0303241:THJT009,2003</journal-ref><abstract>  Many entities managed by HEP Software Frameworks represent spatial
(3-dimensional) real objects. Effective definition, manipulation and
visualization of such objects is an indispensable functionality.
  GraXML is a modular Geometric Modeling toolkit capable of processing
geometric data of various kinds (detector geometry, event geometry) from
different sources and delivering them in ways suitable for further use.
Geometric data are first modeled in one of the Generic Models. Those Models are
then used to populate powerful Geometric Model based on the Java3D technology.
While Java3D has been originally created just to provide visualization of 3D
objects, its light weight and high functionality allow an effective reuse as a
general geometric component. This is possible also thanks to a large overlap
between graphical and general geometric functionality and modular design of
Java3D itself. Its graphical functionalities also allow a natural visualization
of all manipulated elements.
  All these techniques have been developed primarily (or only) for the Java
environment. It is, however, possible to interface them transparently to
Frameworks built in other languages, like for example C++.
  The GraXML toolkit has been tested with data from several sources, as for
example ATLAS and ALICE detector description and ATLAS event data. Prototypes
for other sources, like Geometry Description Markup Language (GDML) exist too
and interface to any other source is easy to add.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306013</id><created>2003-06-02</created><authors><author><keyname>Hrivnac</keyname><forenames>Julius</forenames></author></authors><title>Transparent Persistence with Java Data Objects</title><categories>cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003. PSN TUKT005</comments><acm-class>H.2</acm-class><abstract>  Flexible and performant Persistency Service is a necessary component of any
HEP Software Framework. The building of a modular, non-intrusive and performant
persistency component have been shown to be very difficult task. In the past,
it was very often necessary to sacrifice modularity to achieve acceptable
performance. This resulted in the strong dependency of the overall Frameworks
on their Persistency subsystems.
  Recent development in software technology has made possible to build a
Persistency Service which can be transparently used from other Frameworks. Such
Service doesn't force a strong architectural constraints on the overall
Framework Architecture, while satisfying high performance requirements. Java
Data Object standard (JDO) has been already implemented for almost all major
databases. It provides truly transparent persistency for any Java object (both
internal and external). Objects in other languages can be handled via
transparent proxies. Being only a thin layer on top of a used database, JDO
doesn't introduce any significant performance degradation. Also Aspect-Oriented
Programming (AOP) makes possible to treat persistency as an orthogonal Aspect
of the Application Framework, without polluting it with persistence-specific
concepts.
  All these techniques have been developed primarily (or only) for the Java
environment. It is, however, possible to interface them transparently to
Frameworks built in other languages, like for example C++.
  Fully functional prototypes of flexible and non-intrusive persistency modules
have been build for several other packages, as for example FreeHEP AIDA and LCG
Pool AttributeSet (package Indicium).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306014</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306014</id><created>2003-06-02</created><authors><author><keyname>Wellisch</keyname><forenames>J. P.</forenames><affiliation>CERN, Geneve, Switzerland</affiliation></author><author><keyname>Williams</keyname><forenames>C.</forenames><affiliation>CERN, Geneve, Switzerland</affiliation></author><author><keyname>Ashby</keyname><forenames>S.</forenames><affiliation>CERN, Geneve, Switzerland</affiliation></author></authors><title>SCRAM: Software configuration and management for the LHC Computing Grid
  project</title><categories>cs.OH</categories><comments>Computing in High Energy and Nuclear Physics, La Jolla, California,
  March 24-28, 2003 1 tar file</comments><acm-class>D2.9, K6.2, K6.3, D2.3, D2.6</acm-class><abstract>  Recently SCRAM (Software Configuration And Management) has been adopted by
the applications area of the LHC computing grid project as baseline
configuration management and build support infrastructure tool.
  SCRAM is a software engineering tool, that supports the configuration
management and management processes for software development. It resolves the
issues of configuration definition, assembly break-down, build, project
organization, run-time environment, installation, distribution, deployment, and
source code distribution. It was designed with a focus on supporting a
distributed, multi-project development work-model.
  We will describe the underlying technology, and the solutions SCRAM offers to
the above software engineering processes, while taking a users view of the
system under configuration management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306015</id><created>2003-06-02</created><authors><author><keyname>Ramakrishna</keyname><forenames>P. H. D.</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author><author><keyname>Bhalla</keyname><forenames>Samir</forenames></author><author><keyname>Basu</keyname><forenames>Hironmay</forenames></author><author><keyname>Singh</keyname><forenames>Sudhir Kumar</forenames></author></authors><title>Computing sharp and scalable bounds on errors in approximate zeros of
  univariate polynomials</title><categories>cs.NA</categories><comments>13 pages, no figures</comments><acm-class>F.2.1, G.1.0, G.1.5</acm-class><abstract>  There are several numerical methods for computing approximate zeros of a
given univariate polynomial. In this paper, we develop a simple and novel
method for determining sharp upper bounds on errors in approximate zeros of a
given polynomial using Rouche's theorem from complex analysis. We compute the
error bounds using non-linear optimization. Our bounds are scalable in the
sense that we compute sharper error bounds for better approximations of zeros.
We use high precision computations using the LEDA/real floating-point filter
for computing our bounds robustly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306016</id><created>2003-06-03</created><authors><author><keyname>Llabres</keyname><forenames>Merce</forenames></author><author><keyname>Rossello</keyname><forenames>Francesc</forenames></author></authors><title>Modelling Biochemical Operations on RNA Secondary Structures</title><categories>cs.CE q-bio</categories><comments>10 pages</comments><acm-class>J.3;F.4.2</acm-class><abstract>  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\ss e-Rhode's Algebra Transformation Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306017</identifier>
 <datestamp>2011-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306017</id><created>2003-06-03</created><authors><author><keyname>Rondogiannis</keyname><forenames>Panos</forenames></author><author><keyname>Wadge</keyname><forenames>William W.</forenames></author></authors><title>Minimum Model Semantics for Logic Programs with Negation-as-Failure</title><categories>cs.LO cs.AI cs.PL</categories><comments>28 pages</comments><acm-class>F.3.2;F.4.1;D.1.6;I.2.3</acm-class><journal-ref>ACM Trans. Comput. Log. 6(2): 441-467 (2005)</journal-ref><abstract>  We give a purely model-theoretic characterization of the semantics of logic
programs with negation-as-failure allowed in clause bodies. In our semantics
the meaning of a program is, as in the classical case, the unique minimum model
in a program-independent ordering. We use an expanded truth domain that has an
uncountable linearly ordered set of truth values between False (the minimum
element) and True (the maximum), with a Zero element in the middle. The truth
values below Zero are ordered like the countable ordinals. The values above
Zero have exactly the reverse order. Negation is interpreted as reflection
about Zero followed by a step towards Zero; the only truth value that remains
unaffected by negation is Zero. We show that every program has a unique minimum
model M_P, and that this model can be constructed with a T_P iteration which
proceeds through the countable ordinals. Furthermore, we demonstrate that M_P
can also be obtained through a model intersection construction which
generalizes the well-known model intersection theorem for classical logic
programming. Finally, we show that by collapsing the true and false values of
the infinite-valued model M_P to (the classical) True and False, we obtain a
three-valued model identical to the well-founded one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306018</id><created>2003-06-03</created><authors><author><keyname>Andreozzi</keyname><forenames>S.</forenames></author><author><keyname>Fantinel</keyname><forenames>S.</forenames></author><author><keyname>Rebatto</keyname><forenames>D.</forenames></author><author><keyname>Vaccarossa</keyname><forenames>L.</forenames></author><author><keyname>Tortone</keyname><forenames>G.</forenames></author></authors><title>A monitoring tool for a GRID operation center</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 3 pages, PDF. PSN MOET002</comments><acm-class>C.2.3</acm-class><abstract>  WorldGRID is an intercontinental testbed spanning Europe and the US
integrating architecturally different Grid implementations based on the Globus
toolkit. The WorldGRID testbed has been successfully demonstrated during the
WorldGRID demos at SuperComputing 2002 (Baltimore) and IST2002 (Copenhagen)
where real HEP application jobs were transparently submitted from US and Europe
using &quot;native&quot; mechanisms and run where resources were available, independently
of their location. To monitor the behavior and performance of such testbed and
spot problems as soon as they arise, DataTAG has developed the EDT-Monitor tool
based on the Nagios package that allows for Virtual Organization centric views
of the Grid through dynamic geographical maps. The tool has been used to spot
several problems during the WorldGRID operations, such as malfunctioning
Resource Brokers or Information Servers, sites not correctly configured, job
dispatching problems, etc. In this paper we give an overview of the package,
its features and scalability solutions and we report on the experience acquired
and the benefit that a GRID operation center would gain from such a tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306019</id><created>2003-06-04</created><authors><author><keyname>Sourikova</keyname><forenames>I.</forenames></author><author><keyname>Morrison</keyname><forenames>D.</forenames></author></authors><title>Relational databases for data management in PHENIX</title><categories>cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, 4 eps figures. PSN
  TUKT003</comments><acm-class>H.2.4</acm-class><abstract>  PHENIX is one of the two large experiments at the Relativistic Heavy Ion
Collider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly
100TB of experimental data per year. In addition, large volumes of simulated
data are produced at multiple off-site computing centers. For any file catalog
to play a central role in data management it has to face problems associated
with the need for distributed access and updates. To be used effectively by the
hundreds of PHENIX collaborators in 12 countries the catalog must satisfy the
following requirements: 1) contain up-to-date data, 2) provide fast and
reliable access to the data, 3) have write permissions for the sites that store
portions of data. We present an analysis of several available Relational
Database Management Systems (RDBMS) to support a catalog meeting the above
requirements and discuss the PHENIX experience with building and using the
distributed file catalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306020</id><created>2003-06-04</created><updated>2003-06-04</updated><authors><author><keyname>Adesanya</keyname><forenames>Adeyemi</forenames></author><author><keyname>Azemoon</keyname><forenames>Tofigh</forenames></author><author><keyname>Becla</keyname><forenames>Jacek</forenames></author><author><keyname>Hanushevsky</keyname><forenames>Andrew</forenames></author><author><keyname>Hasan</keyname><forenames>Adil</forenames></author><author><keyname>Kroeger</keyname><forenames>Wilko</forenames></author><author><keyname>Trunov</keyname><forenames>Artem</forenames></author><author><keyname>Wang</keyname><forenames>Daniel</forenames></author><author><keyname>Gaponenko</keyname><forenames>Igor</forenames></author><author><keyname>Patton</keyname><forenames>Simon</forenames></author><author><keyname>Quarrie</keyname><forenames>David</forenames></author></authors><title>On the Verge of One Petabyte - the Story Behind the BaBar Database
  System</title><categories>cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT010</comments><acm-class>C.2.4; H.2</acm-class><abstract>  The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306021</id><created>2003-06-04</created><updated>2003-08-30</updated><authors><author><keyname>Haubold</keyname><forenames>Alexander</forenames></author></authors><title>Visualization for Periodic Population Movement between Distinct
  Localities</title><categories>cs.IR</categories><comments>Poster Summary: 2 pages, 4 figures, InfoVis 2003 Symposium</comments><acm-class>H.3.3</acm-class><abstract>  We present a new visualization method to summarize and present periodic
population movement between distinct locations, such as floors, buildings,
cities, or the like. In the specific case of this paper, we have chosen to
focus on student movement between college dormitories on the Columbia
University campus. The visual information is presented to the information
analyst in the form of an interactive geographical map, in which specific
temporal periods as well as individual buildings can be singled out for
detailed data exploration. The navigational interface has been designed to
specifically meet a geographical setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306022</id><created>2003-06-04</created><authors><author><keyname>Venkataraman</keyname><forenames>Anand</forenames></author><author><keyname>Wang</keyname><forenames>Wen</forenames></author></authors><title>Techniques for effective vocabulary selection</title><categories>cs.CL cs.AI</categories><comments>4 pages. To appear Proc. Eurospeech 2003, Geneva</comments><acm-class>I.2.6;I.2.7</acm-class><abstract>  The vocabulary of a continuous speech recognition (CSR) system is a
significant factor in determining its performance. In this paper, we present
three principled approaches to select the target vocabulary for a particular
domain by trading off between the target out-of-vocabulary (OOV) rate and
vocabulary size. We evaluate these approaches against an ad-hoc baseline
strategy. Results are presented in the form of OOV rate graphs plotted against
increasing vocabulary size for each technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306023</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306023</id><created>2003-06-04</created><authors><author><keyname>Adesanya</keyname><forenames>Adeyemi</forenames></author><author><keyname>Becla</keyname><forenames>Jacek</forenames></author><author><keyname>Wang</keyname><forenames>Daniel</forenames></author></authors><title>The Redesigned BaBar Event Store: Believe the Hype</title><categories>cs.DB cs.DS</categories><comments>Presented at the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), 5 pages, 2 ps figures, PSN TUKT008</comments><report-no>SLAC-PUB-9893</report-no><acm-class>H.2.1; H.2.4; E.2</acm-class><abstract>  As the BaBar experiment progresses, it produces new and unforeseen
requirements and increasing demands on capacity and feature base. The current
system is being utilized well beyond its original design specifications, and
has scaled appropriately, maintaining data consistency and durability. The
persistent event storage system has remained largely unchanged since the
initial implementation, and thus includes many design features which have
become performance bottlenecks. Programming interfaces were designed before
sufficient usage information became available. Performance and efficiency were
traded off for added flexibility to cope with future demands. With significant
experience in managing actual production data under our belt, we are now in a
position to recraft the system to better suit current needs. The Event Store
redesign is intended to eliminate redundant features while adding new ones,
increase overall performance, and contain the physical storage cost of the
world's largest database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306024</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306024</id><created>2003-06-05</created><authors><author><keyname>Brokmann</keyname><forenames>Alwin</forenames></author></authors><title>Monitoring Systems and Services</title><categories>cs.OH</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 3 pages pdf,PSN THET03</comments><acm-class>V1</acm-class><journal-ref>ECONFC0303241:THET003,2003</journal-ref><abstract>  The DESY Computer Center is the home of O(1000) computers supplying a wide
range of different services Monitoring such a large installation is a
challenge. After a long time running a SNMP based commercial Network Management
System, the evaluation of a new System was started. There are a lot of
different commercial and freeware products on the market, but none of them
fully satisfied all our requirements. After re-valuating our original
requirements we selected NAGIOS as our monitoring and alarming tool. After a
successful test we are in production since autumn 2002 and are extending the
service to fully support a distributed monitoring and alarming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306025</id><created>2003-06-05</created><updated>2003-07-09</updated><authors><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Dianjun</forenames></author></authors><title>Permutation Generation: Two New Permutation Algorithms</title><categories>cs.DS cs.CC</categories><comments>7 pages, 4 figures</comments><acm-class>F.2.2</acm-class><abstract>  Two completely new algorithms for generating permutations, shift-cursor
algorithm and level algorithm, and their efficient implementations are
presented in this paper. One implementation of the shift cursor algorithm gives
an optimal solution of the permutation generation problem, and one
implementation of the level algorithm can be used to generate random
permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306026</id><created>2003-06-05</created><authors><author><keyname>Earl</keyname><forenames>A. D.</forenames></author><author><keyname>Hasan</keyname><forenames>A.</forenames></author><author><keyname>Boutigany</keyname><forenames>D.</forenames></author></authors><title>BdbServer++: A User Driven Data Location and Retrieval Tool</title><categories>cs.IR</categories><comments>Paper based on the poster from the 2003 Computing in High Energy and
  Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, 0
  figures. PSN TUCP011</comments><report-no>SLAC-PUB-9925</report-no><acm-class>H.3.3</acm-class><abstract>  The adoption of Grid technology has the potential to greatly aid the BaBar
experiment. BdbServer was originally designed to extract copies of data from
the Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple
locations in a variety of data formats, we are enhancing this tool. This will
enable users to extract selected deep copies of event collections and ship them
to the requested site using the facilities offered by the existing Grid
infrastructure. By building on the work done by various groups in BaBar, and
the European DataGrid, we have successfully expanded the capabilities of the
BdbServer software. This should provide a framework for future work in data
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306027</identifier>
 <datestamp>2009-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306027</id><created>2003-06-05</created><authors><author><keyname>Augustin</keyname><forenames>I.</forenames></author><author><keyname>Carminati</keyname><forenames>F.</forenames></author><author><keyname>Closier</keyname><forenames>J.</forenames></author><author><keyname>van Herwijnen</keyname><forenames>E.</forenames></author><author><keyname>Blaising</keyname><forenames>J. J.</forenames></author><author><keyname>Boutigny</keyname><forenames>D.</forenames></author><author><keyname>Charlot</keyname><forenames>C.</forenames></author><author><keyname>Garonne</keyname><forenames>V.</forenames></author><author><keyname>Tsaregorodtsev</keyname><forenames>A.</forenames></author><author><keyname>Bos</keyname><forenames>K.</forenames></author><author><keyname>Templon</keyname><forenames>J.</forenames></author><author><keyname>Capiluppi</keyname><forenames>P.</forenames></author><author><keyname>Fanfani</keyname><forenames>A.</forenames></author><author><keyname>Barbera</keyname><forenames>R.</forenames></author><author><keyname>Negri</keyname><forenames>G.</forenames></author><author><keyname>Perini</keyname><forenames>L.</forenames></author><author><keyname>Resconi</keyname><forenames>S.</forenames></author><author><keyname>Sitta</keyname><forenames>M.</forenames></author><author><keyname>Reale</keyname><forenames>M.</forenames></author><author><keyname>Vicinanza</keyname><forenames>D.</forenames></author><author><keyname>Bagnasco</keyname><forenames>S.</forenames></author><author><keyname>Cerello</keyname><forenames>P.</forenames></author><author><keyname>Sciaba</keyname><forenames>A.</forenames></author><author><keyname>Smirnova</keyname><forenames>O.</forenames></author><author><keyname>Colling</keyname><forenames>D.</forenames></author><author><keyname>Harris</keyname><forenames>F.</forenames></author><author><keyname>Burke</keyname><forenames>S.</forenames></author></authors><title>HEP Applications Evaluation of the EDG Testbed and Middleware</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  Conference (CHEP03), La Jolla, CA, USA, March 2003, 7 pages. PSN THCT003</comments><acm-class>J.2</acm-class><journal-ref>ECONF C0303241:THCT003,2003</journal-ref><abstract>  Workpackage 8 of the European Datagrid project was formed in January 2001
with representatives from the four LHC experiments, and with experiment
independent people from five of the six main EDG partners. In September 2002
WP8 was strengthened by the addition of effort from BaBar and D0. The original
mandate of WP8 was, following the definition of short- and long-term
requirements, to port experiment software to the EDG middleware and testbed
environment. A major additional activity has been testing the basic
functionality and performance of this environment. This paper reviews
experiences and evaluations in the areas of job submission, data management,
mass storage handling, information systems and monitoring. It also comments on
the problems of remote debugging, the portability of code, and scaling problems
with increasing numbers of jobs, sites and nodes. Reference is made to the
pioneeering work of Atlas and CMS in integrating the use of the EDG Testbed
into their data challenges. A forward look is made to essential software
developments within EDG and to the necessary cooperation between EDG and LCG
for the LCG prototype due in mid 2003.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306028</id><created>2003-06-05</created><authors><author><keyname>Plaisted</keyname><forenames>David A.</forenames></author></authors><title>An Abstract Programming System</title><categories>cs.SE cs.LO</categories><comments>Internal report</comments><acm-class>D.1.2</acm-class><abstract>  The system PL permits the translation of abstract proofs of program
correctness into programs in a variety of programming languages. A programming
language satisfying certain axioms may be the target of such a translation. The
system PL also permits the construction and proof of correctness of programs in
an abstract programming language, and permits the translation of these programs
into correct programs in a variety of languages. The abstract programming
language has an imperative style of programming with assignment statements and
side-effects, to allow the efficient generation of code. The abstract programs
may be written by humans and then translated, avoiding the need to write the
same program repeatedly in different languages or even the same language. This
system uses classical logic, is conceptually simple, and permits reasoning
about nonterminating programs using Scott-Strachey style denotational
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306029</identifier>
 <datestamp>2010-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306029</id><created>2003-06-06</created><authors><author><keyname>Steinbeck</keyname><forenames>Timm M.</forenames><affiliation>Kirchhoff Institute of Physics, Ruprecht-Karls-University Heidelberg, Germany, for the ALICE Collaboration</affiliation></author><author><keyname>Lindenstruth</keyname><forenames>Volker</forenames><affiliation>Kirchhoff Institute of Physics, Ruprecht-Karls-University Heidelberg, Germany, for the ALICE Collaboration</affiliation></author><author><keyname>Tilsner</keyname><forenames>Heinz</forenames><affiliation>Kirchhoff Institute of Physics, Ruprecht-Karls-University Heidelberg, Germany, for the ALICE Collaboration</affiliation></author></authors><title>A Software Data Transport Framework for Trigger Applications on Clusters</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 8 figures (eps), PSN
  TUGT003</comments><acm-class>D.1.3</acm-class><journal-ref>ECONF C0303241:TUGT003,2003</journal-ref><abstract>  In the future ALICE heavy ion experiment at CERN's Large Hadron Collider
input data rates of up to 25 GB/s have to be handled by the High Level Trigger
(HLT) system, which has to scale them down to at most 1.25 GB/s before being
written to permanent storage. The HLT system that is being designed to cope
with these data rates consists of a large PC cluster, up to the order of a 1000
nodes, connected by a fast network. For the software that will run on these
nodes a flexible data transport and distribution software framework has been
developed. This framework consists of a set of separate components, that can be
connected via a common interface, allowing to construct different
configurations for the HLT, that are even changeable at runtime. To ensure a
fault-tolerant operation of the HLT, the framework includes a basic fail-over
mechanism that will be further expanded in the future, utilizing the runtime
reconnection feature of the framework's component interface. First performance
tests show very promising results for the software, indicating that it can
achieve an event rate for the data transport sufficiently high to satisfy
ALICE's requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306030</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306030</id><created>2003-06-06</created><authors><author><keyname>McNab</keyname><forenames>A.</forenames></author></authors><title>Grid-based access control for Unix environments, Filesystems and Web
  Sites</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 3 pages, LaTex. PSN TUBT008</comments><acm-class>K.6.5</acm-class><journal-ref>ECONFC0303241:TUBT008,2003</journal-ref><abstract>  The EU DataGrid has deployed a grid testbed at approximately 20 sites across
Europe, with several hundred registered users. This paper describes
authorisation systems produced by GridPP and currently used on the EU DataGrid
Testbed, including local Unix pool accounts and fine-grained access control
with Access Control Lists and Grid-aware filesystems, fileservers and web
developement environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306031</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306031</id><created>2003-06-06</created><authors><author><keyname>Frailis</keyname><forenames>Marco</forenames></author><author><keyname>Giannitrapani</keyname><forenames>Riccardo</forenames></author></authors><title>The FRED Event Display: an Extensible HepRep Client for GLAST</title><categories>cs.GR</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 3 eps figures. PSN
  MOLT010</comments><acm-class>I.3.2,I.3.3,I.3.7</acm-class><journal-ref>ECONFC0303241:MOLT010,2003</journal-ref><abstract>  A new graphics client prototype for the HepRep protocol is presented. Based
on modern toolkits and high level languages (C++ and Ruby), Fred is an
experiment to test applicability of scripting facilities to the high energy
physics event display domain. Its flexible structure, extensibility and the use
of the HepRep protocol are key features for its use in the astroparticle
experiment GLAST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306032</id><created>2003-06-06</created><authors><author><keyname>Hughes</keyname><forenames>James</forenames></author><author><keyname>Tannenbaum</keyname><forenames>Allen</forenames></author></authors><title>Length-Based Attacks for Certain Group Based Encryption Rewriting
  Systems</title><categories>cs.CR</categories><acm-class>E.3</acm-class><journal-ref>J. Hughes, A Tannenbaum, Length-Based Attacks for Certain Group
  Based Encryption Rewriting Systems, Workshop SECI02 SEcurite de la
  Communication sur Intenet, September, 2002, Tunis, Tunisa</journal-ref><abstract>  In this note, we describe a probabilistic attack on public key cryptosystems
based on the word/conjugacy problems for finitely presented groups of the type
proposed recently by Anshel, Anshel and Goldfeld. In such a scheme, one makes
use of the property that in the given group the word problem has a polynomial
time solution, while the conjugacy problem has no known polynomial solution. An
example is the braid group from topology in which the word problem is solvable
in polynomial time while the only known solutions to the conjugacy problem are
exponential. The attack in this paper is based on having a canonical
representative of each string relative to which a length function may be
computed. Hence the term length attack. Such canonical representatives are
known to exist for the braid group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306033</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306033</id><created>2003-06-06</created><authors><author><keyname>Kehagias</keyname><forenames>Ath.</forenames></author><author><keyname>Serafimidis</keyname><forenames>K.</forenames></author></authors><title>Multi-valued Connectives for Fuzzy Sets</title><categories>cs.OH</categories><acm-class>F.4.1</acm-class><abstract>  We present a procedure for the construction of multi-valued t-norms and
t-conorms. Our procedure makes use of a pair of single-valued t-norms and the
respective dual t-conorms and produces interval-valued t-norms and t-conorms.
In this manner we combine desirable characteristics of different t-norms and
t-conorms; if we use the t-norm min and t-conorm max, then the resulting
structure is a superlattice, i.e. the multivalued analog of a lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306034</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306034</id><created>2003-06-07</created><authors><author><keyname>Tanenbaum</keyname><forenames>William</forenames></author></authors><title>A ROOT/IO Based Software Framework for CMS</title><categories>cs.DB</categories><acm-class>E.1</acm-class><journal-ref>ECONFC0303241:TUKT010,2003</journal-ref><abstract>  The implementation of persistency in the Compact Muon Solenoid (CMS) Software
Framework uses the core I/O functionality of ROOT. We will discuss the current
ROOT/IO implementation, its evolution from the prior Objectivity/DB
implementation, and the plans and ongoing work for the conversion to &quot;POOL&quot;,
provided by the LHC Computing Grid (LCG) persistency project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306035</id><created>2003-06-07</created><updated>2003-06-15</updated><authors><author><keyname>Sakharov</keyname><forenames>Alexander</forenames></author></authors><title>A Transformational Decision Procedure for Non-Clausal Propositional
  Formulas</title><categories>cs.LO cs.CC</categories><comments>12 pages</comments><acm-class>F.4.1; F.2.2</acm-class><abstract>  A decision procedure for detecting valid propositional formulas is presented.
It is based on the Davis-Putnam method and deals with propositional formulas
that are initially converted to negational normal form. This procedure splits
variables but, in contrast to other decision procedures based on the
Davis-Putnam method, it does not branch. Instead, this procedure iteratively
makes validity-preserving transformations of fragments of the formula. The
transformations involve only a minimal formula part containing occurrences of
the selected variable. Selection of the best variable for splitting is crucial
in this decision procedure - it may shorten the decision process dramatically.
A variable whose splitting leads to a minimal size of the transformed formula
is selected. Also, the decision procedure performs plenty of optimizations
based on calculation of delta-sets. Some optimizations lead to removing
fragments of the formula. Others detect variables for which a single truth
value assignment is sufficient. The latest information about this research can
be found at http://www.sakharov.net/valid.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306036</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306036</id><created>2003-06-07</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Sequence Prediction based on Monotone Complexity</title><categories>cs.AI cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>17 pages</comments><report-no>IDSIA-09-03</report-no><acm-class>I.2</acm-class><journal-ref>Proceedings of the 16th Annual Conference on Learning Theory
  (COLT-2003) 506-521</journal-ref><abstract>  This paper studies sequence prediction based on the monotone Kolmogorov
complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is
extremely close to Solomonoff's prior M, the latter being an excellent
predictor in deterministic as well as probabilistic environments, where
performance is measured in terms of convergence of posteriors or losses.
Despite this closeness to M, it is difficult to assess the prediction quality
of m, since little is known about the closeness of their posteriors, which are
the important quantities for prediction. We show that for deterministic
computable environments, the &quot;posterior&quot; and losses of m converge, but rapid
convergence could only be shown on-sequence; the off-sequence behavior is
unclear. In probabilistic environments, neither the posterior nor the losses
converge, in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306037</id><created>2003-06-09</created><authors><author><keyname>Afanasiev</keyname><forenames>F.</forenames></author><author><keyname>Petrov</keyname><forenames>A.</forenames></author><author><keyname>Grachev</keyname><forenames>V.</forenames></author><author><keyname>Sukhov</keyname><forenames>A.</forenames></author></authors><title>Flow-based analysis of Internet traffic</title><categories>cs.NI</categories><comments>8 pages, 2 figures</comments><report-no>SamGAPS-03-14</report-no><acm-class>C.2.5, C.2.3, C.4</acm-class><journal-ref>published in Russian Edition of Network Computing, 5(98), 2003,
  pp.92-95</journal-ref><abstract>  We propose flow-based analysis to estimate quality of an Internet connection.
Using results from the queuing theory we compare two expressions for backbone
traffic that have different scopes of applicability. A curve that shows
dependence of utilization of a link on a number of active flows in it describes
different states of the network. We propose a methodology for plotting such a
curve using data received from a Cisco router by NetFlow protocol, determining
the working area and the overloading point of the network. Our test is an easy
way to find a moment for upgrading the backbone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306038</id><created>2003-06-09</created><authors><author><keyname>Long</keyname><forenames>Bruce</forenames></author></authors><title>Quanta: a Language for Modeling and Manipulating Information Structures</title><categories>cs.LO cs.PL</categories><acm-class>I.1.3; D.3.2; F.4.1</acm-class><abstract>  We present a theory for modeling the structure of information and a language
(Quanta) expressing the theory. Unlike Shannon's information theory, which
focuses on the amount of information in an information system, we focus on the
structure of the information in the system. For example, we can model the
information structure corresponding to an algorithm or a physical process such
as the structure of a quantum interaction. After a brief discussion of the
relation between an evolving state-system and an information structure, we
develop an algebra of information pieces (infons) to represent the structure of
systems where descriptions of complex systems are constructed from expressions
involving descriptions of simpler information systems. We map the theory to the
Von Neumann computing model of sequences/conditionals/repetitions, and to the
class/object theory of object-oriented programming (OOP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306039</id><created>2003-06-10</created><authors><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Bayesian Information Extraction Network</title><categories>cs.CL cs.AI cs.IR</categories><comments>6 pages</comments><acm-class>C.1.3; I.5.1; I.7.2; I.2.7</acm-class><journal-ref>Intl. Joint Conference on Artificial Intelligence, 2003</journal-ref><abstract>  Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various
aspects of language in one model. Many existing algorithms developed for
learning and inference in DBNs are applicable to probabilistic language
modeling. To demonstrate the potential of DBNs for natural language processing,
we employ a DBN in an information extraction task. We show how to assemble
wealth of emerging linguistic instruments for shallow parsing, syntactic and
semantic tagging, morphological decomposition, named entity recognition etc. in
order to incrementally build a robust information extraction system. Our method
outperforms previously published results on an established benchmark domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306040</id><created>2003-06-10</created><authors><author><keyname>Simons</keyname><forenames>Gary</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>The Open Language Archives Community: An infrastructure for distributed
  archiving of language resources</title><categories>cs.CL cs.DL</categories><comments>10 pages, 2 figures</comments><acm-class>H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5</acm-class><abstract>  New ways of documenting and describing language via electronic media coupled
with new ways of distributing the results via the World-Wide Web offer a degree
of access to language resources that is unparalleled in history. At the same
time, the proliferation of approaches to using these new technologies is
causing serious problems relating to resource discovery and resource creation.
This article describes the infrastructure that the Open Language Archives
Community (OLAC) has built in order to address these problems. Its technical
and usage infrastructures address problems of resource discovery by
constructing a single virtual library of distributed resources. Its governance
infrastructure addresses problems of resource creation by providing a mechanism
through which the language-resource community can express its consensus on
recommended best practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306041</id><created>2003-06-10</created><authors><author><keyname>Degtyarev</keyname><forenames>Anatoly</forenames></author><author><keyname>Fisher</keyname><forenames>Michael</forenames></author><author><keyname>Konev</keyname><forenames>Boris</forenames></author></authors><title>Monodic temporal resolution</title><categories>cs.LO</categories><comments>38 pages, 3 figures</comments><acm-class>I.2.3; F.4.1</acm-class><abstract>  Until recently, First-Order Temporal Logic (FOTL) has been little understood.
While it is well known that the full logic has no finite axiomatisation, a more
detailed analysis of fragments of the logic was not previously available.
However, a breakthrough by Hodkinson et.al., identifying a finitely
axiomatisable fragment, termed the monodic fragment, has led to improved
understanding of FOTL. Yet, in order to utilise these theoretical advances, it
is important to have appropriate proof techniques for the monodic fragment.
  In this paper, we modify and extend the clausal temporal resolution
technique, originally developed for propositional temporal logics, to enable
its use in such monodic fragments. We develop a specific normal form for
formulae in FOTL, and provide a complete resolution calculus for formulae in
this form. Not only is this clausal resolution technique useful as a practical
proof technique for certain monodic classes, but the use of this approach
provides us with increased understanding of the monodic fragment. In
particular, we here show how several features of monodic FOTL are established
as corollaries of the completeness result for the clausal temporal resolution
method. These include definitions of new decidable monodic classes,
simplification of existing monodic classes by reductions, and completeness of
clausal temporal resolution in the case of monodic logics with expanding
domains, a case with much significance in both theory and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306042</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306042</id><created>2003-06-10</created><authors><author><keyname>Alverson</keyname><forenames>George</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author><author><keyname>Osborne</keyname><forenames>Ianna</forenames></author><author><keyname>Tuura</keyname><forenames>Lassi A.</forenames></author><author><keyname>Taylor</keyname><forenames>Lucas</forenames></author></authors><title>IGUANA Architecture, Framework and Toolkit for Interactive Graphics</title><categories>cs.SE cs.GR</categories><comments>Presented at the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages LaTeX, 4 eps figures. PSN
  MOLT008 More and higher res figs at
  http://iguana.web.cern.ch/iguana/snapshot/main/gallery.html</comments><acm-class>D.2.11;I.3.8;J.2</acm-class><journal-ref>ECONFC0303241:MOLT008,2003</journal-ref><abstract>  IGUANA is a generic interactive visualisation framework based on a C++
component model. It provides powerful user interface and visualisation
primitives in a way that is not tied to any particular physics experiment or
detector design. The article describes interactive visualisation tools built
using IGUANA for the CMS and D0 experiments, as well as generic GEANT4 and
GEANT3 applications. It covers features of the graphical user interfaces, 3D
and 2D graphics, high-quality vector graphics output for print media, various
textual, tabular and hierarchical data views, and integration with the
application through control panels, a command line and different
multi-threading models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306043</id><created>2003-06-10</created><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Shah</keyname><forenames>Gauri</forenames></author></authors><title>Skip Graphs</title><categories>cs.DS cs.DC</categories><comments>36 pages, 12 figures. Full version of paper appearing in SODA 2003</comments><acm-class>C.2.4; E.1</acm-class><abstract>  Skip graphs are a novel distributed data structure, based on skip lists, that
provide the full functionality of a balanced tree in a distributed system where
resources are stored in separate nodes that may fail at any time. They are
designed for use in searching peer-to-peer systems, and by providing the
ability to perform queries based on key ordering, they improve on existing
search tools that provide only hash table functionality. Unlike skip lists or
other tree data structures, skip graphs are highly resilient, tolerating a
large fraction of failed nodes without losing connectivity. In addition,
constructing, inserting new nodes into, searching a skip graph, and detecting
and repairing errors in the data structure introduced by node failures can be
done using simple and straightforward algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306044</id><created>2003-06-10</created><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Waarts</keyname><forenames>Orli</forenames></author></authors><title>Compositional competitiveness for distributed algorithms</title><categories>cs.DS cs.DC</categories><comments>33 pages, 2 figures; full version of STOC 96 paper titled &quot;Modular
  competitiveness for distributed algorithms.&quot;</comments><acm-class>F.1.2; F.2.m</acm-class><abstract>  We define a measure of competitive performance for distributed algorithms
based on throughput, the number of tasks that an algorithm can carry out in a
fixed amount of work. This new measure complements the latency measure of Ajtai
et al., which measures how quickly an algorithm can finish tasks that start at
specified times. The novel feature of the throughput measure, which
distinguishes it from the latency measure, is that it is compositional: it
supports a notion of algorithms that are competitive relative to a class of
subroutines, with the property that an algorithm that is k-competitive relative
to a class of subroutines, combined with an l-competitive member of that class,
gives a combined algorithm that is kl-competitive.
  In particular, we prove the throughput-competitiveness of a class of
algorithms for collect operations, in which each of a group of n processes
obtains all values stored in an array of n registers. Collects are a
fundamental building block of a wide variety of shared-memory distributed
algorithms, and we show that several such algorithms are competitive relative
to collects. Inserting a competitive collect in these algorithms gives the
first examples of competitive distributed algorithms obtained by composition
using a general construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306045</id><created>2003-06-11</created><authors><author><keyname>Donno</keyname><forenames>Flavia</forenames></author><author><keyname>Ciaschini</keyname><forenames>Vincenzo</forenames></author><author><keyname>Rebatto</keyname><forenames>David</forenames></author><author><keyname>Vaccarossa</keyname><forenames>Luca</forenames></author><author><keyname>Verlato</keyname><forenames>Marco</forenames></author></authors><title>The WorldGrid transatlantic testbed: a successful example of Grid
  interoperability across EU and U.S. domains</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, PDF, 3. PSN THCT006</comments><acm-class>D.2.12</acm-class><abstract>  The European DataTAG project has taken a major step towards making the
concept of a worldwide computing Grid a reality. In collaboration with the
companion U.S. project iVDGL, DataTAG has realized an intercontinental testbed
spanning Europe and the U.S. integrating architecturally different Grid
implementations based on the Globus toolkit. The WorldGrid testbed has been
successfully demonstrated at SuperComputing 2002 and IST2002 where real HEP
application jobs were transparently submitted from U.S. and Europe using native
mechanisms and run where resources were available, independently of their
location. In this paper we describe the architecture of the WorldGrid testbed,
the problems encountered and the solutions taken in realizing such a testbed.
With our work we present an important step towards interoperability of Grid
middleware developed and deployed in Europe and the U.S.. Some of the solutions
developed in WorldGrid will be adopted by the LHC Computing Grid first service.
To the best of our knowledge, this is the first large-scale testbed that
combines middleware components and makes them work together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306046</id><created>2003-06-11</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Compact Approximation of Lattice Functions with Applications to
  Large-Alphabet Text Search</title><categories>cs.DS</categories><report-no>292-03</report-no><acm-class>E.2</acm-class><abstract>  We propose a very simple randomised data structure that stores an
approximation from above of a lattice-valued function. Computing the function
value requires a constant number of steps, and the error probability can be
balanced with space usage, much like in Bloom filters. The structure is
particularly well suited for functions that are bottom on most of their domain.
We then show how to use our methods to store in a compact way the bad-character
shift function for variants of the Boyer-Moore text search algorithms. As a
result, we obtain practical implementations of these algorithms that can be
used with large alphabets, such as Unicode collation elements, with a small
setup time. The ideas described in this paper have been implemented as free
software under the GNU General Public License within the MG4J project
(http://mg4j.dsi.unimi.it/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306047</id><created>2003-06-11</created><authors><author><keyname>Patton</keyname><forenames>S.</forenames></author></authors><title>Concrete uses of XML in software development and data analysis</title><categories>cs.SE cs.GL</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, 8 Figures, LaTeX. PSN
  MOJT004</comments><acm-class>D.2.0</acm-class><abstract>  XML is now becoming an industry standard for data description and exchange.
Despite this there are still some questions about how or if this technology can
be useful in High Energy Physics software development and data analysis. This
paper aims to answer these questions by demonstrating how XML is used in the
IceCube software development system, data handling and analysis. It does this
by first surveying the concepts and tools that make up the XML technology. It
then goes on to discuss concrete examples of how these concepts and tools are
used to speed up software development in IceCube and what are the benefits of
using XML in IceCube's data handling and analysis chain. The overall aim of
this paper it to show that XML does have many benefits to bring High Energy
Physics software development and data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306048</id><created>2003-06-11</created><authors><author><keyname>Li</keyname><forenames>Jianwei</forenames></author><author><keyname>Liao</keyname><forenames>Wei-keng</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author><author><keyname>Ross</keyname><forenames>Robert</forenames></author><author><keyname>Thakur</keyname><forenames>Rajeev</forenames></author><author><keyname>Gropp</keyname><forenames>William</forenames></author><author><keyname>Latham</keyname><forenames>Rob</forenames></author></authors><title>Parallel netCDF: A Scientific High-Performance I/O Interface</title><categories>cs.DC</categories><comments>10 pages,7 figures</comments><report-no>Preprint ANL/MCS-P1048-0503</report-no><acm-class>D.1.3</acm-class><abstract>  Dataset storage, exchange, and access play a critical role in scientific
applications. For such purposes netCDF serves as a portable and efficient file
format and programming interface, which is popular in numerous scientific
application domains. However, the original interface does not provide an
efficient mechanism for parallel data storage and access. In this work, we
present a new parallel interface for writing and reading netCDF datasets. This
interface is derived with minimum changes from the serial netCDF interface but
defines semantics for parallel access and is tailored for high performance. The
underlying parallel I/O is achieved through MPI-IO, allowing for dramatic
performance gains through the use of collective I/O optimizations. We compare
the implementation strategies with HDF5 and analyze both. Our tests indicate
programming convenience and significant I/O performance improvement with this
parallel netCDF interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306049</id><created>2003-06-11</created><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>Hyperdense Coding Modulo 6 with Filter-Machines</title><categories>cs.CC cs.DB</categories><acm-class>F.1.1</acm-class><abstract>  We show how one can encode $n$ bits with $n^{o(1)}$ ``wave-bits'' using still
hypothetical filter-machines (here $o(1)$ denotes a positive quantity which
goes to 0 as $n$ goes to infity). Our present result - in a completely
different computational model - significantly improves on the quantum
superdense-coding breakthrough of Bennet and Wiesner (1992) which encoded $n$
bits by $\lceil{n/2}\rceil$ quantum-bits. We also show that our earlier
algorithm (Tech. Rep. TR03-001, ECCC, See
ftp://ftp.eccc.uni-trier.de/pub/eccc/reports/2003/TR03-001/index.html) which
used $n^{o(1)}$ muliplication for computing a representation of the dot-product
of two $n$-bit sequences modulo 6, and, similarly, an algorithm for computing a
representation of the multiplication of two $n\times n$ matrices with
$n^{2+o(1)}$ multiplications can be turned to algorithms computing the exact
dot-product or the exact matrix-product with the same number of multiplications
with filter-machines. With classical computation, computing the dot-product
needs $\Omega(n)$ multiplications and the best known algorithm for matrix
multiplication (D. Coppersmith and S. Winograd, Matrix multiplication via
arithmetic progressions, J. Symbolic Comput., 9(3):251--280, 1990) uses
$n^{2.376}$ multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306050</id><created>2003-06-12</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author><author><keyname>De Meulder</keyname><forenames>Fien</forenames></author></authors><title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named
  Entity Recognition</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of CoNLL-2003, Edmonton, Canada, 2003, pp. 142-147</journal-ref><abstract>  We describe the CoNLL-2003 shared task: language-independent named entity
recognition. We give background information on the data sets (English and
German) and the evaluation method, present a general overview of the systems
that have taken part in the task and discuss their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306051</id><created>2003-06-12</created><updated>2003-09-03</updated><authors><author><keyname>Manabe</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Kohki</forenames></author><author><keyname>Itoh</keyname><forenames>Yoshihiko</forenames></author><author><keyname>Kawabata</keyname><forenames>Setsuya</forenames></author><author><keyname>Mashimo</keyname><forenames>Tetsuro</forenames></author><author><keyname>Morita</keyname><forenames>Youhei</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author><author><keyname>Sasaki</keyname><forenames>Takashi</forenames></author><author><keyname>Sato</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Tanaka</keyname><forenames>Junichi</forenames></author><author><keyname>Ueda</keyname><forenames>Ikuo</forenames></author><author><keyname>Watase</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Satomi</forenames></author><author><keyname>Yashiro</keyname><forenames>Shigeo</forenames></author></authors><title>A data Grid testbed environment in Gigabit WAN with HPSS</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 9 figures, PSN
  THCT002</comments><acm-class>C.2.4;J.2;H.3.4</acm-class><abstract>  For data analysis of large-scale experiments such as LHC Atlas and other
Japanese high energy and nuclear physics projects, we have constructed a Grid
test bed at ICEPP and KEK. These institutes are connected to national
scientific gigabit network backbone called SuperSINET. In our test bed, we have
installed NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK
as a large scale data store. Atlas simulation data at ICEPP has been
transferred and accessed using SuperSINET. We have tested various performances
and characteristics of HPSS through this high speed WAN. The measurement
includes comparison between computing and storage resources are tightly coupled
with low latency LAN and long distant WAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306052</id><created>2003-06-12</created><authors><author><keyname>Poulard</keyname><forenames>Gilbert</forenames></author></authors><title>ATLAS Data Challenge 1</title><categories>cs.DC</categories><comments>10 pages; 3 figures; CHEP03 Conference, San Diego; Reference MOCT005</comments><acm-class>J.2</acm-class><abstract>  In 2002 the ATLAS experiment started a series of Data Challenges (DC) of
which the goals are the validation of the Computing Model, of the complete
software suite, of the data model, and to ensure the correctness of the
technical choices to be made. A major feature of the first Data Challenge (DC1)
was the preparation and the deployment of the software required for the
production of large event samples for the High Level Trigger (HLT) and physics
communities, and the production of those samples as a world-wide distributed
activity. The first phase of DC1 was run during summer 2002, and involved 39
institutes in 18 countries. More than 10 million physics events and 30 million
single particle events were fully simulated. Over a period of about 40 calendar
days 71000 CPU-days were used producing 30 Tbytes of data in about 35000
partitions. In the second phase the next processing step was performed with the
participation of 56 institutes in 21 countries (~ 4000 processors used in
parallel). The basic elements of the ATLAS Monte Carlo production system are
described. We also present how the software suite was validated and the
participating sites were certified. These productions were already partly
performed by using different flavours of Grid middleware at ~ 20 sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306053</id><created>2003-06-12</created><authors><author><keyname>Pearlman</keyname><forenames>Laura</forenames></author><author><keyname>Welch</keyname><forenames>Von</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author><author><keyname>Kesselman</keyname><forenames>Carl</forenames></author><author><keyname>Tuecke</keyname><forenames>Steven</forenames></author></authors><title>A Community Authorization Service for Group Collaboration</title><categories>cs.DC cs.CR</categories><comments>10 pages,2 figures</comments><report-no>Preprint ANL/MCS-P1042-0502</report-no><acm-class>C.2.4</acm-class><abstract>  In &quot;Grids&quot; and &quot;collaboratories,&quot; we find distributed communities of resource
providers and resource consumers, within which often complex and dynamic
policies govern who can use which resources for which purpose. We propose a new
approach to the representation, maintenance, and enforcement of such policies
that provides a scalable mechanism for specifying and enforcing these policies.
Our approach allows resource providers to delegate some of the authority for
maintaining fine-grained access control policies to communities, while still
maintaining ultimate control over their resources. We also describe a prototype
implementation of this approach and an application in a data management
context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306054</id><created>2003-06-12</created><authors><author><keyname>Chamont</keyname><forenames>D.</forenames></author><author><keyname>Charlot</keyname><forenames>C.</forenames></author></authors><title>OVAL: the CMS Testing Robot</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 0 eps figures. PSN
  MOJT005</comments><acm-class>D.2.5</acm-class><abstract>  Oval is a testing tool which help developers to detect unexpected changes in
the behavior of their software. It is able to automatically compile some test
programs, to prepare on the fly the needed configuration files, to run the
tests within a specified Unix environment, and finally to analyze the output
and check expectations. Oval does not provide utility code to help writing the
tests, therefore it is quite independant of the programming/scripting language
of the software to be tested. It can be seen as a kind of robot which apply the
tests and warn about any unexpected change in the output. Oval was developed by
the LLR laboratory for the needs of the CMS experiment, and it is now
recommended by the CERN LCG project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306055</id><created>2003-06-12</created><authors><author><keyname>Mans</keyname><forenames>Jeremiah</forenames></author><author><keyname>Bengali</keyname><forenames>David</forenames></author></authors><title>BlueOx: A Java Framework for Distributed Data Analysis</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 1 eps figure. PSN
  TULT006</comments><acm-class>J.2;D.4.7</acm-class><abstract>  High energy physics experiments including those at the Tevatron and the
upcoming LHC require analysis of large data sets which are best handled by
distributed computation. We present the design and development of a distributed
data analysis framework based on Java. Analysis jobs run through three phases:
discovery of data sets available, brokering/assignment of data sets to analysis
servers, and job execution. Each phase is represented by a set of abstract
interfaces. These interfaces allow different techniques to be used without
modification to the framework. For example, the communications interface has
been implemented by both a packet protocol and a SOAP-based scheme. User
authentication can be provided either through simple passwords or through a GSI
certificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a
hypothetical high-energy linear collider experiment have been interfaced with
the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306056</id><created>2003-06-12</created><authors><author><keyname>Chamont</keyname><forenames>D.</forenames></author><author><keyname>Charlot</keyname><forenames>C.</forenames></author></authors><title>Twelve Ways to Build CMS Crossings from ROOT Files</title><categories>cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 1 eps figures. PSN
  TUKT004</comments><acm-class>H.2.4</acm-class><abstract>  The simulation of CMS raw data requires the random selection of one hundred
and fifty pileup events from a very large set of files, to be superimposed in
memory to the signal event. The use of ROOT I/O for that purpose is quite
unusual: the events are not read sequentially but pseudo-randomly, they are not
processed one by one in memory but by bunches, and they do not contain orthodox
ROOT objects but many foreign objects and templates. In this context, we have
compared the performance of ROOT containers versus the STL vectors, and the use
of trees versus a direct storage of containers. The strategy with best
performances is by far the one using clones within trees, but it stays hard to
tune and very dependant on the exact use-case. The use of STL vectors could
bring more easily similar performances in a future ROOT release.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306057</id><created>2003-06-12</created><authors><author><keyname>Patton</keyname><forenames>S.</forenames></author><author><keyname>Glowacki</keyname><forenames>D.</forenames></author></authors><title>IceCube's Development Environment</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, 9 Figures, LaTeX. PSN
  MONT001</comments><acm-class>D.2.6</acm-class><abstract>  When the IceCube experiment started serious software development it needed a
development environment in which both its developers and clients could work and
that would encourage and support a good software development process. Some of
the key features that IceCube wanted in such a environment were: the separation
of the configuration and build tools; inclusion of an issue tracking system;
support for the Unified Change Model; support for unit testing; and support for
continuous building. No single, affordable, off the shelf, environment offered
all these features. However there are many open source tools that address
subsets of these feature, therefore IceCube set about selecting those tools
which it could use in developing its own environment and adding its own tools
where no suitable tools were found. This paper outlines the tools that where
chosen, what are their responsibilities in the development environment and how
they fit together. The complete environment will be demonstrated with a walk
through of single cycle of the development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306058</id><created>2003-06-12</created><authors><author><keyname>Bahyl</keyname><forenames>Vladimir</forenames></author><author><keyname>Chardi</keyname><forenames>Benjamin</forenames></author><author><keyname>van Eldik</keyname><forenames>Jan</forenames></author><author><keyname>Fuchs</keyname><forenames>Ulrich</forenames></author><author><keyname>Kleinwort</keyname><forenames>Thorsten</forenames></author><author><keyname>Murth</keyname><forenames>Martin</forenames></author><author><keyname>Smith</keyname><forenames>Tim</forenames></author></authors><title>Installing, Running and Maintaining Large Linux Clusters at CERN</title><categories>cs.DC</categories><comments>5 pages, Proceedings for the CHEP 2003 conference, La Jolla,
  California, March 24 - 28, 2003</comments><acm-class>C.5.3; K.6.3</acm-class><abstract>  Having built up Linux clusters to more than 1000 nodes over the past five
years, we already have practical experience confronting some of the LHC scale
computing challenges: scalability, automation, hardware diversity, security,
and rolling OS upgrades. This paper describes the tools and processes we have
implemented, working in close collaboration with the EDG project [1],
especially with the WP4 subtask, to improve the manageability of our clusters,
in particular in the areas of system installation, configuration, and
monitoring. In addition to the purely technical issues, providing shared
interactive and batch services which can adapt to meet the diverse and changing
requirements of our users is a significant challenge. We describe the
developments and tuning that we have introduced on our LSF based systems to
maximise both responsiveness to users and overall system utilisation. Finally,
this paper will describe the problems we are facing in enlarging our
heterogeneous Linux clusters, the progress we have made in dealing with the
current issues and the steps we are taking to gridify the clusters
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306059</id><created>2003-06-12</created><authors><author><keyname>Perl</keyname><forenames>J.</forenames></author><author><keyname>Giannitrapani</keyname><forenames>R.</forenames></author><author><keyname>Frailis</keyname><forenames>M.</forenames></author></authors><title>The Use of HepRep in GLAST</title><categories>cs.GR</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 9 pages pdf, 15 figures. PSN THLT009</comments><report-no>SLAC-PUB-9908</report-no><acm-class>I.3.2; I.3.4; I.3.6</acm-class><abstract>  HepRep is a generic, hierarchical format for description of graphics
representables that can be augmented by physics information and relational
properties. It was developed for high energy physics event display applications
and is especially suited to client/server or component frameworks. The GLAST
experiment, an international effort led by NASA for a gamma-ray telescope to
launch in 2006, chose HepRep to provide a flexible, extensible and maintainable
framework for their event display without tying their users to any one graphics
application. To support HepRep in their GUADI infrastructure, GLAST developed a
HepRep filler and builder architecture. The architecture hides the details of
XML and CORBA in a set of base and helper classes allowing physics experts to
focus on what data they want to represent. GLAST has two GAUDI services:
HepRepSvc, which registers HepRep fillers in a global registry and allows the
HepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be
published through a CORBA interface and which allows the client application to
feed commands back to GAUDI (such as start next event, or run some GAUDI
algorithm). GLAST's HepRep solution gives users a choice of client
applications, WIRED (written in Java) or FRED (written in C++ and Ruby), and
leaves them free to move to any future HepRep-compliant event display.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306060</id><created>2003-06-12</created><authors><author><keyname>Brook</keyname><forenames>N.</forenames></author><author><keyname>Bogdanchikov</keyname><forenames>A.</forenames></author><author><keyname>Buckley</keyname><forenames>A.</forenames></author><author><keyname>Closier</keyname><forenames>J.</forenames></author><author><keyname>Egede</keyname><forenames>U.</forenames></author><author><keyname>Frank</keyname><forenames>M.</forenames></author><author><keyname>Galli</keyname><forenames>D.</forenames></author><author><keyname>Gandelman</keyname><forenames>M.</forenames></author><author><keyname>Garonne</keyname><forenames>V.</forenames></author><author><keyname>Gaspar</keyname><forenames>C.</forenames></author><author><keyname>Diaz</keyname><forenames>R. Graciani</forenames></author><author><keyname>Harrison</keyname><forenames>K.</forenames></author><author><keyname>van Herwijnen</keyname><forenames>E.</forenames></author><author><keyname>Khan</keyname><forenames>A.</forenames></author><author><keyname>Klous</keyname><forenames>S.</forenames></author><author><keyname>Korolko</keyname><forenames>I.</forenames></author><author><keyname>Kuznetsov</keyname><forenames>G.</forenames></author><author><keyname>Loverre</keyname><forenames>F.</forenames></author><author><keyname>Marconi</keyname><forenames>U.</forenames></author><author><keyname>Palacios</keyname><forenames>J. P.</forenames></author><author><keyname>Patrick</keyname><forenames>G. N.</forenames></author><author><keyname>Pickford</keyname><forenames>A.</forenames></author><author><keyname>Ponce</keyname><forenames>S.</forenames></author><author><keyname>Romanovski</keyname><forenames>V.</forenames></author><author><keyname>Saborido</keyname><forenames>J. J.</forenames></author><author><keyname>Schmelling</keyname><forenames>M.</forenames></author><author><keyname>Soroko</keyname><forenames>A.</forenames></author><author><keyname>Tsaregorodtsev</keyname><forenames>A.</forenames></author><author><keyname>Vagnoni</keyname><forenames>V.</forenames></author><author><keyname>Washbrook</keyname><forenames>A.</forenames></author></authors><title>DIRAC - Distributed Infrastructure with Remote Agent Control</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, Word, 5 figures. PSN
  TUAT006</comments><acm-class>C.2.4</acm-class><abstract>  This paper describes DIRAC, the LHCb Monte Carlo production system. DIRAC has
a client/server architecture based on: Compute elements distributed among the
collaborating institutes; Databases for production management, bookkeeping (the
metadata catalogue) and software configuration; Monitoring and cataloguing
services for updating and accessing the databases. Locally installed software
agents implemented in Python monitor the local batch queue, interrogate the
production database for any outstanding production requests using the XML-RPC
protocol and initiate the job submission. The agent checks and, if necessary,
installs any required software automatically. After the job has processed the
events, the agent transfers the output data and updates the metadata catalogue.
DIRAC has been successfully installed at 18 collaborating institutes, including
the DataGRID, and has been used in recent Physics Data Challenges. In the near
to medium term future we must use a mixed environment with different types of
grid middleware or no middleware. We describe how this flexibility has been
achieved and how ubiquitously available grid middleware would improve DIRAC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306061</id><created>2003-06-12</created><authors><author><keyname>Azemoon</keyname><forenames>Tofigh</forenames></author><author><keyname>Hasan</keyname><forenames>Adil</forenames></author><author><keyname>Kroeger</keyname><forenames>Wilko</forenames></author><author><keyname>Trunov</keyname><forenames>Artem</forenames></author></authors><title>Operational Aspects of Dealing with the Large BaBar Data Set</title><categories>cs.DB cs.DC</categories><comments>Presented for Computing in High Energy Physics, San Diego, March 2003</comments><acm-class>H.2.7; E.5; J.2</acm-class><abstract>  To date, the BaBar experiment has stored over 0.7PB of data in an
Objectivity/DB database. Approximately half this data-set comprises simulated
data of which more than 70% has been produced at more than 20 collaborating
institutes outside of SLAC. The operational aspects of managing such a large
data set and providing access to the physicists in a timely manner is a
challenging and complex problem. We describe the operational aspects of
managing such a large distributed data-set as well as importing and exporting
data from geographically spread BaBar collaborators. We also describe problems
common to dealing with such large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306062</id><created>2003-06-13</created><authors><author><keyname>Dimitromanolaki</keyname><forenames>Aggeliki</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author></authors><title>Learning to Order Facts for Discourse Planning in Natural Language
  Generation</title><categories>cs.CL</categories><comments>8 pages, 4 figures, 1 table</comments><acm-class>H.5.2</acm-class><journal-ref>Proceedings of EACL 2003 Workshop on Natural Language Generation</journal-ref><abstract>  This paper presents a machine learning approach to discourse planning in
natural language generation. More specifically, we address the problem of
learning the most natural ordering of facts in discourse plans for a specific
domain. We discuss our methodology and how it was instantiated using two
different machine learning algorithms. A quantitative evaluation performed in
the domain of museum exhibit descriptions indicates that our approach performs
significantly better than manually constructed ordering rules. Being
retrainable, the resulting planners can be ported easily to other similar
domains, without requiring language technology expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306063</id><created>2003-06-13</created><authors><author><keyname>Baker</keyname><forenames>Richard</forenames></author><author><keyname>Yu</keyname><forenames>Dantong</forenames></author><author><keyname>Wlodek</keyname><forenames>Tomasz</forenames></author></authors><title>A Model for Grid User Management</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, 2 figures and 1 style file,
  PSN TUBT002</comments><acm-class>D.1.3; K.6.m; D.4.6</acm-class><abstract>  Registration and management of users in a large scale Grid computing
environment presents new challenges that are not well addressed by existing
protocols. Within a single Virtual Organization (VO), thousands of users will
potentially need access to hundreds of computing sites, and the traditional
model where users register for local accounts at each site will present
significant scaling problems. However, computing sites must maintain control
over access to the site and site policies generally require individual local
accounts for every user. We present here a model that allows users to register
once with a VO and yet still provides all of the computing sites the
information they require with the required level of trust. We have developed
tools to allow sites to automate the management of local accounts and the
mappings between Grid identities and local accounts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306064</id><created>2003-06-13</created><updated>2003-06-28</updated><authors><author><keyname>Jan</keyname><forenames>Muhammad Asif</forenames><affiliation>Centre for European Nuclear Research</affiliation></author><author><keyname>Zahid</keyname><forenames>Fahd Ali</forenames><affiliation>Foundation University, Islamabad, Pakistan</affiliation></author><author><keyname>Fraz</keyname><forenames>Mohammad Moazam</forenames><affiliation>Foundation University, Islamabad, Pakistan</affiliation></author><author><keyname>Ali</keyname><forenames>Arshad</forenames><affiliation>National University of Science and Technology, Pakistan</affiliation></author></authors><title>Exploiting peer group concept for adaptive and highly available services</title><categories>cs.DC</categories><comments>The Paper Consists of 5 pages, 6 figures submitted in Computing in
  High Energy and Nuclear Physics, 24-28 March 2003 La Jolla California. CHEP03</comments><acm-class>H 3.4</acm-class><abstract>  This paper presents a prototype for redundant, highly available and fault
tolerant peer to peer framework for data management. Peer to peer computing is
gaining importance due to its flexible organization, lack of central authority,
distribution of functionality to participating nodes and ability to utilize
unused computational resources. Emergence of GRID computing has provided much
needed infrastructure and administrative domain for peer to peer computing. The
components of this framework exploit peer group concept to scope service and
information search, arrange services and information in a coherent manner,
provide selective redundancy and ensure availability in face of failure and
high load conditions. A prototype system has been implemented using JXTA peer
to peer technology and XML is used for service description and interfaces,
allowing peers to communicate with services implemented in various platforms
including web services and JINI services. It utilizes code mobility to achieve
role interchange among services and ensure dynamic group membership. Security
is ensured by using Public Key Infrastructure (PKI) to implement group level
security policies for membership and service access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306065</id><created>2003-06-13</created><authors><author><keyname>Cioffi</keyname><forenames>C.</forenames></author><author><keyname>Eckmann</keyname><forenames>S.</forenames></author><author><keyname>Girone</keyname><forenames>M.</forenames></author><author><keyname>Hrivnac</keyname><forenames>J.</forenames></author><author><keyname>Malon</keyname><forenames>D.</forenames></author><author><keyname>Schmuecker</keyname><forenames>H.</forenames></author><author><keyname>Vaniachine</keyname><forenames>A.</forenames></author><author><keyname>Wojcieszuk</keyname><forenames>J.</forenames></author><author><keyname>Xie</keyname><forenames>Z.</forenames></author></authors><title>POOL File Catalog, Collection and Metadata Components</title><categories>cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, 1 eps figure, PSN MOKT009</comments><acm-class>H.2.4</acm-class><abstract>  The POOL project is the common persistency framework for the LHC experiments
to store petabytes of experiment data and metadata in a distributed and grid
enabled way. POOL is a hybrid event store consisting of a data streaming layer
and a relational layer. This paper describes the design of file catalog,
collection and metadata components which are not part of the data streaming
layer of POOL and outlines how POOL aims to provide transparent and efficient
data access for a wide range of environments and use cases - ranging from a
large production site down to a single disconnected laptops. The file catalog
is the central POOL component translating logical data references to physical
data files in a grid environment. POOL collections with their associated
metadata provide an abstract way of accessing experiment data via their logical
grouping into sets of related data objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306066</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306066</id><created>2003-06-13</created><authors><author><keyname>Duic</keyname><forenames>Venicio</forenames></author><author><keyname>Lamanna</keyname><forenames>Massimo</forenames></author></authors><title>The COMPASS Event Store in 2002</title><categories>cs.DB</categories><comments>Talk from the 2003 conference: &quot;Computing in High Energy and Nuclear
  Physics&quot; (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT011</comments><acm-class>H.2; H.3</acm-class><doi>10.1109/TNS.2004.832645</doi><abstract>  COMPASS, the fixed-target experiment at CERN studying the structure of the
nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All
these data, together with reconstructed events information, were put from the
beginning in a database infrastructure based on Objectivity/DB and on the
hierarchical storage manager CASTOR. The experience in the usage of the
database is reviewed and the evolution of the system outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306067</id><created>2003-06-13</created><authors><author><keyname>Buncic</keyname><forenames>P.</forenames></author><author><keyname>Saiz</keyname><forenames>P.</forenames></author><author><keyname>Peters</keyname><forenames>A. J.</forenames></author></authors><title>The AliEn system, status and perspectives</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, Word, 10 figures. PSN
  MOAT004</comments><acm-class>C.2.4; H.2.4</acm-class><abstract>  AliEn is a production environment that implements several components of the
Grid paradigm needed to simulate, reconstruct and analyse HEP data in a
distributed way. The system is built around Open Source components, uses the
Web Services model and standard network protocols to implement the computing
platform that is currently being used to produce and analyse Monte Carlo data
at over 30 sites on four continents. The aim of this paper is to present the
current AliEn architecture and outline its future developments in the light of
emerging standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306068</id><created>2003-06-13</created><authors><author><keyname>Saiz</keyname><forenames>Pablo</forenames></author><author><keyname>Buncic</keyname><forenames>Predrag</forenames></author><author><keyname>Peters</keyname><forenames>Andreas J.</forenames></author></authors><title>AliEn Resource Brokers</title><categories>cs.DC</categories><comments>5 pages, 8 figures, CHEP 03 conference</comments><acm-class>C.2.4</acm-class><abstract>  AliEn (ALICE Environment) is a lightweight GRID framework developed by the
Alice Collaboration. When the experiment starts running, it will collect data
at a rate of approximately 2 PB per year, producing O(109) files per year. All
these files, including all simulated events generated during the preparation
phase of the experiment, must be accounted and reliably tracked in the GRID
environment. The backbone of AliEn is a distributed file catalogue, which
associates universal logical file name to physical file names for each dataset
and provides transparent access to datasets independently of physical location.
The file replication and transport is carried out under the control of the File
Transport Broker. In addition, the file catalogue maintains information about
every job running in the system. The jobs are distributed by the Job Resource
Broker that is implemented using a simplified pull (as opposed to traditional
push) architecture. This paper describes the Job and File Transport Resource
Brokers and shows that a similar architecture can be applied to solve both
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306069</id><created>2003-06-13</created><authors><author><keyname>Pulliam</keyname><forenames>Teela</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Dorigo</keyname><forenames>Alvise</forenames></author></authors><title>Distributed Offline Data Reconstruction in BaBar</title><categories>cs.DC</categories><comments>CHEP03 paper, MODT012</comments><report-no>SLAC-PUB-9903</report-no><acm-class>C.3</acm-class><abstract>  The BaBar experiment at SLAC is in its fourth year of running. The data
processing system has been continuously evolving to meet the challenges of
higher luminosity running and the increasing bulk of data to re-process each
year. To meet these goals a two-pass processing architecture has been adopted,
where 'rolling calibrations' are quickly calculated on a small fraction of the
events in the first pass and the bulk data reconstruction done in the second.
This allows for quick detector feedback in the first pass and allows for the
parallelization of the second pass over two or more separate farms. This
two-pass system allows also for distribution of processing farms off-site. The
first such site has been setup at INFN Padova. The challenges met here were
many. The software was ported to a full Linux-based, commodity hardware system.
The raw dataset, 90 TB, was imported from SLAC utilizing a 155 Mbps network
link. A system for quality control and export of the processed data back to
SLAC was developed. Between SLAC and Padova we are currently running three
pass-one farms, with 32 CPUs each, and nine pass-two farms with 64 to 80 CPUs
each. The pass-two farms can process between 2 and 4 million events per day.
Details about the implementation and performance of the system will be
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306070</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306070</id><created>2003-06-13</created><updated>2003-06-20</updated><authors><author><keyname>Thompson</keyname><forenames>M.</forenames></author><author><keyname>Essiari</keyname><forenames>A.</forenames></author><author><keyname>Keahey</keyname><forenames>K.</forenames></author><author><keyname>Welch</keyname><forenames>V.</forenames></author><author><keyname>Lang</keyname><forenames>S.</forenames></author><author><keyname>Liu</keyname><forenames>B.</forenames></author></authors><title>Fine-Grained Authorization for Job and Resource Management Using Akenti
  and the Globus Toolkit</title><categories>cs.DC cs.CR</categories><comments>CHEP03, La Jolla, Mar 24-27, TUB2006, Grid Security, 7 pages, 5
  figures</comments><report-no>LBNL-52976</report-no><acm-class>C.2.4</acm-class><journal-ref>ECONF C0303241:TUBT006,2003</journal-ref><abstract>  As the Grid paradigm is adopted as a standard way of sharing remote resources
across organizational domains, the need for fine-grained access control to
these resources increases. This paper presents an authorization solution for
job submission and control, developed as part of the National Fusion
Collaboratory, that uses the Globus Toolkit 2 and the Akenti authorization
service in order to perform fine-grained authorization of job and resource
management requests in a Grid environment. At job startup, it allows the system
to evaluate a user's Resource Specification Language request against
authorization policies on resource usage (determining how many CPUs or memory a
user can use on a given resource or which executables the user can run).
Furthermore, based on authorization policies, it allows other virtual
organization members to manage the user's job.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306071</id><created>2003-06-13</created><authors><author><keyname>Peters</keyname><forenames>Andreas J.</forenames></author><author><keyname>Saiz</keyname><forenames>P.</forenames></author><author><keyname>Buncic</keyname><forenames>P.</forenames></author></authors><title>AliEnFS - a Linux File System for the AliEn Grid Services</title><categories>cs.DC</categories><comments>9 pages, 12 figures</comments><acm-class>D.4.3</acm-class><abstract>  Among the services offered by the AliEn (ALICE Environment
http://alien.cern.ch) Grid framework there is a virtual file catalogue to allow
transparent access to distributed data-sets using various file transfer
protocols. $alienfs$ (AliEn File System) integrates the AliEn file catalogue as
a new file system type into the Linux kernel using LUFS, a hybrid user space
file system framework (Open Source http://lufs.sourceforge.net). LUFS uses a
special kernel interface level called VFS (Virtual File System Switch) to
communicate via a generalised file system interface to the AliEn file system
daemon. The AliEn framework is used for authentication, catalogue browsing,
file registration and read/write transfer operations. A C++ API implements the
generic file system operations. The goal of AliEnFS is to allow users easy
interactive access to a worldwide distributed virtual file system using
familiar shell commands (f.e. cp,ls,rm ...) The paper discusses general aspects
of Grid File Systems, the AliEn implementation and present and future
developments for the AliEn Grid File System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306072</id><created>2003-06-13</created><authors><author><keyname>Avellino</keyname><forenames>G.</forenames></author><author><keyname>Barale</keyname><forenames>S.</forenames></author><author><keyname>Beco</keyname><forenames>S.</forenames></author><author><keyname>Cantalupo</keyname><forenames>B.</forenames></author><author><keyname>Colling</keyname><forenames>D.</forenames></author><author><keyname>Giacomini</keyname><forenames>F.</forenames></author><author><keyname>Gianelle</keyname><forenames>A.</forenames></author><author><keyname>Guarise</keyname><forenames>A.</forenames></author><author><keyname>Krenek</keyname><forenames>A.</forenames></author><author><keyname>Kouril</keyname><forenames>D.</forenames></author><author><keyname>Maraschini</keyname><forenames>A.</forenames></author><author><keyname>Matyska</keyname><forenames>L.</forenames></author><author><keyname>Mezzadri</keyname><forenames>M.</forenames></author><author><keyname>Monforte</keyname><forenames>S.</forenames></author><author><keyname>Mulac</keyname><forenames>M.</forenames></author><author><keyname>Pacini</keyname><forenames>F.</forenames></author><author><keyname>Pappalardo</keyname><forenames>M.</forenames></author><author><keyname>Peluso</keyname><forenames>R.</forenames></author><author><keyname>Pospisil</keyname><forenames>J.</forenames></author><author><keyname>Prelz</keyname><forenames>F.</forenames></author><author><keyname>Ronchieri</keyname><forenames>E.</forenames></author><author><keyname>Ruda</keyname><forenames>M.</forenames></author><author><keyname>Salconi</keyname><forenames>L.</forenames></author><author><keyname>Salvet</keyname><forenames>Z.</forenames></author><author><keyname>Sgaravatto</keyname><forenames>M.</forenames></author><author><keyname>Sitera</keyname><forenames>J.</forenames></author><author><keyname>Terracina</keyname><forenames>A.</forenames></author><author><keyname>Vocu</keyname><forenames>M.</forenames></author><author><keyname>Werbrouck</keyname><forenames>A.</forenames></author></authors><title>The EU DataGrid Workload Management System: towards the second major
  release</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003</comments><acm-class>H.3.4</acm-class><abstract>  In the first phase of the European DataGrid project, the 'workload
management' package (WP1) implemented a working prototype, providing users with
an environment allowing to define and submit jobs to the Grid, and able to find
and use the ``best'' resources for these jobs. Application users have now been
experiencing for about a year now with this first release of the workload
management system. The experiences acquired, the feedback received by the user
and the need to plug new components implementing new functionalities, triggered
an update of the existing architecture. A description of this revised and
complemented workload management system is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306073</id><created>2003-06-13</created><authors><author><keyname>Baker</keyname><forenames>Rich</forenames></author><author><keyname>Yu</keyname><forenames>Dantong</forenames></author><author><keyname>Smith</keyname><forenames>Jason</forenames></author><author><keyname>Chan</keyname><forenames>Anthony</forenames></author><author><keyname>De</keyname><forenames>Kaushik</forenames></author><author><keyname>McGuigan</keyname><forenames>Patrick</forenames></author></authors><title>GridMonitor: Integration of Large Scale Facility Fabric Monitoring with
  Meta Data Service in Grid Environment</title><categories>cs.DC cs.PF</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, LaTeX, 1 eps figure, 4 ps
  figures, 1 style file, Monitoring, PSN MOET005</comments><acm-class>C.4; D.4.8; H.3.4; K.6.2</acm-class><abstract>  Grid computing consists of the coordinated use of large sets of diverse,
geographically distributed resources for high performance computation.
Effective monitoring of these computing resources is extremely important to
allow efficient use on the Grid. The large number of heterogeneous computing
entities available in Grids makes the task challenging. In this work, we
describe a Grid monitoring system, called GridMonitor, that captures and makes
available the most important information from a large computing facility. The
Grid monitoring system consists of four tiers: local monitoring, archiving,
publishing and harnessing. This architecture was applied on a large scale linux
farm and network infrastructure. It can be used by many higher-level Grid
services including scheduling services and resource brokering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306074</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306074</id><created>2003-06-13</created><authors><author><keyname>Kowalkowski</keyname><forenames>Jim</forenames><affiliation>Fermilab</affiliation></author></authors><title>Understanding and Coping with Hardware and Software Failures in a Very
  Large Trigger Farm</title><categories>cs.DC</categories><comments>Paper for the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003. PSN THGT001</comments><acm-class>B.8.1</acm-class><journal-ref>ECONFC0303241:THGT001,2003</journal-ref><abstract>  When thousands of processors are involved in performing event filtering on a
trigger farm, there is likely to be a large number of failures within the
software and hardware systems. BTeV, a proton/antiproton collider experiment at
Fermi National Accelerator Laboratory, has designed a trigger, which includes
several thousand processors. If fault conditions are not given proper
treatment, it is conceivable that this trigger system will experience failures
at a high enough rate to have a negative impact on its effectiveness. The RTES
(Real Time Embedded Systems) collaboration is a group of physicists, engineers,
and computer scientists working to address the problem of reliability in
large-scale clusters with real-time constraints such as this. Resulting
infrastructure must be highly scalable, verifiable, extensible by users, and
dynamically changeable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306075</id><created>2003-06-13</created><authors><author><keyname>Jacak</keyname><forenames>Barbara</forenames></author><author><keyname>Lacey</keyname><forenames>Roy</forenames></author><author><keyname>Morrison</keyname><forenames>Dave</forenames></author><author><keyname>Sourikova</keyname><forenames>Irina</forenames></author><author><keyname>Shevel</keyname><forenames>Andrey</forenames></author><author><keyname>Zhiping</keyname><forenames>Qiu</forenames></author></authors><title>Data Management for Physics Analysis in Phenix (BNL, RHIC)</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, no figures. PSN : TUCT009</comments><acm-class>H.3.4</acm-class><abstract>  Every year the PHENIX collaboration deals with increasing volume of data (now
about 1/4 PB/year). Apparently the more data the more questions how to process
all the data in most efficient way. In recent past many developments in HEP
computing were dedicated to the production environment. Now we need more tools
to help to obtain physics results from the analysis of distributed simulated
and experimental data. Developments in Grid architectures gave many examples
how distributed computing facilities can be organized to meet physics analysis
needs. We feel that our main task in this area is to try to use already
developed systems or system components in PHENIX environment.
  We are concentrating here on the followed problems: file/replica catalog
which keep names of our files, data moving over WAN, job submission in
multicluster environment.
  PHENIX is a running experiment and this fact narrowed our ability to test new
software on the collaboration computer facilities. We are experimenting with
system prototypes at State University of New York at Stony Brook (SUNYSB) where
we run midrange computing cluster for physics analysis. The talk is dedicated
to discuss some experience with Grid software and achieved results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306076</id><created>2003-06-13</created><authors><author><keyname>Patton</keyname><forenames>S.</forenames></author></authors><title>FAYE: A Java Implement of the Frame/Stream/Stop Analysis Model</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, 5 Figures, LaTeX. PSN
  MOLT005</comments><acm-class>J.2</acm-class><abstract>  FAYE, The Frame AnalYsis Executable, is a Java based implementation of the
Frame/Stream/Stop model for analyzing data. Unlike traditional Event based
analysis models, the Frame/Stream/Stop model has no preference as to which part
of any data is to be analyzed, and an Event get as equal treatment as a change
in the high voltage. This model means that FAYE is a suitable analysis
framework for many different type of data analysis, such as detector trends or
as a visualization core. During the design of FAYE the emphasis has been on
clearly delineating each of the executable's responsibilities and on keeping
their implementations as completely independent as possible. This leads to the
large part of FAYE being a generic core which is experiment independent, and
smaller section that customizes this core to an experiments own data
structures. This customization can even be done in C++, using JNI, while the
executable's control remains in Java. This paper reviews the Frame/Stream/Stop
model and then looks at how FAYE has approached its implementation, with an
emphasis on which responsibilities are handled by the generic core, and which
parts an experiment must provide as part of the customization portion of the
executable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306077</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306077</id><created>2003-06-13</created><authors><author><keyname>Hagge</keyname><forenames>Lars</forenames></author><author><keyname>Kreutzkamp</keyname><forenames>Jens</forenames></author><author><keyname>Lappe</keyname><forenames>Kathrin</forenames></author></authors><title>The TESLA Requirements Database</title><categories>cs.DB</categories><comments>Contribution to CHEP2003 conference</comments><acm-class>D.2.1;H.4.0;K.6.1</acm-class><abstract>  In preparation for the planned linear collider TESLA, DESY is designing the
required buildings and facilities. The accelerator and infrastructure
components have to be allocated to buildings, and their required areas for
installation, operation and maintenance have to be determined.
Interdisciplinary working groups specify the project from different viewpoints
and need to develop a common vision as a precondition for an optimal solution.
A commercial requirements database is used as a collaborative tool, enabling
concurrent requirements specification by independent working groups. The
requirements database ensures long term storage and availability of the
emerging knowledge, and it offers a central platform for communication which is
available for all project members. It is successfully operating since summer
2002 and has since then become an important tool for the design team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306078</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306078</id><created>2003-06-16</created><authors><author><keyname>Rademakers</keyname><forenames>Fons</forenames></author><author><keyname>Goto</keyname><forenames>Masaharu</forenames></author><author><keyname>Canal</keyname><forenames>Philippe</forenames></author><author><keyname>Brun</keyname><forenames>Rene</forenames></author></authors><title>ROOT Status and Future Developments</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, MSWord, pSN MOJT001</comments><acm-class>D.3.3</acm-class><journal-ref>ECONFC0303241:MOJT001,2003</journal-ref><abstract>  In this talk we will review the major additions and improvements made to the
ROOT system in the last 18 months and present our plans for future
developments. The additons and improvements range from modifications to the I/O
sub-system to allow users to save and restore objects of classes that have not
been instrumented by special ROOT macros, to the addition of a geometry package
designed for building, browsing, tracking and visualizing detector geometries.
Other improvements include enhancements to the quick analysis sub-system
(TTree::Draw()), the addition of classes that allow inter-file object
references (TRef, TRefArray), better support for templated and STL classes,
amelioration of the Automatic Script Compiler and the incorporation of new
fitting and mathematical tools. Efforts have also been made to increase the
modularity of the ROOT system with the introduction of more abstract interfaces
and the development of a plug-in manager. In the near future, we intend to
continue the development of PROOF and its interfacing with GRID environments.
We plan on providing an interface between Geant3, Geant4 and Fluka and the new
geometry package. The ROOT GUI classes will finally be available on Windows and
we plan to release a GUI inspector and builder. In the last year, ROOT has
drawn the endorsement of additional experiments and institutions. It is now
officially supported by CERN and used as key I/O component by the LCG project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306079</id><created>2003-06-13</created><authors><author><keyname>Buerger</keyname><forenames>Jochen</forenames></author><author><keyname>Hagge</keyname><forenames>Lars</forenames></author><author><keyname>Kreutzkamp</keyname><forenames>Jens</forenames></author><author><keyname>Lappe</keyname><forenames>Kathrin</forenames></author><author><keyname>Robben</keyname><forenames>Andrea</forenames></author></authors><title>Integrated Information Management for TESLA</title><categories>cs.DB</categories><comments>Contribution to CHEP2003 conference</comments><acm-class>H.4.0;K.6.1</acm-class><abstract>  Next-generation projects in High Energy Physics will reach again a new
dimension of complexity. Information management has to ensure an efficient and
economic information flow within the collaborations, offering world-wide
up-to-date information access to the collaborators as one condition for
successful projects. DESY introduces several information systems in preparation
for the planned linear collider TESLA: a Requirements Management System (RMS)
is in production for the TESLA planning group, a Product Data Management System
(PDMS) is in production since the beginning of 2002 and is supporting the
cavity preparation and the general engineering of accelerator components. A
pilot Asset Management System (AMS) is in production for supporting the
management and maintenance of the technical infrastructure, and a Facility
Management System (FMS) with a Geographic Information System (GIS) is currently
being introduced to support civil engineering. Efforts have been started to
integrate the systems with the goal that users can retrieve information through
a single point of access. The paper gives an introduction to information
management and the activities at DESY.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306080</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306080</id><created>2003-06-13</created><authors><author><keyname>Ratnikova</keyname><forenames>N.</forenames></author></authors><title>BOA: Framework for Automated Builds</title><categories>cs.SE</categories><comments>3 pages, 2 figures</comments><acm-class>D.2.7; D.2.9</acm-class><journal-ref>ECONFC0303241:TUJT005,2003</journal-ref><abstract>  Managing large-scale software products is a complex software engineering
task. The automation of the software development, release and distribution
process is most beneficial in the large collaborations, where the big number of
developers, multiple platforms and distributed environment are typical factors.
This paper describes Build and Output Analyzer framework and its components
that have been developed in CMS to facilitate software maintenance and improve
software quality. The system allows to generate, control and analyze various
types of automated software builds and tests, such as regular rebuilds of the
development code, software integration for releases and installation of the
existing versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306081</id><created>2003-06-13</created><authors><author><keyname>Barczyc</keyname><forenames>M.</forenames><affiliation>CERN</affiliation></author><author><keyname>Burckhart-Chromek</keyname><forenames>D.</forenames><affiliation>CERN</affiliation></author><author><keyname>Caprini</keyname><forenames>M.</forenames><affiliation>CERN</affiliation></author><author><keyname>Conceicao</keyname><forenames>J. Da Silva</forenames><affiliation>CERN</affiliation></author><author><keyname>Dobson</keyname><forenames>M.</forenames><affiliation>CERN</affiliation></author><author><keyname>Flammer</keyname><forenames>J.</forenames><affiliation>CERN</affiliation></author><author><keyname>Jones</keyname><forenames>R.</forenames><affiliation>CERN</affiliation></author><author><keyname>Kazarov</keyname><forenames>A.</forenames><affiliation>CERN</affiliation></author><author><keyname>Kolos</keyname><forenames>S.</forenames><affiliation>CERN</affiliation></author><author><keyname>Liko</keyname><forenames>D.</forenames><affiliation>CERN</affiliation></author><author><keyname>Mapelli</keyname><forenames>L.</forenames><affiliation>CERN</affiliation></author><author><keyname>Soloviev</keyname><forenames>I.</forenames><affiliation>CERN</affiliation></author><author><keyname>NIKHEF</keyname><forenames>R. Hart</forenames><affiliation>Amsterdam, Netherlands</affiliation></author><author><keyname>Amorim</keyname><forenames>A.</forenames><affiliation>CFNUL/FCUL, Portugal</affiliation></author><author><keyname>Klose</keyname><forenames>D.</forenames><affiliation>CFNUL/FCUL, Portugal</affiliation></author><author><keyname>Lima</keyname><forenames>J.</forenames><affiliation>CFNUL/FCUL, Portugal</affiliation></author><author><keyname>Lucio</keyname><forenames>L.</forenames><affiliation>CFNUL/FCUL, Portugal</affiliation></author><author><keyname>Pedro</keyname><forenames>L.</forenames><affiliation>CFNUL/FCUL, Portugal</affiliation></author><author><keyname>Wolters</keyname><forenames>H.</forenames><affiliation>UCP Figueira da Foz, Portugal</affiliation></author><author><keyname>NIPNE</keyname><forenames>E. Badescu</forenames><affiliation>Bucharest, Romania</affiliation></author><author><keyname>Alexandrov</keyname><forenames>I.</forenames><affiliation>Dubna, Russian Federation</affiliation></author><author><keyname>Kotov</keyname><forenames>V.</forenames><affiliation>Dubna, Russian Federation</affiliation></author><author><keyname>JINR</keyname><forenames>M. Mineev</forenames><affiliation>Dubna, Russian Federation</affiliation></author><author><keyname>PNPI</keyname><forenames>Yu. Ryabov</forenames><affiliation>Gatchina, Russian Federation</affiliation></author></authors><title>An on-line Integrated Bookkeeping: electronic run log book and Meta-Data
  Repository for ATLAS</title><categories>cs.DB</categories><acm-class>H.2.4;H.2.8</acm-class><abstract>  In the context of the ATLAS experiment there is growing evidence of the
importance of different kinds of Meta-data including all the important details
of the detector and data acquisition that are vital for the analysis of the
acquired data. The Online BookKeeper (OBK) is a component of ATLAS online
software that stores all information collected while running the experiment,
including the Meta-data associated with the event acquisition, triggering and
storage. The facilities for acquisition of control data within the on-line
software framework, together with a full functional Web interface, make the OBK
a powerful tool containing all information needed for event analysis, including
an electronic log book.
  In this paper we explain how OBK plays a role as one of the main collectors
and managers of Meta-data produced on-line, and we'll also focus on the Web
facilities already available. The usage of the web interface as an electronic
run logbook is also explained, together with the future extensions.
  We describe the technology used in OBK development and how we arrived at the
present level explaining the previous experience with various DBMS
technologies. The extensive performance evaluations that have been performed
and the usage in the production environment of the ATLAS test beams are also
analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306082</id><created>2003-06-13</created><authors><author><keyname>Pearlman</keyname><forenames>L.</forenames></author><author><keyname>Welch</keyname><forenames>V.</forenames></author><author><keyname>Foster</keyname><forenames>I.</forenames></author><author><keyname>Kesselman</keyname><forenames>C.</forenames></author><author><keyname>Tuecke</keyname><forenames>S.</forenames></author></authors><title>The Community Authorization Service: Status and Future</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003. 9 Pages, PDF</comments><acm-class>C.2.4</acm-class><abstract>  Virtual organizations (VOs) are communities of resource providers and users
distributed over multiple policy domains. These VOs often wish to define and
enforce consistent policies in addition to the policies of their underlying
domains. This is challenging, not only because of the problems in distributing
the policy to the domains, but also because of the fact that those domains may
each have different capabilities for enforcing the policy. The Community
Authorization Service (CAS) solves this problem by allowing resource providers
to delegate some policy authority to the VO while maintaining ultimate control
over their resources. In this paper we describe CAS and our past and current
implementations of CAS, and we discuss our plans for CAS-related research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306083</id><created>2003-06-13</created><authors><author><keyname>Lavrijsen</keyname><forenames>W. T. L. P.</forenames></author></authors><title>The Athena Startup Kit</title><categories>cs.SE</categories><comments>5 pages, 1 figure, CHEP 2003, March 2003, La Jolla, California, PSN
  TUJT002</comments><acm-class>D.2.6</acm-class><abstract>  The Athena Startup Kit (ASK), is an interactive front-end to the Atlas
software framework (ATHENA). Written in python, a very effective &quot;glue&quot;
language, it is build on top of the, in principle unrelated, code repository,
build, configuration, debug, binding, and analysis tools. ASK automates many
error-prone tasks that are otherwise left to the end-user, thereby pre-empting
a whole category of potential problems. Through the existing tools, which ASK
will setup for the user if and as needed, it locates available resources,
maintains job coherency, manages the run-time environment, allows for
interactivity and debugging, and provides standalone execution scripts. An
end-user who wants to run her own analysis algorithms within the standard
environment can let ASK generate the appropriate skeleton package, the needed
dependencies and run-time, as well as a default job options script. For new and
casual users, ASK comes with a graphical user interface; for advanced users,
ASK has a scriptable command line interface. Both are built on top of the same
set of libraries. ASK does not need to be, and isn't, experiment neutral. Thus
it has built-in workarounds for known gotcha's, that would otherwise be a major
time-sink for each and every new user. ASK minimizes the overhead for those
physicists in Atlas who just want to write and run their analysis code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306084</id><created>2003-06-13</created><authors><author><keyname>Barlow</keyname><forenames>R. J.</forenames></author><author><keyname>Forti</keyname><forenames>A.</forenames></author><author><keyname>McNab</keyname><forenames>A.</forenames></author><author><keyname>Salih</keyname><forenames>S.</forenames></author><author><keyname>Smith</keyname><forenames>D.</forenames></author><author><keyname>Adye</keyname><forenames>T.</forenames></author></authors><title>BaBar Web job submission with Globus authentication and AFS access</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, LaTeX, 3 eps</comments><acm-class>A.0; C.2.4</acm-class><journal-ref>Nucl. Instr &amp; Meth. A479 p1 2002; International J. Supercomputer
  Applications, 15(3), 2001; Proceedings of Computing in High Energy and
  Nuclear Physics (CHEP 2001) 9/3/2001-9/7/2001</journal-ref><abstract>  We present two versions of a grid job submission system produced for the
BaBar experiment. Both use globus job submission to process data spread across
various sites, producing output which can be combined for analysis. The
problems encountered with authorisation and authentication, data location, job
submission, and the input and output sandboxes are described, as are the
solutions. The total system is still some way short of the aims of enterprises
such as the EDG, but represent a significant step along the way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306085</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306085</id><created>2003-06-13</created><authors><author><keyname>Harrison</keyname><forenames>K.</forenames></author><author><keyname>Lavrijsen</keyname><forenames>W. T. L. P.</forenames></author><author><keyname>Mato</keyname><forenames>P.</forenames></author><author><keyname>Soroko</keyname><forenames>A.</forenames></author><author><keyname>Tan</keyname><forenames>C. L.</forenames></author><author><keyname>Tull</keyname><forenames>C. E.</forenames></author><author><keyname>Brook</keyname><forenames>N.</forenames></author><author><keyname>Jones</keyname><forenames>R. W. L.</forenames></author></authors><title>GANGA: a user-Grid interface for Atlas and LHCb</title><categories>cs.SE</categories><comments>9 pages, 3 figures, CHEP 2003, March 2003, La Jolla, California, PSN
  TUCT002</comments><acm-class>D.2.6</acm-class><abstract>  The Gaudi/Athena and Grid Alliance (GANGA) is a front-end for the
configuration, submission, monitoring, bookkeeping, output collection, and
reporting of computing jobs run on a local batch system or on the grid. In
particular, GANGA handles jobs that use applications written for the Gaudi
software framework shared by the Atlas and LHCb experiments. GANGA exploits the
commonality of Gaudi-based computing jobs, while insulating against grid-,
batch- and framework-specific technicalities, to maximize end-user productivity
in defining, configuring, and executing jobs. Designed for a python-based
component architecture, GANGA has a modular underpinning and is therefore well
placed for contributing to, and benefiting from, work in related projects. Its
functionality is accessible both from a scriptable command-line interface, for
expert users and automated tasks, and through a graphical interface, which
simplifies the interaction with GANGA for beginning and c1asual users.
  This paper presents the GANGA design and implementation, the development of
the underlying software bus architecture, and the functionality of the first
public GANGA release.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306086</id><created>2003-06-14</created><authors><author><keyname>Tull</keyname><forenames>Craig E.</forenames></author><author><keyname>Gunter</keyname><forenames>Dan</forenames></author><author><keyname>Lavrijsen</keyname><forenames>Wim</forenames></author><author><keyname>Quarrie</keyname><forenames>David</forenames></author><author><keyname>Tierney</keyname><forenames>Brian</forenames></author></authors><title>GMA Instrumentation of the Athena Framework using NetLogger</title><categories>cs.DC cs.IR</categories><acm-class>C.2.2</acm-class><abstract>  Grid applications are, by their nature, wide-area distributed applications.
This WAN aspect of Grid applications makes the use of conventional monitoring
and instrumentation tools (such as top, gprof, LSF Monitor, etc) impractical
for verification that the application is running correctly and efficiently. To
be effective, monitoring data must be &quot;end-to-end&quot;, meaning that all components
between the Grid application endpoints must be monitored. Instrumented
applications can generate a large amount of monitoring data, so typically the
instrumentation is off by default. For jobs running on a Grid, there needs to
be a general mechanism to remotely activate the instrumentation in running
jobs. The NetLogger Toolkit Activation Service provides this mechanism.
  To demonstrate this, we have instrumented the ATLAS Athena Framework with
NetLogger to generate monitoring events. We then use a GMA-based activation
service to control NetLogger's trigger mechanism. The NetLogger trigger
mechanism allows one to easily start, stop, or change the logging level of a
running program by modifying a trigger file. We present here details of the
design of the NetLogger implementation of the GMA-based activation service and
the instrumentation service for Athena. We also describe how this activation
service allows us to non-intrusively collect and visualize the ATLAS Athena
Framework monitoring data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306087</id><created>2003-06-14</created><authors><author><keyname>Fine</keyname><forenames>Valeri</forenames></author><author><keyname>Lauret</keyname><forenames>Jerome</forenames></author><author><keyname>Perevoztchikov</keyname><forenames>Victor</forenames></author></authors><title>OO Model of the STAR offline production &quot;Event Display&quot; and its
  implementation based on Qt-ROOT</title><categories>cs.HC cs.GR</categories><comments>3 pages, 4 figures, Computing in High Energy Physics, CHEP2003, La
  Jolla California, USA, March 24-28</comments><report-no>MOLT011</report-no><acm-class>I.3.7; D.1.5</acm-class><abstract>  The paper presents the &quot;Event Display&quot; package for the STAR offline
production as a special visualization tool to debug the reconstruction code.
This can be achieved if an author of the algorithm / code may build his/her own
custom Event Display alone from the base software blocks and re-used some
well-designed, easy to learn user-friendly patterns. For STAR offline
production Event Display ROOT with Qt lower level interface was chosen as the
base tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306088</id><created>2003-06-16</created><updated>2003-06-18</updated><authors><author><keyname>Tull</keyname><forenames>Craig E.</forenames></author><author><keyname>Canon</keyname><forenames>Shane</forenames></author><author><keyname>Chan</keyname><forenames>Steve</forenames></author><author><keyname>Olson</keyname><forenames>Doug</forenames></author><author><keyname>Pearlman</keyname><forenames>Laura</forenames></author><author><keyname>Welch</keyname><forenames>Von</forenames></author></authors><title>Using CAS to Manage Role-Based VO Sub-Groups</title><categories>cs.CR cs.DC</categories><acm-class>C.2.0</acm-class><abstract>  LHC-era HENP experiments will generate unprecidented volumes of data and
require commensurately large compute resources. These resources are larger than
can be marshalled at any one site within the community. Production
reconstruction, analysis, and simulation will need to take maximum advantage of
these distributed computing and storage resources using the new capabilities
offered by the Grid computing paradigm. Since large-scale, coordinated Grid
computing involves user access across many Regional Centers and national and
funding boundaries, one of the most crucial aspects of Grid computing is that
of user authentication and authorization. While projects such as the DOE Grids
CA have gone a long way to solving the problem of distributed authentication,
the authorization problem is still largely open.
  We have developed and tested a prototype VO-Role management system using the
Community Authorization Service (CAS) from the Globus project. CAS allows for a
flexible definition of resources. In this protoype we define a role as a
resource within the CAS database and assign individuals in the VO access to
that resource to indicate their ability to assert the role. The access of an
individual to this VO-Role resource is then an annotation of the user's CAS
proxy certificate. This annotation is then used by the local resource managers
to authorize access to local compute and storage resources at a granularity
which is base on neither VOs nor individuals. We report here on the
configuration details for the CAS database and the Globus Gatekeeper and on how
this general approch could be formalized and extended to meet the clear needs
of LHC experiments using the Grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306089</id><created>2003-06-14</created><authors><author><keyname>Calafiura</keyname><forenames>P.</forenames></author><author><keyname>Leggett</keyname><forenames>C. G.</forenames></author><author><keyname>Quarrie</keyname><forenames>D. R.</forenames></author><author><keyname>Ma</keyname><forenames>H.</forenames></author><author><keyname>Rajagopalan</keyname><forenames>S.</forenames></author></authors><title>The StoreGate: a Data Model for the Atlas Software Architecture</title><categories>cs.SE</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX, MOJT008</comments><acm-class>D.2.11</acm-class><abstract>  The Atlas collaboration at CERN has adopted the Gaudi software architecture
which belongs to the blackboard family: data objects produced by knowledge
sources (e.g. reconstruction modules) are posted to a common in-memory data
base from where other modules can access them and produce new data objects. The
StoreGate has been designed, based on the Atlas requirements and the experience
of other HENP systems such as Babar, CDF, CLEO, D0 and LHCB, to identify in a
simple and efficient fashion (collections of) data objects based on their type
and/or the modules which posted them to the Transient Data Store (the
blackboard). The developer also has the freedom to use her preferred key class
to uniquely identify a data object according to any other criterion. Besides
this core functionality, the StoreGate provides the developers with a powerful
interface to handle in a coherent fashion persistable references, object
lifetimes, memory management and access control policy for the data objects in
the Store. It also provides a Handle/Proxy mechanism to define and hide the
cache fault mechanism: upon request, a missing Data Object can be transparently
created and added to the Transient Store presumably retrieving it from a
persistent data-base, or even reconstructing it on demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306090</id><created>2003-06-14</created><authors><author><keyname>Tatebe</keyname><forenames>Osamu</forenames></author><author><keyname>Sekiguchi</keyname><forenames>Satoshi</forenames></author><author><keyname>Morita</keyname><forenames>Youhei</forenames></author><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author><author><keyname>Soda</keyname><forenames>Noriyuki</forenames></author></authors><title>Worldwide Fast File Replication on Grid Datafarm</title><categories>cs.PF cs.NI</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, LaTeX, 7 eps figures. PSN
  THAT06</comments><acm-class>C.2.4; C.2.5; C.4; D.4.3; D.4.8</acm-class><abstract>  The Grid Datafarm architecture is designed for global petascale
data-intensive computing. It provides a global parallel filesystem with online
petascale storage, scalable I/O bandwidth, and scalable parallel processing,
and it can exploit local I/O in a grid of clusters with tens of thousands of
nodes. One of features is that it manages file replicas in filesystem metadata
for fault tolerance and load balancing.
  This paper discusses and evaluates several techniques to support
long-distance fast file replication. The Grid Datafarm manages a ranked group
of files as a Gfarm file, each file, called a Gfarm file fragment, being stored
on a filesystem node, or replicated on several filesystem nodes. Each Gfarm
file fragment is replicated independently and in parallel using rate-controlled
HighSpeed TCP with network striping. On a US-Japan testbed with 10,000 km
distance, we achieve 419 Mbps using 2 nodes on each side, and 741 Mbps using 4
nodes out of 893 Mbps with two transpacific networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306091</id><created>2003-06-16</created><updated>2004-09-30</updated><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Universal Sequential Decisions in Unknown Environments</title><categories>cs.AI cs.CC cs.LG</categories><comments>2 pages</comments><acm-class>I.2; G.3</acm-class><journal-ref>Proc. 5th European Workshop on Reinforcement Learning (EWRL-2001)
  25-26</journal-ref><abstract>  We give a brief introduction to the AIXI model, which unifies and overcomes
the limitations of sequential decision theory and universal Solomonoff
induction. While the former theory is suited for active agents in known
environments, the latter is suited for passive prediction of unknown
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306092</id><created>2003-06-14</created><authors><author><keyname>Morita</keyname><forenames>Y.</forenames></author><author><keyname>Sato</keyname><forenames>H.</forenames></author><author><keyname>Watase</keyname><forenames>Y.</forenames></author><author><keyname>Tatebe</keyname><forenames>O.</forenames></author><author><keyname>Sekiguchi</keyname><forenames>S.</forenames></author><author><keyname>Matsuoka</keyname><forenames>S.</forenames></author><author><keyname>Soda</keyname><forenames>N.</forenames></author><author><keyname>Dell'Acqua</keyname><forenames>A.</forenames></author></authors><title>Building A High Performance Parallel File System Using Grid Datafarm and
  ROOT I/O</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, PDF. PSN TUDT010</comments><acm-class>J.2</acm-class><abstract>  Sheer amount of petabyte scale data foreseen in the LHC experiments require a
careful consideration of the persistency design and the system design in the
world-wide distributed computing. Event parallelism of the HENP data analysis
enables us to take maximum advantage of the high performance cluster computing
and networking when we keep the parallelism both in the data processing phase,
in the data management phase, and in the data transfer phase. A modular
architecture of FADS/ Goofy, a versatile detector simulation framework for
Geant4, enables an easy choice of plug-in facilities for persistency
technologies such as Objectivity/DB and ROOT I/O. The framework is designed to
work naturally with the parallel file system of Grid Datafarm (Gfarm).
FADS/Goofy is proven to generate 10^6 Geant4-simulated Atlas Mockup events
using a 512 CPU PC cluster. The data in ROOT I/O files is replicated using
Gfarm file system. The histogram information is collected from the distributed
ROOT files. During the data replication it has been demonstrated to achieve
more than 2.3 Gbps data transfer rate between the PC clusters over seven
participating PC clusters in the United States and in Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306093</id><created>2003-06-14</created><authors><author><keyname>Amorim</keyname><forenames>Antonio</forenames><affiliation>Faculdade de Ciencias, University of Lisbon, Portugal</affiliation></author><author><keyname>Pedro</keyname><forenames>Luis</forenames><affiliation>Faculdade de Ciencias, University of Lisbon, Portugal</affiliation></author><author><keyname>Fei</keyname><forenames>Han</forenames><affiliation>ADETTI, Edificio ISCTE, University of Lisbon, Portugal</affiliation></author><author><keyname>Almeida</keyname><forenames>Nuno</forenames><affiliation>ADETTI, Edificio ISCTE, University of Lisbon, Portugal</affiliation></author><author><keyname>Trezentos</keyname><forenames>Paulo</forenames><affiliation>ADETTI, Edificio ISCTE, University of Lisbon, Portugal</affiliation></author><author><keyname>Villate</keyname><forenames>Jaime E.</forenames><affiliation>Department of Physics, School of Engineering, University of Porto</affiliation></author></authors><title>Grid-Brick Event Processing Framework in GEPS</title><categories>cs.DC</categories><comments>6 pages; document for CHEP'03 conference</comments><acm-class>C.1.4;C.2.1;C.2.4;D.1.3;D.4.3;D.4.7;H.2.4</acm-class><abstract>  Experiments like ATLAS at LHC involve a scale of computing and data
management that greatly exceeds the capability of existing systems, making it
necessary to resort to Grid-based Parallel Event Processing Systems (GEPS).
Traditional Grid systems concentrate the data in central data servers which
have to be accessed by many nodes each time an analysis or processing job
starts. These systems require very powerful central data servers and make
little use of the distributed disk space that is available in commodity
computers. The Grid-Brick system, which is described in this paper, follows a
different approach. The data storage is split among all grid nodes having each
one a piece of the whole information. Users submit queries and the system will
distribute the tasks through all the nodes and retrieve the result, merging
them together in the Job Submit Server. The main advantage of using this system
is the huge scalability it provides, while its biggest disadvantage appears in
the case of failure of one of the nodes. A workaround for this problem involves
data replication or backup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306094</id><created>2003-06-16</created><authors><author><keyname>Cowan</keyname><forenames>Ray</forenames></author><author><keyname>Deshpande</keyname><forenames>Yogesh</forenames></author><author><keyname>White</keyname><forenames>Bebo</forenames></author></authors><title>BaBar - A Community Web Site in an Organizational Setting</title><categories>cs.IR</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, PDF, PSN MONT006</comments><acm-class>H.4.m</acm-class><abstract>  The BABAR Web site was established in 1993 at the Stanford Linear Accelerator
Center (SLAC) to support the BABAR experiment, to report its results, and to
facilitate communication among its scientific and engineering collaborators,
currently numbering about 600 individuals from 75 collaborating institutions in
10 countries. The BABAR Web site is, therefore, a community Web site. At the
same time it is hosted at SLAC and funded by agencies that demand adherence to
policies decided under different priorities. Additionally, the BABAR Web
administrators deal with the problems that arise during the course of managing
users, content, policies, standards, and changing technologies. Desired
solutions to some of these problems may be incompatible with the overall
administration of the SLAC Web sites and/or the SLAC policies and concerns.
There are thus different perspectives of the same Web site and differing
expectations in segments of the SLAC population which act as constraints and
challenges in any review or re-engineering activities. Web Engineering, which
post-dates the BABAR Web, has aimed to provide a comprehensive understanding of
all aspects of Web development. This paper reports on the first part of a
recent review of application of Web Engineering methods to the BABAR Web site,
which has led to explicit user and information models of the BABAR community
and how SLAC and the BABAR community relate and react to each other. The paper
identifies the issues of a community Web site in a hierarchical,
semi-governmental sector and formulates a strategy for periodic reviews of
BABAR and similar sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306095</id><created>2003-06-16</created><authors><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Buncic</keyname><forenames>Predrag</forenames></author><author><keyname>Manset</keyname><forenames>David</forenames></author><author><keyname>Hauer</keyname><forenames>Tamas</forenames></author><author><keyname>Estrella</keyname><forenames>Florida</forenames></author><author><keyname>Saiz</keyname><forenames>Pablo</forenames></author><author><keyname>Rogulin</keyname><forenames>Dmitri</forenames></author></authors><title>The MammoGrid Project Grids Architecture</title><categories>cs.DC cs.DB</categories><comments>Talk PSN MOAT0005 from the 2003 Computing in High Energy and Nuclear
  Physics (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, 4 figures</comments><acm-class>C2.4,H2.4,J.3</acm-class><abstract>  The aim of the recently EU-funded MammoGrid project is, in the light of
emerging Grid technology, to develop a European-wide database of mammograms
that will be used to develop a set of important healthcare applications and
investigate the potential of this Grid to support effective co-working between
healthcare professionals throughout the EU. The MammoGrid consortium intends to
use a Grid model to enable distributed computing that spans national borders.
This Grid infrastructure will be used for deploying novel algorithms as
software directly developed or enhanced within the project. Using the MammoGrid
clinicians will be able to harness the use of massive amounts of medical image
data to perform epidemiological studies, advanced image processing,
radiographic education and ultimately, tele-diagnosis over communities of
medical &quot;virtual organisations&quot;. This is achieved through the use of
Grid-compliant services [1] for managing (versions of) massively distributed
files of mammograms, for handling the distributed execution of mammograms
analysis software, for the development of Grid-aware algorithms and for the
sharing of resources between multiple collaborating medical centres. All this
is delivered via a novel software and hardware information infrastructure that,
in addition guarantees the integrity and security of the medical data. The
MammoGrid implementation is based on AliEn, a Grid framework developed by the
ALICE Collaboration. AliEn provides a virtual file catalogue that allows
transparent access to distributed data-sets and provides top to bottom
implementation of a lightweight Grid applicable to cases when handling of a
large number of files is required. This paper details the architecture that
will be implemented by the MammoGrid project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306096</id><created>2003-06-16</created><authors><author><keyname>Newman</keyname><forenames>H. B.</forenames></author><author><keyname>Legrand</keyname><forenames>I. C.</forenames></author><author><keyname>Galvez</keyname><forenames>P.</forenames></author><author><keyname>Voicu</keyname><forenames>R.</forenames></author><author><keyname>Cirstoiu</keyname><forenames>C.</forenames></author></authors><title>MonALISA : A Distributed Monitoring Service Architecture</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, pdf. PSN MOET001</comments><acm-class>H4.3;H5.2;J2;D2.8</acm-class><abstract>  The MonALISA (Monitoring Agents in A Large Integrated Services Architecture)
system provides a distributed monitoring service. MonALISA is based on a
scalable Dynamic Distributed Services Architecture which is designed to meet
the needs of physics collaborations for monitoring global Grid systems, and is
implemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the
system derives from the use of multithreaded Station Servers to host a variety
of loosely coupled self-describing dynamic services, the ability of each
service to register itself and then to be discovered and used by any other
services, or clients that require such information, and the ability of all
services and clients subscribing to a set of events (state changes) in the
system to be notified automatically. The framework integrates several existing
monitoring tools and procedures to collect parameters describing computational
nodes, applications and network performance. It has built-in SNMP support and
network-performance monitoring algorithms that enable it to monitor end-to-end
network performance as well as the performance and state of site facilities in
a Grid. MonALISA is currently running around the clock on the US CMS test Grid
as well as an increasing number of other sites. It is also being used to
monitor the performance and optimize the interconnections among the reflectors
in the VRVS system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306097</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306097</id><created>2003-06-16</created><authors><author><keyname>Llabr&#xe9;s</keyname><forenames>Merc&#xe9;</forenames></author><author><keyname>Rossell&#xf3;</keyname><forenames>Francesc</forenames></author></authors><title>A family of metrics on contact structures based on edge ideals</title><categories>cs.DM cs.CE q-bio</categories><comments>Presented at the Biomolecular Mathematics Special Session of the
  First Joint AMS-RSME Meeting (Sevilla, june 2003)</comments><acm-class>G.2.3; J.3</acm-class><abstract>  The measurement of the similarity of RNA secondary structures, and in general
of contact structures, of a fixed length has several specific applications. For
instance, it is used in the analysis of the ensemble of suboptimal secondary
structures generated by a given algorithm on a given RNA sequence, and in the
comparison of the secondary structures predicted by different algorithms on a
given RNA molecule. It is also a useful tool in the quantitative study of
sequence-structure maps. A way to measure this similarity is by means of
metrics. In this paper we introduce a new class of metrics $d_{m}$, $m\geq 3$,
on the set of all contact structures of a fixed length, based on their
representation by means of edge ideals in a polynomial ring. These metrics can
be expressed in terms of Hilbert functions of monomial ideals, which allows the
use of several public domain computer algebra systems to compute them. We study
some abstract properties of these metrics, and we obtain explicit descriptions
of them for $m=3,4$ on arbitrary contact structures and for $m=5,6$ on RNA
secondary structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306098</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306098</id><created>2003-06-16</created><authors><author><keyname>Wheeldon</keyname><forenames>Richard</forenames></author><author><keyname>Counsell</keyname><forenames>Steve</forenames></author></authors><title>Making refactoring decisions in large-scale Java systems: an empirical
  stance</title><categories>cs.SE</categories><comments>10 pages</comments><acm-class>D.2.8</acm-class><abstract>  Decisions on which classes to refactor are fraught with difficulty. The
problem of identifying candidate classes becomes acute when confronted with
large systems comprising hundreds or thousands of classes. In this paper, we
describe a metric by which key classes, and hence candidates for refactoring,
can be identified. Measures quantifying the usage of two forms of coupling,
inheritance and aggregation, together with two other class features (number of
methods and attributes) were extracted from the source code of three large Java
systems. Our research shows that metrics from other research domains can be
adapted to the software engineering process. Substantial differences were found
between each of the systems in terms of the key classes identified and hence
opportunities for refactoring those classes varied between those systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306099</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306099</id><created>2003-06-16</created><authors><author><keyname>Li</keyname><forenames>Baoli</forenames></author><author><keyname>Yu</keyname><forenames>Shiwen</forenames></author><author><keyname>Lu</keyname><forenames>Qin</forenames></author></authors><title>An Improved k-Nearest Neighbor Algorithm for Text Categorization</title><categories>cs.CL</categories><comments>7 pages, 2 tables, 2 figures, to appear in the Proceedings of the
  20th International Conference on Computer Processing of Oriental Languages,
  Shenyang, China, August 2003</comments><acm-class>I.2.7; H.3.3</acm-class><abstract>  k is the most important parameter in a text categorization system based on
k-Nearest Neighbor algorithm (kNN).In the classification process, k nearest
documents to the test one in the training set are determined firstly. Then, the
predication can be made according to the category distribution among these k
nearest neighbors. Generally speaking, the class distribution in the training
set is uneven. Some classes may have more samples than others. Therefore, the
system performance is very sensitive to the choice of the parameter k. And it
is very likely that a fixed k value will result in a bias on large categories.
To deal with these problems, we propose an improved kNN algorithm, which uses
different numbers of nearest neighbors for different categories, rather than a
fixed number across all categories. More samples (nearest neighbors) will be
used for deciding whether a test document should be classified to a category,
which has more samples in the training set. Preliminary experiments on Chinese
text categorization show that our method is less sensitive to the parameter k
than the traditional one, and it can properly classify documents belonging to
smaller classes with a large k. The method is promising for some cases, where
estimating the parameter k via cross-validation is not allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306100</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306100</id><created>2003-06-16</created><authors><author><keyname>Sehkri</keyname><forenames>Vijay</forenames></author><author><keyname>Mandrichenko</keyname><forenames>Igor</forenames></author><author><keyname>Skow</keyname><forenames>Dane</forenames></author></authors><title>Site Authorization Service (SAZ)</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, CA, USA, March 2003, 3 pages, PSN TUBT007</comments><acm-class>D.4.6; K.6.5</acm-class><journal-ref>ECONFC0303241:TUBT007,2003</journal-ref><abstract>  In this paper we present a methodology to provide an additional level of
centralized control for the grid resources. This centralized control is applied
to site-wide distribution of various grids and thus providing an upper hand in
the maintenance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306101</identifier>
 <datestamp>2009-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306101</id><created>2003-06-16</created><authors><author><keyname>Lehmann</keyname><forenames>G.</forenames></author></authors><title>The DataFlow System of the ATLAS Trigger and DAQ</title><categories>cs.SE</categories><comments>4 pages, 3 figures, proceedings from the CHEP03 conference, april
  2003, California, La Jolla</comments><acm-class>C.4</acm-class><journal-ref>ECONF C0303241:MOGT009,2003</journal-ref><abstract>  The baseline design and implementation of the DataFlow system, to be
documented in the ATLAS DAQ/HLT Technical Design Report in summer 2003, will be
presented. Empahsis will be placed on the system performance and scalability
based on the results from prototyping studies which have maximised the use of
commercially available hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306102</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306102</id><created>2003-06-16</created><authors><author><keyname>Vaniachine</keyname><forenames>A.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Malon</keyname><forenames>D.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Nevski</keyname><forenames>P.</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>De</keyname><forenames>K.</forenames><affiliation>University of Texas at Arlington</affiliation></author></authors><title>Prototyping Virtual Data Technologies in ATLAS Data Challenge 1
  Production</title><categories>cs.DC cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, 3 figures, pdf. PSN TUCP012</comments><report-no>ANL-HEP-CP-03-049</report-no><acm-class>C.2.4; H.2.4</acm-class><abstract>  For efficiency of the large production tasks distributed worldwide, it is
essential to provide shared production management tools comprised of
integratable and interoperable services. To enhance the ATLAS DC1 production
toolkit, we introduced and tested a Virtual Data services component. For each
major data transformation step identified in the ATLAS data processing pipeline
(event generation, detector simulation, background pile-up and digitization,
etc) the Virtual Data Cookbook (VDC) catalogue encapsulates the specific data
transformation knowledge and the validated parameters settings that must be
provided before the data transformation invocation. To provide for local-remote
transparency during DC1 production, the VDC database server delivered in a
controlled way both the validated production parameters and the templated
production recipes for thousands of the event generation and detector
simulation jobs around the world, simplifying the production management
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306103</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306103</id><created>2003-06-16</created><authors><author><keyname>Vaniachine</keyname><forenames>A.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Eckmann</keyname><forenames>S.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Malon</keyname><forenames>D.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Nevski</keyname><forenames>P.</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>Wenaus</keyname><forenames>T.</forenames><affiliation>Brookhaven National Laboratory</affiliation></author></authors><title>Primary Numbers Database for ATLAS Detector Description Parameters</title><categories>cs.DB cs.HC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, 5 figures, pdf. PSN MOKT006</comments><report-no>ANL-HEP-CP-03-050</report-no><acm-class>H.2.4</acm-class><abstract>  We present the design and the status of the database for detector description
parameters in ATLAS experiment. The ATLAS Primary Numbers are the parameters
defining the detector geometry and digitization in simulations, as well as
certain reconstruction parameters. Since the detailed ATLAS detector
description needs more than 10,000 such parameters, a preferred solution is to
have a single verified source for all these data. The database stores the data
dictionary for each parameter collection object, providing schema evolution
support for object-based retrieval of parameters. The same Primary Numbers are
served to many different clients accessing the database: the ATLAS software
framework Athena, the Geant3 heritage framework Atlsim, the Geant4 developers
framework FADS/Goofy, the generator of XML output for detector description, and
several end-user clients for interactive data navigation, including web-based
browsers and ROOT. The choice of the MySQL database product for the
implementation provides additional benefits: the Primary Numbers database can
be used on the developers laptop when disconnected (using the MySQL embedded
server technology), with data being updated when the laptop is connected (using
the MySQL database replication).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306104</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306104</id><created>2003-06-16</created><authors><author><keyname>Matias</keyname><forenames>Yossi</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Efficient pebbling for list traversal synopses</title><categories>cs.DS</categories><comments>27 pages</comments><acm-class>D.2.5;E.1;E.3;I.6.7;F.2.3</acm-class><abstract>  We show how to support efficient back traversal in a unidirectional list,
using small memory and with essentially no slowdown in forward steps. Using
$O(\log n)$ memory for a list of size $n$, the $i$'th back-step from the
farthest point reached so far takes $O(\log i)$ time in the worst case, while
the overhead per forward step is at most $\epsilon$ for arbitrary small
constant $\epsilon&gt;0$. An arbitrary sequence of forward and back steps is
allowed. A full trade-off between memory usage and time per back-step is
presented: $k$ vs. $kn^{1/k}$ and vice versa. Our algorithms are based on a
novel pebbling technique which moves pebbles on a virtual binary, or $t$-ary,
tree that can only be traversed in a pre-order fashion. The compact data
structures used by the pebbling algorithms, called list traversal synopses,
extend to general directed graphs, and have other interesting applications,
including memory efficient hash-chain implementation. Perhaps the most
surprising application is in showing that for any program, arbitrary rollback
steps can be efficiently supported with small overhead in memory, and marginal
overhead in its ordinary execution. More concretely: Let $P$ be a program that
runs for at most $T$ steps, using memory of size $M$. Then, at the cost of
recording the input used by the program, and increasing the memory by a factor
of $O(\log T)$ to $O(M \log T)$, the program $P$ can be extended to support an
arbitrary sequence of forward execution and rollback steps: the $i$'th rollback
step takes $O(\log i)$ time in the worst case, while forward steps take O(1)
time in the worst case, and $1+\epsilon$ amortized time per step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306105</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306105</id><created>2003-06-17</created><authors><author><keyname>Formica</keyname><forenames>Andrea</forenames></author></authors><title>Design, implementation and deployment of the Saclay muon reconstruction
  algorithms (Muonbox/y) in the Athena software framework of the ATLAS
  experiment</title><categories>cs.CE</categories><comments>4 pages, 8 figures, CHEP2003 conference</comments><acm-class>J.2</acm-class><abstract>  This paper gives an overview of a reconstruction algorithm for muon events in
ATLAS experiment at CERN. After a short introduction on ATLAS Muon
Spectrometer, we will describe the procedure performed by the algorithms
Muonbox and Muonboy (last version) in order to achieve correctly the
reconstruction task. These algorithms have been developed in Fortran language
and are working in the official C++ framework Athena, as well as in stand alone
mode. A description of the interaction between Muonboy and Athena will be
given, together with the reconstruction performances (efficiency and momentum
resolution) obtained with MonteCarlo data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306106</identifier>
 <datestamp>2009-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306106</id><created>2003-06-17</created><updated>2009-04-22</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Lexicographic probability, conditional probability, and nonstandard
  probability</title><categories>cs.GT cs.AI</categories><comments>A preliminary version appears in Proceedings of the Eighth Conference
  on Theoretical Aspects of Rationality and Knowledge, 2001, pp. 17--30. The
  final version will appear in Games and Economic Behavior</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between Popper spaces (conditional probability spaces that
satisfy some regularity conditions), lexicographic probability systems (LPS's),
and nonstandard probability spaces (NPS's) is considered. If countable
additivity is assumed, Popper spaces and a subclass of LPS's are equivalent;
without the assumption of countable additivity, the equivalence no longer
holds. If the state space is finite, LPS's are equivalent to NPS's. However, if
the state space is infinite, NPS's are shown to be more general than LPS's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306107</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306107</id><created>2003-06-17</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>On the Relationship between Strand Spaces and Multi-Agent Systems</title><categories>cs.CR</categories><comments>A preliminary version of this paper appears in the Proceedings of the
  8th ACM Conference on Computer and Communications Security,2001</comments><acm-class>D.4.6</acm-class><journal-ref>ACM Transactions on Information and System Security 6:1, 2003, pp.
  43--70</journal-ref><abstract>  Strand spaces are a popular framework for the analysis of security protocols.
Strand spaces have some similarities to a formalism used successfully to model
protocols for distributed systems, namely multi-agent systems. We explore the
exact relationship between these two frameworks here. It turns out that a key
difference is the handling of agents, which are unspecified in strand spaces
and explicit in multi-agent systems. We provide a family of translations from
strand spaces to multi-agent systems parameterized by the choice of agents in
the strand space. We also show that not every multi-agent system of interest
can be expressed as a strand space. This reveals a lack of expressiveness in
the strand-space framework that can be characterized by our translation. To
highlight this lack of expressiveness, we show one simple way in which strand
spaces can be extended to model more systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306108</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306108</id><created>2003-06-17</created><authors><author><keyname>Deshpande</keyname><forenames>Yogesh</forenames></author><author><keyname>Murugesan</keyname><forenames>San</forenames></author><author><keyname>Ginige</keyname><forenames>Athula</forenames></author><author><keyname>Hansen</keyname><forenames>Steve</forenames></author><author><keyname>Schwabe</keyname><forenames>Daniel</forenames></author><author><keyname>Gaedke</keyname><forenames>Martin</forenames></author><author><keyname>White</keyname><forenames>Bebo</forenames></author></authors><title>Web Engineering</title><categories>cs.SE</categories><comments>Published in the Journal of Web Engineering, Vol. 1, No. 1 (2002),
  Rinton Press, PDF, 15 pages</comments><acm-class>K.6</acm-class><journal-ref>J.Web Engineering 1 (2002) 003-017</journal-ref><abstract>  Web Engineering is the application of systematic, disciplined and
quantifiable approaches to development, operation, and maintenance of Web-based
applications. It is both a pro-active approach and a growing collection of
theoretical and empirical research in Web application development. This paper
gives an overview of Web Engineering by addressing the questions: a) why is it
needed? b) what is its domain of operation? c) how does it help and what should
it do to improve Web application development? and d) how should it be
incorporated in education and training? The paper discusses the significant
differences that exist between Web applications and conventional software, the
taxonomy of Web applications, the progress made so far and the research issues
and experience of creating a specialisation at the master's level. The paper
reaches a conclusion that Web Engineering at this stage is a moving target
since Web technologies are constantly evolving, making new types of
applications possible, which in turn may require innovations in how they are
built, deployed and maintained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306109</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306109</id><created>2003-06-18</created><authors><author><keyname>Iqbal</keyname><forenames>Saima</forenames></author><author><keyname>Bunn</keyname><forenames>Julian J.</forenames></author><author><keyname>Newman</keyname><forenames>Harvey B.</forenames></author></authors><title>Distributed Heterogeneous Relational Data Warehouse In A Grid Environment</title><categories>cs.DC cs.DB</categories><comments>4 pages, 6 figures</comments><acm-class>H.2.1;H.2.2;H.2.4;H.2.7;H.3.1;H.3.5</acm-class><abstract>  This paper examines how a &quot;Distributed Heterogeneous Relational Data
Warehouse&quot; can be integrated in a Grid environment that will provide physicists
with efficient access to large and small object collections drawn from
databases at multiple sites. This paper investigates the requirements of
Grid-enabling such a warehouse, and explores how these requirements may be met
by extensions to existing Grid middleware. We present initial results obtained
with a working prototype warehouse of this kind using both SQLServer and
Oracle9i, where a Grid-enabled web-services interface makes it easier for
web-applications to access the distributed contents of the databases securely.
Based on the success of the prototype, we proposes a framework for using
heterogeneous relational data warehouse through the web-service interface and
create a single &quot;Virtual Database System&quot; for users. The ability to
transparently access data in this way, as shown in prototype, is likely to be a
very powerful facility for HENP and other grid users wishing to collate and
analyze information distributed over Grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306110</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306110</id><created>2003-06-18</created><authors><author><keyname>Bellato</keyname><forenames>M.</forenames></author><author><keyname>Berti</keyname><forenames>L.</forenames></author><author><keyname>Brigljevic</keyname><forenames>V.</forenames></author><author><keyname>Bruno</keyname><forenames>G.</forenames></author><author><keyname>Cano</keyname><forenames>E.</forenames></author><author><keyname>Cittolin</keyname><forenames>S.</forenames></author><author><keyname>Csilling</keyname><forenames>A.</forenames></author><author><keyname>Erhan</keyname><forenames>S.</forenames></author><author><keyname>Gigi</keyname><forenames>D.</forenames></author><author><keyname>Glege</keyname><forenames>F.</forenames></author><author><keyname>Gomez-Reino</keyname><forenames>R.</forenames></author><author><keyname>Gulmini</keyname><forenames>M.</forenames></author><author><keyname>Gutleber</keyname><forenames>J.</forenames></author><author><keyname>Jacobs</keyname><forenames>C.</forenames></author><author><keyname>Kozlovszky</keyname><forenames>M.</forenames></author><author><keyname>Larsen</keyname><forenames>H.</forenames></author><author><keyname>Magrans</keyname><forenames>I.</forenames></author><author><keyname>Maron</keyname><forenames>G.</forenames></author><author><keyname>Meijers</keyname><forenames>F.</forenames></author><author><keyname>Meschi</keyname><forenames>E.</forenames></author><author><keyname>Murray</keyname><forenames>S.</forenames></author><author><keyname>Oh</keyname><forenames>A.</forenames></author><author><keyname>Orsini</keyname><forenames>L.</forenames></author><author><keyname>Pollet</keyname><forenames>L.</forenames></author><author><keyname>Racz</keyname><forenames>A.</forenames></author><author><keyname>Rorato</keyname><forenames>G.</forenames></author><author><keyname>Samyn</keyname><forenames>D.</forenames></author><author><keyname>Scharff-Hansen</keyname><forenames>P.</forenames></author><author><keyname>Schwick</keyname><forenames>C.</forenames></author><author><keyname>Sphicas</keyname><forenames>P.</forenames></author><author><keyname>Toniolo</keyname><forenames>N.</forenames></author><author><keyname>Ventura</keyname><forenames>S.</forenames></author><author><keyname>Zangrando</keyname><forenames>L.</forenames></author></authors><title>Run Control and Monitor System for the CMS Experiment</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, PSN THGT002</comments><acm-class>C.2.4; J.2</acm-class><abstract>  The Run Control and Monitor System (RCMS) of the CMS experiment is the set of
hardware and software components responsible for controlling and monitoring the
experiment during data-taking. It provides users with a &quot;virtual counting
room&quot;, enabling them to operate the experiment and to monitor detector status
and data quality from any point in the world. This paper describes the
architecture of the RCMS with particular emphasis on its scalability through a
distributed collection of nodes arranged in a tree-based hierarchy. The current
implementation of the architecture in a prototype RCMS used in test beam
setups, detector validations and DAQ demonstrators is documented. A discussion
of the key technologies used, including Web Services, and the results of tests
performed with a 128-node system are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306111</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306111</id><created>2003-06-18</created><authors><author><keyname>Andreozzi</keyname><forenames>Sergio</forenames></author><author><keyname>Sgaravatto</keyname><forenames>Massimo</forenames></author><author><keyname>Vistoli</keyname><forenames>Cristina</forenames></author></authors><title>Sharing a conceptual model of grid resources and services</title><categories>cs.DC</categories><comments>4 pages, 0 figures, CHEP 2003</comments><acm-class>H.3.4</acm-class><abstract>  Grid technologies aim at enabling a coordinated resource-sharing and
problem-solving capabilities over local and wide area networks and span
locations, organizations, machine architectures and software boundaries. The
heterogeneity of involved resources and the need for interoperability among
different grid middlewares require the sharing of a common information model.
Abstractions of different flavors of resources and services and conceptual
schemas of domain specific entities require a collaboration effort in order to
enable a coherent information services cooperation.
  With this paper, we present the result of our experience in grid resources
and services modelling carried out within the Grid Laboratory Uniform
Environment (GLUE) effort, a joint US and EU High Energy Physics projects
collaboration towards grid interoperability. The first implementation-neutral
agreement on services such as batch computing and storage manager, resources
such as the hierarchy cluster, sub-cluster, host and the storage library are
presented. Design guidelines and operational results are depicted together with
open issues and future evolutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306112</identifier>
 <datestamp>2009-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306112</id><created>2003-06-18</created><authors><author><keyname>Bonham</keyname><forenames>D.</forenames></author><author><keyname>Garzoglio</keyname><forenames>G.</forenames></author><author><keyname>Herber</keyname><forenames>R.</forenames></author><author><keyname>Kowalkowski</keyname><forenames>J.</forenames></author><author><keyname>Litvintsev</keyname><forenames>D.</forenames></author><author><keyname>Lueking</keyname><forenames>L.</forenames></author><author><keyname>Paterno</keyname><forenames>M.</forenames></author><author><keyname>Petravick</keyname><forenames>D.</forenames></author><author><keyname>Piccoli</keyname><forenames>L.</forenames></author><author><keyname>Pordes</keyname><forenames>R.</forenames></author><author><keyname>Stanfield</keyname><forenames>N.</forenames></author><author><keyname>Terekhov</keyname><forenames>I.</forenames></author><author><keyname>Trumbo</keyname><forenames>J.</forenames></author><author><keyname>Tseng</keyname><forenames>J.</forenames></author><author><keyname>Veseli</keyname><forenames>S.</forenames></author><author><keyname>Votava</keyname><forenames>M.</forenames></author><author><keyname>White</keyname><forenames>V.</forenames></author><author><keyname>Huffman</keyname><forenames>T.</forenames></author><author><keyname>Stonjek</keyname><forenames>S.</forenames></author><author><keyname>Waltkins</keyname><forenames>K.</forenames></author><author><keyname>Crosby</keyname><forenames>P.</forenames></author><author><keyname>Waters</keyname><forenames>D.</forenames></author><author><keyname>Denis</keyname><forenames>R. St.</forenames></author></authors><title>Adapting SAM for CDF</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, pdf format, TUAT004</comments><report-no>TUAT004</report-no><acm-class>C.2.4; H.3.2; H.3.3; H.3.4</acm-class><journal-ref>ECONF C0303241:TUAT004,2003</journal-ref><abstract>  The CDF and D0 experiments probe the high-energy frontier and as they do so
have accumulated hundreds of Terabytes of data on the way to petabytes of data
over the next two years. The experiments have made a commitment to use the
developing Grid based on the SAM system to handle these data. The D0 SAM has
been extended for use in CDF as common patterns of design emerged to meet the
similar requirements of these experiments. The process by which the merger was
achieved is explained with particular emphasis on lessons learned concerning
the database design patterns plus realization of the use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306113</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306113</id><created>2003-06-19</created><updated>2003-10-26</updated><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author></authors><title>Symbolic Parametric Analysis of Embedded Systems with BDD-like
  Data-Structures</title><categories>cs.DS cs.LO</categories><comments>11 pages, 1 figure</comments><acm-class>B.2.2; B.4.4; B.5.2; B.6.3; D.2.4; F.1.1; F.3.1; F.4.1</acm-class><abstract>  We use dense variable-ordering to define HRD (Hybrid-Restriction Diagram), a
new BDD-like data-structure for the representation and manipulation of
state-spaces of linear hybrid automata. We present and discuss various
manipulation algorithms for HRD, including the basic set-oriented operations,
weakest precondition calculation, and normalization. We implemented the ideas
and experimented to see their performance. Finally, we have also developed a
pruning technique for state-space exploration based on parameter valuation
space characterization. The technique showed good promise in our experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306114</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306114</id><created>2003-06-19</created><authors><author><keyname>Baranovski</keyname><forenames>A.</forenames></author><author><keyname>Brock</keyname><forenames>C.</forenames></author><author><keyname>Bonham</keyname><forenames>D.</forenames></author><author><keyname>Carpenter</keyname><forenames>L.</forenames></author><author><keyname>Lueking</keyname><forenames>L.</forenames></author><author><keyname>Merritt</keyname><forenames>W.</forenames></author><author><keyname>Moore</keyname><forenames>C.</forenames></author><author><keyname>Terekhov</keyname><forenames>I.</forenames></author><author><keyname>Trumbo</keyname><forenames>J.</forenames></author><author><keyname>Veseli</keyname><forenames>S.</forenames></author><author><keyname>Weigand</keyname><forenames>J.</forenames></author><author><keyname>White</keyname><forenames>S.</forenames></author><author><keyname>Yip</keyname><forenames>K.</forenames></author></authors><title>D0 Data Handling Operational Experience</title><categories>cs.DC cs.AI</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages. PSN MOKT002</comments><acm-class>H.3</acm-class><journal-ref>ECONF C0303241:MOKT002,2003</journal-ref><abstract>  We report on the production experience of the D0 experiment at the Fermilab
Tevatron, using the SAM data handling system with a variety of computing
hardware configurations, batch systems, and mass storage strategies. We have
stored more than 300 TB of data in the Fermilab Enstore mass storage system. We
deliver data through this system at an average rate of more than 2 TB/day to
analysis programs, with a substantial multiplication factor in the consumed
data through intelligent cache management. We handle more than 1.7 Million
files in this system and provide data delivery to user jobs at Fermilab on four
types of systems: a reconstruction farm, a large SMP system, a Linux batch
cluster, and a Linux desktop cluster. In addition, we import simulation data
generated at 6 sites worldwide, and deliver data to jobs at many more sites. We
describe the scope of the data handling deployment worldwide, the operational
experience with this system, and the feedback of that experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306115</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306115</id><created>2003-06-19</created><authors><author><keyname>Lueking</keyname><forenames>L.</forenames></author><author><keyname>Force</keyname><forenames>representing the D0 Remote Analysis Task</forenames></author></authors><title>D0 Regional Analysis Center Concepts</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 3 eps figures. PSN
  THAT01</comments><acm-class>A.0;H.3</acm-class><journal-ref>ECONF C0303241:TUAT003,2003</journal-ref><abstract>  The D0 experiment is facing many exciting challenges providing a computing
environment for its worldwide collaboration. Transparent access to data for
processing and analysis has been enabled through deployment of its SAM system
to collaborating sites and additional functionality will be provided soon with
SAMGrid components. In order to maximize access to global storage,
computational and intellectual resources, and to enable the system to scale to
the large demands soon to be realized, several strategic sites have been
identified as Regional Analysis Centers (RAC's). These sites play an expanded
role within the system. The philosophy and function of these centers is
discussed and details of their composition and operation are outlined. The plan
for future additional centers is also addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306116</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306116</id><created>2003-06-19</created><updated>2003-07-15</updated><authors><author><keyname>Newman</keyname><forenames>Harvey B.</forenames></author><author><keyname>Galvez</keyname><forenames>Philippe</forenames></author><author><keyname>Denis</keyname><forenames>Gregory</forenames></author><author><keyname>Collados</keyname><forenames>David</forenames></author><author><keyname>Wei</keyname><forenames>Kun</forenames></author><author><keyname>Adamczyk</keyname><forenames>David</forenames></author></authors><title>Global Platform for Rich Media Conferencing and Collaboration</title><categories>cs.MM cs.NI</categories><comments>CHEP03 Conference</comments><acm-class>H.5.3</acm-class><abstract>  The Virtual Rooms Videoconferencing Service (VRVS) provides a worldwide
videoconferencing service and collaborative environment to the research and
education communities. This system provides a low cost, bandwidth-efficient,
extensible means for videoconferencing and remote collaboration over networks
within the High Energy and Nuclear Physics communities (HENP). VRVS has become
a standard part of the toolset used daily by a large sector of HENP, and it is
used increasingly for other DoE/NSF-supported programs. The current features
included multi-protocol, multi-OS support for all significant video enabled
clients including: H.323, Mbone, QuickTime, MPEG2, Java Media Framework, and
other clients. The current architecture makes VRVS a distributed, highly
functional, and efficient software-only system for multipoint audio, video and
web conferencing and collaboration over global IP networks. VRVS has developed
the VRVS-AG Reflector and a specialized Web interface that enables end users to
connect to any Access Grid (AG) session, in any of the AG &quot;virtual venues&quot; from
anywhere worldwide. The VRVS system has now been running for the last five and
half years, offering to the HENP community a working and reliable tool for
collaboration within groups and among physicists dispersed world-wide. The goal
of this ongoing effort is to develop the next generation collaborative systems
running over next generation networks. The new developments area integrate
emerging standards, include all security aspects, and will extend the range of
VRVS video technologies supported to cover the latest high end standards
quality. We will focus the discussion on the new capability provides by the
latest version V3.0 and its future evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306117</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306117</id><created>2003-06-20</created><updated>2004-02-16</updated><authors><author><keyname>Demri</keyname><forenames>Stephane</forenames></author><author><keyname>de Nivelle</keyname><forenames>Hans</forenames></author></authors><title>Deciding regular grammar logics with converse through first-order logic</title><categories>cs.LO</categories><comments>34 pages</comments><acm-class>F.4.1;I.2.4;I.2.3</acm-class><abstract>  We provide a simple translation of the satisfiability problem for regular
grammar logics with converse into GF2, which is the intersection of the guarded
fragment and the 2-variable fragment of first-order logic. This translation is
theoretically interesting because it translates modal logics with certain frame
conditions into first-order logic, without explicitly expressing the frame
conditions.
  A consequence of the translation is that the general satisfiability problem
for regular grammar logics with converse is in EXPTIME. This extends a previous
result of the first author for grammar logics without converse. Using the same
method, we show how some other modal logics can be naturally translated into
GF2, including nominal tense logics and intuitionistic logic.
  In our view, the results in this paper show that the natural first-order
fragment corresponding to regular grammar logics is simply GF2 without extra
machinery such as fixed point-operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306118</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306118</id><created>2003-06-20</created><authors><author><keyname>Adamek</keyname><forenames>J.</forenames></author><author><keyname>Milius</keyname><forenames>S.</forenames></author><author><keyname>Velebil</keyname><forenames>J.</forenames></author></authors><title>On coalgebra based on classes</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><abstract>  Every endofunctor of the category of classes is proved to be set-based in the
sense of Aczel and Mendler, therefore, it has a final coalgebra. Other basic
properties of these endofunctors are proved, e.g. the existence of a free
completely iterative theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306119</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306119</id><created>2003-06-20</created><authors><author><keyname>Vidal</keyname><forenames>Jose M</forenames></author></authors><title>A Method for Solving Distributed Service Allocation Problems</title><categories>cs.MA</categories><acm-class>I.2.11</acm-class><abstract>  We present a method for solving service allocation problems in which a set of
services must be allocated to a set of agents so as to maximize a global
utility. The method is completely distributed so it can scale to any number of
services without degradation. We first formalize the service allocation problem
and then present a simple hill-climbing, a global hill-climbing, and a
bidding-protocol algorithm for solving it. We analyze the expected performance
of these algorithms as a function of various problem parameters such as the
branching factor and the number of agents. Finally, we use the sensor
allocation problem, an instance of a service allocation problem, to show the
bidding protocol at work. The simulations also show that phase transition on
the expected quality of the solution exists as the amount of communication
between agents increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306120</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306120</id><created>2003-06-22</created><updated>2007-03-09</updated><authors><author><keyname>Szita</keyname><forenames>Istvan</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Reinforcement Learning with Linear Function Approximation and LQ control
  Converges</title><categories>cs.LG cs.AI</categories><comments>9 pages</comments><acm-class>I.2.6; I.2.8</acm-class><abstract>  Reinforcement learning is commonly used with function approximation. However,
very few positive results are known about the convergence of function
approximation based RL control algorithms. In this paper we show that TD(0) and
Sarsa(0) with linear function approximation is convergent for a simple class of
problems, where the system is linear and the costs are quadratic (the LQ
control problem). Furthermore, we show that for systems with Gaussian noise and
non-completely observable states (the LQG problem), the mentioned RL algorithms
are still convergent, if they are combined with Kalman filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306121</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306121</id><created>2003-06-22</created><updated>2012-03-20</updated><authors><author><keyname>Pachl</keyname><forenames>Jan</forenames></author></authors><title>Reachability problems for communicating finite state machines</title><categories>cs.LO cs.NI</categories><comments>University of Waterloo, Department of Computer Science Research
  Report; May 1982; 66 pages, 24 figures; formatted for arXiv: June 2003; arXiv
  v2 (corrected typos): March 2012</comments><report-no>CS-82-12</report-no><acm-class>F.1.1; C.2.2; F.3.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the verification of reachability properties in a
commonly used state transition model of communication protocols, which consists
of finite state machines connected by potentially unbounded FIFO channels.
Although simple reachability problems are undecidable for general protocols
with unbounded channels, they are decidable for the protocols with the
recognizable channel property. The decidability question is open for the
protocols with the rational channel property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306122</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306122</id><created>2003-06-22</created><authors><author><keyname>Wheeldon</keyname><forenames>Richard</forenames></author><author><keyname>Levene</keyname><forenames>Mark</forenames></author></authors><title>The Best Trail Algorithm for Assisted Navigation of Web Sites</title><categories>cs.DS cs.IR</categories><comments>11 pages, 11 figures</comments><acm-class>H.3.3;H.5.4;G.2.2;F.2.2</acm-class><abstract>  We present an algorithm called the Best Trail Algorithm, which helps solve
the hypertext navigation problem by automating the construction of memex-like
trails through the corpus. The algorithm performs a probabilistic best-first
expansion of a set of navigation trees to find relevant and compact trails. We
describe the implementation of the algorithm, scoring methods for trails,
filtering algorithms and a new metric called \emph{potential gain} which
measures the potential of a page for future navigation opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306123</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306123</id><created>2003-06-23</created><authors><author><keyname>Etzold</keyname><forenames>Daniel</forenames></author></authors><title>Heuristic to reduce the complexity of complete bipartite graphs to
  accelerate the search for maximum weighted matchings with small error</title><categories>cs.DS</categories><comments>5 pages, 2 figures</comments><acm-class>G.2.2</acm-class><abstract>  A maximum weighted matching for bipartite graphs $G=(A \cup B,E)$ can be
found by using the algorithm of Edmonds and Karp with a Fibonacci Heap and a
modified Dijkstra in $O(nm + n^2 \log{n})$ time where n is the number of nodes
and m the number of edges. For the case that $|A|=|B|$ the number of edges is
$n^2$ and therefore the complexity is $O(n^3)$. In this paper we want to
present a simple heuristic method to reduce the number of edges of complete
bipartite graphs $G=(A \cup B,E)$ with $|A|=|B|$ such that $m = n\log{n}$ and
therefore the complexity of such that $m = n\log{n}$ and therefore the
complexity of $O(n^2 \log{n})$. The weights of all edges in G must be uniformly
distributed in [0,1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306124</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306124</id><created>2003-06-23</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Updating Probabilities</title><categories>cs.AI</categories><comments>This is an expanded version of a paper that appeared in Proceedings
  of the Eighteenth Conference on Uncertainty in AI, 2002, pp. 187--196. to
  appear, Journal of AI Research</comments><acm-class>I.2.4</acm-class><abstract>  As examples such as the Monty Hall puzzle show, applying conditioning to
update a probability distribution on a ``naive space'', which does not take
into account the protocol used, can often lead to counterintuitive results.
Here we examine why. A criterion known as CAR (``coarsening at random'') in the
statistical literature characterizes when ``naive'' conditioning in a naive
space works. We show that the CAR condition holds rather infrequently, and we
provide a procedural characterization of it, by giving a randomized algorithm
that generates all and only distributions for which CAR holds. This
substantially extends previous characterizations of CAR. We also consider more
generalized notions of update such as Jeffrey conditioning and minimizing
relative entropy (MRE). We give a generalization of the CAR condition that
characterizes when Jeffrey conditioning leads to appropriate answers, and show
that there exist some very simple settings in which MRE essentially never gives
the right results. This generalizes and interconnects previous results obtained
in the literature on CAR and MRE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306125</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306125</id><created>2003-06-24</created><authors><author><keyname>Gupta</keyname><forenames>R. C.</forenames></author><author><keyname>Agarwal</keyname><forenames>Ankur</forenames></author><author><keyname>Gupta</keyname><forenames>Ruchi</forenames></author><author><keyname>Gupta</keyname><forenames>Sanjay</forenames></author></authors><title>Predicting Response-Function Results of Electrical/Mechanical Systems
  Through Artificial Neural Network</title><categories>cs.NE</categories><comments>8 pages including 3 figures</comments><report-no>IET-MED-2003-2</report-no><acm-class>F1.1;I2.6;I5,1</acm-class><abstract>  In the present paper a newer application of Artificial Neural Network (ANN)
has been developed i.e., predicting response-function results of
electrical-mechanical system through ANN. This method is specially useful to
complex systems for which it is not possible to find the response-function
because of complexity of the system. The proposed approach suggests that how
even without knowing the response-function, the response-function results can
be predicted with the use of ANN to the system. The steps used are: (i)
Depending on the system, the ANN-architecture and the input &amp; output parameters
are decided, (ii) Training &amp; test data are generated from simplified circuits
and through tactic-superposition of it for complex circuits, (iii) Training the
ANN with training data through many cycles and (iv) Test-data are used for
predicting the response-function results. It is found that the proposed novel
method for response prediction works satisfactorily. Thus this method could be
used specially for complex systems where other methods are unable to tackle it.
In this paper the application of ANN is particularly demonstrated to
electrical-circuit system but can be applied to other systems too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306126</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306126</id><created>2003-06-24</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>Bayesian Treatment of Incomplete Discrete Data applied to Mutual
  Information and Feature Selection</title><categories>cs.LG cs.AI math.PR</categories><comments>11 pages, 1 figure</comments><report-no>IDSIA-15-03</report-no><acm-class>G.3; G.1.2; I.2</acm-class><journal-ref>Proceedings of the 26th German Conference on Artificial
  Intelligence (KI-2003) 396-406</journal-ref><abstract>  Given the joint chances of a pair of random variables one can compute
quantities of interest, like the mutual information. The Bayesian treatment of
unknown chances involves computing, from a second order prior distribution and
the data likelihood, a posterior distribution of the chances. A common
treatment of incomplete data is to assume ignorability and determine the
chances by the expectation maximization (EM) algorithm. The two different
methods above are well established but typically separated. This paper joins
the two approaches in the case of Dirichlet priors, and derives efficient
approximations for the mean, mode and the (co)variance of the chances and the
mutual information. Furthermore, we prove the unimodality of the posterior
distribution, whence the important property of convergence of EM to the global
maximum in the chosen framework. These results are applied to the problem of
selecting features for incremental learning and naive Bayes classification. A
fast filter based on the distribution of mutual information is shown to
outperform the traditional filter based on empirical mutual information on a
number of incomplete real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306127</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306127</id><created>2003-06-24</created><authors><author><keyname>Lim</keyname><forenames>Ngee-Peng</forenames></author><author><keyname>Ling</keyname><forenames>Maurice HT</forenames></author><author><keyname>Lim</keyname><forenames>Shawn YC</forenames></author><author><keyname>Choi</keyname><forenames>Ji-Hee</forenames></author><author><keyname>Teo</keyname><forenames>Henry BK</forenames></author></authors><title>Development of a Java Package for Matrix Programming</title><categories>cs.MS</categories><comments>Secondary school (high school) student project report. Foundation for
  JMaths project</comments><acm-class>K.3.0; G.m</acm-class><abstract>  We had assembled a Java package, known as MatrixPak, of four classes for the
purpose of numerical matrix computation. The classes are matrix,
matrix_operations, StrToMatrix, and MatrixToStr; all of which are inherited
from java.lang.Object class. Class matrix defines a matrix as a two-dimensional
array of float types, and contains the following mathematical methods:
transpose, adjoint, determinant, inverse, minor and cofactor. Class
matrix_operations contains the following mathematical methods: matrix addition,
matrix subtraction, matrix multiplication, and matrix exponential. Class
StrToMatrix contains methods necessary to parse a string representation (for
example, [[2 3 4]-[5 6 7]]) of a matrix into a matrix definition, whereas class
MatrixToStr does the reverse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306128</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306128</id><created>2003-06-24</created><updated>2006-06-14</updated><authors><author><keyname>Marshall</keyname><forenames>James A. R.</forenames><affiliation>University of Bristol</affiliation></author></authors><title>On the suitability of the 2 x 2 games for studying reciprocal
  cooperation and kin selection</title><categories>cs.GT</categories><comments>20 pages, 10 tables, 5 figures</comments><acm-class>J.3; J.4</acm-class><abstract>  The 2 x 2 games, in particular the Prisoner's Dilemma, have been extensively
used in studies into reciprocal cooperation and, to a lesser extent, kin
selection. This paper examines the suitability of the 2 x 2 games for modelling
the evolution of cooperation through reciprocation and kin selection. This
examination is not restricted to the Prisoner's Dilemma, but includes the other
non-trivial symmetric 2 x 2 games. We show that the popularity of the
Prisoner's Dilemma for modelling social and biotic interaction is justified by
its superiority according to these criteria. Indeed, the Prisoner's Dilemma is
unique in providing the simplest support for reciprocal cooperation, and
additive kin-selected altruism. However, care is still required in choosing the
particular Prisoner's Dilemma payoff matrix to use. This paper reviews the
impact of non-linear payoffs for the application of Hamilton's rule to typical
altruistic interactions, and derives new results for cases in which the roles
of potential altruist and beneficiary are separated. In doing so we find the
same equilibrium condition holds in continuous games between relatives, and in
discrete games with roles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306129</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306129</id><created>2003-06-24</created><authors><author><keyname>Welch</keyname><forenames>Von</forenames></author><author><keyname>Siebenlist</keyname><forenames>Frank</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author><author><keyname>Bresnahan</keyname><forenames>John</forenames></author><author><keyname>Czajkowski</keyname><forenames>Karl</forenames></author><author><keyname>Gawor</keyname><forenames>Jarek</forenames></author><author><keyname>Kesselman</keyname><forenames>Carl</forenames></author><author><keyname>Meder</keyname><forenames>Sam</forenames></author><author><keyname>Pearlman</keyname><forenames>Laura</forenames></author><author><keyname>Tuecke</keyname><forenames>Steven</forenames></author></authors><title>Security for Grid Services</title><categories>cs.CR cs.DC</categories><comments>10 pages; 4 figures</comments><report-no>Preprint ANL/MCS-P1024-0203</report-no><acm-class>C.2.4</acm-class><abstract>  Grid computing is concerned with the sharing and coordinated use of diverse
resources in distributed &quot;virtual organizations.&quot; The dynamic and
multi-institutional nature of these environments introduces challenging
security issues that demand new technical approaches. In particular, one must
deal with diverse local mechanisms, support dynamic creation of services, and
enable dynamic creation of trust domains. We describe how these issues are
addressed in two generations of the Globus Toolkit. First, we review the Globus
Toolkit version 2 (GT2) approach; then, we describe new approaches developed to
support the Globus Toolkit version 3 (GT3) implementation of the Open Grid
Services Architecture, an initiative that is recasting Grid concepts within a
service oriented framework based on Web services. GT3's security implementation
uses Web services security mechanisms for credential exchange and other
purposes, and introduces a tight least-privilege model that avoids the need for
any privileged network service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306130</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306130</id><created>2003-06-25</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Chaitanya</keyname><forenames>Vineet</forenames></author><author><keyname>Kulkarni</keyname><forenames>Amba P.</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author></authors><title>Anusaaraka: Machine Translation in Stages</title><categories>cs.CL cs.AI</categories><comments>5 pages, Published in Vivek, A Quarterly in Artificial Intelligence,
  10, 3, July 1997, pp. 22-25</comments><acm-class>I.2.7</acm-class><journal-ref>Vivek, A Quarterly in Artificial Intelligence, 10, 3, July 1997,
  pp. 22-25</journal-ref><abstract>  Fully-automatic general-purpose high-quality machine translation systems
(FGH-MT) are extremely difficult to build. In fact, there is no system in the
world for any pair of languages which qualifies to be called FGH-MT. The
reasons are not far to seek. Translation is a creative process which involves
interpretation of the given text by the translator. Translation would also vary
depending on the audience and the purpose for which it is meant. This would
explain the difficulty of building a machine translation system. Since, the
machine is not capable of interpreting a general text with sufficient accuracy
automatically at present - let alone re-expressing it for a given audience, it
fails to perform as FGH-MT. FOOTNOTE{The major difficulty that the machine
faces in interpreting a given text is the lack of general world knowledge or
common sense knowledge.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306131</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306131</id><created>2003-06-25</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Spakowski</keyname><forenames>Holger</forenames></author><author><keyname>Thakur</keyname><forenames>Mayur</forenames></author></authors><title>Complexity of Cycle Length Modularity Problems in Graphs</title><categories>cs.CC</categories><comments>10 pages</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  The even cycle problem for both undirected and directed graphs has been the
topic of intense research in the last decade. In this paper, we study the
computational complexity of \emph{cycle length modularity problems}. Roughly
speaking, in a cycle length modularity problem, given an input (undirected or
directed) graph, one has to determine whether the graph has a cycle $C$ of a
specific length (or one of several different lengths), modulo a fixed integer.
We denote the two families (one for undirected graphs and one for directed
graphs) of problems by $(S,m)\hbox{-}{\rm UC}$ and $(S,m)\hbox{-}{\rm DC}$,
where $m \in \mathcal{N}$ and $S \subseteq \{0,1, ..., m-1\}$.
$(S,m)\hbox{-}{\rm UC}$ (respectively, $(S,m)\hbox{-}{\rm DC}$) is defined as
follows: Given an undirected (respectively, directed) graph $G$, is there a
cycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, we
fully classify (i.e., as either polynomial-time solvable or as ${\rm
NP}$-complete) each problem $(S,m)\hbox{-}{\rm UC}$ such that $0 \in S$ and
each problem $(S,m)\hbox{-}{\rm DC}$ such that $0 \notin S$. We also give a
sufficient condition on $S$ and $m$ for the following problem to be
polynomial-time computable: $(S,m)\hbox{-}{\rm UC}$ such that $0 \notin S$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306132</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306132</id><created>2003-06-26</created><authors><author><keyname>Giraldi</keyname><forenames>Gilson Antonio</forenames><affiliation>National Laboratory for Scientific Computing</affiliation></author></authors><title>Classical and Nonextensive Information Theory</title><categories>cs.GL</categories><comments>6 pages without figures</comments><acm-class>H.1.1</acm-class><abstract>  In this work we firstly review some results in Classical Information Theory.
Next, we try to generalize these results by using the Tsallis entropy. We
present a preliminary result and discuss our aims in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306133</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306133</id><created>2003-06-26</created><authors><author><keyname>Engh</keyname><forenames>D.</forenames></author><author><keyname>Smallen</keyname><forenames>S.</forenames></author><author><keyname>Gieraltowski</keyname><forenames>J.</forenames></author><author><keyname>Fang</keyname><forenames>L.</forenames></author><author><keyname>Gardner</keyname><forenames>R.</forenames></author><author><keyname>Gannon</keyname><forenames>D.</forenames></author><author><keyname>Bramley</keyname><forenames>R.</forenames></author></authors><title>GRAPPA: Grid Access Portal for Physics Applications</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 3 eps figures. PSN
  TUCT006</comments><acm-class>H.5.2; C.2.4</acm-class><journal-ref>ECONFC0303241:TUCT006,2003</journal-ref><abstract>  Grappa is a Grid portal effort designed to provide physicists convenient
access to Grid tools and services. The ATLAS analysis and control framework,
Athena, was used as the target application. Grappa provides basic Grid
functionality such as resource configuration, credential testing, job
submission, job monitoring, results monitoring, and preliminary integration
with the ATLAS replica catalog system, MAGDA. Grappa uses Jython to combine the
ease of scripting with the power of java-based toolkits. This provides a
powerful framework for accessing diverse Grid resources with uniform
interfaces. The initial prototype system was based on the XCAT Science Portal
developed at the Indiana University Extreme Computing Lab and was demonstrated
by running Monte Carlo production on the U.S. ATLAS test-bed. The portal also
communicated with a European resource broker on WorldGrid as part of the joint
iVDGL-DataTAG interoperability project for the IST2002 and SC2002
demonstrations. The current prototype replaces the XCAT Science Portal with an
xbooks jetspeed portlet for managing user scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306134</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306134</id><created>2003-06-27</created><updated>2004-04-12</updated><authors><author><keyname>B&#xf6;hler</keyname><forenames>Elmar</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Reith</keyname><forenames>Steffen</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>The Complexity of Boolean Constraint Isomorphism</title><categories>cs.CC cs.LO</categories><comments>This is the full version of our STACS 2004 paper</comments><acm-class>F.1.3;F.4.1</acm-class><abstract>  In 1978, Schaefer proved his famous dichotomy theorem for generalized
satisfiability problems. He defined an infinite number of propositional
satisfiability problems (nowadays usually called Boolean constraint
satisfaction problems) and showed that all these satisfiability problems are
either in P or NP-complete. In recent years, similar results have been obtained
for quite a few other problems for Boolean constraints.Almost all of these
problems are variations of the satisfiability problem.
  In this paper, we address a problem that is not a variation of
satisfiability, namely, the isomorphism problem for Boolean constraints.
Previous work by B\&quot;ohler et al. showed that the isomorphism problem is either
coNP-hard or reducible to the graph isomorphism problem (a problem that is in
NP, but not known to be NP-hard), thus distinguishing a hard case and an easier
case. However, they did not classify which cases are truly easy, i.e., in P.
This paper accomplishes exactly that. It shows that Boolean constraint
isomorphism is coNP-hard (and GI-hard), or equivalent to graph isomorphism, or
in P, and it gives simple criteria to determine which case holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306135</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306135</id><created>2003-06-27</created><authors><author><keyname>Grandcolas</keyname><forenames>Stephane</forenames></author><author><keyname>Henocque</keyname><forenames>Laurent</forenames></author><author><keyname>Prcovic</keyname><forenames>Nicolas</forenames></author></authors><title>Pruning Isomorphic Structural Sub-problems in Configuration</title><categories>cs.AI</categories><comments>This research report contains the proofs and full details missing
  from the short paper &quot;A Canonicity Test for Configuration&quot; in proceedings of
  conference CP'03</comments><report-no>LSIS-2003-004</report-no><acm-class>I.2.3; I.2.4; I.2.8; F.4.1</acm-class><abstract>  Configuring consists in simulating the realization of a complex product from
a catalog of component parts, using known relations between types, and picking
values for object attributes. This highly combinatorial problem in the field of
constraint programming has been addressed with a variety of approaches since
the foundation system R1(McDermott82). An inherent difficulty in solving
configuration problems is the existence of many isomorphisms among
interpretations. We describe a formalism independent approach to improve the
detection of isomorphisms by configurators, which does not require to adapt the
problem model. To achieve this, we exploit the properties of a characteristic
subset of configuration problems, called the structural sub-problem, which
canonical solutions can be produced or tested at a limited cost. In this paper
we present an algorithm for testing the canonicity of configurations, that can
be added as a symmetry breaking constraint to any configurator. The cost and
efficiency of this canonicity test are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0306136</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0306136</id><created>2003-06-27</created><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Distributive Computability</title><categories>cs.LO cs.PL</categories><acm-class>F.1.1</acm-class><abstract>  This thesis presents a series of theoretical results and practical
realisations about the theory of computation in distributive categories.
Distributive categories have been proposed as a foundational tool for Computer
Science in the last years, starting from the papers of R.F.C. Walters. We shall
focus on two major topics: distributive computability, i.e., a generalized
theory of computability based on distributive categories, and the Imp(G)
language, which is a language based on the syntax of distributive categories.
The link between the former and the latter is that the functions computed by
Imp(G) programs are exactly the distributively computable functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307001</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307001</id><created>2003-06-30</created><authors><author><keyname>Greenlee</keyname><forenames>Herbert</forenames></author><author><keyname>Illingworth</keyname><forenames>Robert</forenames></author><author><keyname>Kowalkowski</keyname><forenames>Jim</forenames></author><author><keyname>Kumar</keyname><forenames>Anil</forenames></author><author><keyname>Lueking</keyname><forenames>Lee</forenames></author><author><keyname>Yasuda</keyname><forenames>Taka</forenames></author><author><keyname>Vittone</keyname><forenames>Margherita</forenames></author><author><keyname>White</keyname><forenames>Stephen</forenames></author></authors><title>Serving Database Information Using a Flexible Server in a Three Tier
  Architecture</title><categories>cs.DC cs.DB</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages. PSN THKT003</comments><acm-class>A.0;H.2.8</acm-class><journal-ref>ECONFC0303241:THKT003,2003</journal-ref><abstract>  The D0 experiment at Fermilab relies on a central Oracle database for storing
all detector calibration information. Access to this data is needed by hundreds
of physics applications distributed worldwide. In order to meet the demands of
these applications from scarce resources, we have created a distributed system
that isolates the user applications from the database facilities. This system,
known as the Database Application Network (DAN) operates as the middle tier in
a three tier architecture. A DAN server employs a hierarchical caching scheme
and database connection management facility that limits access to the database
resource. The modular design allows for caching strategies and database access
components to be determined by runtime configuration. To solve scalability
problems, a proxy database component allows for DAN servers to be arranged in a
hierarchy. Also included is an event based monitoring system that is currently
being used to collect statistics for performance analysis and problem
diagnosis. DAN servers are currently implemented as a Python multithreaded
program using CORBA for network communications and interface specification. The
requirement details, design, and implementation of DAN are discussed along with
operational experience and future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307002</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307002</id><created>2003-07-01</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>AWESOME: A General Multiagent Learning Algorithm that Converges in
  Self-Play and Learns a Best Response Against Stationary Opponents</title><categories>cs.GT cs.LG cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 20th International Conference on Machine
  Learning (ICML-03), Washington, DC, USA, 2003</journal-ref><abstract>  A satisfactory multiagent learning algorithm should, {\em at a minimum},
learn to play optimally against stationary opponents and converge to a Nash
equilibrium in self-play. The algorithm that has come closest, WoLF-IGA, has
been proven to have these two properties in 2-player 2-action repeated
games--assuming that the opponent's (mixed) strategy is observable. In this
paper we present AWESOME, the first algorithm that is guaranteed to have these
two properties in {\em all} repeated (finite) games. It requires only that the
other players' actual actions (not their strategies) can be observed at each
step. It also learns to play optimally against opponents that {\em eventually
become} stationary. The basic idea behind AWESOME ({\em Adapt When Everybody is
Stationary, Otherwise Move to Equilibrium}) is to try to adapt to the others'
strategies when they appear stationary, but otherwise to retreat to a
precomputed equilibrium strategy. The techniques used to prove the properties
of AWESOME are fundamentally different from those used for previous algorithms,
and may help in analyzing other multiagent learning algorithms also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307003</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307003</id><created>2003-07-02</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>How many candidates are needed to make elections hard to manipulate?</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 9th Conference on Theoretical Aspects of
  Rationality and Knowledge (TARK-03), pp. 201-214, Bloomington, Indiana, USA,
  2003</journal-ref><abstract>  In multiagent settings where the agents have different preferences,
preference aggregation is a central issue. Voting is a general method for
preference aggregation, but seminal results have shown that all general voting
protocols are manipulable. One could try to avoid manipulation by using voting
protocols where determining a beneficial manipulation is hard computationally.
The complexity of manipulating realistic elections where the number of
candidates is a small constant was recently studied (Conitzer 2002), but the
emphasis was on the question of whether or not a protocol becomes hard to
manipulate for some constant number of candidates. That work, in many cases,
left open the question: How many candidates are needed to make elections hard
to manipulate? This is a crucial question when comparing the relative
manipulability of different voting protocols. In this paper we answer that
question for the voting protocols of the earlier study: plurality, Borda, STV,
Copeland, maximin, regular cup, and randomized cup. We also answer that
question for two voting protocols for which no results on the complexity of
manipulation have been derived before: veto and plurality with runoff. It turns
out that the voting protocols under study become hard to manipulate at 3
candidates, 4 candidates, 7 candidates, or never.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307004</id><created>2003-07-02</created><authors><author><keyname>Abrams</keyname><forenames>A.</forenames></author><author><keyname>Ghrist</keyname><forenames>R.</forenames></author></authors><title>State complexes for metamorphic robots</title><categories>cs.RO cs.CG</categories><comments>19 pages; based on paper presented at Workshop in Algorithmic
  Foundations of Robotics, December 2002</comments><acm-class>I.2.9</acm-class><abstract>  A metamorphic robotic system is an aggregate of homogeneous robot units which
can individually and selectively locomote in such a way as to change the global
shape of the system. We introduce a mathematical framework for defining and
analyzing general metamorphic robots. This formal structure, combined with
ideas from geometric group theory, leads to a natural extension of a
configuration space for metamorphic robots -- the state complex -- which is
especially adapted to parallelization. We present an algorithm for optimizing
reconfiguration sequences with respect to elapsed time. A universal geometric
property of state complexes -- non-positive curvature -- is the key to proving
convergence to the globally time-optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307005</id><created>2003-07-02</created><updated>2004-06-10</updated><authors><author><keyname>Baran</keyname><forenames>Ilya</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author></authors><title>Optimal Adaptive Algorithms for Finding the Nearest and Farthest Point
  on a Parametric Black-Box Curve</title><categories>cs.CG cs.DS</categories><comments>19 pages, 8 figures; v3 adds relative-error results; v2 adds
  description of related work in introduction</comments><acm-class>F.2.2; I.3.5</acm-class><abstract>  We consider a general model for representing and manipulating parametric
curves, in which a curve is specified by a black box mapping a parameter value
between 0 and 1 to a point in Euclidean d-space. In this model, we consider the
nearest-point-on-curve and farthest-point-on-curve problems: given a curve C
and a point p, find a point on C nearest to p or farthest from p. In the
general black-box model, no algorithm can solve these problems. Assuming a
known bound on the speed of the curve (a Lipschitz condition), the answer can
be estimated up to an additive error of epsilon using O(1/epsilon) samples, and
this bound is tight in the worst case. However, many instances can be solved
with substantially fewer samples, and we give algorithms that adapt to the
inherent difficulty of the particular instance, up to a logarithmic factor.
More precisely, if OPT(C,p,epsilon) is the minimum number of samples of C that
every correct algorithm must perform to achieve tolerance epsilon, then our
algorithm performs O(OPT(C,p,epsilon) log (epsilon^(-1)/OPT(C,p,epsilon)))
samples. Furthermore, any algorithm requires Omega(k log (epsilon^(-1)/k))
samples for some instance C' with OPT(C',p,epsilon) = k; except that, for the
nearest-point-on-curve problem when the distance between C and p is less than
epsilon, OPT is 1 but the upper and lower bounds on the number of samples are
both Theta(1/epsilon). When bounds on relative error are desired, we give
algorithms that perform O(OPT log (2+(1+epsilon^(-1)) m^(-1)/OPT)) samples
(where m is the exact minimum or maximum distance from p to C) and prove that
Omega(OPT log (1/epsilon)) samples are necessary on some problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307006</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307006</id><created>2003-07-03</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>BL-WoLF: A Framework For Loss-Bounded Learnability In Zero-Sum Games</title><categories>cs.GT cs.LG cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 20th International Conference on Machine
  Learning (ICML-03), Washington, DC, USA, 2003</journal-ref><abstract>  We present BL-WoLF, a framework for learnability in repeated zero-sum games
where the cost of learning is measured by the losses the learning agent accrues
(rather than the number of rounds). The game is adversarially chosen from some
family that the learner knows. The opponent knows the game and the learner's
learning strategy. The learner tries to either not accrue losses, or to quickly
learn about the game so as to avoid future losses (this is consistent with the
Win or Learn Fast (WoLF) principle; BL stands for ``bounded loss''). Our
framework allows for both probabilistic and approximate learning. The resultant
notion of {\em BL-WoLF}-learnability can be applied to any class of games, and
allows us to measure the inherent disadvantage to a player that does not know
which game in the class it is in. We present {\em guaranteed
BL-WoLF-learnability} results for families of games with deterministic payoffs
and families of games with stochastic payoffs. We demonstrate that these
families are {\em guaranteed approximately BL-WoLF-learnable} with lower cost.
We then demonstrate families of games (both stochastic and deterministic) that
are not guaranteed BL-WoLF-learnable. We show that those families,
nevertheless, are {\em BL-WoLF-learnable}. To prove these results, we use a key
lemma which we derive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307007</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307007</id><created>2003-07-03</created><updated>2003-07-08</updated><authors><author><keyname>Baranovski</keyname><forenames>A.</forenames></author><author><keyname>Garzoglio</keyname><forenames>G.</forenames></author><author><keyname>Kreymer</keyname><forenames>A.</forenames></author><author><keyname>Lueking</keyname><forenames>L.</forenames></author><author><keyname>Stonjek</keyname><forenames>S.</forenames></author><author><keyname>Terekhov</keyname><forenames>I.</forenames></author><author><keyname>Wuerthwein</keyname><forenames>F.</forenames></author><author><keyname>Roy</keyname><forenames>A.</forenames></author><author><keyname>Mhashikar</keyname><forenames>P.</forenames></author><author><keyname>Murthi</keyname><forenames>V.</forenames></author><author><keyname>Tannenbaum</keyname><forenames>T.</forenames></author><author><keyname>Walker</keyname><forenames>R.</forenames></author><author><keyname>Ratnikov</keyname><forenames>F.</forenames></author><author><keyname>Rockwell</keyname><forenames>T.</forenames></author></authors><title>Management of Grid Jobs and Information within SAMGrid</title><categories>cs.DC</categories><comments>7 pages including figures, presented at CHEP 2003</comments><acm-class>c.1.4</acm-class><journal-ref>ECONF C0303241:TUAT002,2003</journal-ref><abstract>  We describe some of the key aspects of the SAMGrid system, used by the D0 and
CDF experiments at Fermilab. Having sustained success of the data handling part
of SAMGrid, we have developed new services for job and information services.
Our job management is rooted in \CondorG and uses enhancements that are general
applicability for HEP grids. Our information system is based on a uniform
framework for configuration management based on XML data representation and
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307008</id><created>2003-07-03</created><authors><author><keyname>Warner</keyname><forenames>Simeon</forenames></author></authors><title>Eprints and the Open Archives Initiative</title><categories>cs.DL</categories><comments>13 pages</comments><acm-class>H.3.7</acm-class><journal-ref>Library Hi Tech, Volume 21, Number 2, 151-158 (2003)</journal-ref><doi>10.1108/07378830310479794</doi><abstract>  The Open Archives Initiative (OAI) was created as a practical way to promote
interoperability between eprint repositories. Although the scope of the OAI has
been broadened, eprint repositories still represent a significant fraction of
OAI data providers. In this article I present a brief survey of OAI eprint
repositories, and of services using metadata harvested from eprint repositories
using the OAI protocol for metadata harvesting (OAI-PMH). I then discuss
several situations where metadata harvesting may be used to further improve the
utility of eprint archives as a component of the scholarly communication
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307009</id><created>2003-07-04</created><authors><author><keyname>Brisebarre</keyname><forenames>Nicolas</forenames></author><author><keyname>Muller</keyname><forenames>Jean-Michel</forenames></author></authors><title>Finding the &quot;truncated&quot; polynomial that is closest to a function</title><categories>cs.MS</categories><comments>14 pages, 1 figure</comments><acm-class>G.1.0, G.1.2, B.2.4</acm-class><abstract>  When implementing regular enough functions (e.g., elementary or special
functions) on a computing system, we frequently use polynomial approximations.
In most cases, the polynomial that best approximates (for a given distance and
in a given interval) a function has coefficients that are not exactly
representable with a finite number of bits. And yet, the polynomial
approximations that are actually implemented do have coefficients that are
represented with a finite - and sometimes small - number of bits: this is due
to the finiteness of the floating-point representations (for software
implementations), and to the need to have small, hence fast and/or inexpensive,
multipliers (for hardware implementations). We then have to consider polynomial
approximations for which the degree-$i$ coefficient has at most $m_i$
fractional bits (in other words, it is a rational number with denominator
$2^{m_i}$). We provide a general method for finding the best polynomial
approximation under this constraint. Then, we suggest refinements than can be
used to accelerate our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307010</id><created>2003-07-04</created><updated>2003-07-06</updated><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Probabilistic Reasoning as Information Compression by Multiple
  Alignment, Unification and Search: An Introduction and Overview</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><journal-ref>Journal of Universal Computer Science 5 (7), 418--462, 1999</journal-ref><abstract>  This article introduces the idea that probabilistic reasoning (PR) may be
understood as &quot;information compression by multiple alignment, unification and
search&quot; (ICMAUS). In this context, multiple alignment has a meaning which is
similar to but distinct from its meaning in bio-informatics, while unification
means a simple merging of matching patterns, a meaning which is related to but
simpler than the meaning of that term in logic.
  A software model, SP61, has been developed for the discovery and formation of
'good' multiple alignments, evaluated in terms of information compression. The
model is described in outline.
  Using examples from the SP61 model, this article describes in outline how the
ICMAUS framework can model various kinds of PR including: PR in best-match
pattern recognition and information retrieval; one-step 'deductive' and
'abductive' PR; inheritance of attributes in a class hierarchy; chains of
reasoning (probabilistic decision networks and decision trees, and PR with
'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with
default values; modelling the function of a Bayesian network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307011</id><created>2003-07-04</created><authors><author><keyname>Shenoy</keyname><forenames>Atul</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author></authors><title>Supporting Out-of-turn Interactions in a Multimodal Web Interface</title><categories>cs.IR cs.HC</categories><acm-class>H.5</acm-class><abstract>  Multimodal interfaces are becoming increasingly important with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. This article investigates systems
support for web browsing in a multimodal interface. Specifically, we outline
the design and implementation of a software framework that integrates hyperlink
and speech modes of interaction. Instead of viewing speech as merely an
alternative interaction medium, the framework uses it to support out-of-turn
interaction, providing a flexibility of information access not possible with
hyperlinks alone. This approach enables the creation of websites that adapt to
the needs of users, yet permits the designer fine-grained control over what
interactions to support. Design methodology, implementation details, and two
case studies are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307012</id><created>2003-07-04</created><updated>2003-07-06</updated><authors><author><keyname>Bansal</keyname><forenames>Sorav</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Observation-based Cooperation Enforcement in Ad Hoc Networks</title><categories>cs.NI</categories><comments>10 pages, 7 figures</comments><acm-class>C.2.3; C.2.4</acm-class><abstract>  Ad hoc networks rely on the cooperation of the nodes participating in the
network to forward packets for each other. A node may decide not to cooperate
to save its resources while still using the network to relay its traffic. If
too many nodes exhibit this behavior, network performance degrades and
cooperating nodes may find themselves unfairly loaded. Most previous efforts to
counter this behavior have relied on further cooperation between nodes to
exchange reputation information about other nodes. If a node observes another
node not participating correctly, it reports this observation to other nodes
who then take action to avoid being affected and potentially punish the bad
node by refusing to forward its traffic. Unfortunately, such second-hand
reputation information is subject to false accusations and requires maintaining
trust relationships with other nodes. The objective of OCEAN is to avoid this
trust-management machinery and see how far we can get simply by using direct
first-hand observations of other nodes' behavior. We find that, in many
scenarios, OCEAN can do as well as, or even better than, schemes requiring
second-hand reputation exchanges. This encouraging result could possibly help
obviate solutions requiring trust-management for some contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307013</id><created>2003-07-05</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>'Computing' as Information Compression by Multiple Alignment,
  Unification and Search</title><categories>cs.AI cs.CC</categories><acm-class>F.0</acm-class><journal-ref>Journal of Universal Computer Science 5(11), 776--815, 1999</journal-ref><abstract>  This paper argues that the operations of a 'Universal Turing Machine' (UTM)
and equivalent mechanisms such as the 'Post Canonical System' (PCS) - which are
widely accepted as definitions of the concept of `computing' - may be
interpreted as *information compression by multiple alignment, unification and
search* (ICMAUS).
  The motivation for this interpretation is that it suggests ways in which the
UTM/PCS model may be augmented in a proposed new computing system designed to
exploit the ICMAUS principles as fully as possible. The provision of a
relatively sophisticated search mechanism in the proposed 'SP' system appears
to open the door to the integration and simplification of a range of functions
including unsupervised inductive learning, best-match pattern recognition and
information retrieval, probabilistic reasoning, planning and problem solving,
and others. Detailed consideration of how the ICMAUS principles may be applied
to these functions is outside the scope of this article but relevant sources
are cited in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307014</id><created>2003-07-07</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Syntax, Parsing and Production of Natural Language in a Framework of
  Information Compression by Multiple Alignment, Unification and Search</title><categories>cs.AI cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Journal of Universal Computer Science 6(8), 781--829, 2000</journal-ref><abstract>  This article introduces the idea that &quot;information compression by multiple
alignment, unification and search&quot; (ICMAUS) provides a framework within which
natural language syntax may be represented in a simple format and the parsing
and production of natural language may be performed in a transparent manner.
  The ICMAUS concepts are embodied in a software model, SP61. The organisation
and operation of the model are described and a simple example is presented
showing how the model can achieve parsing of natural language.
  Notwithstanding the apparent paradox of 'decompression by compression', the
ICMAUS framework, without any modification, can produce a sentence by decoding
a compressed code for the sentence. This is illustrated with output from the
SP61 model.
  The article includes four other examples - one of the parsing of a sentence
in French and three from the domain of English auxiliary verbs. These examples
show how the ICMAUS framework and the SP61 model can accommodate 'context
sensitive' features of syntax in a relatively simple and direct manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307015</id><created>2003-07-07</created><updated>2006-06-10</updated><authors><author><keyname>Ling</keyname><forenames>Maurice HT</forenames></author><author><keyname>So</keyname><forenames>Chi Wai</forenames></author></authors><title>Architecture of an Open-Sourced, Extensible Data Warehouse Builder:
  InterBase 6 Data Warehouse Builder (IB-DWB)</title><categories>cs.DB</categories><acm-class>H.2.8; H.4.2</acm-class><journal-ref>Ling, Maurice HT and So, Chi Wai. 2003. Proceedings of the First
  Australian Undergraduate Students' Computing Conference. (pp. 40-45)</journal-ref><abstract>  We report the development of an open-sourced data warehouse builder,
InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open
Edition Database Server. InterBase 6 is used for its low maintenance and small
footprint. IB-DWB is designed modularly and consists of 5 main components, Data
Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query
Supporter, bounded together by a Kernel. It is also an extensible system, made
possible by the Data Plug Platform and the Discoverer Platform. Currently,
extensions are only possible via dynamic linked-libraries (DLLs).
Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The
architectural philosophy of IB-DWB centers around providing a base platform
that is extensible, which is functionally supported by expansion modules.
IB-DWB is currently being hosted by sourceforge.net (Project Unix Name:
ib-dwb), licensed under GNU General Public License, Version 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307016</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307016</id><created>2003-07-07</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Complexity of Determining Nonemptiness of the Core</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 18th International Joint Conference on
  Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 2003</journal-ref><abstract>  Coalition formation is a key problem in automated negotiation among
self-interested agents, and other multiagent applications. A coalition of
agents can sometimes accomplish things that the individual agents cannot, or
can do things more efficiently. However, motivating the agents to abide to a
solution requires careful analysis: only some of the solutions are stable in
the sense that no group of agents is motivated to break off and form a new
coalition. This constraint has been studied extensively in cooperative game
theory. However, the computational questions around this constraint have
received less attention. When it comes to coalition formation among software
agents (that represent real-world parties), these questions become increasingly
explicit.
  In this paper we define a concise general representation for games in
characteristic form that relies on superadditivity, and show that it allows for
efficient checking of whether a given outcome is in the core. We then show that
determining whether the core is nonempty is $\mathcal{NP}$-complete both with
and without transferable utility. We demonstrate that what makes the problem
hard in both cases is determining the collaborative possibilities (the set of
outcomes possible for the grand coalition), by showing that if these are given,
the problem becomes tractable in both cases. However, we then demonstrate that
for a hybrid version of the problem, where utility transfer is possible only
within the grand coalition, the problem remains $\mathcal{NP}$-complete even
when the collaborative possibilities are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307017</id><created>2003-07-07</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Definition and Complexity of Some Basic Metareasoning Problems</title><categories>cs.AI cs.CC</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 18th International Joint Conference on
  Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 2003</journal-ref><abstract>  In most real-world settings, due to limited time or other resources, an agent
cannot perform all potentially useful deliberation and information gathering
actions. This leads to the metareasoning problem of selecting such actions.
Decision-theoretic methods for metareasoning have been studied in AI, but there
are few theoretical results on the complexity of metareasoning.
  We derive hardness results for three settings which most real metareasoning
systems would have to encompass as special cases. In the first, the agent has
to decide how to allocate its deliberation time across anytime algorithms
running on different problem instances. We show this to be
$\mathcal{NP}$-complete. In the second, the agent has to (dynamically) allocate
its deliberation or information gathering resources across multiple actions
that it has to choose among. We show this to be $\mathcal{NP}$-hard even when
evaluating each individual action is extremely simple. In the third, the agent
has to (dynamically) choose a limited number of deliberation or information
gathering actions to disambiguate the state of the world. We show that this is
$\mathcal{NP}$-hard under a natural restriction, and $\mathcal{PSPACE}$-hard in
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307018</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307018</id><created>2003-07-07</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Universal Voting Protocol Tweaks to Make Manipulation Hard</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 18th International Joint Conference on
  Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 2003</journal-ref><abstract>  Voting is a general method for preference aggregation in multiagent settings,
but seminal results have shown that all (nondictatorial) voting protocols are
manipulable. One could try to avoid manipulation by using voting protocols
where determining a beneficial manipulation is hard computationally. A number
of recent papers study the complexity of manipulating existing protocols. This
paper is the first work to take the next step of designing new protocols that
are especially hard to manipulate. Rather than designing these new protocols
from scratch, we instead show how to tweak existing protocols to make
manipulation hard, while leaving much of the original nature of the protocol
intact. The tweak studied consists of adding one elimination preround to the
election. Surprisingly, this extremely simple and universal tweak makes typical
protocols hard to manipulate! The protocols become NP-hard, #P-hard, or
PSPACE-hard to manipulate, depending on whether the schedule of the preround is
determined before the votes are collected, after the votes are collected, or
the scheduling and the vote collecting are interleaved, respectively. We prove
general sufficient conditions on the protocols for this tweak to introduce the
hardness, and show that the most common voting protocols satisfy those
conditions. These are the first results in voting settings where manipulation
is in a higher complexity class than NP (presuming PSPACE $\neq$ NP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307019</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307019</id><created>2003-07-08</created><authors><author><keyname>Holmgren</keyname><forenames>D.</forenames></author><author><keyname>Singh</keyname><forenames>A.</forenames></author><author><keyname>Mackenzie</keyname><forenames>P.</forenames></author><author><keyname>Simone</keyname><forenames>J.</forenames></author></authors><title>Lattice QCD Production on Commodity Clusters at Fermilab</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, LaTeX, 8 eps figures. PSN
  TUIT004</comments><acm-class>C.4;C.1.4;D.1.3</acm-class><journal-ref>ECONFC0303241:TUIT004,2003</journal-ref><abstract>  We describe the construction and results to date of Fermilab's three
Myrinet-networked lattice QCD production clusters (an 80-node dual Pentium III
cluster, a 48-node dual Xeon cluster, and a 128-node dual Xeon cluster). We
examine a number of aspects of performance of the MILC lattice QCD code running
on these clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307020</id><created>2003-07-08</created><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>Defying Dimensions Mod 6</title><categories>cs.CC</categories><comments>Preliminary Version</comments><acm-class>F.1</acm-class><abstract>  We show that a certain representation of the matrix-product can be computed
with $n^{o(1)}$ multiplications. We also show, that siumilar representations of
matrices can be compressed enormously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307021</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307021</id><created>2003-07-08</created><authors><author><keyname>Singh</keyname><forenames>A.</forenames></author><author><keyname>Holmgren</keyname><forenames>D.</forenames></author><author><keyname>Rechenmacher</keyname><forenames>R.</forenames></author><author><keyname>Epsteyn</keyname><forenames>S.</forenames></author></authors><title>Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at
  Fermilab</title><categories>cs.DC</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, PDF. PSN TUIT005</comments><acm-class>C.4; C.m</acm-class><journal-ref>ECONFC0303241:TUIT005,2003</journal-ref><abstract>  Fermilab operates several clusters for lattice gauge computing. Minimal
manpower is available to manage these clusters. We have written a number of
tools and developed techniques to cope with this task. We describe our tools
which use the IPMI facilities of our systems for hardware management tasks such
as remote power control, remote system resets, and health monitoring. We
discuss our techniques involving network booting for installation and upgrades
of the operating system on these computers, and for reloading BIOS and other
firmware. Finally, we discuss our tools for parallel command processing and
their use in monitoring and administrating the PBS batch queue system used on
our clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307022</id><created>2003-07-09</created><updated>2004-02-20</updated><authors><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author></authors><title>Transformations of Logic Programs with Goals as Arguments</title><categories>cs.PL cs.LO</categories><comments>51 pages. Full version of a paper that will appear in Theory and
  Practice of Logic Programming, Cambridge University Press, UK</comments><acm-class>D.1.2;D.1.6;I.2.2;F.3.1</acm-class><abstract>  We consider a simple extension of logic programming where variables may range
over goals and goals may be arguments of predicates. In this language we can
write logic programs which use goals as data. We give practical evidence that,
by exploiting this capability when transforming programs, we can improve
program efficiency.
  We propose a set of program transformation rules which extend the familiar
unfolding and folding rules and allow us to manipulate clauses with goals which
occur as arguments of predicates. In order to prove the correctness of these
transformation rules, we formally define the operational semantics of our
extended logic programming language. This semantics is a simple variant of
LD-resolution. When suitable conditions are satisfied this semantics agrees
with LD-resolution and, thus, the programs written in our extended language can
be run by ordinary Prolog systems.
  Our transformation rules are shown to preserve the operational semantics and
termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307023</identifier>
 <datestamp>2009-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307023</id><created>2003-07-09</created><updated>2007-11-16</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Testing Bipartiteness of Geometric Intersection Graphs</title><categories>cs.CG</categories><comments>32 pages, 20 figures. A shorter (10 page) version of this paper was
  presented at the 15th ACM-SIAM Symp. Discrete Algorithms, New Orleans, 2004,
  pp. 853-861</comments><acm-class>F.2.2</acm-class><journal-ref>ACM Trans. Algorithms 5(2):15, 2009</journal-ref><doi>10.1145/1497290.1497291</doi><abstract>  We show how to test the bipartiteness of an intersection graph of n line
segments or simple polygons in the plane, or of balls in R^d, in time O(n log
n). More generally we find subquadratic algorithms for connectivity and
bipartiteness testing of intersection graphs of a broad class of geometric
objects. For unit balls in R^d, connectivity testing has equivalent randomized
complexity to construction of Euclidean minimum spanning trees, and hence is
unlikely to be solved as efficiently as bipartiteness testing. For line
segments or planar disks, testing k-colorability of intersection graphs for k&gt;2
is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307024</id><created>2003-07-10</created><authors><author><keyname>Ciuffoletti</keyname><forenames>A.</forenames></author><author><keyname>Ghiselli</keyname><forenames>T. Ferrari A.</forenames></author><author><keyname>Vistoli</keyname><forenames>C.</forenames></author></authors><title>Architecture of monitoring elements for the network element modeling in
  a Grid infrastructure</title><categories>cs.NI</categories><acm-class>C.2.4</acm-class><abstract>  Several tools exist that collect host-to-host connectivity measurements. To
improve the usability of such measurements, they should be mapped into a
framework consisting of complex subsystems, and the infrastructure that
connects them. We introduce one such framework, and analyze the architectural
implications on the network structure. In our framework, a complex subsystem
consists of several computing facilities and the infrastructure that connects
them: we call it a -monitoring domain-. The task of measuring the connectivity
between -monitoring domains- is considered distinct from the activity of
-storage- and -computing- elements. Therefore we introduce a new element in our
topology: we call it -theodolite- element, since its function is similar to
that of a transponder. Using these basic concepts, we analyze the architectural
implications on the network structure: in a nutshell, if we want that
-theodolites- serve as a reference, than the contribution to the relevant
network metrics due to the -monitoring domain- infrastructure must be
negligible with respect to contributions of the inter-domain infrastructure. In
addition all -theodolites- of a -monitoring domain- must give an image of the
inter-domain infrastructure that is consistent with that experienced by network
applications. We conclude giving a running SQL example of how information about
-monitoring domains- and -theodolites- could be organized, and we outline the
application of such framework in the GLUE schema activity for the network
element
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307025</id><created>2003-07-10</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Information Compression by Multiple Alignment, Unification and Search as
  a Unifying Principle in Computing and Cognition</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Artificial Intelligence Review 19(3), 193-230, 2003</journal-ref><abstract>  This article presents an overview of the idea that &quot;information compression
by multiple alignment, unification and search&quot; (ICMAUS) may serve as a unifying
principle in computing (including mathematics and logic) and in such aspects of
human cognition as the analysis and production of natural language, fuzzy
pattern recognition and best-match information retrieval, concept hierarchies
with inheritance of attributes, probabilistic reasoning, and unsupervised
inductive learning. The ICMAUS concepts are described together with an outline
of the SP61 software model in which the ICMAUS concepts are currently realised.
A range of examples is presented, illustrated with output from the SP61 model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307026</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307026</id><created>2003-07-10</created><authors><author><keyname>Kotturi</keyname><forenames>Karen D.</forenames></author></authors><title>Improving the Security and Performance of the BaBar Detector Controls
  System</title><categories>cs.NI</categories><acm-class>C.2.3</acm-class><abstract>  It starts out innocently enough - users want to monitor Online data and so
run their own copies of the detector control GUIs in their offices and at home.
But over time, the number of processes making requests for values to display on
GUIs, webpages and stripcharts can grow, and affect the performance of an
Input/Output Controller (IOC) such that it is unable to respond to requests
from requests critical to data-taking. At worst, an IOC can hang, its CPU
having been allocated 100% to responding to network requests.
  For the BaBar Online Detector Control System, we were able to eliminate this
problem and make great gains in security by moving all of the IOCs to a
non-routed, virtual LAN and by enlisting a workstation with two network
interface cards to act as the interface between the virtual LAN and the public
BaBar network. On the interface machine, we run the Experimental Physics
Industrial Control System (EPICS) Channel Access (CA) gateway software
(originating from Advanced Photon Source). This software accepts as inputs, all
the channels which are loaded into the EPICS databases on all the IOCs. It
polls them to update its copy of the values. It answers requests from
applications by sending them the currently cached value.
  We adopted the requirement that data-taking would be independent of the
gateway, so that, in the event of a gateway failure, data-taking would be
uninterrupted. In this way, we avoided introducing any new risk elements to
data-taking. Security rules already in use by the IOC were propagated to the
gateway's own security rules and the security of the IOCs themselves was
improved by removing them from the public BaBar network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307027</identifier>
 <datestamp>2007-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307027</id><created>2003-07-10</created><authors><author><keyname>Bagchi</keyname><forenames>Amitabha</forenames></author><author><keyname>Chaudhary</keyname><forenames>Amitabh</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author></authors><title>Deterministic Sampling and Range Counting in Geometric Data Streams</title><categories>cs.CG</categories><comments>12 pages, 1 figure</comments><acm-class>F.2.2</acm-class><journal-ref>ACM Trans. Algorithms 3(2):A16, 2007</journal-ref><doi>10.1145/1240233.1240239</doi><abstract>  We present memory-efficient deterministic algorithms for constructing
epsilon-nets and epsilon-approximations of streams of geometric data. Unlike
probabilistic approaches, these deterministic samples provide guaranteed bounds
on their approximation factors. We show how our deterministic samples can be
used to answer approximate online iceberg geometric queries on data streams. We
use these techniques to approximate several robust statistics of geometric data
streams, including Tukey depth, simplicial depth, regression depth, the
Thiel-Sen estimator, and the least median of squares. Our algorithms use only a
polylogarithmic amount of memory, provided the desired approximation factors
are inverse-polylogarithmic. We also include a lower bound for non-iceberg
geometric queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307028</id><created>2003-07-11</created><authors><author><keyname>Hasida</keyname><forenames>Koiti</forenames></author></authors><title>Issues in Communication Game</title><categories>cs.CL</categories><comments>6 pages, 5 figures, Proceedings of the 16th International Conference
  on Computational Linguistics, pp.531-536</comments><acm-class>J.5; I.2.7</acm-class><abstract>  As interaction between autonomous agents, communication can be analyzed in
game-theoretic terms. Meaning game is proposed to formalize the core of
intended communication in which the sender sends a message and the receiver
attempts to infer its meaning intended by the sender. Basic issues involved in
the game of natural language communication are discussed, such as salience,
grammaticality, common sense, and common belief, together with some
demonstration of the feasibility of game-theoretic account of language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307029</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307029</id><created>2003-07-11</created><authors><author><keyname>de Vries</keyname><forenames>Andreas</forenames></author></authors><title>The ray attack, an inefficient trial to break RSA cryptosystems</title><categories>cs.CR</categories><comments>18 pages, 4 figures</comments><acm-class>E.3; K.4.4; K.6.5</acm-class><abstract>  The basic properties of RSA cryptosystems and some classical attacks on them
are described. Derived from geometric properties of the Euler functions, the
Euler function rays, a new ansatz to attack RSA cryptosystems is presented. A
resulting, albeit inefficient, algorithm is given. It essentially consists of a
loop with starting value determined by the Euler function ray and with step
width given by a function $\omega_e(n)$ being a multiple of the order
$\mathrm{ord}_n(e)$, where $e$ denotes the public key exponent and $n$ the RSA
modulus. For $n=pq$ and an estimate $r&lt;\sqrt{pq}$ for the smaller prime factor
$p$, the running time is given by $T(e,n,r) = O((r-p)\ln e \ln n \ln r).$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307030</id><created>2003-07-11</created><authors><author><keyname>Hasida</keyname><forenames>Koiti</forenames></author><author><keyname>Miyata</keyname><forenames>Takashi</forenames></author></authors><title>Parsing and Generation with Tabulation and Compilation</title><categories>cs.CL</categories><comments>8 pages, 5 figures, Proceedings of TAPD'98, pp.26-35</comments><acm-class>I.2.7; D.1.6</acm-class><abstract>  The standard tabulation techniques for logic programming presuppose fixed
order of computation. Some data-driven control should be introduced in order to
deal with diverse contexts. The present paper describes a data-driven method of
constraint transformation with a sort of compilation which subsumes
accessibility check and last-call optimization, which characterize standard
natural-language parsing techniques, semantic-head-driven generation, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307031</id><created>2003-07-12</created><updated>2003-07-16</updated><authors><author><keyname>Boinee</keyname><forenames>P.</forenames></author><author><keyname>De Angelis</keyname><forenames>A.</forenames></author><author><keyname>Milotti</keyname><forenames>E.</forenames></author></authors><title>Automatic Classification using Self-Organising Neural Networks in
  Astrophysical Experiments</title><categories>cs.NE astro-ph cs.AI</categories><comments>9 Pages, corrected authors name format</comments><acm-class>I.5.1; I.5.3</acm-class><journal-ref>S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 177</journal-ref><abstract>  Self-Organising Maps (SOMs) are effective tools in classification problems,
and in recent years the even more powerful Dynamic Growing Neural Networks, a
variant of SOMs, have been developed. Automatic Classification (also called
clustering) is an important and difficult problem in many Astrophysical
experiments, for instance, Gamma Ray Burst classification, or gamma-hadron
separation. After a brief introduction to classification problem, we discuss
Self-Organising Maps in section 2. Section 3 discusses with various models of
growing neural networks and finally in section 4 we discuss the research
perspectives in growing neural networks for efficient classification in
astrophysical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307032</id><created>2003-07-12</created><updated>2003-07-16</updated><authors><author><keyname>Frailis</keyname><forenames>M.</forenames></author><author><keyname>De Angelis</keyname><forenames>A.</forenames></author><author><keyname>Roberto</keyname><forenames>V.</forenames></author></authors><title>Data Management and Mining in Astrophysical Databases</title><categories>cs.DB astro-ph physics.data-an</categories><comments>10 pages, Latex</comments><acm-class>H.2.4; H.2.8</acm-class><journal-ref>S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 157</journal-ref><abstract>  We analyse the issues involved in the management and mining of astrophysical
data. The traditional approach to data management in the astrophysical field is
not able to keep up with the increasing size of the data gathered by modern
detectors. An essential role in the astrophysical research will be assumed by
automatic tools for information extraction from large datasets, i.e. data
mining techniques, such as clustering and classification algorithms. This asks
for an approach to data management based on data warehousing, emphasizing the
efficiency and simplicity of data access; efficiency is obtained using
multidimensional access methods and simplicity is achieved by properly handling
metadata. Clustering and classification techniques, on large datasets, pose
additional requirements: computational and memory scalability with respect to
the data size, interpretability and objectivity of clustering or classification
results. In this study we address some possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307033</id><created>2003-07-13</created><authors><author><keyname>Kadanoff</keyname><forenames>Leo P.</forenames></author></authors><title>Excellence in Computer Simulation</title><categories>cs.NA physics.comp-ph</categories><comments>30 pages, 8 figures</comments><acm-class>J.2; K.2</acm-class><abstract>  Excellent computer simulations are done for a purpose. The most valid
purposes are to explore uncharted territory, to resolve a well-posed scientific
or technical question, or to make a design choice. Stand-alone modeling can
serve the first purpose. The other two goals need a full integration of the
modeling effort into a scientific or engineering program.
  Some excellent work, much of it related to the Department of Energy
Laboratories, is reviewed. Some less happy stories are recounted.
  In the past, some of the most impressive work has involved complexity and
chaos. Prediction in a complex world requires a first principles understanding
based upon the intersection of theory, experiment and simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307034</id><created>2003-07-12</created><authors><author><keyname>Krizanc</keyname><forenames>Danny</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Range Mode and Range Median Queries on Lists and Trees</title><categories>cs.DS</categories><comments>12 pages, 6 figures</comments><acm-class>E.1</acm-class><abstract>  We consider algorithms for preprocessing labelled lists and trees so that,
for any two nodes u and v we can answer queries of the form: What is the mode
or median label in the sequence of labels on the path from u to v.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307035</id><created>2003-07-13</created><authors><author><keyname>Motuzenko</keyname><forenames>Pavel</forenames></author></authors><title>Adaptive Domain Model: Dealing With Multiple Attributes of Self-Managing
  Distributed Object Systems</title><categories>cs.AR cs.DC</categories><comments>6 pages, 3 figures</comments><acm-class>C.1.3</acm-class><abstract>  Self-managing software has emerged as modern systems have become more
complex. Some of the distributed object systems may contain thousands of
objects deployed on tens or even hundreds hosts. Development and support of
such systems often costs a lot. To solve this issue the systems, which are
capable supporting multiple self-managing attributes, should be created. In the
paper, the Adaptive domain concept is introduced as an extension to the basic
domain concept to support a generic adaptation environment for building
distributed object systems with multiple self-managing attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307036</id><created>2003-07-13</created><authors><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>Small-World File-Sharing Communities</title><categories>cs.DC cond-mat cs.NI</categories><acm-class>C.2.3</acm-class><abstract>  Web caches, content distribution networks, peer-to-peer file sharing
networks, distributed file systems, and data grids all have in common that they
involve a community of users who generate requests for shared data. In each
case, overall system performance can be improved significantly if we can first
identify and then exploit interesting structure within a community's access
patterns. To this end, we propose a novel perspective on file sharing based on
the study of the relationships that form among users based on the files in
which they are interested.
  We propose a new structure that captures common user interests in data--the
data-sharing graph-- and justify its utility with studies on three
data-distribution systems: a high-energy physics collaboration, the Web, and
the Kazaa peer-to-peer network. We find small-world patterns in the
data-sharing graphs of all three communities. We analyze these graphs and
propose some probable causes for these emergent small-world patterns. The
significance of small-world patterns is twofold: it provides a rigorous support
to intuition and, perhaps most importantly, it suggests ways to design
mechanisms that exploit these naturally emerging patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307037</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307037</id><created>2003-07-14</created><authors><author><keyname>Agarwal</keyname><forenames>D.</forenames></author><author><keyname>Berket</keyname><forenames>K.</forenames></author></authors><title>Supporting Dynamic Ad hoc Collaboration Capabilities</title><categories>cs.OH cs.AI</categories><comments>Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, PSN MONT011</comments><report-no>LBNL-53355</report-no><acm-class>H.5.3</acm-class><journal-ref>ECONF C0303241:MONT011,2003</journal-ref><abstract>  Modern HENP experiments such as CMS and Atlas involve as many as 2000
collaborators around the world. Collaborations this large will be unable to
meet often enough to support working closely together. Many of the tools
currently available for collaboration focus on heavy-weight applications such
as videoconferencing tools. While these are important, there is a more basic
need for tools that support connecting physicists to work together on an ad hoc
or continuous basis. Tools that support the day-to-day connectivity and
underlying needs of a group of collaborators are important for providing
light-weight, non-intrusive, and flexible ways to work collaboratively. Some
example tools include messaging, file-sharing, and shared plot viewers. An
important component of the environment is a scalable underlying communication
framework. In this paper we will describe our current progress on building a
dynamic and ad hoc collaboration environment and our vision for its evolution
into a HENP collaboration environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307038</id><created>2003-07-16</created><authors><author><keyname>Costa</keyname><forenames>Jose</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Manifold Learning with Geodesic Minimal Spanning Trees</title><categories>cs.CV cs.LG</categories><comments>13 pages, 3 figures</comments><acm-class>G.3;F.2.2</acm-class><abstract>  In the manifold learning problem one seeks to discover a smooth low
dimensional surface, i.e., a manifold embedded in a higher dimensional linear
vector space, based on a set of measured sample points on the surface. In this
paper we consider the closely related problem of estimating the manifold's
intrinsic dimension and the intrinsic entropy of the sample points.
Specifically, we view the sample points as realizations of an unknown
multivariate density supported on an unknown smooth manifold. We present a
novel geometrical probability approach, called the
geodesic-minimal-spanning-tree (GMST), to obtaining asymptotically consistent
estimates of the manifold dimension and the R\'{e}nyi $\alpha$-entropy of the
sample density on the manifold. The GMST approach is striking in its simplicity
and does not require reconstructing the manifold or estimating the multivariate
density of the samples. The GMST method simply constructs a minimal spanning
tree (MST) sequence using a geodesic edge matrix and uses the overall lengths
of the MSTs to simultaneously estimate manifold dimension and entropy. We
illustrate the GMST approach for dimension and entropy estimation of a human
face dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307039</id><created>2003-07-17</created><authors><author><keyname>Vitolins</keyname><forenames>Valdis</forenames></author><author><keyname>Kalnins</keyname><forenames>Audris</forenames></author></authors><title>Modeling Business</title><categories>cs.CE</categories><comments>5 pages, 7 figures, Proceedings of Internation Conference 'Modeling
  and Simulation of Business Systems&quot;, Vilnius, Lithuania (May 13-14, 2003)</comments><acm-class>I.6.5; J.1</acm-class><journal-ref>Vitolins Valdis, Audris Kalnins. Modeling Business. Modeling and
  Simulation of Business Systems, Kaunas University of Technology Press,
  Vilnius, May 13-14, 2003, pp. 215.-220.</journal-ref><abstract>  Business concepts are studied using a metamodel-based approach, using UML
2.0. The Notation Independent Business concepts metamodel is introduced. The
approach offers a mapping between different business modeling notations which
could be used for bridging BM tools and boosting the MDA approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307040</id><created>2003-07-17</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>Bridging the gap between modal temporal logics and constraint-based QSR
  as an ALC(D) spatio-temporalisation with weakly cyclic TBoxes</title><categories>cs.AI cs.LO</categories><comments>58 pages, 7 figures (I have only split each of Figures 1, 2 and 3, as
  it appears in the first version, into two figures, so that the number of
  figures is now 7 instead of the original 4 -it is expected that the reader
  will find the look of the paper better)</comments><report-no>Technical Report FBI-HH-M-311/02, Fachbereich Informatik,
  Universitaet Hamburg</report-no><acm-class>I.2 (I.2.4)</acm-class><abstract>  The aim of this work is to provide a family of qualitative theories for
spatial change in general, and for motion of spatial scenes in particular. To
achieve this, we consider a spatio-temporalisation MTALC(D_x), of the
well-known ALC(D) family of Description Logics (DLs) with a concrete domainan.
In particular, the concrete domain D_x is generated by a qualitative spatial
Relation Algebra (RA) x. We show the important result that satisfiability of an
MTALC(D_x) concept with respect to a weakly cyclic TBox is decidable in
nondeterministic exponential time, by reducing it to the emptiness problem of a
weak alternating automaton augmented with spatial constraints, which we show to
remain decidable, although the accepting condition of a run involves,
additionally to the standard case, consistency of a CSP (Constraint
Satisfaction Problem) potentially infinite. The result provides an effective
tableaux-like satisfiability procedure which is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307041</id><created>2003-07-17</created><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>High-density and Secure Data Transmission via Linear Combinations</title><categories>cs.CC cs.AR</categories><comments>Preliminary version</comments><acm-class>F.1</acm-class><abstract>  Suppose that there are $n$ Senders and $n$ Receivers. Our goal is to send
long messages from Sender $i$ to Receiver $i$ such that no other receiver can
retrieve the message intended for Receiver $i$. The task can easily be
completed using $n$ private channels between the pairs. Solutions, using one
channel needs either encryption or switching elements for routing the messages
to their addressee.
  The main result of the present work is a description of a network in which
The Senders and the Receivers are connected with only $n^{o(1)}$ channels; the
encoding and de-coding is nothing else just very fast linear combinations of
the message-bits; and there are no switching or routing-elements in the
network, just linear combinations are computed, with fixed connections
(channels or wires).
  In the proofs we do not use {\em any} unproven cryptographical or complexity
theoretical assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307042</id><created>2003-07-17</created><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>A Note on Objects Built From Bricks without Corners</title><categories>cs.CG cs.DM</categories><comments>5 pages, 3 figures</comments><acm-class>F.2.2,G.2</acm-class><abstract>  We report a small advance on a question raised by Robertson, Schweitzer, and
Wagon in [RSW02]. They constructed a genus-13 polyhedron built from bricks
without corners, and asked whether every genus-0 such polyhedron must have a
corner. A brick is a parallelopiped, and a corner is a brick of degree three or
less in the brick graph. We describe a genus-3 polyhedron built from bricks
with no corner, narrowing the genus gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307043</id><created>2003-07-18</created><authors><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>An Extension of the Lovasz Local Lemma, and its Applications to Integer
  Programming</title><categories>cs.DS</categories><comments>22 pages, preliminary version appeared in the SODA 1996 conference</comments><acm-class>F.1.2; F.2.2; G.2.2; G.3</acm-class><abstract>  The Lovasz Local Lemma due to Erdos and Lovasz is a powerful tool in proving
the existence of rare events. We present an extension of this lemma, which
works well when the event to be shown to exist is a conjunction of individual
events, each of which asserts that a random variable does not deviate much from
its mean. As applications, we consider two classes of NP-hard integer programs:
minimax and covering integer programs. A key technique, randomized rounding of
linear relaxations, was developed by Raghavan and Thompson to derive good
approximation algorithms for such problems. We use our extension of the Local
Lemma to prove that randomized rounding produces, with non-zero probability,
much better feasible solutions than known before, if the constraint matrices of
these integer programs are column-sparse (e.g., routing using short paths,
problems on hypergraphs with small dimension/degree). This complements certain
well-known results from discrepancy theory. We also generalize the method of
pessimistic estimators due to Raghavan, to obtain constructive (algorithmic)
versions of our results for covering integer programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307044</id><created>2003-07-19</created><authors><author><keyname>Hasida</keyname><forenames>Koiti</forenames></author></authors><title>The Linguistic DS: Linguisitic Description in MPEG-7</title><categories>cs.CL</categories><comments>40 pages, 4 figures</comments><acm-class>I.2.7</acm-class><abstract>  MPEG-7 (Moving Picture Experts Group Phase 7) is an XML-based international
standard on semantic description of multimedia content. This document discusses
the Linguistic DS and related tools. The linguistic DS is a tool, based on the
GDA tag set (http://i-content.org/GDA/tagset.html), for semantic annotation of
linguistic data in or associated with multimedia content. The current document
text reflects `Study of FPDAM - MPEG-7 MDS Extensions' issued in March 2003,
and not most part of MPEG-7 MDS, for which the readers are referred to the
first version of MPEG-7 MDS document available from ISO (http://www.iso.org).
Without that reference, however, this document should be mostly intelligible to
those who are familiar with XML and linguistic theories. Comments are welcome
and will be considered in the standardization process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307045</id><created>2003-07-19</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames><affiliation>Utah State University</affiliation></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames><affiliation>Utah State University</affiliation></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames><affiliation>Utah State University</affiliation></author></authors><title>Flexible Camera Calibration Using a New Analytical Radial Undistortion
  Formula with Application to Mobile Robot Localization</title><categories>cs.CV</categories><comments>6 pages, 2 Postscript figures</comments><acm-class>I.4.1</acm-class><abstract>  Most algorithms in 3D computer vision rely on the pinhole camera model
because of its simplicity, whereas virtually all imaging devices introduce
certain amount of nonlinear distortion, where the radial distortion is the most
severe part. Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved. An application of the new radial distortion model is non-iterative
yellow line alignment with a calibrated camera on ODIS, a robot built in our
CSOIS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307046</id><created>2003-07-20</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>A New Analytical Radial Distortion Model for Camera Calibration</title><categories>cs.CV</categories><comments>2 Postscript figures</comments><acm-class>I.4.1</acm-class><abstract>  Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307047</id><created>2003-07-20</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>Rational Radial Distortion Models with Analytical Undistortion Formulae</title><categories>cs.CV</categories><comments>6 pages</comments><acm-class>I.4.1</acm-class><abstract>  The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
class of rational radial distortion models with easy analytical undistortion
formulae. Experimental results are presented to show that with this class of
rational radial distortion models, satisfactory and comparable accuracy is
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307048</id><created>2003-07-21</created><updated>2004-10-05</updated><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>Integrating cardinal direction relations and other orientation relations
  in Qualitative Spatial Reasoning</title><categories>cs.AI</categories><comments>Includes new material, such as a section on the use of the work in
  the concrete domain of the ALC(D) spatio-temporalisation defined in
  http://arXiv.org/abs/cs.AI/0307040</comments><report-no>Technical report FBI-HH-M-304/01, Fachbereich Informatik,
  Universitaet Hamburg</report-no><acm-class>I.2 (I.2.4)</acm-class><abstract>  We propose a calculus integrating two calculi well-known in Qualitative
Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus,
and a coarser version of Freksa's relative orientation calculus. An original
constraint propagation procedure is presented, which implements the interaction
between the two integrated calculi. The importance of taking into account the
interaction is shown with a real example providing an inconsistent knowledge
base, whose inconsistency (a) cannot be detected by reasoning separately about
each of the two components of the knowledge, just because, taken separately,
each is consistent, but (b) is detected by the proposed algorithm, thanks to
the interaction knowledge propagated from each of the two compnents to the
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307049</id><created>2003-07-21</created><authors><author><keyname>Guirardel</keyname><forenames>Vincent</forenames></author></authors><title>Limit groups and groups acting freely on $\bbR^n$-trees</title><categories>cs.DL</categories><acm-class>F.2.2</acm-class><abstract>  We give a simple proof of the finite presentation of Sela's limit groups by
using free actions on $\bbR^n$-trees. We first prove that Sela's limit groups
do have a free action on an $\bbR^n$-tree. We then prove that a finitely
generated group having a free action on an $\bbR^n$-tree can be obtained from
free abelian groups and surface groups by a finite sequence of free products
and amalgamations over cyclic groups. As a corollary, such a group is finitely
presented, has a finite classifying space, its abelian subgroups are finitely
generated and contains only finitely many conjugacy classes of non-cyclic
maximal abelian subgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307050</id><created>2003-07-21</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>A ternary Relation Algebra of directed lines</title><categories>cs.AI</categories><comments>60 pages. Submitted. Technical report mentioned in &quot;Report-no&quot; below
  is an earlier version of the work, and its title differs slightly (Reasoning
  about relative position of directed lines as a ternary Relation Algebra (RA):
  presentation of the RA and of its use in the concrete domain of an
  ALC(D)-like description logic)</comments><report-no>Technical report FBI-HH-M-313/02, Fachbereich Informatik,
  Universitaet Hamburg</report-no><acm-class>I.2 (I.2.4)</acm-class><abstract>  We define a ternary Relation Algebra (RA) of relative position relations on
two-dimensional directed lines (d-lines for short). A d-line has two degrees of
freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The
representation of the RDF of a d-line will be handled by an RA of 2D
orientations, CYC_t, known in the literature. A second algebra, TA_t, which
will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and
TA_t, will constitute, respectively, the translational and the rotational
components of the RA, PA_t, of relative position relations on d-lines: the PA_t
atoms will consist of those pairs &lt;t,r&gt; of a TA_t atom and a CYC_t atom that
are compatible. We present in detail the RA PA_t, with its converse table, its
rotation table and its composition tables. We show that a (polynomial)
constraint propagation algorithm, known in the literature, is complete for a
subset of PA_t relations including almost all of the atomic relations. We will
discuss the application scope of the RA, which includes incidence geometry, GIS
(Geographic Information Systems), shape representation, localisation in
(multi-)robot navigation, and the representation of motion prepositions in NLP
(Natural Language Processing). We then compare the RA to existing ones, such as
an algebra for reasoning about rectangles parallel to the axes of an
(orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval
algebra, and an algebra for reasoning about 2D segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307051</id><created>2003-07-21</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>An Analytical Piecewise Radial Distortion Model for Precision Camera
  Calibration</title><categories>cs.CV</categories><comments>6 pages, 6 PostScript figures</comments><acm-class>I.4.1</acm-class><abstract>  The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
piecewise radial distortion model with easy analytical undistortion formula.
The motivation for seeking a piecewise radial distortion model is that, when a
camera is resulted in a low quality during manufacturing, the nonlinear radial
distortion can be complex. Using low order polynomials to approximate the
radial distortion might not be precise enough. On the other hand, higher order
polynomials suffer from the inverse problem. With the new piecewise radial
distortion function, more flexibility is obtained and the radial undistortion
can be performed analytically. Experimental results are presented to show that
with this new piecewise radial distortion model, better performance is achieved
than that using the single function. Furthermore, a comparable performance with
the conventional polynomial model using 2 coefficients can also be
accomplished.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307052</id><created>2003-07-22</created><authors><author><keyname>Gibbins</keyname><forenames>Hussein</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Gridscape: A Tool for the Creation of Interactive and Dynamic Grid
  Testbed Web Portals</title><categories>cs.DC</categories><comments>12 pages, 6 figures</comments><report-no>July 2003 Research Report, GRIDS Lab @ The University of Melbourne</report-no><acm-class>D.2..2, D.4.9</acm-class><abstract>  The notion of grid computing has gained an increasing popularity recently as
a realistic solution to many of our large-scale data storage and processing
needs. It enables the sharing, selection and aggregation of resources
geographically distributed across collaborative organisations. Now more and
more people are beginning to embrace grid computing and thus are seeing the
need to set up their own grids and grid testbeds. With this comes the need to
have some means to enable them to view and monitor the status of the resources
in these testbeds (eg. Web based Grid portal). Generally developers invest a
substantial amount of time and effort developing custom monitoring software. To
overcome this limitation, this paper proposes Gridscape ? a tool that enables
the rapid creation of interactive and dynamic testbed portals (without any
programming effort). Gridscape primarily aims to provide a solution for those
users who need to be able to create a grid testbed portal but don?t necessarily
have the time or resources to build a system of their own from scratch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307053</id><created>2003-07-23</created><authors><author><keyname>Aliani</keyname><forenames>P.</forenames></author><author><keyname>Antonelli</keyname><forenames>V.</forenames></author><author><keyname>Picariello</keyname><forenames>M.</forenames></author><author><keyname>Torrente-Lujan</keyname><forenames>Emilio</forenames></author></authors><title>Hamevol1.0: a C++ code for differential equations based on Runge-Kutta
  algorithm. An application to matter enhanced neutrino oscillation</title><categories>cs.CE</categories><comments>18 pages, Latex</comments><report-no>IFUM-841-FT; CERN-TH-03-101 ; FT-UM-TH-03-06</report-no><acm-class>J.2</acm-class><abstract>  We present a C++ implementation of a fifth order semi-implicit Runge-Kutta
algorithm for solving Ordinary Differential Equations. This algorithm can be
used for studying many different problems and in particular it can be applied
for computing the evolution of any system whose Hamiltonian is known. We
consider in particular the problem of calculating the neutrino oscillation
probabilities in presence of matter interactions. The time performance and the
accuracy of this implementation is competitive with respect to the other
analytical and numerical techniques used in literature. The algorithm design
and the salient features of the code are presented and discussed and some
explicit examples of code application are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307054</id><created>2003-07-24</created><authors><author><keyname>Putz</keyname><forenames>Viorel</forenames></author><author><keyname>Putz</keyname><forenames>Mihai V.</forenames></author></authors><title>Contributions to the Development and Improvement of a Regulatory and
  Pre-Regulatory Digitally System for the Tools within Flexible Fabrication
  Systems</title><categories>cs.CE cs.SE</categories><comments>5 pages, 3 figures</comments><acm-class>D.2.2; J.2</acm-class><abstract>  The paper reports the obtained results for the projection and realization of
a digitally system aiming to assist the equipment for a regulatory and
pre-regulatory tools and holding tools within the flexible fabrication systems
(FFS). Moreover, based on the present results, the same methodology can be
applied for assisting tools from the point of view of their integrity and to
wear compensation in the FFS framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307055</id><created>2003-07-24</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames><affiliation>Rutgers University</affiliation></author></authors><title>Learning Analogies and Semantic Relations</title><categories>cs.LG cs.CL cs.IR</categories><comments>28 pages, issued 2003</comments><report-no>NRC-46488</report-no><acm-class>H.3.1; I.2.6; I.2.7</acm-class><abstract>  We present an algorithm for learning from unlabeled text, based on the Vector
Space Model (VSM) of information retrieval, that can solve verbal analogy
questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal
analogy has the form A:B::C:D, meaning &quot;A is to B as C is to D&quot;; for example,
mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B,
and the problem is to select the most analogous word pair, C:D, from a set of
five choices. The VSM algorithm correctly answers 47% of a collection of 374
college-level analogy questions (random guessing would yield 20% correct). We
motivate this research by relating it to work in cognitive science and
linguistics, and by applying it to a difficult problem in natural language
processing, determining semantic relations in noun-modifier pairs. The problem
is to classify a noun-modifier pair, such as &quot;laser printer&quot;, according to the
semantic relation between the noun (printer) and the modifier (laser). We use a
supervised nearest-neighbour algorithm that assigns a class to a given
noun-modifier pair by finding the most analogous noun-modifier pair in the
training data. With 30 classes of semantic relations, on a collection of 600
labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5%
(random guessing: 3.3%). With 5 classes of semantic relations, the F value is
43.2% (random: 20%). The performance is state-of-the-art for these challenging
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307056</id><created>2003-07-24</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Grove</keyname><forenames>Adam</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>From Statistical Knowledge Bases to Degrees of Belief</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><journal-ref>Artificial Intelligence 87:1-2, 1996, pp. 75-143</journal-ref><abstract>  An intelligent agent will often be uncertain about various properties of its
environment, and when acting in that environment it will frequently need to
quantify its uncertainty. For example, if the agent wishes to employ the
expected-utility paradigm of decision theory to guide its actions, it will need
to assign degrees of belief (subjective probabilities) to various assertions.
Of course, these degrees of belief should not be arbitrary, but rather should
be based on the information available to the agent. This paper describes one
approach for inducing degrees of belief from very rich knowledge bases, that
can include information about particular individuals, statistical correlations,
physical laws, and default rules. We call our approach the random-worlds
method. The method is based on the principle of indifference: it treats all of
the worlds the agent considers possible as being equally likely. It is able to
integrate qualitative default reasoning with quantitative probabilistic
reasoning by providing a language in which both types of information can be
easily expressed. Our results show that a number of desiderata that arise in
direct inference (reasoning from statistical information to conclusions about
individuals) and default reasoning follow directly {from} the semantics of
random worlds. For example, random worlds captures important patterns of
reasoning such as specificity, inheritance, indifference to irrelevant
information, and default assumptions of independence. Furthermore, the
expressive power of the language used and the intuitive semantics of random
worlds allow the method to deal with problems that are beyond the scope of many
other non-deductive reasoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307057</id><created>2003-07-24</created><updated>2005-03-16</updated><authors><author><keyname>O'Neill</keyname><forenames>Kevin R.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Secrecy in Multiagent Systems</title><categories>cs.CR cs.LO</categories><comments>This is an extended version of a paper that appears in the
  Proceedings of the 15th IEEE Computer Security Foundations Workshop}, 2002,
  pp. 32-46</comments><acm-class>D.4.6, F.4.1</acm-class><abstract>  We introduce a general framework for reasoning about secrecy and privacy
requirements in multiagent systems. Our definitions extend earlier definitions
of secrecy and nondeducibility given by Shannon and Sutherland. Roughly
speaking, one agent maintains secrecy with respect to another if the second
agent cannot rule out any possibilities for the behavior or state of the first
agent. We show that the framework can handle probability and nondeterminism in
a clean way, is useful for reasoning about asynchronous systems as well as
synchronous systems, and suggests generalizations of secrecy that may be useful
for dealing with issues such as resource-bounded reasoning. We also show that a
number of well-known attempts to characterize the absence of information flow
are special cases of our definitions of secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307058</id><created>2003-07-25</created><authors><author><keyname>Metz</keyname><forenames>Edu</forenames></author><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author></authors><title>Efficient Instrumentation for Performance Profiling</title><categories>cs.PF cs.SE</categories><comments>3 pages, 1 figure, published in proceedings of WODA'2003, ICSE
  Workshop on Dynamic Analysis</comments><acm-class>C.4; D.2.8</acm-class><abstract>  Performance profiling consists of tracing a software system during execution
and then analyzing the obtained traces. However, traces themselves affect the
performance of the system distorting its execution. Therefore, there is a need
to minimize the effect of the tracing on the underlying system's performance.
To achieve this, the trace set needs to be optimized according to the
performance profiling problem being solved. Our position is that such
minimization can be achieved only by adding the software trace design and
implementation to the overall software development process. In such a process,
the performance analyst supplies the knowledge of performance measurement
requirements, while the software developer supplies the knowledge of the
software. Both of these are needed for an optimal trace placement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307059</id><created>2003-07-26</created><authors><author><keyname>Guthery</keyname><forenames>Scott B.</forenames></author></authors><title>Group Authentication Using The Naccache-Stern Public-Key Cryptosystem</title><categories>cs.CR</categories><comments>7 pages, no figures</comments><acm-class>D.4.6,K.6.5</acm-class><abstract>  A group authentication protocol authenticates pre-defined groups of
individuals such that:
 - No individual is identified
 - No knowledge of which groups can be successfully authenticated is known to
the verifier
 - No sensitive data is exposed
  The paper presents a group authentication protocol based on splitting the
private keys of the Naccache-Stern public-key cryptosystem in such a way that
the Boolean expression defining the authenticable groups is implicit in the
split.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307060</id><created>2003-07-27</created><updated>2004-05-13</updated><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Neural realisation of the SP theory: cell assemblies revisited</title><categories>cs.AI cs.NE</categories><comments>This is a radical revision with new content which is shorter and, I
  hope, clearer</comments><acm-class>I.2.0</acm-class><abstract>  This paper describes how the elements of the SP theory (Wolff, 2003a) may be
realised with neural structures and processes. To the extent that this is
successful, the insights that have been achieved in the SP theory - the
integration and simplification of a range of phenomena in perception and
cognition - may be incorporated in a neural view of brain function.
  These proposals may be seen as a development of Hebb's (1949) concept of a
'cell assembly'. By contrast with that concept and variants of it, the version
described in this paper proposes that any one neuron can belong in one assembly
and only one assembly. A distinctive feature of the present proposals is that
any neuron or cluster of neurons within a cell assembly may serve as a proxy or
reference for another cell assembly or class of cell assemblies. This device
provides solutions to many of the problems associated with cell assemblies, it
allows information to be stored in a compressed form, and it provides a robust
mechanism by which assemblies may be connected to form hierarchies, grammars
and other kinds of knowledge structure.
  Drawing on insights derived from the SP theory, the paper also describes how
unsupervised learning may be achieved with neural structures and processes.
This theory of learning overcomes weaknesses in the Hebbian concept of learning
and it is, at the same time, compatible with the observations that Hebb's
theory was designed to explain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307061</id><created>2003-07-28</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Boundary knot method for Laplace and biharmonic problems</title><categories>cs.CE cs.MS</categories><comments>Welcome any comments to wenc@simula.no</comments><report-no>Proc. of the 14th Nordic Seminar on Computational Mechanics, pp.
  117-120, Lund, Sweden, Oct. 2001</report-no><acm-class>G1.3; G1.8</acm-class><abstract>  The boundary knot method (BKM) [1] is a meshless boundary-type radial basis
function (RBF) collocation scheme, where the nonsingular general solution is
used instead of fundamental solution to evaluate the homogeneous solution,
while the dual reciprocity method (DRM) is employed to approximation of
particular solution. Despite the fact that there are not nonsingular RBF
general solutions available for Laplace and biharmonic problems, this study
shows that the method can be successfully applied to these problems. The
high-order general and fundamental solutions of Burger and Winkler equations
are also first presented here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307062</id><created>2003-07-28</created><updated>2004-05-05</updated><authors><author><keyname>Baladi</keyname><forenames>Viviane</forenames></author><author><keyname>Vallee</keyname><forenames>Brigitte</forenames></author></authors><title>Euclidean algorithms are Gaussian</title><categories>cs.DS cs.CC</categories><comments>fourth revised version - 2 figures - the strict convexity condition
  used has been clarified</comments><acm-class>F.2.1, I.1.2</acm-class><abstract>  This study provides new results about the probabilistic behaviour of a class
of Euclidean algorithms: the asymptotic distribution of a whole class of
cost-parameters associated to these algorithms is normal. For the cost
corresponding to the number of steps Hensley already has proved a Local Limit
Theorem; we give a new proof, and extend his result to other euclidean
algorithms and to a large class of digit costs, obtaining a faster, optimal,
rate of convergence. The paper is based on the dynamical systems methodology,
and the main tool is the transfer operator. In particular, we use recent
results of Dolgopyat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307063</id><created>2003-07-29</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>An Alternative to RDF-Based Languages for the Representation and
  Processing of Ontologies in the Semantic Web</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><abstract>  This paper describes an approach to the representation and processing of
ontologies in the Semantic Web, based on the ICMAUS theory of computation and
AI. This approach has strengths that complement those of languages based on the
Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main
benefits of the ICMAUS approach are simplicity and comprehensibility in the
representation of ontologies, an ability to cope with errors and uncertainties
in knowledge, and a versatile reasoning system with capabilities in the kinds
of probabilistic reasoning that seem to be required in the Semantic Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307064</id><created>2003-07-29</created><authors><author><keyname>Boman</keyname><forenames>Magnus</forenames></author><author><keyname>Sandin</keyname><forenames>Anna</forenames></author></authors><title>Implementing an Agent Trade Server</title><categories>cs.CE</categories><comments>14 pages, 7 figures, intended for B/W printing</comments><acm-class>{I.2.11}{Artificial Intelligence}{Distributed Artificial
  Intelligence}[Intelligent Agents];{K.4.4}{Computers and Society}{Electronic
  Commerce}[Distributed Commercial Transactions]</acm-class><abstract>  An experimental server for stock trading autonomous agents is presented and
made available, together with an agent shell for swift development. The server,
written in Java, was implemented as proof-of-concept for an agent trade server
for a real financial exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307065</id><created>2003-07-29</created><authors><author><keyname>Tomov</keyname><forenames>Stanimire</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>Bennett</keyname><forenames>Robert</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>McGuigan</keyname><forenames>Michael</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>Peskin</keyname><forenames>Arnold</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>Smith</keyname><forenames>Gordon</forenames><affiliation>Brookhaven National Laboratory</affiliation></author><author><keyname>Spiletic</keyname><forenames>John</forenames><affiliation>Brookhaven National Laboratory</affiliation></author></authors><title>Application of interactive parallel visualization for commodity-based
  clusters using visualization APIs</title><categories>cs.GR</categories><comments>12 pages, 4 figures</comments><acm-class>I.3.6; I.3.4</acm-class><abstract>  We present an efficient and inexpensive to develop application for
interactive high-performance parallel visualization. We extend popular APIs
such as Open Inventor and VTK to support commodity-based cluster visualization.
Our implementation follows a standard master/slave concept: the general idea is
to have a ``Master'' node, which will intercept a sequential graphical user
interface (GUI) and broadcast it to the ``Slave'' nodes. The interactions
between the nodes are implemented using MPI. The parallel remote rendering uses
Chromium. This paper is mainly the report of our implementation experiences. We
present in detail the proposed model and key aspects of its implementation.
Also, we present performance measurements, we benchmark and quantitatively
demonstrate the dependence of the visualization speed on the data size and the
network bandwidth, and we identify the singularities and draw conclusions on
Chromium's sort-first rendering architecture. The most original part of this
work is the combined use of Open Inventor and Chromium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307066</identifier>
 <datestamp>2010-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307066</id><created>2003-07-29</created><authors><author><keyname>Lodygensky</keyname><forenames>Oleg</forenames></author><author><keyname>Fedak</keyname><forenames>Gilles</forenames></author><author><keyname>Neri</keyname><forenames>Vincent</forenames></author><author><keyname>Cordier</keyname><forenames>Alain</forenames></author><author><keyname>Cappello</keyname><forenames>Franck</forenames></author></authors><title>Augernome &amp; XtremWeb: Monte Carlos computation on a global computing
  platform</title><categories>cs.DC</categories><proxy>ccsd ccsd-00000532</proxy><acm-class>D.0</acm-class><journal-ref>ECONF C0303241:THAT001,2003</journal-ref><abstract>  In this paper, we present XtremWeb, a Global Computing platform used to
generate monte carlos showers in Auger, an HEP experiment to study the highest
energy cosmic rays at Mallargue-Mendoza, Argentina.
  XtremWeb main goal, as a Global Computing platform, is to compute distributed
applications using idle time of widely interconnected machines. It is
especially dedicated to -but not limited to- multi-parameters applications such
as monte carlos computations; its security mechanisms ensuring not only hosts
integrity but also results certification and its fault tolerant features,
encouraged us to test it and, finally, to deploy it as to support our CPU needs
to simulate showers.
  We first introduce Auger computing needs and how Global Computing could help.
We then detail XtremWeb architecture and goals. The fourth and last part
presents the profits we have gained to choose this platform. We conclude on
what could be done next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307067</id><created>2003-07-30</created><authors><author><keyname>Vermeulen</keyname><forenames>C. F. M.</forenames></author></authors><title>Sound search in a denotational semantics for first order logic</title><categories>cs.LO</categories><comments>To appear in print shortly in the Progress in Computer Science Series
  of Novapublications</comments><acm-class>D.1.6;D.3.1;F.3.2</acm-class><abstract>  In this paper we adapt the definitions and results from Apt and Vermeulen on
`First order logic as a constraint programming language' (in: Proceedings of
LPAR2001, Baaz and Voronkov (eds.), Springer LNAI 2514) to include important
ideas about search and choice into the system. We give motivating examples.
Then we set up denotational semantics for first order logic as follows: the
semantic universe includes states that consist of two components: a
substitution, which can be seen as the computed answer; and a constraint
satisfaction problem, which can be seen as the residue of the original problem,
yet to be handled by constraint programming. The interaction between these
components is regulated by an operator called: infer. In this paper we regard
infer as an operator on sets of states to enable us to analyze ideas about
search among states and choice between states.
  The precise adaptations of definitions and results are able to deal with the
examples and we show that, given several reasonable conditions, the new
definitions ensure soundness of the system with respect to the standard
interpretation of first order logic. In this way the `reasonable conditions'
can be read as conditions for sound search.
  We indicate briefly how to investigate efficiency of search in future
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307068</id><created>2003-07-30</created><authors><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>Web Access to Cultural Heritage for the Disabled</title><categories>cs.CY cs.HC cs.IR</categories><comments>10 pages, see http://www.museophile.sbu.ac.uk/pub/jpb/eva2003.pdf</comments><acm-class>H.1.2;H.3.5;H.3.7;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0</acm-class><journal-ref>Jonathan P. Bowen, Web Access to Cultural Heritage for the
  Disabled. In James Hemsley, Vito Cappellini and Gerd Stanke (eds.), EVA 2003
  London Conference Proceedings, University College London, UK, 22-26 July
  2003, pages s1:1-11. Keynote address. ISBN 0-9543146-3-8</journal-ref><abstract>  Physical disabled access is something that most cultural institutions such as
museums consider very seriously. Indeed, there are normally legal requirements
to do so. However, online disabled access is still a relatively novel and
developing field. Many cultural organizations have not yet considered the
issues in depth and web developers are not necessarily experts either. The
interface for websites is normally tested with major browsers, but not with
specialist software like text to audio converters for the blind or against the
relevant accessibility and validation standards. We consider the current state
of the art in this area, especially with respect to aspects of particular
importance to the access of cultural heritage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307069</id><created>2003-07-30</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>A logic for reasoning about upper probabilities</title><categories>cs.AI cs.LO</categories><comments>A preliminary version of this paper appeared in Proc. of the 17th
  Conference on Uncertainty in AI, 2001</comments><acm-class>I.2.4; F.2.1</acm-class><journal-ref>Journal of AI Research 17, 2001, pp. 57-81</journal-ref><abstract>  We present a propositional logic %which can be used to reason about the
uncertainty of events, where the uncertainty is modeled by a set of probability
measures assigning an interval of probability to each event. We give a sound
and complete axiomatization for the logic, and show that the satisfiability
problem is NP-complete, no harder than satisfiability for propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307070</id><created>2003-07-30</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Modeling Belief in Dynamic Systems, Part I: Foundations</title><categories>cs.AI cs.LO</categories><acm-class>I.2.4; F.2.1</acm-class><journal-ref>Aritificial Intelligence 95:2, 1997, pp. 257-316</journal-ref><abstract>  Belief change is a fundamental problem in AI: Agents constantly have to
update their beliefs to accommodate new observations. In recent years, there
has been much work on axiomatic characterizations of belief change. We claim
that a better understanding of belief change can be gained from examining
appropriate semantic models. In this paper we propose a general framework in
which to model belief change. We begin by defining belief in terms of knowledge
and plausibility: an agent believes p if he knows that p is more plausible than
its negation. We then consider some properties defining the interaction between
knowledge and plausibility, and show how these properties affect the properties
of belief. In particular, we show that by assuming two of the most natural
properties, belief becomes a KD45 operator. Finally, we add time to the
picture. This gives us a framework in which we can talk about knowledge,
plausibility (and hence belief), and time, which extends the framework of
Halpern and Fagin for modeling knowledge in multi-agent systems. We then
examine the problem of ``minimal change''. This notion can be captured by using
prior plausibilities, an analogue to prior probabilities, which can be updated
by ``conditioning''. We show by example that conditioning on a plausibility
measure can capture many scenarios of interest. In a companion paper, we show
how the two best-studied scenarios of belief change, belief revisionand belief
update, fit into our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307071</id><created>2003-07-30</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Modeling Belief in Dynamic Systems, Part II: Revisions and Update</title><categories>cs.AI cs.LO</categories><acm-class>I.2.4; F.2.1</acm-class><journal-ref>Aritificial Intelligence 95:2, 1997, pp. 257-316</journal-ref><abstract>  The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change, belief revision and belief
update, have been studied in detail. In a companion paper, we introduce a new
framework to model belief change. This framework combines temporal and
epistemic modalities with a notion of plausibility, allowing us to examine the
change of beliefs over time. In this paper, we show how belief revision and
belief update can be captured in our framework. This allows us to compare the
assumptions made by each method, and to better understand the principles
underlying them. In particular, it shows that Katsuno and Mendelzon's notion of
belief update depends on several strong assumptions that may limit its
applicability in artificial intelligence. Finally, our analysis allow us to
identify a notion of minimal change that underlies a broad range of belief
change operations including revision and update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307072</id><created>2003-07-31</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>Camera Calibration: a USU Implementation</title><categories>cs.CV</categories><comments>39 pages, 19 eps figures, source codes are in the codes.m and
  corners.dat</comments><acm-class>I.4.1</acm-class><abstract>  The task of camera calibration is to estimate the intrinsic and extrinsic
parameters of a camera model. Though there are some restricted techniques to
infer the 3-D information about the scene from uncalibrated cameras, effective
camera calibration procedures will open up the possibility of using a wide
range of existing algorithms for 3-D reconstruction and recognition.
 The applications of camera calibration include vision-based metrology, robust
visual platooning and visual docking of mobile robots where the depth
information is important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0307073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0307073</id><created>2003-07-31</created><authors><author><keyname>Wheeldon</keyname><forenames>Richard</forenames></author><author><keyname>Levene</keyname><forenames>Mark</forenames></author><author><keyname>Keenoy</keyname><forenames>Kevin</forenames></author></authors><title>Search and Navigation in Relational Databases</title><categories>cs.DB</categories><comments>12 pages, 7 figures</comments><acm-class>H.3;H.4;H.5</acm-class><abstract>  We present a new application for keyword search within relational databases,
which uses a novel algorithm to solve the join discovery problem by finding
Memex-like trails through the graph of foreign key dependencies. It differs
from previous efforts in the algorithms used, in the presentation mechanism and
in the use of primary-key only database queries at query-time to maintain a
fast response for users. We present examples using the DBLP data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308001</id><created>2003-07-31</created><updated>2003-08-01</updated><authors><author><keyname>Geerts</keyname><forenames>Floris</forenames></author><author><keyname>Smits</keyname><forenames>Lieven</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Two- versus three-dimensional connectivity testing of first-order
  queries to semi-algebraic sets</title><categories>cs.LO cs.CG cs.DB</categories><comments>corrected minor confusion in Proof of Theorem 1</comments><acm-class>F.4.1; F.2.2; H.2.8</acm-class><journal-ref>A revised version has been published online (21 July 2005) in Acta
  Informatica under the title &quot;N- versus (N-1)-dimensional connectivity testing
  of first-order queries to semi-algebraic sets&quot;</journal-ref><doi>10.1007/s00236-005-0171-5</doi><abstract>  This paper addresses the question whether one can determine the connectivity
of a semi-algebraic set in three dimensions by testing the connectivity of a
finite number of two-dimensional ``samples'' of the set, where these samples
are defined by first-order queries. The question is answered negatively for two
classes of first-order queries: cartesian-product-free, and positive one-pass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308002</id><created>2003-08-01</created><updated>2004-03-02</updated><authors><author><keyname>Jakulin</keyname><forenames>Aleks</forenames></author><author><keyname>Bratko</keyname><forenames>Ivan</forenames></author></authors><title>Quantifying and Visualizing Attribute Interactions</title><categories>cs.AI</categories><comments>30 pages, 11 figures. Changes from v2: improved bibliography</comments><acm-class>I.2.6</acm-class><abstract>  Interactions are patterns between several attributes in data that cannot be
inferred from any subset of these attributes. While mutual information is a
well-established approach to evaluating the interactions between two
attributes, we surveyed its generalizations as to quantify interactions between
several attributes. We have chosen McGill's interaction information, which has
been independently rediscovered a number of times under various names in
various disciplines, because of its many intuitively appealing properties. We
apply interaction information to visually present the most important
interactions of the data. Visualization of interactions has provided insight
into the structure of data on a number of domains, identifying redundant
attributes and opportunities for constructing new features, discovering
unexpected regularities in data, and have helped during construction of
predictive models; we illustrate the methods on numerous examples. A machine
learning method that disregards interactions may get caught in two traps:
myopia is caused by learning algorithms assuming independence in spite of
interactions, whereas fragmentation arises from assuming an interaction in
spite of independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308003</id><created>2003-08-01</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>A Family of Simplified Geometric Distortion Models for Camera Calibration</title><categories>cs.CV</categories><comments>14 pages, 11 eps figures</comments><acm-class>I.4.1</acm-class><abstract>  The commonly used radial distortion model for camera calibration is in fact
an assumption or a restriction. In practice, camera distortion could happen in
a general geometrical manner that is not limited to the radial sense. This
paper proposes a simplified geometrical distortion modeling method by using two
different radial distortion functions in the two image axes. A family of
simplified geometric distortion models is proposed, which are either simple
polynomials or the rational functions of polynomials. Analytical geometric
undistortion is possible using two of the distortion functions discussed in
this paper and their performance can be improved by applying a piecewise
fitting idea. Our experimental results show that the geometrical distortion
models always perform better than their radial distortion counterparts.
Furthermore, the proposed geometric modeling method is more appropriate for
cameras whose distortion is not perfectly radially symmetric around the center
of distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308004</id><created>2003-08-02</created><authors><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoqin</forenames></author><author><keyname>Nguyen</keyname><forenames>Viet Ha</forenames></author></authors><title>DPG: A Cache-Efficient Accelerator for Sorting and for Join Operators</title><categories>cs.DB cs.DS</categories><comments>12 pages, 11 figures</comments><acm-class>E.1; E.2; F.2.2</acm-class><abstract>  We present a new algorithm for fast record retrieval,
distribute-probe-gather, or DPG. DPG has important applications both in sorting
and in joins. Current main memory sorting algorithms split their work into
three phases: extraction of key-pointer pairs; sorting of the key-pointer
pairs; and copying of the original records into the destination array according
the sorted key-pointer pairs. The copying in the last phase dominates today's
sorting time. Hence, the use of DPG in the third phase provides an accelerator
for existing sorting algorithms.
  DPG also provides two new join methods for foreign key joins: DPG-move join
and DPG-sort join. The resulting join methods with DPG are faster because DPG
join is cache-efficient and at the same time DPG join avoids the need for
sorting or for hashing. The ideas presented for foreign key join can also be
extended to faster record pair retrieval for spatial and temporal databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308005</id><created>2003-08-03</created><updated>2004-10-06</updated><authors><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>Disabled Access for Museum Websites</title><categories>cs.CY cs.HC cs.IR</categories><comments>2 pages, see
  http://www2003.org/cdrom/papers/poster/p335/p335-bowen.html</comments><acm-class>H.1.2;H.3.5;H.3.7;H.5.1;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0</acm-class><journal-ref>Jonathan P. Bowen. Disabled Access for Museum Websites. WWW2003:
  The Twelfth International World Wide Web Conference, Budapest, Hungary, 20-24
  May 2003. Conference poster</journal-ref><abstract>  Physical disabled access is something that most museums consider very
seriously. Indeed, there are normally legal requirements to do so. However,
online disabled access is still a relatively novel field. Most museums have not
yet considered the issues in depth. The Human-Computer Interface for their
websites is normally tested with major browsers, but not with specialist
browsers or against the relevant accessibility and validation standards. We
consider the current state of the art in this area and mention an accessibility
survey of some museum websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308006</id><created>2003-08-04</created><updated>2005-07-27</updated><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Koehler</keyname><forenames>Ekkehard</forenames></author><author><keyname>Teich</keyname><forenames>Juergen</forenames></author></authors><title>Higher-Dimensional Packing with Order Constraints</title><categories>cs.DS cs.DM</categories><comments>23 pages, 14 figures, 5 tables, Latex; revision clarifies various
  minor points, fixes typos, etc. To appear in SIAM Journal on Discrete
  Mathematics</comments><acm-class>F.2.2</acm-class><abstract>  We present a first exact study on higher-dimensional packing problems with
order constraints. Problems of this type occur naturally in applications such
as logistics or computer architecture and can be interpreted as
higher-dimensional generalizations of scheduling problems. Using
graph-theoretic structures to describe feasible solutions, we develop a novel
exact branch-and-bound algorithm. This extends previous work by Fekete and
Schepers; a key tool is a new order-theoretic characterization of feasible
extensions of a partial order to a given complementarity graph that is
tailor-made for use in a branch-and-bound environment. The usefulness of our
approach is validated by computational results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308007</id><created>2003-08-04</created><authors><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author><author><keyname>Silva</keyname><forenames>Fernando</forenames></author><author><keyname>Costa</keyname><forenames>Vitor Santos</forenames></author></authors><title>On Applying Or-Parallelism and Tabling to Logic Programs</title><categories>cs.PL</categories><comments>45 pages, 12 figures, to appear in the journal of Theory and Practice
  of Logic Programming (TPLP)</comments><acm-class>D.1.6;D.3.2</acm-class><abstract>  The past years have seen widening efforts at increasing Prolog's
declarativeness and expressiveness. Tabling has proved to be a viable technique
to efficiently overcome SLD's susceptibility to infinite loops and redundant
subcomputations. Our research demonstrates that implicit or-parallelism is a
natural fit for logic programs with tabling. To substantiate this belief, we
have designed and implemented an or-parallel tabling engine -- OPTYap -- and we
used a shared-memory parallel machine to evaluate its performance. To the best
of our knowledge, OPTYap is the first implementation of a parallel tabling
engine for logic programming systems. OPTYap builds on Yap's efficient
sequential Prolog engine. Its execution model is based on the SLG-WAM for
tabling, and on the environment copying for or-parallelism.
  Preliminary results indicate that the mechanisms proposed to parallelize
search in the context of SLD resolution can indeed be effectively and naturally
generalized to parallelize tabled computations, and that the resulting systems
can achieve good performance on shared-memory parallel machines. More
importantly, it emphasizes our belief that through applying or-parallelism and
tabling to logic programs the range of applications for Logic Programming can
be increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308008</id><created>2003-08-04</created><authors><author><keyname>Hughes</keyname><forenames>Baden</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>A Grid Based Architecture for High-Performance NLP</title><categories>cs.DC cs.CL</categories><acm-class>J.5; D.1; C.2</acm-class><abstract>  We describe the design and early implementation of an extensible,
component-based software architecture for natural language engineering
applications which interfaces with high performance distributed computing
services. The architecture leverages existing linguistic resource description
and discovery mechanisms based on metadata descriptions, combining these in a
compatible fashion with other software definition abstractions. Within this
architecture, application design is highly flexible, allowing disparate
components to be combined to suit the overall application functionality, and
formally described independently of processing concerns. An application
specification language provides abstraction from the programming environment
and allows ease of interface with high performance computational grids via a
broker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308009</identifier>
 <datestamp>2008-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308009</id><created>2003-08-05</created><updated>2008-05-17</updated><authors><author><keyname>Muldowney</keyname><forenames>Pat</forenames></author><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>The Generalized Riemann or Henstock Integral Underpinning Multivariate
  Data Analysis: Application to Faint Structure Finding in Price Processes</title><categories>cs.CE cs.CV</categories><comments>27 pages, 4 figures. Various changes made relative to previous
  versions, in particular in introductory section</comments><acm-class>G.3; I.5.3</acm-class><abstract>  Practical data analysis involves many implicit or explicit assumptions about
the good behavior of the data, and excludes consideration of various
potentially pathological or limit cases. In this work, we present a new general
theory of data, and of data processing, to bypass some of these assumptions.
The new framework presented is focused on integration, and has direct
applicability to expectation, distance, correlation, and aggregation. In a case
study, we seek to reveal faint structure in financial data. Our new foundation
for data encoding and handling offers increased justification for our
conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308010</id><created>2003-08-05</created><authors><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author></authors><title>On the probabilistic approach to the random satisfiability problem</title><categories>cs.CC cond-mat.dis-nn</categories><comments>11 pages, proceedings of SAT 2003</comments><acm-class>G.3, G.2.1 G.3, G.2.1 G.3, G.2.1</acm-class><abstract>  In this note I will review some of the recent results that have been obtained
in the probabilistic approach to the random satisfiability problem. At the
present moment the results are only heuristic. In the case of the random
3-satisfiability problem a phase transition from the satisfiable to the
unsatisfiable phase is found at $\alpha=4.267$. There are other values of
$\alpha$ that separates different regimes and they will be described in
details. In this context the properties of the survey decimation algorithm will
also be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308011</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308011</id><created>2003-08-05</created><updated>2003-09-10</updated><authors><author><keyname>Batagelj</keyname><forenames>V.</forenames></author><author><keyname>Zaversnik</keyname><forenames>M.</forenames></author></authors><title>Short Cycles Connectivity</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2</acm-class><journal-ref>DISCRETE MATH 307 (3-5): 310-318 FEB 6 2007</journal-ref><abstract>  Short cycles connectivity is a generalization of ordinary connectivity.
Instead by a path (sequence of edges), two vertices have to be connected by a
sequence of short cycles, in which two adjacent cycles have at least one common
vertex. If all adjacent cycles in the sequence share at least one edge, we talk
about edge short cycles connectivity.
  It is shown that the short cycles connectivity is an equivalence relation on
the set of vertices, while the edge short cycles connectivity components
determine an equivalence relation on the set of edges. Efficient algorithms for
determining equivalence classes are presented.
  Short cycles connectivity can be extended to directed graphs (cyclic and
transitive connectivity). For further generalization we can also consider
connectivity by small cliques or other families of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308012</id><created>2003-08-05</created><authors><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author><author><keyname>Segerlind</keyname><forenames>Nathan</forenames></author></authors><title>Constant-Depth Frege Systems with Counting Axioms Polynomially Simulate
  Nullstellensatz Refutations</title><categories>cs.CC cs.LO</categories><comments>17 pages</comments><acm-class>F.4.1</acm-class><abstract>  We show that constant-depth Frege systems with counting axioms modulo $m$
polynomially simulate Nullstellensatz refutations modulo $m$. Central to this
is a new definition of reducibility from formulas to systems of polynomials
with the property that, for most previously studied translations of formulas to
systems of polynomials, a formula reduces to its translation. When combined
with a previous result of the authors, this establishes the first size
separation between Nullstellensatz and polynomial calculus refutations. We also
obtain new, small refutations for certain CNFs by constant-depth Frege systems
with counting axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308013</id><created>2003-08-06</created><authors><author><keyname>Franconi</keyname><forenames>Enrico</forenames></author><author><keyname>Kuper</keyname><forenames>Gabriel</forenames></author><author><keyname>Lopatenko</keyname><forenames>Andrei</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author></authors><title>A Robust and Computational Characterisation of Peer-to-Peer Database
  Systems</title><categories>cs.DC cs.DB</categories><comments>13 pages</comments><acm-class>H.2.4;H.2.5;C.2.4</acm-class><journal-ref>&quot;International Workshop On Databases, Information Systems and
  Peer-to-Peer Computing&quot;, 2003</journal-ref><abstract>  In this paper we give a robust logical and computational characterisation of
peer-to-peer database systems. We first define a pre- cise model-theoretic
semantics of a peer-to-peer system, which allows for local inconsistency
handling. We then characterise the general computa- tional properties for the
problem of answering queries to such a peer-to- peer system. Finally, we devise
tight complexity bounds and distributed procedures for the problem of answering
queries in few relevant special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308014</id><created>2003-08-06</created><updated>2004-03-03</updated><authors><author><keyname>Leinders</keyname><forenames>Dirk</forenames><affiliation>Limburgs Universitair Centrum, Belgium</affiliation></author><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames><affiliation>Warsaw University</affiliation></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames><affiliation>Limburgs Universitair Centrum, Belgium</affiliation></author></authors><title>On the expressive power of semijoin queries</title><categories>cs.DB cs.LO</categories><comments>9 pages, to appear in Information Processing Letters; added results
  that more clearly delineate the expressive power of SA, added a section that
  discusses the impact of order on the expressive power of SA, deemphasized the
  discussion on the relationship with GF</comments><acm-class>H.2.3; F.4.1</acm-class><abstract>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We provide an
Ehrenfeucht-Fraiss\'{e} game, characterizing the discerning power of the
semijoin algebra. This game gives a method for showing that queries are not
expressible in the semijoin algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308015</id><created>2003-08-07</created><authors><author><keyname>Yamane</keyname><forenames>Shinji</forenames></author><author><keyname>Wang</keyname><forenames>Jiahong</forenames></author><author><keyname>Suzuki</keyname><forenames>Hironobu</forenames></author><author><keyname>Segawa</keyname><forenames>Norihisa</forenames></author><author><keyname>Murayama</keyname><forenames>Yuko</forenames></author></authors><title>Rethinking OpenPGP PKI and OpenPGP Public Keyserver</title><categories>cs.CY cs.CR</categories><comments>8 pages, 2 figures</comments><acm-class>K.6, E.3</acm-class><abstract>  OpenPGP, an IETF Proposed Standard based on PGP application, has its own
Public Key Infrastructure (PKI) architecture which is different from the one
based on X.509, another standard from ITU. This paper describes the OpenPGP
PKI; the historical perspective as well as its current use. The current OpenPGP
PKI issues include the capability of a PGP keyserver and its performance. PGP
keyservers have been developed and operated by volunteers since the 1990s. The
keyservers distribute, merge, and expire the OpenPGP public keys. Major
keyserver managers from several countries have built the globally distributed
network of PGP keyservers. However, the current PGP Public Keyserver (pksd) has
some limitations. It does not support fully the OpenPGP format so that it is
neither expandable nor flexible, without any cluster technology. Finally we
introduce the project on the next generation OpenPGP public keyserver called
the OpenPKSD, lead by Hironobu Suzuki, one of the authors, and funded by
Japanese Information-technology Promotion Agency(IPA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308016</id><created>2003-08-07</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author></authors><title>Collaborative Creation of Digital Content in Indian Languages</title><categories>cs.CL</categories><acm-class>I,2,7</acm-class><abstract>  The world is passing through a major revolution called the information
revolution, in which information and knowledge is becoming available to people
in unprecedented amounts wherever and whenever they need it. Those societies
which fail to take advantage of the new technology will be left behind, just
like in the industrial revolution.
  The information revolution is based on two major technologies: computers and
communication. These technologies have to be delivered in a COST EFFECTIVE
manner, and in LANGUAGES accessible to people.
  One way to deliver them in cost effective manner is to make suitable
technology choices, and to allow people to access through shared resources.
This could be done throuch street corner shops (for computer usage, e-mail
etc.), schools, community centres and local library centres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308017</id><created>2003-08-07</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Chaitanya</keyname><forenames>Vineet</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author></authors><title>Information Revolution</title><categories>cs.CL</categories><comments>Published as a keynote lecture in IRIL-99: Information Revolution and
  Indian Languages, 12-14 Nov 1999</comments><report-no>LTRC-TR007</report-no><acm-class>I,2,7</acm-class><journal-ref>Published as a keynote lecture in IRIL-99: Information Revolution
  and Indian Languages, 12-14 Nov 1999</journal-ref><abstract>  The world is passing through a major revolution called the information
revolution, in which information and knowledge is becoming available to people
in unprecedented amounts wherever and whenever they need it. Those societies
which fail to take advantage of the new technology will be left behind, just
like in the industrial revolution.
  The information revolution is based on two major technologies: computers and
communication. These technologies have to be delivered in a COST EFFECTIVE
manner, and in LANGUAGES accessible to people.
  One way to deliver them in cost effective manner is to make suitable
technology choices (discussed later), and to allow people to access through
shared resources. This could be done throuch street corner shops (for computer
usage, e-mail etc.), schools, community centers and local library centres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308018</id><created>2003-08-07</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Chaitanya</keyname><forenames>Vineet</forenames></author><author><keyname>Kulkarni</keyname><forenames>Amba P.</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author><author><keyname>Rao</keyname><forenames>G Umamaheshwara</forenames></author></authors><title>Anusaaraka: Overcoming the Language Barrier in India</title><categories>cs.CL</categories><comments>Published in &quot;Anuvad: Approaches to Translation&quot;, Rukmini Bhaya Nair,
  (editor), Sage, New Delhi, 2001</comments><report-no>LTRC-TR009</report-no><acm-class>I,2,7</acm-class><journal-ref>Published in &quot;Anuvad: Approaches to Translation&quot;, Rukmini Bhaya
  Nair, (editor), Sage, New Delhi, 2001</journal-ref><abstract>  The anusaaraka system makes text in one Indian language accessible in another
Indian language. In the anusaaraka approach, the load is so divided between man
and computer that the language load is taken by the machine, and the
interpretation of the text is left to the man. The machine presents an image of
the source text in a language close to the target language.In the image, some
constructions of the source language (which do not have equivalents) spill over
to the output. Some special notation is also devised. The user after some
training learns to read and understand the output. Because the Indian languages
are close, the learning time of the output language is short, and is expected
to be around 2 weeks.
  The output can also be post-edited by a trained user to make it grammatically
correct in the target language. Style can also be changed, if necessary. Thus,
in this scenario, it can function as a human assisted translation system.
  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali
and Punjabi to Hindi. They can be built for all Indian languages in the near
future. Everybody must pitch in to build such systems connecting all Indian
languages, using the free software model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308019</id><created>2003-08-07</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Chaitanya</keyname><forenames>Vineet</forenames></author><author><keyname>Kulkarni</keyname><forenames>Amba P.</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author></authors><title>Language Access: An Information Based Approach</title><categories>cs.CL</categories><comments>Published in the proceedings of Knowledge Based Computer Systems
  conference, 2000, Tata McGraw-Hill, New Delhi, Dec. 2000</comments><report-no>LTRC-TR010</report-no><acm-class>I,2,7</acm-class><journal-ref>Published in the proceedings of Knowledge Based Computer Systems
  Conference, 2000, Tata McGraw Hill, New Delhi 2000</journal-ref><abstract>  The anusaaraka system (a kind of machine translation system) makes text in
one Indian language accessible through another Indian language. The machine
presents an image of the source text in a language close to the target
language. In the image, some constructions of the source language (which do not
have equivalents in the target language) spill over to the output. Some special
notation is also devised.
  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,
Marathi, Bengali and Punjabi to Hindi. They are available for use through Email
servers.
  Anusaarkas follows the principle of substitutibility and reversibility of
strings produced. This implies preservation of information while going from a
source language to a target language.
  For narrow subject areas, specialized modules can be built by putting subject
domain knowledge into the system, which produce good quality grammatical
output. However, it should be remembered, that such modules will work only in
narrow areas, and will sometimes go wrong. In such a situation, anusaaraka
output will still remain useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308020</id><created>2003-08-07</created><authors><author><keyname>Bharati</keyname><forenames>Akshar</forenames></author><author><keyname>Sharma</keyname><forenames>Dipti M</forenames></author><author><keyname>Chaitanya</keyname><forenames>Vineet</forenames></author><author><keyname>Kulkarni</keyname><forenames>Amba P</forenames></author><author><keyname>Sangal</keyname><forenames>Rajeev</forenames></author><author><keyname>Rao</keyname><forenames>Durgesh D</forenames></author></authors><title>LERIL : Collaborative Effort for Creating Lexical Resources</title><categories>cs.CL</categories><comments>[ To appear in Proceedings of Workshop on Language Resources in Asia,
  along with NLPRS-2001, Tokyo, 27-30 November 2001] Appeared in the
  Proceedings of Workshop on Language Resources in Asia, along with NLPRS-2001,
  Tokyo, 27-30 November 2001. Appeared in the proceedings of Workshop on
  Language Resources in Asia, along with NLPRS-2001, Tokyo, 27-30 November 2001</comments><report-no>LTRC-TR015</report-no><acm-class>I,2,7</acm-class><abstract>  The paper reports on efforts taken to create lexical resources pertaining to
Indian languages, using the collaborative model. The lexical resources being
developed are: (1) Transfer lexicon and grammar from English to several Indian
languages. (2) Dependencey tree bank of annotated corpora for several Indian
languages. The dependency trees are based on the Paninian model. (3) Bilingual
dictionary of 'core meanings'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308021</id><created>2003-08-11</created><authors><author><keyname>Vavasis</keyname><forenames>Stephen</forenames></author></authors><title>A Bernstein-Bezier Sufficient Condition for Invertibility of Polynomial
  Mapping Functions</title><categories>cs.NA cs.CG</categories><acm-class>G.1.8</acm-class><abstract>  We propose a sufficient condition for invertibility of a polynomial mapping
function defined on a cube or simplex. This condition is applicable to finite
element analysis using curved meshes. The sufficient condition is based on an
analysis of the Bernstein-B\'ezier form of the columns of the derivative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308022</id><created>2003-08-14</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Simons</keyname><forenames>Gary</forenames></author></authors><title>Extending Dublin Core Metadata to Support the Description and Discovery
  of Language Resources</title><categories>cs.CL cs.DL</categories><comments>12 pages, 1 figure</comments><acm-class>H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5</acm-class><journal-ref>Computing and the Humanities, 37 (4), 2003</journal-ref><abstract>  As language data and associated technologies proliferate and as the language
resources community expands, it is becoming increasingly difficult to locate
and reuse existing resources. Are there any lexical resources for such-and-such
a language? What tool works with transcripts in this particular format? What is
a good format to use for linguistic data of this type? Questions like these
dominate many mailing lists, since web search engines are an unreliable way to
find language resources. This paper reports on a new digital infrastructure for
discovering language resources being developed by the Open Language Archives
Community (OLAC). At the core of OLAC is its metadata format, which is designed
to facilitate description and discovery of all kinds of language resources,
including data, tools, or advice. The paper describes OLAC metadata, its
relationship to Dublin Core metadata, and its dissemination using the metadata
harvesting protocol of the Open Archives Initiative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308023</identifier>
 <datestamp>2010-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308023</id><created>2003-08-15</created><authors><author><keyname>Chernov</keyname><forenames>N.</forenames></author><author><keyname>Lesort</keyname><forenames>C.</forenames></author><author><keyname>Simanyi</keyname><forenames>N.</forenames></author></authors><title>On the complexity of curve fitting algorithms</title><categories>cs.CC cs.CV</categories><comments>8 pages, no figures</comments><acm-class>I.4.8; I.5.1; G.3</acm-class><journal-ref>Journal of Complexity, Vol. 20, Issue 4, August 2004, pp. 484-492</journal-ref><doi>10.1016/j.jco.2004.01.004</doi><abstract>  We study a popular algorithm for fitting polynomial curves to scattered data
based on the least squares with gradient weights. We show that sometimes this
algorithm admits a substantial reduction of complexity, and, furthermore, find
precise conditions under which this is possible. It turns out that this is,
indeed, possible when one fits circles but not ellipses or hyperbolas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308024</id><created>2003-08-15</created><authors><author><keyname>Byrom</keyname><forenames>Rob</forenames></author><author><keyname>Coghlan</keyname><forenames>Brian</forenames></author><author><keyname>Cooke</keyname><forenames>Andrew W</forenames></author><author><keyname>Cordenonsi</keyname><forenames>Roney</forenames></author><author><keyname>Cornwall</keyname><forenames>Linda</forenames></author><author><keyname>Djaoui</keyname><forenames>Abdeslem</forenames></author><author><keyname>Field</keyname><forenames>Laurence</forenames></author><author><keyname>Fisher</keyname><forenames>Steve</forenames></author><author><keyname>Hicks</keyname><forenames>Steve</forenames></author><author><keyname>Kenny</keyname><forenames>Stuart</forenames></author><author><keyname>Leake</keyname><forenames>Jason</forenames></author><author><keyname>Magowan</keyname><forenames>James</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>O'Callaghan</keyname><forenames>David</forenames></author><author><keyname>Podhorszki</keyname><forenames>Norbert</forenames></author><author><keyname>Ryan</keyname><forenames>John</forenames></author><author><keyname>Soni</keyname><forenames>Manish</forenames></author><author><keyname>Taylor</keyname><forenames>Paul</forenames></author><author><keyname>Wilson</keyname><forenames>Antony J</forenames></author></authors><title>Relational Grid Monitoring Architecture (R-GMA)</title><categories>cs.DC</categories><comments>Talk given at UK e-Science All-Hands meeting, Nottingham, UK,
  September 2-4, 2003. 7 pages of LaTeX and 5 PNG figures</comments><acm-class>H.2.4;H.m</acm-class><abstract>  We describe R-GMA (Relational Grid Monitoring Architecture) which has been
developed within the European DataGrid Project as a Grid Information and
Monitoring System. Is is based on the GMA from GGF, which is a simple
Consumer-Producer model. The special strength of this implementation comes from
the power of the relational model. We offer a global view of the information as
if each Virtual Organisation had one large relational database. We provide a
number of different Producer types with different characteristics; for example
some support streaming of information. We also provide combined
Consumer/Producers, which are able to combine information and republish it. At
the heart of the system is the mediator, which for any query is able to find
and connect to the best Producers for the job. We have developed components to
allow a measure of inter-working between MDS and R-GMA. We have used it both
for information about the grid (primarily to find out about what services are
available at any one time) and for application monitoring. R-GMA has been
deployed in various testbeds; we describe some preliminary results and
experiences of this deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308025</id><created>2003-08-16</created><authors><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Controlled hierarchical filtering: Model of neocortical sensory
  processing</title><categories>cs.NE cs.AI cs.LG q-bio.NC</categories><comments>Technical Report, 38 pages, 10 figures</comments><acm-class>C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1</acm-class><abstract>  A model of sensory information processing is presented. The model assumes
that learning of internal (hidden) generative models, which can predict the
future and evaluate the precision of that prediction, is of central importance
for information extraction. Furthermore, the model makes a bridge to
goal-oriented systems and builds upon the structural similarity between the
architecture of a robust controller and that of the hippocampal entorhinal
loop. This generative control architecture is mapped to the neocortex and to
the hippocampal entorhinal loop. Implicit memory phenomena; priming and
prototype learning are emerging features of the model. Mathematical theorems
ensure stability and attractive learning properties of the architecture.
Connections to reinforcement learning are also established: both the control
network, and the network with a hidden model converge to (near) optimal policy
under suitable conditions. Falsifying predictions, including the role of the
feedback connections between neocortical areas are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308026</id><created>2003-08-18</created><authors><author><keyname>Bennett</keyname><forenames>Charles H.</forenames></author></authors><title>Improvements to time bracketed authentication</title><categories>cs.CR cs.CY</categories><acm-class>K.6.5; H.5.1; H.5.2</acm-class><abstract>  We describe a collection of techniques whereby audiovisual or other
recordings of significant events can be made in a way that hinders
falsification, pre-dating, or post-dating by interested parties, even by the
makers and operators of the recording equipment. A central feature of these
techniques is the interplay between private information, which by its nature is
untrustworthy and susceptible to suppression or manipulation by interested
parties, and public information, which is too widely known to be manipulated by
anyone. While authenticated recordings may be infeasible to falsify, they can
be abused in other ways, such as being used for blackmail or harassment; but
susceptibility to these abuses can be reduced by encryption and secret sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308027</id><created>2003-08-19</created><updated>2004-10-16</updated><authors><author><keyname>Wagner</keyname><forenames>Liam</forenames></author></authors><title>A Comparison of Secret Sharing Schemes Based on Latin Squares and RSA</title><categories>cs.CR cs.DM math.CO</categories><acm-class>C.2.0; D.4.6; H.2.0;H.2.7</acm-class><abstract>  In recent years there has been a great deal of work done on secret sharing
scehemes. Secret Sharing Schemes allow for the division of keys so that an
authorised set of users may access information. In this paper we wish to
present a critical comparison of two of these schemes based on Latin Squares,
[Cooper et., al.] and RSA [Shoup]. These two protocols will be examined in
terms of their positive and negative aspects of their secuirty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308028</id><created>2003-08-19</created><updated>2006-02-05</updated><authors><author><keyname>Wagner</keyname><forenames>Liam</forenames></author><author><keyname>McDonald</keyname><forenames>Stuart</forenames></author></authors><title>Finding Traitors in Secure Networks Using Byzantine Agreements</title><categories>cs.CR cs.DC cs.GT cs.MA</categories><comments>Accepted by International Journal of Network Security</comments><acm-class>B.1.3; C.2.0; C.4.0; D.4.5; H.2.0; H.2.7</acm-class><abstract>  Secure networks rely upon players to maintain security and reliability.
However not every player can be assumed to have total loyalty and one must use
methods to uncover traitors in such networks. We use the original concept of
the Byzantine Generals Problem by Lamport, and the more formal Byzantine
Agreement describe by Linial, to nd traitors in secure networks. By applying
general fault-tolerance methods to develop a more formal design of secure
networks we are able to uncover traitors amongst a group of players. We also
propose methods to integrate this system with insecure channels. This new
resiliency can be applied to broadcast and peer-to-peer secure communication
systems where agents may be traitors or become unreliable due to faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308029</id><created>2003-08-19</created><authors><author><keyname>Grandi</keyname><forenames>Fabio</forenames></author></authors><title>On Decidability of Expressive Description Logics with Composition of
  Roles in Number Restrictions</title><categories>cs.LO</categories><comments>A preliminary version of this paper appeared in Proc. of the 9th
  Intl' Conf. on Logic for Programming, Artificial Intelligence and Reasoning,
  LPAR 2002</comments><report-no>CSITE-01-02 (rev. 8/03)</report-no><acm-class>F.2.2; F.4.1; I.2.4</acm-class><abstract>  Description Logics are knowledge representation formalisms which have been
used in a wide range of application domains. Owing to their appealing
expressiveness, we consider in this paper extensions of the well-known concept
language ALC allowing for number restrictions on complex role expressions.
These have been first introduced by Baader and Sattler as ALCN(M) languages,
with the adoption of role constructors M subset-of {o,-,And,Or}. In particular,
they showed in 1999 that, although ALCN(o) is decidable, the addition of other
operators may easily lead to undecidability: in fact, ALCN(o,And) and
ALCN(o,-,Or) were proved undecidable.
  In this work, we further investigate the computational properties of the ALCN
family, aiming at narrowing the decidability gap left open by Baader and
Sattler's results. In particular, we will show that ALCN(o) extended with
inverse roles both in number and in value restrictions becomes undecidable,
whereas it can be safely extended with qualified number restrictions without
losing decidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308030</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308030</id><created>2003-08-19</created><authors><author><keyname>Vidal</keyname><forenames>Jose M.</forenames></author></authors><title>Learning in Multiagent Systems: An Introduction from a Game-Theoretic
  Perspective</title><categories>cs.MA cs.AI</categories><acm-class>I.2.11</acm-class><journal-ref>In Eduardo Alonso, editor, Adaptive Agents: LNAI 2636. Springer
  Verlag, 2003</journal-ref><abstract>  We introduce the topic of learning in multiagent systems. We first provide a
quick introduction to the field of game theory, focusing on the equilibrium
concepts of iterated dominance, and Nash equilibrium. We show some of the most
relevant findings in the theory of learning in games, including theorems on
fictitious play, replicator dynamics, and evolutionary stable strategies. The
CLRI theory and n-level learning agents are introduced as attempts to apply
some of these findings to the problem of engineering multiagent systems with
learning agents. Finally, we summarize some of the remaining challenges in the
field of learning in multiagent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308031</id><created>2003-08-20</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Artificial Neural Networks for Beginners</title><categories>cs.NE cs.AI</categories><comments>tutorial, 8 pages</comments><acm-class>C.1.3; I.5.1</acm-class><abstract>  The scope of this teaching package is to make a brief induction to Artificial
Neural Networks (ANNs) for people who have no previous knowledge of them. We
first make a brief introduction to models of networks, for then describing in
general terms ANNs. As an application, we explain the backpropagation
algorithm, since it is widely used and many other algorithms are derived from
it. The user should know algebra and the handling of functions and vectors.
Differential calculus is recommendable, but not necessary. The contents of this
package should be understood by people with high school education. It would be
useful for people who are just curious about what are ANNs, or for people who
want to become familiar with them, so when they study them more fully, they
will already have clear notions of ANNs. Also, people who only want to apply
the backpropagation algorithm without a detailed and formal explanation of it
will find this material useful. This work should not be seen as &quot;Nets for
dummies&quot;, but of course it is not a treatise. Much of the formality is skipped
for the sake of simplicity. Detailed explanations and demonstrations can be
found in the referred readings. The included exercises complement the
understanding of the theory. The on-line resources are highly recommended for
extending this brief induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308032</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308032</id><created>2003-08-20</created><authors><author><keyname>Yeh</keyname><forenames>Alexander S.</forenames></author><author><keyname>Hirschman</keyname><forenames>Lynette</forenames></author><author><keyname>Morgan</keyname><forenames>Alexander A.</forenames></author></authors><title>Evaluation of text data mining for database curation: lessons learned
  from the KDD Challenge Cup</title><categories>cs.CL q-bio.OT</categories><comments>9 pages. This is close to how it appears on the publisher's website
  (http://bioinformatics.oupjournals.org/cgi/reprint/19/suppl_1/i331) The
  article wording is the same. Uses bioinformatics-altered.cls,
  bioinformaticsbib.sty, bioinformaticstitle.sty</comments><acm-class>I.2.7; J.3</acm-class><journal-ref>Bioinformatics Vol. 19 Suppl. 1 2003, pages i331-i339</journal-ref><abstract>  MOTIVATION: The biological literature is a major repository of knowledge.
Many biological databases draw much of their content from a careful curation of
this literature. However, as the volume of literature increases, the burden of
curation increases. Text mining may provide useful tools to assist in the
curation process. To date, the lack of standards has made it impossible to
determine whether text mining techniques are sufficiently mature to be useful.
  RESULTS: We report on a Challenge Evaluation task that we created for the
Knowledge Discovery and Data Mining (KDD) Challenge Cup. We provided a training
corpus of 862 articles consisting of journal articles curated in FlyBase, along
with the associated lists of genes and gene products, as well as the relevant
data fields from FlyBase. For the test, we provided a corpus of 213 new
(`blind') articles; the 18 participating groups provided systems that flagged
articles for curation, based on whether the article contained experimental
evidence for gene expression products. We report on the the evaluation results
and describe the techniques used by the top performing groups.
  CONTACT: asy@mitre.org
  KEYWORDS: text mining, evaluation, curation, genomics, data management
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308033</id><created>2003-08-20</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Coherent Keyphrase Extraction via Web Mining</title><categories>cs.LG cs.CL cs.IR</categories><comments>6 pages, related work available at http://purl.org/peter.turney/</comments><report-no>NRC-46496</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><journal-ref>Proceedings of the Eighteenth International Joint Conference on
  Artificial Intelligence (IJCAI-03), (2003), Acapulco, Mexico, 434-439</journal-ref><abstract>  Keyphrases are useful for a variety of purposes, including summarizing,
indexing, labeling, categorizing, clustering, highlighting, browsing, and
searching. The task of automatic keyphrase extraction is to select keyphrases
from within the text of a given document. Automatic keyphrase extraction makes
it feasible to generate keyphrases for the huge number of documents that do not
have manually assigned keyphrases. A limitation of previous keyphrase
extraction algorithms is that the selected keyphrases are occasionally
incoherent. That is, the majority of the output keyphrases may fit together
well, but there may be a minority that appear to be outliers, with no clear
semantic relation to the majority or to each other. This paper presents
enhancements to the Kea keyphrase extraction algorithm that are designed to
increase the coherence of the extracted keyphrases. The approach is to use the
degree of statistical association among candidate keyphrases as evidence that
they may be semantically related. The statistical association is measured using
web mining. Experiments demonstrate that the enhancements improve the quality
of the extracted keyphrases. Furthermore, the enhancements are not
domain-specific: the algorithm generalizes well when it is trained on one
domain (computer science documents) and tested on another (physics documents).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308034</id><created>2003-08-21</created><authors><author><keyname>Iovane</keyname><forenames>G.</forenames></author><author><keyname>Giordano</keyname><forenames>P.</forenames></author><author><keyname>Iovane</keyname><forenames>C.</forenames></author><author><keyname>Rotulo</keyname><forenames>F.</forenames></author></authors><title>Fingerprint based bio-starter and bio-access</title><categories>cs.CV</categories><comments>4 pages, Proceeding of Automotive 2003, Turin (Italy)</comments><acm-class>I.2.10, I.4, I.5</acm-class><abstract>  In the paper will be presented a safety and security system based on
fingerprint technology. The results suggest a new scenario where the new cars
can use a fingerprint sensor integrated in car handle to allow access and in
the dashboard as starter button.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308035</id><created>2003-08-21</created><authors><author><keyname>Iovane</keyname><forenames>G.</forenames></author><author><keyname>Tortoriello</keyname><forenames>F. S.</forenames></author></authors><title>IS (Iris Security)</title><categories>cs.CV</categories><comments>7 pages, Proceeding of NIDays 2003 (Sponsored by National
  Instruments), Rome (Italy)</comments><acm-class>I.5, I.5</acm-class><abstract>  In the paper will be presented a safety system based on iridology. The
results suggest a new scenario where the security problem in supervised and
unsupervised areas can be treat with the present system and the iris image
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308036</id><created>2003-08-21</created><updated>2003-09-17</updated><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J</forenames></author></authors><title>The Rich-Club Phenomenon In The Internet Topology</title><categories>cs.NI</categories><comments>To be appeared in the IEEE Communications Letters</comments><acm-class>C.2.1 and C.2.5</acm-class><journal-ref>IEEE Communications Letters, vol. 8, no. 3, pp.180-182, March
  2004.</journal-ref><doi>10.1109/LCOMM.2004.823426</doi><abstract>  We show that the Internet topology at the Autonomous System (AS) level has a
rich--club phenomenon. The rich nodes, which are a small number of nodes with
large numbers of links, are very well connected to each other. The rich--club
is a core tier that we measured using the rich--club connectivity and the
node--node link distribution. We obtained this core tier without any heuristic
assumption between the ASes. The rich--club phenomenon is a simple qualitative
way to differentiate between power law topologies and provides a criterion for
new network models. To show this, we compared the measured rich--club of the AS
graph with networks obtained using the Barab\'asi--Albert (BA) scale--free
network model, the Fitness BA model and the Inet--3.0 model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308037</id><created>2003-08-22</created><authors><author><keyname>Iovane</keyname><forenames>G.</forenames></author></authors><title>Distributed and Parallel Net Imaging</title><categories>cs.CV astro-ph cs.DC</categories><comments>3 pages, 8 figures, Procedding of NIDays 2003 (sponsored by National
  Instruments), Rome 2003. Winner (2nd classified) of the price &quot;Best
  Application of Measurement and Automation</comments><acm-class>I.4,I.5</acm-class><abstract>  A very complex vision system is developed to detect luminosity variations
connected with the discovery of new planets in the Universe. The traditional
imaging system can not manage a so large load. A private net is implemented to
perform an automatic vision and decision architecture. It lets to carry out an
on-line discrimination of interesting events by using two levels of triggers.
This system can even manage many Tbytes of data per day. The architecture
avails itself of a distributed parallel network system based on a maximum of
256 standard workstations with Microsoft Window as OS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308038</id><created>2003-08-22</created><authors><author><keyname>Iovane</keyname><forenames>G.</forenames></author></authors><title>Image Analysis in Astronomy for very large vision machine</title><categories>cs.CV astro-ph cs.DC</categories><comments>3 pages, 9 figures, Proceeding of NIWEEK 2002 (sponsored by National
  Instruments), Austin (Usa), 2002</comments><acm-class>I.4,I.5</acm-class><abstract>  It is developed a very complex system (hardware/software) to detect
luminosity variations connected with the discovery of new planets outside the
Solar System. Traditional imaging approaches are very demanding in terms of
computing time; then, the implementation of an automatic vision and decision
software architecture is presented. It allows to perform an on-line
discrimination of interesting events by using two levels of triggers. A
fundamental challenge was to work with very large CCD camera (even 16k*16k
pixels) in line with very large telescopes. Then, the architecture can use a
distributed parallel network system based on a maximum of 256 standard
workstations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308039</id><created>2003-08-23</created><authors><author><keyname>Schaale</keyname><forenames>Andreas</forenames></author><author><keyname>Wulf-Mathies</keyname><forenames>Carsten</forenames></author><author><keyname>Lieberam-Schmidt</keyname><forenames>Soenke</forenames></author></authors><title>A new approach to relevancy in Internet searching - the &quot;Vox Populi
  Algorithm&quot;</title><categories>cs.DS cond-mat.dis-nn cs.IR</categories><comments>9 pages Latex</comments><acm-class>H.3.1; H.3.2; H.3.3</acm-class><abstract>  In this paper we will derive a new algorithm for Internet searching. The main
idea of this algorithm is to extend the existing algorithms by a component,
which reflects the interests of the users more than existing methods. The &quot;Vox
Populi Algorithm&quot; (VPA) creates a feedback from the users to the content of the
search index. The information derived from the users query analysis is used to
modify the existing crawling algorithms. The VPA controls the distribution of
the resources of the crawler. Finally, we also discuss methods of suppressing
unwanted content (spam).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308040</id><created>2003-08-23</created><updated>2003-09-07</updated><authors><author><keyname>Rai</keyname><forenames>Sanatan</forenames></author></authors><title>Open source software and peer review</title><categories>cs.SE cs.CY</categories><comments>4 pages</comments><acm-class>K.7.0;D.2.9</acm-class><abstract>  We compare the open source model of software development to peer review in
academia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308041</id><created>2003-08-24</created><authors><author><keyname>Brodnik</keyname><forenames>Andrej</forenames></author><author><keyname>Nilsson</keyname><forenames>Andreas</forenames></author></authors><title>Static Data Structure for Discrete Advance Bandwidth Reservations on the
  Internet</title><categories>cs.DS</categories><report-no>IMFM-(2003)-PS-889</report-no><acm-class>E.1, C.2.3</acm-class><abstract>  In this paper we present a discrete data structure for reservations of
limited resources. A reservation is defined as a tuple consisting of the time
interval of when the resource should be reserved, $I_R$, and the amount of the
resource that is reserved, $B_R$, formally $R=\{I_R,B_R\}$.
  The data structure is similar to a segment tree. The maximum spanning
interval of the data structure is fixed and defined in advance. The granularity
and thereby the size of the intervals of the leaves is also defined in advance.
The data structure is built only once. Neither nodes nor leaves are ever
inserted, deleted or moved. Hence, the running time of the operations does not
depend on the number of reservations previously made. The running time does not
depend on the size of the interval of the reservation either. Let $n$ be the
number of leaves in the data structure. In the worst case, the number of
touched (i.e. traversed) nodes is in any operation $O(\log n)$, hence the
running time of any operation is also $O(\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308042</id><created>2003-08-27</created><authors><author><keyname>Palotai</keyname><forenames>Zsolt</forenames></author><author><keyname>Mandusitz</keyname><forenames>Sandor</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Centralized reward system gives rise to fast and efficient work sharing
  for intelligent Internet agents lacking direct communication</title><categories>cs.IR</categories><acm-class>H.3.3; H.3.4</acm-class><abstract>  WWW has a scale-free structure where novel information is often difficult to
locate. Moreover, Intelligent agents easily get trapped in this structure. Here
a novel method is put forth, which turns these traps into information
repositories, supplies: We populated an Internet environment with intelligent
news foragers. Foraging has its associated cost whereas foragers are rewarded
if they detect not yet discovered novel information. The intelligent news
foragers crawl by using the estimated long-term cumulated reward, and also have
a finite sized memory: the list of most promising supplies. Foragers form an
artificial life community: the most successful ones are allowed to multiply,
while unsuccessful ones die out. The specific property of this community is
that there is no direct communication amongst foragers but the centralized
rewarding system. Still, fast division of work is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308043</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308043</id><created>2003-08-28</created><updated>2011-06-21</updated><authors><author><keyname>Burger</keyname><forenames>John Robert</forenames></author></authors><title>Note on Needle in a Haystack</title><categories>cs.ET quant-ph</categories><comments>Edited and clarified; upgraded statements about use of Grover's
  quantum algorithm</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduced below is a quantum database method, not only for retrieval but
also for creation. It uses a particular structure of true's and false's in a
state vector of n qubits, permitting up to 2**2**n words, vastly more than for
classical bits. Several copies are produced so that later they can be
destructively observed and a word determined with high probability. Grover's
algorithm is proposed below to read out, nondestructively the unknown contents
of a given stored state vector using only one state vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0308044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0308044</id><created>2003-08-29</created><authors><author><keyname>Pivovarov</keyname><forenames>Grigorii</forenames></author><author><keyname>Trunov</keyname><forenames>Sergei</forenames></author></authors><title>EqRank: A Self-Consistent Equivalence Relation on Graph Vertexes</title><categories>cs.DS cs.DL</categories><comments>a kdd cup 2003 submission</comments><acm-class>H.3.3</acm-class><abstract>  A new method of hierarchical clustering of graph vertexes is suggested. In
the method, the graph partition is determined with an equivalence relation
satisfying a recursive definition stating that vertexes are equivalent if the
vertexes they point to (or vertexes pointing to them) are equivalent. Iterative
application of the partitioning yields a hierarchical clustering of graph
vertexes. The method is applied to the citation graph of hep-th. The outcome is
a two-level classification scheme for the subject field presented in hep-th,
and indexing of the papers from hep-th in this scheme. A number of tests show
that the classification obtained is adequate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309001</id><created>2003-08-31</created><updated>2003-09-02</updated><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author></authors><title>Media Affordances of a Mobile Push-To-Talk Communication Service</title><categories>cs.HC</categories><comments>20 pages</comments><acm-class>H.4.3; H.5.3</acm-class><abstract>  This paper presents an exploratory study of college-age students using
two-way, push-to-talk cellular radios. We describe the observed and reported
use of cellular radio by the participants, the activities and purposes for
which they adopted it, and their responses. We then examine these empirical
results using mediated communication theory. Cellular radios have a unique
combination of affordances relative to other media used by this age group,
including instant messaging (IM) and mobile phones; the results of our analysis
do suggest explanations for some observed phenomena but also highlight the
counter-intuitive nature of other phenomena. For example, although the radios
have many important dissimilarities with IM from the viewpoint of mediated
communication theory, the observed use patterns resembled those of IM to a
surprising degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309002</id><created>2003-08-31</created><updated>2003-09-04</updated><authors><author><keyname>Galli</keyname><forenames>Ricardo</forenames></author></authors><title>El informe NERA analizado</title><categories>cs.CY</categories><comments>Spanish</comments><acm-class>K.4.1;K.4.4</acm-class><abstract>  This is a review of the article &quot;Government Preferences for Promoting
Open-Source Software: A Solution in Search of A Problem&quot; by David Evans and
Bernard J. Reddy. This report was paid for by Microsoft and put together at its
request. Now Microsoft is using it as part of their lobbying campaign in Europe
against governments' promotion of Open Source Software. As expected, this
article is strongly biased and most of the conclusions are based upon false
hypotheses and evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309003</id><created>2003-09-01</created><authors><author><keyname>Bozzano</keyname><forenames>M.</forenames></author><author><keyname>Delzanno</keyname><forenames>G.</forenames></author><author><keyname>Martelli</keyname><forenames>M.</forenames></author></authors><title>Model Checking Linear Logic Specifications</title><categories>cs.PL cs.SC</categories><comments>53 pages, 12 figures &quot;Under consideration for publication in Theory
  and Practice of Logic Programming&quot;</comments><acm-class>D.3.1;F.3.1;F.3.2</acm-class><abstract>  The overall goal of this paper is to investigate the theoretical foundations
of algorithmic verification techniques for first order linear logic
specifications. The fragment of linear logic we consider in this paper is based
on the linear logic programming language called LO enriched with universally
quantified goal formulas. Although LO was originally introduced as a
theoretical foundation for extensions of logic programming languages, it can
also be viewed as a very general language to specify a wide range of
infinite-state concurrent systems.
  Our approach is based on the relation between backward reachability and
provability highlighted in our previous work on propositional LO programs.
Following this line of research, we define here a general framework for the
bottom-up evaluation of first order linear logic specifications. The evaluation
procedure is based on an effective fixpoint operator working on a symbolic
representation of infinite collections of first order linear logic formulas.
The theory of well quasi-orderings can be used to provide sufficient conditions
for the termination of the evaluation of non trivial fragments of first order
linear logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309004</id><created>2003-09-02</created><authors><author><keyname>Long</keyname><forenames>Bruce</forenames></author></authors><title>The Structure of Information</title><categories>cs.LO</categories><comments>5 Pages</comments><acm-class>F.0;H.1.1</acm-class><abstract>  A formal model of the structure of information is presented in five axioms
which define identity, containment, and joins of infons. Joins are shown to be
commutative, associative, provide inverses of infons, and, potentially, have
many identity elements, two of which are multiplicative and additive. Those two
types of join are distributive. The other identity elements are for operators
on entwined states. Multiplicative joins correspond to adding or removing new
bits to a system while additive joins correspond to a change of state. The
order or size of an infon is defined. This groundwork is intended to be used to
model continuous and discreet information structures through time, especially
in closed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309005</identifier>
 <datestamp>2007-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309005</id><created>2003-09-05</created><updated>2007-02-08</updated><authors><author><keyname>Stojmirovic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Pestov</keyname><forenames>Vladimir</forenames></author></authors><title>Indexing Schemes for Similarity Search In Datasets of Short Protein
  Fragments</title><categories>cs.DS q-bio.BM</categories><comments>34 pages, 12 figures, 4 tables - Timings for experiments added upon
  referees' request, and a number of less substantial modifications made</comments><acm-class>H.3.1; J.3</acm-class><journal-ref>Information Systems 32 (2007), 1145-1165</journal-ref><abstract>  We propose a family of very efficient hierarchical indexing schemes for
ungapped, score matrix-based similarity search in large datasets of short (4-12
amino acid) protein fragments. This type of similarity search has importance in
both providing a building block to more complex algorithms and for possible use
in direct biological investigations where datasets are of the order of 60
million objects. Our scheme is based on the internal geometry of the amino acid
alphabet and performs exceptionally well, for example outputting 100 nearest
neighbours to any possible fragment of length 10 after scanning on average less
than one per cent of the entire dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309006</id><created>2003-09-06</created><updated>2004-06-21</updated><authors><author><keyname>Kannan</keyname><forenames>Rajgopal</forenames></author></authors><title>The KR-Benes Network: A Control-Optimal Rearrangeable Permutation
  Network</title><categories>cs.NI cs.CC</categories><comments>18 pages, 11 figures, website http://www.csc.lsu.edu/~rkannan V3:
  Proved the (previous) Conjecture on Optimality of K-Benes</comments><report-no>LSU Computer Science Tech Report LSU-CSC-TR03-01</report-no><acm-class>C.2.1</acm-class><journal-ref>IEEE Transactions on Computers, Vol. 54, No. 5, pp. 534-544, May
  2005.</journal-ref><abstract>  The Benes network has been used as a rearrangeable network for over 40 years,
yet the uniform $N(2 \log N-1)$ control complexity of the $N \times N$ Benes is
not optimal for many permutations. In this paper, we present a novel $O(\log
N)$ depth rearrangeable network called KR-Benes that is {\it
permutation-specific control-optimal}. The KR-Benes routes {\it every}
permutation with the minimal control complexity {\it specific} to that
permutation and its worst-case complexity for arbitrary permutations is bounded
by the Benes; thus it replaces the Benes when considering control
complexity/latency. We design the KR-Benes by first constructing a restricted
$2 \log K +2$ depth rearrangeable network called $K$-Benes for routing
$K$-bounded permutations with control $2N \log K$, $0 \leq K \leq N/4$. We then
show that the $N \times N$ Benes network itself (with one additional stage)
contains every $K$-Benes network as a subgraph and use this property to
construct the KR-Benes network. With regard to the control-optimality of the
KR-Benes, we show that any optimal network for rearrangeably routing
$K$-bounded permutations must have depth $2 \log K + 2$, and therefore the
$K$-Benes (and hence the KR-Benes) is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309007</id><created>2003-09-07</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>ROC Curves Within the Framework of Neural Network Assembly Memory Model:
  Some Analytic Results</title><categories>cs.AI cs.IR q-bio.NC q-bio.QM</categories><comments>Proceedings of the KDS-2003 Conference held in Varna, Bulgaria on
  June 16-26, 2003, pages 138-146, 5 Figures, 18 references</comments><acm-class>I.2; E.4; J.3; J.4</acm-class><journal-ref>International Journal on Information Theories &amp; Applications,
  2003, vol. 10, no.2, pp.189-197.</journal-ref><abstract>  On the basis of convolutional (Hamming) version of recent Neural Network
Assembly Memory Model (NNAMM) for intact two-layer autoassociative Hopfield
network optimal receiver operating characteristics (ROCs) have been derived
analytically. A method of taking into account explicitly a priori probabilities
of alternative hypotheses on the structure of information initiating memory
trace retrieval and modified ROCs (mROCs, a posteriori probabilities of correct
recall vs. false alarm probability) are introduced. The comparison of empirical
and calculated ROCs (or mROCs) demonstrates that they coincide quantitatively
and in this way intensities of cues used in appropriate experiments may be
estimated. It has been found that basic ROC properties which are one of
experimental findings underpinning dual-process models of recognition memory
can be explained within our one-factor NNAMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309008</id><created>2003-09-07</created><authors><author><keyname>Appleboim</keyname><forenames>Eli</forenames></author><author><keyname>Saucan</keyname><forenames>Emil</forenames></author></authors><title>Digital Version of Green`s Theorem and its Application to The Coverage
  Problem in Formal Verification</title><categories>cs.SC</categories><comments>13 Pages, 6 figures. Submitted to the IEEE Transactions on
  Computer-Aided Design of Integrated Circuits and Systems</comments><acm-class>B.6.3</acm-class><abstract>  We present a novel scheme to the coverage problem, introducing a quantitative
way to estimate the interaction between a block and its enviroment.This is
achieved by setting a discrete version of Green`s theorem, specially adapted
for Model Checking based verification of integrated circuits.This method is
best suited for the coverage problem since it enables one to quantify the
incompleteness or, on the other hand, the redundancy of a set of rules,
describing the model under verification.Moreover this can be done continuously
throughout the verification process, thus enabling the user to pinpoint the
stages at which incompleteness/redundancy occurs. Although the method is
presented locally on a small hardware example, we additionally show its
possibility to provide precise coverage estimation also for large scale
systems. We compare this method to others by checking it on the same
test-cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309009</id><created>2003-09-08</created><authors><author><keyname>Eliashberg</keyname><forenames>Victor</forenames></author></authors><title>What Is Working Memory and Mental Imagery? A Robot that Learns to
  Perform Mental Computations</title><categories>cs.AI cs.NE</categories><comments>31 pages, 16 figures</comments><acm-class>I.2.0</acm-class><abstract>  This paper goes back to Turing (1936) and treats his machine as a cognitive
model (W,D,B), where W is an &quot;external world&quot; represented by memory device (the
tape divided into squares), and (D,B) is a simple robot that consists of the
sensory-motor devices, D, and the brain, B. The robot's sensory-motor devices
(the &quot;eye&quot;, the &quot;hand&quot;, and the &quot;organ of speech&quot;) allow the robot to simulate
the work of any Turing machine. The robot simulates the internal states of a
Turing machine by &quot;talking to itself.&quot; At the stage of training, the teacher
forces the robot (by acting directly on its motor centers) to perform several
examples of an algorithm with different input data presented on tape. Two
effects are achieved: 1) The robot learns to perform the shown algorithm with
any input data using the tape. 2) The robot learns to perform the algorithm
&quot;mentally&quot; using an &quot;imaginary tape.&quot; The model illustrates the simplest
concept of a universal learning neurocomputer, demonstrates universality of
associative learning as the mechanism of programming, and provides a
simplified, but nontrivial neurobiologically plausible explanation of the
phenomena of working memory and mental imagery. The model is implemented as a
user-friendly program for Windows called EROBOT. The program is available at
www.brain0.com/software.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309010</id><created>2003-09-08</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Ponomarenko</keyname><forenames>Ilia</forenames></author></authors><title>Homomorphic public-key cryptosystems over groups and rings</title><categories>cs.CR</categories><acm-class>I.2.1; I.1.2; F.2.1</acm-class><abstract>  We propose a new homomorphic public-key cryptosystem over arbitrary
nonidentity finite group based on the difficulty of the membership problem for
groups of integer matrices. Besides, a homomorphic cryptosystem is designed for
the first time over finite commutative rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309011</id><created>2003-09-08</created><authors><author><keyname>Egilsson</keyname><forenames>Agust S.</forenames></author><author><keyname>Gudbjartsson</keyname><forenames>Hakon</forenames></author></authors><title>Indexing of Tables Referencing Complex Structures</title><categories>cs.DB</categories><comments>12 pages</comments><acm-class>H.3.1;H.2.8;J.3</acm-class><abstract>  We introduce indexing of tables referencing complex structures such as
digraphs and spatial objects, appearing in genetics and other data intensive
analysis. The indexing is achieved by extracting dimension schemas from the
referenced structures. The schemas and their dimensionality are determined by
proper coloring algorithms and the duality between all such schemas and all
such possible proper colorings is established. This duality, in turn, provides
us with an extensive library of solutions when addressing indexing questions.
It is illustrated how to use the schemas, in connection with additional
relational database technologies, to optimize queries conditioned on the
structural information being referenced. Comparisons using bitmap indexing in
the Oracle 9.2i database, on the one hand, and multidimensional clustering in
DB2 8.1.2, on the other hand, are used to illustrate the applicability of the
indexing to different technology settings. Finally, we illustrate how the
indexing can be used to extract low dimensional schemas from a binary interval
tree in order to resolve efficiently interval and stabbing queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309012</id><created>2003-09-09</created><authors><author><keyname>Huang</keyname><forenames>C.</forenames></author><author><keyname>Rocha</keyname><forenames>L. M.</forenames></author></authors><title>Exploration of RNA Editing and Design of Robust Genetic Algorithms</title><categories>cs.NE cs.AI nlin.AO q-bio.GN</categories><comments>Proceedings of the 2003 IEEE Congress on Evolutionary Computation.
  Camberra, Australia, December 2003</comments><report-no>LAUR 03-4314</report-no><acm-class>I.2</acm-class><abstract>  This paper presents our computational methodology using Genetic Algorithms
(GA) for exploring the nature of RNA editing. These models are constructed
using several genetic editing characteristics that are gleaned from the RNA
editing system as observed in several organisms. We have expanded the
traditional Genetic Algorithm with artificial editing mechanisms as proposed by
(Rocha, 1997). The incorporation of editing mechanisms provides a means for
artificial agents with genetic descriptions to gain greater phenotypic
plasticity, which may be environmentally regulated. Our first implementations
of these ideas have shed some light into the evolutionary implications of RNA
editing. Based on these understandings, we demonstrate how to select proper RNA
editors for designing more robust GAs, and the results will show promising
applications to real-world problems. We expect that the framework proposed will
both facilitate determining the evolutionary role of RNA editing in biology,
and advance the current state of research in Genetic Algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309013</id><created>2003-09-09</created><authors><author><keyname>Rocha</keyname><forenames>L. M.</forenames></author></authors><title>Semi-metric Behavior in Document Networks and its Application to
  Recommendation Systems</title><categories>cs.IR cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.DL cs.HC cs.MA</categories><acm-class>H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5</acm-class><journal-ref>In: Soft Computing Agents: A New Perspective for Dynamic
  Information Systems. V. Loia (Ed.) International Series Frontiers in
  Artificial Intelligence and Applications. IOS Press, pp. 137-163, 2002</journal-ref><abstract>  Recommendation systems for different Document Networks (DN) such as the World
Wide Web (WWW) and Digital Libraries, often use distance functions extracted
from relationships among documents and keywords. For instance, documents in the
WWW are related via a hyperlink network, while documents in bibliographic
databases are related by citation and collaboration networks. Furthermore,
documents are related to keyterms. The distance functions computed from these
relations establish associative networks among items of the DN, referred to as
Distance Graphs, which allow recommendation systems to identify relevant
associations for individual users. However, modern recommendation systems need
to integrate associative data from multiple sources such as different
databases, web sites, and even other users. Thus, we are presented with a
problem of combining evidence (about associations between items) from different
sources characterized by distance functions. In this paper we describe our work
on (1) inferring relevant associations from, as well as characterizing,
semi-metric distance graphs and (2) combining evidence from different distance
graphs in a recommendation system. Regarding (1), we present the idea of
semi-metric distance graphs, and introduce ratios to measure semi-metric
behavior. We compute these ratios for several DN such as digital libraries and
web sites and show that they are useful to identify implicit associations.
Regarding (2), we describe an algorithm to combine evidence from distance
graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy
Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed
for a recommendation system named TalkMine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309014</id><created>2003-09-09</created><updated>2005-06-21</updated><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Sethia</keyname><forenames>Saurabh</forenames></author></authors><title>Optimal Covering Tours with Turn Costs</title><categories>cs.DS cs.CG</categories><comments>36 pages, 19 figures, 2 tables, Latex; to appear in SIAM Journal on
  Computing. New version contains more technical details in Sections 4.1, 5.1,
  5.2, 5.5, four more figures, four more pages, as well as numerous smaller
  changes</comments><acm-class>F.2.2</acm-class><abstract>  We give the first algorithmic study of a class of ``covering tour'' problems
related to the geometric Traveling Salesman Problem: Find a polygonal tour for
a cutter so that it sweeps out a specified region (``pocket''), in order to
minimize a cost that depends mainly on the number of em turns. These problems
arise naturally in manufacturing applications of computational geometry to
automatic tool path generation and automatic inspection systems, as well as arc
routing (``postman'') problems with turn penalties. We prove the
NP-completeness of minimum-turn milling and give efficient approximation
algorithms for several natural versions of the problem, including a
polynomial-time approximation scheme based on a novel adaptation of the
m-guillotine method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309015</id><created>2003-09-10</created><authors><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Herrmann</keyname><forenames>Daniel</forenames></author></authors><title>Reliable and Efficient Inference of Bayesian Networks from Sparse Data
  by Statistical Learning Theory</title><categories>cs.LG</categories><comments>12 pages</comments><acm-class>K.3.2</acm-class><abstract>  To learn (statistical) dependencies among random variables requires
exponentially large sample size in the number of observed random variables if
any arbitrary joint probability distribution can occur.
  We consider the case that sparse data strongly suggest that the probabilities
can be described by a simple Bayesian network, i.e., by a graph with small
in-degree \Delta. Then this simple law will also explain further data with high
confidence. This is shown by calculating bounds on the VC dimension of the set
of those probability measures that correspond to simple graphs. This allows to
select networks by structural risk minimization and gives reliability bounds on
the error of the estimated joint measure without (in contrast to a previous
paper) any prior assumptions on the set of possible joint measures.
  The complexity for searching the optimal Bayesian networks of in-degree
\Delta increases only polynomially in the number of random varibales for
constant \Delta and the optimal joint measure associated with a given graph can
be found by convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309016</id><created>2003-09-10</created><authors><author><keyname>McDonald</keyname><forenames>Stuart</forenames></author><author><keyname>Wagner</keyname><forenames>Liam</forenames></author></authors><title>Using Simulated Annealing to Calculate the Trembles of Trembling Hand
  Perfection</title><categories>cs.GT cs.CC cs.DS cs.LG cs.NE q-bio.PE</categories><comments>To appear in the Proceedings of IEEE Congress on Evolutionary
  Computation 2003 (CEC'03)</comments><acm-class>F.1.1;F.2.2;G.3;I.2.1;J.4</acm-class><journal-ref>Proceedings of IEEE Congress on Evolutionary Computation 2003,
  vol.4, pp. 2482-2489</journal-ref><abstract>  Within the literature on non-cooperative game theory, there have been a
number of attempts to propose logorithms which will compute Nash equilibria.
Rather than derive a new algorithm, this paper shows that the family of
algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate
Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov
chains to ensure its regularity conditions. MCMC has been widely used
throughout the statistics and optimization literature, where variants of this
algorithm are known as simulated annealing. This paper shows that there is
interesting connection between the trembles that underlie the functioning of
this algorithm and the type of Nash refinement known as trembling hand
perfection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309017</id><created>2003-09-11</created><authors><author><keyname>Renault</keyname><forenames>David</forenames></author></authors><title>Enumerating planar locally finite Cayley graphs</title><categories>cs.DM</categories><comments>19 pages, 6 PostScript figures, 12 embedded PsTricks figures. An
  additional file (~ 438ko.) containing the figures in appendix might be found
  at http://www.labri.fr/Perso/~renault/research/pages.ps.gz</comments><acm-class>G.2.2</acm-class><abstract>  We characterize the set of planar locally finite Cayley graphs, and give a
finite representation of these graphs by a special kind of finite state
automata called labeling schemes. As a result, we are able to enumerate and
describe all planar locally finite Cayley graphs of a given degree. This
analysis allows us to solve the problem of decision of the locally finite
planarity for a word-problem-decidable presentation.
  Keywords: vertex-transitive, Cayley graph, planar graph, tiling, labeling
scheme
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309018</id><created>2003-09-11</created><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author><author><keyname>Moa</keyname><forenames>B.</forenames></author></authors><title>Using Propagation for Solving Complex Arithmetic Constraints</title><categories>cs.NA cs.AR cs.CC cs.GL cs.PF cs.RO</categories><comments>10 pages</comments><acm-class>B.8; G.1.5;G.1.6;I.2.9;I.3.1;C.1.4;D.2.4;F.2</acm-class><abstract>  Solving a system of nonlinear inequalities is an important problem for which
conventional numerical analysis has no satisfactory method. With a
box-consistency algorithm one can compute a cover for the solution set to
arbitrarily close approximation. Because of difficulties in the use of
propagation for complex arithmetic expressions, box consistency is computed
with interval arithmetic. In this paper we present theorems that support a
simple modification of propagation that allows complex arithmetic expressions
to be handled efficiently. The version of box consistency that is obtained in
this way is stronger than when interval arithmetic is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309019</id><created>2003-09-12</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author></authors><title>Building a Test Collection for Speech-Driven Web Retrieval</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>Proceedings of the 8th European Conference on Speech Communication
  and Technology (Eurospeech 2003), pp.1153-1156, Sep. 2003</journal-ref><abstract>  This paper describes a test collection (benchmark data) for retrieval systems
driven by spoken queries. This collection was produced in the subtask of the
NTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation
workshop. The search topics and document collection for the Web retrieval task
were used to produce spoken queries and language models for speech recognition,
respectively. We used this collection to evaluate the performance of our
retrieval system. Experimental results showed that (a) the use of target
documents for language modeling and (b) enhancement of the vocabulary size in
speech recognition were effective in improving the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309020</id><created>2003-09-12</created><updated>2005-02-24</updated><authors><author><keyname>Mertens</keyname><forenames>Stephan</forenames></author><author><keyname>Mezard</keyname><forenames>Marc</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Threshold values of Random K-SAT from the cavity method</title><categories>cs.CC cond-mat.dis-nn cs.DM</categories><comments>38 pages; extended explanations and derivations; this version is
  going to appear in Random Structures &amp; Algorithms</comments><acm-class>F.2.0; G.2.0</acm-class><abstract>  Using the cavity equations of
\cite{mezard:parisi:zecchina:02,mezard:zecchina:02}, we derive the various
threshold values for the number of clauses per variable of the random
$K$-satisfiability problem, generalizing the previous results to $K \ge 4$. We
also give an analytic solution of the equations, and some closed expressions
for these thresholds, in an expansion around large $K$. The stability of the
solution is also computed. For any $K$, the satisfiability threshold is found
to be in the stable region of the solution, which adds further credit to the
conjecture that this computation gives the exact satisfiability threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309021</id><created>2003-09-13</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author><author><keyname>Akiba</keyname><forenames>Tomoyosi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>A Cross-media Retrieval System for Lecture Videos</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>Proceedings of the 8th European Conference on Speech Communication
  and Technology (Eurospeech 2003), pp.1149-1152, Sep. 2003</journal-ref><abstract>  We propose a cross-media lecture-on-demand system, in which users can
selectively view specific segments of lecture videos by submitting text
queries. Users can easily formulate queries by using the textbook associated
with a target lecture, even if they cannot come up with effective keywords. Our
system extracts the audio track from a target lecture video, generates a
transcription by large vocabulary continuous speech recognition, and produces a
text index. Experimental results showed that by adapting speech recognition to
the topic of the lecture, the recognition accuracy increased and the retrieval
accuracy was comparable with that obtained by human transcription.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309022</id><created>2003-09-13</created><updated>2003-10-15</updated><authors><author><keyname>Thiemann</keyname><forenames>Christian</forenames></author><author><keyname>Schlenker</keyname><forenames>Michael</forenames></author><author><keyname>Severiens</keyname><forenames>Thomas</forenames></author></authors><title>Proposed Specification of a Distributed XML-Query Network</title><categories>cs.DC cs.IR</categories><acm-class>H.3.4</acm-class><abstract>  W3C's XML-Query language offers a powerful instrument for information
retrieval on XML repositories. This article describes an implementation of this
retrieval in a real world's scenario. Distributed XML-Query processing reduces
load on every single attending node to an acceptable level. The network allows
every participant to control their computing load themselves. Furthermore
XML-repositories may stay at the rights holder, so every Data-Provider can
decide, whether to process critical queries or not. If Data-Providers keep
redundant information, this distributed network improves reliability of
information with duplicates removed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309023</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309023</id><created>2003-09-13</created><authors><author><keyname>Batagelj</keyname><forenames>Vladimir</forenames><affiliation>University of Ljubljana, Slovenia</affiliation></author></authors><title>Efficient Algorithms for Citation Network Analysis</title><categories>cs.DL cs.DM cs.DS</categories><report-no>IMFM 897</report-no><acm-class>E.1; G.2.2; H.3.7</acm-class><journal-ref>inside the book: V. Batagelj, P. Doreian, A. Ferligoj, N.
  Kej\v{z}ar: Understanding Large Temporal Networks and Spatial Networks.
  Wiley, 2014. ISBN: 978-0-470-71452-2</journal-ref><abstract>  In the paper very efficient, linear in number of arcs, algorithms for
determining Hummon and Doreian's arc weights SPLC and SPNP in citation network
are proposed, and some theoretical properties of these weights are presented.
The nonacyclicity problem in citation networks is discussed. An approach to
identify on the basis of arc weights an important small subnetwork is proposed
and illustrated on the citation networks of SOM (self organizing maps)
literature and US patents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309024</id><created>2003-09-14</created><authors><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Morgan</keyname><forenames>Carroll</forenames></author></authors><title>Results on the quantitative mu-calculus qMu</title><categories>cs.LO cs.GT</categories><comments>45 pages, 4 figures</comments><acm-class>D.2.4;F.1.2;F.3.1;F.4.1;G.3</acm-class><abstract>  The mu-calculus is a powerful tool for specifying and verifying transition
systems, including those with both demonic and angelic choice; its quantitative
generalisation qMu extends that to probabilistic choice.
  We show that for a finite-state system the logical interpretation of qMu, via
fixed-points in a domain of real-valued functions into [0,1], is equivalent to
an operational interpretation given as a turn-based gambling game between two
players.
  The logical interpretation provides direct access to axioms, laws and
meta-theorems. The operational, game- based interpretation aids the intuition
and continues in the more general context to provide a surprisingly practical
specification tool.
  A corollary of our proofs is an extension of Everett's singly-nested games
result in the finite turn-based case: we prove well-definedness of the minimax
value, and existence of fixed memoriless strategies, for all qMu
games/formulae, of arbitrary (including alternating) nesting structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309025</id><created>2003-09-15</created><authors><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Evidential Force Aggregation</title><categories>cs.AI</categories><comments>7 pages, 2 figures</comments><report-no>FOI-S-0960-SE</report-no><acm-class>I.2.3; I.5.2</acm-class><journal-ref>in Proceedings of the Sixth International Conference on
  Information Fusion (FUSION 2003), pp. 1223-1229, Cairns, Australia, 8-11 July
  2003, International Society of Information Fusion, 2003</journal-ref><abstract>  In this paper we develop an evidential force aggregation method intended for
classification of evidential intelligence into recognized force structures. We
assume that the intelligence has already been partitioned into clusters and use
the classification method individually in each cluster. The classification is
based on a measure of fitness between template and fused intelligence that
makes it possible to handle intelligence reports with multiple nonspecific and
uncertain propositions. With this measure we can aggregate on a level-by-level
basis, starting from general intelligence to achieve a complete force structure
with recognized units on all hierarchical levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309026</id><created>2003-09-15</created><authors><author><keyname>Little</keyname><forenames>Mark C.</forenames></author></authors><title>A thought experiment on Quantum Mechanics and Distributed Failure
  Detection</title><categories>cs.DC</categories><acm-class>C.2.4; D.4.5</acm-class><abstract>  One of the biggest problems in current distributed systems is that presented
by one machine attempting to determine the liveness of another in a timely
manner. Unfortunately, the symptoms exhibited by a failed machine can also be
the result of other causes, e.g., an overloaded machine or network which drops
messages, making it impossible to detect a machine failure with cetainty until
that machine recovers. This is a well understood problem and one which has led
to a large body of research into failure suspectors: since it is not possible
to detect a failure, the best one can do is suspect a failure and program
accordingly. However, one machine's suspicions may not be the same as
another's; therefore, these algorithms spend a considerable effort in ensuring
a consistent view among all available machines of who is suspects of being
failed. This paper describes a thought experiment on how quantum mechanics may
be used to provide a failure detector that is guaranteed to give both accurate
and instantaneous information about the liveness of machines, no matter the
distances involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309027</id><created>2003-09-15</created><updated>2004-12-21</updated><authors><author><keyname>Ronsse</keyname><forenames>Michiel</forenames></author><author><keyname>De Bosschere</keyname><forenames>Koen</forenames></author></authors><title>Proceedings of the Fifth International Workshop on Automated Debugging
  (AADEBUG 2003)</title><categories>cs.SE cs.PL</categories><comments>Workshop homepage at http://www.elis.UGent.be/aadebug2003/</comments><acm-class>D.2.5</acm-class><abstract>  Over the past decades automated debugging has seen major achievements.
However, as debugging is by necessity attached to particular programming
paradigms, the results are scattered. To alleviate this problem, the Automated
and Algorithmic Debugging workshop (AADEBUG for short) was organised in 1993 in
Link&quot;oping (Sweden). As this workshop proved to be successful, subsequent
workshops have been organised in 1995 (Saint-Malo, France), 1997 (again in
Link&quot;oping, Sweden) and 2000 (Munich, Germany). In 2003, the workshop is
organised in Ghent, Belgium, the proceedings of which you are reading right
now.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309028</id><created>2003-09-16</created><authors><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author></authors><title>cTI: A constraint-based termination inference tool for ISO-Prolog</title><categories>cs.PL</categories><comments>16 pages, 3 tables, to appear on &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>F.3.2</acm-class><abstract>  We present cTI, the first system for universal left-termination inference of
logic programs. Termination inference generalizes termination analysis and
checking. Traditionally, a termination analyzer tries to prove that a given
class of queries terminates. This class must be provided to the system, for
instance by means of user annotations. Moreover, the analysis must be redone
every time the class of queries of interest is updated. Termination inference,
in contrast, requires neither user annotations nor recomputation. In this
approach, terminating classes for all predicates are inferred at once. We
describe the architecture of cTI and report an extensive experimental
evaluation of the system covering many classical examples from the logic
programming termination literature and several Prolog programs of respectable
size and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309029</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309029</id><created>2003-09-16</created><authors><author><keyname>Maebe</keyname><forenames>J.</forenames></author><author><keyname>De Bosschere</keyname><forenames>K.</forenames></author></authors><title>Instrumenting self-modifying code</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Adding small code snippets at key points to existing code fragments is called
instrumentation. It is an established technique to debug certain otherwise hard
to solve faults, such as memory management issues and data races. Dynamic
instrumentation can already be used to analyse code which is loaded or even
generated at run time.With the advent of environments such as the Java Virtual
Machine with optimizing Just-In-Time compilers, a new obstacle arises:
self-modifying code. In order to instrument this kind of code correctly, one
must be able to detect modifications and adapt the instrumentation code
accordingly, preferably without incurring a high penalty speedwise. In this
paper we propose an innovative technique that uses the hardware page protection
mechanism of modern processors to detect such modifications. We also show how
an instrumentor can adapt the instrumented version depending on the kind of
modificiations as well as an experimental evaluation of said techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309030</id><created>2003-09-16</created><authors><author><keyname>Mayer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Stumptner</keyname><forenames>Markus</forenames></author></authors><title>Model-Based Debugging using Multiple Abstract Models</title><categories>cs.SE cs.AI</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  This paper introduces an automatic debugging framework that relies on
model-based reasoning techniques to locate faults in programs. In particular,
model-based diagnosis, together with an abstract interpretation based conflict
detection mechanism is used to derive diagnoses, which correspond to possible
faults in programs. Design information and partial specifications are applied
to guide a model revision process, which allows for automatic detection and
correction of structural faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309031</id><created>2003-09-17</created><authors><author><keyname>Maruyama</keyname><forenames>Kazutaka</forenames></author><author><keyname>Terada</keyname><forenames>Minoru</forenames></author></authors><title>Timestamp Based Execution Control for C and Java Programs</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Many programmers have had to deal with an overwritten variable resulting for
example from an aliasing problem. The culprit is obviously the last
write-access to that memory location before the manifestation of the bug. The
usual technique for removing such bugs starts with the debugger by (1) finding
the last write and (2) moving the control point of execution back to that time
by re-executing the program from the beginning. We wish to automate this. Step
(2) is easy if we can somehow mark the last write found in step (1) and control
the execution-point to move it back to this time.
  In this paper we propose a new concept, position, that is, a point in the
program execution trace, as needed for step (2) above. The position enables
debuggers to automate the control of program execution to support common
debugging activities. We have implemented position in C by modifying GCC and in
Java with a bytecode transformer. Measurements show that position can be
provided with an acceptable amount of overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309032</id><created>2003-09-17</created><authors><author><keyname>Ferrand</keyname><forenames>Gerard</forenames></author><author><keyname>Lesaint</keyname><forenames>Willy</forenames></author><author><keyname>Tessier</keyname><forenames>Alexandre</forenames></author></authors><title>Towards declarative diagnosis of constraint programs over finite domains</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  The paper proposes a theoretical approach of the debugging of constraint
programs based on a notion of explanation tree. The proposed approach is an
attempt to adapt algorithmic debugging to constraint programming. In this
theoretical framework for domain reduction, explanations are proof trees
explaining value removals. These proof trees are defined by inductive
definitions which express the removals of values as consequences of other value
removals. Explanations may be considered as the essence of constraint
programming. They are a declarative view of the computation trace. The
diagnosis consists in locating an error in an explanation rooted by a symptom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309033</id><created>2003-09-17</created><authors><author><keyname>Sen</keyname><forenames>Pranab</forenames></author><author><keyname>Venkatesh</keyname><forenames>S.</forenames></author></authors><title>Lower bounds for predecessor searching in the cell probe model</title><categories>cs.CC cs.DS quant-ph</categories><comments>Journal version of a paper at ICALP 2001 (quant-ph/0104100) and a
  paper at CCC 2003. 27 pages</comments><acm-class>E.1; E.4</acm-class><abstract>  We consider a fundamental problem in data structures, static predecessor
searching: Given a subset S of size n from the universe [m], store S so that
queries of the form &quot;What is the predecessor of x in S?&quot; can be answered
efficiently. We study this problem in the cell probe model introduced by Yao.
Recently, Beame and Fich obtained optimal bounds on the number of probes needed
by any deterministic query scheme if the associated storage scheme uses only
n^{O(1)} cells of word size (\log m)^{O(1)} bits. We give a new lower bound
proof for this problem that matches the bounds of Beame and Fich. Our lower
bound proof has the following advantages: it works for randomised query schemes
too, while Beame and Fich's proof works for deterministic query schemes only.
It also extends to `quantum address-only' query schemes that we define in this
paper, and is simpler than Beame and Fich's proof. We prove our lower bound
using the round elimination approach of Miltersen, Nisan, Safra and Wigderson.
Using tools from information theory, we prove a strong round elimination lemma
for communication complexity that enables us to obtain a tight lower bound for
the predecessor problem. Our strong round elimination lemma also extends to
quantum communication complexity. We also use our round elimination lemma to
obtain a rounds versus communication tradeoff for the `greater-than' problem,
improving on the tradeoff in Miltersen et al. We believe that our round
elimination lemma is of independent interest and should have other
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309034</id><created>2003-09-19</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames><affiliation>Rutgers University</affiliation></author></authors><title>Measuring Praise and Criticism: Inference of Semantic Orientation from
  Association</title><categories>cs.CL cs.IR cs.LG</categories><comments>37 pages, related work available at
  http://www.cs.rutgers.edu/~mlittman/ and http://purl.org/peter.turney/</comments><report-no>NRC-46516</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><journal-ref>ACM Transactions on Information Systems (TOIS), (2003), 21 (4),
  315-346</journal-ref><abstract>  The evaluative character of a word is called its semantic orientation.
Positive semantic orientation indicates praise (e.g., &quot;honest&quot;, &quot;intrepid&quot;) and
negative semantic orientation indicates criticism (e.g., &quot;disturbing&quot;,
&quot;superfluous&quot;). Semantic orientation varies in both direction (positive or
negative) and degree (mild to strong). An automated system for measuring
semantic orientation would have application in text classification, text
filtering, tracking opinions in online discussions, analysis of survey
responses, and automated chat systems (chatbots). This paper introduces a
method for inferring the semantic orientation of a word from its statistical
association with a set of positive and negative paradigm words. Two instances
of this approach are evaluated, based on two different statistical measures of
word association: pointwise mutual information (PMI) and latent semantic
analysis (LSA). The method is experimentally tested with 3,596 words (including
adjectives, adverbs, nouns, and verbs) that have been manually labeled positive
(1,614 words) and negative (1,982 words). The method attains an accuracy of
82.8% on the full test set, but the accuracy rises above 95% when the algorithm
is allowed to abstain from classifying mild words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309035</id><created>2003-09-19</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Bigham</keyname><forenames>Jeffrey</forenames></author><author><keyname>Shnayder</keyname><forenames>Victor</forenames></author></authors><title>Combining Independent Modules to Solve Multiple-choice Synonym and
  Analogy Problems</title><categories>cs.CL cs.IR cs.LG</categories><comments>8 pages, related work available at
  http://www.cs.rutgers.edu/~mlittman/ and http://purl.org/peter.turney/</comments><report-no>NRC-46506</report-no><acm-class>I.2.6; I.2.7; H.3.1; J.5</acm-class><journal-ref>Proceedings of the International Conference on Recent Advances in
  Natural Language Processing (RANLP-03), (2003), Borovets, Bulgaria, 482-489</journal-ref><abstract>  Existing statistical approaches to natural language problems are very coarse
approximations to the true complexity of language processing. As such, no
single technique will be best for all problem instances. Many researchers are
examining ensemble methods that combine the output of successful, separately
developed modules to create more accurate solutions. This paper examines three
merging rules for combining probability distributions: the well known mixture
rule, the logarithmic rule, and a novel product rule. These rules were applied
with state-of-the-art results to two problems commonly used to assess human
mastery of lexical semantics -- synonym questions and analogy questions. All
three merging rules result in ensembles that are more accurate than any of
their component modules. The differences among the three rules are not
statistically significant, but it is suggestive that the popular mixture rule
is not the best rule for either of the two problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309036</id><created>2003-09-21</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>A Neural Network Assembly Memory Model Based on an Optimal Binary Signal
  Detection Theory</title><categories>cs.AI cs.IR cs.NE q-bio.NC q-bio.QM</categories><comments>The revised contribution submitted to the 1st International Workshop
  on the Future of Neural Networks (FUNN 2003) held on July 5, 2003, Eindhoven,
  the Netherlands (Workshop affiliated to ICALP 2003, June 30 - July 4, 2003),
  12 pages, 3 Figures, 23 references</comments><acm-class>I.2; E.4; J.3; J.4</acm-class><journal-ref>Problemy Programmirovaniya (Programming Problems, Kyiv, Ukraine),
  2004, no. 2-3, pp. 473-479.</journal-ref><abstract>  A ternary/binary data coding algorithm and conditions under which Hopfield
networks implement optimal convolutional or Hamming decoding algorithms has
been described. Using the coding/decoding approach (an optimal Binary Signal
Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model
(NNAMM) is built. The model provides optimal (the best) basic memory
performance and demands the use of a new memory unit architecture with
two-layer Hopfield network, N-channel time gate, auxiliary reference memory,
and two nested feedback loops. NNAMM explicitly describes the dependence on
time of a memory trace retrieval, gives a possibility of metamemory simulation,
generalized knowledge representation, and distinct description of conscious and
unconscious mental processes. A model of smallest inseparable part or an &quot;atom&quot;
of consciousness is also defined. The NNAMM's neurobiological backgrounds and
its applications to solving some interdisciplinary problems are shortly
discussed. BSDT could implement the &quot;best neural code&quot; used in nervous tissues
of animals and humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309037</id><created>2003-09-21</created><authors><author><keyname>Cantrill</keyname><forenames>Bryan M.</forenames></author></authors><title>Postmortem Object Type Identification</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  This paper presents a novel technique for the automatic type identification
of arbitrary memory objects from a memory dump. Our motivating application is
debugging memory corruption problems in optimized, production systems -- a
problem domain largely unserved by extant methodologies. We describe our
algorithm as applicable to any typed language, and we discuss it with respect
to the formidable obstacles posed by C. We describe the heuristics that we have
developed to overcome these difficulties and achieve effective type
identification on C-based systems. We further describe the implementation of
our heuristics on one C-based system -- the Solaris operating system kernel --
and describe the extensions that we have added to the Solaris postmortem
debugger to allow for postmortem type identification. We show that our
implementation yields a sufficiently high rate of type identification to be
useful for debugging memory corruption problems. Finally, we discuss some of
the novel automated debugging mechanisms that can be layered upon postmortem
type identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309038</id><created>2003-09-22</created><authors><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author><author><keyname>Campos</keyname><forenames>L. C. D.</forenames></author></authors><title>A novel evolutionary formulation of the maximum independent set problem</title><categories>cs.NE</categories><report-no>ES-615/03</report-no><acm-class>F.2.2; I.2.8</acm-class><journal-ref>Journal of Combinatorial Optimization 8 (2004), 419-437</journal-ref><doi>10.1007/s10878-004-4835-9</doi><abstract>  We introduce a novel evolutionary formulation of the problem of finding a
maximum independent set of a graph. The new formulation is based on the
relationship that exists between a graph's independence number and its acyclic
orientations. It views such orientations as individuals and evolves them with
the aid of evolutionary operators that are very heavily based on the structure
of the graph and its acyclic orientations. The resulting heuristic has been
tested on some of the Second DIMACS Implementation Challenge benchmark graphs,
and has been found to be competitive when compared to several of the other
heuristics that have also been tested on those graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309039</id><created>2003-09-22</created><authors><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author><author><keyname>Assis</keyname><forenames>C. A. G.</forenames></author><author><keyname>Nascimento</keyname><forenames>J. O. do</forenames></author></authors><title>Two novel evolutionary formulations of the graph coloring problem</title><categories>cs.NE</categories><comments>To appear in Journal of Combinatorial Optimization</comments><report-no>ES-553/01</report-no><acm-class>F.2.2; I.2.8</acm-class><journal-ref>Journal of Combinatorial Optimization 8 (2004), 41-63</journal-ref><doi>10.1023/B:JOCO.0000021937.26468.b2</doi><abstract>  We introduce two novel evolutionary formulations of the problem of coloring
the nodes of a graph. The first formulation is based on the relationship that
exists between a graph's chromatic number and its acyclic orientations. It
views such orientations as individuals and evolves them with the aid of
evolutionary operators that are very heavily based on the structure of the
graph and its acyclic orientations. The second formulation, unlike the first
one, does not tackle one graph at a time, but rather aims at evolving a
`program' to color all graphs belonging to a class whose members all have the
same number of nodes and other common attributes. The heuristics that result
from these formulations have been tested on some of the Second DIMACS
Implementation Challenge benchmark graphs, and have been found to be
competitive when compared to the several other heuristics that have also been
tested on those graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309040</id><created>2003-09-22</created><authors><author><keyname>Penso</keyname><forenames>L. D.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>A distributed algorithm to find k-dominating sets</title><categories>cs.DC</categories><comments>To appear in Discrete Applied Mathematics</comments><report-no>ES-552/01</report-no><acm-class>F.1.2; F.2.2</acm-class><journal-ref>Discrete Applied Mathematics 141 (2004), 243-253</journal-ref><doi>10.1016/S0166-218X(03)00368-8</doi><abstract>  We consider a connected undirected graph $G(n,m)$ with $n$ nodes and $m$
edges. A $k$-dominating set $D$ in $G$ is a set of nodes having the property
that every node in $G$ is at most $k$ edges away from at least one node in $D$.
Finding a $k$-dominating set of minimum size is NP-hard. We give a new
synchronous distributed algorithm to find a $k$-dominating set in $G$ of size
no greater than $\lfloor n/(k+1)\rfloor$. Our algorithm requires $O(k\log^*n)$
time and $O(m\log k+n\log k\log^*n)$ messages to run. It has the same time
complexity as the best currently known algorithm, but improves on that
algorithm's message complexity and is, in addition, conceptually simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309041</id><created>2003-09-23</created><updated>2003-11-24</updated><authors><author><keyname>Rybnikov</keyname><forenames>Konstantin</forenames></author></authors><title>Fast Verification of Convexity of Piecewise-linear Surfaces</title><categories>cs.CG cs.CV</categories><comments>10 pages (abbreviated version). Significantly different from all
  older versions. Discount the previous version -- it had many omissions and
  typos, like the following one: everything works starting from dimension n=3,
  not n=2 as was printed in the old abstract. Hyperbolic and spherical cases
  have been substantially rewritten and errors fixed. This preprint is close to
  a similar preprint on the MATH part of arxiv.org</comments><acm-class>Primary: G.4, F.2.2; Secondary: 1.4.7, 1.4.8</acm-class><abstract>  We show that a realization of a closed connected PL-manifold of dimension n-1
in n-dimensional Euclidean space (n&gt;2) is the boundary of a convex polyhedron
(finite or infinite) if and only if the interior of each (n-3)-face has a
point, which has a neighborhood lying on the boundary of an n-dimensional
convex body. No initial assumptions about the topology or orientability of the
input surface are made. The theorem is derived from a refinement and
generalization of Van Heijenoort's theorem on locally convex manifolds to
spherical spaces. Our convexity criterion for PL-manifolds implies an easy
polynomial-time algorithm for checking convexity of a given PL-surface in
n-dimensional Euclidean or spherical space, n&gt;2. The algorithm is worst case
optimal with respect to both the number of operations and the algebraic degree.
The algorithm works under significantly weaker assumptions and is easier to
implement than convexity verification algorithms suggested by Mehlhorn et al
(1996-1999), and Devillers et al.(1998). A paradigm of approximate convexity is
suggested and a simplified algorithm of smaller degree and complexity is
suggested for approximate floating point convexity verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309042</id><created>2003-09-23</created><authors><author><keyname>Drummond</keyname><forenames>L. M. A.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>On reducing the complexity of matrix clocks</title><categories>cs.DC</categories><acm-class>C.2.4; D.1.3</acm-class><journal-ref>Parallel Computing 29 (2003), 895-905</journal-ref><doi>10.1016/S0167-8191(03)00066-8</doi><abstract>  Matrix clocks are a generalization of the notion of vector clocks that allows
the local representation of causal precedence to reach into an asynchronous
distributed computation's past with depth $x$, where $x\ge 1$ is an integer.
Maintaining matrix clocks correctly in a system of $n$ nodes requires that
everymessage be accompanied by $O(n^x)$ numbers, which reflects an exponential
dependency of the complexity of matrix clocks upon the desired depth $x$. We
introduce a novel type of matrix clock, one that requires only $nx$ numbers to
be attached to each message while maintaining what for many applications may be
the most significant portion of the information that the original matrix clock
carries. In order to illustrate the new clock's applicability, we demonstrate
its use in the monitoring of certain resource-sharing computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309043</id><created>2003-09-23</created><authors><author><keyname>Porto</keyname><forenames>A. H. L.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>Finding approximate palindromes in strings</title><categories>cs.DS</categories><acm-class>F.2.2; I.2.8</acm-class><journal-ref>Pattern Recognition 35 (2002), 2581-2591</journal-ref><doi>10.1016/S0031-3203(01)00179-0</doi><abstract>  We introduce a novel definition of approximate palindromes in strings, and
provide an algorithm to find all maximal approximate palindromes in a string
with up to $k$ errors. Our definition is based on the usual edit operations of
approximate pattern matching, and the algorithm we give, for a string of size
$n$ on a fixed alphabet, runs in $O(k^2 n)$ time. We also discuss two
implementation-related improvements to the algorithm, and demonstrate their
efficacy in practice by means of both experiments and an average-case analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309044</id><created>2003-09-23</created><authors><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>The combinatorics of resource sharing</title><categories>cs.OS cs.DC</categories><comments>R. Correa et alii (eds.), Models for Parallel and Distributed
  Computation, pp. 27-52. Kluwer Academic Publishers, Dordrecht, The
  Netherlands, 2002</comments><acm-class>D.1.3; D.4.1</acm-class><abstract>  We discuss general models of resource-sharing computations, with emphasis on
the combinatorial structures and concepts that underlie the various deadlock
models that have been proposed, the design of algorithms and deadlock-handling
policies, and concurrency issues. These structures are mostly graph-theoretic
in nature, or partially ordered sets for the establishment of priorities among
processes and acquisition orders on resources. We also discuss graph-coloring
concepts as they relate to resource sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309045</id><created>2003-09-24</created><authors><author><keyname>Dovier</keyname><forenames>Agostino</forenames></author><author><keyname>Piazza</keyname><forenames>Carla</forenames></author><author><keyname>Rossi</keyname><forenames>Gianfranco</forenames></author></authors><title>A uniform approach to constraint-solving for lists, multisets, compact
  lists, and sets</title><categories>cs.PL cs.LO cs.SC</categories><comments>27 pages, 11 figures</comments><report-no>&quot;Quaderni del Dipartimento di Matematica&quot;, 235</report-no><acm-class>D.3.3; F.4.1; F.2.2; I.1.2; I.2.3</acm-class><abstract>  Lists, multisets, and sets are well-known data structures whose usefulness is
widely recognized in various areas of Computer Science. These data structures
have been analyzed from an axiomatic point of view with a parametric approach
in (*) where the relevant unification algorithms have been developed. In this
paper we extend these results considering more general constraints including
not only equality but also membership constraints as well as their negative
counterparts.
  (*) A. Dovier, A. Policriti, and G. Rossi. A uniform axiomatic view of lists,
multisets, and sets, and the relevant unification algorithms. Fundamenta
Informaticae, 36(2/3):201--234, 1998.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309046</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309046</id><created>2003-09-24</created><authors><author><keyname>Vezerides</keyname><forenames>K.</forenames></author><author><keyname>Kehagias</keyname><forenames>Ath.</forenames></author></authors><title>The Liar and Related Paradoxes: Fuzzy Truth Value Assignment for
  Collections of Self-Referential Sentences</title><categories>cs.LO</categories><acm-class>F.4.1; G.1.5; I.2.3</acm-class><abstract>  We study self-referential sentences of the type related to the Liar paradox.
In particular, we consider the problem of assigning consistent fuzzy truth
values to collections of self-referential sentences. We show that the problem
can be reduced to the solution of a system of nonlinear equations. Furthermore,
we prove that, under mild conditions, such a system always has a solution (i.e.
a consistent truth value assignment) and that, for a particular implementation
of logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution is
always consistent. Next we turn to computational issues and present several
truth-value assignment algorithms; we argue that these algorithms can be
understood as generalized sequential reasoning. In an Appendix we present a
large number of examples of self-referential collections (including the Liar
and the Strengthened Liar), we formulate the corresponding truth value
equations and solve them analytically and/ or numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309047</id><created>2003-09-24</created><authors><author><keyname>Zeller</keyname><forenames>Andreas</forenames></author></authors><title>Causes and Effects in Computer Programs</title><categories>cs.SE</categories><acm-class>D.2.5</acm-class><abstract>  Debugging is commonly understood as finding and fixing the cause of a
problem. But what does ``cause'' mean? How can we find causes? How can we prove
that a cause is a cause--or even ``the'' cause? This paper defines common terms
in debugging, highlights the principal techniques, their capabilities and
limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309048</id><created>2003-09-25</created><updated>2006-12-17</updated><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Goedel Machines: Self-Referential Universal Problem Solvers Making
  Provably Optimal Self-Improvements</title><categories>cs.LO cs.AI</categories><comments>29 pages, 1 figure, minor improvements, updated references</comments><report-no>IDSIA-19-03</report-no><acm-class>F.4.1</acm-class><journal-ref>Variants published in &quot;Adaptive Agents and Multi-Agent Systems
  II&quot;, LNCS 3394, p. 1-23, Springer, 2005: ISBN 978-3-540-25260-3; as well as
  in Proc. ICANN 2005, LNCS 3697, p. 223-233, Springer, 2005 (plenary talk); as
  well as in &quot;Artificial General Intelligence&quot;, Series: Cognitive Technologies,
  Springer, 2006: ISBN-13: 978-3-540-23733-4</journal-ref><abstract>  We present the first class of mathematically rigorous, general, fully
self-referential, self-improving, optimally efficient problem solvers. Inspired
by Kurt Goedel's celebrated self-referential formulas (1931), such a problem
solver rewrites any part of its own code as soon as it has found a proof that
the rewrite is useful, where the problem-dependent utility function and the
hardware and the entire initial code are described by axioms encoded in an
initial proof searcher which is also part of the initial code. The searcher
systematically and efficiently tests computable proof techniques (programs
whose outputs are proofs) until it finds a provably useful, computable
self-rewrite. We show that such a self-rewrite is globally optimal - no local
maxima! - since the code first had to prove that it is not useful to continue
the proof search for alternative self-rewrites. Unlike previous
non-self-referential methods based on hardwired proof searchers, ours not only
boasts an optimal order of complexity but can optimally reduce any slowdowns
hidden by the O()-notation, provided the utility of such speed-ups is provable
at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309049</id><created>2003-09-26</created><authors><author><keyname>Lourenco</keyname><forenames>Joao</forenames></author><author><keyname>Cunha</keyname><forenames>Jose C.</forenames></author><author><keyname>Moreira</keyname><forenames>Vitor</forenames></author></authors><title>Control and Debugging of Distributed Programs Using Fiddle</title><categories>cs.DC</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  The main goal of Fiddle, a distributed debugging engine, is to provide a
flexible platform for developing debugging tools. Fiddle provides a layered set
of interfaces with a minimal set of debugging functionalities, for the
inspection and control of distributed and multi-threaded applications.
  This paper illustrates how Fiddle is used to support integrated testing and
debugging. The approach described is based on a tool, called Deipa, that
interprets sequences of commands read from an input file, generated by an
independent testing tool. Deipa acts as a Fiddle client, in order to enforce
specific execution paths in a distributed PVM program. Other Fiddle clients may
be used along with Deipa for the fine debugging at process level. Fiddle and
Deipa functionalities and architectures are described, and a working example
shows a step-by-step application of these tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309050</id><created>2003-09-26</created><authors><author><keyname>Zuniga-Galindo</keyname><forenames>W. A.</forenames></author></authors><title>Computing Igusa's Local Zeta Functions of Univariate Polynomials, and
  Linear Feedback Shift Registers</title><categories>cs.SC cs.CR</categories><comments>To appear in The Journal of Integer Sequences</comments><acm-class>I.1.2</acm-class><abstract>  We give a polynomial time algorithm for computing the Igusa local zeta
function $Z(s,f)$ attached to a polynomial $f(x)\in \QTR{Bbb}{Z}[x]$, in one
variable, with splitting field $\QTR{Bbb}{Q}$, and a prime number $p$. We also
propose a new class of Linear Feedback Shift Registers based on the computation
of Igusa's local zeta function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309051</id><created>2003-09-28</created><authors><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>New Lattice Based Cryptographic Constructions</title><categories>cs.CR</categories><acm-class>E.3</acm-class><abstract>  We introduce the use of Fourier analysis on lattices as an integral part of a
lattice based construction. The tools we develop provide an elegant description
of certain Gaussian distributions around lattice points. Our results include
two cryptographic constructions which are based on the worst-case hardness of
the unique shortest vector problem. The main result is a new public key
cryptosystem whose security guarantee is considerably stronger than previous
results ($O(n^{1.5})$ instead of $O(n^7)$). This provides the first alternative
to Ajtai and Dwork's original 1996 cryptosystem. Our second result is a family
of collision resistant hash functions which, apart from improving the security
in terms of the unique shortest vector problem, is also the first example of an
analysis which is not based on Ajtai's iterative step. Surprisingly, both
results are derived from one theorem which presents two indistinguishable
distributions on the segment $[0,1)$. It seems that this theorem can have
further applications and as an example we mention how it can be used to solve
an open problem related to quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309052</identifier>
 <datestamp>2007-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309052</id><created>2003-09-29</created><authors><author><keyname>Alexeev</keyname><forenames>Boris</forenames></author></authors><title>Minimal DFAs for Testing Divisibility</title><categories>cs.CC</categories><comments>LaTeX, 7 pages (corrected typo in new version)</comments><acm-class>F.1.1; F.4.3</acm-class><journal-ref>J. Comput. System Sci. 69 (2004), no. 2, 235--243</journal-ref><doi>10.1016/j.jcss.2004.02.001</doi><abstract>  We present and prove a theorem answering the question &quot;how many states does a
minimal deterministic finite automaton (DFA) that recognizes the set of base-b
numbers divisible by k have?&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309053</id><created>2003-09-29</created><authors><author><keyname>Plaisted</keyname><forenames>David A.</forenames></author></authors><title>A Hierarchical Situation Calculus</title><categories>cs.AI cs.LO</categories><comments>Internal report, University of North Carolina at Chapel Hill, January
  2003</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  A situation calculus is presented that provides a solution to the frame
problem for hierarchical situations, that is, situations that have a modular
structure in which parts of the situation behave in a relatively independent
manner. This situation calculus is given in a relational, functional, and modal
logic form. Each form permits both a single level hierarchy or a multiple level
hierarchy, giving six versions of the formalism in all, and a number of
sub-versions of these. For multiple level hierarchies, it is possible to give
equations between parts of the situation to impose additional structure on the
problem. This approach is compared to others in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309054</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309054</id><created>2003-09-29</created><authors><author><keyname>Argyraki</keyname><forenames>Katerina J.</forenames></author><author><keyname>Cheriton</keyname><forenames>David R.</forenames></author></authors><title>Active Internet Traffic Filtering: Real-time Response to Denial of
  Service Attacks</title><categories>cs.NI</categories><comments>Briefly describes the core ideas of AITF, a protocol for facing
  Denial of Service Attacks. 6 pages long</comments><acm-class>C.2.2</acm-class><journal-ref>Updated versions in Proc. USENIX Annual Technical Conference,
  April 2005, and IEEE/ACM Transactions on Networking, 17(4):1284-1297, August
  2009</journal-ref><doi>10.1109/TNET.2008.2007431</doi><abstract>  Denial of Service (DoS) attacks are one of the most challenging threats to
Internet security. An attacker typically compromises a large number of
vulnerable hosts and uses them to flood the victim's site with malicious
traffic, clogging its tail circuit and interfering with normal traffic. At
present, the network operator of a site under attack has no other resolution
but to respond manually by inserting filters in the appropriate edge routers to
drop attack traffic. However, as DoS attacks become increasingly sophisticated,
manual filter propagation becomes unacceptably slow or even infeasible.
  In this paper, we present Active Internet Traffic Filtering, a new automatic
filter propagation protocol. We argue that this system provides a guaranteed,
significant level of protection against DoS attacks in exchange for a
reasonable, bounded amount of router resources. We also argue that the proposed
system cannot be abused by a malicious node to interfere with normal Internet
operation. Finally, we argue that it retains its efficiency in the face of
continued Internet growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0309055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0309055</id><created>2003-09-29</created><authors><author><keyname>Ohta</keyname><forenames>Tsuyoshi</forenames><affiliation>Shizuoka University</affiliation></author><author><keyname>Mizuno</keyname><forenames>Tadanori</forenames><affiliation>Shizuoka University</affiliation></author></authors><title>A mathematical framework for automated bug localization</title><categories>cs.SE</categories><comments>4 pages, 2 figures, In M. Ronsse, K. De Bosschere (eds), proceedings
  of the Fifth International Workshop on Automated Debugging (AADEBUG 2003),
  September 2003, Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  In this paper, we propose a mathematical framework for automated bug
localization. This framework can be briefly summarized as follows. A program
execution can be represented as a rooted acyclic directed graph. We define an
execution snapshot by a cut-set on the graph. A program state can be regarded
as a conjunction of labels on edges in a cut-set. Then we argue that a
debugging task is a pruning process of the execution graph by using cut-sets. A
pruning algorithm, i.e., a debugging task, is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310001</id><created>2003-10-03</created><authors><author><keyname>Metz</keyname><forenames>Edu</forenames></author><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author></authors><title>A Performance Analysis Tool for Nokia Mobile Phone Software</title><categories>cs.SE cs.PF</categories><comments>4 pages, 5 figures. In M. Ronsse, K. De Bosschere (eds), Proceedings
  of the Fifth International Workshop on Automated Debugging (AADEBUG 2003),
  September 2003, Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Performance problems are often observed in embedded software systems. The
reasons for poor performance are frequently not obvious. Bottlenecks can occur
in any of the software components along the execution path. Therefore it is
important to instrument and monitor the different components contributing to
the runtime behavior of an embedded software system. Performance analysis tools
can help locate performance bottlenecks in embedded software systems by
monitoring the software's execution and producing easily understandable
performance data. We maintain and further develop a tool for analyzing the
performance of Nokia mobile phone software. The user can select among four
performance analysis reports to be generated: average processor load, processor
utilization, task execution time statistics, and task execution timeline. Each
of these reports provides important information about where execution time is
being spent. The demo will show how the tool helps to identify performance
bottlenecks in Nokia mobile phone software and better understand areas of poor
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310002</id><created>2003-10-05</created><updated>2003-10-07</updated><authors><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>The Graphics Card as a Streaming Computer</title><categories>cs.GR cs.AR</categories><comments>2 pages: corrected missing bibliographic references</comments><acm-class>C.1.2;F.1.1;I.3.1</acm-class><journal-ref>In SIGMOD Workshop on Management and Processing of Massive Data
  (June 2003)</journal-ref><abstract>  Massive data sets have radically changed our understanding of how to design
efficient algorithms; the streaming paradigm, whether it in terms of number of
passes of an external memory algorithm, or the single pass and limited memory
of a stream algorithm, appears to be the dominant method for coping with large
data.
  A very different kind of massive computation has had the same effect at the
level of the CPU. The most prominent example is that of the computations
performed by a graphics card. The operations themselves are very simple, and
require very little memory, but require the ability to perform many
computations extremely fast and in parallel to whatever degree possible. What
has resulted is a stream processor that is highly optimized for stream
computations. An intriguing side effect of this is the growing use of a
graphics card as a general purpose stream processing engine. In an
ever-increasing array of applications, researchers are discovering that
performing a computation on a graphics card is far faster than performing it on
a CPU, and so are using a GPU as a stream co-processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310003</id><created>2003-10-05</created><authors><author><keyname>Goldstein</keyname><forenames>Darin</forenames></author><author><keyname>Meyer</keyname><forenames>Nick</forenames></author></authors><title>The Wake Up and Report Problem is Time-Equivalent to the Firing Squad
  Synchronization Problem</title><categories>cs.DC cs.DS</categories><comments>13 pages, 4 figures, in Symposium on Discrete Algorithms, (SODA) pp.
  578-587, January 6-8, 2002 (journal version to appear in the Journal of
  Distributed Computing)</comments><acm-class>E.1; C.2.1; C.2.2</acm-class><abstract>  We consider several problems relating to strongly-connected directed networks
of identical finite-state processors that work synchronously in discrete time
steps. The conceptually simplest of these is the Wake Up and Report Problem;
this is the problem of having a unique &quot;root&quot; processor send a signal to all
other processors in the network and then enter a special &quot;done&quot; state only when
all other processors have received the signal. The most difficult of the
problems we consider is the classic Firing Squad Synchronization Problem; this
is the much-studied problem of achieving macro-synchronization in a network
given micro-synchronization. We show via a complex algorithmic application of
the &quot;snake&quot; data structure first introduced in Even, Litman, and Winkler [ELW],
that these two problems are asymptotically time-equivalent up to a constant
factor. This result leads immediately to the inclusion of several other related
problems into this new asymptotic time-class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310004</id><created>2003-10-05</created><authors><author><keyname>Goldstein</keyname><forenames>Darin</forenames></author></authors><title>Determination of the Topology of a Directed Network</title><categories>cs.DC cs.DS</categories><comments>9 pages, no figures, accepted to appear in IPDPS 2002 (unable to
  attend), (journal version to appear in Information Processing Letters)</comments><acm-class>C.2.1; C.2.2; E.1</acm-class><abstract>  We consider strongly-connected directed networks of identical synchronous,
finite-state processors with in- and out-degree uniformly bounded by a network
constant. Via a straightforward extension of Ostrovsky and Wilkerson's
Backwards Communication Algorithm in [OW], we exhibit a protocol which solves
the Global Topology Determination Problem, the problem of having the root
processor map the global topology of a network of unknown size and topology,
with running time O(ND) where N represents the number of processors and D
represents the diameter of the network. A simple counting argument suffices to
show that the Global Topology Determination Problem has time-complexity Omega(N
logN) which makes the protocol presented asymptotically time-optimal for many
large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310005</id><created>2003-10-05</created><authors><author><keyname>Goldstein</keyname><forenames>Darin</forenames></author><author><keyname>Murray</keyname><forenames>William</forenames></author><author><keyname>Yang</keyname><forenames>Binh</forenames></author></authors><title>Using Artificial Intelligence for Model Selection</title><categories>cs.AI q-bio.QM</categories><comments>10 pages, no figures, in Proceedings, Hawaii International Conference
  on Statistics and Related Fields, June 5-8, 2003</comments><acm-class>H.2.8; J.3</acm-class><abstract>  We apply the optimization algorithm Adaptive Simulated Annealing (ASA) to the
problem of analyzing data on a large population and selecting the best model to
predict that an individual with various traits will have a particular disease.
We compare ASA with traditional forward and backward regression on computer
simulated data. We find that the traditional methods of modeling are better for
smaller data sets whereas a numerically stable ASA seems to perform better on
larger and more complicated data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310006</id><created>2003-10-06</created><authors><author><keyname>Abiteboul</keyname><forenames>Serge</forenames></author><author><keyname>Agrawal</keyname><forenames>Rakesh</forenames></author><author><keyname>Bernstein</keyname><forenames>Phil</forenames></author><author><keyname>Carey</keyname><forenames>Mike</forenames></author><author><keyname>Ceri</keyname><forenames>Stefano</forenames></author><author><keyname>Croft</keyname><forenames>Bruce</forenames></author><author><keyname>DeWitt</keyname><forenames>David</forenames></author><author><keyname>Franklin</keyname><forenames>Mike</forenames></author><author><keyname>Molina</keyname><forenames>Hector Garcia</forenames></author><author><keyname>Gawlick</keyname><forenames>Dieter</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Haas</keyname><forenames>Laura</forenames></author><author><keyname>Halevy</keyname><forenames>Alon</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joe</forenames></author><author><keyname>Ioannidis</keyname><forenames>Yannis</forenames></author><author><keyname>Kersten</keyname><forenames>Martin</forenames></author><author><keyname>Pazzani</keyname><forenames>Michael</forenames></author><author><keyname>Lesk</keyname><forenames>Mike</forenames></author><author><keyname>Maier</keyname><forenames>David</forenames></author><author><keyname>Naughton</keyname><forenames>Jeff</forenames></author><author><keyname>Schek</keyname><forenames>Hans</forenames></author><author><keyname>Sellis</keyname><forenames>Timos</forenames></author><author><keyname>Silberschatz</keyname><forenames>Avi</forenames></author><author><keyname>Stonebraker</keyname><forenames>Mike</forenames></author><author><keyname>Snodgrass</keyname><forenames>Rick</forenames></author><author><keyname>Ullman</keyname><forenames>Jeff</forenames></author><author><keyname>Weikum</keyname><forenames>Gerhard</forenames></author><author><keyname>Widom</keyname><forenames>Jennifer</forenames></author><author><keyname>Zdonik</keyname><forenames>Stan</forenames></author></authors><title>The Lowell Database Research Self Assessment</title><categories>cs.DB</categories><comments>Details of this workshop (presentations and notes) are at
  http://research.microsoft.com/~gray/lowell/</comments><acm-class>H;H.2; H.3; H.4; H.5</acm-class><abstract>  A group of senior database researchers gathers every few years to assess the
state of database research and to point out problem areas that deserve
additional focus. This report summarizes the discussion and conclusions of the
sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that
information management continues to be a critical component of most complex
software systems. It recommends that database researchers increase focus on:
integration of text, data, code, and streams; fusion of information from
heterogeneous data sources; reasoning about uncertain data; unsupervised data
mining for interesting correlations; information privacy; and self-adaptation
and repair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310007</id><created>2003-10-06</created><authors><author><keyname>Schaubschlaeger</keyname><forenames>Ch.</forenames></author><author><keyname>Kranzlmueller</keyname><forenames>D.</forenames></author><author><keyname>Volkert</keyname><forenames>J.</forenames></author></authors><title>Event-based Program Analysis with DeWiz</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Due to the increased complexity of parallel and distributed programs,
debugging of them is considered to be the most difficult and time consuming
part of the software lifecycle. Tool support is hence a crucial necessity to
hide complexity from the user. However, most existing tools seem inadequate as
soon as the program under consideration exploits more than a few processors
over a long execution time. This problem is addressed by the novel debugging
tool DeWiz (Debugging Wizard), whose focus lies on scalability. DeWiz has a
modular, scalable architecture, and uses the event graph model as a
representation of the investigated program. DeWiz provides a set of modules,
which can be combined to generate, analyze, and visualize event graph data.
Within this processing pipeline the toolset tries to extract useful
information, which is presented to the user at an arbitrary level of
abstraction. Additionally, DeWiz is a framework, which can be used to easily
implement arbitrary user-defined modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310008</id><created>2003-10-06</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Poster on MPI application in Computational Fluid Dynamics</title><categories>cs.DC cs.GR</categories><comments>1 page, PDF version of a poster-session presentation during
  &quot;EuroPVM/MPI 2003&quot;, Sep. 29 - Oct. 2, Venice (Italy), please visit
  http://www.dsi.unive.it/pvmmpi03</comments><acm-class>D.1.3</acm-class><abstract>  Poster-presentation of the paper &quot;Message Passing Fluids: molecules as
processes in parallel computational fluids&quot; held at &quot;EURO PVMMPI 2003&quot;
Congress; the paper is on the proceedings &quot;Recent Advances in Parallel Virtual
Machine and Message Passing Interface&quot;, 10th European PVM/MPI User's Group
Meeting, LNCS 2840, Springer-Verlag, Dongarra-Laforenza-Orlando editors, pp.
550-554.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310009</id><created>2003-10-06</created><updated>2003-11-04</updated><authors><author><keyname>Rataj</keyname><forenames>Artur</forenames></author></authors><title>On Interference of Signals and Generalization in Feedforward Neural
  Networks</title><categories>cs.NE</categories><comments>6 pages, 3 figures. Some changes in text to make it more concise</comments><report-no>IITiS-2002-08-1-1.04</report-no><acm-class>I.2.6</acm-class><abstract>  This paper studies how the generalization ability of neurons can be affected
by mutual processing of different signals. This study is done on the basis of a
feedforward artificial neural network. The mutual processing of signals can
possibly be a good model of patterns in a set generalized by a neural network
and in effect may improve generalization. In this paper it is discussed that
the interference may also cause a highly random generalization. Adaptive
activation functions are discussed as a way of reducing that type of
generalization. A test of a feedforward neural network is performed that shows
the discussed random generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310010</id><created>2003-10-06</created><authors><author><keyname>Lyback</keyname><forenames>David</forenames></author></authors><title>Transient Diversity in Multi-Agent Systems</title><categories>cs.AI cs.MA</categories><comments>44 pages, 5 figures</comments><report-no>M.Sc. thesis No. 99-x-097. Dept. of Computer and Systems Sciences,
  Royal Inst. of Technology, Sweden</report-no><acm-class>I.2.11</acm-class><abstract>  Diversity is an important aspect of highly efficient multi-agent teams. We
introduce the main factors that drive a multi-agent system in either direction
along the diversity scale. A metric for diversity is described, and we
speculate on the concept of transient diversity. Finally, an experiment on
social entropy using a RoboCup simulated soccer team is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310011</id><created>2003-10-06</created><authors><author><keyname>Capra</keyname><forenames>Robert G.</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author></authors><title>Re-Finding Found Things: An Exploratory Study of How Users Re-Find
  Information</title><categories>cs.HC cs.IR</categories><acm-class>H.1.2; H.3.3; H.5.2</acm-class><abstract>  The problem of how people find information is studied extensively; however,
the problem of how people organize, re-use, and re-find information that they
have found is not as well understood. Recently, several projects have conducted
in-situ studies to explore how people re-find and re-use information. Here, we
present results and observations from a controlled, laboratory study of
refinding information found on the web.
  Our study was conducted as a collaborative exercise with pairs of
participants. One participant acted as a retriever, helping the other
participant re-find information by telephone. This design allowed us to gain
insight into the strategies that users employed to re-find information, and
into how domain artifacts and contextual information were used to aid the
re-finding process. We also introduced the ability for users to add their own
explicitly artifacts in the form of making annotations on the web pages they
viewed.
  We observe that re-finding often occurs as a two stage, iterative process in
which users first attempt to locate an information source (search), and once
found, begin a process to find the specific information being sought (browse).
Our findings are consistent with research on waypoints; orienteering approaches
to re-finding; and navigation of electronic spaces. Furthermore, we observed
that annotations were utilized extensively, indicating that explicitly added
context by the user can play an important role in re-finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310012</id><created>2003-10-07</created><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>A Formal Comparison of Visual Web Wrapper Generators</title><categories>cs.DB</categories><comments>12 pages, 0 figures, second part of long version of PODS 2002 paper
  &quot;Monadic Datalog and the Expressive Power of Languages for Web Information
  Extraction&quot;. First part accepted for JACM</comments><acm-class>F.1.1, F.4.1, F.4.3, H.2.3, I.7.2</acm-class><abstract>  We study the core fragment of the Elog wrapping language used in the Lixto
system (a visual wrapper generator) and formally compare Elog to other wrapping
languages proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310013</id><created>2003-10-08</created><authors><author><keyname>Bagnoli</keyname><forenames>Franco</forenames></author><author><keyname>Franci</keyname><forenames>Fabio</forenames></author><author><keyname>Mugelli</keyname><forenames>Francesco</forenames></author><author><keyname>Sterbini</keyname><forenames>Andrea</forenames></author></authors><title>WebTeach in practice: the entrance test to the Engineering faculty in
  Florence</title><categories>cs.HC cs.IR</categories><comments>6 pages, 5 figures</comments><acm-class>K.3.1</acm-class><abstract>  We present the WebTeach project, formed by a web interface to database for
test management, a wiki site for the diffusion of teaching material and student
forums, and a suite for the generation of multiple-choice mathematical quiz
with automatic elaboration of forms. This system has been massively tested for
the entrance test to the Engineering Faculty of the University of Florence,
Italy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310014</id><created>2003-10-08</created><authors><author><keyname>Clarke</keyname><forenames>Rodney J.</forenames></author><author><keyname>Windridge</keyname><forenames>Philip C.</forenames></author><author><keyname>Dong</keyname><forenames>Dali</forenames></author></authors><title>Effective XML Representation for Spoken Language in Organisations</title><categories>cs.CL</categories><comments>10 pages, 3 figures</comments><acm-class>I.2.7</acm-class><abstract>  Spoken Language can be used to provide insights into organisational
processes, unfortunately transcription and coding stages are very time
consuming and expensive. The concept of partial transcription and coding is
proposed in which spoken language is indexed prior to any subsequent
processing. The functional linguistic theory of texture is used to describe the
effects of partial transcription on observational records. The standard used to
encode transcript context and metadata is called CHAT, but a previous XML
schema developed to implement it contains design assumptions that make it
difficult to support partial transcription for example. This paper describes a
more effective XML schema that overcomes many of these problems and is intended
for use in applications that support the rapid development of spoken language
deliverables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310015</id><created>2003-10-09</created><authors><author><keyname>Okita</keyname><forenames>Masao</forenames></author><author><keyname>Ino</keyname><forenames>Fumihiko</forenames></author><author><keyname>Hagihara</keyname><forenames>Kenichi</forenames></author></authors><title>Debugging Tool for Localizing Faulty Processes in Message Passing
  Programs</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  In message passing programs, once a process terminates with an unexpected
error, the terminated process can propagate the error to the rest of processes
through communication dependencies, resulting in a program failure. Therefore,
to locate faults, developers must identify the group of processes involved in
the original error and faulty processes that activate faults. This paper
presents a novel debugging tool, named MPI-PreDebugger (MPI-PD), for localizing
faulty processes in message passing programs. MPI-PD automatically
distinguishes the original and the propagated errors by checking communication
errors during program execution. If MPI-PD observes any communication errors,
it backtraces communication dependencies and points out potential faulty
processes in a timeline view. We also introduce three case studies, in which
MPI-PD has been shown to play the key role in their debugging. From these
studies, we believe that MPI-PD helps developers to locate faults and allows
them to concentrate in correcting their programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310016</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310016</id><created>2003-10-09</created><authors><author><keyname>Lewis</keyname><forenames>Bil</forenames></author></authors><title>Debugging Backwards in Time</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  By recording every state change in the run of a program, it is possible to
present the programmer every bit of information that might be desired.
Essentially, it becomes possible to debug the program by going ``backwards in
time,'' vastly simplifying the process of debugging. An implementation of this
idea, the ``Omniscient Debugger,'' is used to demonstrate its viability and has
been used successfully on a number of large programs. Integration with an event
analysis engine for searching and control is presented. Several small-scale
user studies provide encouraging results. Finally performance issues and
implementation are discussed along with possible optimizations.
  This paper makes three contributions of interest: the concept and technique
of ``going backwards in time,'' the GUI which presents a global view of the
program state and has a formal notion of ``navigation through time,'' and the
integration with an event analyzer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310017</id><created>2003-10-09</created><authors><author><keyname>Doran</keyname><forenames>Chris</forenames></author></authors><title>Circle and sphere blending with conformal geometric algebra</title><categories>cs.CG cs.GR</categories><comments>20 pages, 13 figures</comments><acm-class>I.3.5</acm-class><abstract>  Blending schemes based on circles provide smooth `fair' interpolations
between series of points. Here we demonstrate a simple, robust set of
algorithms for performing circle blends for a range of cases. An arbitrary
level of G-continuity can be achieved by simple alterations to the underlying
parameterisation. Our method exploits the computational framework provided by
conformal geometric algebra. This employs a five-dimensional representation of
points in space, in contrast to the four-dimensional representation typically
used in projective geometry. The advantage of the conformal scheme is that
straight lines and circles are treated in a single, unified framework. As a
further illustration of the power of the conformal framework, the basic idea is
extended to the case of sphere blending to interpolate over a surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310018</id><created>2003-10-10</created><authors><author><keyname>Jia</keyname><forenames>Jiyou</forenames></author></authors><title>The Study of the Application of a Keywords-based Chatbot System on the
  Teaching of Foreign Languages</title><categories>cs.CY cs.CL</categories><comments>11 pages, 2 figures, 10 tables</comments><acm-class>K.3.1;I.2.7</acm-class><abstract>  This paper reports the findings of a study conducted on the application of an
on-line human-computer dialog system with natural language (chatbot) on the
teaching of foreign languages. A keywords-based human-computer dialog system
makes it possible that the user could chat with the computer using a natural
language, i.e. in English or in German to some extent. So an experiment has
been made using this system online to work as a chat partner with the users
learning the foreign languages. Dialogs between the users and the chatbot are
collected. Findings indicate that the dialogs between the human and the
computer are mostly very short because the user finds the responses from the
computer are mostly repeated and irrelevant with the topics and context and the
program does not understand the language at all. With analysis of the keywords
or pattern-matching mechanism used in this chatbot it can be concluded that
this kind of system can not work as a teaching assistant program in foreign
language learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310019</id><created>2003-10-10</created><authors><author><keyname>Koskas</keyname><forenames>Michel</forenames></author></authors><title>A hierarchical Algorithm to Solve the Shortest Path Problem in Valued
  Graphs</title><categories>cs.DS cs.DM</categories><comments>18 pages, 5 figures</comments><acm-class>G.2.2; G.4</acm-class><abstract>  This paper details a new algorithm to solve the shortest path problem in
valued graphs. Its complexity is $O(D \log v)$ where $D$ is the graph diameter
and $v$ its number of vertices. This complexity has to be compared to the one
of the Dijkstra's algorithm, which is $O(e\log v)$ where $e$ is the number of
edges of the graph. This new algorithm lies on a hierarchical representation of
the graph, using radix trees. The performances of this algorithm show a major
improvement over the ones of the algorithms known up to now.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310020</id><created>2003-10-10</created><authors><author><keyname>Kulas</keyname><forenames>Marija</forenames></author></authors><title>Pure Prolog Execution in 21 Rules</title><categories>cs.PL cs.LO</categories><comments>15 pages. Appeared in Proc. of the 5th Workshop on Rule-Based
  Constraint Reasoning and Programming (RCoRP'03), within CP'03, Kinsale,
  September 2003</comments><acm-class>F.3.2; F.3.1; D.2.5; I.2.3</acm-class><abstract>  A simple mathematical definition of the 4-port model for pure Prolog is
given. The model combines the intuition of ports with a compact representation
of execution state. Forward and backward derivation steps are possible. The
model satisfies a modularity claim, making it suitable for formal reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310021</id><created>2003-10-11</created><authors><author><keyname>Kohout</keyname><forenames>Ladislav J.</forenames></author><author><keyname>Kim</keyname><forenames>Eunjin</forenames></author><author><keyname>Zenz</keyname><forenames>Gary</forenames></author></authors><title>Fuzzy Relational Modeling of Cost and Affordability for Advanced
  Technology Manufacturing Environment</title><categories>cs.CE cs.AI math.OC</categories><comments>30 pages, Design &amp; Manufacturing Grantees Conference Proceedings.
  Table of contents added</comments><acm-class>J2; J1; I.2.3; I.2.4</acm-class><abstract>  Relational representation of knowledge makes it possible to perform all the
computations and decision making in a uniform relational way by means of
special relational compositions called triangle and square products. In this
paper some applications in manufacturing related to cost analysis are
described. Testing fuzzy relational structures for various relational
properties allows us to discover dependencies, hierarchies, similarities, and
equivalences of the attributes characterizing technological processes and
manufactured artifacts in their relationship to costs and performance.
  A brief overview of mathematical aspects of BK-relational products is given
in Appendix 1 together with further references in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310022</id><created>2003-10-12</created><updated>2005-11-21</updated><authors><author><keyname>Sankar</keyname><forenames>Arvind</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Smoothed Analysis of the Condition Numbers and Growth Factors of
  Matrices</title><categories>cs.NA cs.DS</categories><comments>corrected some minor mistakes</comments><acm-class>G.1.3</acm-class><abstract>  Let $\orig{A}$ be any matrix and let $A$ be a slight random perturbation of
$\orig{A}$. We prove that it is unlikely that $A$ has large condition number.
Using this result, we prove it is unlikely that $A$ has large growth factor
under Gaussian elimination without pivoting. By combining these results, we
bound the smoothed precision needed by Gaussian elimination without pivoting.
Our results improve the average-case analysis of Gaussian elimination without
pivoting performed by Yeung and Chan (SIAM J. Matrix Anal. Appl., 1997).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310023</id><created>2003-10-13</created><authors><author><keyname>Bocharov</keyname><forenames>Igor</forenames></author><author><keyname>Lukin</keyname><forenames>Pavel</forenames></author></authors><title>Application of Kullback-Leibler Metric to Speech Recognition</title><categories>cs.AI</categories><comments>10 pages, 4 figures, Word to PDF auto converted</comments><acm-class>I.2.7</acm-class><abstract>  Article discusses the application of Kullback-Leibler divergence to the
recognition of speech signals and suggests three algorithms implementing this
divergence criterion: correlation algorithm, spectral algorithm and filter
algorithm. Discussion covers an approach to the problem of speech variability
and is illustrated with the results of experimental modeling of speech signals.
The article gives a number of recommendations on the choice of appropriate
model parameters and provides a comparison to some other methods of speech
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310024</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310024</id><created>2003-10-14</created><authors><author><keyname>Huselius</keyname><forenames>Joel</forenames></author><author><keyname>Thane</keyname><forenames>Henrik</forenames></author><author><keyname>Sundmark</keyname><forenames>Daniel</forenames></author></authors><title>Availability Guarantee for Deterministic Replay Starting Points in
  Real-Time Systems</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Cyclic debugging requires repeatable executions. As non-deterministic or
real-time systems typically do not have the potential to provide this, special
methods are required. One such method is replay, a process that requires
monitoring of a running system and logging of the data produced by that
monitoring. We shall discuss the process of preparing the replay, a part of the
process that has not been very well described before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310025</id><created>2003-10-14</created><authors><author><keyname>Auguston</keyname><forenames>Mikhail</forenames></author><author><keyname>Jeffery</keyname><forenames>Clinton</forenames></author><author><keyname>Underwood</keyname><forenames>Scott</forenames></author></authors><title>A Monitoring Language for Run Time and Post-Mortem Behavior Analysis and
  Visualization</title><categories>cs.SE cs.PL</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  UFO is a new implementation of FORMAN, a declarative monitoring language, in
which rules are compiled into execution monitors that run on a virtual machine
supported by the Alamo monitor architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310026</id><created>2003-10-15</created><authors><author><keyname>Sasaki</keyname><forenames>Akira</forenames></author><author><keyname>Sassa</keyname><forenames>Masataka</forenames></author></authors><title>Generalized Systematic Debugging for Attribute Grammars</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Attribute grammars (AGs) are known to be a useful formalism for semantic
analysis and translation. However, debugging AGs is complex owing to inherent
difficulties of AGs, such as recursive grammar structure and attribute
dependency. In this paper, a new systematic method of debugging AGs is
proposed. Our approach is, in principle, based on previously proposed
algorithmic debugging of AGs, but is more general. This easily enables
integration of various query-based systematic debugging methods, including the
slice-based method. The proposed method has been implemented in Aki, a debugger
for AG description. We evaluated our new approach experimentally using Aki,
which demonstrates the usability of our debugging method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310027</id><created>2003-10-15</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Beurer</keyname><forenames>Karin</forenames></author></authors><title>On the continuous Fermat-Weber problem</title><categories>cs.CG cs.DS</categories><comments>28 pages, 18 figures, Latex, to appear in Operations Research,
  extended abstract version appeared in SoCG 2000</comments><acm-class>F.2.2</acm-class><abstract>  We give the first exact algorithmic study of facility location problems that
deal with finding a median for a continuum of demand points. In particular, we
consider versions of the ``continuous k-median (Fermat-Weber) problem'' where
the goal is to select one or more center points that minimize the average
distance to a set of points in a demand region. In such problems, the average
is computed as an integral over the relevant region, versus the usual discrete
sum of distances. The resulting facility location problems are inherently
geometric, requiring analysis techniques of computational geometry. We provide
polynomial-time algorithms for various versions of the L1 1-median
(Fermat-Weber) problem. We also consider the multiple-center version of the L1
k-median problem, which we prove is NP-hard for large k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310028</id><created>2003-10-15</created><authors><author><keyname>Jain</keyname><forenames>Anoop</forenames></author><author><keyname>Sarda</keyname><forenames>Parag</forenames></author><author><keyname>Haritsa</keyname><forenames>Jayant R.</forenames></author></authors><title>Providing Diversity in K-Nearest Neighbor Query Results</title><categories>cs.DB</categories><comments>20 pages, 11 figures</comments><acm-class>H.2.4</acm-class><abstract>  Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN)
queries return the K closest answers according to given distance metric in the
database with respect to Q. In this scenario, it is possible that a majority of
the answers may be very similar to some other, especially when the data has
clusters. For a variety of applications, such homogeneous result sets may not
add value to the user. In this paper, we consider the problem of providing
diversity in the results of KNN queries, that is, to produce the closest result
set such that each answer is sufficiently different from the rest. We first
propose a user-tunable definition of diversity, and then present an algorithm,
called MOTLEY, for producing a diverse result set as per this definition.
Through a detailed experimental evaluation on real and synthetic data, we show
that MOTLEY can produce diverse result sets by reading only a small fraction of
the tuples in the database. Further, it imposes no additional overhead on the
evaluation of traditional KNN queries, thereby providing a seamless interface
between diversity and distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310029</id><created>2003-10-15</created><authors><author><keyname>Thakur</keyname><forenames>Rajeev</forenames></author><author><keyname>Gropp</keyname><forenames>William</forenames></author><author><keyname>Lusk</keyname><forenames>Ewing</forenames></author></authors><title>Optimizing Noncontiguous Accesses in MPI-IO</title><categories>cs.DC</categories><comments>18 pages, 12 figures</comments><report-no>ANL/MCS-P913-1001</report-no><acm-class>B.4.3; D.1.3</acm-class><journal-ref>Parallel Computing 28(1) (January 2002), pp. 83-105</journal-ref><abstract>  The I/O access patterns of many parallel applications consist of accesses to
a large number of small, noncontiguous pieces of data. If an application's I/O
needs are met by making many small, distinct I/O requests, however, the I/O
performance degrades drastically. To avoid this problem, MPI-IO allows users to
access noncontiguous data with a single I/O function call, unlike in Unix I/O.
In this paper, we explain how critical this feature of MPI-IO is for high
performance and how it enables implementations to perform optimizations. We
first provide a classification of the different ways of expressing an
application's I/O needs in MPI-IO--we classify them into four levels, called
level~0 through level~3. We demonstrate that, for applications with
noncontiguous access patterns, the I/O performance improves dramatically if
users write their applications to make level-3 requests (noncontiguous,
collective) rather than level-0 requests (Unix style). We then describe how our
MPI-IO implementation, ROMIO, delivers high performance for noncontiguous
requests. We explain in detail the two key optimizations ROMIO performs: data
sieving for noncontiguous requests from one process and collective I/O for
noncontiguous requests from multiple processes. We describe how we have
implemented these optimizations portably on multiple machines and file systems,
controlled their memory requirements, and also achieved high performance. We
demonstrate the performance and portability with performance results for three
applications--an astrophysics-application template (DIST3D), the NAS BTIO
benchmark, and an unstructured code (UNSTRUC)--on five different parallel
machines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310030</id><created>2003-10-15</created><authors><author><keyname>Oppitz</keyname><forenames>Oliver</forenames></author></authors><title>A Particular Bug Trap: Execution Replay Using Virtual Machines</title><categories>cs.DC</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Execution-replay (ER) is well known in the literature but has been restricted
to special system architectures for many years. Improved hardware resources and
the maturity of virtual machine technology promise to make ER useful for a
broader range of development projects.
  This paper describes an approach to create a practical, generic ER
infrastructure for desktop PC systems using virtual machine technology. In the
created VM environment arbitrary application programs will run and be replayed
unmodified, neither instrumentation nor recompilation are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310031</id><created>2003-10-15</created><authors><author><keyname>de Silva</keyname><forenames>Vin</forenames></author></authors><title>A weak definition of Delaunay triangulation</title><categories>cs.CG</categories><comments>8 pages, 4 figures</comments><acm-class>I.3.5</acm-class><abstract>  We show that the traditional criterion for a simplex to belong to the
Delaunay triangulation of a point set is equivalent to a criterion which is a
priori weaker. The argument is quite general; as well as the classical
Euclidean case, it applies to hyperbolic and hemispherical geometries and to
Edelsbrunner's weighted Delaunay triangulation. In spherical geometry, we
establish a similar theorem under a genericity condition. The weak definition
finds natural application in the problem of approximating a point-cloud data
set with a simplical complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310032</id><created>2003-10-16</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Schepers</keyname><forenames>Joerg</forenames></author></authors><title>A combinatorial characterization of higher-dimensional orthogonal
  packing</title><categories>cs.DS cs.CG</categories><comments>21 pages, 8 figures, Latex, to appear in Mathematics of Operations
  Research</comments><acm-class>F.2.2</acm-class><abstract>  Higher-dimensional orthogonal packing problems have a wide range of practical
applications, including packing, cutting, and scheduling. Previous efforts for
exact algorithms have been unable to avoid structural problems that appear for
instances in two- or higher-dimensional space. We present a new approach for
modeling packings, using a graph-theoretical characterization of feasible
packings. Our characterization allows it to deal with classes of packings that
share a certain combinatorial structure, instead of having to consider one
packing at a time. In addition, we can make use of elegant algorithmic
properties of certain classes of graphs. This allows our characterization to be
the basis for a successful branch-and-bound framework.
  This is the first in a series of papers describing new approaches to
higher-dimensional packing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310033</id><created>2003-10-16</created><updated>2003-10-27</updated><authors><author><keyname>Ozsari</keyname><forenames>Turker</forenames></author></authors><title>A Hash of Hash Functions</title><categories>cs.CR cs.CY</categories><comments>24 pages, a survey</comments><acm-class>E.3; D.4.6</acm-class><abstract>  In this paper, we present a general review of hash functions in a
cryptographic sense. We give special emphasis on some particular topics such as
cipher block chaining message authentication code (CBC MAC) and its variants.
This paper also broadens the information given in some well known surveys, by
including more details on block-cipher based hash functions and security of
different hash schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310034</identifier>
 <datestamp>2008-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310034</id><created>2003-10-16</created><updated>2008-09-05</updated><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Luebbecke</keyname><forenames>Marco</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author></authors><title>Minimizing the stabbing number of matchings, trees, and triangulations</title><categories>cs.CG cs.DS</categories><comments>25 pages, 12 figures, Latex. To appear in &quot;Discrete and Computational
  Geometry&quot;. Previous version (extended abstract) appears in SODA 2004, pp.
  430-439</comments><acm-class>F.2.2</acm-class><doi>10.1007/s00454-008-9114-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (axis-parallel) stabbing number of a given set of line segments is the
maximum number of segments that can be intersected by any one (axis-parallel)
line. This paper deals with finding perfect matchings, spanning trees, or
triangulations of minimum stabbing number for a given set of points. The
complexity of these problems has been a long-standing open question; in fact,
it is one of the original 30 outstanding open problems in computational
geometry on the list by Demaine, Mitchell, and O'Rourke. The answer we provide
is negative for a number of minimum stabbing problems by showing them NP-hard
by means of a general proof technique. It implies non-trivial lower bounds on
the approximability. On the positive side we propose a cut-based integer
programming formulation for minimizing the stabbing number of matchings and
spanning trees. We obtain lower bounds (in polynomial time) from the
corresponding linear programming relaxations, and show that an optimal
fractional solution always contains an edge of at least constant weight. This
result constitutes a crucial step towards a constant-factor approximation via
an iterated rounding scheme. In computational experiments we demonstrate that
our approach allows for actually solving problems with up to several hundred
points optimally or near-optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310035</id><created>2003-10-17</created><authors><author><keyname>Kadlag</keyname><forenames>Abhijit</forenames></author><author><keyname>Wanjari</keyname><forenames>Amol</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author><author><keyname>Haritsa</keyname><forenames>Jayant R.</forenames></author></authors><title>Supporting Exploratory Queries in Database Centric Web Applications</title><categories>cs.DB</categories><comments>Version After chaning the authors, earlier by mistake only first two
  Authors were taken</comments><acm-class>H.2.4</acm-class><abstract>  Users of database-centric Web applications, especially in the e-commerce
domain, often resort to exploratory ``trial-and-error'' queries since the
underlying data space is huge and unfamiliar, and there are several
alternatives for search attributes in this space. For example, scouting for
cheap airfares typically involves posing multiple queries, varying flight
times, dates, and airport locations. Exploratory queries are problematic from
the perspective of both the user and the server. For the database server, it
results in a drastic reduction in effective throughput since much of the
processing is duplicated in each successive query. For the client, it results
in a marked increase in response times, especially when accessing the service
through wireless channels.
  In this paper, we investigate the design of automated techniques to minimize
the need for repetitive exploratory queries. Specifically, we present SAUNA, a
server-side query relaxation algorithm that, given the user's initial range
query and a desired cardinality for the answer set, produces a relaxed query
that is expected to contain the required number of answers. The algorithm
incorporates a range-query-specific distance metric that is weighted to produce
relaxed queries of a desired shape (e.g. aspect ratio preserving), and utilizes
multi-dimensional histograms for query size estimation. A detailed performance
evaluation of SAUNA over a variety of multi-dimensional data sets indicates
that its relaxed queries can significantly reduce the costs associated with
exploratory query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310036</id><created>2003-10-17</created><updated>2004-03-31</updated><authors><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Solving Sparse, Symmetric, Diagonally-Dominant Linear Systems in Time $O
  (m^{1.31})$</title><categories>cs.DS cs.NA</categories><comments>fixed a typo on page 9</comments><acm-class>F.2.1; G.1.3</acm-class><abstract>  We present a linear-system solver that, given an $n$-by-$n$ symmetric
positive semi-definite, diagonally dominant matrix $A$ with $m$ non-zero
entries and an $n$-vector $\bb $, produces a vector $\xxt$ within relative
distance $\epsilon$ of the solution to $A \xx = \bb$ in time $O (m^{1.31} \log
(n \kappa_{f} (A)/\epsilon)^{O (1)})$, where $\kappa_{f} (A)$ is the log of the
ratio of the largest to smallest non-zero eigenvalue of $A$. In particular,
$\log (\kappa_{f} (A)) = O (b \log n)$, where $b$ is the logarithm of the ratio
of the largest to smallest non-zero entry of $A$. If the graph of $A$ has genus
$m^{2\theta}$ or does not have a $K_{m^{\theta}} $ minor, then the exponent of
$m$ can be improved to the minimum of $1 + 5 \theta $ and $(9/8) (1+\theta)$.
The key contribution of our work is an extension of Vaidya's techniques for
constructing and analyzing combinatorial preconditioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310037</id><created>2003-10-17</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author></authors><title>Maximum dispersion and geometric maximum weight cliques</title><categories>cs.DS cs.CG</categories><comments>13 pages, 1 figure, Latex, to appear in Algorithmica</comments><acm-class>F.2.2</acm-class><journal-ref>Algorithmica, 38 (3) 2004, 501-511.</journal-ref><abstract>  We consider a facility location problem, where the objective is to
``disperse'' a number of facilities, i.e., select a given number k of locations
from a discrete set of n candidates, such that the average distance between
selected locations is maximized. In particular, we present algorithmic results
for the case where vertices are represented by points in d-dimensional space,
and edge weights correspond to rectilinear distances. Problems of this type
have been considered before, with the best result being an approximation
algorithm with performance ratio 2. For the case where k is fixed, we establish
a linear-time algorithm that finds an optimal solution. For the case where k is
part of the input, we present a polynomial-time approximation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310038</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310038</id><created>2003-10-17</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Krishnan</keyname><forenames>Vijay</forenames></author><author><keyname>Haritsa</keyname><forenames>Jayant</forenames></author></authors><title>On Addressing Efficiency Concerns in Privacy Preserving Data Mining</title><categories>cs.DB</categories><acm-class>H.2.8</acm-class><abstract>  Data mining services require accurate input data for their results to be
meaningful, but privacy concerns may influence users to provide spurious
information. To encourage users to provide correct inputs, we recently proposed
a data distortion scheme for association rule mining that simultaneously
provides both privacy to the user and accuracy in the mining results. However,
mining the distorted database can be orders of magnitude more time-consuming as
compared to mining the original database. In this paper, we address this issue
and demonstrate that by (a) generalizing the distortion process to perform
symbol-specific distortion, (b) appropriately choosing the distortion
parameters, and (c) applying a variety of optimizations in the reconstruction
process, runtime efficiencies that are well within an order of magnitude of
undistorted mining can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310039</id><created>2003-10-17</created><authors><author><keyname>Buragohain</keyname><forenames>Chiranjeeb</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author></authors><title>A Game Theoretic Framework for Incentives in P2P Systems</title><categories>cs.GT</categories><acm-class>H.4, J.4</acm-class><journal-ref>Proc. of the Third International Conference on P2P Computing
  (P2P2003), Linkoping Sweden, 2003</journal-ref><abstract>  Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with no
centralized authority or infrastructure. Because of the voluntary
participation, the availability of resources in a P2P system can be highly
variable and unpredictable. In this paper, we use ideas from Game Theory to
study the interaction of strategic and rational peers, and propose a
differential service-based incentive scheme to improve the system's
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310040</id><created>2003-10-18</created><authors><author><keyname>Pytlik</keyname><forenames>Brock</forenames></author><author><keyname>Renieris</keyname><forenames>Manos</forenames></author><author><keyname>Krishnamurthi</keyname><forenames>Shriram</forenames></author><author><keyname>Reiss</keyname><forenames>Steven P.</forenames></author></authors><title>Automated Fault Localization Using Potential Invariants</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  We present a general method for fault localization based on abstracting over
program traces, and a tool that implements the method using Ernst's notion of
potential invariants. Our experiments so far have been unsatisfactory,
suggesting that further research is needed before invariants can be used to
locate faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310041</id><created>2003-10-21</created><authors><author><keyname>Fragkou</keyname><forenames>Pavlina</forenames></author></authors><title>A Dynamic Programming Algorithm for the Segmentation of Greek Texts</title><categories>cs.CL cs.DL</categories><comments>This paper will appear in the Proceedings of the CONSOLE XII
  Conference (Patras, Greece, 2003)</comments><acm-class>H.3.1; H.3.3; H.3.7</acm-class><abstract>  In this paper we introduce a dynamic programming algorithm to perform linear
text segmentation by global minimization of a segmentation cost function which
consists of: (a) within-segment word similarity and (b) prior information about
segment length. The evaluation of the segmentation accuracy of the algorithm on
a text collection consisting of Greek texts showed that the algorithm achieves
high segmentation accuracy and appears to be very innovating and promissing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310042</id><created>2003-10-22</created><authors><author><keyname>Ducasse</keyname><forenames>Mireille</forenames></author><author><keyname>Langevine</keyname><forenames>Ludovic</forenames></author><author><keyname>Deransart</keyname><forenames>Pierre</forenames></author></authors><title>Rigorous design of tracers: an experiment for constraint logic
  programming</title><categories>cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  In order to design and implement tracers, one must decide what exactly to
trace and how to produce this trace. On the one hand, trace designs are too
often guided by implementation concerns and are not as useful as they should
be. On the other hand, an interesting trace which cannot be produced
efficiently, is not very useful either. In this article we propose a
methodology which helps to efficiently produce accurate traces. Firstly, design
a formal specification of the trace model. Secondly, derive a prototype tracer
from this specification. Thirdly, analyze the produced traces. Fourthly,
implement an efficient tracer. Lastly, compare the traces of the two tracers.
At each step, problems can be found. In that case one has to iterate the
process. We have successfully applied the proposed methodology to the design
and implementation of a real tracer for constraint logic programming which is
able to efficiently generate information required to build interesting
graphical views of executions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310043</id><created>2003-10-22</created><updated>2003-10-29</updated><authors><author><keyname>Kamdem</keyname><forenames>Jules Sadefo</forenames></author></authors><title>Value-at-Risk and Expected Shortfall for Quadratic portfolio of
  securities with mixture of elliptic Distributed Risk Factors</title><categories>cs.CE math.CA</categories><proxy>ccsd ccsd-00000777</proxy><acm-class>G.1.9; G.1.10; G.1.2; G.1.1; J.1; J.2; J.4</acm-class><abstract>  Generally, in the financial literature, the notion of quadratic VaR is
implicitly confused with the Delta-Gamma VaR, because more authors dealt with
portfolios that contains derivatives instruments.
  In this paper, we postpone to estimate the Value-at-Risk of a quadratic
portfolio of securities (i.e equities) without the Delta and Gamma greeks, when
the joint log-returns changes with multivariate elliptic distribution. We have
reduced the estimation of the quadratic VaR of such portfolio to a resolution
of one dimensional integral equation. To illustrate our method, we give special
attention to the mixture of normal and mixture of t-student distribution. For
given VaR, when joint Risk Factors changes with elliptic distribution, we show
how to estimate an Expected Shortfall .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310044</id><created>2003-10-22</created><authors><author><keyname>Abbas</keyname><forenames>Ali E.</forenames></author></authors><title>The Algebra of Utility Inference</title><categories>cs.AI</categories><comments>15 pages</comments><acm-class>J.4</acm-class><abstract>  Richard Cox [1] set the axiomatic foundations of probable inference and the
algebra of propositions. He showed that consistency within these axioms
requires certain rules for updating belief. In this paper we use the analogy
between probability and utility introduced in [2] to propose an axiomatic
foundation for utility inference and the algebra of preferences. We show that
consistency within these axioms requires certain rules for updating preference.
We discuss a class of utility functions that stems from the axioms of utility
inference and show that this class is the basic building block for any general
multiattribute utility function. We use this class of utility functions
together with the algebra of preferences to construct utility functions
represented by logical operations on the attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310045</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310045</id><created>2003-10-22</created><authors><author><keyname>Abbas</keyname><forenames>Ali E.</forenames></author></authors><title>An information theory for preferences</title><categories>cs.AI</categories><acm-class>J.4</acm-class><doi>10.1063/1.1751362</doi><abstract>  Recent literature in the last Maximum Entropy workshop introduced an analogy
between cumulative probability distributions and normalized utility functions.
Based on this analogy, a utility density function can de defined as the
derivative of a normalized utility function. A utility density function is
non-negative and integrates to unity. These two properties form the basis of a
correspondence between utility and probability. A natural application of this
analogy is a maximum entropy principle to assign maximum entropy utility
values. Maximum entropy utility interprets many of the common utility functions
based on the preference information needed for their assignment, and helps
assign utility values based on partial preference information. This paper
reviews maximum entropy utility and introduces further results that stem from
the duality between probability and utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310046</identifier>
 <datestamp>2010-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310046</id><created>2003-10-23</created><updated>2009-07-17</updated><authors><author><keyname>Tadaki</keyname><forenames>Kohtaro</forenames></author><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Lin</keyname><forenames>Jack C. H.</forenames></author></authors><title>Theory of One Tape Linear Time Turing Machines</title><categories>cs.CC</categories><comments>26 pages, 10pt, letter size. A few corrections. This is a complete
  version of the paper that appeared in the Proceedings of the 30th SOFSEM
  Conference on Current Trends in Theory and Practice of Computer Science,
  Lecture Notes in Computer Science, Vol.2932, pp.335-348, Springer-Verlag,
  January 24-30, 2004</comments><acm-class>F.1.1; F.1.2; F.4.3</acm-class><journal-ref>(journal version) Theoretical Computer Science, Vol.411, pp.22-43,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theory of one-tape (one-head) linear-time Turing machines is essentially
different from its polynomial-time counterpart since these machines are closely
related to finite state automata. This paper discusses structural-complexity
issues of one-tape Turing machines of various types (deterministic,
nondeterministic, reversible, alternating, probabilistic, counting, and quantum
Turing machines) that halt in linear time, where the running time of a machine
is defined as the length of any longest computation path. We explore structural
properties of one-tape linear-time Turing machines and clarify how the
machines' resources affect their computational patterns and power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310047</id><created>2003-10-24</created><authors><author><keyname>Perri</keyname><forenames>Simona</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Abductive Logic Programs with Penalization: Semantics, Complexity and
  Implementation</title><categories>cs.AI</categories><comments>36 pages; will be published in Theory and Practice of Logic
  Programming</comments><acm-class>D.1.6</acm-class><abstract>  Abduction, first proposed in the setting of classical logics, has been
studied with growing interest in the logic programming area during the last
years.
  In this paper we study abduction with penalization in the logic programming
framework. This form of abductive reasoning, which has not been previously
analyzed in logic programming, turns out to represent several relevant
problems, including optimization problems, very naturally. We define a formal
model for abduction with penalization over logic programs, which extends the
abductive framework proposed by Kakas and Mancarella. We address knowledge
representation issues, encoding a number of problems in our abductive
framework. In particular, we consider some relevant problems, taken from
different domains, ranging from optimization theory to diagnosis and planning;
their encodings turn out to be simple and elegant in our formalism. We
thoroughly analyze the computational complexity of the main problems arising in
the context of abduction with penalization from logic programs. Finally, we
implement a system supporting the proposed abductive framework on top of the
DLV engine. To this end, we design a translation from abduction problems with
penalties into logic programs with weak constraints. We prove that this
approach is sound and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310048</id><created>2003-10-24</created><authors><author><keyname>Gaspard</keyname><forenames>Sebastien</forenames></author><author><keyname>Estrella</keyname><forenames>Florida</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Dindeleux</keyname><forenames>Regis</forenames></author></authors><title>Managing Evolving Business Workflows through the Capture of Descriptive
  Information</title><categories>cs.SE cs.DB</categories><comments>12 pages, 4 figures. Presented at the eCOMO'2003 4th Int. Workshop on
  Conceptual Modeling Approaches for e-Business</comments><acm-class>H3.4;K4.4</acm-class><abstract>  Business systems these days need to be agile to address the needs of a
changing world. In particular the discipline of Enterprise Application
Integration requires business process management to be highly reconfigurable
with the ability to support dynamic workflows, inter-application integration
and process reconfiguration. Basing EAI systems on model-resident or on a
so-called description-driven approach enables aspects of flexibility,
distribution, system evolution and integration to be addressed in a
domain-independent manner. Such a system called CRISTAL is described in this
paper with particular emphasis on its application to EAI problem domains. A
practical example of the CRISTAL technology in the domain of manufacturing
systems, called Agilium, is described to demonstrate the principles of
model-driven system evolution and integration. The approach is compared to
other model-driven development approaches such as the Model-Driven Architecture
of the OMG and so-called Adaptive Object Models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310049</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310049</id><created>2003-10-25</created><authors><author><keyname>Batagelj</keyname><forenames>V.</forenames></author><author><keyname>Zaversnik</keyname><forenames>M.</forenames></author></authors><title>An O(m) Algorithm for Cores Decomposition of Networks</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2</acm-class><journal-ref>Advances in Data Analysis and Classification, 2011. Volume 5,
  Number 2, 129-145</journal-ref><abstract>  The structure of large networks can be revealed by partitioning them to
smaller parts, which are easier to handle. One of such decompositions is based
on $k$--cores, proposed in 1983 by Seidman. In the paper an efficient, $O(m)$,
$m$ is the number of lines, algorithm for determining the cores decomposition
of a given network is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310050</id><created>2003-10-27</created><updated>2005-03-31</updated><authors><author><keyname>Rataj</keyname><forenames>Artur</forenames></author></authors><title>Feedforward Neural Networks with Diffused Nonlinear Weight Functions</title><categories>cs.NE</categories><comments>17 pages, 7 figures. Corrected, some parts rewritten</comments><report-no>IITiS-2003-02-23-1-1.06</report-no><acm-class>I.2.6</acm-class><abstract>  In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310051</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310051</id><created>2003-10-28</created><updated>2008-09-19</updated><authors><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Nearly-Linear Time Algorithms for Graph Partitioning, Graph
  Sparsification, and Solving Linear Systems</title><categories>cs.DS cs.NA</categories><comments>withdrawn by author</comments><acm-class>F.2.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been divided into three papers. arXiv:0809.3232,
arXiv:0808.4134, arXiv:cs/0607105
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310052</id><created>2003-10-28</created><authors><author><keyname>Kulesza</keyname><forenames>Kamil</forenames></author><author><keyname>Kotulski</keyname><forenames>Zbigniew</forenames></author></authors><title>On secret sharing for graphs</title><categories>cs.CR</categories><comments>11 pages</comments><acm-class>D.4.6; E.4</acm-class><abstract>  In the paper we discuss how to share the secrets, that are graphs. So, far
secret sharing schemes were designed to work with numbers. As the first step,
we propose conditions for &quot;graph to number&quot; conversion methods. Hence, the
existing schemes can be used, without weakening their properties. Next, we show
how graph properties can be used to extend capabilities of secret sharing
schemes. This leads to proposal of using such properties for number based
secret sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310053</id><created>2003-10-28</created><authors><author><keyname>Kulesza</keyname><forenames>Kamil</forenames></author><author><keyname>Kotulski</keyname><forenames>Zbigniew</forenames></author></authors><title>Secret Sharing for n-Colorable Graphs with Application to Public Key
  Cryptography</title><categories>cs.CR</categories><comments>10 pages, 5 figures</comments><acm-class>D.4.6; E.3; K 6.5</acm-class><journal-ref>Proceedings of 5th NATO Regional Conference on Military
  Communication and Information Systems, Capturing New CIS Technologies, RCMIS
  2003. 22-24 pazdziernik 2003</journal-ref><abstract>  At the beginning some results from the field of graph theory are presented.
Next we show how to share a secret that is proper n-coloring of the graph, with
the known structure. The graph is described and converted to the form, where
colors assigned to vertices form the number with entries from Zn. A secret
sharing scheme (SSS) for the graph coloring is proposed. The proposed method is
applied to the public-key cryptosystem called &quot;Polly Cracker&quot;. In this case the
graph structure is a public key, while proper 3-colouring of the graph is a
private key. We show how to share the private key. Sharing particular
n-coloring (color-to-vertex assignment) for the known-structure graph is
presented next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310054</id><created>2003-10-28</created><authors><author><keyname>Desharnais</keyname><forenames>J.</forenames></author><author><keyname>M&#xf6;ller</keyname><forenames>B.</forenames></author><author><keyname>Struth</keyname><forenames>G.</forenames></author></authors><title>Kleene algebra with domain</title><categories>cs.LO</categories><comments>40 pages</comments><acm-class>D.2.4; F.3.1; F.3.2; I.1.3</acm-class><abstract>  We propose Kleene algebra with domain (KAD), an extension of Kleene algebra
with two equational axioms for a domain and a codomain operation, respectively.
KAD considerably augments the expressiveness of Kleene algebra, in particular
for the specification and analysis of state transition systems. We develop the
basic calculus, discuss some related theories and present the most important
models of KAD. We demonstrate applicability by two examples: First, an
algebraic reconstruction of Noethericity and well-foundedness; second, an
algebraic reconstruction of propositional Hoare logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310055</id><created>2003-10-28</created><authors><author><keyname>McCune</keyname><forenames>William</forenames></author></authors><title>Mace4 Reference Manual and Guide</title><categories>cs.SC cs.MS</categories><comments>17 pages</comments><report-no>ANL/MCS-TM-264</report-no><acm-class>F.4.1</acm-class><abstract>  Mace4 is a program that searches for finite models of first-order formulas.
For a given domain size, all instances of the formulas over the domain are
constructed. The result is a set of ground clauses with equality. Then, a
decision procedure based on ground equational rewriting is applied. If
satisfiability is detected, one or more models are printed. Mace4 is a useful
complement to first-order theorem provers, with the prover searching for proofs
and Mace4 looking for countermodels, and it is useful for work on finite
algebras. Mace4 performs better on equational problems than did our previous
model-searching program Mace2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310056</id><created>2003-10-28</created><authors><author><keyname>McCune</keyname><forenames>William</forenames></author></authors><title>OTTER 3.3 Reference Manual</title><categories>cs.SC cs.MS</categories><comments>66 pages</comments><report-no>ANL/MCS-TM-263</report-no><acm-class>F.4.1</acm-class><abstract>  OTTER is a resolution-style theorem-proving program for first-order logic
with equality. OTTER includes the inference rules binary resolution,
hyperresolution, UR-resolution, and binary paramodulation. Some of its other
abilities and features are conversion from first-order formulas to clauses,
forward and back subsumption, factoring, weighting, answer literals, term
ordering, forward and back demodulation, evaluable functions and predicates,
Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is
free, and is portable to many different kinds of computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310057</id><created>2003-10-28</created><authors><author><keyname>Naumann</keyname><forenames>Uwe</forenames></author><author><keyname>Walther</keyname><forenames>Andrea</forenames></author></authors><title>An Introduction to Using Software Tools for Automatic Differentiation</title><categories>cs.MS</categories><comments>23 pages</comments><report-no>ANL/MCS-TM-254</report-no><acm-class>G.4</acm-class><abstract>  We give a gentle introduction to using various software tools for automatic
differentiation (AD). Ready-to-use examples are discussed, and links to further
information are presented. Our target audience includes all those who are
looking for a straightforward way to get started using the available AD
technology. The document is dynamic in the sense that its content will be
updated as the AD software evolves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310058</id><created>2003-10-29</created><authors><author><keyname>Clarke</keyname><forenames>Rodney J.</forenames></author><author><keyname>Dong</keyname><forenames>Dali</forenames></author><author><keyname>Windridge</keyname><forenames>Philip C.</forenames></author></authors><title>Application Architecture for Spoken Language Resources in Organisational
  Settings</title><categories>cs.CL</categories><comments>12 pages, 3 figures</comments><acm-class>I.2.7</acm-class><abstract>  Special technologies need to be used to take advantage of, and overcome, the
challenges associated with acquiring, transforming, storing, processing, and
distributing spoken language resources in organisations. This paper introduces
an application architecture consisting of tools and supporting utilities for
indexing and transcription, and describes how these tools, together with
downstream processing and distribution systems, can be integrated into a
workflow. Two sample applications for this architecture are outlined- the
analysis of decision-making processes in organisations and the deployment of
systems development methods by designers in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310059</id><created>2003-10-30</created><authors><author><keyname>Liu</keyname><forenames>Jiuxing</forenames></author><author><keyname>Jiang</keyname><forenames>Weihang</forenames></author><author><keyname>Wyckoff</keyname><forenames>Pete</forenames></author><author><keyname>Panda</keyname><forenames>Dhabaleswar K.</forenames></author><author><keyname>Ashton</keyname><forenames>David</forenames></author><author><keyname>Buntinas</keyname><forenames>Darius</forenames></author><author><keyname>Gropp</keyname><forenames>William</forenames></author><author><keyname>Toonen</keyname><forenames>Brian</forenames></author></authors><title>Design and Implementation of MPICH2 over InfiniBand with RDMA Support</title><categories>cs.AR cs.DC</categories><comments>12 pages, 17 figures</comments><report-no>Preprint ANL/MCS-P1103-1003</report-no><acm-class>C.1.4; C.2.4</acm-class><abstract>  For several years, MPI has been the de facto standard for writing parallel
applications. One of the most popular MPI implementations is MPICH. Its
successor, MPICH2, features a completely new design that provides more
performance and flexibility. To ensure portability, it has a hierarchical
structure based on which porting can be done at different levels. In this
paper, we present our experiences designing and implementing MPICH2 over
InfiniBand. Because of its high performance and open standard, InfiniBand is
gaining popularity in the area of high-performance computing. Our study focuses
on optimizing the performance of MPI-1 functions in MPICH2. One of our
objectives is to exploit Remote Direct Memory Access (RDMA) in Infiniband to
achieve high performance. We have based our design on the RDMA Channel
interface provided by MPICH2, which encapsulates architecture-dependent
communication functionalities into a very small set of functions. Starting with
a basic design, we apply different optimizations and also propose a
zero-copy-based design. We characterize the impact of our optimizations and
designs using microbenchmarks. We have also performed an application-level
evaluation using the NAS Parallel Benchmarks. Our optimized MPICH2
implementation achieves 7.6 $\mu$s latency and 857 MB/s bandwidth, which are
close to the raw performance of the underlying InfiniBand layer. Our study
shows that the RDMA Channel interface in MPICH2 provides a simple, yet
powerful, abstraction that enables implementations with high performance by
exploiting RDMA operations in InfiniBand. To the best of our knowledge, this is
the first high-performance design and implementation of MPICH2 on InfiniBand
using RDMA support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310060</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310060</id><created>2003-10-31</created><updated>2014-10-06</updated><authors><author><keyname>Feinstein</keyname><forenames>Craig Alan</forenames></author></authors><title>Puzzle: Zermelo-Fraenkel set theory is inconsistent</title><categories>cs.CC</categories><comments>1 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present a puzzle. We prove that Zermelo-Fraenkel set theory
is inconsistent by proving, using Zermelo-Fraenkel set theory, the false
statement that any algorithm that determines whether any $n \times n$ matrix
over $\mathbb F_2$, the finite field of order 2, is nonsingular must run in
exponential time in the worst-case scenario. The object of the puzzle is to
find the error in the proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310061</id><created>2003-10-31</created><authors><author><keyname>Liu</keyname><forenames>Lengning</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Local-search techniques for propositional logic extended with
  cardinality constraints</title><categories>cs.AI</categories><comments>Proceedings of the 9th International Conference on Pronciples and
  Practice of Constraint Programming - CP 2003, LNCS 2833, pp. 495-509</comments><acm-class>I.2.8; F.4.1</acm-class><abstract>  We study local-search satisfiability solvers for propositional logic extended
with cardinality atoms, that is, expressions that provide explicit ways to
model constraints on cardinalities of sets. Adding cardinality atoms to the
language of propositional logic facilitates modeling search problems and often
results in concise encodings. We propose two ``native'' local-search solvers
for theories in the extended language. We also describe techniques to reduce
the problem to standard propositional satisfiability and allow us to use
off-the-shelf SAT solvers. We study these methods experimentally. Our general
finding is that native solvers designed specifically for the extended language
perform better than indirect methods relying on SAT solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310062</id><created>2003-10-31</created><authors><author><keyname>Liu</keyname><forenames>Lengning</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>WSAT(cc) - a fast local-search ASP solver</title><categories>cs.AI</categories><comments>Proceedings of LPNMR-03 (7th International Conference), LNCS,
  Springer Verlag</comments><acm-class>I.2.8; F.4.1</acm-class><abstract>  We describe WSAT(cc), a local-search solver for computing models of theories
in the language of propositional logic extended by cardinality atoms. WSAT(cc)
is a processing back-end for the logic PS+, a recently proposed formalism for
answer-set programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310063</id><created>2003-10-31</created><authors><author><keyname>Marek</keyname><forenames>Victor W.</forenames></author><author><keyname>Niemela</keyname><forenames>Ilkka</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Logic programs with monotone cardinality atoms</title><categories>cs.LO</categories><comments>Proceedings of LPNMR-03 (7th International Conference), LNCS,
  Springer Verlag</comments><acm-class>D.1.6; I.2.3</acm-class><abstract>  We investigate mca-programs, that is, logic programs with clauses built of
monotone cardinality atoms of the form kX, where k is a non-negative integer
and X is a finite set of propositional atoms. We develop a theory of
mca-programs. We demonstrate that the operational concept of the one-step
provability operator generalizes to mca-programs, but the generalization
involves nondeterminism. Our main results show that the formalism of
mca-programs is a common generalization of (1) normal logic programming with
its semantics of models, supported models and stable models, (2) logic
programming with cardinality atoms and with the semantics of stable models, as
defined by Niemela, Simons and Soininen, and (3) of disjunctive logic
programming with the possible-model semantics of Sakama and Inoue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310064</id><created>2003-10-31</created><authors><author><keyname>Dransfield</keyname><forenames>Michael R.</forenames></author><author><keyname>Marek</keyname><forenames>Victor W.</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Satisfiability and computing van der Waerden numbers</title><categories>cs.LO</categories><comments>Proceedings of SAT-2003, LNCS, Springer Verlag</comments><acm-class>I.2.8; F.4.1</acm-class><abstract>  In this paper we bring together the areas of combinatorics and propositional
satisfiability. Many combinatorial theorems establish, often constructively,
the existence of positive integer functions, without actually providing their
closed algebraic form or tight lower and upper bounds. The area of Ramsey
theory is especially rich in such results. Using the problem of computing van
der Waerden numbers as an example, we show that these problems can be
represented by parameterized propositional theories in such a way that
decisions concerning their satisfiability determine the numbers (function) in
question. We show that by using general-purpose complete and local-search
techniques for testing propositional satisfiability, this approach becomes
effective -- competitive with specialized approaches. By following it, we were
able to obtain several new results pertaining to the problem of computing van
der Waerden numbers. We also note that due to their properties, especially
their structural simplicity and computational hardness, propositional theories
that arise in this research can be of use in development, testing and
benchmarking of SAT solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0310065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0310065</id><created>2003-10-31</created><updated>2003-11-21</updated><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Holm</keyname><forenames>Jacob</forenames></author><author><keyname>de Lichtenberg</keyname><forenames>Kristian</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Maintaining Information in Fully-Dynamic Trees with Top Trees</title><categories>cs.DS</categories><comments>Preliminary versions of this work presented at ICALP'97 and SWAT'00.
  The new version takes layered top trees into account</comments><acm-class>E.1;F.2.2;G.2.2</acm-class><abstract>  We introduce top trees as a design of a new simpler interface for data
structures maintaining information in a fully-dynamic forest. We demonstrate
how easy and versatile they are to use on a host of different applications. For
example, we show how to maintain the diameter, center, and median of each tree
in the forest. The forest can be updated by insertion and deletion of edges and
by changes to vertex and edge weights. Each update is supported in O(log n)
time, where n is the size of the tree(s) involved in the update. Also, we show
how to support nearest common ancestor queries and level ancestor queries with
respect to arbitrary roots in O(log n) time. Finally, with marked and unmarked
vertices, we show how to compute distances to a nearest marked vertex. The
later has applications to approximate nearest marked vertex in general graphs,
and thereby to static optimization problems over shortest path metrics.
  Technically speaking, top trees are easily implemented either with
Frederickson's topology trees [Ambivalent Data Structures for Dynamic
2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp.
484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure for
Dynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, we
claim that the interface is simpler for many applications, and indeed our new
bounds are quadratic improvements over previous bounds where they exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311001</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311001</id><created>2003-11-03</created><authors><author><keyname>Peischl</keyname><forenames>Bernhard</forenames></author><author><keyname>Wotawa</keyname><forenames>Franz</forenames></author></authors><title>Modeling State in Software Debugging of VHDL-RTL Designs -- A
  Model-Based Diagnosis Approach</title><categories>cs.AI cs.SE</categories><acm-class>D. 2.5</acm-class><abstract>  In this paper we outline an approach of applying model-based diagnosis to the
field of automatic software debugging of hardware designs. We present our
value-level model for debugging VHDL-RTL designs and show how to localize the
erroneous component responsible for an observed misbehavior. Furthermore, we
discuss an extension of our model that supports the debugging of sequential
circuits, not only at a given point in time, but also allows for considering
the temporal behavior of VHDL-RTL designs. The introduced model is capable of
handling state inherently present in every sequential circuit. The principal
applicability of the new model is outlined briefly and we use industrial-sized
real world examples from the ISCAS'85 benchmark suite to discuss the
scalability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311002</id><created>2003-11-04</created><authors><author><keyname>Benoy</keyname><forenames>Florence</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author></authors><title>Computing Convex Hulls with a Linear Solver</title><categories>cs.PL</categories><comments>13 pages, 1 figure</comments><acm-class>D.1.6; F.3.2</acm-class><abstract>  A programming tactic involving polyhedra is reported that has been widely
applied in the polyhedral analysis of (constraint) logic programs. The method
enables the computations of convex hulls that are required for polyhedral
analysis to be coded with linear constraint solving machinery that is available
in many Prolog systems.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311003</id><created>2003-11-05</created><authors><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Enhancing a Search Algorithm to Perform Intelligent Backtracking</title><categories>cs.AI cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>I.2.8; I.2.3; F.3.3; F.4.1; D.1.6; D3.3</acm-class><abstract>  This paper illustrates how a Prolog program, using chronological backtracking
to find a solution in some search space, can be enhanced to perform intelligent
backtracking. The enhancement crucially relies on the impurity of Prolog that
allows a program to store information when a dead end is reached. To illustrate
the technique, a simple search program is enhanced.
  To appear in Theory and Practice of Logic Programming.
  Keywords: intelligent backtracking, dependency-directed backtracking,
backjumping, conflict-directed backjumping, nogood sets, look-back.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311004</id><created>2003-11-06</created><authors><author><keyname>Abbas</keyname><forenames>Ali</forenames></author><author><keyname>Matheson</keyname><forenames>Jim</forenames></author></authors><title>Utility-Probability Duality</title><categories>cs.AI</categories><acm-class>G.3.3</acm-class><abstract>  This paper presents duality between probability distributions and utility
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311005</id><created>2003-11-06</created><authors><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author></authors><title>On The Cost Distribution of a Memory Bound Function</title><categories>cs.CR cs.DL</categories><comments>8 pages</comments><report-no>LOCKSS TR2003-02</report-no><acm-class>D.4.6</acm-class><abstract>  Memory Bound Functions have been proposed for fighting spam, resisting Sybil
attacks and other purposes. A particular implementation of such functions has
been proposed in which the average effort required to generate a proof of
effort is set by parameters E and l to E * l. The distribution of effort
required to generate an individual proof about this average is fairly broad.
When particular uses of these functions are envisaged, the choice of E and l,
and the system design surrounding the generation and verification of proofs of
effort, need to take the breadth of the distribution into account.
  We show the distribution for this implementation, discuss the system design
issues in the context of two proposed applications, and suggest an improved
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311006</id><created>2003-11-06</created><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author></authors><title>How Push-To-Talk Makes Talk Less Pushy</title><categories>cs.HC</categories><comments>10 pages</comments><acm-class>H.4.3; H.5.3</acm-class><journal-ref>Proc. ACM SIGGROUP Conf. on Supporting Group Work, Sanibel Island,
  FL, Nov. 2003, 170-179. ACM Press.</journal-ref><doi>10.1145/958160.958187</doi><abstract>  This paper presents an exploratory study of college-age students using
two-way, push-to-talk cellular radios. We describe the observed and reported
use of cellular radio by the participants. We discuss how the half-duplex,
lightweight cellular radio communication was associated with reduced
interactional commitment, which meant the cellular radios could be used for a
wide range of conversation styles. One such style, intermittent conversation,
is characterized by response delays. Intermittent conversation is surprising in
an audio medium, since it is typically associated with textual media such as
instant messaging. We present design implications of our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311007</id><created>2003-11-07</created><authors><author><keyname>Perri</keyname><forenames>Simona</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Parametric Connectives in Disjunctive Logic Programming</title><categories>cs.AI</categories><acm-class>D.1.6; D.3.1</acm-class><abstract>  Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge
Representation and Reasoning (KRR). \DLP is very expressive in a precise
mathematical sense: it allows to express every property of finite structures
that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$).
Importantly, the \DLP encodings are often simple and natural.
  In this paper, we single out some limitations of \DLP for KRR, which cannot
naturally express problems where the size of the disjunction is not known ``a
priori'' (like N-Coloring), but it is part of the input. To overcome these
limitations, we further enhance the knowledge modelling abilities of \DLP, by
extending this language by {\em Parametric Connectives (OR and AND)}. These
connectives allow us to represent compactly the disjunction/conjunction of a
set of atoms having a given property. We formally define the semantics of the
new language, named $DLP^{\bigvee,\bigwedge}$ and we show the usefulness of the
new constructs on relevant knowledge-based problems. We address implementation
issues and discuss related works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311008</id><created>2003-11-08</created><authors><author><keyname>Schweimeier</keyname><forenames>Ralf</forenames></author><author><keyname>Schroeder</keyname><forenames>Michael</forenames></author></authors><title>A Parameterised Hierarchy of Argumentation Semantics for Extended Logic
  Programming and its Application to the Well-founded Semantics</title><categories>cs.LO cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>D.1.6.; F.3.2.; F.4.1; I.2.3.; I.2.4</acm-class><abstract>  Argumentation has proved a useful tool in defining formal semantics for
assumption-based reasoning by viewing a proof as a process in which proponents
and opponents attack each others arguments by undercuts (attack to an
argument's premise) and rebuts (attack to an argument's conclusion). In this
paper, we formulate a variety of notions of attack for extended logic programs
from combinations of undercuts and rebuts and define a general hierarchy of
argumentation semantics parameterised by the notions of attack chosen by
proponent and opponent. We prove the equivalence and subset relationships
between the semantics and examine some essential properties concerning
consistency and the coherence principle, which relates default negation and
explicit negation. Most significantly, we place existing semantics put forward
in the literature in our hierarchy and identify a particular argumentation
semantics for which we prove equivalence to the paraconsistent well-founded
semantics with explicit negation, WFSX$_p$. Finally, we present a general proof
theory, based on dialogue trees, and show that it is sound and complete with
respect to the argumentation semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311009</id><created>2003-11-10</created><authors><author><keyname>Demichev</keyname><forenames>A.</forenames><affiliation>Skobeltsyn Institute of Nuclear Physics, Moscow State University, Moscow, Russia</affiliation></author><author><keyname>Foster</keyname><forenames>D.</forenames><affiliation>CERN-IT, Geneva, Switzerland</affiliation></author><author><keyname>Kalyaev</keyname><forenames>V.</forenames><affiliation>Skobeltsyn Institute of Nuclear Physics, Moscow State University, Moscow, Russia</affiliation></author><author><keyname>Kryukov</keyname><forenames>A.</forenames><affiliation>Skobeltsyn Institute of Nuclear Physics, Moscow State University, Moscow, Russia</affiliation></author><author><keyname>Lamanna</keyname><forenames>M.</forenames><affiliation>CERN-IT, Geneva, Switzerland</affiliation></author><author><keyname>Pose</keyname><forenames>V.</forenames><affiliation>JINR LIT, Dubna, Russia</affiliation></author><author><keyname>Da Rocha</keyname><forenames>R. B.</forenames><affiliation>CERN-IT, Geneva, Switzerland</affiliation></author><author><keyname>Wang</keyname><forenames>C.</forenames><affiliation>Academica Sinica, Taipei, Taiwan</affiliation></author></authors><title>OGSA/Globus Evaluation for Data Intensive Applications</title><categories>cs.DC</categories><comments>To be published in the proceedings of the XIX International Symposium
  on Nuclear Electronics and Computing (NEC'2003), Bulgaria, Varna, 15-20
  September, 2003</comments><acm-class>C.2.4</acm-class><abstract>  We present an architecture of Globus Toolkit 3 based testbed intended for
evaluation of applicability of the Open Grid Service Architecture (OGSA) for
Data Intensive Applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311010</id><created>2003-11-10</created><authors><author><keyname>Kalyaev</keyname><forenames>V.</forenames><affiliation>Skobektsyn Institute of Nuclear Physics Moscow State University, Moscow, Russia</affiliation></author><author><keyname>Kryukov</keyname><forenames>A.</forenames><affiliation>Skobektsyn Institute of Nuclear Physics Moscow State University, Moscow, Russia</affiliation></author></authors><title>Problem of Application Job Monitoring in GRID Systems</title><categories>cs.DC</categories><comments>To be published in the proceedings of the XIX International Symposium
  on Nuclear Electronics and Computing (NEC'2003), Bulgaria, Varna, 15-20
  September, 2003</comments><acm-class>C.2.4</acm-class><abstract>  We present a new approach to monitoring of the execution process of an
application job in the GRID environment. The main point of the approach is use
of GRID ervices to access monitoring information with the security level
available in GRID.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311011</id><created>2003-11-10</created><authors><author><keyname>Yuste</keyname><forenames>S. B.</forenames></author><author><keyname>Acedo</keyname><forenames>L.</forenames></author></authors><title>On an explicit finite difference method for fractional diffusion
  equations</title><categories>cs.NA cond-mat.stat-mech cs.CE physics.comp-ph</categories><comments>22 pages, 6 figures</comments><acm-class>G.1.9, G.1.4, G.1.0</acm-class><journal-ref>SIAM J. Numer. Anal. Vol. 42, No. 5, pp. 1862--1874 (2005)</journal-ref><doi>10.1137/030602666</doi><abstract>  A numerical method to solve the fractional diffusion equation, which could
also be easily extended to many other fractional dynamics equations, is
considered. These fractional equations have been proposed in order to describe
anomalous transport characterized by non-Markovian kinetics and the breakdown
of Fick's law. In this paper we combine the forward time centered space (FTCS)
method, well known for the numerical integration of ordinary diffusion
equations, with the Grunwald-Letnikov definition of the fractional derivative
operator to obtain an explicit fractional FTCS scheme for solving the
fractional diffusion equation. The resulting method is amenable to a stability
analysis a la von Neumann. We show that the analytical stability bounds are in
excellent agreement with numerical tests. Comparison between exact analytical
solutions and numerical predictions are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311012</id><created>2003-11-12</created><authors><author><keyname>Carvalho</keyname><forenames>Rui</forenames></author><author><keyname>Batty</keyname><forenames>Michael</forenames></author></authors><title>A rigorous definition of axial lines: ridges on isovist fields</title><categories>cs.CV cs.CG</categories><comments>18 pages, 5 figures</comments><acm-class>I2.10; I.4.10</acm-class><abstract>  We suggest that 'axial lines' defined by (Hillier and Hanson, 1984) as lines
of uninterrupted movement within urban streetscapes or buildings, appear as
ridges in isovist fields (Benedikt, 1979). These are formed from the maximum
diametric lengths of the individual isovists, sometimes called viewsheds, that
make up these fields (Batty and Rana, 2004). We present an image processing
technique for the identification of lines from ridges, discuss current
strengths and weaknesses of the method, and show how it can be implemented
easily and effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311013</id><created>2003-11-12</created><authors><author><keyname>Paruchuri</keyname><forenames>Vamsi</forenames></author><author><keyname>Durresi</keyname><forenames>Arjan</forenames></author><author><keyname>Jain</keyname><forenames>Raj</forenames></author></authors><title>Optimized Flooding Protocol for Ad hoc Networks</title><categories>cs.NI</categories><comments>Number of Pages: 11</comments><acm-class>C.2.1</acm-class><abstract>  Flooding provides important control and route establishment functionality for
a number of unicast and multicast protocols in Mobile Ad Hoc Networks.
Considering its wide use as a building block for other network layer protocols,
the flooding methodology should deliver a packet from one node to all other
network nodes using as few messages as possible. In this paper, we propose the
Optimized Flooding Protocol (OFP), based on a variation of The Covering Problem
that is encountered in geometry, to minimize the unnecessary transmissions
drastically and still be able to cover the whole region. OFP does not need
hello messages and hence OFP saves a significant amount of wireless bandwidth
and incurs lesser overhead. We present simulation results to show the
efficiency of OFP in both ideal cases and randomly distributed networks.
Moreover, OFP is scalable with respect to density; in fact OFP requires lesser
number of transmissions at higher densities. OFP is also resilient to
transmission errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311014</id><created>2003-11-13</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Optimality of Universal Bayesian Sequence Prediction for General Loss
  and Alphabet</title><categories>cs.LG cs.AI math.PR</categories><comments>34 pages</comments><report-no>IDSIA-02-02</report-no><acm-class>E.4;I.2.6;G.3</acm-class><journal-ref>Journal of Machine Learning Research 4 (2003) 971-1000</journal-ref><abstract>  Various optimality properties of universal sequence predictors based on
Bayes-mixtures in general, and Solomonoff's prediction scheme in particular,
will be studied. The probability of observing $x_t$ at time $t$, given past
observations $x_1...x_{t-1}$ can be computed with the chain rule if the true
generating distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. If
$\mu$ is unknown, but known to belong to a countable or continuous class $\M$
one can base ones prediction on the Bayes-mixture $\xi$ defined as a
$w_\nu$-weighted sum or integral of distributions $\nu\in\M$. The cumulative
expected loss of the Bayes-optimal universal prediction scheme based on $\xi$
is shown to be close to the loss of the Bayes-optimal, but infeasible
prediction scheme based on $\mu$. We show that the bounds are tight and that no
other predictor can lead to significantly smaller bounds. Furthermore, for
various performance measures, we show Pareto-optimality of $\xi$ and give an
Occam's razor argument that the choice $w_\nu\sim 2^{-K(\nu)}$ for the weights
is optimal, where $K(\nu)$ is the length of the shortest program describing
$\nu$. The results are applied to games of chance, defined as a sequence of
bets, observations, and rewards. The prediction schemes (and bounds) are
compared to the popular predictors based on expert advice. Extensions to
infinite alphabets, partial, delayed and probabilistic prediction,
classification, and more active systems are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311015</id><created>2003-11-14</created><updated>2003-11-15</updated><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Guo</keyname><forenames>Yiping</forenames></author><author><keyname>Fang</keyname><forenames>Ming</forenames></author></authors><title>Make search become the internal function of Internet</title><categories>cs.IR cs.DL cs.NI</categories><comments>7 pages, 3 figures</comments><acm-class>H.3.3;H.3.5;H.3.7</acm-class><abstract>  Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a distributed information retrieval system, which will solve problems like poor
coverage, long update interval in current web search system. The most distinct
character of DRIS is that it's a public opening system, and acts as an internal
component of Internet, but not the production of a company. The implementation
of DRIS is also represented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311016</id><created>2003-11-14</created><authors><author><keyname>Jahier</keyname><forenames>Erwan</forenames></author><author><keyname>Ducass'e</keyname><forenames>Mireille</forenames></author></authors><title>Generic and Efficient Program Monitoring by trace analysis</title><categories>cs.PL</categories><acm-class>D.2.5</acm-class><journal-ref>E. Jahier and M. Ducass'e &quot;Generic Program Monitoring by Trace
  Analysis&quot; in the Theory and Practice of Logic Programming journal, Volume 2
  part 4&amp;5, pp 613-645, September 2002, Special Issue Program Development,
  Cambridge University Press</journal-ref><abstract>  Program execution monitoring consists of checking whole executions for given
properties in order to collect global run-time information.
  Monitoring is very useful to maintain programs. However, application
developers face the following dilemma: either they use existing tools which
never exactly fit their needs, or they invest a lot of effort to implement
monitoring code.
  In this article we argue that, when an event-oriented tracer exists, the
compiler developers can enable the application developers to easily code their
own, relevant, monitors which will run efficiently.
  We propose a high-level operator, called foldt, which operates on execution
traces. One of the key advantages of our approach is that it allows a clean
separation of concerns; the definition of monitors is neither intertwined in
the user source code nor in the language compiler.
  We give a number of applications of the foldt operator to compute monitors
for Mercury program executions: execution profiles, graphical abstract views,
and two test coverage measurements. Each example is implemented by a few simple
lines of Mercury.
  Detailed measurements show acceptable performance of the basic mechanism of
foldt for executions of several millions of execution events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311017</id><created>2003-11-14</created><authors><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Baker</keyname><forenames>Mary</forenames><affiliation>HP Labs</affiliation></author><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames><affiliation>Stanford University Libraries</affiliation></author><author><keyname>Giuli</keyname><forenames>TJ</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Maniatis</keyname><forenames>Petros</forenames><affiliation>Intel Research</affiliation></author><author><keyname>Mogul</keyname><forenames>Jeff</forenames><affiliation>HP Labs</affiliation></author></authors><title>2 P2P or Not 2 P2P?</title><categories>cs.NI cs.AR</categories><comments>6 pages, 1 figure</comments><acm-class>C.2.4</acm-class><abstract>  In the hope of stimulating discussion, we present a heuristic decision tree
that designers can use to judge the likely suitability of a P2P architecture
for their applications. It is based on the characteristics of a wide range of
P2P systems from the literature, both proposed and deployed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311018</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311018</id><created>2003-11-16</created><authors><author><keyname>Piazza</keyname><forenames>Carla</forenames><affiliation>Universita' Ca' Foscari di Venezia</affiliation></author><author><keyname>Policriti</keyname><forenames>Alberto</forenames><affiliation>Universita' degli Studi di Udine</affiliation></author></authors><title>Ackermann Encoding, Bisimulations, and OBDDs</title><categories>cs.LO cs.DS</categories><comments>To appear on 'Theory and Practice of Logic Programming'</comments><acm-class>E.1; F.2.2; D.2.4</acm-class><abstract>  We propose an alternative way to represent graphs via OBDDs based on the
observation that a partition of the graph nodes allows sharing among the
employed OBDDs. In the second part of the paper we present a method to compute
at the same time the quotient w.r.t. the maximum bisimulation and the OBDD
representation of a given graph. The proposed computation is based on an
OBDD-rewriting of the notion of Ackermann encoding of hereditarily finite sets
into natural numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311019</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311019</id><created>2003-11-17</created><authors><author><keyname>Sundmark</keyname><forenames>Daniel</forenames></author><author><keyname>Thane</keyname><forenames>Henrik</forenames></author><author><keyname>Huselius</keyname><forenames>Joel</forenames></author><author><keyname>Pettersson</keyname><forenames>Anders</forenames></author><author><keyname>Mellander</keyname><forenames>Roger</forenames></author><author><keyname>Reiyer</keyname><forenames>Ingemar</forenames></author><author><keyname>Kallvi</keyname><forenames>Mattias</forenames></author></authors><title>Replay Debugging of Complex Real-Time Systems: Experiences from Two
  Industrial Case Studies</title><categories>cs.RO</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Deterministic replay is a method for allowing complex multitasking real-time
systems to be debugged using standard interactive debuggers. Even though
several replay techniques have been proposed for parallel, multi-tasking and
real-time systems, the solutions have so far lingered on a prototype academic
level, with very little results to show from actual state-of-the-practice
commercial applications. This paper describes a major deterministic replay
debugging case study performed on a full-scale industrial robot control system,
as well as a minor replay instrumentation case study performed on a military
aircraft radar system. In this article, we will show that replay debugging is
feasible in complex multi-million lines of code software projects running on
top of off-the-shelf real-time operating systems. Furthermore, we will discuss
how replay debugging can be introduced in existing systems without
impracticable analysis efforts. In addition, we will present benchmarking
results from both studies, indicating that the instrumentation overhead is
acceptable and affordable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311020</id><created>2003-11-17</created><authors><author><keyname>Chung</keyname><forenames>Kai-min</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>An Optimal Algorithm for the Maximum-Density Segment Problem</title><categories>cs.DS cs.DM</categories><comments>15 pages, 12 figures, an early version of this paper was presented at
  11th Annual European Symposium on Algorithms (ESA 2003), Budapest, Hungary,
  September 15-20, 2003</comments><acm-class>J.3; F.2.2; G.2.1; I.1.2</acm-class><journal-ref>SIAM Journal on Computing, 34(2):373-387, 2004</journal-ref><doi>10.1137/S0097539704440430</doi><abstract>  We address a fundamental problem arising from analysis of biomolecular
sequences. The input consists of two numbers $w_{\min}$ and $w_{\max}$ and a
sequence $S$ of $n$ number pairs $(a_i,w_i)$ with $w_i&gt;0$. Let {\em segment}
$S(i,j)$ of $S$ be the consecutive subsequence of $S$ between indices $i$ and
$j$. The {\em density} of $S(i,j)$ is
$d(i,j)=(a_i+a_{i+1}+...+a_j)/(w_i+w_{i+1}+...+w_j)$. The {\em maximum-density
segment problem} is to find a maximum-density segment over all segments
$S(i,j)$ with $w_{\min}\leq w_i+w_{i+1}+...+w_j \leq w_{\max}$. The best
previously known algorithm for the problem, due to Goldwasser, Kao, and Lu,
runs in $O(n\log(w_{\max}-w_{\min}+1))$ time. In the present paper, we solve
the problem in O(n) time. Our approach bypasses the complicated {\em right-skew
decomposition}, introduced by Lin, Jiang, and Chao. As a result, our algorithm
has the capability to process the input sequence in an online manner, which is
an important feature for dealing with genome-scale sequences. Moreover, for a
type of input sequences $S$ representable in $O(m)$ space, we show how to
exploit the sparsity of $S$ and solve the maximum-density segment problem for
$S$ in $O(m)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311021</id><created>2003-11-17</created><authors><author><keyname>Shamardin</keyname><forenames>L.</forenames><affiliation>Skobektsyn Institute of Nuclear Physics Moscow State University, Moscow, Russia</affiliation></author></authors><title>LCG-1 Deployment and usage experience</title><categories>cs.DC</categories><comments>To be published in the proceedings of the XIX International Symposium
  on Nuclear Electronics and Computing (NEC'2003), Bulgaria, Varna, 15-20
  September, 2003</comments><acm-class>C.2.4</acm-class><abstract>  LCG-1 is the second release of the software framework for the LHC Computing
Grid project. In our work we describe the installation process, arising
problems and their solutions, and configuration tuning details of the complete
LCG-1 site, including all LCG elements required for the self-sufficient site.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311022</id><created>2003-11-17</created><authors><author><keyname>Franceschet</keyname><forenames>M.</forenames></author><author><keyname>Montanari</keyname><forenames>A.</forenames></author></authors><title>Temporalized logics and automata for time granularity</title><categories>cs.LO</categories><comments>Journal: Theory and Practice of Logic Programming Journal Acronym:
  TPLP Category: Paper for Special Issue (Verification and Computational Logic)
  Submitted: 18 March 2002, revised: 14 Januari 2003, accepted: 5 September
  2003</comments><acm-class>F.3.1</acm-class><abstract>  Suitable extensions of the monadic second-order theory of k successors have
been proposed in the literature to capture the notion of time granularity. In
this paper, we provide the monadic second-order theories of downward unbounded
layered structures, which are infinitely refinable structures consisting of a
coarsest domain and an infinite number of finer and finer domains, and of
upward unbounded layered structures, which consist of a finest domain and an
infinite number of coarser and coarser domains, with expressively complete and
elementarily decidable temporal logic counterparts.
  We obtain such a result in two steps. First, we define a new class of
combined automata, called temporalized automata, which can be proved to be the
automata-theoretic counterpart of temporalized logics, and show that relevant
properties, such as closure under Boolean operations, decidability, and
expressive equivalence with respect to temporal logics, transfer from component
automata to temporalized ones. Then, we exploit the correspondence between
temporalized logics and automata to reduce the task of finding the temporal
logic counterparts of the given theories of time granularity to the easier one
of finding temporalized automata counterparts of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311023</id><created>2003-11-17</created><authors><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author><author><keyname>Sulzmann</keyname><forenames>Martin</forenames></author><author><keyname>Wazny</keyname><forenames>Jeremy</forenames></author></authors><title>The Chameleon Type Debugger (Tool Demonstration)</title><categories>cs.PL</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  In this tool demonstration, we give an overview of the Chameleon type
debugger. The type debugger's primary use is to identify locations within a
source program which are involved in a type error. By further examining these
(potentially) problematic program locations, users gain a better understanding
of their program and are able to work towards the actual mistake which was the
cause of the type error. The debugger is interactive, allowing the user to
provide additional information to narrow down the search space. One of the
novel aspects of the debugger is the ability to explain erroneous-looking
types. In the event that an unexpected type is inferred, the debugger can
highlight program locations which contributed to that result. Furthermore, due
to the flexible constraint-based foundation that the debugger is built upon, it
can naturally handle advanced type system features such as Haskell's type
classes and functional dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311024</id><created>2003-11-20</created><authors><author><keyname>Mascardi</keyname><forenames>Viviana</forenames></author><author><keyname>Martelli</keyname><forenames>Maurizio</forenames></author><author><keyname>Sterling</keyname><forenames>Leon</forenames></author></authors><title>Logic-Based Specification Languages for Intelligent Software Agents</title><categories>cs.AI</categories><comments>67 pages, 1 table, 1 figure. Accepted for publication by the Journal
  &quot;Theory and Practice of Logic Programming&quot;, volume 4, Maurice Bruynooghe
  Editor-in-Chief</comments><acm-class>A.1; F.4.1; I.2.11</acm-class><abstract>  The research field of Agent-Oriented Software Engineering (AOSE) aims to find
abstractions, languages, methodologies and toolkits for modeling, verifying,
validating and prototyping complex applications conceptualized as Multiagent
Systems (MASs). A very lively research sub-field studies how formal methods can
be used for AOSE. This paper presents a detailed survey of six logic-based
executable agent specification languages that have been chosen for their
potential to be integrated in our ARPEGGIO project, an open framework for
specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the
IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each
executable language, the logic foundations are described and an example of use
is shown. A comparison of the six languages and a survey of similar approaches
complete the paper, together with considerations of the advantages of using
logic-based languages in MAS modeling and prototyping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311025</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311025</id><created>2003-11-20</created><authors><author><keyname>Keahey</keyname><forenames>K.</forenames></author><author><keyname>Welch</keyname><forenames>V.</forenames></author><author><keyname>Lang</keyname><forenames>S.</forenames></author><author><keyname>Liu</keyname><forenames>B.</forenames></author><author><keyname>Meder</keyname><forenames>S.</forenames></author></authors><title>Fine-Grained Authorization for Job Execution in the Grid: Design and
  Implementation</title><categories>cs.CR cs.DC</categories><comments>13 pages, 2 figures</comments><report-no>Preprint ANL/MCS-P1094-0903</report-no><acm-class>D.4.6; D.4.7</acm-class><abstract>  In this paper we describe our work on enabling fine-grained authorization for
resource usage and management. We address the need of virtual organizations to
enforce their own polices in addition to those of the resource owners, in
regard to both resource consumption and job management. To implement this
design, we propose changes and extensions to the Globus Toolkit's version 2
resource management mechanism. We describe the prototype and the policy
language that we designed to express fine-grained policies, and we present an
analysis of our solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311026</id><created>2003-11-20</created><authors><author><keyname>Chu</keyname><forenames>Francis C.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Great Expectations. Part I: On the Customizability of Generalized
  Expected Utility</title><categories>cs.AI</categories><comments>Preliminary version appears in Proc. 18th International Joint
  Conference on AI (IJCAI), 2003, pp. 291-296</comments><acm-class>I.2.4</acm-class><abstract>  We propose a generalization of expected utility that we call generalized EU
(GEU), where a decision maker's beliefs are represented by plausibility
measures, and the decision maker's tastes are represented by general (i.e.,not
necessarily real-valued) utility functions. We show that every agent,
``rational'' or not, can be modeled as a GEU maximizer. We then show that we
can customize GEU by selectively imposing just the constraints we want. In
particular, we show how each of Savage's postulates corresponds to constraints
on GEU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311027</id><created>2003-11-20</created><authors><author><keyname>Chu</keyname><forenames>Francis C.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Great Expectations. Part II: Generalized Expected Utility as a Universal
  Decision Rule</title><categories>cs.AI</categories><comments>Preliminary version appears in Proc. 18th International Joint
  Conference on AI (IJCAI), 2003, pp. 297-302</comments><acm-class>I.2.4</acm-class><abstract>  Many different rules for decision making have been introduced in the
literature. We show that a notion of generalized expected utility proposed in
Part I of this paper is a universal decision rule, in the sense that it can
represent essentially all other decision rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311028</id><created>2003-11-20</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Using Counterfactuals in Knowledge-Based Programming</title><categories>cs.DC cs.AI</categories><comments>Preliminary version appears in Proc. 7th Conference on Theoretical
  Aspects of Rationality and Knowledge (TARK), 1998, pp. 97-110. Submitted to
  Distributed Computing</comments><acm-class>F.4.1, F.3.1, I.2.4, C.2.2, C.2.4</acm-class><abstract>  This paper adds counterfactuals to the framework of knowledge-based programs
of Fagin, Halpern, Moses, and Vardi. The use of counterfactuals is illustrated
by designing a protocol in which an agent stops sending messages once it knows
that it is safe to do so. Such behavior is difficult to capture in the original
framework because it involves reasoning about counterfactual executions,
including ones that are not consistent with the protocol. Attempts to formalize
these notions without counterfactuals are shown to lead to rather
counterintuitive behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311029</id><created>2003-11-20</created><authors><author><keyname>Narayan</keyname><forenames>Michael</forenames></author><author><keyname>Williams</keyname><forenames>Chris</forenames></author><author><keyname>Perugini</keyname><forenames>Saverio</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Staging Transformations for Multimodal Web Interaction Management</title><categories>cs.IR cs.PL</categories><comments>Describes framework and software architecture for multimodal web
  interaction management</comments><acm-class>H.5.4; H.5.2; F.3.2</acm-class><abstract>  Multimodal interfaces are becoming increasingly ubiquitous with the advent of
mobile devices, accessibility considerations, and novel software technologies
that combine diverse interaction media. In addition to improving access and
delivery capabilities, such interfaces enable flexible and personalized dialogs
with websites, much like a conversation between humans. In this paper, we
present a software framework for multimodal web interaction management that
supports mixed-initiative dialogs between users and websites. A
mixed-initiative dialog is one where the user and the website take turns
changing the flow of interaction. The framework supports the functional
specification and realization of such dialogs using staging transformations --
a theory for representing and reasoning about dialogs based on partial input.
It supports multiple interaction interfaces, and offers sessioning, caching,
and co-ordination functions through the use of an interaction manager. Two case
studies are presented to illustrate the promise of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311030</id><created>2003-11-20</created><authors><author><keyname>Abrams</keyname><forenames>Zoe</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Plotkin</keyname><forenames>Serge</forenames></author></authors><title>Set K-Cover Algorithms for Energy Efficient Monitoring in Wireless
  Sensor Networks</title><categories>cs.DS</categories><acm-class>F.2</acm-class><abstract>  Wireless sensor networks (WSNs) are emerging as an effective means for
environment monitoring. This paper investigates a strategy for energy efficient
monitoring in WSNs that partitions the sensors into covers, and then activates
the covers iteratively in a round-robin fashion. This approach takes advantage
of the overlap created when many sensors monitor a single area. Our work builds
upon previous work in &quot;Power Efficient Organization of Wireless Sensor
Networks&quot; by Slijepcevic and Potkonjak, where the model is first formulated. We
have designed three approximation algorithms for a variation of the SET K-COVER
problem, where the objective is to partition the sensors into covers such that
the number of covers that include an area, summed over all areas, is maximized.
The first algorithm is randomized and partitions the sensors, in expectation,
within a fraction 1 - 1/e (~.63) of the optimum. We present two other
deterministic approximation algorithms. One is a distributed greedy algorithm
with a 1/2 approximation ratio and the other is a centralized greedy algorithm
with a 1 - 1/e approximation ratio. We show that it is NP-Complete to guarantee
better than 15/16 of the optimal coverage, indicating that all three algorithms
perform well with respect to the best approximation algorithm possible.
Simulations indicate that in practice, the deterministic algorithms perform far
above their worst case bounds, consistently covering more than 72% of what is
covered by an optimum solution. Simulations also indicate that the increase in
longevity is proportional to the amount of overlap amongst the sensors. The
algorithms are fast, easy to use, and according to simulations, significantly
increase the longevity of sensor networks. The randomized algorithm in
particular seems quite practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311031</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311031</id><created>2003-11-21</created><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Towards an Intelligent Database System Founded on the SP Theory of
  Computing and Cognition</title><categories>cs.DB cs.AI</categories><acm-class>H.2.1</acm-class><journal-ref>J G Wolff, Data &amp; Knowledge Engineering 60, 596-624, 2007</journal-ref><doi>10.1016/j.datak.2006.04.003</doi><abstract>  The SP theory of computing and cognition, described in previous publications,
is an attractive model for intelligent databases because it provides a simple
but versatile format for different kinds of knowledge, it has capabilities in
artificial intelligence, and it can also function like established database
models when that is required.
  This paper describes how the SP model can emulate other models used in
database applications and compares the SP model with those other models. The
artificial intelligence capabilities of the SP model are reviewed and its
relationship with other artificial intelligence systems is described. Also
considered are ways in which current prototypes may be translated into an
'industrial strength' working system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311032</id><created>2003-11-21</created><authors><author><keyname>Mazonka</keyname><forenames>Oleg</forenames></author><author><keyname>Cristofani</keyname><forenames>Daniel B.</forenames></author></authors><title>A Very Short Self-Interpreter</title><categories>cs.PL</categories><acm-class>D.3</acm-class><abstract>  In this paper we would like to present a very short (possibly the shortest)
self-interpreter, based on a simplistic Turing-complete imperative language.
This interpreter explicitly processes the statements of the language, which
means the interpreter constitutes a description of the language inside that
same language. The paper does not require any specific knowledge; however,
experience in programming and a vivid imagination are beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311033</id><created>2003-11-21</created><authors><author><keyname>Buk</keyname><forenames>Solomija N.</forenames></author><author><keyname>Rovenchak</keyname><forenames>Andrij A.</forenames></author></authors><title>The Rank-Frequency Analysis for the Functional Style Corpora in the
  Ukrainian Language</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Journal of Quantitative Linguistics, Vol. 11, No. 3, P. 161-171
  (2004)</journal-ref><doi>10.1080/0929617042000314912</doi><abstract>  We use the rank-frequency analysis for the estimation of Kernel Vocabulary
size within specific corpora of Ukrainian. The extrapolation of high-rank
behaviour is utilized for estimation of the total vocabulary size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311034</id><created>2003-11-22</created><authors><author><keyname>Koldenhof</keyname><forenames>Gibby</forenames></author></authors><title>Visualization of variations in human brain morphology using
  differentiating reflection functions</title><categories>cs.GR</categories><comments>10 pages, keywords: MRI, Medical Visualization, Volume rendering,
  BRDF, Specular reflection overlap</comments><acm-class>I.4.7;I.4.8;I.4.10;I.3.7</acm-class><abstract>  Conventional visualization media such as MRI prints and computer screens are
inherently two dimensional, making them incapable of displaying true 3D volume
data sets. By applying only transparency or intensity projection, and ignoring
light-matter interaction, results will likely fail to give optimal results.
Little research has been done on using reflectance functions to visually
separate the various segments of a MRI volume. We will explore if applying
specific reflectance functions to individual anatomical structures can help in
building an intuitive 2D image from a 3D dataset. We will test our hypothesis
by visualizing a statistical analysis of the genetic influences on variations
in human brain morphology because it inherently contains complex and many
different types of data making it a good candidate for our approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311035</id><created>2003-11-23</created><authors><author><keyname>Petrovic</keyname><forenames>Milenko</forenames></author><author><keyname>Aboelaze</keyname><forenames>Mokhtar</forenames></author></authors><title>Improving TCP/IP Performance over Wireless IEEE 802.11 Link</title><categories>cs.NI cs.PF</categories><comments>7 pages, ICWN 2002 (International Conference on Wireless Networks)</comments><acm-class>C.2.1</acm-class><abstract>  Cellular phones, wireless laptops, personal portable devices that supports
both voice and data access are all examples of communicating devices that uses
wireless communication. Sine TCP/IP (and UDP) is the dominant technology in use
in the internet, it is expected that they will be used (and they are currently)
over wireless connections. In this paper, we investigate the performance of the
TCP (and UDP) over IEEE802.11 wireless MAC protocol. We investigate the
performance of the TCP and UDP assuming three different traffic patterns. First
bulk transmission where the main concern is the throughput. Second real-time
audio (using UDP) in the existence of bulk TCP transmission where the main
concern is the packet loss for audio traffic. Finally web traffic where the
main concern is the response time. We also investigate the effect of using
forward Error Correction (FEC) technique and the MAC sublayer parameters on the
throughput and response time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311036</id><created>2003-11-24</created><authors><author><keyname>Surendran</keyname><forenames>Dinoj</forenames></author><author><keyname>Niyogi</keyname><forenames>Partha</forenames></author></authors><title>Measuring the Functional Load of Phonological Contrasts</title><categories>cs.CL</categories><comments>28 pages, 3 figures, submitted to Computational Linguistics,
  currently available as a tech report on
  http://www.cs.uchicago.edu/research/publications/techreports</comments><report-no>TR-2003-12</report-no><acm-class>I.2.6;I.2.7;J.5</acm-class><abstract>  Frequency counts are a measure of how much use a language makes of a
linguistic unit, such as a phoneme or word. However, what is often important is
not the units themselves, but the contrasts between them. A measure is
therefore needed for how much use a language makes of a contrast, i.e. the
functional load (FL) of the contrast. We generalize previous work in
linguistics and speech recognition and propose a family of measures for the FL
of several phonological contrasts, including phonemic oppositions, distinctive
features, suprasegmentals, and phonological rules. We then test it for
robustness to changes of corpora. Finally, we provide examples in Cantonese,
Dutch, English, German and Mandarin, in the context of historical linguistics,
language acquisition and speech recognition. More information can be found at
http://dinoj.info/research/fload
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311037</id><created>2003-11-24</created><authors><author><keyname>Searle</keyname><forenames>Aaron</forenames></author><author><keyname>Gough</keyname><forenames>John</forenames></author><author><keyname>Abramson</keyname><forenames>David</forenames></author></authors><title>DUCT: An Interactive Define-Use Chain Navigation Tool for Relative
  Debugging</title><categories>cs.SE</categories><comments>7 pages</comments><acm-class>D.2.5</acm-class><journal-ref>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated De-bugging (AADEBUG 2003), September
  2003, Ghent</journal-ref><abstract>  This paper describes an interactive tool that facilitates following
define-use chains in large codes. The motivation for the work is to support
relative debugging, where it is necessary to iteratively refine a set of
asser-tions between different versions of a program. DUCT is novel because it
exploits the Microsoft Intermediate Language (MSIL) that underpins the .NET
Framework. Accordingly, it works on a wide range of programming languages
without any modification. The paper describes the design and implementation of
DUCT, and then illustrates its use with a small case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311038</id><created>2003-11-25</created><authors><author><keyname>May</keyname><forenames>Wolfgang</forenames></author></authors><title>XPath-Logic and XPathLog: A Logic-Programming Style XML Data
  Manipulation Language</title><categories>cs.DB</categories><acm-class>H.2; D.3</acm-class><abstract>  We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a
clear, declarative language for querying and manipulating XML whose
perspectives are especially in XML data integration. In our characterization,
the formal semantics is defined wrt. an edge-labeled graph-based model which
covers the XML data model. We give a complete, logic-based characterization of
XML data and the main language concept for XML, XPath. XPath-Logic extends the
XPath language with variable bindings and embeds it into first-order logic.
XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,
rule-based language for querying and manipulating XML data. The model-theoretic
semantics of XPath-Logic serves as the base of XPathLog as a logic-programming
language, whereas also an equivalent answer-set semantics for evaluating
XPathLog queries is given. In contrast to other approaches, the XPath syntax
and semantics is also used for a declarative specification how the database
should be updated: when used in rule heads, XPath filters are interpreted as
specifications of elements and properties which should be added to the
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311039</id><created>2003-11-26</created><updated>2004-04-15</updated><authors><author><keyname>Chen</keyname><forenames>Zhide</forenames></author><author><keyname>Zhu</keyname><forenames>Hong</forenames></author></authors><title>Quantum m-out-of-n Oblivious Transfer</title><categories>cs.CR quant-ph</categories><comments>To Appear in The Ninth IEEE Symposium On Computers And Communications
  (ISCC'2004)</comments><acm-class>C.2.2</acm-class><abstract>  In the m-out-of-n oblivious transfer (OT) model, one party Alice sends n bits
to another party Bob, Bob can get only m bits from the n bits. However, Alice
cannot know which m bits Bob received. Y.Mu[MJV02]} and Naor[Naor01] presented
classical m-out-of-n oblivious transfer based on discrete logarithm. As the
work of Shor [Shor94], the discrete logarithm can be solved in polynomial time
by quantum computers, so such OTs are unsafe to the quantum computer. In this
paper, we construct a quantum m-out-of-n OT (QOT) scheme based on the
transmission of polarized light and show that the scheme is robust to general
attacks, i.e. the QOT scheme satisfies statistical correctness and statistical
privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311040</id><created>2003-11-25</created><authors><author><keyname>Somogyi</keyname><forenames>Zoltan</forenames></author></authors><title>Idempotent I/O for safe time travel</title><categories>cs.PL cs.SE</categories><comments>In M. Ronsse, K. De Bosschere (eds), proceedings of the Fifth
  International Workshop on Automated Debugging (AADEBUG 2003), September 2003,
  Ghent. cs.SE/0309027</comments><acm-class>D.2.5</acm-class><abstract>  Debuggers for logic programming languages have traditionally had a capability
most other debuggers did not: the ability to jump back to a previous state of
the program, effectively travelling back in time in the history of the
computation. This ``retry'' capability is very useful, allowing programmers to
examine in detail a part of the computation that they previously stepped over.
Unfortunately, it also creates a problem: while the debugger may be able to
restore the previous values of variables, it cannot restore the part of the
program's state that is affected by I/O operations. If the part of the
computation being jumped back over performs I/O, then the program will perform
these I/O operations twice, which will result in unwanted effects ranging from
the benign (e.g. output appearing twice) to the fatal (e.g. trying to close an
already closed file). We present a simple mechanism for ensuring that every I/O
action called for by the program is executed at most once, even if the
programmer asks the debugger to travel back in time from after the action to
before the action. The overhead of this mechanism is low enough and can be
controlled well enough to make it practical to use it to debug computations
that do significant amounts of I/O.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311041</id><created>2003-11-26</created><authors><author><keyname>Petrovic</keyname><forenames>Milenko</forenames></author><author><keyname>Burcea</keyname><forenames>Ioana</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author></authors><title>S-ToPSS: Semantic Toronto Publish/Subscribe System</title><categories>cs.DC cs.DB</categories><comments>4 pages, 2 figures, VLDB 2003 demo (29th International Conference on
  Very Large Databases)</comments><acm-class>C.2.4</acm-class><abstract>  The increase in the amount of data on the Internet has led to the development
of a new generation of applications based on selective information
dissemination where, data is distributed only to interested clients. Such
applications require a new middleware architecture that can efficiently match
user interests with available information. Middleware that can satisfy this
requirement include event-based architectures such as publish-subscribe
systems. In this demonstration paper we address the problem of semantic
matching. We investigate how current publish/subscribe systems can be extended
with semantic capabilities. Our main contribution is the development and
validation (through demonstration) of a semantic pub/sub system prototype
S-ToPSS (Semantic Toronto Publish/Subscribe System).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311042</id><created>2003-11-27</created><authors><author><keyname>Klivans</keyname><forenames>Adam R.</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Toward Attribute Efficient Learning Algorithms</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><abstract>  We make progress on two important problems regarding attribute efficient
learnability.
  First, we give an algorithm for learning decision lists of length $k$ over
$n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time
$n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision
lists that has both subexponential sample complexity and subexponential running
time in the relevant parameters. Our approach establishes a relationship
between attribute efficient learning and polynomial threshold functions and is
based on a new construction of low degree, low weight polynomial threshold
functions for decision lists. For a wide range of parameters our construction
matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives
an essentially optimal tradeoff between polynomial threshold function degree
and weight.
  Second, we give an algorithm for learning an unknown parity function on $k$
out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.
For $k=o(\log n)$ this yields a polynomial time algorithm with sample
complexity $o(n)$. This is the first polynomial time algorithm for learning
parity on a superconstant number of variables with sublinear sample complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311043</id><created>2003-11-27</created><authors><author><keyname>Fioravanti</keyname><forenames>F.</forenames></author><author><keyname>Pettorossi</keyname><forenames>A.</forenames></author><author><keyname>Proietti</keyname><forenames>M.</forenames></author></authors><title>Combining Logic Programs and Monadic Second Order Logics by Program
  Transformation</title><categories>cs.PL cs.LO</categories><comments>25 pages. Full version of a paper that appears in: M. Leuschel (ed.)
  Proceedings of LOPSTR'02, Twelfth International Workshop on Logic-based
  Program Development and Transformation, Madrid, Spain, 17-20 Sept. 2002.
  Lecture Notes in Computer Science 2664. Springer-Verlag Berlin Heidelberg,
  2003, pp. 160-181</comments><acm-class>D.1.2;D.1.6;I.2.2;F.3.1</acm-class><abstract>  We present a program synthesis method based on unfold/fold transformation
rules which can be used for deriving terminating definite logic programs from
formulas of the Weak Monadic Second Order theory of one successor (WS1S). This
synthesis method can also be used as a proof method which is a decision
procedure for closed formulas of WS1S. We apply our synthesis method for
translating CLP(WS1S) programs into logic programs and we use it also as a
proof method for verifying safety properties of infinite state systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311044</id><created>2003-11-27</created><authors><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author><author><keyname>Renault</keyname><forenames>Sophie</forenames></author></authors><title>Derivation of Efficient Logic Programs by Specialization and Reduction
  of Nondeterminism</title><categories>cs.PL cs.LO</categories><comments>74 pages. To appear in: Higher-Order and Symbolic Computation
  (Special Issue in Honor of Bob Paige)</comments><acm-class>D.1.2;D.1.6;I.2.2;F.3.1</acm-class><abstract>  Program specialization is a program transformation methodology which improves
program efficiency by exploiting the information about the input data which are
available at compile time. We show that current techniques for program
specialization based on partial evaluation do not perform well on
nondeterministic logic programs. We then consider a set of transformation rules
which extend the ones used for partial evaluation, and we propose a strategy
for guiding the application of these extended rules so to derive very efficient
specialized programs. The efficiency improvements which sometimes are
exponential, are due to the reduction of nondeterminism and to the fact that
the computations which are performed by the initial programs in different
branches of the computation trees, are performed by the specialized programs
within single branches. In order to reduce nondeterminism we also make use of
mode information for guiding the unfolding process. To exemplify our technique,
we show that we can automatically derive very efficient matching programs and
parsers for regular languages. The derivations we have performed could not have
been done by previously known partial evaluation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311045</id><created>2003-11-27</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Unsupervised Grammar Induction in a Framework of Information Compression
  by Multiple Alignment, Unification and Search</title><categories>cs.AI</categories><acm-class>I.2.6</acm-class><journal-ref>Proceedings of the Workshop and Tutorial on Learning Context-Free
  Grammars (in association with the 14th European Conference on Machine
  Learning and the 7th European Conference on Principles and Practice of
  Knowledge Discovery in Databases (ECML/PKDD 2003), September 2003,
  Cavtat-Dubrovnik, Croata), editors: C. de la Higuera and P. Adriaans and M.
  van Zaanen and J. Oncina, pp 113-124</journal-ref><abstract>  This paper describes a novel approach to grammar induction that has been
developed within a framework designed to integrate learning with other aspects
of computing, AI, mathematics and logic. This framework, called &quot;information
compression by multiple alignment, unification and search&quot; (ICMAUS), is founded
on principles of Minimum Length Encoding pioneered by Solomonoff and others.
Most of the paper describes SP70, a computer model of the ICMAUS framework that
incorporates processes for unsupervised learning of grammars. An example is
presented to show how the model can infer a plausible grammar from appropriate
input. Limitations of the current model and how they may be overcome are
briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311046</id><created>2003-11-27</created><authors><author><keyname>Odelstad</keyname><forenames>Jan</forenames></author><author><keyname>Boman</keyname><forenames>Magnus</forenames></author></authors><title>Algebras for Agent Norm-Regulation</title><categories>cs.LO</categories><comments>25 pages</comments><acm-class>I.2.11</acm-class><abstract>  An abstract architecture for idealized multi-agent systems whose behaviour is
regulated by normative systems is developed and discussed. Agent choices are
determined partially by the preference ordering of possible states and
partially by normative considerations: The agent chooses that act which leads
to the best outcome of all permissible actions. If an action is non-permissible
depends on if the result of performing that action leads to a state satisfying
a condition which is forbidden, according to the norms regulating the
multi-agent system. This idea is formalized by defining set-theoretic
predicates characterizing multi-agent systems. The definition of the predicate
uses decision theory, the Kanger-Lindahl theory of normative positions, and an
algebraic representation of normative systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311047</id><created>2003-11-27</created><authors><author><keyname>Burcea</keyname><forenames>Ioana</forenames></author><author><keyname>Petrovic</keyname><forenames>Milenko</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author></authors><title>I know what you mean: semantic issues in Internet-scale
  publish/subscribe systems</title><categories>cs.DC cs.DB</categories><comments>12 pages, 1 figure, SWDB 2003 (1st Workshop on Semantic Web and
  Databases)</comments><acm-class>C.2.4</acm-class><abstract>  In recent years, the amount of information on the Internet has increased
exponentially developing great interest in selective information dissemination
systems. The publish/subscribe paradigm is particularly suited for designing
systems for routing information and requests according to their content
throughout wide-area network of brokers. Current publish/subscribe systems use
limited syntax-based content routing but since publishers and subscribers are
anonymous and decoupled in time, space and location, often over wide-area
network boundary, they do not necessarily speak the same language.
Consequently, adding semantics to current publish/subscribe systems is
important. In this paper we identify and examine the issues in developing
semantic-based content routing for publish/subscribe broker networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311048</id><created>2003-11-27</created><authors><author><keyname>Kumar</keyname><forenames>Deept</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Potts</keyname><forenames>Malcolm</forenames></author><author><keyname>Helm</keyname><forenames>Richard F.</forenames></author></authors><title>Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions</title><categories>cs.CE cs.AI</categories><acm-class>H.2.8</acm-class><abstract>  We present an unusual algorithm involving classification trees where two
trees are grown in opposite directions so that they are matched at their
leaves. This approach finds application in a new data mining task we formulate,
called &quot;redescription mining&quot;. A redescription is a shift-of-vocabulary, or a
different way of communicating information about a given subset of data; the
goal of redescription mining is to find subsets of data that afford multiple
descriptions. We highlight the importance of this problem in domains such as
bioinformatics, which exhibit an underlying richness and diversity of data
descriptors (e.g., genes can be studied in a variety of ways). Our approach
helps integrate multiple forms of characterizing datasets, situates the
knowledge gained from one dataset in the context of others, and harnesses
high-level abstractions for uncovering cryptic and subtle features of data.
Algorithm design decisions, implementation details, and experimental results
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311049</id><created>2003-11-27</created><authors><author><keyname>Petrovic</keyname><forenames>Milenko</forenames></author><author><keyname>Aboelaze</keyname><forenames>Mokhtar</forenames></author></authors><title>Performance of TCP/UDP under Ad Hoc IEEE802.11</title><categories>cs.NI cs.PF</categories><comments>9 pages, 5 figures, ICT 2003 (10th International Conference on
  Telecommunication)</comments><acm-class>C.2.1</acm-class><abstract>  TCP is the De facto standard for connection oriented transport layer
protocol, while UDP is the De facto standard for transport layer protocol,
which is used with real time traffic for audio and video. Although there have
been many attempts to measure and analyze the performance of the TCP protocol
in wireless networks, very few research was done on the UDP or the interaction
between TCP and UDP traffic over the wireless link. In this paper, we tudy the
performance of TCP and UDP over IEEE802.11 ad hoc network. We used two
topologies, a string and a mesh topology. Our work indicates that IEEE802.11 as
a ad-hoc network is not very suitable for bulk transfer using TCP. It also
indicates that it is much better for real-time audio. Although one has to be
careful here since real-time audio does require much less bandwidth than the
wireless link bandwidth. Careful and detailed studies are needed to further
clarify that issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311050</id><created>2003-11-27</created><authors><author><keyname>Voskob</keyname><forenames>Max</forenames></author><author><keyname>Punin</keyname><forenames>Nuck</forenames></author></authors><title>Data mining and Privacy in Public Sector using Intelligent Agents
  (discussion paper)</title><categories>cs.CY cs.AI cs.IR cs.MA</categories><comments>12 pages, 5 figures, scope limited to high level concepts</comments><acm-class>C.2.4; D.2.11; H.1.1; H.1.2; H.3.5; I.2.11; K.4.1</acm-class><abstract>  The public sector comprises government agencies, ministries, education
institutions, health providers and other types of government, commercial and
not-for-profit organisations. Unlike commercial enterprises, this environment
is highly heterogeneous in all aspects. This forms a complex network which is
not always optimised. A lack of optimisation and communication hinders
information sharing between the network nodes limiting the flow of information.
Another limiting aspect is privacy of personal information and security of
operations of some nodes or segments of the network. Attempts to reorganise the
network or improve communications to make more information available for
sharing and analysis may be hindered or completely halted by public concerns
over privacy, political agendas, social and technological barriers. This paper
discusses a technical solution for information sharing while addressing the
privacy concerns with no need for reorganisation of the existing public sector
infrastructure . The solution is based on imposing an additional layer of
Intelligent Software Agents and Knowledge Bases for data mining and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311051</id><created>2003-11-27</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>Integrating existing cone-shaped and projection-based cardinal direction
  relations and a TCSP-like decidable generalisation</title><categories>cs.AI</categories><comments>I should be able to provide a longer version soon. A shorter version
  has been submitted to the conference KR'2004</comments><acm-class>I.2 (I.2.4)</acm-class><abstract>  We consider the integration of existing cone-shaped and projection-based
calculi of cardinal direction relations, well-known in QSR. The more general,
integrating language we consider is based on convex constraints of the
qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal
direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$,
with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning
of the quantitative constraint, in particular, is that point $x$ belongs to the
(convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and
$\beta$. The general form of a constraint is a disjunction of the form
$[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha
_{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha
_i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above:
the meaning of such a general constraint is that, for some $i=1... n_1$,
$r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A
conjunction of such general constraints is a $\tcsp$-like CSP, which we will
refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective
solution search algorithm for an $\scsp$ will be described, which uses (1)
constraint propagation, based on a composition operation to be defined, as the
filtering method during the search, and (2) the Simplex algorithm, guaranteeing
completeness, at the leaves of the search tree. The approach is particularly
suited for large-scale high-level vision, such as, e.g., satellite-like
surveillance of a geographic area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311052</id><created>2003-11-28</created><updated>2004-04-02</updated><authors><author><keyname>Wen-Yu</keyname><forenames>Dong</forenames></author><author><keyname>Ke</keyname><forenames>Xu</forenames></author><author><keyname>Meng-Xiang</keyname><forenames>Lin</forenames></author></authors><title>A Situation Calculus-based Approach To Model Ubiquitous Information
  Services</title><categories>cs.AI cs.HC</categories><acm-class>I.2.0;H.1.2</acm-class><abstract>  This paper presents an augmented situation calculus-based approach to model
autonomous computing paradigm in ubiquitous information services. To make it
practical for commercial development and easier to support autonomous paradigm
imposed by ubiquitous information services, we made improvements based on
Reiter's standard situation calculus. First we explore the inherent
relationship between fluents and evolution: since not all fluents contribute to
systems' evolution and some fluents can be derived from some others, we define
those fluents that are sufficient and necessary to determine evolutional
potential as decisive fluents, and then we prove that their successor states
wrt to deterministic complex actions satisfy Markov property. Then, within the
calculus framework we build, we introduce validity theory to model the
autonomous services with application-specific validity requirements, including:
validity fluents to axiomatize validity requirements, heuristic multiple
alternative service choices ranging from complete acceptance, partial
acceptance, to complete rejection, and validity-ensured policy to comprise such
alternative service choices into organic, autonomously-computable services. Our
approach is demonstrated by a ubiquitous calendaring service, ACS, throughout
the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311053</id><created>2003-11-28</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author></authors><title>Weak Bezout inequality for D-modules</title><categories>cs.SC cs.CC</categories><comments>10 pages</comments><acm-class>I.1.2</acm-class><abstract>  Let $\{w_{i,j}\}_{1\leq i\leq n, 1\leq j\leq s} \subset
L_m=F(X_1,...,X_m)[{\partial \over \partial X_1},..., {\partial \over \partial
X_m}]$ be linear partial differential operators of orders with respect to
${\partial \over \partial X_1},..., {\partial \over \partial X_m}$ at most $d$.
We prove an upper bound n(4m^2d\min\{n,s\})^{4^{m-t-1}(2(m-t))} on the leading
coefficient of the Hilbert-Kolchin polynomial of the left $L_m$-module
$&lt;\{w_{1,j}, ..., w_{n,j}\}_{1\leq j \leq s} &gt; \subset L_m^n$ having the
differential type $t$ (also being equal to the degree of the Hilbert-Kolchin
polynomial). The main technical tool is the complexity bound on solving systems
of linear equations over {\it algebras of fractions} of the form
$$L_m(F[X_1,..., X_m, {\partial \over \partial X_1},..., {\partial \over
\partial X_k}])^{-1}.$$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0311054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0311054</id><created>2003-11-28</created><authors><author><keyname>Galbi</keyname><forenames>Douglas A.</forenames></author></authors><title>Copyright and Creativity: Authors and Photographers</title><categories>cs.CY cs.DL</categories><comments>Part of a larger work, &quot;Sense in Communication,&quot; available at
  http://www.galbithink.org That work includes material on &quot;human/computer
  interaction&quot; and &quot;neurons and cognition.&quot;</comments><acm-class>K.5.1</acm-class><abstract>  The history of the occupations &quot;author&quot; and &quot;photographer&quot; provides an
insightful perspective on copyright and creativity. The concept of the romantic
author, associated with personal creative genius, gained prominence in the
eighteenth century. However, in the U.S. in 1900 only about three thousand
persons professed their occupation to be &quot;author.&quot; Self-professed
&quot;photographers&quot; were then about ten times as numerous as authors. Being a
photographer was associated with manufacturing and depended only on mastering
technical skills and making a living. Being an author, in contrast, was an
elite status associated with science and literature. Across the twentieth
century, the number of writers and authors grew much more rapidly than the
number of photographers. The relative success of writers and authors in
creating jobs seems to have depended not on differences in copyright or
possibilities for self-production, but on greater occupational innovation.
Creativity in organizing daily work is an important form of creativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312001</id><created>2003-11-29</created><updated>2006-03-30</updated><authors><author><keyname>Lisewski</keyname><forenames>A. M.</forenames></author></authors><title>The concept of strong and weak virtual reality</title><categories>cs.LO nlin.AO physics.comp-ph</categories><comments>17 pages; several edits in v2</comments><acm-class>F.4.1; I.2.4; H.1.2; H.5.1</acm-class><journal-ref>Minds and Machines, 16 (2), 201-219 (2006)</journal-ref><doi>10.1007/s11023-006-9037-z</doi><abstract>  We approach the virtual reality phenomenon by studying its relationship to
set theory, and we investigate the case where this is done using the
wellfoundedness property of sets. Our hypothesis is that non-wellfounded sets
(hypersets) give rise to a different quality of virtual reality than do
familiar wellfounded sets. We initially provide an alternative approach to
virtual reality based on Sommerhoff's idea of first and second order
self-awareness; both categories of self-awareness are considered as necessary
conditions for consciousness in terms of higher cognitive functions. We then
introduce a representation of first and second order self-awareness through
sets, and assume that these sets, which we call events, originally form a
collection of wellfounded sets. Strong virtual reality characterizes virtual
reality environments which have the limited capacity to create only events
associated with wellfounded sets. In contrast, the more general concept of weak
virtual reality characterizes collections of virtual reality mediated events
altogether forming an entirety larger than any collection of wellfounded sets.
By giving reference to Aczel's hyperset theory we indicate that this definition
is not empty, because hypersets encompass wellfounded sets already. Moreover,
we argue that weak virtual reality could be realized in human history through
continued progress in computer technology. Finally, we reformulate our
characterization into a more general framework, and use Baltag's Structural
Theory of Sets (STS) to show that within this general hyperset theory
Sommerhoff's first and second order self-awareness as well as both concepts of
virtual reality admit a consistent mathematical representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312002</id><created>2003-12-01</created><authors><author><keyname>Bruscoli</keyname><forenames>Paola</forenames></author><author><keyname>Guglielmi</keyname><forenames>Alessio</forenames></author></authors><title>On Structuring Proof Search for First Order Linear Logic</title><categories>cs.LO</categories><comments>Author website at http://alessio.guglielmi.name/res/</comments><report-no>Technical Report WV-03-10 TU Dresden</report-no><acm-class>F.4.1</acm-class><abstract>  Full first order linear logic can be presented as an abstract logic
programming language in Miller's system Forum, which yields a sensible
operational interpretation in the 'proof search as computation' paradigm.
However, Forum still has to deal with syntactic details that would normally be
ignored by a reasonable operational semantics. In this respect, Forum improves
on Gentzen systems for linear logic by restricting the language and the form of
inference rules. We further improve on Forum by restricting the class of
formulae allowed, in a system we call G-Forum, which is still equivalent to
full first order linear logic. The only formulae allowed in G-Forum have the
same shape as Forum sequents: the restriction does not diminish expressiveness
and makes G-Forum amenable to proof theoretic analysis. G-Forum consists of two
(big) inference rules, for which we show a cut elimination procedure. This does
not need to appeal to finer detail in formulae and sequents than is provided by
G-Forum, thus successfully testing the internal symmetries of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312003</id><created>2003-11-29</created><authors><author><keyname>Sazonov</keyname><forenames>E. S.</forenames></author><author><keyname>Klinkhachorn</keyname><forenames>P.</forenames></author><author><keyname>Klein</keyname><forenames>R. L.</forenames></author></authors><title>Hybrid LQG-Neural Controller for Inverted Pendulum System</title><categories>cs.NE cs.LG</categories><acm-class>I.2.6;C.1.3;I.5.1</acm-class><journal-ref>Proceedings of 35th Southeastern Symposium on System Theory
  (SSST), Morgantown, WV, March 2003</journal-ref><abstract>  The paper presents a hybrid system controller, incorporating a neural and an
LQG controller. The neural controller has been optimized by genetic algorithms
directly on the inverted pendulum system. The failure free optimization process
stipulated a relatively small region of the asymptotic stability of the neural
controller, which is concentrated around the regulation point. The presented
hybrid controller combines benefits of a genetically optimized neural
controller and an LQG controller in a single system controller. High quality of
the regulation process is achieved through utilization of the neural
controller, while stability of the system during transient processes and a wide
range of operation are assured through application of the LQG controller. The
hybrid controller has been validated by applying it to a simulation model of an
inherently unstable system of inverted pendulum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312004</id><created>2003-11-30</created><authors><author><keyname>Etzold</keyname><forenames>Daniel</forenames></author></authors><title>Improving spam filtering by combining Naive Bayes with simple k-nearest
  neighbor searches</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><abstract>  Using naive Bayes for email classification has become very popular within the
last few months. They are quite easy to implement and very efficient. In this
paper we want to present empirical results of email classification using a
combination of naive Bayes and k-nearest neighbor searches. Using this
technique we show that the accuracy of a Bayes filter can be improved slightly
for a high number of features and significantly for a small number of features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312005</id><created>2003-12-02</created><updated>2004-03-02</updated><authors><author><keyname>Huertas-Rosero</keyname><forenames>Alvaro Francisco</forenames></author></authors><title>A Cartography for 2x2 Symmetric Games</title><categories>cs.GT</categories><comments>13 pages, 7 figures This is a new version of the work, adapted to be
  presented in the III Colombian Congress and I Andean International Conference
  of Operational Research (Cartagena, Colombia, March 2004)</comments><acm-class>I.2.1</acm-class><abstract>  A bidimensional representation of the space of 2x2 Symmetric Games in the
strategic representation is proposed. This representation provides a tool for
the classification of 2x2 symmetric games, quantification of the fraction of
them having a certain feature, and predictions of changes in the
characteristics of a game when a change in done on the payoff matrix that
defines it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312006</id><created>2003-12-02</created><authors><author><keyname>Tomov</keyname><forenames>S.</forenames><affiliation>Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</affiliation></author><author><keyname>McGuigan</keyname><forenames>M.</forenames><affiliation>Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</affiliation></author><author><keyname>Bennett</keyname><forenames>R.</forenames><affiliation>Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</affiliation></author><author><keyname>Smith</keyname><forenames>G.</forenames><affiliation>Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</affiliation></author><author><keyname>Spiletic</keyname><forenames>J.</forenames><affiliation>Brookhaven National Laboratory, Data Analysis and Visualization, Upton, NY</affiliation></author></authors><title>Benchmarking and Implementation of Probability-Based Simulations on
  Programmable Graphics Cards</title><categories>cs.GR cs.PF</categories><comments>16 pages, 5 figures</comments><acm-class>I.6.3; I.3.1; B.8.2</acm-class><abstract>  The latest Graphics Processing Units (GPUs) are reported to reach up to
 200 billion floating point operations per second (200 Gflops) and to have
price performance of 0.1 cents per M flop. These facts raise great interest in
the plausibility of extending the GPUs' use to non-graphics applications, in
particular numerical simulations on structured grids (lattice).
 We review previous work on using GPUs for non-graphics applications, implement
probability-based simulations on the GPU, namely the
 Ising and percolation models, implement vector operation benchmarks for the
GPU, and finally compare the CPU's and GPU's performance.
 A general conclusion from the results obtained is that moving computations
from the CPU to the GPU is feasible, yielding good time and price performance,
for certain lattice computations.
 Preliminary results also show that it is feasible to use them in parallel
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312007</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312007</id><created>2003-12-02</created><authors><author><keyname>Buergisser</keyname><forenames>Peter</forenames></author><author><keyname>Cucker</keyname><forenames>Felipe</forenames></author></authors><title>Counting complexity classes for numeric computations II: algebraic and
  semialgebraic sets</title><categories>cs.CC math.AT</categories><comments>54 pages, 2 figures</comments><acm-class>F.1.3; I1.2</acm-class><journal-ref>Journal of Complexity 22(2): 147-191 (2006)</journal-ref><abstract>  We define counting classes #P_R and #P_C in the Blum-Shub-Smale setting of
computations over the real or complex numbers, respectively. The problems of
counting the number of solutions of systems of polynomial inequalities over R,
or of systems of polynomial equalities over C, respectively, turn out to be
natural complete problems in these classes. We investigate to what extent the
new counting classes capture the complexity of computing basic topological
invariants of semialgebraic sets (over R) and algebraic sets (over C). We prove
that the problem of computing the (modified) Euler characteristic of
semialgebraic sets is FP_R^{#P_R}-complete, and that the problem of computing
the geometric degree of complex algebraic sets is FP_C^{#P_C}-complete. We also
define new counting complexity classes in the classical Turing model via taking
Boolean parts of the classes above, and show that the problems to compute the
Euler characteristic and the geometric degree of (semi)algebraic sets given by
integer polynomials are complete in these classes. We complement the results in
the Turing model by proving, for all k in N, the FPSPACE-hardness of the
problem of computing the k-th Betti number of the zet of real zeros of a given
integer polynomial. This holds with respect to the singular homology as well as
for the Borel-Moore homology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312008</id><created>2003-12-03</created><authors><author><keyname>Kraaij</keyname><forenames>Wessel</forenames></author><author><keyname>Nie</keyname><forenames>Jian-Yun</forenames></author><author><keyname>Simard</keyname><forenames>Michel</forenames></author></authors><title>Embedding Web-based Statistical Translation Models in Cross-Language
  Information Retrieval</title><categories>cs.CL cs.IR</categories><comments>37 pages</comments><acm-class>H.3.1; H.3.3; I.2.7</acm-class><journal-ref>Computational Linguistics 29(3) september 2003</journal-ref><abstract>  Although more and more language pairs are covered by machine translation
services, there are still many pairs that lack translation resources.
Cross-language information retrieval (CLIR) is an application which needs
translation functionality of a relatively low level of sophistication since
current models for information retrieval (IR) are still based on a
bag-of-words. The Web provides a vast resource for the automatic construction
of parallel corpora which can be used to train statistical translation models
automatically. The resulting translation models can be embedded in several ways
in a retrieval model. In this paper, we will investigate the problem of
automatically mining parallel texts from the Web and different ways of
integrating the translation models within the retrieval process. Our
experiments on standard test collections for CLIR show that the Web-based
translation models can surpass commercial MT systems in CLIR tasks. These
results open the perspective of constructing a fully automatic query
translation device for CLIR at a very low cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312009</id><created>2003-12-03</created><authors><author><keyname>Sazonov</keyname><forenames>E. S.</forenames></author><author><keyname>Del Gobbo</keyname><forenames>D.</forenames></author><author><keyname>Klinkhachorn</keyname><forenames>P.</forenames></author><author><keyname>Klein</keyname><forenames>R. L.</forenames></author></authors><title>Failure-Free Genetic Algorithm Optimization of a System Controller Using
  SAFE/LEARNING Controllers in Tandem</title><categories>cs.NE cs.LG</categories><acm-class>I.2.6;C.1.3;I.5.1</acm-class><journal-ref>Proceedings of 34th Southeastern Symposium on System Theory
  (SSST), Huntsville, AL, March 2002</journal-ref><abstract>  The paper presents a method for failure free genetic algorithm optimization
of a system controller. Genetic algorithms present a powerful tool that
facilitates producing near-optimal system controllers. Applied to such methods
of computational intelligence as neural networks or fuzzy logic, these methods
are capable of combining the non-linear mapping capabilities of the latter with
learning the system behavior directly, that is, without a prior model. At the
same time, genetic algorithms routinely produce solutions that lead to the
failure of the controlled system. Such solutions are generally unacceptable for
applications where safe operation must be guaranteed. We present here a method
of design, which allows failure-free application of genetic algorithms through
utilization of SAFE and LEARNING controllers in tandem, where the SAFE
controller recovers the system from dangerous states while the LEARNING
controller learns its behavior. The method has been validated by applying it to
an inherently unstable system of inverted pendulum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312010</id><created>2003-12-04</created><authors><author><keyname>McDevitt</keyname><forenames>Kathleen</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Padilla-Falto</keyname><forenames>Olga I.</forenames></author></authors><title>Designing of a Community-based Translation Center</title><categories>cs.HC cs.DL</categories><comments>8 pages, 4 figures</comments><report-no>TR-03-30</report-no><acm-class>H.5.2; H.3.7</acm-class><abstract>  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312011</id><created>2003-12-05</created><authors><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author></authors><title>Constraint Optimization and Statistical Mechanics</title><categories>cs.CC cond-mat.dis-nn cs.DS</categories><comments>22 pages, 1 figure Lectures given at the Varenna summer school</comments><acm-class>G.3; G.2.1 G.3, G.2.1 G.3, G.2.1</acm-class><abstract>  In these lectures I will present an introduction to the results that have
been recently obtained in constraint optimization of random problems using
statistical mechanics techniques. After presenting the general results, in
order to simplify the presentation I will describe in details only the problems
related to the coloring of a random graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312012</id><created>2003-12-05</created><authors><author><keyname>Matlin</keyname><forenames>Olga Shumsky</forenames></author><author><keyname>McCune</keyname><forenames>William</forenames></author><author><keyname>Lusk</keyname><forenames>Ewing</forenames></author></authors><title>Methods to Model-Check Parallel Systems Software</title><categories>cs.LO cs.DC</categories><comments>12 pages, 3 figures, 1 table</comments><report-no>ANL/MCS-TM-261</report-no><acm-class>D.2.4; D.1.3</acm-class><abstract>  We report on an effort to develop methodologies for formal verification of
parts of the Multi-Purpose Daemon (MPD) parallel process management system. MPD
is a distributed collection of communicating processes. While the individual
components of the collection execute simple algorithms, their interaction leads
to unexpected errors that are difficult to uncover by conventional means. Two
verification approaches are discussed here: the standard model checking
approach using the software model checker SPIN and the nonstandard use of a
general-purpose first-order resolution-style theorem prover OTTER to conduct
the traditional state space exploration. We compare modeling methodology and
analyze performance and scalability of the two methods with respect to
verification of MPD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312013</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312013</id><created>2003-12-06</created><updated>2012-04-12</updated><authors><author><keyname>Jurkovic</keyname><forenames>F.</forenames></author></authors><title>Fuzziness versus probability again</title><categories>cs.LO</categories><comments>Withdrawn by the author</comments><acm-class>G.3;I2.3</acm-class><abstract>  A construction of a fuzzy logic controller based on an analogy between fuzzy
conditional rule of inference and marginal probability in terms of the
conditional probability function has been proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312014</id><created>2003-12-07</created><updated>2005-03-16</updated><authors><author><keyname>Yorsh</keyname><forenames>G.</forenames></author><author><keyname>Reps</keyname><forenames>T.</forenames></author><author><keyname>Sagiv</keyname><forenames>M.</forenames></author><author><keyname>Wilhelm</keyname><forenames>R.</forenames></author></authors><title>Logical Characterizations of Heap Abstractions</title><categories>cs.LO</categories><acm-class>D.2.4</acm-class><abstract>  Shape analysis concerns the problem of determining &quot;shape invariants&quot; for
programs that perform destructive updating on dynamically allocated storage. In
recent work, we have shown how shape analysis can be performed, using an
abstract interpretation based on 3-valued first-order logic. In that work,
concrete stores are finite 2-valued logical structures, and the sets of stores
that can possibly arise during execution are represented (conservatively) using
a certain family of finite 3-valued logical structures. In this paper, we show
how 3-valued structures that arise in shape analysis can be characterized using
formulas in first-order logic with transitive closure.
  We also define a non-standard (&quot;supervaluational&quot;) semantics for 3-valued
first-order logic that is more precise than a conventional 3-valued semantics,
and demonstrate that the supervaluational semantics can be effectively
implemented using existing theorem provers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312015</id><created>2003-12-07</created><authors><author><keyname>Baillot</keyname><forenames>Patrick</forenames></author><author><keyname>Mogbil</keyname><forenames>Virgile</forenames></author></authors><title>Soft lambda-calculus: a language for polynomial time computation</title><categories>cs.LO cs.CC</categories><comments>20 pages</comments><acm-class>F.4; F.4.1; F.4.2</acm-class><abstract>  Soft linear logic ([Lafont02]) is a subsystem of linear logic characterizing
the class PTIME. We introduce Soft lambda-calculus as a calculus typable in the
intuitionistic and affine variant of this logic. We prove that the (untyped)
terms of this calculus are reducible in polynomial time. We then extend the
type system of Soft logic with recursive types. This allows us to consider
non-standard types for representing lists. Using these datatypes we examine the
concrete expressivity of Soft lambda-calculus with the example of the insertion
sort algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312016</id><created>2003-12-08</created><authors><author><keyname>Perugini</keyname><forenames>Saverio</forenames></author><author><keyname>Pinney</keyname><forenames>Mary E.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Rosson</keyname><forenames>Mary Beth</forenames></author></authors><title>Taking the Initiative with Extempore: Exploring Out-of-Turn Interactions
  with Websites</title><categories>cs.HC cs.IR</categories><comments>Introduces out-of-turn interaction with websites: a technique which
  sustains, rather than curbs, the user-system dialog</comments><acm-class>H.5.2; H.5.4</acm-class><abstract>  We present the first study to explore the use of out-of-turn interaction in
websites. Out-of-turn interaction is a technique which empowers the user to
supply unsolicited information while browsing. This approach helps flexibly
bridge any mental mismatch between the user and the website, in a manner
fundamentally different from faceted browsing and site-specific search tools.
We built a user interface (Extempore) which accepts out-of-turn input via voice
or text; and employed it in a US congressional website, to determine if users
utilize out-of-turn interaction for information-finding tasks, and their
rationale for doing so. The results indicate that users are adept at discerning
when out-of-turn interaction is necessary in a particular task, and actively
interleaved it with browsing. However, users found cascading information across
information-finding subtasks challenging. Therefore, this work not only
improves our understanding of out-of-turn interaction, but also suggests
further opportunities to enrich browsing experiences for users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312017</id><created>2003-12-08</created><authors><author><keyname>Prekop</keyname><forenames>Paul</forenames></author></authors><title>An Exploratory Study of Mobile Computing Use by Knowledge Workers</title><categories>cs.HC</categories><comments>To appear in: Proceedings of the 2003 Australasian Computer Human
  Interaction Conference (OzCHI 2003), University of Queensland, Australia,
  November 26-28 2003</comments><acm-class>H1.2;H5.2;J1</acm-class><abstract>  This paper describes some preliminary results from a 20-week study on the use
of Compaq iPAQ Personal Digital Assistants (PDAs) by 10 senior developers,
analysts, technical managers, and senior organisational managers. The goal of
the study was to identify what applications were used, how and where they were
used, the problems and issues that arose, and how use of the iPAQs changed over
the study period. The paper highlights some interesting uses of the iPAQs, and
identifies some of the characteristics of successful mobile applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312018</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312018</id><created>2003-12-11</created><authors><author><keyname>Ginsparg</keyname><forenames>Paul</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Houle</keyname><forenames>Paul</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Sul</keyname><forenames>Jae-Hoon</forenames><affiliation>Cornell University</affiliation></author></authors><title>Mapping Subsets of Scholarly Information</title><categories>cs.IR cs.LG</categories><comments>10 pages, 4 figures, presented at Arthur M. Sackler Colloquium on
  &quot;Mapping Knowledge Domains&quot;, 9--11 May 2003, Beckman Center, Irvine, CA,
  proceedings to appear in PNAS</comments><acm-class>H.3.1; H.3.6; I.2.6</acm-class><doi>10.1073/pnas.0308253100</doi><abstract>  We illustrate the use of machine learning techniques to analyze, structure,
maintain, and evolve a large online corpus of academic literature. An emerging
field of research can be identified as part of an existing corpus, permitting
the implementation of a more coherent community structure for its
practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312019</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312019</id><created>2003-12-11</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author><author><keyname>Benerecetti</keyname><forenames>Massimo</forenames></author><author><keyname>Peron</keyname><forenames>Adriano</forenames></author></authors><title>Verification of recursive parallel systems</title><categories>cs.LO</categories><comments>49 pages, 1 figure</comments><acm-class>68Q60</acm-class><abstract>  In this paper we consider the problem of proving properties of infinite
behaviour of formalisms suitable to describe (infinite state) systems with
recursion and parallelism. As a formal setting, we consider the framework of
Process Rewriting Systems (PRSs). For a meaningfull fragment of PRSs, allowing
to accommodate both Pushdown Automata and Petri Nets, we state decidability
results for a class of properties about infinite derivations (infinite term
rewritings). The given results can be exploited for the automatic verification
of some classes of linear time properties of infinite state systems described
by PRSs. In order to exemplify the assessed results, we introduce a meaningful
automaton based formalism which allows to express both recursion and
multi--treading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312020</id><created>2003-12-12</created><authors><author><keyname>Henocque</keyname><forenames>Laurent</forenames></author></authors><title>Modeling Object Oriented Constraint Programs in Z</title><categories>cs.AI</categories><report-no>RR-LSIS-03-006</report-no><acm-class>I.2.4;F.4.1</acm-class><abstract>  Object oriented constraint programs (OOCPs) emerge as a leading evolution of
constraint programming and artificial intelligence, first applied to a range of
industrial applications called configuration problems. The rich variety of
technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP,
Terminological systems, constraint programs with set variables ...) is a source
of difficulty. No universally accepted formal language exists for communicating
about OOCPs, which makes the comparison of systems difficult. We present here a
Z based specification of OOCPs which avoids the falltrap of hidden object
semantics. The object system is part of the specification, and captures all of
the most advanced notions from the object oriented modeling standard UML. The
paper illustrates these issues and the conciseness and precision of Z by the
specification of a working OOCP that solves an historical AI problem : parsing
a context free grammar. Being written in Z, an OOCP specification also supports
formal proofs. The whole builds the foundation of an adaptative and evolving
framework for communicating about constrained object models and programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312021</id><created>2003-12-12</created><authors><author><keyname>Camarda</keyname><forenames>Domenico</forenames></author></authors><title>ICT-based planning and the missing educational link</title><categories>cs.CY</categories><comments>Issues and ideas reflected in this paper were partly presented by the
  Author during the III INCOSUSW Meeting (Local resources and global trades:
  Environments and agriculture in the Mediterranean region), held in Rabat,
  Morocco, from April 25 to 30, 2002, and funded by the European Union</comments><acm-class>K.4.0;k.3.2</acm-class><abstract>  The past century ended with an unexpected explosion of Information and
Communication Technologies (ICT), both in planning/managing public policies,
and in exchanging knowledge. However, the extent to which ICT-based tools
increase the level of public knowledge, or help decision makers is still
uncertain. Although indirectly, the overload of unfiltered Web-based
information seems able to hamper the knowledge growth of people, particularly
in some developing communities, whereas Decision Support Systems (DSS) and
Geographical Information Systems (GIS) prove to be ineffective if managed by
unskilled planning bodies. Given such warns, this paper outlines how the
different social and cultural awareness of local communities can affect the
outcomes of ICT-based tools. It further explores the impacts of ICT-based tools
on community development and spatial planning, emphasizing the role of proper
literacy and education for effective management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312022</id><created>2003-12-12</created><authors><author><keyname>Soysa</keyname><forenames>Manjuka</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Nath</keyname><forenames>Baikunth</forenames></author></authors><title>GridEmail: A Case for Economically Regulated Internet-based
  Interpersonal Communications</title><categories>cs.DC</categories><comments>15 pages, 10 figures, A Technical Report from Grid Computing and
  Distributed Systems Laboratory, University of Melbourne, Australia</comments><report-no>GRIDS-TR-2003-6</report-no><acm-class>C.2.2, C.2.4</acm-class><abstract>  Email has emerged as a dominant form of electronic communication between
people. Spam is a major problem for email users, with estimates of up to 56% of
email falling into that category. Control of Spam is being attempted with
technical and legislative methods. In this paper we look at email and spam from
a supply-demand perspective. We propose Gridemail, an email system based on an
economy of communicating parties, where participants? motivations are
represented as pricing policies and profiles. This system is expected to help
people regulate their personal communications to suit their conditions, and
help in removing unwanted messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312023</id><created>2003-12-12</created><authors><author><keyname>Genaim</keyname><forenames>Samir</forenames></author><author><keyname>Codish</keyname><forenames>Michael</forenames></author></authors><title>Inferring Termination Conditions for Logic Programs using Backwards
  Analysis</title><categories>cs.PL</categories><acm-class>D.1.6,F.3.1</acm-class><abstract>  This paper focuses on the inference of modes for which a logic program is
guaranteed to terminate. This generalises traditional termination analysis
where an analyser tries to verify termination for a specified mode. Our
contribution is a methodology in which components of traditional termination
analysis are combined with backwards analysis to obtain an analyser for
termination inference. We identify a condition on the components of the
analyser which guarantees that termination inference will infer all modes which
can be checked to terminate. The application of this methodology to enhance a
traditional termination analyser to perform also termination inference is
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312024</id><created>2003-12-12</created><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>Yiping</keyname><forenames>Guo</forenames></author><author><keyname>Ming</keyname><forenames>Fang</forenames></author></authors><title>Evolution: Google vs. DRIS</title><categories>cs.DL cs.IR cs.NI</categories><comments>5 pages,6 figures</comments><acm-class>H.3.3;H.3.5;H.3.7</acm-class><abstract>  This paper gives an absolute new search system that builds the information
retrieval infrastructure for Internet. Now most search engine companies are
mainly concerned with how to make profit from company users by advertisement
and ranking prominence, but never consider what its real customers will feel.
Few web search engines can sell billions dollars just at the cost of
inconvenience of most Internet users, but not its high quality of search
service. When we have to bear the bothersome advertisements in the awful
results and have no choices, Internet as the kind of public good will surely be
undermined. If current Internet can't fully ensure our right to know, it may
need some sound improvements or a revolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312025</id><created>2003-12-14</created><authors><author><keyname>Bella</keyname><forenames>Giampaolo</forenames></author><author><keyname>Bistarelli</keyname><forenames>Stefano</forenames></author></authors><title>Soft Constraint Programming to Analysing Security Protocols</title><categories>cs.CR cs.AI</categories><comments>29 pages, To appear in Theory and Practice of Logic Programming
  (TPLP) Paper for Special Issue (Verification and Computational Logic)</comments><acm-class>D.3.3; K.6.5; D.4.6</acm-class><abstract>  Security protocols stipulate how the remote principals of a computer network
should interact in order to obtain specific security goals. The crucial goals
of confidentiality and authentication may be achieved in various forms, each of
different strength. Using soft (rather than crisp) constraints, we develop a
uniform formal notion for the two goals. They are no longer formalised as mere
yes/no properties as in the existing literature, but gain an extra parameter,
the security level. For example, different messages can enjoy different levels
of confidentiality, or a principal can achieve different levels of
authentication with different principals.
  The goals are formalised within a general framework for protocol analysis
that is amenable to mechanisation by model checking. Following the application
of the framework to analysing the asymmetric Needham-Schroeder protocol, we
have recently discovered a new attack on that protocol as a form of retaliation
by principals who have been attacked previously. Having commented on that
attack, we then demonstrate the framework on a bigger, largely deployed
protocol consisting of three phases, Kerberos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312026</id><created>2003-12-15</created><authors><author><keyname>Hruza</keyname><forenames>Jan</forenames></author><author><keyname>Stepanek</keyname><forenames>Petr</forenames></author></authors><title>Speedup of Logic Programs by Binarization and Partial Deduction</title><categories>cs.PL cs.AI</categories><comments>15 pages; to appear in Theory and Practice of Logic Programming</comments><acm-class>D.1.6;I.2.2;I.2.3;F.4.1</acm-class><abstract>  Binary logic programs can be obtained from ordinary logic programs by a
binarizing transformation. In most cases, binary programs obtained this way are
less efficient than the original programs. (Demoen, 1992) showed an interesting
example of a logic program whose computational behaviour was improved when it
was transformed to a binary program and then specialized by partial deduction.
The class of B-stratifiable logic programs is defined. It is shown that for
every B-stratifiable logic program, binarization and subsequent partial
deduction produce a binary program which does not contain variables for
continuations introduced by binarization. Such programs usually have a better
computational behaviour than the original ones. Both binarization and partial
deduction can be easily automated. A comparison with other related approaches
to program transformation is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312027</id><created>2003-12-15</created><authors><author><keyname>Vandecasteele</keyname><forenames>Henk</forenames></author><author><keyname>Janssens</keyname><forenames>Gerda</forenames></author></authors><title>An Open Ended Tree</title><categories>cs.PL</categories><acm-class>D.1.6;D.3.3;.E.2</acm-class><journal-ref>TPLP Vol 3(3) 2003 pp 377-385</journal-ref><abstract>  An open ended list is a well known data structure in Prolog programs. It is
frequently used to represent a value changing over time, while this value is
referred to from several places in the data structure of the application. A
weak point in this technique is that the time complexity is linear in the
number of updates to the value represented by the open ended list. In this
programming pearl we present a variant of the open ended list, namely an open
ended tree, with an update and access time complexity logarithmic in the number
of updates to the value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312028</id><created>2003-12-15</created><authors><author><keyname>Furfaro</keyname><forenames>Filippo</forenames></author><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Greco</keyname><forenames>Sergio</forenames></author></authors><title>Minimal founded semantics for disjunctive logic programs and deductive
  databases</title><categories>cs.LO cs.AI</categories><comments>20 pages</comments><acm-class>I.2.3; F.4.1</acm-class><journal-ref>Theory and Practice of Logic Programming, 4(1): 75-93 (2004)</journal-ref><abstract>  In this paper, we propose a variant of stable model semantics for disjunctive
logic programming and deductive databases. The semantics, called minimal
founded, generalizes stable model semantics for normal (i.e. non disjunctive)
programs but differs from disjunctive stable model semantics (the extension of
stable model semantics for disjunctive programs). Compared with disjunctive
stable model semantics, minimal founded semantics seems to be more intuitive,
it gives meaning to programs which are meaningless under stable model semantics
and is no harder to compute. More specifically, minimal founded semantics
differs from stable model semantics only for disjunctive programs having
constraint rules or rules working as constraints. We study the expressive power
of the semantics and show that for general disjunctive datalog programs it has
the same power as disjunctive stable model semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312029</id><created>2003-12-15</created><authors><author><keyname>Turner</keyname><forenames>Hudson</forenames></author></authors><title>Strong Equivalence Made Easy: Nested Expressions and Weight Constraints</title><categories>cs.LO cs.AI</categories><acm-class>D.1.6</acm-class><journal-ref>Theory and Practice of Logic Programming, vol 3 (4&amp;5), pages
  609-622, 2003</journal-ref><abstract>  Logic programs P and Q are strongly equivalent if, given any program R,
programs P union R and Q union R are equivalent (that is, have the same answer
sets). Strong equivalence is convenient for the study of equivalent
transformations of logic programs: one can prove that a local change is correct
without considering the whole program. Lifschitz, Pearce and Valverde showed
that Heyting's logic of here-and-there can be used to characterize strong
equivalence for logic programs with nested expressions (which subsume the
better-known extended disjunctive programs). This note considers a simpler,
more direct characterization of strong equivalence for such programs, and shows
that it can also be applied without modification to the weight constraint
programs of Niemela and Simons. Thus, this characterization of strong
equivalence is convenient for the study of equivalent transformations of logic
programs written in the input languages of answer set programming systems dlv
and smodels. The note concludes with a brief discussion of results that can be
used to automate reasoning about strong equivalence, including a novel encoding
that reduces the problem of deciding the strong equivalence of a pair of weight
constraint programs to that of deciding the inconsistency of a weight
constraint program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312030</id><created>2003-12-16</created><authors><author><keyname>Jia</keyname><forenames>Jiyou</forenames></author></authors><title>CSIEC (Computer Simulator in Educational Communication): An Intelligent
  Web-Based Teaching System for Foreign Language Learning</title><categories>cs.CY</categories><comments>8 pages, 4 figures, 5 tables to appear on ED-MEDIA 2004 (World
  Conference on Educational Multimedia, Hypermedia &amp; Telecommunications)</comments><acm-class>I.2.7; K.3.1</acm-class><abstract>  In this paper we present an innovative intelligent web-based computer-aided
instruction system for foreign language learning: CSIEC (Computer Simulator in
Educational Communication). This system can not only grammatically understand
the sentences in English given from the users via Internet, but also reasonably
and individually speak with the users. At first the related works in this
research field are analyzed. Then we introduce the system goals and the system
framework, i.e., the natural language understanding mechanism (NLML, NLOMJ and
NLDB) and the communicational response (CR). Finally we give the syntactic and
semantic content of this instruction system, i.e. some important notations of
English grammar used in it and their relations with the NLOMJ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312031</id><created>2003-12-16</created><authors><author><keyname>Cabeza</keyname><forenames>Daniel</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Distributed WWW Programming using (Ciao-)Prolog and the PiLLoW library</title><categories>cs.DC cs.PL</categories><comments>32 pages, 4 figures</comments><acm-class>D.1.3; D.1.6</acm-class><journal-ref>Theory and Practice of Logic Programming, Vol 1(3), 2001, 251-282</journal-ref><abstract>  We discuss from a practical point of view a number of issues involved in
writing distributed Internet and WWW applications using LP/CLP systems. We
describe PiLLoW, a public-domain Internet and WWW programming library for
LP/CLP systems that we have designed in order to simplify the process of
writing such applications. PiLLoW provides facilities for accessing documents
and code on the WWW; parsing, manipulating and generating HTML and XML
structured documents and data; producing HTML forms; writing form handlers and
CGI-scripts; and processing HTML/XML templates. An important contribution of
PiLLoW is to model HTML/XML code (and, thus, the content of WWW pages) as
terms. The PiLLoW library has been developed in the context of the Ciao Prolog
system, but it has been adapted to a number of popular LP/CLP systems,
supporting most of its functionality. We also describe the use of concurrency
and a high-level model of client-server interaction, Ciao Prolog's active
modules, in the context of WWW programming. We propose a solution for
client-side downloading and execution of Prolog code, using generic browsers.
Finally, we also provide an overview of related work on the topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312032</id><created>2003-12-16</created><authors><author><keyname>Remshagen</keyname><forenames>Anja</forenames></author><author><keyname>Truemper</keyname><forenames>Klaus</forenames></author></authors><title>Learning in a Compiler for MINSAT Algorithms</title><categories>cs.LO</categories><comments>16 pages</comments><acm-class>F.4.1, I.2.3</acm-class><journal-ref>Theory and practice of Logic Programming, Vol 3(3), pp 271-286,
  2003</journal-ref><abstract>  This paper describes learning in a compiler for algorithms solving classes of
the logic minimization problem MINSAT, where the underlying propositional
formula is in conjunctive normal form (CNF) and where costs are associated with
the True/False values of the variables. Each class consists of all instances
that may be derived from a given propositional formula and costs for True/False
values by fixing or deleting variables, and by deleting clauses. The learning
step begins once the compiler has constructed a solution algorithm for a given
class. The step applies that algorithm to comparatively few instances of the
class, analyses the performance of the algorithm on these instances, and
modifies the underlying propositional formula, with the goal that the algorithm
will perform much better on all instances of the class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312033</id><created>2003-12-17</created><authors><author><keyname>Zemskov</keyname><forenames>Ilya</forenames></author></authors><title>Using sensors in the web crawling process</title><categories>cs.IR cs.DL</categories><comments>4 pages, 4 figures. The article was accepted for the IADIS
  International WWW/Internet 2003 Conference but not published in the
  proceedings due to the lack of financial support</comments><acm-class>H.3.4; I.6.3</acm-class><abstract>  This paper offers a short description of an Internet information field
monitoring system, which places a special module-sensor on the side of the
Web-server to detect changes in information resources and subsequently
reindexes only the resources signalized by the corresponding sensor. Concise
results of simulation research and an implementation attempt of the given
&quot;sensors&quot; concept are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312034</id><created>2003-12-17</created><authors><author><keyname>Alvarez</keyname><forenames>Gonzalo</forenames></author><author><keyname>Hernandez</keyname><forenames>Luis</forenames></author><author><keyname>Martin</keyname><forenames>Angel</forenames></author></authors><title>Sharing secret color images using cellular automata with memory</title><categories>cs.CR</categories><comments>17 pages, 6 figures, LaTeX format</comments><acm-class>I.4.9</acm-class><abstract>  A {k,n}-threshold scheme based on two-dimensional memory cellular automata is
proposed to share images in a secret way. This method allows to encode an image
into n shared images so that only qualified subsets of k or more shares can
recover the secret image, but any k-1 or fewer of them gain no information
about the original image. The main characteristics of this new scheme are: each
shared image has the same size that the original one, and the recovered image
is exactly the same than the secret image; i.e., there is no loss of
resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312035</id><created>2003-12-17</created><authors><author><keyname>Rogawski</keyname><forenames>Marcin</forenames></author></authors><title>Analysis of Implementation Hierocrypt-3 algorithm (and its comparison to
  Camellia algorithm) using ALTERA devices</title><categories>cs.CR cs.PF</categories><comments>29 pages, presented on Enigma conferance in 2003r</comments><acm-class>B.7.1</acm-class><abstract>  Alghoritms: HIEROCRYPT-3, CAMELLIA and ANUBIS, GRAND CRU, NOEKEON, NUSH, Q,
RC6, SAFER++128, SC2000, SHACAL were requested for the submission of block
ciphers (high level block cipher) to NESSIE (New European Schemes for
Signatures, Integrity, and Encryption) project. The main purpose of this
project was to put forward a portfolio of strong cryptographic primitives of
various types. The NESSIE project was a three year long project and has been
divided into two phases. The first was finished in June 2001r. CAMELLIA, RC6,
SAFER++128 and SHACAL were accepted for the second phase of the evaluation
process. HIEROCRYPT-3 had key schedule problems, and there were attacks for up
to 3,5 rounds out of 6, at least hardware implementations of this cipher were
extremely slow [12]. HIEROCRYPT-3 was not selected to Phase II. CAMELLIA was
selected as an algorithm suggested for future standard. In the paper we present
the hardware implementations these two algorithms with 128-bit blocks and
128-bit keys, using ALTERA devices and their comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312036</id><created>2003-12-17</created><authors><author><keyname>Chockler</keyname><forenames>Hana</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Kupferman</keyname><forenames>Orna</forenames></author></authors><title>What Causes a System to Satisfy a Specification?</title><categories>cs.LO cs.AI</categories><acm-class>F.4.1; F.3.1; I.2.4</acm-class><abstract>  Even when a system is proven to be correct with respect to a specification,
there is still a question of how complete the specification is, and whether it
really covers all the behaviors of the system. Coverage metrics attempt to
check which parts of a system are actually relevant for the verification
process to succeed. Recent work on coverage in model checking suggests several
coverage metrics and algorithms for finding parts of the system that are not
covered by the specification. The work has already proven to be effective in
practice, detecting design errors that escape early verification efforts in
industrial settings. In this paper, we relate a formal definition of causality
given by Halpern and Pearl [2001] to coverage. We show that it gives
significant insight into unresolved issues regarding the definition of coverage
and leads to potentially useful extensions of coverage. In particular, we
introduce the notion of responsibility, which assigns to components of a system
a quantitative measure of their relevance to the satisfaction of the
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312037</id><created>2003-12-17</created><updated>2007-04-20</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Characterizing and Reasoning about Probabilistic and Non-Probabilistic
  Expectation</title><categories>cs.AI cs.LO</categories><comments>To appear in Journal of the ACM</comments><acm-class>F.4.1; I.2.3; I.2.4; G.3</acm-class><abstract>  Expectation is a central notion in probability theory. The notion of
expectation also makes sense for other notions of uncertainty. We introduce a
propositional logic for reasoning about expectation, where the semantics
depends on the underlying representation of uncertainty. We give sound and
complete axiomatizations for the logic in the case that the underlying
representation is (a) probability, (b) sets of probability measures, (c) belief
functions, and (d) possibility measures. We show that this logic is more
expressive than the corresponding logic for reasoning about likelihood in the
case of sets of probability measures, but equi-expressive in the case of
probability, belief, and possibility. Finally, we show that satisfiability for
these logics is NP-complete, no harder than satisfiability for propositional
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312038</id><created>2003-12-17</created><authors><author><keyname>Chockler</keyname><forenames>Hana</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Responsibility and blame: a structural-model approach</title><categories>cs.AI cs.LO</categories><acm-class>I.2.1</acm-class><abstract>  Causality is typically treated an all-or-nothing concept; either A is a cause
of B or it is not. We extend the definition of causality introduced by Halpern
and Pearl [2001] to take into account the degree of responsibility of A for B.
For example, if someone wins an election 11--0, then each person who votes for
him is less responsible for the victory than if he had won 6--5. We then define
a notion of degree of blame, which takes into account an agent's epistemic
state. Roughly speaking, the degree of blame of A for B is the expected degree
of responsibility of A for B, taken over the epistemic state of an agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312039</id><created>2003-12-17</created><updated>2006-09-20</updated><authors><author><keyname>Gacs</keyname><forenames>Peter</forenames></author></authors><title>Uniform test of algorithmic randomness over a general space</title><categories>cs.CC</categories><comments>40 pages. Journal reference and a slight correction in the proof of
  Theorem 7 added</comments><acm-class>E.4; G.3; H.1.1</acm-class><journal-ref>Theoretical Computer Science 341 (2005) 91-137</journal-ref><abstract>  The algorithmic theory of randomness is well developed when the underlying
space is the set of finite or infinite sequences and the underlying probability
distribution is the uniform distribution or a computable distribution. These
restrictions seem artificial. Some progress has been made to extend the theory
to arbitrary Bernoulli distributions (by Martin-Loef), and to arbitrary
distributions (by Levin). We recall the main ideas and problems of Levin's
theory, and report further progress in the same framework.
 - We allow non-compact spaces (like the space of continuous functions,
underlying the Brownian motion).
 - The uniform test (deficiency of randomness) d_P(x) (depending both on the
outcome x and the measure P should be defined in a general and natural way.
 - We see which of the old results survive: existence of universal tests,
conservation of randomness, expression of tests in terms of description
complexity, existence of a universal measure, expression of mutual information
as &quot;deficiency of independence.
 - The negative of the new randomness test is shown to be a generalization of
complexity in continuous spaces; we show that the addition theorem survives.
  The paper's main contribution is introducing an appropriate framework for
studying these questions and related ones (like statistics for a general family
of distributions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312040</id><created>2003-12-18</created><authors><author><keyname>Balduccini</keyname><forenames>Marcello</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author></authors><title>Diagnostic reasoning with A-Prolog</title><categories>cs.AI</categories><comments>46 pages, 1 Postscript figure</comments><acm-class>F.4.1; F.2.2</acm-class><journal-ref>TPLP Vol 3(4&amp;5) (2003) 425-461</journal-ref><abstract>  In this paper we suggest an architecture for a software agent which operates
a physical device and is capable of making observations and of testing and
repairing the device's components. We present simplified definitions of the
notions of symptom, candidate diagnosis, and diagnosis which are based on the
theory of action language ${\cal AL}$. The definitions allow one to give a
simple account of the agent's behavior in which many of the agent's tasks are
reduced to computing stable models of logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312041</id><created>2003-12-18</created><authors><author><keyname>Greco</keyname><forenames>Sergio</forenames></author><author><keyname>Zaniolo</keyname><forenames>Carlo</forenames></author></authors><title>Greedy Algorithms in Datalog</title><categories>cs.DB cs.AI</categories><comments>27 pages</comments><acm-class>D.1.6; F.3.1; F.4.1</acm-class><journal-ref>Theory and Practice of Logic Programming, 1(4): 381-407, 2001</journal-ref><abstract>  In the design of algorithms, the greedy paradigm provides a powerful tool for
solving efficiently classical computational problems, within the framework of
procedural languages. However, expressing these algorithms within the
declarative framework of logic-based languages has proven a difficult research
challenge. In this paper, we extend the framework of Datalog-like languages to
obtain simple declarative formulations for such problems, and propose effective
implementation techniques to ensure computational complexities comparable to
those of procedural formulations. These advances are achieved through the use
of the &quot;choice&quot; construct, extended with preference annotations to effect the
selection of alternative stable-models and nondeterministic fixpoints. We show
that, with suitable storage structures, the differential fixpoint computation
of our programs matches the complexity of procedural algorithms in classical
search and optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312042</id><created>2003-12-18</created><authors><author><keyname>Flesca</keyname><forenames>Sergio</forenames></author><author><keyname>Greco</keyname><forenames>Sergio</forenames></author></authors><title>Declarative Semantics for Active Rules</title><categories>cs.DB</categories><comments>27 pages</comments><acm-class>D.1.6; F.3.1; F.4.1</acm-class><journal-ref>Theory and Practice of Logic Programming, 1(1): 43-69, 2001</journal-ref><abstract>  In this paper we analyze declarative deterministic and non-deterministic
semantics for active rules. In particular we consider several (partial) stable
model semantics, previously defined for deductive rules, such as well-founded,
max deterministic, unique total stable model, total stable model, and maximal
stable model semantics. The semantics of an active program AP is given by first
rewriting it into a deductive program P, then computing a model M defining the
declarative semantics of P and, finally, applying `consistent' updates
contained in M to the source database. The framework we propose permits a
natural integration of deductive and active rules and can also be applied to
queries with function symbols or to queries over infinite databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312043</id><created>2003-12-18</created><authors><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author><author><keyname>Sadri</keyname><forenames>Fereidoon</forenames></author></authors><title>On A Theory of Probabilistic Deductive Databases</title><categories>cs.DB</categories><comments>38 pages, 0 figures</comments><acm-class>H.2.m</acm-class><abstract>  We propose a framework for modeling uncertainty where both belief and doubt
can be given independent, first-class status. We adopt probability theory as
the mathematical formalism for manipulating uncertainty. An agent can express
the uncertainty in her knowledge about a piece of information in the form of a
confidence level, consisting of a pair of intervals of probability, one for
each of her belief and doubt. The space of confidence levels naturally leads to
the notion of a trilattice, similar in spirit to Fitting's bilattices.
Intuitively, thep oints in such a trilattice can be ordered according to truth,
information, or precision. We develop a framework for probabilistic deductive
databases by associating confidence levels with the facts and rules of a
classical deductive database. While the trilattice structure offers a variety
of choices for defining the semantics of probabilistic deductive databases, our
choice of semantics is based on the truth-ordering, which we find to be closest
to the classical framework for deductive databases. In addition to proposing a
declarative semantics based on valuations and an equivalent semantics based on
fixpoint theory, we also propose a proof procedure and prove it sound and
complete. We show that while classical Datalog query programs have a polynomial
time data complexity, certain query programs in the probabilistic deductive
database framework do not even terminate on some input databases. We identify a
large natural class of query programs of practical interest in our framework,
and show that programs in this class possess polynomial time data complexity,
i.e., not only do they terminate on every input database, they are guaranteed
to do so in a number of steps polynomial in the input database size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312044</id><created>2003-12-19</created><updated>2004-04-09</updated><authors><author><keyname>Cilibrasi</keyname><forenames>Rudi</forenames><affiliation>CWI</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Clustering by compression</title><categories>cs.CV cond-mat.stat-mech cs.AI physics.data-an q-bio.GN q-bio.QM</categories><comments>LaTeX, 27 pages, 20 figures</comments><acm-class>E4, H.3.3, H.5.5, I.2.6, I.2.10, I.5.3, J.3,J.5</acm-class><abstract>  We present a new method for clustering based on compression. The method
doesn't use subject-specific features or background knowledge, and works as
follows: First, we determine a universal similarity distance, the normalized
compression distance or NCD, computed from the lengths of compressed data files
(singly and in pairwise concatenation). Second, we apply a hierarchical
clustering method. The NCD is universal in that it is not restricted to a
specific application area, and works across application area boundaries. A
theoretical precursor, the normalized information distance, co-developed by one
of the authors, is provably optimal but uses the non-computable notion of
Kolmogorov complexity. We propose precise notions of similarity metric, normal
compressor, and show that the NCD based on a normal compressor is a similarity
metric that approximates universality. To extract a hierarchy of clusters from
the distance matrix, we determine a dendrogram (binary tree) by a new quartet
method and a fast heuristic to implement it. The method is implemented and
available as public software, and is robust under choice of different
compressors. To substantiate our claims of universality and robustness, we
report evidence of successful application in areas as diverse as genomics,
virology, languages, literature, music, handwritten digits, astronomy, and
combinations of objects from completely different domains, using statistical,
dictionary, and block sorting compressors. In genomics we presented new
evidence for major questions in Mammalian evolution, based on
whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta
hypothesis against the Theria hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312045</id><created>2003-12-19</created><authors><author><keyname>Ferraris</keyname><forenames>Paolo</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author></authors><title>Weight Constraints as Nested Expressions</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>F.4.1; I.2.3</acm-class><abstract>  We compare two recent extensions of the answer set (stable model) semantics
of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the
bodies and heads of rules to contain nested expressions. The other, due to
Niemela and Simons, uses weight constraints. We show that there is a simple,
modular translation from the language of weight constraints into the language
of nested expressions that preserves the program's answer sets. Nested
expressions can be eliminated from the result of this translation in favor of
additional atoms. The translation makes it possible to compute answer sets for
some programs with weight constraints using satisfiability solvers, and to
prove the strong equivalence of programs with weight constraints using the
logic of here-and there.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312046</id><created>2003-12-19</created><authors><author><keyname>Teniente</keyname><forenames>Ernest</forenames></author><author><keyname>Urpi</keyname><forenames>Toni</forenames></author></authors><title>On the Abductive or Deductive Nature of Database Schema Validation and
  Update Processing Problems</title><categories>cs.DB cs.LO</categories><acm-class>H.2.1;H.2.4; H.2.3</acm-class><journal-ref>Theory and Practice of Logic Programming 3(3):287-327, may 2003</journal-ref><abstract>  We show that database schema validation and update processing problems such
as view updating, materialized view maintenance, integrity constraint checking,
integrity constraint maintenance or condition monitoring can be classified as
problems of either abductive or deductive nature, according to the reasoning
paradigm that inherently suites them. This is done by performing abductive and
deductive reasoning on the event rules [Oli91], a set of rules that define the
difference between consecutive database states In this way, we show that it is
possible to provide methods able to deal with all these problems as a whole. We
also show how some existing general deductive and abductive procedures may be
used to reason on the event rules. In this way, we show that these procedures
can deal with all database schema validation and update processing problems
considered in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312047</id><created>2003-12-20</created><authors><author><keyname>Merelo-Guervos</keyname><forenames>Juan-J.</forenames></author><author><keyname>Prieto</keyname><forenames>Beatriz</forenames></author><author><keyname>Rateb</keyname><forenames>Fatima</forenames></author><author><keyname>Tricas</keyname><forenames>Fernando</forenames></author></authors><title>Mapping weblog communities</title><categories>cs.NE</categories><comments>22 pages, 8 figures, to be submitted to Computer Networks</comments><acm-class>J.4;H.3.5;I.2.m</acm-class><abstract>  Websites of a particular class form increasingly complex networks, and new
tools are needed to map and understand them. A way of visualizing this complex
network is by mapping it. A map highlights which members of the community have
similar interests, and reveals the underlying social network. In this paper, we
will map a network of websites using Kohonen's self-organizing map (SOM), a
neural-net like method generally used for clustering and visualization of
complex data sets. The set of websites considered has been the Blogalia weblog
hosting site (based at http://www.blogalia.com/), a thriving community of
around 200 members, created in January 2002. In this paper we show how SOM
discovers interesting community features, its relation with other
community-discovering algorithms, and the way it highlights the set of
communities formed over the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312048</id><created>2003-12-20</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Representation Dependence in Probabilistic Inference</title><categories>cs.AI cs.LO</categories><comments>A preliminary version of the is paper appears in IJCAI, 1995. This
  version will appear in the Journal of AI Research</comments><acm-class>I.2.4; F.4.q</acm-class><abstract>  Non-deductive reasoning systems are often {\em representation dependent}:
representing the same situation in two different ways may cause such a system
to return two different answers. Some have viewed this as a significant
problem. For example, the principle of maximum entropy has been subjected to
much criticism due to its representation dependence. There has, however, been
almost no work investigating representation dependence. In this paper, we
formalize this notion and show that it is not a problem specific to maximum
entropy. In fact, we show that any representation-independent probabilistic
inference procedure that ignores irrelevant information is essentially
entailment, in a precise sense. Moreover, we show that representation
independence is incompatible with even a weak default assumption of
independence. We then show that invariance under a restricted class of
representation changes can form a reasonable compromise between representation
independence and other desiderata, and provide a construction of a family of
inference procedures that provides such restricted representation independence,
using relative entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312049</id><created>2003-12-21</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Using virtual processors for SPMD parallel programs</title><categories>cs.DC</categories><comments>9 pages; numeric tables; script code inserted</comments><acm-class>F.1.2</acm-class><abstract>  In this paper I describe some results on the use of virtual processors
technology for parallelize some SPMD computational programs. The tested
technology is the INTEL Hyper Threading on real processors, and the programs
are MATLAB scripts for floating points computation. The conclusions of the work
concern on the utility and limits of the used approach. The main result is that
using virtual processors is a good technique for improving parallel programs
not only for memory-based computations, but in the case of massive disk-storage
operations too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312050</id><created>2003-12-22</created><authors><author><keyname>Piwek</keyname><forenames>Paul</forenames></author></authors><title>A Flexible Pragmatics-driven Language Generator for Animated Agents</title><categories>cs.CL cs.MM</categories><report-no>ITRI-03-05</report-no><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Research Note Sessions of the 10th Conference
  of the European Chapter of the Association for Computational Linguistics
  (EACL'03), 2003, pp. 151-154</journal-ref><abstract>  This paper describes the NECA MNLG; a fully implemented Multimodal Natural
Language Generation module. The MNLG is deployed as part of the NECA system
which generates dialogues between animated agents. The generation module
supports the seamless integration of full grammar rules, templates and canned
text. The generator takes input which allows for the specification of
syntactic, semantic and pragmatic constraints on the output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312051</id><created>2003-12-22</created><authors><author><keyname>Piwek</keyname><forenames>Paul</forenames></author><author><keyname>van Deemter</keyname><forenames>Kees</forenames></author></authors><title>Towards Automated Generation of Scripted Dialogue: Some Time-Honoured
  Strategies</title><categories>cs.CL cs.AI</categories><report-no>ITRI-02-11</report-no><acm-class>I.2.7</acm-class><journal-ref>Proceedings of EDILOG: 6th Workshop on the Semantics and
  Pragmatics of Dialogue, 2002, pp. 141-148</journal-ref><abstract>  The main aim of this paper is to introduce automated generation of scripted
dialogue as a worthwhile topic of investigation. In particular the fact that
scripted dialogue involves two layers of communication, i.e., uni-directional
communication between the author and the audience of a scripted dialogue and
bi-directional pretended communication between the characters featuring in the
dialogue, is argued to raise some interesting issues. Our hope is that the
combined study of the two layers will forge links between research in text
generation and dialogue processing. The paper presents a first attempt at
creating such links by studying three types of strategies for the automated
generation of scripted dialogue. The strategies are derived from examples of
human-authored and naturally occurring dialogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312052</id><created>2003-12-22</created><authors><author><keyname>Piwek</keyname><forenames>Paul</forenames></author><author><keyname>van Deemter</keyname><forenames>Kees</forenames></author></authors><title>Dialogue as Discourse: Controlling Global Properties of Scripted
  Dialogue</title><categories>cs.CL cs.AI</categories><report-no>ITRI-03-04</report-no><acm-class>I.2.7</acm-class><journal-ref>Proceedings of AAAI Spring Symposium on Natural Language
  Generation in Spoken and Written Dialogue, Stanford, 2003</journal-ref><abstract>  This paper explains why scripted dialogue shares some crucial properties with
discourse. In particular, when scripted dialogues are generated by a Natural
Language Generation system, the generator can apply revision strategies that
cannot normally be used when the dialogue results from an interaction between
autonomous agents (i.e., when the dialogue is not scripted). The paper explains
that the relevant revision operators are best applied at the level of a
dialogue plan and discusses how the generator may decide when to apply a given
revision operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312053</id><created>2003-12-22</created><authors><author><keyname>Marek</keyname><forenames>Victor W.</forenames></author><author><keyname>Remmel</keyname><forenames>Jeffrey B.</forenames></author></authors><title>On the Expressibility of Stable Logic Programming</title><categories>cs.AI</categories><comments>17 pages</comments><acm-class>F.4.1</acm-class><journal-ref>TCLP 3(2003), pp. 551-567q</journal-ref><abstract>  (We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic
Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend
Schlipf's result to prove that SLP solves all search problems in the class
$\mathit{NP}$. Moreover, we do this in a uniform way as defined in \cite{mt99}.
Specifically, we show that there is a single $\mathrm{DATALOG}^{\neg}$ program
$P_{\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$
with non-negative integer coefficients and any input $\sigma$ of size $n$ over
a fixed alphabet $\Sigma$, there is an extensional database
$\mathit{edb}_{M,p,\sigma}$ such that there is a one-to-one correspondence
between the stable models of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$
and the accepting computations of the machine $M$ that reach the final state in
at most $p(n)$ steps. Moreover, $\mathit{edb}_{M,p,\sigma}$ can be computed in
polynomial time from $p$, $\sigma$ and the description of $M$ and the decoding
of such accepting computations from its corresponding stable model of
$\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ can be computed in linear
time. A similar statement holds for Default Logic with respect to
$\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result
involves additional technical complications and will be a subject of another
publication.}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312054</id><created>2003-12-22</created><authors><author><keyname>Kiwiel</keyname><forenames>Krzysztof C.</forenames></author></authors><title>Partitioning schemes for quicksort and quickselect</title><categories>cs.DS</categories><comments>21 pages</comments><report-no>PMMO-03-01</report-no><acm-class>F.2.2; G3</acm-class><abstract>  We introduce several modifications of the partitioning schemes used in
Hoare's quicksort and quickselect algorithms, including ternary schemes which
identify keys less or greater than the pivot. We give estimates for the numbers
of swaps made by each scheme. Our computational experiments indicate that
ternary schemes allow quickselect to identify all keys equal to the selected
key at little additional cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312055</id><created>2003-12-22</created><authors><author><keyname>Kiwiel</keyname><forenames>Krzysztof C.</forenames></author></authors><title>Randomized selection with quintary partitions</title><categories>cs.DS</categories><comments>21 pages</comments><report-no>PMMO-03-02</report-no><acm-class>F.2.2; G3</acm-class><abstract>  We show that several versions of Floyd and Rivest's algorithm Select for
finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+o(n)$ comparisons on average and with high probability. This
rectifies the analysis of Floyd and Rivest, and extends it to the case of
nondistinct elements. Our computational results confirm that Select may be the
best algorithm in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312056</id><created>2003-12-24</created><authors><author><keyname>Duncan</keyname><forenames>Christian A.</forenames><affiliation>University of Miami</affiliation></author><author><keyname>Eppstein</keyname><forenames>David</forenames><affiliation>University of California, Irvine</affiliation></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames><affiliation>University of Arizona</affiliation></author></authors><title>The Geometric Thickness of Low Degree Graphs</title><categories>cs.CG cs.DM</categories><comments>10 pages, 7 figures, submitted to SoCG 2004</comments><acm-class>I.3.5; F.2.2; G.2.2</acm-class><abstract>  We prove that the geometric thickness of graphs whose maximum degree is no
more than four is two. All of our algorithms run in O(n) time, where n is the
number of vertices in the graph. In our proofs, we present an embedding
algorithm for graphs with maximum degree three that uses an n x n grid and a
more complex algorithm for embedding a graph with maximum degree four. We also
show a variation using orthogonal edges for maximum degree four graphs that
also uses an n x n grid. The results have implications in graph theory, graph
drawing, and VLSI design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312057</id><created>2003-12-24</created><authors><author><keyname>Alferes</keyname><forenames>Jos&#xe9; J&#xfa;lio</forenames></author><author><keyname>Pereira</keyname><forenames>Lu\'&#x131;s Moniz</forenames></author><author><keyname>Swift</keyname><forenames>Terrance</forenames></author></authors><title>Abduction in Well-Founded Semantics and Generalized Stable Models</title><categories>cs.LO cs.AI</categories><comments>48 pages; To appear in Theory and Practice in Logic Programming</comments><acm-class>D.1.6; I.2.4</acm-class><abstract>  Abductive logic programming offers a formalism to declaratively express and
solve problems in areas such as diagnosis, planning, belief revision and
hypothetical reasoning. Tabled logic programming offers a computational
mechanism that provides a level of declarativity superior to that of Prolog,
and which has supported successful applications in fields such as parsing,
program analysis, and model checking. In this paper we show how to use tabled
logic programming to evaluate queries to abductive frameworks with integrity
constraints when these frameworks contain both default and explicit negation.
The result is the ability to compute abduction over well-founded semantics with
explicit negation and answer sets. Our approach consists of a transformation
and an evaluation method. The transformation adjoins to each objective literal
$O$ in a program, an objective literal $not(O)$ along with rules that ensure
that $not(O)$ will be true if and only if $O$ is false. We call the resulting
program a {\em dual} program. The evaluation method, \wfsmeth, then operates on
the dual program. \wfsmeth{} is sound and complete for evaluating queries to
abductive frameworks whose entailment method is based on either the
well-founded semantics with explicit negation, or on answer sets. Further,
\wfsmeth{} is asymptotically as efficient as any known method for either class
of problems. In addition, when abduction is not desired, \wfsmeth{} operating
on a dual program provides a novel tabling method for evaluating queries to
ground extended programs whose complexity and termination properties are
similar to those of the best tabling methods for the well-founded semantics. A
publicly available meta-interpreter has been developed for \wfsmeth{} using the
XSB system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312058</id><created>2003-12-25</created><authors><author><keyname>Glickman</keyname><forenames>Oren</forenames></author><author><keyname>Dagan</keyname><forenames>Ido</forenames></author></authors><title>Acquiring Lexical Paraphrases from a Single Corpus</title><categories>cs.CL cs.AI cs.IR cs.LG</categories><acm-class>I.7</acm-class><abstract>  This paper studies the potential of identifying lexical paraphrases within a
single corpus, focusing on the extraction of verb paraphrases. Most previous
approaches detect individual paraphrase instances within a pair (or set) of
comparable corpora, each of them containing roughly the same information, and
rely on the substantial level of correspondence of such corpora. We present a
novel method that successfully detects isolated paraphrase instances within a
single corpus without relying on any a-priori structure and information. A
comparison suggests that an instance-based approach may be combined with a
vector based approach in order to assess better the paraphrase likelihood for
many verb pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312059</id><created>2003-12-26</created><authors><author><keyname>Babikov</keyname><forenames>Pavel</forenames></author><author><keyname>Gontcharov</keyname><forenames>Oleg</forenames></author><author><keyname>Babikova</keyname><forenames>Maria</forenames></author></authors><title>Polyhierarchical Classifications Induced by Criteria Polyhierarchies,
  and Taxonomy Algebra</title><categories>cs.AI cs.IR</categories><comments>11 pages; 1 figure; to be presented at the &quot;Artificial Intelligence
  and Applications 2004&quot; conference</comments><acm-class>I.2.4; H.3.3</acm-class><abstract>  A new approach to the construction of general persistent polyhierarchical
classifications is proposed. It is based on implicit description of category
polyhierarchy by a generating polyhierarchy of classification criteria.
Similarly to existing approaches, the classification categories are defined by
logical functions encoded by attributive expressions. However, the generating
hierarchy explicitly predefines domains of criteria applicability, and the
semantics of relations between categories is invariant to changes in the
universe composition, extending variety of criteria, and increasing their
cardinalities. The generating polyhierarchy is an independent, compact,
portable, and re-usable information structure serving as a template
classification. It can be associated with one or more particular sets of
objects, included in more general classifications as a standard component, or
used as a prototype for more comprehensive classifications. The approach
dramatically simplifies development and unplanned modifications of persistent
hierarchical classifications compared with tree, DAG, and faceted schemes. It
can be efficiently implemented in common DBMS, while considerably reducing
amount of computer resources required for storage, maintenance, and use of
complex polyhierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0312060</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0312060</id><created>2003-12-27</created><authors><author><keyname>Savova</keyname><forenames>Virginia</forenames></author><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author></authors><title>Part-of-Speech Tagging with Minimal Lexicalization</title><categories>cs.CL cs.LG</categories><comments>10 pages text; 1 figure. To appear in &quot;Current Issues in Linguistic
  Theory: Recent Advances in Natural Language Processing&quot;;John Benjamins
  Publishers, Amsterdam</comments><acm-class>I.2.7</acm-class><abstract>  We use a Dynamic Bayesian Network to represent compactly a variety of
sublexical and contextual features relevant to Part-of-Speech (PoS) tagging.
The outcome is a flexible tagger (LegoTag) with state-of-the-art performance
(3.6% error on a benchmark corpus). We explore the effect of eliminating
redundancy and radically reducing the size of feature vocabularies. We find
that a small but linguistically motivated set of suffixes results in improved
cross-corpora generalization. We also show that a minimal lexicon limited to
function words is sufficient to ensure reasonable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401001</id><created>2004-01-05</created><authors><author><keyname>Harrison</keyname><forenames>Terry L.</forenames></author><author><keyname>Elango</keyname><forenames>Aravind</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Nelson</keyname><forenames>Michael</forenames></author></authors><title>Initial Experiences Re-Exporting Duplicate and Similarity Computation
  with an OAI-PMH aggregator</title><categories>cs.DL cs.DS</categories><comments>10 pages</comments><acm-class>H.3.7;H.3.3</acm-class><abstract>  The proliferation of the Open Archive Initiative Protocol for Metadata
Harvesting (OAI-PMH) has resulted in the creation of a large number of service
providers, all harvesting from either data providers or aggregators. If data
were available regarding the similarity of metadata records, service providers
could track redundant records across harvests from multiple sources as well as
provide additional end-user services. Due to the large number of metadata
formats and the diverse mapping strategies employed by data providers,
similarity calculation requirements necessitate the use of information
retrieval strategies. We describe an OAI-PMH aggregator implementation that
uses the optional ``&lt;about&gt;'' container to re-export the results of similarity
calculations. Metadata records (3751) were harvested from a NASA data provider
and similarities for the records were computed. The results were useful for
detecting duplicates, similarities and metadata errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401002</id><created>2004-01-03</created><authors><author><keyname>Holden</keyname><forenames>Joshua</forenames></author></authors><title>A Comparison of Cryptography Courses</title><categories>cs.CR cs.CY</categories><comments>14 pages; to appear in Cryptologia</comments><acm-class>K.3.2; E.3; K.4.0; K.5.0</acm-class><journal-ref>Cryptologia, 28 (2), 2004</journal-ref><abstract>  The author taught two courses on cryptography, one at Duke University aimed
at non-mathematics majors and one at Rose-Hulman Institute of Technology aimed
at mathematics and computer science majors. Both tried to incorporate technical
and societal aspects of cryptography, with varying emphases. This paper will
discuss the strengths and weaknesses of both courses and compare the
differences in the author's approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401003</id><created>2004-01-04</created><authors><author><keyname>Kiwiel</keyname><forenames>Krzysztof C.</forenames></author></authors><title>Randomized selection with tripartitioning</title><categories>cs.DS</categories><comments>19 pages</comments><report-no>PMMO-04-01</report-no><acm-class>F.2.2; G3</acm-class><abstract>  We show that several versions of Floyd and Rivest's algorithm Select [Comm.\
ACM {\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require
at most $n+\min\{k,n-k\}+o(n)$ comparisons on average, even when equal elements
occur. This parallels our recent analysis of another variant due to Floyd and
Rivest [Comm. ACM {\bf 18} (1975) 165--172]. Our computational results suggest
that both variants perform well in practice, and may compete with other
selection methods, such as Hoare's Find or quickselect with median-of-3 pivots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401004</id><created>2004-01-02</created><updated>2004-05-09</updated><authors><author><keyname>McGuire</keyname><forenames>Patrick C.</forenames></author><author><keyname>Rodriguez-Manfredi</keyname><forenames>J. A.</forenames></author><author><keyname>Sebastian-Martinez</keyname><forenames>E.</forenames></author><author><keyname>Gomez-Elvira</keyname><forenames>J.</forenames></author><author><keyname>Diaz-Martinez</keyname><forenames>E.</forenames></author><author><keyname>Ormo</keyname><forenames>J.</forenames></author><author><keyname>Neuffer</keyname><forenames>K.</forenames></author><author><keyname>Giaquinta</keyname><forenames>A.</forenames></author><author><keyname>Camps-Martinez</keyname><forenames>F.</forenames></author><author><keyname>Lepinette-Malvitte</keyname><forenames>A.</forenames></author><author><keyname>Perez-Mercader</keyname><forenames>J.</forenames></author><author><keyname>Ritter</keyname><forenames>H.</forenames></author><author><keyname>Oesker</keyname><forenames>M.</forenames></author><author><keyname>Ontrup</keyname><forenames>J.</forenames></author><author><keyname>Walter</keyname><forenames>J.</forenames></author></authors><title>Cyborg Systems as Platforms for Computer-Vision Algorithm-Development
  for Astrobiology</title><categories>cs.CV astro-ph cs.AI</categories><comments>Updated biblio info</comments><acm-class>I.4.0; I.4.6; I.4.8; I.4.9; I.5.4; I.5.5; J.2; I.2.5; I.2.10</acm-class><journal-ref>Proc. of the III European Workshop on Exo-Astrobiology, &quot;Mars: The
  Search for Life&quot;, Madrid, November 2003 (ESA SP-545, March 2004) pp.141-144</journal-ref><abstract>  Employing the allegorical imagery from the film &quot;The Matrix&quot;, we motivate and
discuss our `Cyborg Astrobiologist' research program. In this research program,
we are using a wearable computer and video camcorder in order to test and train
a computer-vision system to be a field-geologist and field-astrobiologist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401005</id><created>2004-01-08</created><authors><author><keyname>Victor</keyname><forenames>Kromer</forenames></author></authors><title>About Unitary Rating Score Constructing</title><categories>cs.LG</categories><comments>10 pages, 3 figures</comments><acm-class>1.2.6</acm-class><abstract>  It is offered to pool test points of different subjects and different aspects
of the same subject together in order to get the unitary rating score, by the
way of nonlinear transformation of indicator points in accordance with Zipf's
distribution. It is proposed to use the well-studied distribution of
Intellectuality Quotient IQ as the reference distribution for latent variable
&quot;progress in studies&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401006</id><created>2004-01-09</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Cluster computing performances using virtual processors and mathematical
  software</title><categories>cs.DC cs.MS</categories><comments>9 pages; 1 figure; keywords: cluster computing, virtual processors,
  performances</comments><acm-class>F.1.2; D.2.8</acm-class><abstract>  In this paper I describe some results on the use of virtual processors
technology for parallelize some SPMD computational programs in a cluster
environment. The tested technology is the INTEL Hyper Threading on real
processors, and the programs are MATLAB 6.5 Release 13 scripts for floating
points computation. By the use of this technology, I tested that a cluster can
run with benefit a number of concurrent processes double the amount of physical
processors. The conclusions of the work concern on the utility and limits of
the used approach. The main result is that using virtual processors is a good
technique for improving parallel programs not only for memory-based
computations, but in the case of massive disk-storage operations too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401007</id><created>2004-01-12</created><authors><author><keyname>McDevitt</keyname><forenames>K.</forenames></author><author><keyname>P&#xe9;rez-Qui&#xf1;ones</keyname><forenames>M. A.</forenames></author><author><keyname>Padilla-Falto</keyname><forenames>O. I.</forenames></author></authors><title>Design of a Community-based Translation Center</title><categories>cs.HC cs.DL</categories><acm-class>H.3.7; J.5; H.1.2</acm-class><abstract>  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401008</id><created>2004-01-13</created><authors><author><keyname>Gil</keyname><forenames>Amparo</forenames></author><author><keyname>Segura</keyname><forenames>Javier</forenames></author><author><keyname>Temme</keyname><forenames>Nico M.</forenames></author></authors><title>Algorithm xxx: Modified Bessel functions of imaginary order and positive
  argument</title><categories>cs.MS cs.NA math.NA</categories><comments>6 pages, 4 figures. To appear in ACM T. Math. Software</comments><acm-class>G.4</acm-class><abstract>  Fortran 77 programs for the computation of modified Bessel functions of
purely imaginary order are presented. The codes compute the functions
$K_{ia}(x)$, $L_{ia}(x)$ and their derivatives for real $a$ and positive $x$;
these functions are independent solutions of the differential equation $x^2 w''
+x w' +(a^2 -x^2)w=0$. The code also computes exponentially scaled functions.
The range of computation is $(x,a)\in (0,1500]\times [-1500,1500]$ when scaled
functions are considered and it is larger than $(0,500]\times [-400,400]$ for
standard IEEE double precision arithmetic. The relative accuracy is better than
$10^{-13}$ in the range $(0,200]\times [-200,200]$ and close to $10^{-12}$ in
$(0,1500]\times [-1500,1500]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401009</id><created>2004-01-13</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Unifying Computing and Cognition: The SP Theory and its Applications</title><categories>cs.AI</categories><acm-class>F.1.0; I.2.0</acm-class><abstract>  This book develops the conjecture that all kinds of information processing in
computers and in brains may usefully be understood as &quot;information compression
by multiple alignment, unification and search&quot;. This &quot;SP theory&quot;, which has
been under development since 1987, provides a unified view of such things as
the workings of a universal Turing machine, the nature of 'knowledge', the
interpretation and production of natural language, pattern recognition and
best-match information retrieval, several kinds of probabilistic reasoning,
planning and problem solving, unsupervised learning, and a range of concepts in
mathematics and logic. The theory also provides a basis for the design of an
'SP' computer with several potential advantages compared with traditional
digital computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401010</id><created>2004-01-13</created><authors><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author><author><keyname>Chuang</keyname><forenames>John</forenames></author></authors><title>On the Cost of Participating in a Peer-to-Peer Network</title><categories>cs.NI</categories><comments>17 pages, 4 figures. Short version to be published in the Proceedings
  of the Third International Workshop on Peer-to-Peer Systems (IPTPS'04). San
  Diego, CA. February 2004</comments><report-no>p2pecon TR-2003-12-CC</report-no><acm-class>C.2.4</acm-class><abstract>  In this paper, we model the cost incurred by each peer participating in a
peer-to-peer network. Such a cost model allows to gauge potential disincentives
for peers to collaborate, and provides a measure of the ``total cost'' of a
network, which is a possible benchmark to distinguish between proposals. We
characterize the cost imposed on a node as a function of the experienced load
and the node connectivity, and show how our model applies to a few proposed
routing geometries for distributed hash tables (DHTs). We further outline a
number of open questions this research has raised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401011</identifier>
 <datestamp>2008-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401011</id><created>2004-01-14</created><authors><author><keyname>Cocco</keyname><forenames>Simona</forenames></author><author><keyname>Monasson</keyname><forenames>Remi</forenames></author></authors><title>Heuristic average-case analysis of the backtrack resolution of random
  3-Satisfiability instances</title><categories>cs.DS cond-mat.stat-mech cs.CC</categories><comments>to appear in Theoretical Computer Science</comments><proxy>ccsd ccsd-00001030</proxy><acm-class>A.0</acm-class><journal-ref>Theoretical Computer Science (2004) A 320, 345</journal-ref><abstract>  An analysis of the average-case complexity of solving random 3-Satisfiability
(SAT) instances with backtrack algorithms is presented. We first interpret
previous rigorous works in a unifying framework based on the statistical
physics notions of dynamical trajectories, phase diagram and growth process. It
is argued that, under the action of the Davis--Putnam--Loveland--Logemann
(DPLL) algorithm, 3-SAT instances are turned into 2+p-SAT instances whose
characteristic parameters (ratio alpha of clauses per variable, fraction p of
3-clauses) can be followed during the operation, and define resolution
trajectories. Depending on the location of trajectories in the phase diagram of
the 2+p-SAT model, easy (polynomial) or hard (exponential) resolutions are
generated. Three regimes are identified, depending on the ratio alpha of the
3-SAT instance to be solved. Lower sat phase: for small ratios, DPLL almost
surely finds a solution in a time growing linearly with the number N of
variables. Upper sat phase: for intermediate ratios, instances are almost
surely satisfiable but finding a solution requires exponential time (2 ^ (N
omega) with omega&gt;0) with high probability. Unsat phase: for large ratios,
there is almost always no solution and proofs of refutation are exponential. An
analysis of the growth of the search tree in both upper sat and unsat regimes
is presented, and allows us to estimate omega as a function of alpha. This
analysis is based on an exact relationship between the average size of the
search tree and the powers of the evolution operator encoding the elementary
steps of the search heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401012</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401012</id><created>2004-01-15</created><updated>2004-12-13</updated><authors><author><keyname>Duchamp</keyname><forenames>Gerard</forenames><affiliation>LIFAR, LIPN</affiliation></author><author><keyname>Kacem</keyname><forenames>Hatem Hadj</forenames><affiliation>LIFAR</affiliation></author><author><keyname>Laugerotte</keyname><forenames>Eric</forenames><affiliation>LIFAR</affiliation></author></authors><title>Algebraic Elimination of epsilon-transitions</title><categories>cs.SC cs.DS</categories><comments>13 decembre 2004</comments><proxy>ccsd ccsd-00001038</proxy><acm-class>H. 2</acm-class><abstract>  We present here algebraic formulas associating a k-automaton to a
k-epsilon-automaton. The existence depends on the definition of the star of
matrices and of elements in the semiring k. For this reason, we present the
theorem which allows the transformation of k-epsilon-automata into k-automata.
The two automata have the same behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401013</id><created>2004-01-16</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author></authors><title>Verification of Process Rewrite Systems in normal form</title><categories>cs.OH</categories><comments>53 pages, 1 figure</comments><acm-class>68Q60</acm-class><abstract>  We consider the problem of model--checking for Process Rewrite Systems (PRSs)
in normal form. In a PRS in normal form every rewrite rule either only deals
with procedure calls and procedure termination, possibly with value return,
(this kind of rules allows to capture Pushdown Processes), or only deals with
dynamic activation of processes and synchronization (this kind of rules allows
to capture Petri Nets). The model-checking problem for PRSs and action-based
linear temporal logic (ALTL) is undecidable. However, decidability of
model--checking for PRSs and some interesting fragment of ALTL remains an open
question. In this paper we state decidability results concerning generalized
acceptance properties about infinite derivations (infinite term rewritings) in
PRSs in normal form. As a consequence, we obtain decidability of the
model-checking (restricted to infinite runs) for PRSs in normal form and a
meaningful fragment of ALTL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401014</id><created>2004-01-17</created><authors><author><keyname>Tropashko</keyname><forenames>Vadim</forenames></author></authors><title>Nested Intervals with Farey Fractions</title><categories>cs.DB</categories><comments>1 figure</comments><acm-class>H.2.4</acm-class><abstract>  Relational Databases are universally conceived as an advance over their
predecessors Network and Hierarchical models. Superior in every querying
respect, they turned out to be surprisingly incomplete when modeling transitive
dependencies. Almost every couple of months a question how to model a tree in
the database surfaces at comp.database.theory newsgroup. This article completes
a series of articles exploring Nested Intervals Model. Previous articles
introduced tree encoding with Binary Rational Numbers. However, binary encoding
grows exponentially, both in breadth and in depth. In this article, we'll
leverage Farey fractions in order to overcome this problem. We'll also
demonstrate that our implementation scales to a tree with 1M nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401015</id><created>2004-01-20</created><authors><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Bravo</keyname><forenames>Loreto</forenames></author></authors><title>Query Answering in Peer-to-Peer Data Exchange Systems</title><categories>cs.DB cs.LO</categories><comments>14 pages</comments><acm-class>H.2.4;F.4.1;I.2.3</acm-class><abstract>  The problem of answering queries posed to a peer who is a member of a
peer-to-peer data exchange system is studied. The answers have to be consistent
wrt to both the local semantic constraints and the data exchange constraints
with other peers; and must also respect certain trust relationships between
peers. A semantics for peer consistent answers under exchange constraints and
trust relationships is introduced and some techniques for obtaining those
answers are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401016</id><created>2004-01-21</created><updated>2006-03-14</updated><authors><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author><author><keyname>Tapparo</keyname><forenames>Francesco</forenames></author></authors><title>Generalized Strong Preservation by Abstract Interpretation</title><categories>cs.LO cs.PL</categories><acm-class>D.2.4; F.3.1; F.3.2</acm-class><abstract>  Standard abstract model checking relies on abstract Kripke structures which
approximate concrete models by gluing together indistinguishable states, namely
by a partition of the concrete state space. Strong preservation for a
specification language L encodes the equivalence of concrete and abstract model
checking of formulas in L. We show how abstract interpretation can be used to
design abstract models that are more general than abstract Kripke structures.
Accordingly, strong preservation is generalized to abstract
interpretation-based models and precisely related to the concept of
completeness in abstract interpretation. The problem of minimally refining an
abstract model in order to make it strongly preserving for some language L can
be formulated as a minimal domain refinement in abstract interpretation in
order to get completeness w.r.t. the logical/temporal operators of L. It turns
out that this refined strongly preserving abstract model always exists and can
be characterized as a greatest fixed point. As a consequence, some well-known
behavioural equivalences, like bisimulation, simulation and stuttering, and
their corresponding partition refinement algorithms can be elegantly
characterized in abstract interpretation as completeness properties and
refinements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401017</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401017</id><created>2004-01-21</created><updated>2004-07-26</updated><authors><author><keyname>Howe</keyname><forenames>Nicholas R.</forenames></author><author><keyname>Deschamps</keyname><forenames>Alexandra</forenames></author></authors><title>Better Foreground Segmentation Through Graph Cuts</title><categories>cs.CV</categories><comments>8 pages, 110 figures. Revision: Added web link to downloadable Matlab
  implementation</comments><acm-class>I.4.6</acm-class><doi>10.1016/j.eswa.2010.09.137</doi><abstract>  For many tracking and surveillance applications, background subtraction
provides an effective means of segmenting objects moving in front of a static
background. Researchers have traditionally used combinations of morphological
operations to remove the noise inherent in the background-subtracted result.
Such techniques can effectively isolate foreground objects, but tend to lose
fidelity around the borders of the segmentation, especially for noisy input.
This paper explores the use of a minimum graph cut algorithm to segment the
foreground, resulting in qualitatively and quantitiatively cleaner
segmentations. Experiments on both artificial and real data show that the
graph-based method reduces the error around segmented foreground objects. A
MATLAB code implementation is available at
http://www.cs.smith.edu/~nhowe/research/code/#fgseg
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401018</id><created>2004-01-22</created><authors><author><keyname>Bolotin</keyname><forenames>E. I.</forenames></author><author><keyname>Tsitsiashvili</keyname><forenames>G. Sh.</forenames></author><author><keyname>Golycheva</keyname><forenames>I. V.</forenames></author></authors><title>Factor Temporal Prognosis of Tick-Borne Encephalitis Foci Functioning on
  the South of Russian Far East</title><categories>cs.CV</categories><comments>4 pages</comments><acm-class>B.1.3</acm-class><abstract>  A method of temporal factor prognosis of TE (tick-borne encephalitis)
infection has been developed. The high precision of the prognosis results for a
number of geographical regions of Primorsky Krai has been achieved. The method
can be applied not only to epidemiological research but also to others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401019</id><created>2004-01-23</created><authors><author><keyname>Ord</keyname><forenames>Toby</forenames></author><author><keyname>Kieu</keyname><forenames>Tien D.</forenames></author></authors><title>Using biased coins as oracles</title><categories>cs.OH quant-ph</categories><comments>11 pages</comments><acm-class>F.1.1; F.1.2</acm-class><abstract>  While it is well known that a Turing machine equipped with the ability to
flip a fair coin cannot compute more that a standard Turing machine, we show
that this is not true for a biased coin. Indeed, any oracle set $X$ may be
coded as a probability $p_{X}$ such that if a Turing machine is given a coin
which lands heads with probability $p_{X}$ it can compute any function
recursive in $X$ with arbitrarily high probability. We also show how the
assumption of a non-recursive bias can be weakened by using a sequence of
increasingly accurate recursive biases or by choosing the bias at random from a
distribution with a non-recursive mean. We conclude by briefly mentioning some
implications regarding the physical realisability of such methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401020</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401020</id><created>2004-01-24</created><authors><author><keyname>Scheler</keyname><forenames>Gabriele</forenames></author><author><keyname>Schumann</keyname><forenames>Johann</forenames></author></authors><title>Presynaptic modulation as fast synaptic switching: state-dependent
  modulation of task performance</title><categories>cs.NE q-bio.NC</categories><comments>6 pages, 13 figures</comments><acm-class>F.1.1;K.3.2;I.1.10</acm-class><journal-ref>Neural Networks, 2003. Proceedings of the International Joint
  Conference on (Volume:1 ) 218 - 223</journal-ref><doi>10.1109/IJCNN.2003.1223347</doi><abstract>  Neuromodulatory receptors in presynaptic position have the ability to
suppress synaptic transmission for seconds to minutes when fully engaged. This
effectively alters the synaptic strength of a connection. Much work on
neuromodulation has rested on the assumption that these effects are uniform at
every neuron. However, there is considerable evidence to suggest that
presynaptic regulation may be in effect synapse-specific. This would define a
second &quot;weight modulation&quot; matrix, which reflects presynaptic receptor efficacy
at a given site. Here we explore functional consequences of this hypothesis. By
analyzing and comparing the weight matrices of networks trained on different
aspects of a task, we identify the potential for a low complexity &quot;modulation
matrix&quot;, which allows to switch between differently trained subtasks while
retaining general performance characteristics for the task. This means that a
given network can adapt itself to different task demands by regulating its
release of neuromodulators. Specifically, we suggest that (a) a network can
provide optimized responses for related classification tasks without the need
to train entirely separate networks and (b) a network can blend a &quot;memory mode&quot;
which aims at reproducing memorized patterns and a &quot;novelty mode&quot; which aims to
facilitate classification of new patterns. We relate this work to the known
effects of neuromodulators on brain-state dependent processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401021</id><created>2004-01-26</created><authors><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author></authors><title>A correct, precise and efficient integration of set-sharing, freeness
  and linearity for the analysis of finite and rational tree languages</title><categories>cs.PL</categories><comments>35 pages, 1 table, to appear on &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>F.3.2</acm-class><abstract>  It is well-known that freeness and linearity information positively interact
with aliasing information, allowing both the precision and the efficiency of
the sharing analysis of logic programs to be improved. In this paper we present
a novel combination of set-sharing with freeness and linearity information,
which is characterized by an improved abstract unification operator. We provide
a new abstraction function and prove the correctness of the analysis for both
the finite tree and the rational tree cases. Moreover, we show that the same
notion of redundant information as identified in (Bagnara et al. 2002;
Zaffanella et al. 2002) also applies to this abstract domain combination: this
allows for the implementation of an abstract unification operator running in
polynomial time and achieving the same precision on all the considered
observable properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401022</id><created>2004-01-26</created><authors><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author></authors><title>Enhanced sharing analysis techniques: a comprehensive evaluation</title><categories>cs.PL</categories><comments>43 pages, 10 tables, to appear on &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>F.3.2</acm-class><abstract>  Sharing, an abstract domain developed by D. Jacobs and A. Langen for the
analysis of logic programs, derives useful aliasing information. It is
well-known that a commonly used core of techniques, such as the integration of
Sharing with freeness and linearity information, can significantly improve the
precision of the analysis. However, a number of other proposals for refined
domain combinations have been circulating for years. One feature that is common
to these proposals is that they do not seem to have undergone a thorough
experimental evaluation even with respect to the expected precision gains. In
this paper we experimentally evaluate: helping Sharing with the definitely
ground variables found using Pos, the domain of positive Boolean formulas; the
incorporation of explicit structural information; a full implementation of the
reduced product of Sharing and Pos; the issue of reordering the bindings in the
computation of the abstract mgu; an original proposal for the addition of a new
mode recording the set of variables that are deemed to be ground or free; a
refined way of using linearity to improve the analysis; the recovery of hidden
information in the combination of Sharing with freeness information. Finally,
we discuss the issue of whether tracking compoundness allows the computation of
more sharing information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401023</id><created>2004-01-26</created><authors><author><keyname>Saucan</keyname><forenames>Emil</forenames></author></authors><title>Surface Triangulation -- The Metric Approach</title><categories>cs.GR cs.CG math.MG</categories><comments>29 pages, 13 figures Preliminary version</comments><acm-class>G.1.2; I.4.7</acm-class><abstract>  We embark in a program of studying the problem of better approximating
surfaces by triangulations(triangular meshes) by considering the approximating
triangulations as finite metric spaces and the target smooth surface as their
Haussdorff-Gromov limit. This allows us to define in a more natural way the
relevant elements, constants and invariants s.a. principal directions and
principal values, Gaussian and Mean curvature, etc. By a &quot;natural way&quot; we mean
an intrinsic, discrete, metric definitions as opposed to approximating or
paraphrasing the differentiable notions. In this way we hope to circumvent
computational errors and, indeed, conceptual ones, that are often inherent to
the classical, &quot;numerical&quot; approach. In this first study we consider the
problem of determining the Gaussian curvature of a polyhedral surface, by using
the {\em embedding curvature} in the sense of Wald (and Menger). We present two
modalities of employing these definitions for the computation of Gaussian
curvature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401024</id><created>2004-01-26</created><authors><author><keyname>Madina</keyname><forenames>Duraid</forenames></author><author><keyname>Standish</keyname><forenames>Russell K.</forenames></author></authors><title>A system for reflection in C++</title><categories>cs.PL</categories><acm-class>D.1.5;D.2.3</acm-class><journal-ref>Proceedings AUUG 2001: Always on and Everywhere, 207. ISBN
  0957753225</journal-ref><abstract>  Object-oriented programming languages such as Java and Objective C have
become popular for implementing agent-based and other object-based simulations
since objects in those languages can {\em reflect} (i.e. make runtime queries
of an object's structure). This allows, for example, a fairly trivial {\em
serialisation} routine (conversion of an object into a binary representation
that can be stored or passed over a network) to be written. However C++ does
not offer this ability, as type information is thrown away at compile time. Yet
C++ is often a preferred development environment, whether for performance
reasons or for its expressive features such as operator overloading.
  In this paper, we present the {\em Classdesc} system which brings many of the
benefits of object reflection to C++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401025</id><created>2004-01-26</created><authors><author><keyname>Leow</keyname><forenames>Richard</forenames></author><author><keyname>Standish</keyname><forenames>Russell K.</forenames></author></authors><title>Running C++ models undet the Swarm environment</title><categories>cs.MA</categories><acm-class>D.1.5;D.2.3;I.2.11</acm-class><journal-ref>Proceedings SwarmFest 2003</journal-ref><abstract>  Objective-C is still the language of choice if users want to run their
simulation efficiently under the Swarm environment since the Swarm environment
itself was written in Objective-C. The language is a fast, object-oriented and
easy to learn. However, the language is less well known than, less expressive
than, and lacks support for many important features of C++ (eg. OpenMP for high
performance computing application). In this paper, we present a methodology and
software tools that we have developed for auto generating an Objective-C object
template (and all the necessary interfacing functions) from a given C++ model,
utilising the Classdesc's object description technology, so that the C++ model
can both be run and accessed under the Objective-C and C++ environments. We
also present a methodology for modifying an existing Swarm application to make
part of the model (eg. the heatbug's step method) run under the C++
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401026</id><created>2004-01-26</created><authors><author><keyname>Standish</keyname><forenames>Russell K.</forenames></author><author><keyname>Leow</keyname><forenames>Richard</forenames></author></authors><title>EcoLab: Agent Based Modeling for C++ programmers</title><categories>cs.MA</categories><acm-class>D.1.5;D.2.3;I.2.11</acm-class><journal-ref>Proceedings SwarmFest 2003</journal-ref><abstract>  \EcoLab{} is an agent based modeling system for C++ programmers, strongly
influenced by the design of Swarm. This paper is just a brief outline of
\EcoLab's features, more details can be found in other published articles,
documentation and source code from the \EcoLab{} website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401027</id><created>2004-01-26</created><authors><author><keyname>Standish</keyname><forenames>Russell K.</forenames></author><author><keyname>Madina</keyname><forenames>Duraid</forenames></author></authors><title>ClassdescMP: Easy MPI programming in C++</title><categories>cs.DC</categories><acm-class>D.1.3;D.1.5;D.2.3;D.2.12</acm-class><journal-ref>Computational Science, Sloot et al. (eds), Lecture Notes in
  Computer Science 2660, Springer, 896. (2003)</journal-ref><abstract>  ClassdescMP is a distributed memory parallel programming system for use with
C++ and MPI. It uses the Classdesc reflection system to ease the task of
building complicated messages to be sent between processes. It doesn't hide the
underlying MPI API, so it is an augmentation of MPI capabilities. Users can
still call standard MPI function calls if needed for performance reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401028</id><created>2004-01-27</created><authors><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Eichhorn</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn S.</forenames></author><author><keyname>Murray</keyname><forenames>Steven S.</forenames></author></authors><title>Automated Resolution of Noisy Bibliographic References</title><categories>cs.DL</categories><comments>10 pages, 1 figure; accepted for publication in the proceedings of
  the 2004 Meeting of the International Federation of Classification Societies</comments><acm-class>H.3.7; H.3.2</acm-class><abstract>  We describe a system used by the NASA Astrophysics Data System to identify
bibliographic references obtained from scanned article pages by OCR methods
with records in a bibliographic database. We analyze the process generating the
noisy references and conclude that the three-step procedure of correcting the
OCR results, parsing the corrected string and matching it against the database
provides unsatisfactory results. Instead, we propose a method that allows a
controlled merging of correction, parsing and matching, inspired by dependency
grammars. We also report on the effectiveness of various heuristics that we
have employed to improve recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401029</id><created>2004-01-27</created><authors><author><keyname>Elango</keyname><forenames>Aravind</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Dynamic Linking of Smart Digital Objects Based on User Navigation
  Patterns</title><categories>cs.DL</categories><comments>pages: 12</comments><acm-class>H.3.7.1</acm-class><abstract>  We discuss a methodology to dynamically generate links among digital objects
by means of an unsupervised learning mechanism which analyzes user link
traversal patterns. We performed an experiment with a test bed of 150 complex
data objects, referred to as buckets. Each bucket manages its own content,
provides methods to interact with users and individually maintains a set of
links to other buckets. We demonstrate that buckets were capable of dynamically
adjusting their links to other buckets according to user link selections,
thereby generating a meaningful network of bucket relations. Our results
indicate such adaptive networks of linked buckets approximate the collective
link preferences of a community of user
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0401030</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0401030</id><created>2004-01-29</created><authors><author><keyname>Anashin</keyname><forenames>Vladimir</forenames></author></authors><title>Pseudorandom number generation by $p$-adic ergodic transformations</title><categories>cs.CR</categories><comments>Submitted</comments><acm-class>E.3</acm-class><journal-ref>&quot;Applied Algebraic Dynamics&quot;, volume 49 of de Gruyter Expositions
  in Mathematics, 2009, 269-304</journal-ref><abstract>  The paper study counter-dependent pseudorandom generators; the latter are
generators such that their state transition function (and output function) is
being modified dynamically while working: For such a generator the recurrence
sequence of states satisfies a congruence $x_{i+1}\equiv f_i(x_i)\pmod{2^n}$,
while its output sequence is of the form $z_{i}=F_i(u_i)$. The paper introduces
techniques and constructions that enable one to compose generators that output
uniformly distributed sequences of a maximum period length and with high linear
and 2-adic spans. The corresponding stream chipher is provably strong against a
known plaintext attack (up to a plausible conjecture). Both state transition
function and output function could be key-dependent, so the only information
available to a cryptanalyst is that these functions belong to some
(exponentially large) class. These functions are compositions of standard
machine instructions (such as addition, multiplication, bitwise logical
operations, etc.) The compositions should satisfy rather loose conditions; so
the corresponding generators are flexible enough and could be easily
implemented as computer programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402001</id><created>2004-01-31</created><authors><author><keyname>Capra</keyname><forenames>Robert G.</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author></authors><title>Mobile Re-Finding of Web Information Using a Voice Interface</title><categories>cs.HC cs.IR</categories><acm-class>H.1.2; H.3.3; H.5.2</acm-class><abstract>  Mobile access to information is a considerable problem for many users,
especially to information found on the Web. In this paper, we explore how a
voice-controlled service, accessible by telephone, could support mobile users'
needs for refinding specific information previously found on the Web. We
outline challenges in creating such a service and describe architectural and
user interfaces issues discovered in an exploratory prototype we built called
WebContext.
  We also present the results of a study, motivated by our experience with
WebContext, to explore what people remember about information that they are
trying to refind and how they express information refinding requests in a
collaborative conversation. As part of the study, we examine how
end-usercreated Web page annotations can be used to help support mobile
information re-finding. We observed the use of URLs, page titles, and
descriptions of page contents to help identify waypoints in the search process.
Furthermore, we observed that the annotations were utilized extensively,
indicating that explicitly added context by the user can play an important role
in re-finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402002</id><created>2004-02-01</created><authors><author><keyname>Strichman</keyname><forenames>Ofer</forenames></author></authors><title>Deciding Disjunctive Linear Arithmetic with SAT</title><categories>cs.LO</categories><acm-class>B.1.4</acm-class><abstract>  Disjunctive Linear Arithmetic (DLA) is a major decidable theory that is
supported by almost all existing theorem provers. The theory consists of
Boolean combinations of predicates of the form $\Sigma_{j=1}^{n}a_j\cdot x_j
\le b$, where the coefficients $a_j$, the bound $b$ and the variables $x_1 &gt;...
x_n$ are of type Real ($\mathbb{R}$). We show a reduction to propositional
logic from disjunctive linear arithmetic based on Fourier-Motzkin elimination.
While the complexity of this procedure is not better than competing techniques,
it has practical advantages in solving verification problems. It also promotes
the option of deciding a combination of theories by reducing them to this
logic. Results from experiments show that this method has a strong advantage
over existing techniques when there are many disjunctions in the formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402003</id><created>2004-02-01</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author></authors><title>Semantic Optimization of Preference Queries</title><categories>cs.DB</categories><comments>16 pages</comments><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  The notion of preference is becoming more and more ubiquitous in present-day
information systems. Preferences are primarily used to filter and personalize
the information reaching the users of such systems. In database systems,
preferences are usually captured as preference relations that are used to build
preference queries. In our approach, preference queries are relational algebra
or SQL queries that contain occurrences of the winnow operator (&quot;find the most
preferred tuples in a given relation&quot;).
 We present here a number of semantic optimization techniques applicable to
preference queries. The techniques make use of integrity constraints, and make
it possible to remove redundant occurrences of the winnow operator and to apply
a more efficient algorithm for the computation of winnow. We also study the
propagation of integrity constraints in the result of the winnow. We have
identified necessary and sufficient conditions for the applicability of our
techniques, and formulated those conditions as constraint satisfiability
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402004</id><created>2004-02-02</created><updated>2004-11-03</updated><authors><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Mou</keyname><forenames>Xuanqin</forenames></author><author><keyname>Cai</keyname><forenames>Yuanlong</forenames></author></authors><title>Baptista-type chaotic cryptosystems: Problems and countermeasures</title><categories>cs.CR nlin.CD</categories><comments>13 pages, 2 figures</comments><acm-class>E.3</acm-class><journal-ref>Physics Letters A, 332(5-6):368-375, 2004</journal-ref><doi>10.1016/j.physleta.2004.09.028</doi><abstract>  In 1998, M. S. Baptista proposed a chaotic cryptosystem, which has attracted
much attention from the chaotic cryptography community: some of its
modifications and also attacks have been reported in recent years. In [Phys.
Lett. A 307 (2003) 22], we suggested a method to enhance the security of
Baptista-type cryptosystem, which can successfully resist all proposed attacks.
However, the enhanced Baptista-type cryptosystem has a nontrivial defect, which
produces errors in the decrypted data with a generally small but nonzero
probability, and the consequent error propagation exists. In this Letter, we
analyze this defect and discuss how to rectify it. In addition, we point out
some newly-found problems existing in all Baptista-type cryptosystems and
consequently propose corresponding countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402005</id><created>2004-02-02</created><authors><author><keyname>Kiwiel</keyname><forenames>Krzysztof C.</forenames></author></authors><title>Improved randomized selection</title><categories>cs.DS</categories><comments>14 pages</comments><report-no>PMMO-04-02</report-no><acm-class>F.2.2, G3</acm-class><abstract>  We show that several versions of Floyd and Rivest's improved algorithm Select
for finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+O(n^{1/2}\ln^{1/2}n)$ comparisons on average and with high
probability. This rectifies the analysis of Floyd and Rivest, and extends it to
the case of nondistinct elements. Encouraging computational results on large
median-finding problems are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402006</id><created>2004-02-02</created><authors><author><keyname>Amendolia</keyname><forenames>S. Roberto</forenames></author><author><keyname>Brady</keyname><forenames>Michael</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Mulet-Parada</keyname><forenames>Miguel</forenames></author><author><keyname>Odeh</keyname><forenames>Mohammed</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author></authors><title>MammoGrid: Large-Scale Distributed Mammogram Analysis</title><categories>cs.SE</categories><comments>6 Pages, Presented at Medical Informatics Europe MIE'2003. ST Malo,
  France May 2003</comments><acm-class>J.3;H2.4</acm-class><abstract>  Breast cancer as a medical condition and mammograms as images exhibit many
dimensions of variability across the population. Similarly, the way diagnostic
systems are used and maintained by clinicians varies between imaging centres
and breast screening programmes, and so does the appearance of the mammograms
generated. A distributed database that reflects the spread of pathologies
across the population is an invaluable tool for the epidemiologist and the
understanding of the variation in image acquisition protocols is essential to a
radiologist in a screening programme. Exploiting emerging grid technology, the
aim of the MammoGrid [1] project is to develop a Europe-wide database of
mammograms that will be used to investigate a set of important healthcare
applications and to explore the potential of the grid to support effective
co-working between healthcare professionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402007</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402007</id><created>2004-02-02</created><authors><author><keyname>Ahmad</keyname><forenames>Uzair</forenames></author><author><keyname>Hassan</keyname><forenames>Mohammad Waseem</forenames></author><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author></authors><title>An Integrated Approach for Extraction of Objects from XML and
  Transformation to Heterogeneous Object Oriented Databases</title><categories>cs.DB cs.SE</categories><comments>4 pages, 5 figures. Presented at the 5th Int Conf on Enterprise
  Information Systems, ICEIS'03. Angers France April 2003</comments><acm-class>H2.4</acm-class><abstract>  CERN's (European Organization for Nuclear Research) WISDOM project uses XML
for the replication of data between different data repositories in a
heterogeneous operating system environment. For exchanging data from
Web-resident databases, the data needs to be transformed into XML and back to
the database format. Many different approaches are employed to do this
transformation. This paper addresses issues that make this job more efficient
and robust than existing approaches. It incorporates the World Wide Web
Consortium (W3C) XML Schema specification in the database-XML relationship.
Incorporation of the XML Schema exhibits significant improvements in XML
content usage and reduces the limitations of DTD-based database XML services.
Secondly the paper explores the possibility of database independent
transformation of data between XML and different databases. It proposes a
standard XML format that every serialized object should follow. This makes it
possible to use objects of heterogeneous database seamlessly using XML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402008</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402008</id><created>2004-02-02</created><authors><author><keyname>Odeh</keyname><forenames>Mohammed</forenames></author><author><keyname>Hauer</keyname><forenames>Tamas</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author></authors><title>A Use-Case Driven Approach in Requirements Engineering : The Mammogrid
  Project</title><categories>cs.DB cs.SE</categories><comments>6 pages, 3 figures. Presented at the 7th IASTED Int Conf on Software
  Engineering Applications. Marina del Rey, USA November 2003</comments><acm-class>H2.4</acm-class><abstract>  We report on the application of the use-case modeling technique to identify
and specify the user requirements of the MammoGrid project in an incremental
and controlled iterative approach. Modeling has been carried out in close
collaboration with clinicians and radiologists with no prior experience of use
cases. The study reveals the advantages and limitations of applying this
technique to requirements specification in the domains of breast cancer
screening and mammography research, with implications for medical imaging more
generally. In addition, this research has shown a return on investment in
use-case modeling in shorter gaps between phases of the requirements
engineering process. The qualitative result of this analysis leads us to
propose that a use-case modeling approach may result in reducing the cycle of
the requirements engineering process for medical imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402009</id><created>2004-02-03</created><authors><author><keyname>Estrella</keyname><forenames>F</forenames></author><author><keyname>del Frate</keyname><forenames>C</forenames></author><author><keyname>Hauer</keyname><forenames>T</forenames></author><author><keyname>McClatchey</keyname><forenames>R</forenames></author><author><keyname>Odeh</keyname><forenames>M</forenames></author><author><keyname>Rogulin</keyname><forenames>D</forenames></author><author><keyname>Amendolia</keyname><forenames>S R</forenames></author><author><keyname>Schottlander</keyname><forenames>D</forenames></author><author><keyname>Solomonides</keyname><forenames>T</forenames></author><author><keyname>Warren</keyname><forenames>R</forenames></author></authors><title>Resolving Clinicians Queries Across a Grids Infrastructure</title><categories>cs.DB cs.SE</categories><comments>8 pages, 3 figures. Presented at the 2nd Int Conf on HealthGrids
  Clermont-Ferrand, France January 2004 and accepted by Methods of Information
  in Medicine</comments><acm-class>H2.4; J.3</acm-class><abstract>  The past decade has witnessed order of magnitude increases in computing
power, data storage capacity and network speed, giving birth to applications
which may handle large data volumes of increased complexity, distributed over
the Internet. Grids computing promises to resolve many of the difficulties in
facilitating medical image analysis to allow radiologists to collaborate
without having to co-locate. The EU-funded MammoGrid project aims to
investigate the feasibility of developing a Grid-enabled European database of
mammograms and provide an information infrastructure which federates multiple
mammogram databases. This will enable clinicians to develop new common,
collaborative and co-operative approaches to the analysis of mammographic data.
This paper focuses on one of the key requirements for large-scale distributed
mammogram analysis: resolving queries across a grid-connected federation of
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402010</id><created>2004-02-03</created><authors><author><keyname>Matlin</keyname><forenames>Olga Shumsky</forenames></author><author><keyname>McCune</keyname><forenames>William</forenames></author></authors><title>Encapsulation for Practical Simplification Procedures</title><categories>cs.LO</categories><comments>6 pages</comments><report-no>Preprint ANL/MCS-P1039-0403</report-no><acm-class>F.3.1; F.4.1</acm-class><abstract>  ACL2 was used to prove properties of two simplification procedures. The
procedures differ in complexity but solve the same programming problem that
arises in the context of a resolution/paramodulation theorem proving system.
Term rewriting is at the core of the two procedures, but details of the
rewriting procedure itself are irrelevant. The ACL2 encapsulate construct was
used to assert the existence of the rewriting function and to state some of its
properties. Termination, irreducibility, and soundness properties were
established for each procedure. The availability of the encapsulation mechanism
in ACL2 is considered essential to rapid and efficient verification of this
kind of algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402011</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402011</id><created>2004-02-05</created><updated>2004-09-13</updated><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>Accurately modeling the Internet topology</title><categories>cs.NI</categories><comments>20 pages and 17 figures</comments><acm-class>C.2.1 and C.2.5</acm-class><journal-ref>Physical Review E, vol. 70, no. 066108, Dec. 2004.</journal-ref><doi>10.1103/PhysRevE.70.066108</doi><abstract>  Based on measurements of the Internet topology data, we found out that there
are two mechanisms which are necessary for the correct modeling of the Internet
topology at the Autonomous Systems (AS) level: the Interactive Growth of new
nodes and new internal links, and a nonlinear preferential attachment, where
the preference probability is described by a positive-feedback mechanism. Based
on the above mechanisms, we introduce the Positive-Feedback Preference (PFP)
model which accurately reproduces many topological properties of the AS-level
Internet, including: degree distribution, rich-club connectivity, the maximum
degree, shortest path length, short cycles, disassortative mixing and
betweenness centrality. The PFP model is a phenomenological model which
provides a novel insight into the evolutionary dynamics of real complex
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402012</id><created>2004-02-05</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Ricciardi</keyname><forenames>Aleta</forenames></author></authors><title>A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and
  Failure Detectors</title><categories>cs.DC</categories><comments>A preliminary version of this paper appeared in the 18th ACM
  Symposium on Principles of Distributed Computing, 1999, pp. 73-82. This
  version will appear in Distributed Computing</comments><acm-class>C.2.2, C.2.4, F.3.1</acm-class><abstract>  It is shown that, in a precise sense, if there is no bound on the number of
faulty processes in a system with unreliable but fair communication, Uniform
Distributed Coordination (UDC) can be attained if and only if a system has
perfect failure detectors. This result is generalized to the case where there
is a bound t on the number of faulty processes. It is shown that a certain type
of generalized failure detector is necessary and sufficient for achieving UDC
in a context with at most t faulty processes. Reasoning about processes'
knowledge as to which other processes are faulty plays a key role in the
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402013</id><created>2004-02-09</created><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames></author></authors><title>Corollaries on the fixpoint completion: studying the stable semantics by
  means of the Clark completion</title><categories>cs.AI cs.LO</categories><comments>15 pages. Presented at the 18th Workshop on Logic Programming,
  Potsdam, Germany, March 2004</comments><acm-class>F.4.1</acm-class><abstract>  The fixpoint completion fix(P) of a normal logic program P is a program
transformation such that the stable models of P are exactly the models of the
Clark completion of fix(P). This is well-known and was studied by Dung and
Kanchanasut (1989). The correspondence, however, goes much further: The
Gelfond-Lifschitz operator of P coincides with the immediate consequence
operator of fix(P), as shown by Wendt (2002), and even carries over to standard
operators used for characterizing the well-founded and the Kripke-Kleene
semantics. We will apply this knowledge to the study of the stable semantics,
and this will allow us to almost effortlessly derive new results concerning
fixed-point and metric-based semantics, and neural-symbolic integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402014</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402014</id><created>2004-02-09</created><authors><author><keyname>De Angelis</keyname><forenames>A.</forenames></author><author><keyname>Boinee</keyname><forenames>P.</forenames></author><author><keyname>Frailis</keyname><forenames>M.</forenames></author><author><keyname>Milotti</keyname><forenames>E.</forenames></author></authors><title>Self-Organising Networks for Classification: developing Applications to
  Science Analysis for Astroparticle Physics</title><categories>cs.NE astro-ph cs.AI</categories><acm-class>I.5.1; I.5.3</acm-class><doi>10.1016/j.physa.2004.02.023</doi><abstract>  Physics analysis in astroparticle experiments requires the capability of
recognizing new phenomena; in order to establish what is new, it is important
to develop tools for automatic classification, able to compare the final result
with data from different detectors. A typical example is the problem of Gamma
Ray Burst detection, classification, and possible association to known sources:
for this task physicists will need in the next years tools to associate data
from optical databases, from satellite experiments (EGRET, GLAST), and from
Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402015</id><created>2004-02-09</created><authors><author><keyname>Monge</keyname><forenames>Ramon Asensio</forenames></author><author><keyname>Marco</keyname><forenames>Francisco Sanchis</forenames></author><author><keyname>Cervigon</keyname><forenames>Fernando Torre</forenames></author><author><keyname>Garcia</keyname><forenames>Victor Garcia</forenames></author><author><keyname>Paino</keyname><forenames>Gustavo Uria</forenames></author></authors><title>A Preliminary Study for the development of an Early Method for the
  Measurement in Function Points of a Software Product</title><categories>cs.SE</categories><comments>6 pages, 3 figures</comments><acm-class>D.4.8</acm-class><abstract>  The Function Points Analysis (FPA) of A.J. Albrecht is a method to determine
the functional size of software products. The International Function Point
Users Group, (IFPUG), establishes the FPA like a standard in the software
functional size measurement. The IFPUG [3] [4] method follows the Albrecht's
method and incorporates in its succesive versions modifications to the rules
and hints with the intention of improving it [7]. The required documentation
level to apply the method is the functional specification which corresponds to
level I in the Rudolph's clasification [8]. This documentation is avalaible
with some difficulty for those companies which are dedicated to develop
software for third parties when they have to prepare the appropiate budget for
this development. Then, we face the need of developing an early method [6] [9]
for measuring the functional size of a software product that we will name to
abbreviate it Early Method or EFPM (Early Function Point Method). The required
documentation to apply the EFPM would be the User Requirements or some
analogous documentations. This is a part of a research work now in process in
Oviedo University. In this article we only show the following, results:
  From the measurements of a set of projects using the IFPUG method v. 4.1 we
obtain the linear correlation coefficients between the total number of Function
Points for each project and the counters of the ILFs number, ILFs+EIFs number
and EIs+EOs+EQs number.
  Using the preliminary results we compute the regression functions. This
results we will allow us to determine the factors to be considered in the
development of EFPM and to estimate the function points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402016</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402016</id><created>2004-02-09</created><authors><author><keyname>Frailis</keyname><forenames>M.</forenames></author><author><keyname>De Angelis</keyname><forenames>A.</forenames></author><author><keyname>Roberto</keyname><forenames>V.</forenames></author></authors><title>Perspects in astrophysical databases</title><categories>cs.DB astro-ph</categories><acm-class>H.2.4; H.2.8</acm-class><journal-ref>Physica A338 (2004) 54-59</journal-ref><doi>10.1016/j.physa.2004.02.024</doi><abstract>  Astrophysics has become a domain extremely rich of scientific data. Data
mining tools are needed for information extraction from such large datasets.
This asks for an approach to data management emphasizing the efficiency and
simplicity of data access; efficiency is obtained using multidimensional access
methods and simplicity is achieved by properly handling metadata. Moreover,
clustering and classification techniques on large datasets pose additional
requirements in terms of computation and memory scalability and
interpretability of results. In this study we review some possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402017</id><created>2004-02-10</created><authors><author><keyname>Luther</keyname><forenames>Akshay</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author></authors><title>Alchemi: A .NET-based Grid Computing Framework and its Integration into
  Global Grids</title><categories>cs.DC</categories><comments>17 pages, 12 figures, 2 tables</comments><report-no>GRIDS-TR-2003-8</report-no><acm-class>C.2.4</acm-class><journal-ref>Technical Report, GRIDS-TR-2003-8, Grid Computing and Distributed
  Systems Laboratory, University of Melbourne, Australia, December 2003</journal-ref><abstract>  Computational grids that couple geographically distributed resources are
becoming the de-facto computing platform for solving large-scale problems in
science, engineering, and commerce. Software to enable grid computing has been
primarily written for Unix-class operating systems, thus severely limiting the
ability to effectively utilize the computing resources of the vast majority of
desktop computers i.e. those running variants of the Microsoft Windows
operating system. Addressing Windows-based grid computing is particularly
important from the software industry's viewpoint where interest in grids is
emerging rapidly. Microsoft's .NET Framework has become near-ubiquitous for
implementing commercial distributed systems for Windows-based platforms,
positioning it as the ideal platform for grid computing in this context. In
this paper we present Alchemi, a .NET-based grid computing framework that
provides the runtime machinery and programming environment required to
construct desktop grids and develop grid applications. It allows flexible
application composition by supporting an object-oriented grid application
programming model in addition to a grid job model. Cross-platform support is
provided via a web services interface and a flexible execution model supports
dedicated and non-dedicated (voluntary) execution by grid nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402018</id><created>2004-02-10</created><authors><author><keyname>Ding</keyname><forenames>Choon Hoong</forenames></author><author><keyname>Nutanong</keyname><forenames>Sarana</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>P2P Networks for Content Sharing</title><categories>cs.DC</categories><comments>35 pages, 26 figures</comments><report-no>GRIDS-TR-2003-7</report-no><acm-class>C.2.4</acm-class><journal-ref>Technical Report, GRIDS-TR-2003-7, Grid Computing and Distributed
  Systems Laboratory, University of Melbourne, Australia, December 2003</journal-ref><abstract>  Peer-to-peer (P2P) technologies have been widely used for content sharing,
popularly called &quot;file-swapping&quot; networks. This chapter gives a broad overview
of content sharing P2P technologies. It starts with the fundamental concept of
P2P computing followed by the analysis of network topologies used in
peer-to-peer systems. Next, three milestone peer-to-peer technologies: Napster,
Gnutella, and Fasttrack are explored in details, and they are finally concluded
with the comparison table in the last section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402019</id><created>2004-02-10</created><authors><author><keyname>Fruehwirth</keyname><forenames>Thom</forenames></author><author><keyname>Abdennadher</keyname><forenames>Slim</forenames></author></authors><title>The Munich Rent Advisor: A Success for Logic Programming on the Internet</title><categories>cs.AI cs.DS</categories><acm-class>D.3.3;G.1.6</acm-class><abstract>  Most cities in Germany regularly publish a booklet called the {\em
Mietspiegel}. It basically contains a verbal description of an expert system.
It allows the calculation of the estimated fair rent for a flat. By hand, one
may need a weekend to do so. With our computerized version, the {\em Munich
Rent Advisor}, the user just fills in a form in a few minutes and the rent is
calculated immediately. We also extended the functionality and applicability of
the {\em Mietspiegel} so that the user need not answer all questions on the
form. The key to computing with partial information using high-level
programming was to use constraint logic programming. We rely on the internet,
and more specifically the World Wide Web, to provide this service to a broad
user group. More than ten thousand people have used our service in the last
three years. This article describes the experiences in implementing and using
the {\em Munich Rent Advisor}. Our results suggests that logic programming with
constraints can be an important ingredient in intelligent internet systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402020</id><created>2004-02-11</created><authors><author><keyname>Ho</keyname><forenames>Tin Kam</forenames></author></authors><title>Geometrical Complexity of Classification Problems</title><categories>cs.CV</categories><comments>Proceedings of the 7th Course on Ensemble Methods for Learning
  Machines at the International School on Neural Nets ``E.R. Caianiello''</comments><acm-class>I.5.0</acm-class><abstract>  Despite encouraging recent progresses in ensemble approaches, classification
methods seem to have reached a plateau in development. Further advances depend
on a better understanding of geometrical and topological characteristics of
point sets in high-dimensional spaces, the preservation of such characteristics
under feature transformations and sampling processes, and their interaction
with geometrical models used in classifiers. We discuss an attempt to measure
such properties from data sets and relate them to classifier accuracies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402021</id><created>2004-02-11</created><authors><author><keyname>Ho</keyname><forenames>Tin Kam</forenames></author></authors><title>A Numerical Example on the Principles of Stochastic Discrimination</title><categories>cs.CV cs.LG</categories><comments>Proceedings of the 7th Course on Ensemble Methods for Learning
  Machines at the International School on Neural Nets ``E.R. Caianiello''</comments><acm-class>I.5.0</acm-class><abstract>  Studies on ensemble methods for classification suffer from the difficulty of
modeling the complementary strengths of the components. Kleinberg's theory of
stochastic discrimination (SD) addresses this rigorously via mathematical
notions of enrichment, uniformity, and projectability of an ensemble. We
explain these concepts via a very simple numerical example that captures the
basic principles of the SD theory and method. We focus on a fundamental
symmetry in point set covering that is the key observation leading to the
foundation of the theory. We believe a better understanding of the SD method
will lead to developments of better tools for analyzing other ensemble methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402022</id><created>2004-02-11</created><authors><author><keyname>Perugini</keyname><forenames>Saverio</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Fox</keyname><forenames>Edward A.</forenames></author></authors><title>Automatically Generating Interfaces for Personalized Interaction with
  Digital Libraries</title><categories>cs.DL cs.HC</categories><comments>Communicated as a short paper to the Fourth ACM/IEEE-CS Joint
  Conference on Digital Libraries (JCDL'04), Tuscon, AZ, June 2004; 2 pages, 1
  figure</comments><acm-class>H.3.7 [Digital Libraries]: User Issues; H.5.2 [User Interfaces]:
  Graphical user interfaces, Interaction styles; H.5.4 [Hypertext/Hypermedia]:
  Navigation</acm-class><abstract>  We present an approach to automatically generate interfaces supporting
personalized interaction with digital libraries; these interfaces augment the
user-DL dialog by empowering the user to (optionally) supply out-of-turn
information during an interaction, flatten or restructure the dialog, and
enquire about dialog options. Interfaces generated using this approach for
CITIDEL are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402023</id><created>2004-02-12</created><authors><author><keyname>Estrella</keyname><forenames>Florida</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Rogulina</keyname><forenames>Dmitry</forenames></author><author><keyname>Amendolia</keyname><forenames>Roberto</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author></authors><title>A Service-Based Approach for Managing Mammography Data</title><categories>cs.DB cs.SE</categories><comments>5 pages, 7 figures. Accepted by the 11th World Congress on Medical
  Informatics (MedInfo'04). San Francisco, USA. September 2004</comments><acm-class>H2.4; J.3</acm-class><abstract>  Grid-based technologies are emerging as a potential open-source
standards-based solution for managing and collabo-rating distributed resources.
In view of these new computing solutions, the Mammogrid project is developing a
service-based and Grid-aware application which manages a Euro-pean-wide
database of mammograms. Medical conditions such as breast cancer, and
mammograms as images, are ex-tremely complex with many dimensions of
variability across the population. An effective solution for the management of
disparate mammogram data sources is a federation of autonomous multi-centre
sites which transcends national boundaries. The Mammogrid solution utilizes the
Grid tech-nologies to integrate geographically distributed data sets. The
Mammogrid application will explore the potential of the Grid to support
effective co-working among radiologists through-out the EU. This paper outlines
the Mammogrid service-based approach in managing a federation of grid-connected
mam-mography databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402024</id><created>2004-02-12</created><authors><author><keyname>Estrella</keyname><forenames>Florida</forenames></author><author><keyname>Kovacs</keyname><forenames>Zsolt</forenames></author><author><keyname>Goff</keyname><forenames>Jean-Marie Le</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author><author><keyname>Toth</keyname><forenames>Norbert</forenames></author></authors><title>Pattern Reification as the Basis for Description-Driven Systems</title><categories>cs.DB cs.SE</categories><comments>20 pages, 10 figures</comments><acm-class>H2.4,J.3</acm-class><journal-ref>Journal of Software and System Modeling Vol 2 No 2, pp 108-119
  Springer-Verlag, ISSN: 1619-1366, 2003</journal-ref><abstract>  One of the main factors driving object-oriented software development for
information systems is the requirement for systems to be tolerant to change. To
address this issue in designing systems, this paper proposes a pattern-based,
object-oriented, description-driven system (DDS) architecture as an extension
to the standard UML four-layer meta-model. A DDS architecture is proposed in
which aspects of both static and dynamic systems behavior can be captured via
descriptive models and meta-models. The proposed architecture embodies four
main elements - firstly, the adoption of a multi-layered meta-modeling
architecture and reflective meta-level architecture, secondly the
identification of four data modeling relationships that can be made explicit
such that they can be modified dynamically, thirdly the identification of five
design patterns which have emerged from practice and have proved essential in
providing reusable building blocks for data management, and fourthly the
encoding of the structural properties of the five design patterns by means of
one fundamental pattern, the Graph pattern. A practical example of this
philosophy, the CRISTAL project, is used to demonstrate the use of
description-driven data objects to handle system evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402025</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402025</id><created>2004-02-12</created><authors><author><keyname>Breton</keyname><forenames>V.</forenames></author><author><keyname>Solomonides</keyname><forenames>A. E.</forenames></author><author><keyname>McClatchey</keyname><forenames>R. H.</forenames></author></authors><title>A perspective on the Healthgrid initiative</title><categories>cs.DB cs.SE</categories><comments>6 pages, 1 figure. Accepted by the Second International Workshop on
  Biomedical Computations on the Grid, at the 4th IEEE/ACM International
  Symposium on Cluster Computing and the Grid (CCGrid 2004). Chicago USA, April
  2004</comments><acm-class>H2.4,J.3</acm-class><abstract>  This paper presents a perspective on the Healthgrid initiative which involves
European projects deploying pioneering applications of grid technology in the
health sector. In the last couple of years, several grid projects have been
funded on health related issues at national and European levels. A crucial
issue is to maximize their cross fertilization in the context of an environment
where data of medical interest can be stored and made easily available to the
different actors in healthcare, physicians, healthcare centres and
administrations, and of course the citizens. The Healthgrid initiative,
represented by the Healthgrid association (http://www.healthgrid.org), was
initiated to bring the necessary long term continuity, to reinforce and promote
awareness of the possibilities and advantages linked to the deployment of GRID
technologies in health. Technologies to address the specific requirements for
medical applications are under development. Results from the DataGrid and other
projects are given as examples of early applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402026</id><created>2004-02-12</created><authors><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author></authors><title>Redundancy and Robustness of the AS-level Internet topology and its
  models</title><categories>cs.NI</categories><acm-class>C.2.1 and C.2.5</acm-class><journal-ref>IEE Electronic Letters, vol. 40, no. 2, pp. 151-15. on 22 January
  2004</journal-ref><doi>10.1049/el:20040078</doi><abstract>  A comparison between the topological properties of the measured Internet
topology, at the autonomous system level (AS graph), and the equivalent graphs
generated by two different power law topology generators is presented. Only one
of the synthetic generators reproduces the tier connectivity of the AS graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402027</id><created>2004-02-12</created><authors><author><keyname>Yu</keyname><forenames>Weikuan</forenames></author><author><keyname>Buntinas</keyname><forenames>Darius</forenames></author><author><keyname>Graham</keyname><forenames>Rich L.</forenames></author><author><keyname>Panda</keyname><forenames>Dhabaleswar K.</forenames></author></authors><title>Efficient and Scalable Barrier over Quadrics and Myrinet with a New
  NIC-Based Collective Message Passing Protocol</title><categories>cs.DC cs.AR</categories><comments>8 pages, 8 figures</comments><report-no>Preprint ANL/MCS-P1121-0204</report-no><acm-class>B.4.3; C.1.4; C.2.4</acm-class><abstract>  Modern interconnects often have programmable processors in the network
interface that can be utilized to offload communication processing from host
CPU. In this paper, we explore different schemes to support collective
operations at the network interface and propose a new collective protocol. With
barrier as an initial case study, we have demontrated that much of the
communication processing can be greatly simplified with this collective
protocol. Accordingly, %with our proposed collective processing scheme, we have
designed and implemented efficient and scalable NIC-based barrier operations
over two high performance interconnects, Quadrics and Myrinet.
  Our evaluation shows that, over a Quadrics cluster of 8 nodes with ELan3
Network, the NIC-based barrier operation achieves a barrier latency of only
5.60$\mu$s. This result is a 2.48 factor of improvement over the Elanlib
tree-based barrier operation. Over a Myrinet cluster of 8 nodes with LANai-XP
NIC cards, a barrier latency of 14.20$\mu$s over 8 nodes is achieved. This is a
2.64 factor of improvement over the host-based barrier algorithm. Furthermore,
an analytical model developed for the proposed scheme indicates that a
NIC-based barrier operation on a 1024-node cluster can be performed with only
22.13$\mu$s latency over Quadrics and with 38.94$\mu$s latency over Myrinet.
These results indicate the potential for developing high performance
communication subsystems for next generation clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402028</id><created>2004-02-13</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>The lattice dimension of a graph</title><categories>cs.DS math.CO</categories><comments>6 pages, 3 figures</comments><acm-class>F.2.2</acm-class><journal-ref>Eur. J. Combinatorics 26(6):585-592, 2005</journal-ref><doi>10.1016/j.ejc.2004.05.001</doi><abstract>  We describe a polynomial time algorithm for, given an undirected graph G,
finding the minimum dimension d such that G may be isometrically embedded into
the d-dimensional integer lattice Z^d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402029</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402029</id><created>2004-02-13</created><authors><author><keyname>Mane</keyname><forenames>Ketan</forenames></author><author><keyname>B&#xf6;rner</keyname><forenames>Katy</forenames></author></authors><title>Mapping Topics and Topic Bursts in PNAS</title><categories>cs.IR cs.HC</categories><acm-class>H.3.3; H.1.2</acm-class><doi>10.1073/pnas.0307626100</doi><abstract>  Scientific research is highly dynamic. New areas of science continually
evolve;others gain or lose importance, merge or split. Due to the steady
increase in the number of scientific publications it is hard to keep an
overview of the structure and dynamic development of one's own field of
science, much less all scientific domains. However, knowledge of hot topics,
emergent research frontiers, or change of focus in certain areas is a critical
component of resource allocation decisions in research labs, governmental
institutions, and corporations. This paper demonstrates the utilization of
Kleinberg's burst detection algorithm, co-word occurrence analysis, and graph
layout techniques to generate maps that support the identification of major
research topics and trends. The approach was applied to analyze and map the
complete set of papers published in the Proceedings of the National Academy of
Sciences (PNAS) in the years 1982-2001. Six domain experts examined and
commented on the resulting maps in an attempt to reconstruct the evolution of
major research areas covered by PNAS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402030</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402030</id><created>2004-02-15</created><authors><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Ocenasek</keyname><forenames>Jiri</forenames></author><author><keyname>Trebst</keyname><forenames>Simon</forenames></author><author><keyname>Troyer</keyname><forenames>Matthias</forenames></author><author><keyname>Alet</keyname><forenames>Fabien</forenames></author></authors><title>Computational complexity and simulation of rare events of Ising spin
  glasses</title><categories>cs.NE cs.AI</categories><comments>12 pages, submitted to GECCO-2004</comments><acm-class>G.1.6; I.2.8; I.2.6; J.2</acm-class><abstract>  We discuss the computational complexity of random 2D Ising spin glasses,
which represent an interesting class of constraint satisfaction problems for
black box optimization. Two extremal cases are considered: (1) the +/- J spin
glass, and (2) the Gaussian spin glass. We also study a smooth transition
between these two extremal cases. The computational complexity of all studied
spin glass systems is found to be dominated by rare events of extremely hard
spin glass samples. We show that complexity of all studied spin glass systems
is closely related to Frechet extremal value distribution. In a hybrid
algorithm that combines the hierarchical Bayesian optimization algorithm (hBOA)
with a deterministic bit-flip hill climber, the number of steps performed by
both the global searcher (hBOA) and the local searcher follow Frechet
distributions. Nonetheless, unlike in methods based purely on local search, the
parameters of these distributions confirm good scalability of hBOA with local
search. We further argue that standard performance measures for optimization
algorithms--such as the average number of evaluations until convergence--can be
misleading. Finally, our results indicate that for highly multimodal constraint
satisfaction problems, such as Ising spin glasses, recombination-based search
can provide qualitatively better results than mutation-based search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402031</id><created>2004-02-15</created><authors><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Lin</keyname><forenames>Tz-Kai</forenames></author></authors><title>Parameter-less hierarchical BOA</title><categories>cs.NE cs.AI</categories><comments>about 12 pages, submitted to GECCO-2004</comments><acm-class>G.1.6; I.2.6; I.2.8</acm-class><abstract>  The parameter-less hierarchical Bayesian optimization algorithm (hBOA)
enables the use of hBOA without the need for tuning parameters for solving each
problem instance. There are three crucial parameters in hBOA: (1) the selection
pressure, (2) the window size for restricted tournaments, and (3) the
population size. Although both the selection pressure and the window size
influence hBOA performance, performance should remain low-order polynomial with
standard choices of these two parameters. However, there is no standard
population size that would work for all problems of interest and the population
size must thus be eliminated in a different way. To eliminate the population
size, the parameter-less hBOA adopts the population-sizing technique of the
parameter-less genetic algorithm. Based on the existing theory, the
parameter-less hBOA should be able to solve nearly decomposable and
hierarchical problems in quadratic or subquadratic number of function
evaluations without the need for setting any parameters whatsoever. A number of
experiments are presented to verify scalability of the parameter-less hBOA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402032</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402032</id><created>2004-02-15</created><authors><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author></authors><title>Fitness inheritance in the Bayesian optimization algorithm</title><categories>cs.NE cs.AI cs.LG</categories><comments>IlliGAL Report No. 2004009, Illinois Genetic Algorithms Laboratory,
  University of Illinois at Urbana-Champaign, Urbana, IL. Download also from
  http://www-illigal.ge.uiuc.edu/</comments><report-no>IlliGAL Report No. 2004009</report-no><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  This paper describes how fitness inheritance can be used to estimate fitness
for a proportion of newly sampled candidate solutions in the Bayesian
optimization algorithm (BOA). The goal of estimating fitness for some candidate
solutions is to reduce the number of fitness evaluations for problems where
fitness evaluation is expensive. Bayesian networks used in BOA to model
promising solutions and generate the new ones are extended to allow not only
for modeling and sampling candidate solutions, but also for estimating their
fitness. The results indicate that fitness inheritance is a promising concept
in BOA, because population-sizing requirements for building appropriate models
of promising solutions lead to good fitness estimates even if only a small
proportion of candidate solutions is evaluated using the actual fitness
function. This can lead to a reduction of the number of actual fitness
evaluations by a factor of 30 or more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402033</id><created>2004-02-16</created><authors><author><keyname>Lin</keyname><forenames>Fangzhen</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author></authors><title>Recycling Computed Answers in Rewrite Systems for Abduction</title><categories>cs.AI</categories><comments>20 pages. Full version of our IJCAI-03 paper</comments><acm-class>F.4.1</acm-class><abstract>  In rule-based systems, goal-oriented computations correspond naturally to the
possible ways that an observation may be explained. In some applications, we
need to compute explanations for a series of observations with the same domain.
The question whether previously computed answers can be recycled arises. A yes
answer could result in substantial savings of repeated computations. For
systems based on classic logic, the answer is YES. For nonmonotonic systems
however, one tends to believe that the answer should be NO, since recycling is
a form of adding information. In this paper, we show that computed answers can
always be recycled, in a nontrivial way, for the class of rewrite procedures
that we proposed earlier for logic programs with negation. We present some
experimental results on an encoding of the logistics domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402034</id><created>2004-02-16</created><authors><author><keyname>Fouch&#xe9;</keyname><forenames>W. L.</forenames></author><author><keyname>Potgieter</keyname><forenames>P. H.</forenames></author></authors><title>Kolmogorov complexity and symmetric relational structures</title><categories>cs.CC cs.DM</categories><comments>11 pages, two diagrams</comments><acm-class>F.1.1; ; F.4.1; G.2.3</acm-class><journal-ref>The Journal of Symbolic Logic, Volume 63, Number 3, September
  1998, 1083-1094</journal-ref><abstract>  We study partitions of Fra\&quot;{\i}ss\'{e} limits of classes of finite
relational structures where the partitions are encoded by infinite binary
sequences which are random in the sense of Kolmogorov, Chaitin and Solomonoff.
It is shown that partition by a random sequence of a Fra\&quot;{\i}ss\'{e} limit
preserves the limit property of the object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402035</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402035</id><created>2004-02-16</created><authors><author><keyname>Chauvet</keyname><forenames>Jean-Marie</forenames></author></authors><title>Memory As A Monadic Control Construct In Problem-Solving</title><categories>cs.AI</categories><report-no>ND-2004-1</report-no><acm-class>6Q655</acm-class><abstract>  Recent advances in programming languages study and design have established a
standard way of grounding computational systems representation in category
theory. These formal results led to a better understanding of issues of control
and side-effects in functional and imperative languages. This framework can be
successfully applied to the investigation of the performance of Artificial
Intelligence (AI) inference and cognitive systems. In this paper, we delineate
a categorical formalisation of memory as a control structure driving
performance in inference systems. Abstracting away control mechanisms from
three widely used representations of memory in cognitive systems (scripts,
production rules and clusters) we explain how categorical triples capture the
interaction between learning and problem-solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402036</id><created>2004-02-16</created><authors><author><keyname>Pyla</keyname><forenames>Pardha S.</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author><author><keyname>Hartson</keyname><forenames>H. Rex</forenames></author></authors><title>Towards a Model-Based Framework for Integrating Usability and Software
  Engineering Life Cycles</title><categories>cs.HC</categories><comments>This document contains 8 pages with 5 figures. This appeared in
  Bridging the SE &amp; HCI Communities Workshop in INTERACT 2003
  (http://www.se-hci.org/bridging/interact)</comments><acm-class>H.5.0; K.6.1; K.6.3; D.2.9</acm-class><abstract>  In this position paper we propose a process model that provides a development
infrastructure in which the usability engineering and software engineering life
cycles co-exist in complementary roles. We describe the motivation, hurdles,
rationale, arguments, and implementation plan for the need, specification, and
the usefulness of such a model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402037</id><created>2004-02-17</created><updated>2004-11-16</updated><authors><author><keyname>Potgieter</keyname><forenames>P. H.</forenames></author></authors><title>The pre-history of quantum computation</title><categories>cs.GL</categories><comments>11 pages, in Afrikaans (title: Die voorgeskiedenis van
  kwantumberekening) with English abstract</comments><acm-class>F.1.2</acm-class><journal-ref>Suid-Afrikaanse Tydskrif vir Natuurwetenskap en Tegnologie, Vol
  23, Issue 1 / 2, Mar / Jun, 2-6 (2004)</journal-ref><abstract>  The main ideas behind developments in the theory and technology of quantum
computation were formulated in the late 1970s and early 1980s by two physicists
in the West and a mathematician in the former Soviet Union. It is not generally
known in the West that the subject has roots in the Russian technical
literature. The author hopes to present as impartial a synthesis as possible of
the early history of thought on this subject. The role of reversible and
irreversible computational processes is examined briefly as it relates to the
origins of quantum computing and the so-called Information Paradox in physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402038</id><created>2004-02-17</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Towards a Mathematical Theory of the Delays of the Asynchronous Circuits</title><categories>cs.LO</categories><acm-class>I.6.0</acm-class><journal-ref>Analele Universitatii din Oradea, Fascicola Matematica, TOM IX,
  2002</journal-ref><abstract>  The inequations of the delays of the asynchronous circuits are written, by
making use of pseudo-Boolean differential calculus. We consider these efforts
to be a possible starting point in the semi-formalized reconstruction of the
digital electrical engineering (which is a non-formalized theory).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402039</id><created>2004-02-17</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>On the Inertia of the Asynchronous Circuits</title><categories>cs.LO</categories><acm-class>H.1.0</acm-class><journal-ref>CAIM 2003, Oradea, Romania, May 29-31, 2003</journal-ref><abstract>  We present the bounded delays, the absolute inertia and the relative inertia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402040</id><created>2004-02-17</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Defining the Delays of the Asynchronous Circuits</title><categories>cs.LO</categories><acm-class>H.1.0</acm-class><journal-ref>CAIM 2003, Oradea, Romania, May 29-31, 2003</journal-ref><abstract>  We define the delays of a circuit, as well as the properties of determinism,
order, time invariance, constancy, symmetry and the serial connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402041</id><created>2004-02-17</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Examples of Models of the Asynchronous Circuits</title><categories>cs.LO</categories><acm-class>H.1.0</acm-class><journal-ref>the 10-th Symposium of Mathematics and its Applications,
  Politehnica University of Timisoara, Timisoara, 2003</journal-ref><abstract>  We define the delays of a circuit, as well as the properties of determinism,
order, time invariance, constancy, symmetry and the serial connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402042</id><created>2004-02-18</created><updated>2004-05-17</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>O'Neill</keyname><forenames>Kevin R.</forenames></author></authors><title>Anonymity and Information Hiding in Multiagent Systems</title><categories>cs.CR cs.LO cs.MA</categories><comments>Replacement. 36 pages. Full version of CSFW '03 paper, submitted to
  JCS. Made substantial changes to Section 6; added references throughout</comments><acm-class>D.4.6; D.2.1</acm-class><abstract>  We provide a framework for reasoning about information-hiding requirements in
multiagent systems and for reasoning about anonymity in particular. Our
framework employs the modal logic of knowledge within the context of the runs
and systems framework, much in the spirit of our earlier work on secrecy
[Halpern and O'Neill 2002]. We give several definitions of anonymity with
respect to agents, actions, and observers in multiagent systems, and we relate
our definitions of anonymity to other definitions of information hiding, such
as secrecy. We also give probabilistic definitions of anonymity that are able
to quantify an observer s uncertainty about the state of the system. Finally,
we relate our definitions of anonymity to other formalizations of anonymity and
information hiding, including definitions of anonymity in the process algebra
CSP and definitions of information hiding using function views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402043</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402043</id><created>2004-02-18</created><authors><author><keyname>Vitchev</keyname><forenames>Evgueniy</forenames></author></authors><title>The UPLNC Compiler: Design and Implementation</title><categories>cs.PL</categories><comments>134 pages, 2 figures, LaTeX</comments><acm-class>D.3.4</acm-class><abstract>  The implementation of the compiler of the UPLNC language is presented with a
full source code listing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402044</id><created>2004-02-18</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Schepers</keyname><forenames>Joerg</forenames></author></authors><title>A General Framework for Bounds for Higher-Dimensional Orthogonal Packing
  Problems</title><categories>cs.DS cs.CG</categories><comments>16 pages, 4 figures, Latex, to appear in Mathematical Methods of
  Operations Research</comments><acm-class>F.2.2</acm-class><abstract>  Higher-dimensional orthogonal packing problems have a wide range of practical
applications, including packing, cutting, and scheduling. In the context of a
branch-and-bound framework for solving these packing problems to optimality, it
is of crucial importance to have good and easy bounds for an optimal solution.
Previous efforts have produced a number of special classes of such bounds.
Unfortunately, some of these bounds are somewhat complicated and hard to
generalize. We present a new approach for obtaining classes of lower bounds for
higher-dimensional packing problems; our bounds improve and simplify several
well-known bounds from previous literature. In addition, our approach provides
an easy framework for proving correctness of new bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402045</id><created>2004-02-18</created><updated>2005-09-07</updated><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>The Freeze-Tag Problem: How to Wake Up a Swarm of Robots</title><categories>cs.DS</categories><comments>27 pages, 9 figures, Latex, to appear in Algorithmica. Cleaned up
  various parts of the paper, removed one overly technical section</comments><acm-class>F.2.2</acm-class><abstract>  An optimization problem that naturally arises in the study of swarm robotics
is the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, by
having an awakened robot move to their locations. Once a robot is awake, it can
assist in awakening other slumbering robots.The objective is to have all robots
awake as early as possible. While the FTP bears some resemblance to problems
from areas in combinatorial optimization such as routing, broadcasting,
scheduling, and covering, its algorithmic characteristics are surprisingly
different. We consider both scenarios on graphs and in geometric
environments.In graphs, robots sleep at vertices and there is a length function
on the edges. Awake robots travel along edges, with time depending on edge
length. For most scenarios, we consider the offline version of the problem, in
which each awake robot knows the position of all other robots. We prove that
the problem is NP-hard, even for the special case of star graphs. We also
establish hardness of approximation, showing that it is NP-hard to obtain an
approximation factor better than 5/3, even for graphs of bounded degree.These
lower bounds are complemented with several positive algorithmic results,
including: (1) We show that the natural greedy strategy on star graphs has a
tight worst-case performance of 7/3 and give a polynomial-time approximation
scheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive online
algorithm for graphs with maximum degree D and locally bounded edge weights.
(3) We give a PTAS, running in nearly linear time, for geometrically embedded
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402046</id><created>2004-02-19</created><authors><author><keyname>Garcia</keyname><forenames>Flavio D.</forenames></author><author><keyname>Hoepman</keyname><forenames>Jaap-Henk</forenames></author></authors><title>Spam filter analysis</title><categories>cs.CR</categories><comments>Submitted to SEC 2004</comments><acm-class>D.4.6; I.5.1; I.5.4; K.6.5</acm-class><abstract>  Unsolicited bulk email (aka. spam) is a major problem on the Internet. To
counter spam, several techniques, ranging from spam filters to mail protocol
extensions like hashcash, have been proposed. In this paper we investigate the
effectiveness of several spam filtering techniques and technologies. Our
analysis was performed by simulating email traffic under different conditions.
We show that genetic algorithm based spam filters perform best at server level
and naive Bayesian filters are the most appropriate for filtering at user
level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402047</id><created>2004-02-19</created><authors><author><keyname>Lima</keyname><forenames>Claudio F.</forenames></author><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author></authors><title>Parameter-less Optimization with the Extended Compact Genetic Algorithm
  and Iterated Local Search</title><categories>cs.NE</categories><comments>12 pages, submitted to gecco 2004</comments><acm-class>G.1.6; I.2.6; I.2.8</acm-class><abstract>  This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402048</id><created>2004-02-20</created><authors><author><keyname>Fioravanti</keyname><forenames>Fabio</forenames></author><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author></authors><title>Transformation Rules for Locally Stratified Constraint Logic Programs</title><categories>cs.PL cs.LO</categories><comments>To appear in: M. Bruynooghe, K.-K. Lau (Eds.) Program Development in
  Computational Logic, Lecture Notes in Computer Science, Springer</comments><acm-class>D.1.2;D.1.6;I.2.2;F.3.1</acm-class><abstract>  We propose a set of transformation rules for constraint logic programs with
negation. We assume that every program is locally stratified and, thus, it has
a unique perfect model. We give sufficient conditions which ensure that the
proposed set of transformation rules preserves the perfect model of the
programs. Our rules extend in some respects the rules for logic programs and
constraint logic programs already considered in the literature and, in
particular, they include a rule for unfolding a clause with respect to a
negative literal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402049</id><created>2004-02-20</created><authors><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author><author><keyname>Lima</keyname><forenames>Claudio F.</forenames></author><author><keyname>Martires</keyname><forenames>Hugo</forenames></author></authors><title>An architecture for massive parallelization of the compact genetic
  algorithm</title><categories>cs.NE</categories><comments>12 pages, submitted to gecco 2004</comments><acm-class>C.1.4; G.1.6; I.2.8</acm-class><abstract>  This paper presents an architecture which is suitable for a massive
parallelization of the compact genetic algorithm. The resulting scheme has
three major advantages. First, it has low synchronization costs. Second, it is
fault tolerant, and third, it is scalable.
  The paper argues that the benefits that can be obtained with the proposed
approach is potentially higher than those obtained with traditional parallel
genetic algorithms. In addition, the ideas suggested in the paper may also be
relevant towards parallelizing more complex probabilistic model building
genetic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402050</id><created>2004-02-20</created><authors><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author></authors><title>A philosophical essay on life and its connections with genetic algorithms</title><categories>cs.NE</categories><comments>10 pages, submitted to gecco 2004</comments><acm-class>I.2.8; J.4; K.4.0</acm-class><abstract>  This paper makes a number of connections between life and various facets of
genetic and evolutionary algorithms research. Specifically, it addresses the
topics of adaptation, multiobjective optimization, decision making, deception,
and search operators, among others. It argues that human life, from birth to
death, is an adaptive or dynamic optimization problem where people are
continuously searching for happiness. More important, the paper speculates that
genetic algorithms can be used as a source of inspiration for helping people
make decisions in their everyday life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402051</id><created>2004-02-20</created><updated>2004-05-11</updated><authors><author><keyname>Tropashko</keyname><forenames>Vadim</forenames></author></authors><title>Nested Intervals Tree Encoding with Continued Fractions</title><categories>cs.DB</categories><comments>8 pages</comments><acm-class>H.2.4</acm-class><abstract>  We introduce a new variation of Tree Encoding with Nested Intervals, find
connections with Materialized Path, and suggest a method for moving parts of
the hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402052</id><created>2004-02-20</created><authors><author><keyname>Dujella</keyname><forenames>Andrej</forenames></author></authors><title>Continued fractions and RSA with small secret exponent</title><categories>cs.CR math.NT</categories><comments>11 pages, to appear in Tatra Mt. Math. Publ</comments><acm-class>E.3</acm-class><journal-ref>Tatra Mt. Math. Publ. 29 (2004), 101-112.</journal-ref><abstract>  Extending the classical Legendre's result, we describe all solutions of the
inequality |x - a/b| &lt; c/b^2 in terms of convergents of continued fraction
expansion of x. Namely, we show that a/b = (rp_{m+1} +- sp_m) / (rq_{m+1} +-
sq_m) for some nonnegative integers m,r,s such that rs &lt; 2c.
  As an application of this result, we describe a modification of Verheul and
van Tilborg variant of Wiener's attack on RSA cryptosystem with small secret
exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402053</id><created>2004-02-23</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>The Complexity of Modified Instances</title><categories>cs.CC cs.AI</categories><acm-class>F.1.3; I.2.8</acm-class><abstract>  In this paper we study the complexity of solving a problem when a solution of
a similar instance is known. This problem is relevant whenever instances may
change from time to time, and known solutions may not remain valid after the
change. We consider two scenarios: in the first one, what is known is only a
solution of the problem before the change; in the second case, we assume that
some additional information, found during the search for this solution, is also
known. In the first setting, the techniques from the theory of NP-completeness
suffice to show complexity results. In the second case, negative results can
only be proved using the techniques of compilability, and are often related to
the size of considered changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402054</id><created>2004-02-24</created><updated>2004-12-01</updated><authors><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Mou</keyname><forenames>Xuanqin</forenames></author></authors><title>On the Security of the Yi-Tan-Siew Chaos-Based Cipher</title><categories>cs.CR cs.PF nlin.CD</categories><comments>5 pages, 3 figures, IEEEtrans.cls v 1.6</comments><acm-class>E.3</acm-class><journal-ref>IEEE Trans. CAS-II, vol. 51, no. 12, pp. 665-669, 2004</journal-ref><doi>10.1109/TCSII.2004.838657</doi><abstract>  This paper presents a comprehensive analysis on the security of the
Yi-Tan-Siew chaotic cipher proposed in [IEEE TCAS-I 49(12):1826-1829 (2002)]. A
differential chosen-plaintext attack and a differential chosen-ciphertext
attack are suggested to break the sub-key K, under the assumption that the time
stamp can be altered by the attacker, which is reasonable in such attacks.
Also, some security Problems about the sub-keys $\alpha$ and $\beta$ are
clarified, from both theoretical and experimental points of view. Further
analysis shows that the security of this cipher is independent of the use of
the chaotic tent map, once the sub-key $K$ is removed via the proposed
suggested differential chosen-plaintext attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402055</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402055</id><created>2004-02-24</created><authors><author><keyname>Buk</keyname><forenames>Solomiya</forenames></author></authors><title>Lexical Base as a Compressed Language Model of the World (on the
  material of the Ukrainian language)</title><categories>cs.CL</categories><comments>8 pages, 2 tables</comments><acm-class>I.2.7</acm-class><journal-ref>Psychology of Language and Communication. 2009, vol. 13, no. 2,
  pp. 35-44</journal-ref><doi>10.2478/v10057-009-0008-3</doi><abstract>  In the article the fact is verified that the list of words selected by formal
statistical methods (frequency and functional genre unrestrictedness) is not a
conglomerate of non-related words. It creates a system of interrelated items
and it can be named &quot;lexical base of language&quot;. This selected list of words
covers all the spheres of human activities. To verify this statement the
invariant synoptical scheme common for ideographic dictionaries of different
language was determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402056</id><created>2004-02-25</created><authors><author><keyname>Gonzalez</keyname><forenames>C. M.</forenames></author><author><keyname>Larrondo</keyname><forenames>H. A.</forenames></author><author><keyname>Gayoso</keyname><forenames>C. A.</forenames></author><author><keyname>Arnone</keyname><forenames>L. J.</forenames></author><author><keyname>Boemo</keyname><forenames>E. I.</forenames></author></authors><title>Digital Signal Transmission with Chaotic Encryption: Design and
  Evaluation of a FPGA Realization</title><categories>cs.CR</categories><acm-class>E.3</acm-class><abstract>  A discrete-time discrete-value pseudo-chaotic encoder/decoder system is
presented. The pseudo-chaotic module is a 3D discrete version of the well-known
Lorenz dynamical system. Scaling and biasing transformations as well as natural
number arithmetics are employed in order to simplify realizations on a small
size Field Programmable Gate Array (FPGA. The encryption ability is improved by
using only the least significant byte of one of the pseudo chaotic state
variables as the key to encrypt the plain text. The key is periodically
perturbed by another chaotic state variable. The statistical properties of the
pseudo chaotic cipher are compared with those of other pseudo-random generators
available in the literature. As an example of applicability of the technique, a
full duplex communication system is designed and constructed using FPGA's as
technological framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402057</id><created>2004-02-25</created><updated>2004-05-28</updated><authors><author><keyname>Gomez</keyname><forenames>Sergio Alejandro</forenames></author><author><keyname>Ches&#xf1;evar</keyname><forenames>Carlos Ivan</forenames></author></authors><title>Integrating Defeasible Argumentation and Machine Learning Techniques</title><categories>cs.AI</categories><comments>5 pages</comments><acm-class>I.2.0</acm-class><journal-ref>Procs. WICC 2003 . Pp. 787-791. Tandil, Argentina, Mayo 2003</journal-ref><abstract>  The field of machine learning (ML) is concerned with the question of how to
construct algorithms that automatically improve with experience. In recent
years many successful ML applications have been developed, such as datamining
programs, information-filtering systems, etc. Although ML algorithms allow the
detection and extraction of interesting patterns of data for several kinds of
problems, most of these algorithms are based on quantitative reasoning, as they
rely on training data in order to infer so-called target functions.
  In the last years defeasible argumentation has proven to be a sound setting
to formalize common-sense qualitative reasoning. This approach can be combined
with other inference techniques, such as those provided by machine learning
theory.
  In this paper we outline different alternatives for combining defeasible
argumentation and machine learning techniques. We suggest how different aspects
of a generic argument-based framework can be integrated with other ML-based
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402058</id><created>2004-02-25</created><authors><author><keyname>Cohen</keyname><forenames>Jacques</forenames></author></authors><title>A Tribute to Alain Colmerauer</title><categories>cs.PL</categories><comments>9 pages</comments><acm-class>D 3.2</acm-class><abstract>  The paper describes the contributions of Alain Colmerauer to the areas of
logic programs (LP) and constraint logic programs (CLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402059</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402059</id><created>2004-02-26</created><updated>2004-05-11</updated><authors><author><keyname>Baillot</keyname><forenames>Patrick</forenames></author><author><keyname>Terui</keyname><forenames>Kazushige</forenames></author></authors><title>Light types for polynomial time computation in lambda-calculus</title><categories>cs.LO</categories><comments>20 pages (including 10 pages of appendix). (revised version; in
  particular section 5 has been modified). A short version is to appear in the
  proceedings of the conference LICS 2004 (IEEE Computer Society Press)</comments><acm-class>F.4</acm-class><abstract>  We propose a new type system for lambda-calculus ensuring that well-typed
programs can be executed in polynomial time: Dual light affine logic (DLAL).
 DLAL has a simple type language with a linear and an intuitionistic type
arrow, and one modality. It corresponds to a fragment of Light affine logic
(LAL). We show that contrarily to LAL, DLAL ensures good properties on
lambda-terms: subject reduction is satisfied and a well-typed term admits a
polynomial bound on the reduction by any strategy. We establish that as LAL,
DLAL allows to represent all polytime functions. Finally we give a type
inference procedure for propositional DLAL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402060</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402060</id><created>2004-02-26</created><authors><author><keyname>Anashin</keyname><forenames>Vladimir</forenames></author></authors><title>Pseudorandom number generation by p-adic ergodic transformations: an
  addendum</title><categories>cs.CR</categories><comments>9 pages, no figures, LaTeX 2e. An addendum to an earlier posting</comments><acm-class>E.3</acm-class><journal-ref>&quot;Applied Algebraic Dynamics&quot;, volume 49 of de Gruyter Expositions
  in Mathematics, 2009, 269-304</journal-ref><abstract>  The paper study counter-dependent pseudorandom number generators based on
$m$-variate ($m&gt;1$) ergodic mappings of the space of 2-adic integers $\Z_2$.
The sequence of internal states of these generators is defined by the
recurrence law $\mathbf x_{i+1}= H^B_i(\mathbf x_i)\bmod{2^n}$, whereas their
output sequence is %while its output sequence is of the $\mathbf
z_{i}=F^B_i(\mathbf x_i)\mod 2^n$; here $\mathbf x_j, \mathbf z_j$ are
$m$-dimensional vectors over $\Z_2$. It is shown how the results obtained for a
univariate case could be extended to a multivariate case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0402061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0402061</id><created>2004-02-27</created><authors><author><keyname>Falcone</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Albuquerque</keyname><forenames>Paul</forenames></author></authors><title>A Correlation-Based Distance</title><categories>cs.IR</categories><acm-class>I.5.3</acm-class><abstract>  In this short technical report, we define on the sample space R^D a distance
between data points which depends on their correlation. We also derive an
expression for the center of mass of a set of points with respect to this
distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403001</id><created>2004-02-28</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Evolving a Stigmergic Self-Organized Data-Mining</title><categories>cs.AI cs.IR</categories><comments>10 pages, 3 figures, submitted to Intelligent Systems, Design and
  Applications 2004</comments><acm-class>I.2.11</acm-class><abstract>  Self-organizing complex systems typically are comprised of a large number of
frequently similar components or events. Through their process, a pattern at
the global-level of a system emerges solely from numerous interactions among
the lower-level components of the system. Moreover, the rules specifying
interactions among the system's components are executed using only local
information, without reference to the global pattern, which, as in many
real-world problems is not easily accessible or possible to be found.
Stigmergy, a kind of indirect communication and learning by the environment
found in social insects is a well know example of self-organization, providing
not only vital clues in order to understand how the components can interact to
produce a complex pattern, as can pinpoint simple biological non-linear rules
and methods to achieve improved artificial intelligent adaptive categorization
systems, critical for Data-Mining. On the present work it is our intention to
show that a new type of Data-Mining can be designed based on Stigmergic
paradigms, taking profit of several natural features of this phenomenon. By
hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we
seek for an entire distributed, adaptive, collective and cooperative
self-organized Data-Mining. As a real-world, real-time test bed for our
proposal, World-Wide-Web Mining will be used. Having that purpose in mind, Web
usage Data was collected from the Monash University's Web site (Australia),
with over 7 million hits every week. Results are compared to other recent
systems, showing that the system presented is by far promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403002</id><created>2004-03-02</created><updated>2005-06-22</updated><authors><author><keyname>Loyer</keyname><forenames>Y.</forenames></author><author><keyname>Straccia</keyname><forenames>U.</forenames></author></authors><title>Epistemic Foundation of Stable Model Semantics</title><categories>cs.AI</categories><comments>41 pages. To appear in Theory and Practice of Logic Programming (TPLP)</comments><abstract>  Stable model semantics has become a very popular approach for the management
of negation in logic programming. This approach relies mainly on the closed
world assumption to complete the available knowledge and its formulation has
its basis in the so-called Gelfond-Lifschitz transformation.
  The primary goal of this work is to present an alternative and
epistemic-based characterization of stable model semantics, to the
Gelfond-Lifschitz transformation. In particular, we show that stable model
semantics can be defined entirely as an extension of the Kripke-Kleene
semantics. Indeed, we show that the closed world assumption can be seen as an
additional source of `falsehood' to be added cumulatively to the Kripke-Kleene
semantics. Our approach is purely algebraic and can abstract from the
particular formalism of choice as it is based on monotone operators (under the
knowledge order) over bilattices only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403003</id><created>2004-03-04</created><authors><author><keyname>Giraldi</keyname><forenames>Gilson A.</forenames></author><author><keyname>Portugal</keyname><forenames>Renato</forenames></author><author><keyname>Thess</keyname><forenames>Ricardo N.</forenames></author></authors><title>Genetic Algorithms and Quantum Computation</title><categories>cs.NE</categories><comments>27 pages, 5 figures, 3 tables</comments><acm-class>D.1.0</acm-class><abstract>  Recently, researchers have applied genetic algorithms (GAs) to address some
problems in quantum computation. Also, there has been some works in the
designing of genetic algorithms based on quantum theoretical concepts and
techniques. The so called Quantum Evolutionary Programming has two major
sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic
Algorithms (QGAs). The former adopts qubit chromosomes as representations and
employs quantum gates for the search of the best solution. The later tries to
solve a key question in this field: what GAs will look like as an
implementation on quantum hardware? As we shall see, there is not a complete
answer for this question. An important point for QGAs is to build a quantum
algorithm that takes advantage of both the GA and quantum computing parallelism
as well as true randomness provided by quantum computers. In the first part of
this paper we present a survey of the main works in GAs plus quantum computing
including also our works in this area. Henceforth, we review some basic
concepts in quantum computation and GAs and emphasize their inherent
parallelism. Next, we review the application of GAs for learning quantum
operators and circuit design. Then, quantum evolutionary programming is
considered. Finally, we present our current research in this field and some
perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403004</id><created>2004-03-04</created><authors><author><keyname>Izhakian</keyname><forenames>Zur</forenames></author></authors><title>New Visualization of Surfaces in Parallel Coordinates - Eliminating
  Ambiguity and Some &quot;Over-Plotting&quot;</title><categories>cs.OH</categories><comments>13 pages 8 figures</comments><acm-class>F.2.1.;I.1.1</acm-class><journal-ref>Journal of WSCG, 2004, volume 1-3, no 12, pp 183-191, ISSN
  1213-6972</journal-ref><abstract>  $\cal{A}$ point $P \in \Real^n$ is represented in Parallel Coordinates by a
polygonal line $\bar{P}$ (see \cite{Insel99a} for a recent survey). Earlier
\cite{inselberg85plane}, a surface $\sigma$ was represented as the {\em
envelope} of the polygonal lines representing it's points. This is ambiguous in
the sense that {\em different} surfaces can provide the {\em same} envelopes.
Here the ambiguity is eliminated by considering the surface $\sigma$ as the
envelope of it's {\em tangent planes} and in turn, representing each of these
planes by $n$-1 points \cite{Insel99a}. This, with some future extension, can
yield a new and unambiguous representation, $\bar{\sigma}$, of the surface
consisting of $n$-1 planar regions whose properties correspond lead to the {\em
recognition} of the surfaces' properties i.e. developable, ruled etc.
\cite{hung92smooth}) and {\em classification} criteria.
  It is further shown that the image (i.e. representation) of an algebraic
surface of degree 2 in $\Real^n$ is a region whose boundary is also an
algebraic curve of degree 2. This includes some {\em non-convex} surfaces which
with the previous ambiguous representation could not be treated. An efficient
construction algorithm for the representation of the quadratic surfaces (given
either by {\em explicit} or {\em implicit} equation) is provided. The results
obtained are suitable for applications, to be presented in a future paper, and
in particular for the approximation of complex surfaces based on their {\em
planar} images. An additional benefit is the elimination of the
``over-plotting'' problem i.e. the ``bunching'' of polygonal lines which often
obscure part of the parallel-coordinate display.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403005</id><created>2004-03-04</created><authors><author><keyname>Izhakian</keyname><forenames>Zur</forenames></author></authors><title>Algebraic Curves in Parallel Coordinates - Avoiding the &quot;Over-Plotting&quot;
  Problem</title><categories>cs.OH</categories><acm-class>F.2.1.; I.1.1</acm-class><abstract>  ${\cal U}$ntil now the representation (i.e. plotting) of curve in Parallel
Coordinates is constructed from the point $\leftrightarrow$ line duality. The
result is a ``line-curve'' which is seen as the envelope of it's tangents.
Usually this gives an unclear image and is at the heart of the
``over-plotting'' problem; a barrier in the effective use of Parallel
Coordinates. This problem is overcome by a transformation which provides
directly the ``point-curve'' representation of a curve. Earlier this was
applied to conics and their generalizations. Here the representation, also
called dual, is extended to all planar algebraic curves. Specifically, it is
shown that the dual of an algebraic curve of degree $n$ is an algebraic of
degree at most $n(n - 1)$ in the absence of singular points. The result that
conics map into conics follows as an easy special case. An algorithm, based on
algebraic geometry using resultants and homogeneous polynomials, is obtained
which constructs the dual image of the curve. This approach has potential
generalizations to multi-dimensional algebraic surfaces and their
approximation. The ``trade-off'' price then for obtaining {\em planar}
representation of multidimensional algebraic curves and hyper-surfaces is the
higher degree of the image's boundary which is also an algebraic curve in
$\|$-coords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403006</id><created>2004-03-05</created><authors><author><keyname>B.</keyname><forenames>Carlos R. de la Mora</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Garcia-Vega</keyname><forenames>Angelica</forenames></author></authors><title>The role of behavior modifiers in representation development</title><categories>cs.AI</categories><comments>8 pages</comments><acm-class>I.2.0</acm-class><abstract>  We address the problem of the development of representations and their
relationship to the environment. We study a software agent which develops in a
network a representation of its simple environment which captures and
integrates the relationships between agent and environment through a closure
mechanism. The inclusion of a variable behavior modifier allows better
representation development. This can be confirmed with an internal description
of the closure mechanism, and with an external description of the properties of
the representation network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403007</id><created>2004-03-05</created><authors><author><keyname>Candea</keyname><forenames>George</forenames></author><author><keyname>Fox</keyname><forenames>Armando</forenames></author></authors><title>End-User Effects of Microreboots in Three-Tiered Internet Systems</title><categories>cs.OS cs.AR cs.NI</categories><comments>14 pages</comments><acm-class>D.4.5</acm-class><abstract>  Microreboots restart fine-grained components of software systems &quot;with a
clean slate,&quot; and only take a fraction of the time needed for full system
reboot. Microreboots provide an application-generic recovery technique for
Internet services, which can be supported entirely in middleware and requires
no changes to the applications or any a priori knowledge of application
semantics.
  This paper investigates the effect of microreboots on end-users of an
eBay-like online auction application; we find that microreboots are nearly as
effective as full reboots, but are significantly less disruptive in terms of
downtime and lost work. In our experiments, microreboots reduced the number of
failed user requests by 65% and the perceived downtime by 78% compared to a
server process restart. We also show how to replace user-visible transient
failures with transparent call-retry, at the cost of a slight increase in
end-user-visible latency during recovery. Due to their low cost, microreboots
can be used aggressively, even when their necessity is less than certain, hence
adding to the reduced recovery time a reduction in the fault detection time,
which further improves availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403008</id><created>2004-03-06</created><updated>2004-12-31</updated><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Pasechnik</keyname><forenames>Dmitrii V.</forenames></author></authors><title>Polynomial-time computing over quadratic maps I: sampling in real
  algebraic sets</title><categories>cs.SC cs.CG math.AG</categories><comments>34 pages, LaTeX (Computational Complexity (cc.cls) class used);
  updated version, to appear in Comp. Complexity</comments><acm-class>I.1.2; G.1.5</acm-class><journal-ref>Computational Complexity 14(2005) 20-52</journal-ref><doi>10.1007/s00037-005-0189-7</doi><abstract>  Given a quadratic map Q : K^n -&gt; K^k defined over a computable subring D of a
real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider
the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a
procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of
(real univariate representations of) sampling points in K^n that intersects
nontrivially each connected component of Z. As soon as k=o(n), this is faster
than the standard methods that all have exponential dependence on n in the
complexity. In particular, our procedure is polynomial-time for constant k. In
contrast, the best previously known procedure (due to A.Barvinok) is only
capable of deciding in n^O(k^2) operations the nonemptiness (rather than
constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and
homogeneous Q.
  A by-product of our procedure is a bound (dn)^O(k) on the number of connected
components of Z.
  The procedure consists of exact symbolic computations in D and outputs
vectors of algebraic numbers. It involves extending K by infinitesimals and
subsequent limit computation by a novel procedure that utilizes knowledge of an
explicit isomorphism between real algebraic sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403009</id><created>2004-03-08</created><updated>2004-03-11</updated><authors><author><keyname>Schmied</keyname><forenames>Wolfram</forenames></author></authors><title>Demolishing Searle's Chinese Room</title><categories>cs.AI cs.GL</categories><comments>3 pages, 0 figures; inserted missing ``no'' in section 3</comments><acm-class>I.2.0</acm-class><abstract>  Searle's Chinese Room argument is refuted by showing that he has actually
given two different versions of the room, which fail for different reasons.
Hence, Searle does not achieve his stated goal of showing ``that a system could
have input and output capabilities that duplicated those of a native Chinese
speaker and still not understand Chinese''.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403010</id><created>2004-03-08</created><authors><author><keyname>Appel</keyname><forenames>Andrew W.</forenames></author><author><keyname>Felty</keyname><forenames>Amy P.</forenames></author></authors><title>Polymorphic lemmas and definitions in Lambda Prolog and Twelf</title><categories>cs.LO cs.PL</categories><acm-class>F.3.1; D.2.4; I.2.3; D.1.6</acm-class><journal-ref>Andrew W. Appel and Amy P. Felty, Polymorphic Lemmas and
  Definitions in Lambda Prolog and Twelf, Theory and Practice of Logic
  Programming, 4(1&amp;2):1-39, January &amp; March 2004</journal-ref><abstract>  Lambda Prolog is known to be well-suited for expressing and implementing
logics and inference systems. We show that lemmas and definitions in such
logics can be implemented with a great economy of expression. We encode a
higher-order logic using an encoding that maps both terms and types of the
object logic (higher-order logic) to terms of the metalanguage (Lambda Prolog).
We discuss both the Terzo and Teyjus implementations of Lambda Prolog. We also
encode the same logic in Twelf and compare the features of these two
metalanguages for our purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403011</id><created>2004-03-09</created><authors><author><keyname>Alpuente</keyname><forenames>Maria</forenames></author><author><keyname>Hanus</keyname><forenames>Michael</forenames></author><author><keyname>Lucas</keyname><forenames>Salvador</forenames></author><author><keyname>Vidal</keyname><forenames>German</forenames></author></authors><title>Specialization of Functional Logic Programs Based on Needed Narrowing</title><categories>cs.PL</categories><comments>48 pages. This paper has been accepted for publication in the Journal
  of Theory and Practice of Logic Programming. In contrast to the journal
  version, this paper contains the detailed proofs of the results presented in
  this paper</comments><acm-class>D.1.1; D.1.6; D.3.4</acm-class><abstract>  Many functional logic languages are based on narrowing, a unification-based
goal-solving mechanism which subsumes the reduction mechanism of functional
languages and the resolution principle of logic languages. Needed narrowing is
an optimal evaluation strategy which constitutes the basis of modern
(narrowing-based) lazy functional logic languages. In this work, we present the
fundamentals of partial evaluation in such languages. We provide correctness
results for partial evaluation based on needed narrowing and show that the nice
properties of this strategy are essential for the specialization process. In
particular, the structure of the original program is preserved by partial
evaluation and, thus, the same evaluation strategy can be applied for the
execution of specialized programs. This is in contrast to other partial
evaluation schemes for lazy functional logic programs which may change the
program structure in a negative way. Recent proposals for the partial
evaluation of declarative multi-paradigm programs use (some form of) needed
narrowing to perform computations at partial evaluation time. Therefore, our
results constitute the basis for the correctness of such partial evaluators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403012</id><created>2004-03-09</created><authors><author><keyname>Wolpert</keyname><forenames>David H.</forenames></author><author><keyname>Bieniawski</keyname><forenames>Stefan</forenames></author></authors><title>Distributed Control by Lagrangian Steepest Descent</title><categories>cs.MA cs.GT nlin.AO</categories><comments>8 pages</comments><acm-class>J.6; J.7; G.m</acm-class><abstract>  Often adaptive, distributed control can be viewed as an iterated game between
independent players. The coupling between the players' mixed strategies,
arising as the system evolves from one instant to the next, is determined by
the system designer. Information theory tells us that the most likely joint
strategy of the players, given a value of the expectation of the overall
control objective function, is the minimizer of a Lagrangian function of the
joint strategy. So the goal of the system designer is to speed evolution of the
joint strategy to that Lagrangian minimizing point, lower the expectated value
of the control objective function, and repeat. Here we elaborate the theory of
algorithms that do this using local descent procedures, and that thereby
achieve efficient, adaptive, distributed control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403013</id><created>2004-03-10</created><authors><author><keyname>Candea</keyname><forenames>George</forenames></author></authors><title>Predictable Software -- A Shortcut to Dependable Computing ?</title><categories>cs.OS cs.DC</categories><comments>6 pages; submitted to 11th ACM SIGOPS European Workshop</comments><acm-class>D.4.7</acm-class><journal-ref>WIP Session, USENIX Technical Conference, Boston, MA, June 2004</journal-ref><abstract>  Many dependability techniques expect certain behaviors from the underlying
subsystems and fail in chaotic ways if these expectations are not met. Under
expected circumstances, however, software tends to work quite well. This paper
suggests that, instead of fixing elusive bugs or rewriting software, we improve
the predictability of conditions faced by our programs. This approach might be
a cheaper and faster way to improve dependability of software. After
identifying some of the common triggers of unpredictability, the paper
describes three engineering principles that hold promise in combating
unpredictability, suggests a way to benchmark predictability, and outlines a
brief research agenda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403014</id><created>2004-03-11</created><updated>2004-03-12</updated><authors><author><keyname>Motwani</keyname><forenames>Girish</forenames></author><author><keyname>Nair</keyname><forenames>Sandhya G.</forenames></author></authors><title>Search Efficiency in Indexing Structures for Similarity Searching</title><categories>cs.DB</categories><acm-class>H.2.m</acm-class><abstract>  Similarity searching finds application in a wide variety of domains including
multilingual databases, computational biology, pattern recognition and text
retrieval. Similarity is measured in terms of a distance function, edit
distance, in general metric spaces, which is expensive to compute. Indexing
techniques can be used reduce the number of distance computations. We present
an analysis of various existing similarity indexing structures for the same.
The performance obtained using the index structures studied was found to be
unsatisfactory . We propose an indexing technique that combines the features of
clustering with M tree(MTB) and the results indicate that this gives better
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403015</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403015</id><created>2004-03-11</created><updated>2004-03-11</updated><authors><author><keyname>Adachi</keyname><forenames>Ichiro</forenames></author><author><keyname>Hibino</keyname><forenames>Taisuke</forenames></author><author><keyname>Hinz</keyname><forenames>Luc</forenames></author><author><keyname>Itoh</keyname><forenames>Ryosuke</forenames></author><author><keyname>Katayama</keyname><forenames>Nobu</forenames></author><author><keyname>Nishida</keyname><forenames>Shohei</forenames></author><author><keyname>Ronga</keyname><forenames>Frederic</forenames></author><author><keyname>Tsukamoto</keyname><forenames>Toshifumi</forenames></author><author><keyname>Yokoyama</keyname><forenames>Masahiko</forenames></author></authors><title>Belle Computing System</title><categories>cs.DC</categories><comments>6 pages, 6 figures, talk given at ACAT03, Tsukuba, KEK, December 1-5,
  Japan</comments><acm-class>B.8.2;C.4</acm-class><journal-ref>Nucl.Instrum.Meth. A534 (2004) 53-58</journal-ref><doi>10.1016/j.nima.2004.07.058</doi><abstract>  We describe the present status of the computing system in the Belle
experiment at the KEKB $e^+e^-$ asymmetric-energy collider. So far, we have
logged more than 160 fb$^{-1}$ of data, corresponding to the world's largest
data sample of 170M $B\bar{B}$ pairs at the $\Upsilon(4S)$ energy region. A
large amount of event data has to be processed to produce an analysis event
sample in a timely fashion. In addition, Monte Carlo events have to be created
to control systematic errors accurately. This requires stable and efficient
usage of computing resources. Here we review our computing model and then
describe how we efficiently proceed DST/MC productions in our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403016</id><created>2004-03-12</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Zoeteweij</keyname><forenames>Peter</forenames></author></authors><title>A Comparative Study of Arithmetic Constraints on Integer Intervals</title><categories>cs.PL cs.AI</categories><comments>24 pages. To appear in &quot;Recent Advances in Constraints, 2003&quot; K.R.
  Apt, F. Fages, F. Rossi, P. Szeredi and J. Vancza, eds, LNAI 3010,
  Springer-Verlag, 2004</comments><acm-class>D.3.2; D.3.3</acm-class><abstract>  We propose here a number of approaches to implement constraint propagation
for arithmetic constraints on integer intervals. To this end we introduce
integer interval arithmetic. Each approach is explained using appropriate proof
rules that reduce the variable domains. We compare these approaches using a set
of benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403017</id><created>2004-03-12</created><authors><author><keyname>Nieto-Santisteban</keyname><forenames>Maria A.</forenames></author><author><keyname>O'Mullane</keyname><forenames>William</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Li</keyname><forenames>Nolan</forenames></author><author><keyname>Budavari</keyname><forenames>Tamas</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Thakar</keyname><forenames>Aniruddha R.</forenames></author></authors><title>Extending the SDSS Batch Query System to the National Virtual
  Observatory Grid</title><categories>cs.DB</categories><comments>original available at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=714</comments><report-no>MSR-TR-2004-12</report-no><acm-class>H.2.4</acm-class><abstract>  The Sloan Digital Sky Survey science database is approaching 2TB. While the
vast majority of queries normally execute in seconds or minutes, this
interactive execution time can be disproportionately increased by a small
fraction of queries that take hours or days to run; either because they require
non-index scans of the largest tables or because they request very large result
sets. In response to this, we added a multi-queue job submission and tracking
system. The transfer of very large result sets from queries over the network is
another serious problem. Statistics suggested that much of this data transfer
is unnecessary; users would prefer to store results locally in order to allow
further cross matching and filtering. To allow local analysis, we implemented a
system that gives users their own personal database (MyDB) at the portal site.
Users may transfer data to their MyDB, and then perform further analysis before
extracting it to their own machine.
  We intend to extend the MyDB and asynchronous query ideas to multiple NVO
nodes. This implies development, in a distributed manner, of several features,
which have been demonstrated for a single node in the SDSS Batch Query System
(CasJobs). The generalization of asynchronous queries necessitates some form of
MyDB storage as well as workflow tracking services on each node and
coordination strategies among nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403018</id><created>2004-03-12</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author></authors><title>The World Wide Telescope: An Archetype for Online Science</title><categories>cs.DB</categories><comments>6 pages, msword posted at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=590</comments><report-no>MSR-TR-2002-75</report-no><acm-class>H.0</acm-class><journal-ref>CACM, V. 45.11, pp. 50-54, Nov. 2002</journal-ref><abstract>  Most scientific data will never be directly examined by scientists; rather it
will be put into online databases where it will be analyzed and summarized by
computer programs. Scientists increasingly see their instruments through online
scientific archives and analysis tools, rather than examining the raw data.
Today this analysis is primarily driven by scientists asking queries, but
scientific archives are becoming active databases that self-organize and
recognize interesting and anomalous facts as data arrives. In some fields, data
from many different archives can be cross-correlated to produce new insights.
Astronomy presents an excellent example of these trends; and, federating
Astronomy archives presents interesting challenges for computer scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403019</id><created>2004-03-12</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>Distributed Computing Economics</title><categories>cs.NI cs.DC</categories><comments>6 pages, orginal at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=655</comments><report-no>MSR-TR-2003-24</report-no><acm-class>K.6.0</acm-class><abstract>  Computing economics are changing. Today there is rough price parity between
(1) one database access, (2) ten bytes of network traffic, (3) 100,000
instructions, (4) 10 bytes of disk storage, and (5) a megabyte of disk
bandwidth. This has implications for how one structures Internet-scale
distributed computing: one puts computing as close to the data as possible in
order to avoid expensive network traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403020</id><created>2004-03-12</created><authors><author><keyname>Thakar</keyname><forenames>Aniruddha R.</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter Z.</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte
  Astronomical Archive from Object to Relational DBMS</title><categories>cs.DB</categories><acm-class>H.0</acm-class><journal-ref>Comput.Sci.Eng. 5 (2003) 16-29</journal-ref><abstract>  The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403021</id><created>2004-03-12</created><authors><author><keyname>Barclay</keyname><forenames>Tom</forenames></author><author><keyname>Chong</keyname><forenames>Wyman</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>A Quick Look at SATA Disk Performance</title><categories>cs.DB cs.PF</categories><report-no>MSR-TR-2003-70, Oct. 2003</report-no><acm-class>C.4</acm-class><abstract>  We have been investigating the use of low-cost, commodity components for
multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are
white box PCs containing the largest ATA drives, value-priced AMD or Intel
processors, and inexpensive ECC memory. One issue has been the wiring mess, air
flow problems, length restrictions, and connector failures created by seven or
more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount
chassis. Large capacity Serial ATA (SATA) drives have recently become widely
available for the PC environment at a reasonable price. In addition to being
faster, the SATA connectors seem more reliable, have a more reasonable length
restriction (1m) and allow better airflow. We tested two drive brands along
with two RAID controllers to evaluate SATA drive performance and reliablility.
This paper documents our results so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403022</id><created>2004-03-12</created><updated>2004-06-25</updated><authors><author><keyname>N&#xfc;sken</keyname><forenames>Michael</forenames></author><author><keyname>Ziegler</keyname><forenames>Martin</forenames></author></authors><title>Fast Multipoint-Evaluation of Bivariate Polynomials</title><categories>cs.DS</categories><comments>12 pages, 1 figure. To appear in Proc. 12th ESA 2004</comments><acm-class>F.2.1</acm-class><abstract>  We generalize univariate multipoint evaluation of polynomials of degree n at
sublinear amortized cost per point. More precisely, it is shown how to evaluate
a bivariate polynomial p of maximum degree less than n, specified by its n^2
coefficients, simultaneously at n^2 given points using a total of O(n^{2.667})
arithmetic operations. In terms of the input size N being quadratic in n, this
amounts to an amortized cost of O(N^{0.334}) per point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403023</id><created>2004-03-14</created><authors><author><keyname>Belal</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Abdelhamid</keyname><forenames>Abdelhamid S.</forenames></author></authors><title>Secure Transmission of Sensitive data using multiple channels</title><categories>cs.CR</categories><comments>5 pages</comments><acm-class>C.2.0</acm-class><abstract>  A new scheme for transmitting sensitive data is proposed, the proposed scheme
depends on partitioning the output of a block encryption module using the
Chinese Remainder Theorem among a set of channels. The purpose of using the
Chinese Remainder Theorem is to hide the cipher text in order to increase the
difficulty of attacking the cipher. The theory, implementation and the security
of this scheme are described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403024</id><created>2004-03-15</created><updated>2004-09-27</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Uniform Proofs of Order Independence for Various Strategy Elimination
  Procedures</title><categories>cs.GT cs.LO</categories><comments>48 pages</comments><acm-class>J.4</acm-class><journal-ref>Contributions to Theoretical Economics, Vol. 4: No. 1, Article 5,
  2004, http://www.bepress.com/bejte/contributions/vol4/iss1/art5</journal-ref><abstract>  We provide elementary and uniform proofs of order independence for various
strategy elimination procedures for finite strategic games, both for dominance
by pure and by mixed strategies. The proofs follow the same pattern and focus
on the structural properties of the dominance relations. They rely on Newman's
Lemma established in 1942 and related results on the abstract reduction
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403025</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403025</id><created>2004-03-15</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>Distribution of Mutual Information from Complete and Incomplete Data</title><categories>cs.LG cs.AI cs.IT math.IT math.ST stat.TH</categories><comments>26 pages, LaTeX, 5 figures, 4 tables</comments><report-no>IDSIA-11-02</report-no><acm-class>I.2</acm-class><journal-ref>Computational Statistics &amp; Data Analysis, Vol.48, No.3, March
  2005, pages 633--657</journal-ref><abstract>  Mutual information is widely used, in a descriptive way, to measure the
stochastic dependence of categorical random variables. In order to address
questions such as the reliability of the descriptive value, one must consider
sample-to-population inferential approaches. This paper deals with the
posterior distribution of mutual information, as obtained in a Bayesian
framework by a second-order Dirichlet prior distribution. The exact analytical
expression for the mean, and analytical approximations for the variance,
skewness and kurtosis are derived. These approximations have a guaranteed
accuracy level of the order O(1/n^3), where n is the sample size. Leading order
approximations for the mean and the variance are derived in the case of
incomplete samples. The derived analytical expressions allow the distribution
of mutual information to be approximated reliably and quickly. In fact, the
derived expressions can be computed with the same order of complexity needed
for descriptive mutual information. This makes the distribution of mutual
information become a concrete alternative to descriptive mutual information in
many applications which would benefit from moving to the inductive side. Some
of these prospective applications are discussed, and one of them, namely
feature selection, is shown to perform significantly better when inductive
mutual information is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403026</id><created>2004-03-15</created><authors><author><keyname>Pyla</keyname><forenames>Pardha S.</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author><author><keyname>Hartson</keyname><forenames>H. Rex</forenames></author></authors><title>What we should teach, but don't: Proposal for a cross pollinated HCI-SE
  curriculum</title><categories>cs.OH</categories><acm-class>K.3.2</acm-class><abstract>  Software engineering (SE) and usability engineering (UE), as disciplines,
have reached substantial levels of maturity. Each of these two disciplines is
now well represented with respect to most computer science (CS) curricula. But,
the two disciplines are practiced almost independently - missing oppurtunities
to collaborate, coordinate and communicate about the overall design - and
thereby contributing to system failures. Today, a confluence of several
ingredients contribute to these failures: the increasing importance of the user
interface (UI) component in the overall system, the independent maturation of
the human computer interaction area, and the lack of a cohesive process model
to integrate the UI experts' UE development efforts with that of SE. This in
turn, we believe, is a result of a void in computing curricula: a lack of
education and training regarding the importance of communication, collaboration
and coordination between the SE and UE processes. In this paper we describe the
current approach to teaching SE and UE and its shortcomings. We identify and
analyze the barriers and issues involved in developing systems having
substantial interactive components. We then propose four major themes of
learning for a comprehensive computing curriculum integrating SE, UE, and
system architectures in a project environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403027</id><created>2004-03-16</created><updated>2004-05-11</updated><authors><author><keyname>Casasnovas</keyname><forenames>Jaume</forenames></author><author><keyname>Miro</keyname><forenames>Joe</forenames></author><author><keyname>Moya</keyname><forenames>Manuel</forenames></author><author><keyname>Rossello</keyname><forenames>Francesc</forenames></author></authors><title>An approach to membrane computing under inexactitude</title><categories>cs.OH cs.NE</categories><comments>20 pages, 0 figures</comments><acm-class>F.4.2; F.1.1</acm-class><abstract>  In this paper we introduce a fuzzy version of symport/antiport membrane
systems. Our fuzzy membrane systems handle possibly inexact copies of reactives
and their rules are endowed with threshold functions that determine whether a
rule can be applied or not to a given set of objects, depending of the degree
of accuracy of these objects to the reactives specified in the rule. We prove
that these fuzzy membrane systems generate exactly the recursively enumerable
finite-valued fuzzy sets of natural numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403028</id><created>2004-03-16</created><authors><author><keyname>Carro</keyname><forenames>Manuel</forenames></author></authors><title>An Application of Rational Trees in a Logic Programming Interpreter for
  a Procedural Language</title><categories>cs.DS cs.LO</categories><comments>LaTeX2e, 13 pages, 2 tables, 11 figures (several of them, text). Yet
  unpublished</comments><acm-class>D.1.6 ; D.3.2 ; D.3.3 ; D.3.4 ; E.1 ; E.2</acm-class><abstract>  We describe here a simple application of rational trees to the implementation
of an interpreter for a procedural language written in a logic programming
language. This is possible in languages designed to support rational trees
(such as Prolog II and its descendants), but also in traditional Prolog, whose
data structures are initially based on Herbrand terms, but in which
implementations often omit the occurs check needed to avoid the creation of
infinite data structures. We provide code implementing two interpreters, one of
which needs non-occurs-check unification, which makes it faster (and more
economic). We provide experimental data supporting this, and we argue that
rational trees are interesting enough as to receive thorough support inside the
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403029</id><created>2004-03-17</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author><author><keyname>Christensen</keyname><forenames>Kenneth J.</forenames></author><author><keyname>Yoshigoe</keyname><forenames>Kenji</forenames></author></authors><title>Characterization of the Burst Stabilization Protocol for the RR/RR CICQ
  Switch</title><categories>cs.NI cs.PF</categories><comments>Presented at the 28th Annual IEEE Conference on Local Computer
  Networks (LCN), Bonn/Konigswinter, Germany, Oct 20-24, 2003</comments><acm-class>B.6.3;B.7.1;B.8.2;C.2.1</acm-class><abstract>  Input buffered switches with Virtual Output Queueing (VOQ) can be unstable
when presented with unbalanced loads. Existing scheduling algorithms, including
iSLIP for Input Queued (IQ) switches and Round Robin (RR) for Combined Input
and Crossbar Queued (CICQ) switches, exhibit instability for some schedulable
loads. We investigate the use of a queue length threshold and bursting
mechanism to achieve stability without requiring internal speed-up. An
analytical model is developed to prove that the burst stabilization protocol
achieves stability and to predict the minimum burst value needed as a function
of offered load. The analytical model is shown to have very good agreement with
simulation results. These results show the advantage of the RR/RR CICQ switch
as a contender for the next generation of high-speed switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403030</id><created>2004-03-17</created><authors><author><keyname>Christensen</keyname><forenames>K. J.</forenames></author><author><keyname>Yoshigoe</keyname><forenames>K.</forenames></author><author><keyname>Roginsky</keyname><forenames>A.</forenames></author><author><keyname>Gunther</keyname><forenames>N. J.</forenames></author></authors><title>Performance Evaluation of Packet-to-Cell Segmentation Schemes in Input
  Buffered Packet Switches</title><categories>cs.NI cs.PF</categories><comments>To be presented at the IEEE International Conference on
  Communications (ICC 2004) Paris, France, June 20-24 2004</comments><acm-class>B.6.3;B.7.1;B.8.2;C.2.1</acm-class><abstract>  Most input buffered packet switches internally segment variable-length
packets into fixed-length cells. The last cell in a segmented packet will
contain overhead bytes if the packet length is not evenly divisible by the cell
length. Switch speed-up is used to compensate for this overhead. In this paper,
we develop an analytical model of a single-server queue where an input stream
of packets is segmented into cells for service. Analytical models are developed
for M/M/1, M/H2/1, and M/E2/1 queues with a discretized (or quantized) service
time. These models and simulation using real packet traces are used to evaluate
the effect of speed-up on mean queue length. We propose and evaluate a new
method of segmenting a packet trailer and subsequent packet header into a
single cell. This cell merging method reduces the required speed-up. No changes
to switch-matrix scheduling algorithms are needed. Simulation with a packet
trace shows a reduction in the needed speed-up for an iSLIP scheduled input
buffered switch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403031</id><created>2004-03-19</created><updated>2004-03-20</updated><authors><author><keyname>Eliashberg</keyname><forenames>Victor</forenames></author></authors><title>Concept of E-machine: How does a &quot;dynamical&quot; brain learn to process
  &quot;symbolic&quot; information? Part I</title><categories>cs.AI cs.LG</categories><comments>40 pages, 15 figures. The author is a consulting professor at the
  Stanford University, Department of Electrical Engineering and the president
  of Avel Electronics (consulting company)</comments><acm-class>I.2.0</acm-class><abstract>  The human brain has many remarkable information processing characteristics
that deeply puzzle scientists and engineers. Among the most important and the
most intriguing of these characteristics are the brain's broad universality as
a learning system and its mysterious ability to dynamically change
(reconfigure) its behavior depending on a combinatorial number of different
contexts.
  This paper discusses a class of hypothetically brain-like dynamically
reconfigurable associative learning systems that shed light on the possible
nature of these brain's properties. The systems are arranged on the general
principle referred to as the concept of E-machine.
  The paper addresses the following questions:
  1. How can &quot;dynamical&quot; neural networks function as universal programmable
&quot;symbolic&quot; machines?
  2. What kind of a universal programmable symbolic machine can form
arbitrarily complex software in the process of programming similar to the
process of biological associative learning?
  3. How can a universal learning machine dynamically reconfigure its software
depending on a combinatorial number of possible contexts?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403032</id><created>2004-03-19</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Where Fail-Safe Default Logics Fail</title><categories>cs.AI cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  Reiter's original definition of default logic allows for the application of a
default that contradicts a previously applied one. We call failure this
condition. The possibility of generating failures has been in the past
considered as a semantical problem, and variants have been proposed to solve
it. We show that it is instead a computational feature that is needed to encode
some domains into default logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403033</id><created>2004-03-21</created><authors><author><keyname>Banyasad</keyname><forenames>Omid</forenames></author><author><keyname>Cox</keyname><forenames>Philip T.</forenames></author></authors><title>Integrating design synthesis and assembly of structured objects in a
  visual design language</title><categories>cs.LO cs.PL</categories><comments>20 pages, 15 figures, to be published in The Theory and Practice of
  Logic programming (TPLP)</comments><acm-class>D.1.6; D.1.7; J.6</acm-class><abstract>  Computer Aided Design systems provide tools for building and manipulating
models of solid objects. Some also provide access to programming languages so
that parametrised designs can be expressed. There is a sharp distinction,
therefore, between building models, a concrete graphical editing activity, and
programming, an abstract, textual, algorithm-construction activity. The
recently proposed Language for Structured Design (LSD) was motivated by a
desire to combine the design and programming activities in one language. LSD
achieves this by extending a visual logic programming language to incorporate
the notions of solids and operations on solids. Here we investigate another
aspect of the LSD approach; namely, that by using visual logic programming as
the engine to drive the parametrised assembly of objects, we also gain the
powerful symbolic problem-solving capability that is the forte of logic
programming languages. This allows the designer/programmer to work at a higher
level, giving declarative specifications of a design in order to obtain the
design descriptions. Hence LSD integrates problem solving, design synthesis,
and prototype assembly in a single homogeneous programming/design environment.
We demonstrate this specification-to-final-assembly capability using the
masterkeying problem for designing systems of locks and keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403034</id><created>2004-03-23</created><updated>2006-01-29</updated><authors><author><keyname>Fluet</keyname><forenames>Matthew</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Phantom Types and Subtyping</title><categories>cs.PL</categories><comments>41 pages. Preliminary version appears in the Proceedings of the 2nd
  IFIP International Conference on Theoretical Computer Science, pp. 448--460,
  2002</comments><acm-class>D.1.1; D.3.3; F.3.3</acm-class><abstract>  We investigate a technique from the literature, called the phantom-types
technique, that uses parametric polymorphism, type constraints, and unification
of polymorphic types to model a subtyping hierarchy. Hindley-Milner type
systems, such as the one found in Standard ML, can be used to enforce the
subtyping relation, at least for first-order values. We show that this
technique can be used to encode any finite subtyping hierarchy (including
hierarchies arising from multiple interface inheritance). We formally
demonstrate the suitability of the phantom-types technique for capturing
first-order subtyping by exhibiting a type-preserving translation from a simple
calculus with bounded polymorphism to a calculus embodying the type system of
SML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403035</id><created>2004-03-23</created><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>Yi-Ping</keyname><forenames>Guo</forenames></author><author><keyname>Ming</keyname><forenames>Fang</forenames></author></authors><title>Web pages search engine based on DNS</title><categories>cs.NI cs.IR</categories><comments>4 pages,1 figure</comments><acm-class>H.3.3;H.3.7;C.2.2</acm-class><abstract>  Search engine is main access to the largest information source in this world,
Internet. Now Internet is changing every aspect of our life. Information
retrieval service may be its most important services. But for common user,
internet search service is still far from our expectation, too many unrelated
search results, old information, etc. To solve these problems, a new system,
search engine based on DNS is proposed. The original idea, detailed content and
implementation of this system all are introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403036</id><created>2004-03-23</created><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>Yi-Ping</keyname><forenames>Guo</forenames></author><author><keyname>Ming</keyname><forenames>Fang</forenames></author></authors><title>Domain resource integration system</title><categories>cs.NI cs.DL</categories><comments>6 pages,4 figures</comments><acm-class>H.3.3;H.3.7;C.2.2</acm-class><abstract>  Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a hierarchical distributed Internet information retrieval system. This system
will solve some bottleneck problems such as long update interval, poor coverage
in current web search system. DRIS will build the information retrieval
infrastructure of Internet, but not a commercial search engine. The protocol
series of DRIS are also detailed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403037</id><created>2004-03-23</created><updated>2004-11-02</updated><authors><author><keyname>Brand</keyname><forenames>Sebastian</forenames></author><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Schedulers and Redundancy for a Class of Constraint Propagation Rules</title><categories>cs.DS cs.PL</categories><comments>25 pages, to appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>I.2.2; I.2.3; D.1.2; D.3.3; D.3.4</acm-class><abstract>  We study here schedulers for a class of rules that naturally arise in the
context of rule-based constraint programming. We systematically derive a
scheduler for them from a generic iteration algorithm of [Apt 2000]. We apply
this study to so-called membership rules of [Apt and Monfroy 2001]. This leads
to an implementation that yields a considerably better performance for these
rules than their execution as standard CHR rules. Finally, we show how
redundant rules can be identified and how appropriately reduced sets of rules
can be computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403038</id><created>2004-03-23</created><authors><author><keyname>Legg</keyname><forenames>Shane</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Kumar</keyname><forenames>Akshat</forenames></author></authors><title>Tournament versus Fitness Uniform Selection</title><categories>cs.LG cs.AI</categories><comments>10 pages, 8 figures</comments><report-no>IDSIA-04-04</report-no><acm-class>I.2; I.2.6; I.2.8; F.2</acm-class><journal-ref>Proc. 2004 Congress on Evolutionary Computation (CEC-2004), pages
  2144--2151</journal-ref><abstract>  In evolutionary algorithms a critical parameter that must be tuned is that of
selection pressure. If it is set too low then the rate of convergence towards
the optimum is likely to be slow. Alternatively if the selection pressure is
set too high the system is likely to become stuck in a local optimum due to a
loss of diversity in the population. The recent Fitness Uniform Selection
Scheme (FUSS) is a conceptually simple but somewhat radical approach to
addressing this problem - rather than biasing the selection towards higher
fitness, FUSS biases selection towards sparsely populated fitness levels. In
this paper we compare the relative performance of FUSS with the well known
tournament selection scheme on a range of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403039</id><created>2004-03-23</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames></author><author><keyname>Ulrich</keyname><forenames>Stefan</forenames></author><author><keyname>Hammervold</keyname><forenames>Kathrine</forenames></author></authors><title>A Flexible Rule Compiler for Speech Synthesis</title><categories>cs.CL cs.AI</categories><comments>10 pages, 6 figures</comments><acm-class>H.5.2; F.4.3</acm-class><journal-ref>In: Klopotek, Mieczyslaw A.; Wierzchon, Slawomir T.; Trojanowski,
  Krzysztof (Eds.): &quot;Intelligent Information Processing and Web Mining -
  Proceedings of the International IIS:IIPWM?04 Conference&quot;; Springer Verlag,
  2004</journal-ref><abstract>  We present a flexible rule compiler developed for a text-to-speech (TTS)
system. The compiler converts a set of rules into a finite-state transducer
(FST). The input and output of the FST are subject to parameterization, so that
the system can be applied to strings and sequences of feature-structures. The
resulting transducer is guaranteed to realize a function (as opposed to a
relation), and therefore can be implemented as a deterministic device (either a
deterministic FST or a bimachine).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403040</id><created>2004-03-25</created><authors><author><keyname>Melancon</keyname><forenames>Guy</forenames></author><author><keyname>Philippe</keyname><forenames>Fabrice</forenames></author></authors><title>Generating connected acyclic digraphs uniformly at random</title><categories>cs.DM cs.DS</categories><comments>6 pages</comments><acm-class>F.2.2;G.2.2;G.3</acm-class><abstract>  We describe a simple algorithm based on a Markov chain process to generate
simply connected acyclic directed graphs over a fixed set of vertices. This
algorithm is an extension of a previous one, designed to generate acyclic
digraphs, non necessarily connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403041</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403041</id><created>2004-03-29</created><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>A Theory of Computation Based on Quantum Logic (I)</title><categories>cs.LO</categories><acm-class>F.1.1; F.1.2</acm-class><journal-ref>Theoretical Computer Science 344(2-3): 134-207 (2005)</journal-ref><abstract>  The (meta)logic underlying classical theory of computation is Boolean
(two-valued) logic. Quantum logic was proposed by Birkhoff and von Neumann as a
logic of quantum mechanics more than sixty years ago. The major difference
between Boolean logic and quantum logic is that the latter does not enjoy
distributivity in general. The rapid development of quantum computation in
recent years stimulates us to establish a theory of computation based on
quantum logic. The present paper is the first step toward such a new theory and
it focuses on the simplest models of computation, namely finite automata. It is
found that the universal validity of many properties of automata depend heavily
upon the distributivity of the underlying logic. This indicates that these
properties does not universally hold in the realm of quantum logic. On the
other hand, we show that a local validity of them can be recovered by imposing
a certain commutativity to the (atomic) statements about the automata under
consideration. This reveals an essential difference between the classical
theory of computation and the computation theory based on quantum logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403042</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403042</id><created>2004-03-29</created><updated>2004-05-24</updated><authors><author><keyname>Argyraki</keyname><forenames>Katerina J.</forenames></author><author><keyname>Cheriton</keyname><forenames>David R.</forenames></author></authors><title>Protecting Public-Access Sites Against Distributed Denial-of-Service
  Attacks</title><categories>cs.NI</categories><comments>Description and evaluation of a filter management protocol that
  reactively protects public-access sites against DDoS attacks. 12 pages long</comments><acm-class>C.2.2</acm-class><journal-ref>Updated versions in Proc. USENIX Annual Technical Conference,
  April 2005, and IEEE/ACM Transactions on Networking, 17(4):1284-1297, August
  2009</journal-ref><doi>10.1109/TNET.2008.2007431</doi><abstract>  A distributed denial-of-service (DDoS) attack can flood a victim site with
malicious traffic, causing service disruption or even complete failure.
Public-access sites like amazon or ebay are particularly vulnerable to such
attacks, because they have no way of a priori blocking unauthorized traffic.
  We present Active Internet Traffic Filtering (AITF), a mechanism that
protects public-access sites from highly distributed attacks by causing
undesired traffic to be blocked as close as possible to its sources. We
identify filters as a scarce resource and show that AITF protects a significant
amount of the victim's bandwidth, while requiring from each participating
router a number of filters that can be accommodated by today's routers. AITF is
incrementally deployable, because it offers a substantial benefit even to the
first sites that deploy it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403043</id><created>2004-03-30</created><updated>2004-04-22</updated><authors><author><keyname>Gligoroski</keyname><forenames>Danilo</forenames></author></authors><title>Stream cipher based on quasigroup string transformations in $Z_p^*$</title><categories>cs.CR</categories><comments>Small revisions and added references</comments><acm-class>E.3</acm-class><abstract>  In this paper we design a stream cipher that uses the algebraic structure of
the multiplicative group $\bbbz_p^*$ (where p is a big prime number used in
ElGamal algorithm), by defining a quasigroup of order $p-1$ and by doing
quasigroup string transformations. The cryptographical strength of the proposed
stream cipher is based on the fact that breaking it would be at least as hard
as solving systems of multivariate polynomial equations modulo big prime number
$p$ which is NP-hard problem and there are no known fast randomized or
deterministic algorithms for solving it. Unlikely the speed of known ciphers
that work in $\bbbz_p^*$ for big prime numbers $p$, the speed of this stream
cipher both in encryption and decryption phase is comparable with the fastest
symmetric-key stream ciphers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0403044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0403044</id><created>2004-03-31</created><updated>2004-05-05</updated><authors><author><keyname>Roy</keyname><forenames>Amitabha</forenames></author><author><keyname>Gopinath</keyname><forenames>K.</forenames></author></authors><title>Scalable Probabilistic Models for 802.11 Protocol Verification</title><categories>cs.LO cs.NI</categories><comments>Currently in the process of submission</comments><acm-class>F.4.1</acm-class><abstract>  The IEEE 802.11 protocol is a popular standard for wireless local area
networks. Its medium access control layer (MAC) is a carrier sense multiple
access with collision avoidance (CSMA/CA) design and includes an exponential
backoff mechanism that makes it a possible target for probabilistic model
checking. In this work, we identify ways to increase the scope of application
of probabilistic model checking to the 802.11 MAC. Current techniques do not
scale to networks of even moderate size. To work around this problem, we
identify properties of the protocol that can be used to simplify the models and
make verification feasible. Using these observations, we directly optimize the
probabilistic timed automata models while preserving probabilistic reachability
measures. We substantiate our claims of significant reduction by our results
from using the probabilistic model checker PRISM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404001</id><created>2004-04-01</created><authors><author><keyname>Greenwood</keyname><forenames>Garrison W.</forenames></author></authors><title>On the Practicality of Intrinsic Reconfiguration As a Fault Recovery
  Method in Analog Systems</title><categories>cs.PF cs.NE</categories><comments>6 pages</comments><acm-class>B.8.1</acm-class><abstract>  Evolvable hardware combines the powerful search capability of evolutionary
algorithms with the flexibility of reprogrammable devices, thereby providing a
natural framework for reconfiguration. This framework has generated an interest
in using evolvable hardware for fault-tolerant systems because reconfiguration
can effectively deal with hardware faults whenever it is impossible to provide
spares. But systems cannot tolerate faults indefinitely, which means
reconfiguration does have a deadline. The focus of previous evolvable hardware
research relating to fault-tolerance has been primarily restricted to restoring
functionality, with no real consideration of time constraints. In this paper we
are concerned with evolvable hardware performing reconfiguration under deadline
constraints. In particular, we investigate reconfigurable hardware that
undergoes intrinsic evolution. We show that fault recovery done by intrinsic
reconfiguration has some restrictions, which designers cannot ignore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404002</id><created>2004-04-01</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Mathematical Analysis of Multi-Agent Systems</title><categories>cs.RO cs.MA</categories><comments>latex, 15 figures, 42 pages</comments><acm-class>I.2.9; I.2.11; I.6.5</acm-class><abstract>  We review existing approaches to mathematical modeling and analysis of
multi-agent systems in which complex collective behavior arises out of local
interactions between many simple agents. Though the behavior of an individual
agent can be considered to be stochastic and unpredictable, the collective
behavior of such systems can have a simple probabilistic description. We show
that a class of mathematical models that describe the dynamics of collective
behavior of multi-agent systems can be written down from the details of the
individual agent controller. The models are valid for Markov or memoryless
agents, in which each agents future state depends only on its present state and
not any of the past states. We illustrate the approach by analyzing in detail
applications from the robotics domain: collaboration and foraging in groups of
robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404003</id><created>2004-04-01</created><authors><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author><author><keyname>Catania</keyname><forenames>Barbara</forenames></author><author><keyname>Gori</keyname><forenames>Roberta</forenames></author></authors><title>Enhancing the expressive power of the U-Datalog language</title><categories>cs.DB</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 1,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 1, 2001</journal-ref><abstract>  U-Datalog has been developed with the aim of providing a set-oriented logical
update language, guaranteeing update parallelism in the context of a
Datalog-like language. In U-Datalog, updates are expressed by introducing
constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote
deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP
program. In this framework, a set of updates (constraints) is satisfiable if it
does not represent an inconsistent theory, that is, it does not require the
insertion and the deletion of the same fact. This approach resembles a very
simple form of negation. However, on the other hand, U-Datalog does not provide
any mechanism to explicitly deal with negative information, resulting in a
language with limited expressive power. In this paper, we provide a semantics,
based on stratification, handling the use of negated atoms in U-Datalog
programs, and we show which problems arise in defining a compositional
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404004</id><created>2004-04-02</created><authors><author><keyname>Wagner</keyname><forenames>Liam</forenames></author></authors><title>Dealing With Curious Players in Secure Networks</title><categories>cs.CR cs.GT cs.MA</categories><comments>4 pages and 1 figure</comments><acm-class>C.2.0; I.2.1</acm-class><abstract>  In secure communications networks there are a great number of user
behavioural problems, which need to be dealt with. Curious players pose a very
real and serious threat to the integrity of such a network. By traversing a
network a Curious player could uncover secret information, which that user has
no need to know, by simply posing as a loyalty check. Loyalty checks are done
simply to gauge the integrity of the network with respect to players who act in
a malicious manner. We wish to propose a method, which can deal with Curious
players trying to obtain &quot;Need to Know&quot; information using a combined
Fault-tolerant, Cryptographic and Game Theoretic Approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404005</id><created>2004-04-02</created><authors><author><keyname>Dornseif</keyname><forenames>Maximillian</forenames></author></authors><title>Government mandated blocking of foreign Web content</title><categories>cs.CY cs.NI</categories><comments>Preprint, revised 30.6.2003</comments><acm-class>K.4.1;K.4.2; K.5.2</acm-class><journal-ref>In: Jan von Knop, Wilhelm Haverkamp, Eike Jessen (Editors)
  Security, E-Learning, E-Services: Proceedings of the 17. DFN-Arbeitstagung
  ueber Kommunikationsnetze, Duesseldorf 2003, ISBN 3-88579-373-3; Series:
  Lecture Notes in Informatics ISSN 1617-5468; Pages 617-648</journal-ref><abstract>  Blocking of foreign Web content by Internet access providers has been a hot
topic for the last 18 months in Germany. Since fall 2001 the state of
North-Rhine-Westphalia very actively tries to mandate such blocking. This paper
will take a technical view on the problems imposed by the blocking orders and
blocking content at access or network provider level in general. It will also
give some empirical data on the effects of the blocking orders to help in the
legal assessment of the orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404006</id><created>2004-04-04</created><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames></author></authors><title>Delimited continuations in natural language: quantification and polarity
  sensitivity</title><categories>cs.CL cs.PL</categories><comments>10 pages</comments><acm-class>D.3.3; J.5</acm-class><journal-ref>In CW'04: Proceedings of the 4th ACM SIGPLAN workshop on
  continuations, ed. Hayo Thielecke, 55-64. Technical report CSR-04-1, School
  of Computer Science, University of Birmingham (2004)</journal-ref><abstract>  Making a linguistic theory is like making a programming language: one
typically devises a type system to delineate the acceptable utterances and a
denotational semantics to explain observations on their behavior. Via this
connection, the programming language concept of delimited continuations can
help analyze natural language phenomena such as quantification and polarity
sensitivity. Using a logical metalanguage whose syntax includes control
operators and whose semantics involves evaluation order, these analyses can be
expressed in direct style rather than continuation-passing style, and these
phenomena can be thought of as computational side effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404007</id><created>2004-04-04</created><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames></author></authors><title>Polarity sensitivity and evaluation order in type-logical grammar</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>J.5; D.3.3</acm-class><journal-ref>Proceedings of the 2004 Human Language Technology Conference of
  the North American Chapter of the Association for Computational Linguistics</journal-ref><abstract>  We present a novel, type-logical analysis of_polarity sensitivity_: how
negative polarity items (like &quot;any&quot; and &quot;ever&quot;) or positive ones (like &quot;some&quot;)
are licensed or prohibited. It takes not just scopal relations but also linear
order into account, using the programming-language notions of delimited
continuations and evaluation order, respectively. It thus achieves greater
empirical coverage than previous proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404008</id><created>2004-04-05</created><updated>2004-04-19</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames></author></authors><title>Efficient dot product over word-size finite fields</title><categories>cs.SC</categories><proxy>ccsd ccsd-00001380</proxy><report-no>IMAG - LMC RR n 1064 - I</report-no><acm-class>F.2.4; B.2.4</acm-class><abstract>  We want to achieve efficiency for the exact computation of the dot product of
two vectors over word-size finite fields. We therefore compare the practical
behaviors of a wide range of implementation techniques using different
representations. The techniques used include oating point representations,
discrete logarithms, tabulations, Montgomery reduction, delayed modulus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404009</id><created>2004-04-05</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames></author></authors><title>Tabular Parsing</title><categories>cs.CL</categories><comments>21 pages, 14 figures</comments><acm-class>F.4.2</acm-class><journal-ref>M.-J. Nederhof and G. Satta. Tabular Parsing. In C. Martin-Vide,
  V. Mitrana, and G. Paun, editors, Formal Languages and Applications, Studies
  in Fuzziness and Soft Computing 148, pages 529-549. Springer, 2004</journal-ref><abstract>  This is a tutorial on tabular parsing, on the basis of tabulation of
nondeterministic push-down automata. Discussed are Earley's algorithm, the
Cocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse
trees, and further issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404010</id><created>2004-04-05</created><updated>2005-08-05</updated><authors><author><keyname>Krashakov</keyname><forenames>Serge A.</forenames></author><author><keyname>Teslyuk</keyname><forenames>Anton B.</forenames></author><author><keyname>Shchur</keyname><forenames>Lev N.</forenames></author></authors><title>On the universality of rank distributions of website popularity</title><categories>cs.NI cond-mat.stat-mech</categories><comments>6 pages with 5 figures, IEEEtran.cls, paper substantially rewritten,
  new figure and tables added</comments><acm-class>K.4; J.2; H.4.0</acm-class><journal-ref>Computer Networks, 50, 1769-1780 (2006)</journal-ref><abstract>  We present an extensive analysis of long-term statistics of the queries to
websites using logs collected on several web caches in Russian academic
networks and on US IRCache caches. We check the sensitivity of the statistics
to several parameters: (1) duration of data collection, (2) geographical
location of the cache server collecting data, and (3) the year of data
collection. We propose a two-parameter modification of the Zipf law and
interpret the parameters. We find that the rank distribution of websites is
stable when approximated by the modified Zipf law. We suggest that website
popularity may be a universal property of Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404011</id><created>2004-04-05</created><authors><author><keyname>Ianni</keyname><forenames>G.</forenames></author><author><keyname>Calimeri</keyname><forenames>F.</forenames></author><author><keyname>Pietramala</keyname><forenames>A.</forenames></author><author><keyname>Santoro</keyname><forenames>M. C.</forenames></author></authors><title>Parametric external predicates for the DLV System</title><categories>cs.AI</categories><comments>10 pages</comments><acm-class>I.2.4</acm-class><abstract>  This document describes syntax, semantics and implementation guidelines in
order to enrich the DLV system with the possibility to make external C function
calls. This feature is realized by the introduction of parametric external
predicates, whose extension is not specified through a logic program but
implicitly computed through external code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404012</id><created>2004-04-05</created><authors><author><keyname>Calimeri</keyname><forenames>Francesco</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Toward the Implementation of Functions in the DLV System (Preliminary
  Technical Report)</title><categories>cs.AI</categories><comments>7 pages</comments><acm-class>I.2.4</acm-class><abstract>  This document describes the functions as they are treated in the DLV system.
We give first the language, then specify the main implementation issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404013</id><created>2004-04-05</created><authors><author><keyname>Lai</keyname><forenames>Kevin</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author><author><keyname>Fine</keyname><forenames>Leslie</forenames></author></authors><title>Tycoon: A Distributed Market-based Resource Allocation System</title><categories>cs.DC cs.MA</categories><acm-class>C.2.4; D.4.1; D.4.7; K.6.0</acm-class><abstract>  P2P clusters like the Grid and PlanetLab enable in principle the same
statistical multiplexing efficiency gains for computing as the Internet
provides for networking. The key unsolved problem is resource allocation.
Existing solutions are not economically efficient and require high latency to
acquire resources. We designed and implemented Tycoon, a market based
distributed resource allocation system based on an Auction Share scheduling
algorithm. Preliminary results show that Tycoon achieves low latency and high
fairness while providing incentives for truth-telling on the part of strategic
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404014</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404014</id><created>2004-04-06</created><authors><author><keyname>Steinbeck</keyname><forenames>Timm M.</forenames></author></authors><title>A Modular and Fault-Tolerant Data Transport Framework</title><categories>cs.DC</categories><comments>Ph.D. Thesis, Ruprecht-Karls-University Heidelberg Large, 251 pages</comments><acm-class>D.1.3; C.2.4; C.4; J.2</acm-class><abstract>  The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to
reduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output
before the data is written to permanent storage. To cope with these data rates
a large PC cluster system is being designed to scale to several 1000 nodes,
connected by a fast network. For the software that will run on these nodes a
flexible data transport and distribution software framework, described in this
thesis, has been developed. The framework consists of a set of separate
components, that can be connected via a common interface. This allows to
construct different configurations for the HLT, that are even changeable at
runtime. To ensure a fault-tolerant operation of the HLT, the framework
includes a basic fail-over mechanism that allows to replace whole nodes after a
failure. The mechanism will be further expanded in the future, utilizing the
runtime reconnection feature of the framework's component interface. To connect
cluster nodes a communication class library is used that abstracts from the
actual network technology and protocol used to retain flexibility in the
hardware choice. It contains already two working prototype versions for the TCP
protocol as well as SCI network adapters. Extensions can be added to the
library without modifications to other parts of the framework. Extensive tests
and measurements have been performed with the framework. Their results as well
as conclusions drawn from them are also presented in this thesis. Performance
tests show very promising results for the system, indicating that it can
fulfill ALICE's requirements concerning the data transport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404015</identifier>
 <datestamp>2011-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404015</id><created>2004-04-06</created><updated>2011-01-15</updated><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author></authors><title>The study of distributed computing algorithms by multithread
  applications</title><categories>cs.DC</categories><comments>17 pages</comments><msc-class>68M14, 68Q85</msc-class><acm-class>C.1.4; D.1.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The material in this note is used as an introduction to distributed
algorithms in a four year course on software and automatic control system in
the computer technology department of the Komsomolsk-on-Amur state technical
university. All our the program examples are written in Borland C/C++ 5.02 for
Windows 95/98/2000/NT/XP, and hence suit to compile and execute by Visual
C/C++. We consider the following approaches of the distributed computing: the
conversion of recursive algorithms to multithread applications, a realization
of the pairing algorithm, the building of wave systems by Petri nets and object
oriented programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404016</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404016</id><created>2004-04-07</created><authors><author><keyname>Tang</keyname><forenames>T. W.</forenames></author><author><keyname>Allison</keyname><forenames>A.</forenames></author><author><keyname>Abbott</keyname><forenames>D.</forenames></author></authors><title>Parrondo's games with chaotic switching</title><categories>cs.GT</categories><comments>11 pages, 9 figures</comments><acm-class>G.m</acm-class><doi>10.1117/12.561307</doi><abstract>  This paper investigates the different effects of chaotic switching on
Parrondo's games, as compared to random and periodic switching. The rate of
winning of Parrondo's games with chaotic switching depends on coefficient(s)
defining the chaotic generator, initial conditions of the chaotic sequence and
the proportion of Game A played. Maximum rate of winning can be obtained with
all the above mentioned factors properly set, and this occurs when chaotic
switching approaches periodic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404017</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404017</id><created>2004-04-07</created><authors><author><keyname>Berryman</keyname><forenames>Matthew J.</forenames></author><author><keyname>Khoo</keyname><forenames>Wei-Li</forenames></author><author><keyname>Nguyen</keyname><forenames>Hiep</forenames></author><author><keyname>O'Neill</keyname><forenames>Erin</forenames></author><author><keyname>Allison</keyname><forenames>Andrew</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Exploring tradeoffs in pleiotropy and redundancy using evolutionary
  computing</title><categories>cs.NE cs.NI</categories><comments>10 pages, 6 figures</comments><acm-class>G.1.6; C.2.1</acm-class><journal-ref>Proc. SPIE 5275, BioMEMS and Nanotechnology, Ed. Dan V. Nicolau,
  Perth, Australia, Dec. 2003, pp49-58</journal-ref><doi>10.1117/12.548001</doi><abstract>  Evolutionary computation algorithms are increasingly being used to solve
optimization problems as they have many advantages over traditional
optimization algorithms. In this paper we use evolutionary computation to study
the trade-off between pleiotropy and redundancy in a client-server based
network. Pleiotropy is a term used to describe components that perform multiple
tasks, while redundancy refers to multiple components performing one same task.
Pleiotropy reduces cost but lacks robustness, while redundancy increases
network reliability but is more costly, as together, pleiotropy and redundancy
build flexibility and robustness into systems. Therefore it is desirable to
have a network that contains a balance between pleiotropy and redundancy. We
explore how factors such as link failure probability, repair rates, and the
size of the network influence the design choices that we explore using genetic
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404018</id><created>2004-04-07</created><authors><author><keyname>Jia</keyname><forenames>Jiyou</forenames></author></authors><title>NLML--a Markup Language to Describe the Unlimited English Grammar</title><categories>cs.CL cs.AI</categories><comments>15 Pages, 2 Figures, 3 Tables submitted to German Conference for
  Artificial Intelligence 2004 Ulm</comments><acm-class>I.2.7</acm-class><abstract>  In this paper we present NLML (Natural Language Markup Language), a markup
language to describe the syntactic and semantic structure of any grammatically
correct English expression. At first the related works are analyzed to
demonstrate the necessity of the NLML: simple form, easy management and direct
storage. Then the description of the English grammar with NLML is introduced in
details in three levels: sentences (with different complexities, voices, moods,
and tenses), clause (relative clause and noun clause) and phrase (noun phrase,
verb phrase, prepositional phrase, adjective phrase, adverb phrase and
predicate phrase). At last the application fields of the NLML in NLP are shown
with two typical examples: NLOJM (Natural Language Object Modal in Java) and
NLDB (Natural Language Database).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404019</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404019</id><created>2004-04-07</created><authors><author><keyname>Berryman</keyname><forenames>Matthew J.</forenames></author><author><keyname>Allison</keyname><forenames>Andrew</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Optimizing genetic algorithm strategies for evolving networks</title><categories>cs.NE cs.NI</categories><comments>9 pages, 5 figures</comments><acm-class>G.1.6; C.2.1</acm-class><doi>10.1117/12.548122</doi><abstract>  This paper explores the use of genetic algorithms for the design of networks,
where the demands on the network fluctuate in time. For varying network
constraints, we find the best network using the standard genetic algorithm
operators such as inversion, mutation and crossover. We also examine how the
choice of genetic algorithm operators affects the quality of the best network
found. Such networks typically contain redundancy in servers, where several
servers perform the same task and pleiotropy, where servers perform multiple
tasks. We explore this trade-off between pleiotropy versus redundancy on the
cost versus reliability as a measure of the quality of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404020</id><created>2004-04-07</created><authors><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>A treatment of higher-order features in logic programming</title><categories>cs.PL</categories><comments>50 pages, 4 figures, 2 tables. To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>D.3.2; D.3.3; D.3.4</acm-class><abstract>  The logic programming paradigm provides the basis for a new intensional view
of higher-order notions. This view is realized primarily by employing the terms
of a typed lambda calculus as representational devices and by using a richer
form of unification for probing their structures. These additions have
important meta-programming applications but they also pose non-trivial
implementation problems. One issue concerns the machine representation of
lambda terms suitable to their intended use: an adequate encoding must
facilitate comparison operations over terms in addition to supporting the usual
reduction computation. Another aspect relates to the treatment of a unification
operation that has a branching character and that sometimes calls for the
delaying of the solution of unification problems. A final issue concerns the
execution of goals whose structures become apparent only in the course of
computation. These various problems are exposed in this paper and solutions to
them are described. A satisfactory representation for lambda terms is developed
by exploiting the nameless notation of de Bruijn as well as explicit encodings
of substitutions. Special mechanisms are molded into the structure of
traditional Prolog implementations to support branching in unification and
carrying of unification problems over other computation steps; a premium is
placed in this context on exploiting determinism and on emulating usual
first-order behaviour. An extended compilation model is presented that treats
higher-order unification and also handles dynamically emergent goals. The ideas
described here have been employed in the Teyjus implementation of the Lambda
Prolog language, a fact that is used to obtain a preliminary assessment of
their efficacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404021</id><created>2004-04-07</created><updated>2005-07-08</updated><authors><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Kurka</keyname><forenames>Petr</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent</forenames></author></authors><title>Decidability and Universality in Symbolic Dynamical Systems</title><categories>cs.CC cs.LO</categories><comments>23 pages; a shorter version is submitted to conference MCU 2004 v2:
  minor orthographic changes v3: section 5.2 (collatz functions) mathematically
  improved v4: orthographic corrections, one reference added v5:27 pages.
  Important modifications. The formalism is strengthened: temporal logic
  replaced by finite automata. New results. Submitted</comments><acm-class>F.1.1; F.4.1</acm-class><abstract>  Many different definitions of computational universality for various types of
dynamical systems have flourished since Turing's work. We propose a general
definition of universality that applies to arbitrary discrete time symbolic
dynamical systems. Universality of a system is defined as undecidability of a
model-checking problem. For Turing machines, counter machines and tag systems,
our definition coincides with the classical one. It yields, however, a new
definition for cellular automata and subshifts. Our definition is robust with
respect to initial condition, which is a desirable feature for physical
realizability.
  We derive necessary conditions for undecidability and universality. For
instance, a universal system must have a sensitive point and a proper
subsystem. We conjecture that universal systems have infinite number of
subsystems. We also discuss the thesis according to which computation should
occur at the `edge of chaos' and we exhibit a universal chaotic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404022</id><created>2004-04-08</created><authors><author><keyname>Rataj</keyname><forenames>Artur</forenames></author></authors><title>An Algorithm for Transforming Color Images into Tactile Graphics</title><categories>cs.GR</categories><comments>9 pages, 7 figures</comments><report-no>IITiS-20040408-1-1</report-no><acm-class>I.4.0</acm-class><abstract>  This paper presents an algorithm that transforms color visual images, like
photographs or paintings, into tactile graphics. In the algorithm, the edges of
objects are detected and colors of the objects are estimated. Then, the edges
and the colors are encoded into lines and textures in the output tactile image.
Design of the method is substantiated by various qualities of haptic
recognizing of images. Also, means of presentation of the tactile images in
printouts are discussed. Example translated images are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404023</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404023</id><created>2004-04-08</created><updated>2004-06-21</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Propositional computability logic I</title><categories>cs.LO math.LO</categories><comments>To appear in ACM Transactions on Computational Logic</comments><acm-class>F.1.1; F.1.2; F.4.1</acm-class><journal-ref>ACM Transactions on Computational Logic 7 (2006), pp. 302-330</journal-ref><doi>10.1145/1131313.1131318</doi><abstract>  In the same sense as classical logic is a formal theory of truth, the
recently initiated approach called computability logic is a formal theory of
computability. It understands (interactive) computational problems as games
played by a machine against the environment, their computability as existence
of a machine that always wins the game, logical operators as operations on
computational problems, and validity of a logical formula as being a scheme of
&quot;always computable&quot; problems. The present contribution gives a detailed
exposition of a soundness and completeness proof for an axiomatization of one
of the most basic fragments of computability logic. The logical vocabulary of
this fragment contains operators for the so called parallel and choice
operations, and its atoms represent elementary problems, i.e. predicates in the
standard sense. This article is self-contained as it explains all relevant
concepts. While not technically necessary, however, familiarity with the
foundational paper &quot;Introduction to computability logic&quot; [Annals of Pure and
Applied Logic 123 (2003), pp.1-99] would greatly help the reader in
understanding the philosophy, underlying motivations, potential and utility of
computability logic, -- the context that determines the value of the present
results. Online introduction to the subject is available at
http://www.cis.upenn.edu/~giorgi/cl.html and
http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404024</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404024</id><created>2004-04-08</created><updated>2004-12-10</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Computability Logic: a formal theory of interaction</title><categories>cs.LO cs.AI math.LO</categories><acm-class>F.1.1; F.1.2</acm-class><journal-ref>Interactive Computation: The New Paradigm. D.Goldin, S.Smolka and
  P.Wegner, eds. Springer Verlag, Berlin 2006, pp. 183-223</journal-ref><doi>10.1007/3-540-34874-3_9</doi><abstract>  Computability logic is a formal theory of (interactive) computability in the
same sense as classical logic is a formal theory of truth. This approach was
initiated very recently in &quot;Introduction to computability logic&quot; (Annals of
Pure and Applied Logic 123 (2003), pp.1-99). The present paper reintroduces
computability logic in a more compact and less technical way. It is written in
a semitutorial style with a general computer science, logic or mathematics
audience in mind. An Internet source on the subject is available at
http://www.cis.upenn.edu/~giorgi/cl.html, and additional material at
http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404025</id><created>2004-04-10</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Iwayama</keyname><forenames>Makoto</forenames></author><author><keyname>Kando</keyname><forenames>Noriko</forenames></author></authors><title>Test Collections for Patent-to-Patent Retrieval and Patent Map
  Generation in NTCIR-4 Workshop</title><categories>cs.CL</categories><comments>4 pages, Proceedings of the 4th International Conference on Language
  Resources and Evaluation (to appear)</comments><acm-class>H.3.3; H.3.4; I.2.7</acm-class><journal-ref>Proceedings of the 4th International Conference on Language
  Resources and Evaluation (LREC-2004), pp.1643-1646, May. 2004.</journal-ref><abstract>  This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop,
and the test collections produced in this task. We perform the invalidity
search task, in which each participant group searches a patent collection for
the patents that can invalidate the demand in an existing claim. We also
perform the automatic patent map generation task, in which the patents
associated with a specific topic are organized in a multi-dimensional matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404026</id><created>2004-04-11</created><authors><author><keyname>Nathan</keyname><forenames>Darran</forenames></author><author><keyname>Rosdiana</keyname><forenames>Eva</forenames></author><author><keyname>Koon</keyname><forenames>Chua Beng</forenames></author></authors><title>DAB Content Annotation and Receiver Hardware Control with XML</title><categories>cs.GL cs.CL</categories><comments>6 pages, 7 figures</comments><acm-class>D.3.2</acm-class><abstract>  The Eureka-147 Digital Audio Broadcasting (DAB) standard defines the 'dynamic
labels' data field for holding information about the transmission content.
However, this information does not follow a well-defined structure since it is
designed to carry text for direct output to displays, for human interpretation.
This poses a problem when machine interpretation of DAB content information is
desired. Extensible Markup Language (XML) was developed to allow for the
well-defined, structured machine-to-machine exchange of data over computer
networks. This article proposes a novel technique of machine-interpretable DAB
content annotation and receiver hardware control, involving the utilisation of
XML as metadata in the transmitted DAB frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404027</id><created>2004-04-13</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author></authors><title>The Gridbus Toolkit for Service Oriented Grid and Utility Computing: An
  Overview and Status Report</title><categories>cs.DC</categories><comments>11 pages, 3 figures, 3 tables</comments><report-no>Technical Report, GRIDS-TR-2004-2, Grid Computing and Distributed
  Systems Laboratory, University of Melbourne, Australia, April 2004</report-no><acm-class>C.2.4, C.1.4, C.2.1</acm-class><abstract>  Grids aim at exploiting synergies that result from cooperation of autonomous
distributed entities. The synergies that result from grid cooperation include
the sharing, exchange, selection, and aggregation of geographically distributed
resources such as computers, data bases, software, and scientific instruments
for solving large-scale problems in science, engineering, and commerce. For
this cooperation to be sustainable, participants need to have economic
incentive. Therefore, &quot;incentive&quot; mechanisms should be considered as one of key
design parameters of Grid architectures. In this article, we present an
overview and status of an open source Grid toolkit, called Gridbus, whose
architecture is fundamentally driven by the requirements of Grid economy.
Gridbus technologies provide services for both computational and data grids
that power the emerging eScience and eBusiness applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404028</id><created>2004-04-13</created><authors><author><keyname>Dominic</keyname><forenames>Saju Jude</forenames></author><author><keyname>Sajith</keyname><forenames>G.</forenames></author></authors><title>The Random Buffer Tree : A Randomized Technique for I/O-efficient
  Algorithms</title><categories>cs.DS</categories><comments>13 pages with no figures, unpublished</comments><acm-class>E1;F.2.2;G.3</acm-class><abstract>  In this paper, we present a probabilistic self-balancing dictionary data
structure for massive data sets, and prove expected amortized I/O-optimal
bounds on the dictionary operations. We show how to use the structure as an
I/O-optimal priority queue. The data structure, which we call as the random
buffer tree, abstracts the properties of the random treap and the buffer tree
and has the same expected I/O-bounds as the buffer tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404029</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404029</id><created>2004-04-13</created><authors><author><keyname>Bagchi</keyname><forenames>Amitabha</forenames></author><author><keyname>Bhargava</keyname><forenames>Ankur</forenames></author><author><keyname>Chaudhary</keyname><forenames>Amitabh</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author></authors><title>The Effect of Faults on Network Expansion</title><categories>cs.DC cs.DM</categories><comments>8 pages; to appear at SPAA 2004</comments><acm-class>C.2; G.2.2</acm-class><journal-ref>Theor. Comput. Syst. 39(6):903-928. November 2006</journal-ref><doi>10.1007/s00224-006-1349-0</doi><abstract>  In this paper we study the problem of how resilient networks are to node
faults. Specifically, we investigate the question of how many faults a network
can sustain so that it still contains a large (i.e. linear-sized) connected
component that still has approximately the same expansion as the original
fault-free network. For this we apply a pruning technique which culls away
parts of the faulty network which have poor expansion. This technique can be
applied to both adversarial faults and to random faults. For adversarial faults
we prove that for every network with expansion alpha, a large connected
component with basically the same expansion as the original network exists for
up to a constant times alpha n faults. This result is tight in the sense that
every graph G of size n and uniform expansion alpha(.), i.e. G has an expansion
of alpha(n) and every subgraph G' of size m of G has an expansion of
O(alpha(m)), can be broken into sublinear components with omega(alpha(n) n)
faults.
  For random faults we observe that the situation is significantly different,
because in this case the expansion of a graph only gives a very weak bound on
its resilience to random faults. More specifically, there are networks of
uniform expansion O(sqrt{n}) that are resilient against a constant fault
probability but there are also networks of uniform expansion Omega(1/log n)
that are not resilient against a O(1/log n) fault probability. Thus, a
different parameter is needed. For this we introduce the span of a graph which
allows us to determine the maximum fault probability in a much better way than
the expansion can. We use the span to show the first known results for the
effect of random faults on the expansion of d-dimensional meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404030</id><created>2004-04-14</created><authors><author><keyname>de Vries</keyname><forenames>Andreas</forenames></author></authors><title>XML framework for concept description and knowledge representation</title><categories>cs.AI cs.LO</categories><comments>9 pages</comments><acm-class>I.7.2; E.2; H.1.1; G.2.3</acm-class><abstract>  An XML framework for concept description is given, based upon the fact that
the tree structure of XML implies the logical structure of concepts as defined
by attributional calculus. Especially, the attribute-value representation is
implementable in the XML framework. Since the attribute-value representation is
an important way to represent knowledge in AI, the framework offers a further
and simpler way than the powerful RDF technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404031</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404031</id><created>2004-04-14</created><authors><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Characterisations of Intersection Graphs by Vertex Orderings</title><categories>cs.DM</categories><comments>submitted</comments><acm-class>F.2.2</acm-class><journal-ref>Australasian J. Combinatorics 34:261-268, 2006</journal-ref><abstract>  Characterisations of interval graphs, comparability graphs, co-comparability
graphs, permutation graphs, and split graphs in terms of linear orderings of
the vertex set are presented. As an application, it is proved that interval
graphs, co-comparability graphs, AT-free graphs, and split graphs have
bandwidth bounded by their maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404032</id><created>2004-04-14</created><authors><author><keyname>Finton</keyname><forenames>David J.</forenames></author></authors><title>When Do Differences Matter? On-Line Feature Extraction Through Cognitive
  Economy</title><categories>cs.LG cs.AI cs.NE</categories><comments>20 pages, 10 PostScript figures, LaTeX2e</comments><acm-class>I.2.6; I.2.4; I.2.8</acm-class><abstract>  For an intelligent agent to be truly autonomous, it must be able to adapt its
representation to the requirements of its task as it interacts with the world.
Most current approaches to on-line feature extraction are ad hoc; in contrast,
this paper presents an algorithm that bases judgments of state compatibility
and state-space abstraction on principled criteria derived from the
psychological principle of cognitive economy. The algorithm incorporates an
active form of Q-learning, and partitions continuous state-spaces by merging
and splitting Voronoi regions. The experiments illustrate a new methodology for
testing and comparing representations by means of learning curves. Results from
the puck-on-a-hill task demonstrate the algorithm's ability to learn effective
representations, superior to those produced by some other, well-known, methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404033</id><created>2004-04-14</created><authors><author><keyname>Dominic</keyname><forenames>Saju Jude</forenames></author><author><keyname>Sajith</keyname><forenames>G.</forenames></author></authors><title>The Persistent Buffer Tree : An I/O-efficient Index for Temporal Data</title><categories>cs.GL cs.DB</categories><comments>11 pages with no figures, unpublished</comments><acm-class>E.2;H.2.2;G.3</acm-class><abstract>  In a variety of applications, we need to keep track of the development of a
data set over time. For maintaining and querying this multi version data
I/O-efficiently, external memory data structures are required. In this paper,
we present a probabilistic self-balancing persistent data structure in external
memory called the persistent buffer tree, which supports insertions, updates
and deletions of data items at the present version and range queries for any
version, past or present. The persistent buffer tree is I/O-optimal in the
sense that the expected amortized I/O performance bounds are asymptotically the
same as the deterministic amortized bounds of the (single version) buffer tree
in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404034</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404034</id><created>2004-04-15</created><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author><author><keyname>Moa</keyname><forenames>B.</forenames></author></authors><title>Propagation by Selective Initialization and Its Application to Numerical
  Constraint Satisfaction Problems</title><categories>cs.NA</categories><acm-class>D.3.1; F.2.1; G.1.0; G.1.2; G.1.6; G.1.5 ; I.2.8; I.2.9</acm-class><abstract>  Numerical analysis has no satisfactory method for the more realistic
optimization models. However, with constraint programming one can compute a
cover for the solution set to arbitrarily close approximation. Because the use
of constraint propagation for composite arithmetic expressions is
computationally expensive, consistency is computed with interval arithmetic. In
this paper we present theorems that support, selective initialization, a simple
modification of constraint propagation that allows composite arithmetic
expressions to be handled efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404035</id><created>2004-04-16</created><updated>2005-05-16</updated><authors><author><keyname>Mielke</keyname><forenames>Andreas</forenames></author></authors><title>Elements for Response Time Statistics in ERP Transaction Systems</title><categories>cs.PF</categories><comments>revtex, twocolumn, 8 pages, 13 figures. figures replaced by coloured
  versions</comments><acm-class>D.4.8; C.4</acm-class><journal-ref>Performance Evaluation 64, 635-653 (2006)</journal-ref><doi>10.1016/j.peva.2005.05.006</doi><abstract>  We present some measurements and ideas for response time statistics in ERP
systems. It is shown that the response time distribution of a given transaction
in a given system is generically a log-normal distribution or, in some
situations, a sum of two or more log-normal distributions. We present some
arguments for this form of the distribution based on heuristic rules for
response times, and we show data from performance measurements in actual
systems to support the log-normal form. Deviations of the log-normal form can
often be traced back to performance problems in the system. Consequences for
the interpretation of response time data and for service level agreements are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404036</id><created>2004-04-16</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Klein</keyname><forenames>Rolf</forenames></author><author><keyname>Nuechter</keyname><forenames>Andreas</forenames></author></authors><title>Online Searching with an Autonomous Robot</title><categories>cs.RO cs.DS</categories><comments>16 pages, 8 figures, 12 photographs, 1 table, Latex, submitted for
  publication</comments><acm-class>I.2.9</acm-class><abstract>  We discuss online strategies for visibility-based searching for an object
hidden behind a corner, using Kurt3D, a real autonomous mobile robot. This task
is closely related to a number of well-studied problems. Our robot uses a
three-dimensional laser scanner in a stop, scan, plan, go fashion for building
a virtual three-dimensional environment. Besides planning trajectories and
avoiding obstacles, Kurt3D is capable of identifying objects like a chair. We
derive a practically useful and asymptotically optimal strategy that guarantees
a competitive ratio of 2, which differs remarkably from the well-studied
scenario without the need of stopping for surveying the environment. Our
strategy is used by Kurt3D, documented in a separate video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404037</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404037</id><created>2004-04-19</created><updated>2004-04-19</updated><authors><author><keyname>Xie</keyname><forenames>Gaoyan</forenames></author><author><keyname>Dang</keyname><forenames>Zhe</forenames></author></authors><title>Model-checking Driven Black-box Testing Algorithms for Systems with
  Unspecified Components</title><categories>cs.SE cs.LO</categories><comments>Submitted to FSE'04</comments><acm-class>D.2.4;D.2.5;F.4.1</acm-class><abstract>  Component-based software development has posed a serious challenge to system
verification since externally-obtained components could be a new source of
system failures. This issue can not be completely solved by either
model-checking or traditional software testing techniques alone due to several
reasons:
 1) externally obtained components are usually unspecified/partially specified;
2)it is generally difficult to establish an adequacy criteria for testing a
component; 3)components may be used to dynamically upgrade a system.
 This paper introduces a new approach (called {\em model-checking driven
black-box testing}) that combines model-checking with traditional black-box
software testing to tackle the problem in a complete, sound, and automatic way.
 The idea is to, with respect to some requirement (expressed in CTL or LTL)
about the system, use model-checking techniques to derive a condition
(expressed in communication graphs) for an unspecified component such that the
system satisfies the requirement iff the condition is satisfied by the
component, and which can be established by testing the component with test
cases generated from the condition on-the-fly. In this paper, we present
model-checking driven black-box testing algorithms to handle both CTL and LTL
requirements.
 We also illustrate the idea through some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404038</id><created>2004-04-20</created><authors><author><keyname>Powell</keyname><forenames>D. B.</forenames></author></authors><title>2-Sat Sub-Clauses and the Hypernodal Structure of the 3-Sat Problem</title><categories>cs.CC cs.AI</categories><comments>16 pages; 8 figures</comments><acm-class>G.2.1; G.2.2</acm-class><abstract>  Like simpler graphs, nested (hypernodal) graphs consist of two components: a
set of nodes and a set of edges, where each edge connects a pair of nodes. In
the hypernodal graph model, however, a node may contain other graphs, so that a
node may be contained in a graph that it contains. The inherently recursive
structure of the hypernodal graph model aptly characterizes both the structure
and dynamic of the 3-sat problem, a broadly applicable, though intractable,
computer science problem. In this paper I first discuss the structure of the
3-sat problem, analyzing the relation of 3-sat to 2-sat, a related, though
tractable problem. I then discuss sub-clauses and sub-clause thresholds and the
transformation of sub-clauses into implication graphs, demonstrating how
combinations of implication graphs are equivalent to hypernodal graphs. I
conclude with a brief discussion of the use of hypernodal graphs to model the
3-sat problem, illustrating how hypernodal graphs model both the conditions for
satisfiability and the process by which particular 3-sat assignments either
succeed or fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404039</id><created>2004-04-20</created><authors><author><keyname>Kaltchenko</keyname><forenames>Alexei</forenames></author></authors><title>Algorithms for Estimating Information Distance with Application to
  Bioinformatics and Linguistics</title><categories>cs.CC cs.CE q-bio.GN</categories><comments>4 pages</comments><acm-class>J.3; E.4</acm-class><abstract>  After reviewing unnormalized and normalized information distances based on
incomputable notions of Kolmogorov complexity, we discuss how Kolmogorov
complexity can be approximated by data compression algorithms. We argue that
optimal algorithms for data compression with side information can be
successfully used to approximate the normalized distance. Next, we discuss an
alternative information distance, which is based on relative entropy rate (also
known as Kullback-Leibler divergence), and compression-based algorithms for its
estimation. Based on available biological and linguistic data, we arrive to
unexpected conclusion that in Bioinformatics and Computational Linguistics this
alternative distance is more relevant and important than the ones based on
Kolmogorov complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404040</id><created>2004-04-20</created><authors><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author><author><keyname>Grossklags</keyname><forenames>Jens</forenames></author><author><keyname>Chuang</keyname><forenames>John</forenames></author></authors><title>Near Rationality and Competitive Equilibria in Networked Systems</title><categories>cs.GT cs.NI</categories><comments>13 pages, no figures</comments><report-no>p2pecon TR-2004-04-CGC</report-no><acm-class>C.2</acm-class><abstract>  A growing body of literature in networked systems research relies on game
theory and mechanism design to model and address the potential lack of
cooperation between self-interested users. Most game-theoretic models applied
to system research only describe competitive equilibria in terms of pure Nash
equilibria, that is, a situation where the strategy of each user is
deterministic, and is her best response to the strategies of all the other
users. However, the assumptions necessary for a pure Nash equilibrium to hold
may be too stringent for practical systems. Using three case studies on
computer security, TCP congestion control, and network formation, we outline
the limits of game-theoretic models relying on Nash equilibria, and we argue
that considering competitive equilibria of a more general form may help
reconcile predictions from game-theoretic models with empirically observed
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404041</id><created>2004-04-21</created><updated>2006-02-06</updated><authors><author><keyname>Jia</keyname><forenames>Jiyou</forenames></author></authors><title>NLOMJ--Natural Language Object Model in Java</title><categories>cs.CL cs.PL</categories><comments>11 pages, 1 figure. Submitted to ICICP04</comments><acm-class>I.2.7; D.1.5</acm-class><abstract>  In this paper we present NLOMJ--a natural language object model in Java with
English as the experiment language. This modal describes the grammar elements
of any permissible expression in a natural language and their complicated
relations with each other with the concept &quot;Object&quot; in OOP(Object Oriented
Programming). Directly mapped to the syntax and semantics of the natural
language, it can be used in information retrieval as a linguistic method.
Around the UML diagram of the NLOMJ the important classes(Sentence, Clause and
Phrase) and their sub classes are introduced and their syntactic and semantic
meanings are explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404042</id><created>2004-04-21</created><updated>2004-04-22</updated><authors><author><keyname>Ali</keyname><forenames>W.</forenames></author><author><keyname>Mondragon</keyname><forenames>R. J.</forenames></author><author><keyname>Alavi</keyname><forenames>F.</forenames></author></authors><title>Extraction of topological features from communication network
  topological patterns using self-organizing feature maps</title><categories>cs.NE cs.CV</categories><comments>8 Pages, 5 figures, To be appeared in IEE Electronics Letter Journal</comments><acm-class>C.2; I.5</acm-class><abstract>  Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404043</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404043</id><created>2004-04-21</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>Benchmarking Blunders and Things That Go Bump in the Night</title><categories>cs.PF cs.SE</categories><comments>Invited presentation at the Workshop On Software Performance and
  Reliability (WOPR2) Menlo Park, California, April 15-17 2004</comments><acm-class>B.8.1;B.8.2;D.2.5;K.7.3</acm-class><abstract>  Benchmarking; by which I mean any computer system that is driven by a
controlled workload, is the ultimate in performance testing and simulation.
Aside from being a form of institutionalized cheating, it also offer countless
opportunities for systematic mistakes in the way the workloads are applied and
the resulting measurements interpreted. Right test, wrong conclusion is a
ubiquitous mistake that happens because test engineers tend to treat data as
divine. Such reverence is not only misplaced, it's also a sure ticket to
production hell when the application finally goes live. I demonstrate how such
mistakes can be avoided by means of two war stories that are real WOPRs. (a)
How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks
can go flat with too much Java juice. In each case I present simple performance
models and show how they can be applied to correctly assess benchmark data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404044</identifier>
 <datestamp>2008-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404044</id><created>2004-04-22</created><updated>2004-04-22</updated><authors><author><keyname>Gu</keyname><forenames>Xiaoyang</forenames></author></authors><title>A note on dimensions of polynomial size circuits</title><categories>cs.CC</categories><comments>11 pages</comments><acm-class>F.1.3</acm-class><doi>10.1016/j.tcs.2006.02.022</doi><abstract>  In this paper, we use resource-bounded dimension theory to investigate
polynomial size circuits. We show that for every $i\geq 0$, $\Ppoly$ has $i$th
order scaled $\pthree$-strong dimension 0. We also show that $\Ppoly^\io$ has
$\pthree$-dimension 1/2, $\pthree$-strong dimension 1. Our results improve
previous measure results of Lutz (1992) and dimension results of Hitchcock and
Vinodchandran (2004).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404045</id><created>2004-04-22</created><updated>2004-04-24</updated><authors><author><keyname>Shanks</keyname><forenames>Bayle</forenames></author></authors><title>Speculation on graph computation architectures and computing via
  synchronization</title><categories>cs.NE cs.AI</categories><comments>61 pages. Informal, rambling. (replacment changed only abstract)</comments><acm-class>F.1.1; J.3; I.2.m</acm-class><abstract>  A speculative overview of a future topic of research. The paper is a
collection of ideas concerning two related areas:
  1) Graph computation machines (&quot;computing with graphs&quot;). This is the class of
models of computation in which the state of the computation is represented as a
graph or network.
  2) Arc-based neural networks, which store information not as activation in
the nodes, but rather by adding and deleting arcs. Sometimes the arcs may be
interpreted as synchronization.
  Warnings to readers: this is not the sort of thing that one might submit to a
journal or conference. No proofs are presented. The presentation is informal,
and written at an introductory level. You'll probably want to wait for a more
concise presentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404046</id><created>2004-04-22</created><authors><author><keyname>Rana</keyname><forenames>Sanjay</forenames></author><author><keyname>Batty</keyname><forenames>Mike</forenames></author></authors><title>Visualising the structure of architectural open spaces based on shape
  analysis</title><categories>cs.CV cs.CG cs.DS</categories><comments>10 pages, 9 figures</comments><acm-class>I.3.5;I.4.8;I.5.2</acm-class><journal-ref>International Journal of Architectural Computing, 2(1), 2004</journal-ref><abstract>  This paper proposes the application of some well known two-dimensional
geometrical shape descriptors for the visualisation of the structure of
architectural open spaces. The paper demonstrates the use of visibility
measures such as distance to obstacles and amount of visible space to calculate
shape descriptors such as convexity and skeleton of the open space. The aim of
the paper is to indicate a simple, objective and quantifiable approach to
understand the structure of open spaces otherwise impossible due to the complex
construction of built structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404047</id><created>2004-04-22</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Using matrices in post-processing phase of CFD simulations</title><categories>cs.NA cs.DC physics.comp-ph</categories><comments>Paper based on presentation-talk at SCICOMP9, Bologna (Italy), March
  23-26, 2004; workshop organized by IBM, CINECA (Italy) (dr. Sigismondo
  Boschi, dr. Giovanni Erbacci), NERSC-DOE (USA) (dr. David Skinner), web site:
  www.spscicomp.org ; main topics: Computational Fluid Dynamics</comments><acm-class>C.1.4; D.1.3; G.1.0</acm-class><journal-ref>Progress in Industrial Mathematics at ECMI 2004 - Eindhoven
  (Netherlands), Springer, 2005</journal-ref><abstract>  In this work I present a technique of construction and fast evaluation of a
family of cubic polynomials for analytic smoothing and graphical rendering of
particles trajectories for flows in a generic geometry. The principal result of
the work was implementation and test of a method for interpolating 3D points by
regular parametric curves and their fast and efficient evaluation for a good
resolution of rendering. For the purpose I have used a parallel environment
using a multiprocessor cluster architecture. The efficiency of the used method
is good, mainly reducing the number of floating-points computations by caching
the numerical values of some line-parameter's powers, and reducing the
necessity of communication among processes. This work has been developed for
the Research and Development Department of my company for planning advanced
customized models of industrial burners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404048</id><created>2004-04-23</created><updated>2005-08-24</updated><authors><author><keyname>Giacobazzi</keyname><forenames>Roberto</forenames></author><author><keyname>Ranzato</keyname><forenames>Francesco</forenames></author></authors><title>Incompleteness of States w.r.t. Traces in Model Checking</title><categories>cs.LO</categories><acm-class>D.2.4; F.3.1; F.3.2</acm-class><abstract>  Cousot and Cousot introduced and studied a general past/future-time
specification language, called mu*-calculus, featuring a natural time-symmetric
trace-based semantics. The standard state-based semantics of the mu*-calculus
is an abstract interpretation of its trace-based semantics, which turns out to
be incomplete (i.e., trace-incomplete), even for finite systems. As a
consequence, standard state-based model checking of the mu*-calculus is
incomplete w.r.t. trace-based model checking. This paper shows that any
refinement or abstraction of the domain of sets of states induces a
corresponding semantics which is still trace-incomplete for any propositional
fragment of the mu*-calculus. This derives from a number of results, one for
each incomplete logical/temporal connective of the mu*-calculus, that
characterize the structure of models, i.e. transition systems, whose
corresponding state-based semantics of the mu*-calculus is trace-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404049</id><created>2004-04-23</created><authors><author><keyname>Afantenos</keyname><forenames>Stergos D.</forenames></author><author><keyname>Doura</keyname><forenames>Irene</forenames></author><author><keyname>Kapellou</keyname><forenames>Eleni</forenames></author><author><keyname>Karkaletsis</keyname><forenames>Vangelis</forenames></author></authors><title>Exploiting Cross-Document Relations for Multi-document Evolving
  Summarization</title><categories>cs.CL cs.AI</categories><comments>10 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Methods and Applications of Artificial Intelligence, Volume 3025
  of Lecture Notes in Computer Science. Springer-Verlag Heidelberg 2004. pp
  410-419.</journal-ref><abstract>  This paper presents a methodology for summarization from multiple documents
which are about a specific topic. It is based on the specification and
identification of the cross-document relations that occur among textual
elements within those documents. Our methodology involves the specification of
the topic-specific entities, the messages conveyed for the specific entities
by certain textual elements and the specification of the relations that can
hold among these messages. The above resources are necessary for setting up a
specific topic for our query-based summarization approach which uses these
resources to identify the query-specific messages within the documents and the
query-specific relations that connect these messages across documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404050</id><created>2004-04-24</created><authors><author><keyname>Arenas-Sanchez</keyname><forenames>Puri</forenames></author><author><keyname>Rodriguez-Artalejo</keyname><forenames>Mario</forenames></author></authors><title>A General Framework For Lazy Functional Logic Programming With Algebraic
  Polymorphic Types</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 2,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 2, 2001</journal-ref><abstract>  We propose a general framework for first-order functional logic programming,
supporting lazy functions, non-determinism and polymorphic datatypes whose data
constructors obey a set C of equational axioms. On top of a given C, we specify
a program as a set R of C-based conditional rewriting rules for defined
functions. We argue that equational logic does not supply the proper semantics
for such programs. Therefore, we present an alternative logic which includes
C-based rewriting calculi and a notion of model. We get soundness and
completeness for C-based rewriting w.r.t. models, existence of free models for
all programs, and type preservation results. As operational semantics, we
develop a sound and complete procedure for goal solving, which is based on the
combination of lazy narrowing with unification modulo C. Our framework is quite
expressive for many purposes, such as solving action and change problems, or
realizing the GAMMA computation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404051</id><created>2004-04-24</created><authors><author><keyname>Lobo</keyname><forenames>Jorge</forenames></author><author><keyname>Mendez</keyname><forenames>Gisela</forenames></author><author><keyname>Taylor</keyname><forenames>Stuart R.</forenames></author></authors><title>Knowledge And The Action Description Language A</title><categories>cs.AI</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 2,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 2, 2001</journal-ref><abstract>  We introduce Ak, an extension of the action description language A (Gelfond
and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing
actions to increase an agent's knowledge of the world and non-deterministic
actions to remove knowledge. We include complex plans involving conditionals
and loops in our query language for hypothetical reasoning. We also present a
translation of Ak domain descriptions into epistemic logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404052</id><created>2004-04-24</created><authors><author><keyname>Clark</keyname><forenames>Keith L.</forenames></author><author><keyname>Robinson</keyname><forenames>Peter J.</forenames></author><author><keyname>Hagen</keyname><forenames>Richard</forenames></author></authors><title>Multi-Threading And Message Communication In Qu-Prolog</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 3,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 3, 2001</journal-ref><abstract>  This paper presents the multi-threading and internet message communication
capabilities of Qu-Prolog. Message addresses are symbolic and the
communications package provides high-level support that completely hides
details of IP addresses and port numbers as well as the underlying TCP/IP
transport layer. The combination of the multi-threads and the high level
inter-thread message communications provide simple, powerful support for
implementing internet distributed intelligent applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404053</id><created>2004-04-26</created><authors><author><keyname>Leach</keyname><forenames>Javier</forenames></author><author><keyname>Nieva</keyname><forenames>Susana</forenames></author><author><keyname>Rodriguez-Artalejo</keyname><forenames>Mario</forenames></author></authors><title>Constraint Logic Programming with Hereditary Harrop Formula</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 4,
  2001. Appeared in Theory and Practice of Logic Programming, vol. 1, no. 4,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><abstract>  Constraint Logic Programming (CLP) and Hereditary Harrop formulas (HH) are
two well known ways to enhance the expressivity of Horn clauses. In this paper,
we present a novel combination of these two approaches. We show how to enrich
the syntax and proof theory of HH with the help of a given constraint system,
in such a way that the key property of HH as a logic programming language
(namely, the existence of uniform proofs) is preserved. We also present a
procedure for goal solving, showing its soundness and completeness for
computing answer constraints. As a consequence of this result, we obtain a new
strong completeness theorem for CLP that avoids the need to build disjunctions
of computed answers, as well as a more abstract formulation of a known
completeness theorem for HH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404054</id><created>2004-04-26</created><authors><author><keyname>Bauer</keyname><forenames>Matthias</forenames></author></authors><title>New Covert Channels in HTTP</title><categories>cs.CR cs.NI</categories><comments>7 pages</comments><acm-class>C.2.0; K.4.1</acm-class><journal-ref>Proceedings of the 2003 ACM Workshop on Privacy in the Electronic
  Society</journal-ref><abstract>  This paper presents new methods enabling anonymous communication on the
Internet. We describe a new protocol that allows us to create an anonymous
overlay network by exploiting the web browsing activities of regular users. We
show that the overlay network provides an anonymity set greater than the set of
senders and receivers in a realistic threat model. In particular, the protocol
provides unobservability in our threat model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404055</id><created>2004-04-26</created><updated>2004-04-27</updated><authors><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author><author><keyname>Gori</keyname><forenames>Roberta</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author></authors><title>Finite-Tree Analysis for Constraint Logic-Based Languages: The Complete
  Unabridged Version</title><categories>cs.PL</categories><comments>89 pages, 1 table</comments><acm-class>F.3.2</acm-class><abstract>  Logic languages based on the theory of rational, possibly infinite, trees
have much appeal in that rational trees allow for faster unification (due to
the safe omission of the occurs-check) and increased expressivity (cyclic terms
can provide very efficient representations of grammars and other useful
objects). Unfortunately, the use of infinite rational trees has problems. For
instance, many of the built-in and library predicates are ill-defined for such
trees and need to be supplemented by run-time checks whose cost may be
significant. Moreover, some widely-used program analysis and manipulation
techniques are correct only for those parts of programs working over finite
trees. It is thus important to obtain, automatically, a knowledge of the
program variables (the finite variables) that, at the program points of
interest, will always be bound to finite terms. For these reasons, we propose
here a new data-flow analysis, based on abstract interpretation, that captures
such information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404056</identifier>
 <datestamp>2009-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404056</id><created>2004-04-27</created><updated>2004-11-11</updated><authors><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Valiron</keyname><forenames>Benoit</forenames></author></authors><title>A lambda calculus for quantum computation with classical control</title><categories>cs.LO</categories><comments>15 pages, submitted to TLCA'05. Note: this is basically the work done
  during the first author master, his thesis can be found on his webpage.
  Modifications: almost everything reformulated; recursion removed since the
  way it was stated didn't satisfy lemma 11; type inference algorithm added;
  example of an implementation of quantum teleportation added</comments><acm-class>F.4.1</acm-class><journal-ref>Proc. of TLCA 2005</journal-ref><doi>10.1007/11417170_26</doi><abstract>  The objective of this paper is to develop a functional programming language
for quantum computers. We develop a lambda calculus for the classical control
model, following the first author's work on quantum flow-charts. We define a
call-by-value operational semantics, and we give a type system using affine
intuitionistic linear logic. The main results of this paper are the safety
properties of the language and the development of a type inference algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404057</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404057</id><created>2004-04-28</created><authors><author><keyname>Poland</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Convergence of Discrete MDL for Sequential Prediction</title><categories>cs.LG cs.AI math.ST stat.TH</categories><comments>17 pages</comments><report-no>IDSIA-03-04</report-no><acm-class>I.2.6; E.4; G.3</acm-class><journal-ref>Proc. 17th Annual Conf. on Learning Theory (COLT-2004), pages
  300--314</journal-ref><abstract>  We study the properties of the Minimum Description Length principle for
sequence prediction, considering a two-part MDL estimator which is chosen from
a countable class of models. This applies in particular to the important case
of universal sequence prediction, where the model class corresponds to all
algorithms for some fixed universal Turing machine (this correspondence is by
enumerable semimeasures, hence the resulting models are stochastic). We prove
convergence theorems similar to Solomonoff's theorem of universal induction,
which also holds for general Bayes mixtures. The bound characterizing the
convergence speed for MDL predictions is exponentially larger as compared to
Bayes mixtures. We observe that there are at least three different ways of
using MDL for prediction. One of these has worse prediction properties, for
which predictions only converge if the MDL estimator stabilizes. We establish
sufficient conditions for this to occur. Finally, some immediate consequences
for complexity relations and randomness criteria are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0404058</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0404058</id><created>2004-04-29</created><authors><author><keyname>Knuth</keyname><forenames>Donald E.</forenames></author><author><keyname>Ruskey</keyname><forenames>Frank</forenames></author></authors><title>Efficient coroutine generation of constrained Gray sequences</title><categories>cs.DS</categories><report-no>Knuth migration 11/2004</report-no><journal-ref>Lecture Notes in Computer Science 2635 (2004), 183--204</journal-ref><abstract>  We study an interesting family of cooperating coroutines, which is able to
generate all patterns of bits that satisfy certain fairly general ordering
constraints, changing only one bit at a time. (More precisely, the directed
graph of constraints is required to be cycle-free when it is regarded as an
undirected graph.) If the coroutines are implemented carefully, they yield an
algorithm that needs only a bounded amount of computation per bit change,
thereby solving an open problem in the field of combinatorial pattern
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405001</id><created>2004-05-01</created><authors><author><keyname>Djenchuraev</keyname><forenames>Nurlan</forenames></author></authors><title>Toward a New Policy for Scientific and Technical Communication: the Case
  of Kyrgyz Republic</title><categories>cs.CY</categories><comments>Policy Paper</comments><acm-class>K.4.1</acm-class><abstract>  The objective of this policy paper is to formulate a new policy in the field
of scientific and technical information (STI) in Kyrgyz Republic in the light
of emergence and rapid development of electronic scientific communication. The
major problem with communication in science in the Republic is lack of adequate
access to information by scientists. An equally serious problem is poor
visibility of research conducted in Kyrgyzstan and, as consequence, negligible
research impact on academic society globally. The paper proposes an integrated
approach to formulation of a new STI policy based on a number of policy
components: telecommunication networks, computerization, STI systems,
legislation &amp; standards, and education &amp; trainings. Two alternatives were
considered: electronic vs. paper-based scientific communication and development
of the national STI system vs. cross-national virtual collaboration. The study
results in suggesting a number of policy recommendations for identified
stakeholders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405002</id><created>2004-05-03</created><updated>2006-10-26</updated><authors><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Gilis</keyname><forenames>David</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Splitting an operator: Algebraic modularity results for logics with
  fixpoint semantics</title><categories>cs.AI cs.LO</categories><comments>Revised to correct a substantial error in Section 4.2.2 (certain
  results which only hold for_consistent_ possible world sets were stated to
  hold in general)</comments><acm-class>I.2.3; I.2.4</acm-class><journal-ref>ACM Transactions on Computational Logic, Volume 7, Number 4, 2006</journal-ref><abstract>  It is well known that, under certain conditions, it is possible to split
logic programs under stable model semantics, i.e. to divide such a program into
a number of different &quot;levels&quot;, such that the models of the entire program can
be constructed by incrementally constructing models for each level. Similar
results exist for other non-monotonic formalisms, such as auto-epistemic logic
and default logic. In this work, we present a general, algebraicsplitting
theory for logics with a fixpoint semantics. Together with the framework of
approximation theory, a general fixpoint theory for arbitrary operators, this
gives us a uniform and powerful way of deriving splitting results for each
logic with a fixpoint semantics. We demonstrate the usefulness of these
results, by generalizing existing results for logic programming, auto-epistemic
logic and default logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405003</id><created>2004-05-03</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author></authors><title>Model checking for Process Rewrite Systems and a class of action--based
  regular properties</title><categories>cs.OH</categories><comments>31 pages, 1 figures</comments><acm-class>68Q60</acm-class><abstract>  We consider the model checking problem for Process Rewrite Systems (PRSs), an
infinite-state formalism (non Turing-powerful) which subsumes many common
models such as Pushdown Processes and Petri Nets. PRSs can be adopted as formal
models for programs with dynamic creation and synchronization of concurrent
processes, and with recursive procedures. The model-checking problem for PRSs
and action-based linear temporal logic (ALTL) is undecidable. However,
decidability for some interesting fragment of ALTL remains an open question. In
this paper we state decidability results concerning generalized acceptance
properties about infinite derivations (infinite term rewriting) in PRSs. As a
consequence, we obtain decidability of the model-checking (restricted to
infinite runs) for PRSs and a meaningful fragment of ALTL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405004</id><created>2004-05-03</created><authors><author><keyname>Avaliani</keyname><forenames>Archil</forenames></author></authors><title>Quantum Computers</title><categories>cs.AI cs.AR</categories><acm-class>B.0; C.0; K.4.0; I.0</acm-class><abstract>  This research paper gives an overview of quantum computers - description of
their operation, differences between quantum and silicon computers, major
construction problems of a quantum computer and many other basic aspects. No
special scientific knowledge is necessary for the reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405005</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405005</id><created>2004-05-04</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>Maximum-likelihood decoding of Reed-Solomon Codes is NP-hard</title><categories>cs.CC cs.DM cs.IT math.IT</categories><comments>16 pages, no figures</comments><acm-class>E.4; F.1.3; F.2.1</acm-class><abstract>  Maximum-likelihood decoding is one of the central algorithmic problems in
coding theory. It has been known for over 25 years that maximum-likelihood
decoding of general linear codes is NP-hard. Nevertheless, it was so far
unknown whether maximum- likelihood decoding remains hard for any specific
family of codes with nontrivial algebraic structure. In this paper, we prove
that maximum-likelihood decoding is NP-hard for the family of Reed-Solomon
codes. We moreover show that maximum-likelihood decoding of Reed-Solomon codes
remains hard even with unlimited preprocessing, thereby strengthening a result
of Bruck and Naor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405006</id><created>2004-05-04</created><updated>2005-01-20</updated><authors><author><keyname>Dutot</keyname><forenames>Pierre-Francois</forenames><affiliation>ID - IMAG</affiliation></author><author><keyname>Eyraud</keyname><forenames>Lionel</forenames><affiliation>ID - IMAG</affiliation></author><author><keyname>Mouni&#xe9;</keyname><forenames>Gr&#xe9;gory</forenames><affiliation>ID - IMAG</affiliation></author><author><keyname>Trystram</keyname><forenames>Denis</forenames><affiliation>ID - IMAG</affiliation></author></authors><title>Bi-criteria Algorithm for Scheduling Jobs on Cluster Platforms</title><categories>cs.DC cs.DS</categories><proxy>ccsd ccsd-00001520</proxy><acm-class>ACM F.2.2, ACM D.4.1</acm-class><journal-ref>ACM Symposium on Parallel Algorithms and Architectures (2004)
  125-132</journal-ref><abstract>  We describe in this paper a new method for building an efficient algorithm
for scheduling jobs in a cluster. Jobs are considered as parallel tasks (PT)
which can be scheduled on any number of processors. The main feature is to
consider two criteria that are optimized together. These criteria are the
makespan and the weighted minimal average completion time (minsum). They are
chosen for their complementarity, to be able to represent both user-oriented
objectives and system administrator objectives. We propose an algorithm based
on a batch policy with increasing batch sizes, with a smart selection of jobs
in each batch. This algorithm is assessed by intensive simulation results,
compared to a new lower bound (obtained by a relaxation of ILP) of the optimal
schedules for both criteria separately. It is currently implemented in an
actual real-size cluster platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405007</id><created>2004-05-04</created><authors><author><keyname>Fawcett</keyname><forenames>Tom</forenames></author></authors><title>&quot;In vivo&quot; spam filtering: A challenge problem for data mining</title><categories>cs.AI cs.DB cs.IR</categories><acm-class>I.2.6;H.2.8</acm-class><journal-ref>KDD Explorations vol.5 no.2, Dec 2003. pp.140-148</journal-ref><abstract>  Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email
communication. Many data mining researchers have addressed the problem of
detecting spam, generally by treating it as a static text classification
problem. True in vivo spam filtering has characteristics that make it a rich
and challenging domain for data mining. Indeed, real-world datasets with these
characteristics are typically difficult to acquire and to share. This paper
demonstrates some of these characteristics and argues that researchers should
pursue in vivo spam filtering as an accessible domain for investigating them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405008</id><created>2004-05-04</created><authors><author><keyname>Jain</keyname><forenames>Ravi</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>A Comparative Study of Fuzzy Classification Methods on Breast Cancer
  Data</title><categories>cs.AI</categories><acm-class>I.2.1</acm-class><journal-ref>Australiasian Physical And Engineering Sciences in Medicine,
  Australia, 2004 (forth coming)</journal-ref><abstract>  In this paper, we examine the performance of four fuzzy rule generation
methods on Wisconsin breast cancer data. The first method generates fuzzy if
then rules using the mean and the standard deviation of attribute values. The
second approach generates fuzzy if then rules using the histogram of attributes
values. The third procedure generates fuzzy if then rules with certainty of
each attribute into homogeneous fuzzy sets. In the fourth approach, only
overlapping areas are partitioned. The first two approaches generate a single
fuzzy if then rule for each class by specifying the membership function of each
antecedent fuzzy set using the information about attribute values of training
patterns. The other two approaches are based on fuzzy grids with homogeneous
fuzzy partitions of each attribute. The performance of each approach is
evaluated on breast cancer data sets. Simulation results show that the Modified
grid approach has a high classification rate of 99.73 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405009</id><created>2004-05-04</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Intelligent Systems: Architectures and Perspectives</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Recent Advances in Intelligent Paradigms and Applications, Abraham
  A., Jain L. and Kacprzyk J. (Eds.), Studies in Fuzziness and Soft Computing,
  Springer Verlag Germany, ISBN 3790815381, Chapter 1, pp. 1-35, 2002</journal-ref><abstract>  The integration of different learning and adaptation techniques to overcome
individual limitations and to achieve synergetic effects through the
hybridization or fusion of these techniques has, in recent years, contributed
to a large number of new intelligent system designs. Computational intelligence
is an innovative framework for constructing intelligent hybrid architectures
involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic
Reasoning (PR) and derivative free optimization techniques such as Evolutionary
Computation (EC). Most of these hybridization approaches, however, follow an ad
hoc design methodology, justified by success in certain application domains.
Due to the lack of a common framework it often remains difficult to compare the
various hybrid systems conceptually and to evaluate their performance
comparatively. This chapter introduces the different generic architectures for
integrating intelligent systems. The designing aspects and perspectives of
different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC
systems are presented. Some conclusions are also provided towards the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405010</id><created>2004-05-04</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Nath</keyname><forenames>Baikunth</forenames></author></authors><title>A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Applied Soft Computing Journal, Elsevier Science, Volume 1&amp;2, pp.
  127-138, 2001</journal-ref><abstract>  Neuro-fuzzy systems have attracted growing interest of researchers in various
scientific and engineering areas due to the increasing need of intelligent
systems. This paper evaluates the use of two popular soft computing techniques
and conventional statistical approach based on Box--Jenkins autoregressive
integrated moving average (ARIMA) model to predict electricity demand in the
State of Victoria, Australia. The soft computing methods considered are an
evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN)
trained using scaled conjugate gradient algorithm (CGA) and backpropagation
(BP) algorithm. The forecast accuracy is compared with the forecasts used by
Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we
considered load demand patterns for 10 consecutive months taken every 30 min
for training the different prediction models. Test results show that the
neuro-fuzzy system performed better than neural networks, ARIMA model and the
VPX forecasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405011</id><created>2004-05-04</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Connectionist Models of Neurons, Learning Processes, and
  Artificial Intelligence, Lecture Notes in Computer Science. Volume. 2084,
  Springer Verlag Germany, Jose Mira and Alberto Prieto (Eds.), ISBN
  3540422358, Spain, pp. 269-276, 2001</journal-ref><abstract>  Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)
have attracted the growing interest of researchers in various scientific and
engineering areas due to the growing need of adaptive intelligent systems to
solve the real world problems. ANN learns from scratch by adjusting the
interconnections between layers. FIS is a popular computing framework based on
the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The
advantages of a combination of ANN and FIS are obvious. There are several
approaches to integrate ANN and FIS and very often it depends on the
application. We broadly classify the integration of ANN and FIS into three
categories namely concurrent model, cooperative model and fully fused model.
This paper starts with a discussion of the features of each model and
generalize the advantages and deficiencies of each model. We further focus the
review on the different types of fused neuro-fuzzy systems and citing the
advantages and disadvantages of each model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405012</id><created>2004-05-04</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Steinberg</keyname><forenames>Dan</forenames></author></authors><title>Is Neural Network a Reliable Forecaster on Earth? A MARS Query!</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Bio-Inspired Applications of Connectionism, Lecture Notes in
  Computer Science. Volume. 2085, Springer Verlag Germany, Jose Mira and
  Alberto Prieto (Eds.), ISBN 3540422374, Spain, pp.679-686, 2001</journal-ref><abstract>  Long-term rainfall prediction is a challenging task especially in the modern
world where we are facing the major environmental problem of global warming. In
general, climate and rainfall are highly non-linear phenomena in nature
exhibiting what is known as the butterfly effect. While some regions of the
world are noticing a systematic decrease in annual rainfall, others notice
increases in flooding and severe storms. The global nature of this phenomenon
is very complicated and requires sophisticated computer modeling and simulation
to predict accurately. In this paper, we report a performance analysis for
Multivariate Adaptive Regression Splines (MARS)and artificial neural networks
for one month ahead prediction of rainfall. To evaluate the prediction
efficiency, we made use of 87 years of rainfall data in Kerala state, the
southern part of the Indian peninsula situated at latitude -longitude pairs
(8o29'N - 76o57' E). We used an artificial neural network trained using the
scaled conjugate gradient algorithm. The neural network and MARS were trained
with 40 years of rainfall data. For performance evaluation, network predicted
outputs were compared with the actual rainfall data. Simulation results reveal
that MARS is a good forecasting tool and performed better than the considered
neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405013</id><created>2004-05-04</created><authors><author><keyname>Sorwar</keyname><forenames>Golam</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>DCT Based Texture Classification Using Soft Computing Approach</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Malaysian Journal of Computer Science, 2004 (forth coming)</journal-ref><abstract>  Classification of texture pattern is one of the most important problems in
pattern recognition. In this paper, we present a classification method based on
the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works
on gray level image, the color scheme of each image is transformed into gray
levels. For classifying the images using DCT we used two popular soft computing
techniques namely neurocomputing and neuro-fuzzy computing. We used a
feedforward neural network trained using the backpropagation learning and an
evolving fuzzy neural network to classify the textures. The soft computing
models were trained using 80% of the texture data and remaining was used for
testing and validation purposes. A performance comparison was made among the
soft computing models for the texture classification problem. We also analyzed
the effects of prolonged training of neural networks. It is observed that the
proposed neuro-fuzzy model performed better than neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405014</id><created>2004-05-04</created><authors><author><keyname>AuYeung</keyname><forenames>Andy</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Estimating Genome Reversal Distance by Genetic Algorithm</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>2003 IEEE Congress on Evolutionary Computation (CEC2003),
  Australia, IEEE Press, ISBN 0780378040, pp. 1157-1161, 2003</journal-ref><abstract>  Sorting by reversals is an important problem in inferring the evolutionary
relationship between two genomes. The problem of sorting unsigned permutation
has been proven to be NP-hard. The best guaranteed error bounded is the 3/2-
approximation algorithm. However, the problem of sorting signed permutation can
be solved easily. Fast algorithms have been developed both for finding the
sorting sequence and finding the reversal distance of signed permutation. In
this paper, we present a way to view the problem of sorting unsigned
permutation as signed permutation. And the problem can then be seen as
searching an optimal signed permutation in all n2 corresponding signed
permutations. We use genetic algorithm to conduct the search. Our experimental
result shows that the proposed method outperform the 3/2-approximation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405015</id><created>2004-05-04</created><authors><author><keyname>Nathan</keyname><forenames>Darran</forenames></author><author><keyname>Kit</keyname><forenames>Kelvin Lim Mun</forenames></author><author><keyname>Min</keyname><forenames>Kelly Choo Hon</forenames></author><author><keyname>Chin</keyname><forenames>Philip Wong Jit</forenames></author><author><keyname>Weisensee</keyname><forenames>Andreas</forenames></author></authors><title>A High-Level Reconfigurable Computing Platform Software Frameworks</title><categories>cs.AR</categories><comments>4 pages, 8 figures</comments><acm-class>D.2.11</acm-class><abstract>  Reconfigurable computing refers to the use of processors, such as Field
Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to
take on different processing tasks. A reconfigurable computing platform
describes the hardware and software base on top of which modular extensions can
be created, depending on the desired application. Such reconfigurable computing
platforms can take on varied designs and implementations, according to the
constraints imposed and features desired by the scope of applications. This
paper introduces a PC-based reconfigurable computing platform software
frameworks that is flexible and extensible enough to abstract the different
hardware types and functionality that different PCs may have. The requirements
of the software platform, architectural issues addressed, rationale behind the
decisions made, and frameworks design implemented are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405016</id><created>2004-05-04</created><authors><author><keyname>Mukkamala</keyname><forenames>Srinivas</forenames></author><author><keyname>Sung</keyname><forenames>Andrew H.</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>Intrusion Detection Systems Using Adaptive Regression Splines</title><categories>cs.AI</categories><acm-class>C.2.0</acm-class><journal-ref>6th International Conference on Enterprise Information Systems,
  ICEIS'04, Portugal, I. Seruca, J. Filipe, S. Hammoudi and J. Cordeiro (Eds.),
  ISBN 972-8865-00-7, Vol. 3, pp. 26-33, 2004</journal-ref><abstract>  Past few years have witnessed a growing recognition of intelligent techniques
for the construction of efficient and reliable intrusion detection systems. Due
to increasing incidents of cyber attacks, building effective intrusion
detection systems (IDS) are essential for protecting information systems
security, and yet it remains an elusive goal and a great challenge. In this
paper, we report a performance analysis between Multivariate Adaptive
Regression Splines (MARS), neural networks and support vector machines. The
MARS procedure builds flexible regression models by fitting separate splines to
distinct intervals of the predictor variables. A brief comparison of different
neural network learning algorithms is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405017</id><created>2004-05-04</created><authors><author><keyname>Paprzycki</keyname><forenames>Marcin</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Guo</keyname><forenames>Ruiyuan</forenames></author></authors><title>Data Mining Approach for Analyzing Call Center Performance</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>The 17th International Conference on Industrial &amp; Engineering
  Applications of Artificial Intelligence and Expert Systems, Canada, Springer
  Verlag, Germany, 2004 (forth coming)</journal-ref><abstract>  The aim of our research was to apply well-known data mining techniques (such
as linear neural networks, multi-layered perceptrons, probabilistic neural
networks, classification and regression trees, support vector machines and
finally a hybrid decision tree neural network approach) to the problem of
predicting the quality of service in call centers; based on the performance
data actually collected in a call center of a large insurance company. Our aim
was two-fold. First, to compare the performance of models built using the
above-mentioned techniques and, second, to analyze the characteristics of the
input sensitivity in order to better understand the relationship between the
perform-ance evaluation process and the actual performance and in this way help
improve the performance of call centers. In this paper we summarize our
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405018</id><created>2004-05-04</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Philip</keyname><forenames>Ninan Sajith</forenames></author><author><keyname>Saratchandran</keyname><forenames>P.</forenames></author></authors><title>Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>International Journal of Neural, Parallel &amp; Scientific
  Computations, USA, Volume 11, Issue (1&amp;2), pp. 143-160, 2003</journal-ref><abstract>  The use of intelligent systems for stock market predictions has been widely
established. In this paper, we investigate how the seemingly chaotic behavior
of stock markets could be well represented using several connectionist
paradigms and soft computing techniques. To demonstrate the different
techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&amp;P
CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4
year's NIFTY index values. This paper investigates the development of a
reliable and efficient technique to model the seemingly chaotic behavior of
stock markets. We considered an artificial neural network trained using
Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno
neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper
briefly explains how the different connectionist paradigms could be formulated
using different learning methods and then investigates whether they can provide
the required level of performance, which are sufficiently good and robust so as
to provide a reliable forecast model for stock market indices. Experiment
results reveal that all the connectionist paradigms considered could represent
the stock indices behavior very accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405019</id><created>2004-05-04</created><authors><author><keyname>Petrovic-Lazarevic</keyname><forenames>Sonja</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision
  Making Problems</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>International Journal of Neural, Parallel &amp; Scientific
  Computations, USA, Volume 11, Issues (1&amp;2), pp. 53-68, 2003</journal-ref><abstract>  The purpose of this paper is to point to the usefulness of applying a linear
mathematical formulation of fuzzy multiple criteria objective decision methods
in organising business activities. In this respect fuzzy parameters of linear
programming are modelled by preference-based membership functions. This paper
begins with an introduction and some related research followed by some
fundamentals of fuzzy set theory and technical concepts of fuzzy multiple
objective decision models. Further a real case study of a manufacturing plant
and the implementation of the proposed technique is presented. Empirical
results clearly show the superiority of the fuzzy technique in optimising
individual objective functions when compared to non-fuzzy approach.
Furthermore, for the problem considered, the optimal solution helps to infer
that by incorporating fuzziness in a linear programming model either in
constraints, or both in objective functions and constraints, provides a similar
(or even better) level of satisfaction for obtained results compared to
non-fuzzy linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405020</id><created>2004-05-05</created><authors><author><keyname>Friedman</keyname><forenames>Joel</forenames></author></authors><title>A proof of Alon's second eigenvalue conjecture and related problems</title><categories>cs.DM math.CO</categories><comments>To appear in Memoirs of the American Mathematical Society. 118 pages.
  This newer version should have a two page glossary</comments><acm-class>G.2.2</acm-class><abstract>  In this paper we show the following conjecture of Noga Alon. Fix a positive
integer d&gt;2 and real epsilon &gt; 0; consider the probability that a random
d-regular graph on n vertices has the second eigenvalue of its adjacency matrix
greater than 2 sqrt(d-1) + epsilon; then this probability goes to zero as n
tends to infinity.
  We prove the conjecture for a number of notions of random d-regular graph,
including models for d odd. We also estimate the aforementioned probability
more precisely, showing in many cases and models (but not all) that it decays
like a polynomial in 1/n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405021</identifier>
 <datestamp>2007-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405021</id><created>2004-05-05</created><authors><author><keyname>Malajovich</keyname><forenames>Gregorio</forenames></author><author><keyname>Meer</keyname><forenames>Klaus</forenames></author></authors><title>Computing Multi-Homogeneous Bezout Numbers is Hard</title><categories>cs.CC cs.SC</categories><acm-class>F.2.1;G.1.5</acm-class><journal-ref>Theory of Computing Systems, Volume 40, Number 4 / June, 2007</journal-ref><doi>10.1007/s00224-006-1322-y</doi><abstract>  The multi-homogeneous Bezout number is a bound for the number of solutions of
a system of multi-homogeneous polynomial equations, in a suitable product of
projective spaces.
  Given an arbitrary, not necessarily multi-homogeneous system, one can ask for
the optimal multi-homogenization that would minimize the Bezout number.
  In this paper, it is proved that the problem of computing, or even estimating
the optimal multi-homogeneous Bezout number is actually NP-hard.
  In terms of approximation theory for combinatorial optimization, the problem
of computing the best multi-homogeneous structure does not belong to APX,
unless P = NP.
  Moreover, polynomial time algorithms for estimating the minimal
multi-homogeneous Bezout number up to a fixed factor cannot exist even in a
randomized setting, unless BPP contains NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405022</id><created>2004-05-06</created><authors><author><keyname>Harkins</keyname><forenames>Ryan</forenames></author><author><keyname>Weber</keyname><forenames>Eric</forenames></author><author><keyname>Westmeyer</keyname><forenames>Andrew</forenames></author></authors><title>Encryption Schemes using Finite Frames and Hadamard Arrays</title><categories>cs.CR</categories><comments>14 pages, 11 figures</comments><acm-class>E.3</acm-class><abstract>  We propose a cipher similar to the One Time Pad and McEliece cipher based on
a subband coding scheme. The encoding process is an approximation to the One
Time Pad encryption scheme. We present results of numerical experiments which
suggest that a brute force attack to the proposed scheme does not result in all
possible plaintexts, as the One Time Pad does, but still the brute force attack
does not compromise the system. However, we demonstrate that the cipher is
vulnerable to a chosen-plaintext attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405023</id><created>2004-05-06</created><authors><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Winton</keyname><forenames>Lyle</forenames></author></authors><title>A Grid Service Broker for Scheduling Distributed Data-Oriented
  Applications on Global Grids</title><categories>cs.DC</categories><comments>15 pages, 11 figures, 1 table</comments><report-no>Technical Report, GRIDS-TR-2004-1, Grid Computing and Distributed
  Systems Laboratory, University of Melbourne, Australia, February 2004</report-no><acm-class>C.1.4</acm-class><abstract>  The next generation of scientific experiments and studies, popularly called
as e-Science, is carried out by large collaborations of researchers distributed
around the world engaged in analysis of huge collections of data generated by
scientific instruments. Grid computing has emerged as an enabler for e-Science
as it permits the creation of virtual organizations that bring together
communities with common objectives. Within a community, data collections are
stored or replicated on distributed resources to enhance storage capability or
efficiency of access. In such an environment, scientists need to have the
ability to carry out their studies by transparently accessing distributed data
and computational resources. In this paper, we propose and develop a Grid
broker that mediates access to distributed resources by (a) discovering
suitable data sources for a given analysis scenario, (b) suitable computational
resources, (c) optimally mapping analysis jobs to resources, (d) deploying and
monitoring job execution on selected resources, (e) accessing data from local
or remote data source during job execution and (f) collating and presenting
results. The broker supports a declarative and dynamic parametric programming
model for creating grid applications. We have used this model in grid-enabling
a high energy physics analysis application (Belle Analysis Software Framework).
The broker has been used in deploying Belle experiment data analysis jobs on a
grid testbed, called Belle Analysis Data Grid, having resources distributed
across Australia interconnected through GrangeNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405024</id><created>2004-05-06</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Meta-Learning Evolutionary Artificial Neural Networks</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Neurocomputing Journal, Elsevier Science, Netherlands, Vol. 56c,
  pp. 1-38, 2004</journal-ref><abstract>  In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405025</id><created>2004-05-06</created><authors><author><keyname>Auyeung</keyname><forenames>Andy</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>The Largest Compatible Subset Problem for Phylogenetic Data</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>Genetic and Evolutionary Computation 2004 Conference (GECCO-2004),
  Bird-of-a-feather Workshop On Application of Hybrid Evolutionary Algorithms
  to Complex Optimization Problems, Springer Verlag Germany, 2004 (forth
  coming)</journal-ref><abstract>  The phylogenetic tree construction is to infer the evolutionary relationship
between species from the experimental data. However, the experimental data are
often imperfect and conflicting each others. Therefore, it is important to
extract the motif from the imperfect data. The largest compatible subset
problem is that, given a set of experimental data, we want to discard the
minimum such that the remaining is compatible. The largest compatible subset
problem can be viewed as the vertex cover problem in the graph theory that has
been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary
Computing (EC) method for this problem. The proposed method combines the EC
approach and the algorithmic approach for special structured graphs. As a
result, the complexity of the problem is dramatically reduced. Experiments were
performed on randomly generated graphs with different edge densities. The
vertex covers produced by the proposed method were then compared to the vertex
covers produced by a 2-approximation algorithm. The experimental results showed
that the proposed method consistently outperformed a classical 2- approximation
algorithm. Furthermore, a significant improvement was found when the graph
density was small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405026</id><created>2004-05-06</created><authors><author><keyname>Tran</keyname><forenames>Cong</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Jain</keyname><forenames>Lakhmi</forenames></author></authors><title>A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>The IEEE International Conference on Fuzzy Systems, FUZZ-IEEE'03,
  IEEE Press, ISBN 0780378113, pp. 1092-1097, 2003</journal-ref><abstract>  Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing
technologies that underlie the conception, design and utilization of
intelligent systems. Several works have been done where engineers and
scientists have applied intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a concurrent
fuzzy-neural network approach combining unsupervised and supervised learning
techniques to develop the Tactical Air Combat Decision Support System (TACDSS).
Experiment results clearly demonstrate the efficiency of the proposed
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405027</id><created>2004-05-06</created><authors><author><keyname>Togelius</keyname><forenames>Julian</forenames></author></authors><title>Evolution of a Subsumption Architecture Neurocontroller</title><categories>cs.AI cs.NE</categories><acm-class>I.2.9; I.2.6</acm-class><journal-ref>Journal of Intelligent and Fuzzy Systems, Vol. 15, No. 1 (2004)</journal-ref><abstract>  An approach to robotics called layered evolution and merging features from
the subsumption architecture into evolutionary robotics is presented, and its
advantages are discussed. This approach is used to construct a layered
controller for a simulated robot that learns which light source to approach in
an environment with obstacles. The evolvability and performance of layered
evolution on this task is compared to (standard) monolithic evolution,
incremental and modularised evolution. To corroborate the hypothesis that a
layered controller performs at least as well as an integrated one, the evolved
layers are merged back into a single network. On the grounds of the test
results, it is argued that layered evolution provides a superior approach for
many tasks, and it is suggested that this approach may be the key to scaling up
evolutionary robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405028</id><created>2004-05-06</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Analysis of Hybrid Soft and Hard Computing Techniques for Forex
  Monitoring Systems</title><categories>cs.AI</categories><acm-class>1.2.0</acm-class><journal-ref>IEEE International Conference on Fuzzy Systems (IEEE FUZZ'02),
  2002 IEEE World Congress on Computational Intelligence, Hawaii, ISBN
  0780372808, IEEE Press pp. 1616 -1622, 2002</journal-ref><abstract>  In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405029</identifier>
 <datestamp>2010-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405029</id><created>2004-05-06</created><updated>2010-11-10</updated><authors><author><keyname>Schlei</keyname><forenames>B. R.</forenames></author></authors><title>A New Computational Framework For 2D Shape-Enclosing Contours</title><categories>cs.CV cs.CG</categories><comments>12 pages, 14 figures, updated version including some further, minor
  corrections</comments><report-no>LA-UR-04-3115</report-no><acm-class>G.1.2</acm-class><journal-ref>Image and Vision Computing Volume 27, Issue 6, 4 May 2009, Pages
  637-647</journal-ref><doi>10.1016/j.imavis.2008.06.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new framework for one-dimensional contour extraction from
discrete two-dimensional data sets is presented. Contour extraction is
important in many scientific fields such as digital image processing, computer
vision, pattern recognition, etc. This novel framework includes (but is not
limited to) algorithms for dilated contour extraction, contour displacement,
shape skeleton extraction, contour continuation, shape feature based contour
refinement and contour simplification. Many of the new techniques depend
strongly on the application of a Delaunay tessellation. In order to demonstrate
the versatility of this novel toolbox approach, the contour extraction
techniques presented here are applied to scientific problems in material
science, biology and heavy ion physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405030</id><created>2004-05-06</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Business Intelligence from Web Usage Mining</title><categories>cs.AI</categories><acm-class>1.2.0</acm-class><journal-ref>Journal of Information &amp; Knowledge Management (JIKM), World
  Scientific Publishing Co., Singapore, Vol. 2, No. 4, pp. 375-390, 2003</journal-ref><abstract>  The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management, creating adaptive Web
sites, business and support services, personalization, network traffic flow
analysis and so on. In this paper, we present the important concepts of Web
usage mining and its various practical applications. We further present a novel
approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture
of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy
inference system to analyze the Web site visitor trends. A hybrid evolutionary
fuzzy clustering algorithm is proposed in this paper to optimally segregate
similar user interests. The clustered data is then used to analyze the trends
using a Takagi-Sugeno fuzzy inference system learned using a combination of
evolutionary algorithm and neural network learning. Proposed approach is
compared with self-organizing maps (to discover patterns) and several function
approximation techniques like neural networks, linear genetic programming and
Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are
graphically illustrated and the practical significance is discussed in detail.
Empirical results clearly show that the proposed Web usage-mining framework is
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405031</id><created>2004-05-06</created><authors><author><keyname>Tran</keyname><forenames>Cong</forenames></author><author><keyname>Jain</keyname><forenames>Lakhmi</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic
  Approach for Tactical Air Combat Decision Support System</title><categories>cs.AI</categories><acm-class>1.2.0</acm-class><journal-ref>15th Australian Joint Conference on Artificial Intelligence
  (AI'02) Australia, LNAI 2557, Springer Verlag, Germany, pp. 672-679, 2002</journal-ref><abstract>  Normally a decision support system is build to solve problem where
multi-criteria decisions are involved. The knowledge base is the vital part of
the decision support containing the information or data that is used in
decision-making process. This is the field where engineers and scientists have
applied several intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a hybrid
neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference
system for the Tactical Air Combat Decision Support System (TACDSS). Some
simulation results demonstrating the difference of the learning techniques and
are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405032</id><created>2004-05-06</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using
  Neural Network Learning and Evolutionary Computation</title><categories>cs.AI</categories><acm-class>1.2.0</acm-class><journal-ref>The 17th IEEE International Symposium on Intelligent Control,
  ISIC'02, IEEE Press, ISBN 0780376218, pp 327-332, 2002</journal-ref><abstract>  Several adaptation techniques have been investigated to optimize fuzzy
inference systems. Neural network learning algorithms have been used to
determine the parameters of fuzzy inference system. Such models are often
called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model
there is no guarantee that the neural network learning algorithm converges and
the tuning of fuzzy inference system will be successful. Success of
evolutionary search procedures for optimization of fuzzy inference system is
well proven and established in many application areas. In this paper, we will
explore how the optimization of fuzzy inference systems could be further
improved using a meta-heuristic approach combining neural network learning and
evolutionary computation. The proposed technique could be considered as a
methodology to integrate neural networks, fuzzy inference systems and
evolutionary search procedures. We present the theoretical frameworks and some
experimental results to demonstrate the efficiency of the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405033</id><created>2004-05-06</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Optimization of Evolutionary Neural Networks Using Hybrid Learning
  Algorithms</title><categories>cs.AI</categories><acm-class>1.2.0</acm-class><journal-ref>IEEE International Joint Conference on Neural Networks (IJCNN'02),
  2002 IEEE World Congress on Computational Intelligence, Hawaii, ISBN
  0780372786, IEEE Press, Volume 3, pp. 2797-2802, 2002</journal-ref><abstract>  Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405034</id><created>2004-05-07</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 45</title><categories>cs.CG cs.DM</categories><comments>4 pages, to appear in SIGACT News and in IJCGA, 2004</comments><acm-class>F.2.2</acm-class><abstract>  The algorithm of Edelsbrunner for surface reconstruction by ``wrapping'' a
set of points in R^3 is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405035</id><created>2004-05-10</created><updated>2004-06-21</updated><authors><author><keyname>Kannan</keyname><forenames>Rajgopal</forenames></author><author><keyname>Ray</keyname><forenames>Lydia</forenames></author><author><keyname>Durresi</keyname><forenames>Arjan</forenames></author><author><keyname>Iyengar</keyname><forenames>S.</forenames></author></authors><title>Security-Performance Tradeoffs of Inheritance based Key Predistribution
  for Wireless Sensor Networks</title><categories>cs.NI cs.CR</categories><comments>18 pages, ESAS'04 1st Euro Wkshp on Security in Ad-Hoc and Sensor
  Networks</comments><acm-class>C.2.0; C.2.1; K.6.5</acm-class><abstract>  Key predistribution is a well-known technique for ensuring secure
communication via encryption among sensors deployed in an ad-hoc manner to form
a sensor network. In this paper, we propose a novel 2-Phase technique for key
predistribution based on a combination of inherited and random key assignments
from the given key pool to individual sensor nodes. We also develop an
analytical framework for measuring security-performance tradeoffs of different
key distribution schemes by providing metrics for measuring sensornet
connectivity and resiliency to enemy attacks. In particular, we show
analytically that the 2-Phase scheme provides better average connectivity and
superior $q$-composite connectivity than the random scheme. We then prove that
the invulnerability of a communication link under arbitrary number of node
captures by an adversary is higher under the 2-Phase scheme. The probability of
a communicating node pair having an exclusive key also scales better with
network size under the 2-Phase scheme. We also show analytically that the
vulnerability of an arbitrary communication link in the sensornet to single
node capture is lower under 2-Phase assuming both network-wide as well as
localized capture. Simulation results also show that the number of exclusive
keys shared between any two nodes is higher while the number of $q$-composite
links compromised when a given number of nodes are captured by the enemy is
smaller under the 2-Phase scheme as compared to the random one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405036</id><created>2004-05-10</created><authors><author><keyname>Gopi</keyname><forenames>M.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Single-Strip Triangulation of Manifolds with Arbitrary Topology</title><categories>cs.CG cs.GR</categories><comments>12 pages, 10 figures. To appear at Eurographics 2004</comments><acm-class>I.3.5; G.2.2</acm-class><abstract>  Triangle strips have been widely used for efficient rendering. It is
NP-complete to test whether a given triangulated model can be represented as a
single triangle strip, so many heuristics have been proposed to partition
models into few long strips. In this paper, we present a new algorithm for
creating a single triangle loop or strip from a triangulated model. Our method
applies a dual graph matching algorithm to partition the mesh into cycles, and
then merges pairs of cycles by splitting adjacent triangles when necessary. New
vertices are introduced at midpoints of edges and the new triangles thus formed
are coplanar with their parent triangles, hence the visual fidelity of the
geometry is not changed. We prove that the increase in the number of triangles
due to this splitting is 50% in the worst case, however for all models we
tested the increase was less than 2%. We also prove tight bounds on the number
of triangles needed for a single-strip representation of a model with holes on
its boundary. Our strips can be used not only for efficient rendering, but also
for other applications including the generation of space filling curves on a
manifold of any arbitrary topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405037</id><created>2004-05-10</created><authors><author><keyname>Miram</keyname><forenames>G. E.</forenames></author><author><keyname>Petrov</keyname><forenames>V. K.</forenames></author></authors><title>A Probabilistic Model of Machine Translation</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><abstract>  A probabilistic model for computer-based generation of a machine translation
system on the basis of English-Russian parallel text corpora is suggested. The
model is trained using parallel text corpora with pre-aligned source and target
sentences. The training of the model results in a bilingual dictionary of words
and &quot;word blocks&quot; with relevant translation probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405038</id><created>2004-05-11</created><updated>2006-01-18</updated><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Deductive Algorithmic Knowledge</title><categories>cs.AI cs.LO</categories><comments>28 pages. A preliminary version of this paper appeared in the
  Proceedings of the 8th International Symposium on Artificial Intelligence and
  Mathematics, AI&amp;M 22-2004, 2004</comments><acm-class>I.2.4; F.4.1</acm-class><journal-ref>Journal of Logic and Computation 16 (2), pp. 287-309, 2006</journal-ref><abstract>  The framework of algorithmic knowledge assumes that agents use algorithms to
compute the facts they explicitly know. In many cases of interest, a deductive
system, rather than a particular algorithm, captures the formal reasoning used
by the agents to compute what they explicitly know. We introduce a logic for
reasoning about both implicit and explicit knowledge with the latter defined
with respect to a deductive system formalizing a logical theory for agents. The
highly structured nature of deductive systems leads to very natural
axiomatizations of the resulting logic when interpreted over any fixed
deductive system. The decision problem for the logic, in the presence of a
single agent, is NP-complete in general, no harder than propositional logic. It
remains NP-complete when we fix a deductive system that is decidable in
nondeterministic polynomial time. These results extend in a straightforward way
to multiple agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405039</id><created>2004-05-12</created><authors><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Catching the Drift: Probabilistic Content Models, with Applications to
  Generation and Summarization</title><categories>cs.CL</categories><comments>Best paper award</comments><acm-class>I.2.7</acm-class><journal-ref>HLT-NAACL 2004: Proceedings of the Main Conference, pp. 113--120</journal-ref><abstract>  We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405040</id><created>2004-05-11</created><authors><author><keyname>Cao</keyname><forenames>Yongzhi</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Supervisory Control of Fuzzy Discrete Event Systems</title><categories>cs.DM cs.DC</categories><comments>12 pages, 2 figures</comments><acm-class>G.3;I.6.8</acm-class><journal-ref>A short version has been published in the IEEE Transactions on
  Systems, Man, and Cybernetics--Part B: Cybernetics, 35(2), pp. 366-371, April
  2005.</journal-ref><abstract>  In order to cope with situations in which a plant's dynamics are not
precisely known, we consider the problem of supervisory control for a class of
discrete event systems modelled by fuzzy automata. The behavior of such
discrete event systems is described by fuzzy languages; the supervisors are
event feedback and can disable only controllable events with any degree. The
concept of discrete event system controllability is thus extended by
incorporating fuzziness. In this new sense, we present a necessary and
sufficient condition for a fuzzy language to be controllable. We also study the
supremal controllable fuzzy sublanguage and the infimal controllable fuzzy
superlanguage when a given pre-specified desired fuzzy language is
uncontrollable. Our framework generalizes that of Ramadge-Wonham and reduces to
Ramadge-Wonham framework when membership grades in all fuzzy languages must be
either 0 or 1. The theoretical development is accompanied by illustrative
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405041</id><created>2004-05-12</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The modulus in the CAD system drawings as a base of developing of the
  problem-oriented extensions</title><categories>cs.CE cs.DS</categories><comments>2 pages, no figures, in Russian</comments><acm-class>J.6</acm-class><abstract>  The concept of the &quot;modulus&quot; in the CAD system drawings is characterized,
being a base of developing of the problem-oriented extensions. The modulus
consists of visible geometric elements of the drawing and invisible parametric
representation of the modelling object. The technological advantages of
moduluss in a complex CAD system developing are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405042</id><created>2004-05-12</created><updated>2004-05-19</updated><authors><author><keyname>Herman</keyname><forenames>T.</forenames></author><author><keyname>Tixeuil</keyname><forenames>S.</forenames></author></authors><title>A Distributed TDMA Slot Assignment Algorithm for Wireless Sensor
  Networks</title><categories>cs.DC cs.NI</categories><report-no>TR04-02 University of Iowa Department of Computer Science</report-no><acm-class>C.2.1; C.2.5; H.3.4</acm-class><abstract>  Wireless sensor networks benefit from communication protocols that reduce
power requirements by avoiding frame collision. Time Division Media Access
methods schedule transmission in slots to avoid collision, however these
methods often lack scalability when implemented in \emph{ad hoc} networks
subject to node failures and dynamic topology. This paper reports a distributed
algorithm for TDMA slot assignment that is self-stabilizing to transient faults
and dynamic topology change. The expected local convergence time is O(1) for
any size network satisfying a constant bound on the size of a node
neighborhood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405043</id><created>2004-05-12</created><updated>2004-05-12</updated><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Poland</keyname><forenames>Jan</forenames></author></authors><title>Prediction with Expert Advice by Following the Perturbed Leader for
  General Weights</title><categories>cs.LG cs.AI</categories><comments>16 LaTeX pages</comments><report-no>IDSIA-08-04</report-no><acm-class>I.2.6; G.3</acm-class><journal-ref>Proc. 15th International Conf. on Algorithmic Learning Theory
  (ALT-2004), pages 279-293</journal-ref><abstract>  When applying aggregating strategies to Prediction with Expert Advice, the
learning rate must be adaptively tuned. The natural choice of
sqrt(complexity/current loss) renders the analysis of Weighted Majority
derivatives quite complicated. In particular, for arbitrary weights there have
been no results proven so far. The analysis of the alternative &quot;Follow the
Perturbed Leader&quot; (FPL) algorithm from Kalai (2003} (based on Hannan's
algorithm) is easier. We derive loss bounds for adaptive learning rate and both
finite expert classes with uniform weights and countable expert classes with
arbitrary weights. For the former setup, our loss bounds match the best known
results so far, while for the latter our results are (to our knowledge) new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405044</id><created>2004-05-12</created><authors><author><keyname>Kurland</keyname><forenames>Oren</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Corpus structure, language models, and ad hoc information retrieval</title><categories>cs.IR cs.CL</categories><comments>To appear, SIGIR 2004</comments><acm-class>H.3.3; I.2.7</acm-class><abstract>  Most previous work on the recently developed language-modeling approach to
information retrieval focuses on document-specific characteristics, and
therefore does not take into account the structure of the surrounding corpus.
We propose a novel algorithmic framework in which information provided by
document-based language models is enhanced by the incorporation of information
drawn from clusters of similar documents. Using this framework, we develop a
suite of new algorithms. Even the simplest typically outperforms the standard
language-modeling approach in precision and recall, and our new interpolation
algorithm posts statistically significant improvements for both metrics over
all three corpora tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405045</id><created>2004-05-13</created><authors><author><keyname>Vatrapu</keyname><forenames>Ravikiran</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author></authors><title>Culture and International Usability Testing: The Effects of Culture in
  Structured Interviews</title><categories>cs.HC cs.SE</categories><comments>8 pages</comments><acm-class>H5.2</acm-class><abstract>  The global audience for software products includes members of different
countries, religions, and cultures: people who speak different languages, have
different life styles, and have different perceptions and expectations of any
given product. A major impediment in interface development is that there is
inadequate empirical evidence for the effects of culture in the usability
engineering methods used for developing user interfaces. This paper presents a
controlled study investigating the effects of culture on the effectiveness of
structured interviews in usability testing. The experiment consisted of
usability testing of a website with two independent groups of Indian
participants by two interviewers; one belonging to the Indian culture and the
other to the Anglo-American culture. Participants found more usability problems
and made more suggestions to an interviewer who was a member of the same
(Indian) culture than to the foreign (Anglo-American) interviewer. The results
of the study empirically establish that culture significantly affects the
efficacy of structured interviews during international user testing. The
implications of this work for usability engineering are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405046</id><created>2004-05-13</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Jain</keyname><forenames>Ravi</forenames></author></authors><title>Soft Computing Models for Network Intrusion Detection Systems</title><categories>cs.CR</categories><acm-class>K.6.5</acm-class><journal-ref>Soft Computing in Knowledge Discovery: Methods and Applications,
  Saman Halgamuge and Lipo Wang (Eds.), Studies in Fuzziness and Soft
  Computing, Springer Verlag Germany, Chapter 16, 20 pages, 2004</journal-ref><abstract>  Security of computers and the networks that connect them is increasingly
becoming of great significance. Computer security is defined as the protection
of computing systems against threats to confidentiality, integrity, and
availability. There are two types of intruders: external intruders, who are
unauthorized users of the machines they attack, and internal intruders, who
have permission to access the system with some restrictions. This chapter
presents a soft computing approach to detect intrusions in a network. Among the
several soft computing paradigms, we investigated fuzzy rule-based classifiers,
decision trees, support vector machines, linear genetic programming and an
ensemble method to model fast and efficient intrusion detection systems.
Empirical results clearly show that soft computing approach could play a major
role for intrusion detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405047</id><created>2004-05-14</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>Modular technology of developing of the problem-oriented extensions of a
  CAD system of reconstruction of the plant</title><categories>cs.CE cs.DS</categories><comments>8 pages, no figures, in Russian</comments><acm-class>I.2.1, J.6</acm-class><abstract>  The modular technology of creation of the problem-oriented extensions of a
CAD system is described, which was realised in a system TechnoCAD GlassX for
designing of reconstruction of the plants. The modularity of the technology is
expressed in storage of all parameters of the design in one element of the
drawing - modulus, with automatic generation of a geometrical part of the
modulus from these parameters. The common principles of the system organization
of extensions developing are described: separation of the part of the design to
automize in this extension, architecture of parameters in the form of the lists
of objects with their properties and links to another objects, separation of
common and special operations, stages of the developing, boundaries of
applicability of technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405048</id><created>2004-05-14</created><authors><author><keyname>Tomov</keyname><forenames>Stanimire</forenames></author><author><keyname>McGuigan</keyname><forenames>Michael</forenames></author></authors><title>Interactive visualization of higher dimensional data in a multiview
  environment</title><categories>cs.GR</categories><comments>6 pages, 3 figures</comments><acm-class>I.3.6; I.3.8; H.5.2</acm-class><abstract>  We develop multiple view visualization of higher dimensional data. Our work
was chiefly motivated by the need to extract insight from four dimensional
Quantum Chromodynamic (QCD) data. We develop visualization where multiple
views, generally views of 3D projections or slices of a higher dimensional
data, are tightly coupled not only by their specific order but also by a view
synchronizing interaction style, and an internally defined interaction
language. The tight coupling of the different views allows a fast and
well-coordinated exploration of the data. In particular, the visualization
allowed us to easily make consistency checks of the 4D QCD data and to infer
the correctness of particle properties calculations. The software developed was
also successfully applied in material studies, in particular studies of
meteorite properties. Our implementation uses the VTK API. To handle a large
number of views (slices/projections) and to still maintain good resolution, we
use IBM T221 display (3840 X 2400 pixels).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405049</id><created>2004-05-15</created><authors><author><keyname>Edwards</keyname><forenames>Ron</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Petrovic-Lazarevic</keyname><forenames>Sonja</forenames></author></authors><title>Export Behaviour Modeling Using EvoNF Approach</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>The International Conference on Computational Science 2003 (ICCS
  2003), Springer Verlag, Lecture Notes in Computer Science Volume 2660, Sloot
  P.M.A. et al (Eds.), pp. 169-178, 2003</journal-ref><abstract>  The academic literature suggests that the extent of exporting by
multinational corporation subsidiaries (MCS) depends on their product
manufactured, resources, tax protection, customers and markets, involvement
strategy, financial independence and suppliers' relationship with a
multinational corporation (MNC). The aim of this paper is to model the complex
export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order
to determine the actual volume of MCS export output (sales exported). The
proposed fuzzy inference system is optimised by using neural network learning
and evolutionary computation. Empirical results clearly show that the proposed
approach could model the export behaviour reasonable well compared to a direct
neural network approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405050</id><created>2004-05-15</created><authors><author><keyname>Chong</keyname><forenames>Miao M.</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Paprzycki</keyname><forenames>Marcin</forenames></author></authors><title>Traffic Accident Analysis Using Decision Trees and Neural Networks</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>IADIS International Conference on Applied Computing, Portugal,
  IADIS Press, Pedro Isaias et al. (Eds.), ISBN: 9729894736, Volume 2, pp.
  39-42, 2004</journal-ref><abstract>  The costs of fatalities and injuries due to traffic accident have a great
impact on society. This paper presents our research to model the severity of
injury resulting from traffic accidents using artificial neural networks and
decision trees. We have applied them to an actual data set obtained from the
National Automotive Sampling System (NASS) General Estimates System (GES).
Experiment results reveal that in all the cases the decision tree outperforms
the neural network. Our research analysis also shows that the three most
important factors in fatal injury are: driver's seat belt usage, light
condition of the roadway, and driver's alcohol usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405051</id><created>2004-05-15</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Riaz</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Short Term Load Forecasting Models in Czech Republic Using Soft
  Computing Paradigms</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>International Journal of Knowledge-Based Intelligent Engineering
  Systems, IOS Press Netherlands, Volume 7, Number 4, pp. 172-179, 2003</journal-ref><abstract>  This paper presents a comparative study of six soft computing models namely
multilayer perceptron networks, Elman recurrent neural network, radial basis
function network, Hopfield model, fuzzy inference system and hybrid fuzzy
neural network for the hourly electricity demand forecast of Czech Republic.
The soft computing models were trained and tested using the actual hourly load
data for seven years. A comparison of the proposed techniques is presented for
predicting 2 day ahead demands for electricity. Simulation results indicate
that hybrid fuzzy neural network and radial basis function networks are the
best candidates for the analysis and forecasting of electricity demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405052</id><created>2004-05-15</created><authors><author><keyname>Tran</keyname><forenames>Cong</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Jain</keyname><forenames>Lakhmi</forenames></author></authors><title>Decision Support Systems Using Intelligent Paradigms</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><journal-ref>International Journal of American Romanian Academy of Arts and
  Sciences, 2004 (forth coming)</journal-ref><abstract>  Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing (SC)
technologies that underlie the conception, design and utilization of
intelligent systems. In this paper, we present different SC paradigms involving
an artificial neural network trained using the scaled conjugate gradient
algorithm, two different fuzzy inference methods optimised using neural network
learning/evolutionary algorithms and regression trees for developing
intelligent decision support systems. We demonstrate the efficiency of the
different algorithms by developing a decision support system for a Tactical Air
Combat Environment (TACE). Some empirical comparisons between the different
algorithms are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405053</id><created>2004-05-16</created><authors><author><keyname>Lubachevsky</keyname><forenames>Boris</forenames></author><author><keyname>Weiss</keyname><forenames>Alan</forenames></author></authors><title>Synchronous Relaxation for Parallel Ising Spin Simulations</title><categories>cs.DC cond-mat.mtrl-sci cs.DS physics.comp-ph</categories><comments>Extended abstract. Conference version. The full paper in preparation</comments><acm-class>D.1.3; D.4.1; D.4.8; I.6.8</acm-class><journal-ref>15th Workshop on Parallel and Distributed Simulation, Lake
  Arrowhead, California, May 2001, pp.185-192</journal-ref><abstract>  A new parallel algorithm for simulating Ising spin systems is presented. The
sequential prototype is the n-fold way algorithm cite{BKL75}, which is
efficient but is hard to parallelize using conservative methods. Our parallel
algorithm is optimistic. Unlike other optimistic algorithms, e.g., Time Warp,
our algorithm is synchronous. It also belongs to the class of simulations known
as ``relaxation'' cite{CS8 hence it is named ``synchronous relaxation.'' We
derive performance guarantees for this algorithm. If N is the number of PEs,
then under weak assumptions we show that the number of correct events processed
per unit of time is, on average, at least of order N/log(N). All communication
delays, processing time, and busy waits are taken into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405054</id><created>2004-05-17</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The model of the tables in design documentation for operating with the
  electronic catalogs and for specifications making in a CAD system</title><categories>cs.CE cs.DS</categories><comments>5 pages, 4 figures, in Russian</comments><acm-class>E.2, I.2.1, J.6</acm-class><abstract>  The hierarchic block model of the tables in design documentation as a part of
a CAD system is described, intended for automatic specifications making of
elements of the drawings, with usage of the electronic catalogs. The model is
created for needs of a CAD system of reconstruction of the industrial plants,
where the result of designing are the drawings, which include the
specifications of different types. The adequate simulation of the specification
tables is ensured with technology of storing in the drawing of the visible
geometric elements and invisible parametric representation, sufficient for
generation of this elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405055</id><created>2004-05-17</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Kafiatullov</keyname><forenames>Rustem R.</forenames></author><author><keyname>Safin</keyname><forenames>Ilsur T.</forenames></author></authors><title>Modular technology of developing of the extensions of a CAD system.
  Axonometric piping diagrams. Parametric representation</title><categories>cs.CE cs.DS</categories><comments>8 pages, 1 figure, in Russian</comments><acm-class>E.2, I.2.1, J.6</acm-class><abstract>  Applying the modular technology of developing of the problem-oriented
extensions of a CAD system to a problem of automation of creating of the
axonometric piping diagrams on an example of the program system TechnoCAD
GlassX is described. The proximity of composition of the schemas is detected
for special technological pipe lines, systems of a water line and water drain,
heating, heat supply, ventilating, air conditioning. The structured parametric
representation of the schemas, including properties of objects, their link,
common settings, settings by default and the special links of compatibility is
reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405056</id><created>2004-05-17</created><authors><author><keyname>Safin</keyname><forenames>Ilsur T.</forenames></author><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Kafiatullov</keyname><forenames>Rustem R.</forenames></author></authors><title>Modular technology of developing of the extensions of a CAD system. The
  axonometric piping diagrams. Common and special operations</title><categories>cs.CE cs.DS</categories><comments>8 pages, 7 figures, in Russian</comments><acm-class>I.2.1, J.6</acm-class><abstract>  Applying the modular technology of developing of the problem-oriented
extensions of a CAD system to a problem of automation of creating of the
axonometric piping diagrams on an example of the program system TechnoCAD
GlassX is described. The features of realization of common operations,
composition and realization of special operations of a designing of the schemas
of the special technological pipe lines, systems of a water line and water
drain, heating, heat supply, ventilating, air conditioning are reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405057</id><created>2004-05-17</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>Mathematical and programming toolkit of the computer aided design of the
  axonometric piping diagrams</title><categories>cs.CE cs.DS</categories><comments>3 pages, no figures, in Russian</comments><acm-class>I.2.1, J.6</acm-class><abstract>  The problem of the automation of the designing of the axonometric piping
diagrams include, as the minimum, manipulations with the flat schemas of
three-dimensional wireframe objects (with dimension of 2,5). The specialized
model, methodical and mathematical approaches are required because of large
bulk of calculuss. Coordinate systems, data types, common principles of
realization of operation with data and composition of the basic operations are
described which are realised in the complex CAD system of the reconstruction of
the plants TechnoCAD GlassX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405058</id><created>2004-05-17</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Kroeller</keyname><forenames>Alexander</forenames></author><author><keyname>Pfisterer</keyname><forenames>Dennis</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan</forenames></author><author><keyname>Buschmann</keyname><forenames>Carsten</forenames></author></authors><title>Neighborhood-Based Topology Recognition in Sensor Networks</title><categories>cs.DS cs.DC</categories><comments>14 pages, 6 figures, Latex, to appear in Workshop on Algorithms
  Aspects of Sensor Networks (ALGOSENSORS 2004)</comments><acm-class>C.2.1; F.2.2; G.3</acm-class><abstract>  We consider a crucial aspect of self-organization of a sensor network
consisting of a large set of simple sensor nodes with no location hardware and
only very limited communication range. After having been distributed randomly
in a given two-dimensional region, the nodes are required to develop a sense
for the environment, based on a limited amount of local communication. We
describe algorithmic approaches for determining the structure of boundary nodes
of the region, and the topology of the region. We also develop methods for
determining the outside boundary, the distance to the closest boundary for each
point, the Voronoi diagram of the different boundaries, and the geometric
thickness of the network. Our methods rely on a number of natural assumptions
that are present in densely distributed sets of nodes, and make use of a
combination of stochastics, topology, and geometry. Evaluation requires only a
limited number of simple local computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405059</id><created>2004-05-17</created><updated>2006-01-09</updated><authors><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames><affiliation>Leibniz - IMAG</affiliation></author><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Leibniz - IMAG</affiliation></author></authors><title>Erratum : MCColor is not optimal on Meyniel graphs</title><categories>cs.DM math.CO</categories><proxy>ccsd ccsd-00001574</proxy><acm-class>G.2.2</acm-class><abstract>  A Meyniel graph is a graph in which every odd cycle of length at least five
has two chords. In the manuscript &quot;Coloring Meyniel graphs in linear time&quot; we
claimed that our algorithm MCColor produces an optimal coloring for every
Meyniel graph. But later we found a mistake in the proof and a couterexample to
the optimality, which we present here. MCColor can still be used to find a
stable set that intersects all maximal cliques of a Meyniel graph in linear
time. Consequently it can be used to find an optimal coloring in time O(nm),
and the same holds for Algorithm MCS+Color. This is explained in the manuscript
&quot;A linear algorithm to find a strong stable set in a Meyniel graph&quot; but this is
equivalent to Hertz's algorithm. The current best algorithm for coloring
Meyniel graphs is the O(n^2) algorithm LexColor due to Roussel and Rusu. The
question of finding a linear-time algorithm to color Meyniel graphs is still
open.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="96000" completeListSize="102538">1122234|97001</resumptionToken>
</ListRecords>
</OAI-PMH>
